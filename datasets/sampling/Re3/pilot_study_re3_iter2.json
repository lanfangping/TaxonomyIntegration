{"text_src":"A growing number of universities is further providing support for setting up OA journals or transforming closed to OA journals (for example, by providing an Open Journal Systems infrastructure).","text_tgt":"A growing number of universities is further providing support for setting up OA journals or transforming closed to OA journals (for example, by providing an OJS infrastructure).","gold":"Modify,Clarity","annotator_labels":["Modify,Clarity","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"Here, result, region of interest (ROI) and legend files were obtained with a homemade segmentation macro developed under Fiji as described in the Data requirement section.","text_tgt":"Here, result, region of interest (ROI) and legend files were obtained with an in house-made segmentation macro developed under Fiji.","gold":"Modify,Clarity","annotator_labels":["Modify,Fact\/Evidence","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"To further boost model performance for AC-TUNE, we design two techniques to improve the query strategy and suppress label noise, namely region-aware sampling (RS) and momentum-based memory bank (MMB).","text_tgt":"To further boost the performance on downstream tasks, we design two techniques, namely regionaware sampling (RS) and momentum-based memory bank (MMB) to improve the query strategies and suppress label noise for ACTUNE.","gold":"Modify,Clarity","annotator_labels":["Modify,Fact\/Evidence","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"To fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks.","text_tgt":"In order to fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks.","gold":"Modify,Clarity","annotator_labels":["Modify,Clarity","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"We achieve new state-of-the-art (SOTA) results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset.","text_tgt":"We achieve new state-of-the-art results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset.","gold":"Modify,Clarity","annotator_labels":["Modify,Clarity","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"The focus of this work is text classification.","text_tgt":"","gold":"Add,Fact\/Evidence","annotator_labels":["Add,Fact\/Evidence","Add,Fact\/Evidence","Add,Fact\/Evidence"],"action":"Add","intention":"Fact\/Evidence"}
{"text_src":"The model's goal is to predict whether x b logically follows from x a , i.e. their entailment.","text_tgt":"","gold":"Add,Fact\/Evidence","annotator_labels":["Add,Fact\/Evidence","Add,Fact\/Evidence","Add,Fact\/Evidence"],"action":"Add","intention":"Fact\/Evidence"}
{"text_src":"Additionally, we use a 6-layers Transformer encoder for temporal modelling instead of a 12-layers conformer encoder, which resulted in a smaller back-end size.","text_tgt":"Additionally, we use a 6-layers Transformer encoder for temporal modelling instead of a 12-layers conformer encoder, which resulted in a smaller model size.","gold":"Modify,Fact\/Evidence","annotator_labels":["Modify,Fact\/Evidence","Modify,Fact\/Evidence","Modify,Clarity"],"action":"Modify","intention":"Fact\/Evidence"}
{"text_src":"","text_tgt":"The dimension of predicate embeddings and biGRU layer is 100 and 200, respectively.","gold":"Delete,Fact\/Evidence","annotator_labels":["Delete,Fact\/Evidence","Delete,Fact\/Evidence","Delete,Fact\/Evidence"],"action":"Delete","intention":"Fact\/Evidence"}
{"text_src":"For the generation of Th17 cells, na\u00efve CD4 + T cells (CD4 + CD62L high CD25 \u2212 Foxp3 GFP\u2212 ) carrying the MOG 35-55 -specific 2D2 TCR as a transgene were FACS-purified from peripheral lymphoid tissues of four- to six-week-old 2D2 \u00d7 Foxp3 GFP mice ( Figure 2A ) and cultured under T cell stimulatory conditions that promote efficient differentiation into Th17 cells with a ROR-\u03b3t + IL-17 + phenotype ( Figure 2B and C ).","text_tgt":"For the generation of Th17 cells, na\u00efve CD4 + T cells (CD4 + CD62L high CD25 \u2212 Foxp3 GFP\u2212 ) carrying the MOG 35-55 -specific 2D2 TCR as a transgene were FACS-purified from peripheral lymphoid tissues of 2D2 \u00d7 Foxp3 GFP mice ( Figure 2A ) and cultured under T cell stimulatory conditions that promote efficient differentiation into Th17 cells with a ROR-\u03b3t + IL-17 + phenotype ( Figure 2B and C ).","gold":"Modify,Fact\/Evidence","annotator_labels":["Modify,Fact\/Evidence","Modify,Fact\/Evidence","Modify,Fact\/Evidence"],"action":"Modify","intention":"Fact\/Evidence"}
{"text_src":"ReLU(\u2022) here denotes the ReLU activation function (Nair and Hinton, 2010), \u03c3(\u2022) represents the sigmoid function.","text_tgt":"ReLU(.) here denotes the ReLU activation function (Nair and Hinton, 2010), \u03c3(.) represents the sigmoid function.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"We publicly release SuicideED to support future research in this important area.","text_tgt":"We will publicly release Sui-cideED to support future research in this important area.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"During the lifetime the BGP may be threatened or damaged by socioeconomic disadvantages, diseases, injuries and\/or defects.","text_tgt":"During lifetime the BGP may be threatened or damaged by socioeconomic disadvantages, diseases, injuries and defects.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Clarity"],"action":"Modify","intention":"Grammar"}
{"text_src":"Methodologically, we use the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot fewer than the original pretraining steps).","text_tgt":"Methodologically, we used the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot smaller than the original pretraining steps).","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"The reason is that the link between the attention layer and the model's output cannot be isolated from the other components of the model.","text_tgt":"The reason is that the link between the attention layer and a model's output cannot be isolated from the other components of the model.","gold":"Modify,Grammar","annotator_labels":["Modify,Clarity","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"Ethics Impact","text_tgt":"","gold":"Add,Other","annotator_labels":["Add,Other","Add,Other","Add,Other"],"action":"Add","intention":"Other"}
{"text_src":"Hyper-parameters","text_tgt":"","gold":"Add,Other","annotator_labels":["Add,Other","Add,Other","Add,Other"],"action":"Add","intention":"Other"}
{"text_src":"Discussion","text_tgt":"Results and discussion","gold":"Modify,Other","annotator_labels":["Modify,Other","Modify,Claim","Modify,Clarity"],"action":"Modify","intention":"Other"}
{"text_src":"","text_tgt":"Experimental Settings","gold":"Delete,Other","annotator_labels":["Delete,Other","Delete,Other","Delete,Other"],"action":"Delete","intention":"Other"}
{"text_src":"Effects of interventions for existing self-harm: summary of findings and implications","text_tgt":"Summary of findings: interventions for existing self-harm","gold":"Modify,Other","annotator_labels":["Modify,Other","Modify,Claim","Modify,Clarity"],"action":"Modify","intention":"Other"}
