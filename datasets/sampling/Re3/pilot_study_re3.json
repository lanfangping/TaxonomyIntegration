{"text_src":"Mutations of the recombination-activating genes RAG 1 and RAG 2 are associated with a range of clinical presentations including, severe combined immunodeficiency and autoimmunity.","text_tgt":"","gold":"Add,Claim","annotator_labels":["Add,Claim","Add,Claim","Add,Claim"],"action":"Add","intention":"Claim"}
{"text_src":"There are also national league tables, such as the Complete University guide in the UK, and there are also national ranking systems such as the UK Research Excellence Framework, but in this study we only consider international league tables.","text_tgt":"There are also national league tables, such as the Complete University guide in the UK, but in this study we only consider international league tables.","gold":"Modify,Claim","annotator_labels":["Modify,Claim","Modify,Claim","Modify,Claim"],"action":"Modify","intention":"Claim"}
{"text_src":"This relationship between researcher and sovereign Indigenous institutions can be thought of as highly analogous to the relationship between the researcher and governmental granting agencies such as the U.S. National Science Foundation.","text_tgt":"","gold":"Add,Claim","annotator_labels":["Add,Claim","Add,Claim","Add,Claim"],"action":"Add","intention":"Claim"}
{"text_src":"However, the findings on beneficial effects are overall promising.","text_tgt":"","gold":"Add,Claim","annotator_labels":["Add,Claim","Add,Claim","Add,Claim"],"action":"Add","intention":"Claim"}
{"text_src":"TIFF files however can rarely be properly used in programs for figure assembly (e.g. Inkscape, PowerPoint).","text_tgt":"","gold":"Add,Claim","annotator_labels":["Add,Claim","Add,Claim","Add,Claim"],"action":"Add","intention":"Claim"}
{"text_src":"- 7) To identify what precisely determines the accumulation of UPF1 on some transcripts, and why this appears to be dependent UPF1 ATPase activity <REF-111> .","text_tgt":"","gold":"Add,Claim","annotator_labels":["Add,Claim","Add,Claim","Add,Claim"],"action":"Add","intention":"Claim"}
{"text_src":"","text_tgt":"Publishing in journals (closed-access and Gold, Hybrid or Bronze OA) is less prominent for scholars within these fields.","gold":"Delete,Claim","annotator_labels":["Delete,Claim","Delete,Claim","Delete,Claim"],"action":"Delete","intention":"Claim"}
{"text_src":"Consultants, psychologists, and therapists can use the findings of this study to provide services to families.","text_tgt":"","gold":"Add,Claim","annotator_labels":["Add,Claim","Add,Claim","Add,Claim"],"action":"Add","intention":"Claim"}
{"text_src":"","text_tgt":"Tuning on the backtranslated data seemed to overwhelm the far lower amount of clean parallel data and did not produce a higher BLEU score in any direction.","gold":"Delete,Claim","annotator_labels":["Delete,Claim","Delete,Claim","Delete,Fact\/Evidence"],"action":"Delete","intention":"Claim"}
{"text_src":"","text_tgt":"While long-term access to research outputs is questionable within these models, OpenEdition and others managed to convince otherwise conservative publishers to create open versions of their journal volumes and monographs <REF-66> .","gold":"Delete,Claim","annotator_labels":["Delete,Claim","Delete,Fact\/Evidence","Delete,Claim"],"action":"Delete","intention":"Claim"}
{"text_src":"","text_tgt":"Factors facilitating OA in the natural and technical sciences can be identified as the long-existing culture of preprint distribution, availability in funding for APCs and high levels of awareness of and familiarity with OA publishing.","gold":"Delete,Claim","annotator_labels":["Delete,Claim","Delete,Claim","Delete,Claim"],"action":"Delete","intention":"Claim"}
{"text_src":"I love this movie and have seen it quite a few times over the years.","text_tgt":"","gold":"Add,Claim","annotator_labels":["Add,Claim","Add,Claim","Add,Claim"],"action":"Add","intention":"Claim"}
{"text_src":"An input file may pass or fail the tests run in each module, and high-quality sequencing data from most protocols is expected to pass all tests.","text_tgt":"","gold":"Add,Claim","annotator_labels":["Add,Claim","Add,Claim","Add,Claim"],"action":"Add","intention":"Claim"}
{"text_src":"Thus there is some uncertainty as to how accurate the assumptions are for the costs, attrition rates, and cycle times per phase for the three vaccine archetypes used in our study.","text_tgt":"","gold":"Add,Claim","annotator_labels":["Add,Claim","Add,Claim","Add,Claim"],"action":"Add","intention":"Claim"}
{"text_src":"It would be helpful for future iterations of the P2I tool to incorporate these differences across study settings.","text_tgt":"","gold":"Add,Claim","annotator_labels":["Add,Claim","Add,Claim","Add,Claim"],"action":"Add","intention":"Claim"}
{"text_src":"Our results highlight the importance of carefully specifying the question of interest before selecting a statistical approach.","text_tgt":"Our results highlight the importance of correctly specifying the question of interest before selecting a statistical approach.","gold":"Modify,Clarity","annotator_labels":["Modify,Claim","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"They all had a suppressed HIV viral load for several years before switching to DTG-monotherapy.","text_tgt":"These three patients did not have a history of previous VF and had a suppressed HIV viral load for several years before switching to DTG-monotherapy.","gold":"Split+Modify,Clarity","annotator_labels":["Split+Modify,Clarity","Split+Modify,Clarity","Split+Modify,Clarity"],"action":"Split+Modify","intention":"Clarity"}
{"text_src":"Faithfulness and practicality is often evaluated using automated procedures such as input reduction experiments or measuring time and model complexity.","text_tgt":"Faithfulness and practicality can be evaluated using automated procedures such as input reduction experiments or measuring time and model complexity.","gold":"Modify,Clarity","annotator_labels":["Modify,Claim","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"We found that 95% confidence intervals were narrower as compared to an SD-OCT and TD-OCT and correcting the algorithm errors further narrowed the intervals.","text_tgt":"We found that 95% confidence intervals were narrower compared to an SD-OCT and TD-OCT and correcting the algorithm errors further narrowed the intervals.","gold":"Modify,Clarity","annotator_labels":["Modify,Clarity","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"For example, when only 1% of the training data is available, DEGREE and DEGREE(PIPE) achieve more than 15 points of improvement in trigger classification F1 scores and more than 5 points in argument classification F1 scores.","text_tgt":"For example, when only 1% of training data is available, DEGREE and DEGREE(PIPE) achieve more than 15 points of trigger classification F1scores improvement and more than 5 points of argument classification F1-scores.","gold":"Modify,Clarity","annotator_labels":["Modify,Clarity","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"We run all the experiments by using fairseq (Ott et al., 2019) framework.","text_tgt":"We run all the experiments using WMT14 datasets with fairseq (Ott et al., 2019) framework.","gold":"Modify,Clarity","annotator_labels":["Modify,Clarity","Modify,Fact\/Evidence","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"Transcribed recordings from a single speaker of Kurmanji Kurdish were kindly shared with us by Translators without Borders.","text_tgt":"Transcribed recordings from a single speaker of Kurmanji Kurdish transcribed were kindly shared with us by Translators without Borders.","gold":"Modify,Clarity","annotator_labels":["Modify,Grammar","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"We test faithfulness of these different attention patterns to produce the correct classification via an input reduction experiment on task-tuned BERT models.","text_tgt":"We test this via an input reduction experiment on task-tuned BERT models which highlights the trade-off between a model's faithfulness and sparsity when comparing importance scores to human attention, i.e., less sparse (higher entropy) attention vectors seem to be less faithful with respect to model predictions.","gold":"Link+Modify,Clarity","annotator_labels":["Link+Modify,Clarity","Link+Modify,Clarity","Link+Modify,Clarity"],"action":"Link+Modify","intention":"Clarity"}
{"text_src":"We use the self-collected database that consists of 858 and 1057 of the first two groups, respectively, and 7471 generic Jewish surnames.","text_tgt":"We use the database that consists of 858 and 1057 of the first two groups, respectively, and 7471 generic Jewish surnames.","gold":"Modify,Clarity","annotator_labels":["Modify,Fact\/Evidence","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"Results were validated based on the type of migraine diagnosed by the treating physician and using the respective measurement made during the classification process provided by the artificial neural network.","text_tgt":"Results were validated based on the type of migraine diagnosed by the treating physician and using the respective measurement made during the classification process provided by the neural network.","gold":"Modify,Clarity","annotator_labels":["Modify,Clarity","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"We achieve new state-of-the-art (SOTA) results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset.","text_tgt":"We achieve new state-of-the-art results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset.","gold":"Modify,Clarity","annotator_labels":["Modify,Clarity","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"A growing number of universities is further providing support for setting up OA journals or transforming closed to OA journals (for example, by providing an Open Journal Systems infrastructure).","text_tgt":"A growing number of universities is further providing support for setting up OA journals or transforming closed to OA journals (for example, by providing an OJS infrastructure).","gold":"Modify,Clarity","annotator_labels":["Modify,Clarity","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"Here, result, region of interest (ROI) and legend files were obtained with a homemade segmentation macro developed under Fiji as described in the Data requirement section.","text_tgt":"Here, result, region of interest (ROI) and legend files were obtained with an in house-made segmentation macro developed under Fiji.","gold":"Modify,Clarity","annotator_labels":["Modify,Fact\/Evidence","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"To fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks.","text_tgt":"In order to fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks.","gold":"Modify,Clarity","annotator_labels":["Modify,Clarity","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"To further boost model performance for AC-TUNE, we design two techniques to improve the query strategy and suppress label noise, namely region-aware sampling (RS) and momentum-based memory bank (MMB).","text_tgt":"To further boost the performance on downstream tasks, we design two techniques, namely regionaware sampling (RS) and momentum-based memory bank (MMB) to improve the query strategies and suppress label noise for ACTUNE.","gold":"Modify,Clarity","annotator_labels":["Modify,Fact\/Evidence","Modify,Clarity","Modify,Clarity"],"action":"Modify","intention":"Clarity"}
{"text_src":"We train our model using mini-batches to decrease memory requirements and improve the performance.","text_tgt":"We train our model using mini-batches and an adversarial loss to decrease memory requirements and improve the performance.","gold":"Modify,Fact\/Evidence","annotator_labels":["Modify,Fact\/Evidence","Modify,Fact\/Evidence","Modify,Fact\/Evidence"],"action":"Modify","intention":"Fact\/Evidence"}
{"text_src":"","text_tgt":"Therefore, we handcrafted templates for all 354 predicates, including unseen predicates in the test set.","gold":"Delete,Fact\/Evidence","annotator_labels":["Delete,Fact\/Evidence","Delete,Fact\/Evidence","Delete,Fact\/Evidence"],"action":"Delete","intention":"Fact\/Evidence"}
{"text_src":"","text_tgt":"The definition is defined by the average of the pre-trained embeddings of the terms the definition, and the synset is represented by the pre-trained embedding of the synonym word itself.","gold":"Delete,Fact\/Evidence","annotator_labels":["Delete,Fact\/Evidence","Delete,Fact\/Evidence","Delete,Fact\/Evidence"],"action":"Delete","intention":"Fact\/Evidence"}
{"text_src":"The presence of such building blocks can be a sign that an NLP model exhibits compositional behaviour (Baroni, 2020).","text_tgt":"The presence of such building blocks is a necessary condition for an NLP model to exhibit compositional behaviour (Baroni, 2020).","gold":"Modify,Fact\/Evidence","annotator_labels":["Modify,Fact\/Evidence","Modify,Fact\/Evidence","Modify,Fact\/Evidence"],"action":"Modify","intention":"Fact\/Evidence"}
{"text_src":"For the generation of Th17 cells, na\u00efve CD4 + T cells (CD4 + CD62L high CD25 \u2212 Foxp3 GFP\u2212 ) carrying the MOG 35-55 -specific 2D2 TCR as a transgene were FACS-purified from peripheral lymphoid tissues of four- to six-week-old 2D2 \u00d7 Foxp3 GFP mice ( Figure 2A ) and cultured under T cell stimulatory conditions that promote efficient differentiation into Th17 cells with a ROR-\u03b3t + IL-17 + phenotype ( Figure 2B and C ).","text_tgt":"For the generation of Th17 cells, na\u00efve CD4 + T cells (CD4 + CD62L high CD25 \u2212 Foxp3 GFP\u2212 ) carrying the MOG 35-55 -specific 2D2 TCR as a transgene were FACS-purified from peripheral lymphoid tissues of 2D2 \u00d7 Foxp3 GFP mice ( Figure 2A ) and cultured under T cell stimulatory conditions that promote efficient differentiation into Th17 cells with a ROR-\u03b3t + IL-17 + phenotype ( Figure 2B and C ).","gold":"Modify,Fact\/Evidence","annotator_labels":["Modify,Fact\/Evidence","Modify,Fact\/Evidence","Modify,Fact\/Evidence"],"action":"Modify","intention":"Fact\/Evidence"}
{"text_src":"Note that we set the learning rate to 3e-4, warmup steps to 500 when fine-tuning both MASS and MASS-ZH.","text_tgt":"Note that we set the learning rate to 3e-4, warmup steps to 500, and random seed to 1111 when fine-tuning both MASS and MASS-ZH.","gold":"Modify,Fact\/Evidence","annotator_labels":["Modify,Fact\/Evidence","Modify,Fact\/Evidence","Modify,Fact\/Evidence"],"action":"Modify","intention":"Fact\/Evidence"}
{"text_src":"Additionally, we use a 6-layers Transformer encoder for temporal modelling instead of a 12-layers conformer encoder, which resulted in a smaller back-end size.","text_tgt":"Additionally, we use a 6-layers Transformer encoder for temporal modelling instead of a 12-layers conformer encoder, which resulted in a smaller model size.","gold":"Modify,Fact\/Evidence","annotator_labels":["Modify,Fact\/Evidence","Modify,Fact\/Evidence","Modify,Clarity"],"action":"Modify","intention":"Fact\/Evidence"}
{"text_src":"The focus of this work is text classification.","text_tgt":"","gold":"Add,Fact\/Evidence","annotator_labels":["Add,Fact\/Evidence","Add,Fact\/Evidence","Add,Fact\/Evidence"],"action":"Add","intention":"Fact\/Evidence"}
{"text_src":"The model's goal is to predict whether x b logically follows from x a , i.e. their entailment.","text_tgt":"","gold":"Add,Fact\/Evidence","annotator_labels":["Add,Fact\/Evidence","Add,Fact\/Evidence","Add,Fact\/Evidence"],"action":"Add","intention":"Fact\/Evidence"}
{"text_src":"We probe the first 50K words in GloVe's vocabulary with SpellingBee.","text_tgt":"","gold":"Add,Fact\/Evidence","annotator_labels":["Add,Fact\/Evidence","Add,Fact\/Evidence","Add,Fact\/Evidence"],"action":"Add","intention":"Fact\/Evidence"}
{"text_src":"","text_tgt":"The dimension of predicate embeddings and biGRU layer is 100 and 200, respectively.","gold":"Delete,Fact\/Evidence","annotator_labels":["Delete,Fact\/Evidence","Delete,Fact\/Evidence","Delete,Fact\/Evidence"],"action":"Delete","intention":"Fact\/Evidence"}
{"text_src":"From 1993 to 2013 a total of 1,537 C. acronotus were tagged and released, of these 24 were recaptured.","text_tgt":"","gold":"Add,Fact\/Evidence","annotator_labels":["Add,Fact\/Evidence","Add,Fact\/Evidence","Add,Fact\/Evidence"],"action":"Add","intention":"Fact\/Evidence"}
{"text_src":"The fastest methods was RCA , with both taking less than 25 seconds on average for the entire dataset analysis.","text_tgt":"The fastest methods were RCA and TSCAN , with both taking less than 25 seconds on average for the entire dataset analysis.","gold":"Modify,Fact\/Evidence","annotator_labels":["Modify,Fact\/Evidence","Modify,Fact\/Evidence","Modify,Fact\/Evidence"],"action":"Modify","intention":"Fact\/Evidence"}
{"text_src":"","text_tgt":"As can be seen from Tables Tables 6 unfortu-9 Raw average scores for models in the Ice-breaker run are additionally provided in Table 10 in Appendix A.4.","gold":"Delete,Fact\/Evidence","annotator_labels":["Delete,Fact\/Evidence","Delete,Fact\/Evidence","Delete,Fact\/Evidence"],"action":"Delete","intention":"Fact\/Evidence"}
{"text_src":"","text_tgt":"Figure 5 shows the full distribution of POS tags of the first tokens flipped.","gold":"Delete,Fact\/Evidence","annotator_labels":["Delete,Fact\/Evidence","Delete,Fact\/Evidence","Delete,Fact\/Evidence"],"action":"Delete","intention":"Fact\/Evidence"}
{"text_src":"Sentence-level QE has evolved from the first feature-heavy prediction models (Blatz et al., 2004) to neural architectures such as RNNs and Transformers (Vaswani et al., 2017), which accelerated the developments in the field by reducing the work of manual feature engineering and improving contextual representations (Kim et al., 2017;Wang et al., 2018;Fan et al., 2019).","text_tgt":"Sentencelevel QE has evolved from the first feature-heavy prediction models in Blatz et al. (2004) to neural architectures such as RNNs and Transformers (Vaswani et al., 2017), which accelerated the developments in the field by reducing the work of manual feature engineering and improving contextual representations (Kim et al., 2017;Wang et al., 2018;Fan et al., 2019).","gold":"Modify,Grammar","annotator_labels":["Modify,Clarity","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"The result suggests that meaning-matching is a safe intermediate task that ensures a positive transfer with target downstream tasks.","text_tgt":"The result suggests that the meaningmatching is a safe intermediate task that ensures positive transfer with target downstream tasks.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"In addition to generating the detoxified versions of texts, we consider a way to distill existing datasets of paraphrases for style-specific data.","text_tgt":"In addition to generating the detoxified versions of texts, we consider a way to distil existing datasets of paraphrases for style-specific data.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"GGRaSP was similarly used to select the 250 most diverse genomes including the outliers from the 1,249 downloaded genomes while eliminating very closely related genomes.","text_tgt":"GGRaSP was similarly used to select the 250 most diverse genomes including the outliers from the 1249 downloaded genomes while eliminating very closely related genomes.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"The reason is that the link between the attention layer and the model's output cannot be isolated from the other components of the model.","text_tgt":"The reason is that the link between the attention layer and a model's output cannot be isolated from the other components of the model.","gold":"Modify,Grammar","annotator_labels":["Modify,Clarity","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"Our assessment of certainty of evidence corresponds to GRADE-tables in Table 3 \u2013 Table 16 .","text_tgt":"Our assessment of certainty on the evidence corresponds to GRADE-tables in Table 3 \u2013Table 18.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Fact\/Evidence"],"action":"Modify","intention":"Grammar"}
{"text_src":"During the lifetime the BGP may be threatened or damaged by socioeconomic disadvantages, diseases, injuries and\/or defects.","text_tgt":"During lifetime the BGP may be threatened or damaged by socioeconomic disadvantages, diseases, injuries and defects.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Clarity"],"action":"Modify","intention":"Grammar"}
{"text_src":"ReLU(\u2022) here denotes the ReLU activation function (Nair and Hinton, 2010), \u03c3(\u2022) represents the sigmoid function.","text_tgt":"ReLU(.) here denotes the ReLU activation function (Nair and Hinton, 2010), \u03c3(.) represents the sigmoid function.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"We publicly release SuicideED to support future research in this important area.","text_tgt":"We will publicly release Sui-cideED to support future research in this important area.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"We based our selection of method on the online list within www.scRNA-tools.org <REF-2> in October 2017.","text_tgt":"We based our selection of method on the online list within www.scrna-tools.org <REF-2> in October 2017.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"Methodologically, we use the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot fewer than the original pretraining steps).","text_tgt":"Methodologically, we used the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot smaller than the original pretraining steps).","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"Step 2: modelling of costs to move candidates through pipeline and likely launches, using P2I v.2 model with its existing assumptions","text_tgt":"Step 2: modeling of costs to move candidates through pipeline and likely launches, using P2I v.2 model with its existing assumptions","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"We propose to name clade M\/Hoffmann cluster IV Enterobacter roggenkampii after Andreas Roggenkamp for his work on elucidating the phylogenetic structure of the E. cloacae complex <REF-2> .","text_tgt":"We propose to name clade M \/ Hoffmann cluster IV Enterobacter roggenkampii after Andreas Roggenkamp for his work on elucidating the phylogenetic structure of the E. cloacae complex <REF-2> .","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"In this paper, we first provide a novel approach for automatically creating high-precision sense-annotated parallel corpora, and then put forward a specifically tailored fine-tuning strategy for exploiting these sense annotations during training without introducing any additional requirement at inference time.","text_tgt":"In this paper, we first provide a novel approach for automatically creating highprecision sense-annotated parallel corpora, and then we put forward a specifically tailored finetuning strategy to exploit such sense annotations during training without introducing any additional requirement at inference time.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Clarity"],"action":"Modify","intention":"Grammar"}
{"text_src":"Sent-ctrl uses one control label per target sentence and controls generation on the sentence-level.","text_tgt":"Sent-ctrl uses one control label per target sentence and controls generation on a sentence-level.","gold":"Modify,Grammar","annotator_labels":["Modify,Grammar","Modify,Grammar","Modify,Grammar"],"action":"Modify","intention":"Grammar"}
{"text_src":"ACTUNE: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models","text_tgt":"ACTUNE: Uncertainty-Aware Active Self-Training for Active Fine-Tuning of Pretrained Language Models","gold":"Modify,Other","annotator_labels":["Modify,Other","Modify,Other","Modify,Other"],"action":"Modify","intention":"Other"}
{"text_src":"Word Alignments (MPWAs)","text_tgt":"","gold":"Add,Other","annotator_labels":["Add,Other","Add,Other","Add,Other"],"action":"Add","intention":"Other"}
{"text_src":"","text_tgt":"Multimodal Fusion.","gold":"Delete,Other","annotator_labels":["Delete,Other","Delete,Other","Delete,Fact\/Evidence"],"action":"Delete","intention":"Other"}
{"text_src":"Limitations of Contrastive-Probe","text_tgt":"","gold":"Add,Other","annotator_labels":["Add,Other","Add,Other","Add,Other"],"action":"Add","intention":"Other"}
{"text_src":"Effects of interventions for existing self-harm: summary of findings and implications","text_tgt":"Summary of findings: interventions for existing self-harm","gold":"Modify,Other","annotator_labels":["Modify,Other","Modify,Claim","Modify,Clarity"],"action":"Modify","intention":"Other"}
{"text_src":"Supplementary Figure S3.","text_tgt":"","gold":"Add,Other","annotator_labels":["Add,Fact\/Evidence","Add,Other","Add,Other"],"action":"Add","intention":"Other"}
{"text_src":"Discussion","text_tgt":"Results and discussion","gold":"Modify,Other","annotator_labels":["Modify,Other","Modify,Claim","Modify,Clarity"],"action":"Modify","intention":"Other"}
{"text_src":"Ethics Impact","text_tgt":"","gold":"Add,Other","annotator_labels":["Add,Other","Add,Other","Add,Other"],"action":"Add","intention":"Other"}
{"text_src":"Hyper-parameters","text_tgt":"","gold":"Add,Other","annotator_labels":["Add,Other","Add,Other","Add,Other"],"action":"Add","intention":"Other"}
{"text_src":"PCR and DNA sequencing of ATP synthase a -subunit of Stenotrophomonas species DL18","text_tgt":"DNA sequencing and analysis","gold":"Modify,Other","annotator_labels":["Modify,Other","Modify,Other","Modify,Other"],"action":"Modify","intention":"Other"}
{"text_src":"","text_tgt":"Experimental Settings","gold":"Delete,Other","annotator_labels":["Delete,Other","Delete,Other","Delete,Other"],"action":"Delete","intention":"Other"}
{"text_src":"We use max-pooling to aggregate attention for same-sentence input tokens, because summation unfairly gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out.","text_tgt":"We use max-pooling to aggregate attention for same-sentence input tokens, because summation gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out.","gold":"Modify,Other","annotator_labels":["Modify,Other","Modify,Other","Modify,Claim"],"action":"Modify","intention":"Other"}
{"text_src":"Training Details for Few-Shot Language Learners","text_tgt":"","gold":"Add,Other","annotator_labels":["Add,Other","Add,Other","Add,Other"],"action":"Add","intention":"Other"}
{"text_src":"Experiment 4a","text_tgt":"Experiment 4","gold":"Modify,Other","annotator_labels":["Modify,Other","Modify,Other","Modify,Other"],"action":"Modify","intention":"Other"}
{"text_src":"","text_tgt":"Problem Formulation","gold":"Delete,Other","annotator_labels":["Delete,Other","Delete,Other","Delete,Other"],"action":"Delete","intention":"Other"}
