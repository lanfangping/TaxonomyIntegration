edit_index,doc_name,node_ix_src,node_ix_tgt,text_src,text_tgt,gold,label
0,10-ARR,10-ARR_v2_4@2,,"With this in mind, it is natural to consider how the advancement in natural language processing can be leveraged to help counseling.",,"Add,Claim",Claim
1,10-ARR,10-ARR_v2_5@1,,"Reflective listening asks the counselor not only to listen to the client carefully, but also to actively make a guess of what the client means.",,"Add,Claim",Claim
2,10-ARR,10-ARR_v2_5@3,,"However, people do not always say what they mean, which is especially the case for patients seeking mental support.",,"Add,Claim",Claim
3,10-ARR,10-ARR_v2_5@4,,"Reflection, as the response made based on reflective listening, sometimes needs to decode the client's meaning not explicitly expressed in words.",,"Add,Claim",Claim
4,10-ARR,10-ARR_v2_5@5,,"On the other hand, pressing the client to clarify the missing part may hinder them from expressing their own experience (Miller and Rollnick, 2012).",,"Add,Fact/Evidence",Fact/Evidence
5,10-ARR,10-ARR_v2_5@10,,All these cases pose challenges to state-of-the-art language models.,,"Add,Claim",Claim
6,10-ARR,10-ARR_v2_12@0,,Previous research has addressed the task of automating response generation in health care and counseling settings.,,"Add,Fact/Evidence",Fact/Evidence
7,10-ARR,10-ARR_v2_12@1,,Greer et al. (2019) used a decision tree to deliver pre-written scripts and guide the user to learn a set of positive emotion skills.,,"Add,Fact/Evidence",Fact/Evidence
8,10-ARR,10-ARR_v2_12@2,,V et al. (2019) identified medical entities and the client's intent to fetch an answer for cancer related questions.,,"Add,Fact/Evidence",Fact/Evidence
9,10-ARR,10-ARR_v2_12@3,,Almusharraf et al. (2020) classified client's responses to choose which question to ask next for smoking cessation.,,"Add,Fact/Evidence",Fact/Evidence
10,10-ARR,10-ARR_v2_12@4,,"There are also commercial systems like Woebot (Fitzpatrick et al., 2017) that detect mental health issues mentioned by the user and direct them to relevant information.",,"Add,Fact/Evidence",Fact/Evidence
11,10-ARR,10-ARR_v2_12@5,,"However, there is a limited amount of work on free-form generation as compared to the template-based approaches described above.",,"Add,Claim",Claim
12,10-ARR,10-ARR_v2_12@6,,Shen et al. (2020) focused on generating counseling reflections with GPT-2 based on the dialogue context and responses retrieved from similar counseling sessions.,,"Add,Fact/Evidence",Fact/Evidence
13,10-ARR,10-ARR_v2_12@8,,"To the best of our knowledge, the effect of knowledge in counseling response generation is not yet well studied.",,"Add,Claim",Claim
14,10-ARR,10-ARR_v2_35@4,,We use the original implementation 6 and the pretrained weights on ConceptNet.,,"Add,Fact/Evidence",Fact/Evidence
15,10-ARR,,10-ARR_v1_12@1,,"For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets.","Delete,Fact/Evidence",Fact/Evidence
16,10-ARR,,10-ARR_v1_12@2,,Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation.,"Delete,Fact/Evidence",Fact/Evidence
17,10-ARR,,10-ARR_v1_12@3,,Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation.,"Delete,Fact/Evidence",Fact/Evidence
18,10-ARR,,10-ARR_v1_12@4,,Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet.,"Delete,Fact/Evidence",Fact/Evidence
19,10-ARR,10-ARR_v2_24@1,10-ARR_v1_22@3,Each entity types identified during the extraction has a set of eleven distinct query templates as shown in Table 1.,Each entity types identified during the extraction has a set of distinct query templates as shown in Table 1.,"Modify,Fact/Evidence",Fact/Evidence
20,10-ARR,10-ARR_v2_25@2,10-ARR_v1_23@2,4 The resulting sentences with medical concepts are then considered as knowledge candidates during our next step.,4 The resulting sentences are then considered as knowledge candidates during our next step.,"Modify,Fact/Evidence",Fact/Evidence
21,10-ARR,10-ARR_v2_26@2,10-ARR_v1_24@2,"The positive samples used for this classifier consist of 1,331 sentences with cause-effect relationships (e.g., He had chest pains and headaches from mold in the bedrooms) from the SemEval10 Task 8 dataset (Hendrickx et al., 2010) and an equal amount of negative samples randomly selected from sentences containing other types of semantic relationships in the same dataset.","The positive samples used for this classifier consist of 1,331 cause-effect relationships (e.g., He had chest pains and headaches from mold in the bedrooms) from the SemEval10 Task 8 dataset (Hendrickx et 2010) and an equal amount of negative samples randomly selected from sentences containing other types of semantic relationships in the same dataset.","Modify,Clarity",Clarity
22,10-ARR,10-ARR_v2_5@0,10-ARR_v1_4@2,"Across different counseling styles, reflective listening has always been a fundamental procedure underlying effective counseling practices (Katz and McNulty, 1994).","Effective counseling practice calls for reflective listening as an essential skill (Katz and McNulty, 1994).","Modify,Fact/Evidence",Fact/Evidence
23,10-ARR,10-ARR_v2_5@2,10-ARR_v1_4@3,"If carried out the right way, it gives the client a sense of being understood and facilitates further self-exploration.","It requires the counselor to perceive the other's need or problem, and respond in a way letting the other know he is being understood.","Modify,Claim",Claim
24,10-ARR,10-ARR_v2_36@1,10-ARR_v1_33@6,"Following the categorization in (Hwang et al., 2021), we limit the relationships to the commonsense subset to reduce noise and to limit the number of generated knowledge triplets.","Following the categorization in (Hwang et al., 2020), we limit the relationships to the commonsense subset to reduce noise and to limit the number of generated knowledge triplets.","Modify,Fact/Evidence",Fact/Evidence
25,10-ARR,10-ARR_v2_5@6,10-ARR_v1_4@4,"Thus, counseling frequently calls for counselors to make inferences based on their prior knowledge.","This process frequently involves making inferences based on the counselor's prior knowledge (Miller and Rollnick, 2012).","Modify,Clarity",Clarity
26,10-ARR,10-ARR_v2_5@7,10-ARR_v1_4@5,"For example, when the client says I had a really hard time sticking to my diet this week, a plausible reflection may be You're wondering whether you'll be able to lose weight this way, which relates diet with losing weight as an inference based on commonsense knowledge.","For example, the client says I had a really hard time sticking to my diet this week, a plausible reflection may be You're wondering whether you'll be able to lose weight this way, which relates diet with losing weight as an inference.","Modify,Claim",Claim
27,10-ARR,10-ARR_v2_5@9,10-ARR_v1_4@7,"For example, to understand the client in Figure 1, the counselor needs to know that smoking can be a possible cause of emphysema, and Chantix is a medication for smoke cessation.","For example, to understand the client in Figure 1, the counselor needs to know that smoking can be a possible cause of emphysema, and Chantix is a medication for quit smoking.","Modify,Clarity",Clarity
28,10-ARR,10-ARR_v2_51@3,10-ARR_v1_48@0,"The inplace method, which inserts the relation r and the generated e 2 next to e 1 , shows a significant improvement over the baseline.","The inplace method, which inserts the relation r and the generated e 2 next to e1, shows a significant improvement over the baseline.","Modify,Grammar",Grammar
29,10-ARR,10-ARR_v2_6@1,10-ARR_v1_5@1,"This extra knowledge is needed since existing pre-trained language models struggle to produce coherent and informative responses that capture relevant knowledge, even if they have acquired some knowledge during the pre-training phase (Petroni et al., 2019a).","This is a challenging task since existing pre-trained language models struggle to produce coherent and informative responses that capture relevant knowledge, even if they have acquired some knowledge during the pretraining (Petroni et al., 2019a) phase.","Modify,Claim",Claim
30,10-ARR,10-ARR_v2_60@0,10-ARR_v1_56@0,"We conduct a human evaluation where we ask annotators to indicate their preferences between our best performing models from both the retrieval and the generative settings, and a model without knowledge enhancement.","We conduct a human evaluation where we ask annotators to indicate their preferences between our best performing models from both the retrieval and the generative settings ,and a model without knowledge enhancement.","Modify,Grammar",Grammar
31,10-ARR,10-ARR_v2_62@2,10-ARR_v1_58@2,7 The annotators had no information on which model generated the the response being annotated.,7 The annotators have no information on which model generates the the response being annotated.,"Modify,Grammar",Grammar
32,10-ARR,10-ARR_v2_6@2,10-ARR_v1_5@2,A system that generates accurate counseling reflections can serve as a tool to aid counseling training or assist counselors during a session by providing alternative reflections in response to client's statements.,A system that generates good counseling reflections can serve as a tool to aid counseling training or assist counselors during a session by providing candidate responses.,"Modify,Clarity",Clarity
33,10-ARR,10-ARR_v2_66@3,10-ARR_v1_62@3,"Through an ablation study, we found that commonsense related to intentional and causal relationships is essential for the counseling domain.","Through an ablation study, we found that commonsense related to intentional and causal relationships are essential for the counseling domain.","Modify,Grammar",Grammar
34,10-ARR,10-ARR_v2_7@1,10-ARR_v1_6@1,"The first is retrieval, which acquires sentences containing relevant knowledge based on the vector representations of sentences from the dialogue and assertions in the knowledge base using a BERT-based model (Reimers and Gurevych, 2019a).","The first is retrieval, which acquires sentences containing relevant knowledge using a BERT-based model (Reimers and Gurevych, 2019a) to get vector representations of sentences from the dialogue and assertions in the knowledge base.","Modify,Clarity",Clarity
35,10-ARR,10-ARR_v2_7@2,10-ARR_v1_6@2,"The second strategy is generative, where we first extract key phrases from the dialogue, and query a COMET model for plausible knowledge triplets with a predefined set of relations (Bosselut et al., 2019).","The second strategy is generative, where we first extract key phrases from the dialogue, and query a COMET model for plausible knowledge triplets with a defined set of relations (Bosselut et al., 2019).","Modify,Clarity",Clarity
36,10-ARR,10-ARR_v2_12@7,10-ARR_v1_12@5,We address a similar task but enhance the generation process by infusing commonsense and domain specific knowledge to better emulate what counselors do in practice.,"Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.","Modify,Claim",Claim
37,10-ARR,10-ARR_v2_13@4,10-ARR_v1_13@0,External knowledge resources have been found useful for enhancing language models.,"There are various types of knowledge resources that can be used to enhance language models, focusing on different aspects.","Modify,Claim",Claim
38,10-ARR,10-ARR_v2_13@5,10-ARR_v1_13@1,"For example, large-scale commonsense knowledge graphs (CSKG) that store structured commonsense knowledge in the form of knowledge triplets.","For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets.","Modify,Grammar",Grammar
39,10-ARR,10-ARR_v2_18@0,10-ARR_v1_18@0,"In the following section, we describe the method to obtain relevant knowledge k c and the approach we use to incorporate knowledge into the language model.","In the following section, we describe the method to obtain relevant knowledge k c and the approach we use to incorporate knowledge in the language model.","Modify,Grammar",Grammar
40,10-ARR,10-ARR_v2_20@0,10-ARR_v1_20@0,"Despite their large size, existing commonsense knowledge bases contain a limited amount of information on domain-specific concepts, especially for causal relationships such as the reason to take a medicine or its side effects.","Despite their large size, existing commonsense knowledge bases contain a limited amount of information on some domain-specific concepts, especially causal relationships such as the reason to take a medicine or its side effects.","Modify,Clarity",Clarity
41,103-ARR,,103-ARR_v1_41@1,,"Table 3 reports our NLI system, including the median F1-Score and the standard deviation across 3 different runs of our implementations NLI and EM.","Delete,Fact/Evidence",Fact/Evidence
42,103-ARR,,103-ARR_v1_41@2,,On ACE our system is best on all comparable results.,"Delete,Fact/Evidence",Fact/Evidence
43,103-ARR,,103-ARR_v1_41@3,,"Note that RCEE_ER is better on 3 data splits, but unfortunately the splits are different.","Delete,Fact/Evidence",Fact/Evidence
44,103-ARR,103-ARR_v2_53@0,,"Our work paves the way for a new paradigm for IE, where the expert defines the schema using natural language and directly runs those specifications, annotating a handful of examples in the process, and allowing for quick trial-and-error iterations.",,"Add,Claim",Claim
45,103-ARR,103-ARR_v2_53@1,,Sainz et al. (2022) propose a user interface alongside this paradigm.,,"Add,Fact/Evidence",Fact/Evidence
46,103-ARR,103-ARR_v2_53@2,,"More generally, inference capability could be extended, acquired and applied from other tasks, in a research avenue where entailment and task performance improve in tandem.",,"Add,Claim",Claim
47,103-ARR,103-ARR_v2_55@0,,The fine-tuned models derived from this work will be uploaded to HuggingFace Models repository.,,"Add,Fact/Evidence",Fact/Evidence
48,103-ARR,103-ARR_v2_55@1,,Check the GitHub repository for updated information.,,"Add,Fact/Evidence",Fact/Evidence
49,103-ARR,103-ARR_v2_2@4,103-ARR_v1_2@4,"Thanks to the entailment, the multi-source transfer between ACE and WikiEvents further reduces annotation down to 10% and 5% (respectively) of the full training without transfer.","Thanks to entailment, the multi-source transfer between ACE and WikiEvents further reduces annotation down to 10% and 5% (respectively) of the full training without transfer.","Modify,Grammar",Grammar
50,103-ARR,103-ARR_v2_21@0,103-ARR_v1_22@0,"Inference takes into account three key factors to output the role label for an argument candidate: the entailment probabilities of each verbalization, the type constraints of the specific role, and a threshold.","Inference takes into account three key factors to output the role label for an argument candidate: the entailment probabilities of each verbalization, the type constraints of the specific role and a threshold.","Modify,Grammar",Grammar
51,103-ARR,103-ARR_v2_2@5,103-ARR_v1_2@5,Our analysis shows that the key to good results is the use of several entailment datasets to pre-train the entailment model.,Our analysis shows that key to good results is the use of several entailment datasets to pre-train the entailment model.,"Modify,Grammar",Grammar
52,103-ARR,103-ARR_v2_21@5,103-ARR_v1_23@2,"For this purpose, we convert the EAE training dataset into a NLI format, i.e we generate entailment, neutral and contradiction hypotheses heuristically from the data using the templates themselves.","For this purpose we convert the EAE training dataset into an NLI format, i.e we generate entailment, neutral and contradiction hypotheses heuristically from the data using the templates themselves.","Modify,Grammar",Grammar
53,103-ARR,103-ARR_v2_2@6,103-ARR_v1_2@6,"Similar to previous approaches, our method requires a small amount of effort for manual verbalization: only less than 15 minutes per event argument type is needed, and comparable results can be achieved with users with different level of expertise.","Similar to previous approaches, our method requires a small amount of effort for manual verbalization: only less than 15 minutes per event argument types is needed; comparable results can be achieved from users of different level of expertise.","Modify,Grammar",Grammar
54,103-ARR,103-ARR_v2_29@1,103-ARR_v1_31@1,"During the creation, the template developers had access to the guidelines that describe each of the roles (which can include one or two examples) and a NLI model that the developer could use to verify whether the generated verbalizations of these examples were entailed by the model.","During the creation, the template developers had access to the guidelines that describe each of the roles (which can include one or two examples) and an NLI model that the developer could use to verify whether the generated verbalizations of these examples are entailed by the model.","Modify,Grammar",Grammar
55,103-ARR,103-ARR_v2_31@3,103-ARR_v1_33@3,The WikiEvents dataset is instead more focused on document-level argument extraction task.,"The WikiEvents dataset instead, is more focused on document-level argument extraction task.","Modify,Clarity",Clarity
56,103-ARR,103-ARR_v2_4@0,103-ARR_v1_4@0,"Building Information Extraction (IE) systems for real-world applications is very costly and has suffered from data-scarcity problems, due in part to the expertise and time required to annotate training data at a large scale with sufficient consistency, but also due to poor transfer between domains: IE annotations depend on the schema used in each domain, and moving to new domains requires new schemas, new annotation guidelines and the manual annotation of new data.","Building Information Extraction (IE) systems for real-world applications is very costly and has suffered from data-scarcity problems, due in part to the expertise and time required to annotate training data at a large scale with sufficient consistency, but also due to poor transfer between domains: IE annotations depend on the schema used in each domain, and moving to new domains requires a new schemas, new annotation guidelines and manual annotation of new data.","Modify,Grammar",Grammar
57,103-ARR,103-ARR_v2_34@2,103-ARR_v1_36@2,"EM is a state-of-the-art (Zhou and Chen, 2021) model that uses ROBERTA LARGE as a backbone.",EM uses RoBERTa large.,"Modify,Fact/Evidence",Fact/Evidence
58,103-ARR,103-ARR_v2_2@0,103-ARR_v1_2@0,"Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as Textual Entailment tasks using verbalizations, with strong performance in zero-shot and fewshot settings thanks to pre-trained entailment models.","Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as a Textual Entailment tasks using verbalizations, with strong performance in zero-shot and few-shot settings thanks to pre-trained entailment models.","Modify,Grammar",Grammar
59,103-ARR,103-ARR_v2_37@2,103-ARR_v1_39@2,"In total, 464.56 hours (154.86 if only a single run is done) of computation time are required to reproduce all the experiments, that in our setting corresponds to 21.36 kgCO 2 eq carbon footprint 10 (roughly equivalent to the CO 2 emitted by 88.2 km driven by an average car).","In total, 464.56 hours (154.86 if only a single run is done) of computation time are required to reproduce all the experiments, that in our setting corresponds to 21.36 kgCO 2 eq carbon footprint 10 (roughly equivalent to the CO 2 emitted by 88.2Km driven by an average car).","Modify,Grammar",Grammar
60,103-ARR,103-ARR_v2_40@1,103-ARR_v1_42@1,"Sequentially fine-tuning our NLI model in TA-CRED and then in our target task shows small improvements on low-resource scenarios (0% split for ACE, 0% and 5% splits for WikiEvents).","Sequentially fine-tuning our NLI model in TA-CRED and then in our target task show small improvements on low-resource scenarios (0% split for ACE, 0% and 5% splits for WikiEvents).","Modify,Grammar",Grammar
61,103-ARR,103-ARR_v2_45@6,103-ARR_v1_47@5,"Our results suggest that new, more challenging NLI datasets, as well as NLI datasets automatically generated from other sources (as done in this work with WikiEvents and ACE) will yield more robust entailment models, and could further increase the performance of entailment-based EAE and IE.","Our results suggest that new, more challenging NLI datasets, as well as NLI datasets automatically generated from other sources (as done in this work with Wikievents and ACE) will yield more robust entailment models, and could further increase the performance of entailment-based EAE and IE.","Modify,Grammar",Grammar
62,103-ARR,103-ARR_v2_49@17,103-ARR_v1_51@17,"Based on our estimation, 9 hours would allow an annotator to annotate 5% of the dataset which yields a 37.5 F1 (Figure 5), while 5 hours of template building yields 40.6 F1-Score in the zero-shot setting.","Based on our estimation, 9 hours would allow an annotator to annotate 5% of the dataset which yields an 37.5 F1 (Figure 5), while 5 hours of template building yields 40.6 F1-Score in the zero-shot setting.","Modify,Grammar",Grammar
63,103-ARR,103-ARR_v2_49@19,103-ARR_v1_51@19,"Figure 5 plots the performance according to manual hours on ACE, showing the huge gains provided by the initial 5 hours writing templates, plus the reuse of WikiEvents annotations.","Figure 5 plots the performance according to manual hours on ACE, showing the huge gains provided by the initial 5 hours writing templates, plus the reuse of WikiEvent annotations.","Modify,Grammar",Grammar
64,103-ARR,103-ARR_v2_52@0,103-ARR_v1_54@0,"This paper shows the entailment-based approach for event argument extraction is extremely effective in zero-shot, few-shot and full train scenarios both on ACE and WikiEvents, outperforming previous methods.","This paper shows the entailment-base approach for event argument extraction is extremely effective in zero-shot, few-shot and full train scenarios both on ACE and WikiEvents, outperforming previous methods.","Modify,Grammar",Grammar
65,103-ARR,103-ARR_v2_54@4,103-ARR_v1_55@4,"For the future, we plan to test new hyperparameter sets that uses bigger batch-sizes, as recent works (Aribandi et al., 2022) suggest to be optimal for multi-task and -source learning experiments.","For the future, we plan to test new hyperparameter sets that uses bigger batch-sizes, as recent works (Aribandi et al., 2021) suggest to be optimal for multi-task and -source learning experiments.","Modify,Fact/Evidence",Fact/Evidence
66,103-ARR,103-ARR_v2_7@0,103-ARR_v1_7@0,"(1) We show that our method reduces schema dependency, as it improves the performance on the WikiEvents results using additional ACE training data and vice versa with no extra manual work.","(1) We show that our method reduces schema dependency, as it improves the performance on the Wikievents results using additional ACE training data and vice versa with no extra manual work.","Modify,Grammar",Grammar
67,103-ARR,103-ARR_v2_8@1,103-ARR_v1_8@1,"We make the code, templates and models publicly available.",We make the code and templates publicly available 2 .,"Modify,Fact/Evidence",Fact/Evidence
68,103-ARR,103-ARR_v2_2@2,103-ARR_v1_2@2,"In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE and WikiEvents respectively, while achieving the same performance as with full training.","In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE and WikiEvents, respectively, while achieving the same performance as with full training.","Modify,Grammar",Grammar
69,103-ARR,103-ARR_v2_12@0,103-ARR_v1_12@0,"Multi-task learning reformulates multiple tasks to a single and common task via prompting large pre-trained language models, leveraging multiple data sources to improve each task of interest.","Multi-task learning reformulates multiple tasks to a single and common task via prompting large pre-trained language models, leveraging multiple data sources to improve each tasks of interest.","Modify,Grammar",Grammar
70,103-ARR,103-ARR_v2_12@3,103-ARR_v1_13@2,Wei et al. (2021a) and Mishra et al. (2022) obtained contradictory results.,Wei et al. (2021a) and Mishra et al. (2021) obtained contradictory results.,"Modify,Fact/Evidence",Fact/Evidence
71,103-ARR,103-ARR_v2_12@5,103-ARR_v1_13@4,"In this work, we explore multi-source learning, where datasets from different or similar tasks are used to build a model for the target task.","In this work we explore multi-source learning, where datasets from different or similar tasks are used to build a model for the target task.","Modify,Grammar",Grammar
72,103-ARR,103-ARR_v2_13@0,103-ARR_v1_14@0,Event Argument Extraction is a sub-task of Event Extraction.,Event Argument Extraction (EAE) is a subtask of Event Extraction.,"Modify,Clarity",Clarity
73,103-ARR,103-ARR_v2_14@1,103-ARR_v1_15@1,"Lately, with the recent paradigm shift to prompt design learning (Min et al., 2021), several works reformulated the task as a Question Answering problem Feng et al., 2020;Du and Cardie, 2020b;Wei et al., 2021b;Lyu et al., 2021;Sulem et al., 2022) or as a Constrained Text Generation problem Du et al., 2021; using predefined prompts, questions or templates.","Lately, with the recent paradigm shift to prompt design learning (Min et al., 2021), several works reformulated the task as a Question Answering problem Feng et al., 2020;Du and Cardie, 2020b;Wei et al., 2021b;Lyu et al., 2021) or as a Constrained Text Generation problem Du et al., 2021; using predefined prompts, questions or templates.","Modify,Fact/Evidence",Fact/Evidence
74,103-ARR,103-ARR_v2_17@3,103-ARR_v1_18@2,"First, the possible roles are verbalized by means of predefined templates and the input, which comprises the context, trigger and argument candidate.","First, the possible roles are verbalized by means of predefined templates and the input, which comprises context, trigger and argument candidate.","Modify,Grammar",Grammar
75,110-ARR,,110-ARR_v1_37@0,,PLMs lack knowledge of antonyms.,"Delete,Claim",Claim
76,110-ARR,,110-ARR_v1_52@4,,Training details.,"Delete,Other",Other
77,110-ARR,,110-ARR_v1_59@0,,Catastrophic forgetting.,"Delete,Other",Other
78,110-ARR,110-ARR_v2_48@4,,"However, the representation hardly captures their semantic antonomy, e.g., gender.",,"Add,Claim",Claim
79,110-ARR,110-ARR_v2_53@1,,We multiply 100 to each value for a better readability.,,"Add,Fact/Evidence",Fact/Evidence
80,110-ARR,110-ARR_v2_53@2,,Note that the lower the values the better.,,"Add,Fact/Evidence",Fact/Evidence
81,110-ARR,110-ARR_v2_67@2,,We train the models for 10 epochs for each dataset and apply the early stopping technique where the patience number is set to 3.,,"Add,Fact/Evidence",Fact/Evidence
82,110-ARR,110-ARR_v2_67@3,,It is observed that the training is generally finished within 8 epochs for all the models.,,"Add,Fact/Evidence",Fact/Evidence
83,110-ARR,110-ARR_v2_67@4,,The batch size per GPU and learning rates used for each dataset are described in Table 8.,,"Add,Fact/Evidence",Fact/Evidence
84,110-ARR,110-ARR_v2_67@5,,"Datasets with large training set (e.g., MNLI, QNLI, and QQP) were not sensitive to the hyperparameters.",,"Add,Fact/Evidence",Fact/Evidence
85,110-ARR,110-ARR_v2_26@0,110-ARR_v1_27@0,"To reflect the prediction confidence score to the evaluation metric, we additionally define the weighted top-k hit rate (WHR@k) that uses the confidence score as weights.","To reflect the confidence score to the evaluation metric, we additionally define the weighted top-k hit rate (WHR@k) that uses the confidence score as weights.","Modify,Clarity",Clarity
86,110-ARR,110-ARR_v2_26@1,110-ARR_v1_27@1,It is worth to mention that lower metrics mean a better model performance in both cases as the metrics assess how likely the models make inaccurate answers that they must avoid.,It is worth to mention that lower metrics mean a better model performance in both cases.,"Modify,Fact/Evidence",Fact/Evidence
87,110-ARR,110-ARR_v2_30@2,110-ARR_v1_31@2,"We added the ELECTRA-small/base/large models (Clark et al., 2020) for the SAR task, but it is not used for the MKR-NQ and MWR experiments, as the discriminator of the ELECTRA models are trained with the replaced token prediction (RTP) training objective and have no MLM classifier.","We added the Electra-small/base/large models (Clark et al., 2020) for the SAR task, which are trained with the replaced token prediction (RTP) training objective.","Modify,Fact/Evidence",Fact/Evidence
88,110-ARR,110-ARR_v2_30@5,110-ARR_v1_31@5,"We use the AdamW optimiser (Loshchilov and Hutter, 2019) for training with a learning rate of 5e −6 and a batch size of 32.","We use the AdamW optimiser (Loshchilov and Hutter, 2017) for training with a learning rate of 5e −6 and a batch size of 32.","Modify,Fact/Evidence",Fact/Evidence
89,110-ARR,110-ARR_v2_4@0,110-ARR_v1_4@0,"Contemporary large-size PLMs, such as BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020), and GPT-2 and -3 (Radford et al., 2019;Brown et al., 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al., 2018) and Super-GLUE (Wang et al., 2019b) benchmark datasets.","Contemporary large-size PLMs, such as BERT (Devlin et al., 2019), Electra (Clark et al., 2020), and GPT-2 and -3 (Radford et al., 2019;Brown et al., 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) benchmark datasets.","Modify,Grammar",Grammar
90,110-ARR,110-ARR_v2_42@2,110-ARR_v1_42@4,"However, the difference between the large and small encoderfixed models is insignificant, except for the ELEC-TRA models that exhibit only a marginal improvement.","However, the difference between the large and small encoder-fixed models is insignificant, except for the Electra models that exhibit only a marginal improvement.","Modify,Grammar",Grammar
91,110-ARR,110-ARR_v2_42@3,110-ARR_v1_42@5,"The two phenomenons suggest that PLMs' outstanding performance is predicated on updating many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019;McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information.","The two phenomenons suggest that PLMs' outstanding performance is predicated on updating a great many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019;McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information.","Modify,Clarity",Clarity
92,110-ARR,110-ARR_v2_48@0,110-ARR_v1_48@0,"However, the problem is that the distributional hypothesis has limitations in reflecting a word's semantic meanings, because words having different or even opposite semantic meanings can appear in similar or the same contexts.","However, the problem is that the distributional hypothesis does not consistently hold in natural language.","Merge+Modify,Claim",Claim
93,110-ARR,110-ARR_v2_48@0,110-ARR_v1_48@1,"However, the problem is that the distributional hypothesis has limitations in reflecting a word's semantic meanings, because words having different or even opposite semantic meanings can appear in similar or the same contexts.",Words having different meanings can appear in similar or even the same contexts.,"Merge+Modify,Claim",Claim
94,110-ARR,110-ARR_v2_48@2,110-ARR_v1_48@3,"We can readily imagine sentences in which the two words appear in the same context, e.g., ""the little boy/girl cuddled the teddy bear closely"".","Despite their antonymy, we can readily imagine sentences in which the two words appear in the same context, e.g., ""The little boy/girl cuddled the teddy bear closely."".","Modify,Clarity",Clarity
95,110-ARR,110-ARR_v2_48@3,110-ARR_v1_48@4,"As a result, a model can learn their common functional meanings, i.e., young human beings, and the vector representations would be very similar if they were trained based on the distributional hypothesis.","As a result, the meaning of the two words would become quite similar if they were trained based on the distributional hypothesis.","Modify,Claim",Claim
96,110-ARR,110-ARR_v2_48@6,110-ARR_v1_48@6,"As a result, models cannot effectively learn the semantic meaning of words and negation expressions, provided they leverage only the text forms.","As a result, models can not learn the true meaning of words and negation expressions, provided they leverage only the text forms.","Modify,Clarity",Clarity
97,110-ARR,110-ARR_v2_5@1,110-ARR_v1_5@1,"Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2021;Gupta et al., 2021; Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2020;Gupta et al., 2021;Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
98,110-ARR,110-ARR_v2_54@7,110-ARR_v1_52@12,"We conjecture that a leading cause is that the dataset contains many words with similar meanings, mostly derived from the same stem.","We conjecture a leading cause is that the dataset contains many words with similar meanings, mostly derived from the same stem.","Modify,Clarity",Clarity
99,110-ARR,110-ARR_v2_59@1,110-ARR_v1_57@1,"After the intermediate training, all models are fine-tuned on the SAR task with the same hyperparameters described in Section 3.","For training, we use the same hyperparameters as described in Section 3.","Modify,Fact/Evidence",Fact/Evidence
100,110-ARR,110-ARR_v2_59@6,110-ARR_v1_58@3,Our results show that the proposed approach assists PLMs to learn enhanced representations with more abundant lexical semantic information.,Our results show that the proposed approach assists PLMs to learn enhanced contextualised representations with more abundant lexical semantic information.,"Modify,Clarity",Clarity
101,110-ARR,110-ARR_v2_60@0,110-ARR_v1_59@1,"We find that small PLMs, such as ELECTRA-small and ALBERT models, show no significant increase in performance or are negatively impacted.","We find that small PLMs, such as Electra-small and ALBERT models, show no significant increase in performance or are negatively impacted.","Modify,Grammar",Grammar
102,110-ARR,110-ARR_v2_62@1,110-ARR_v1_61@1,"We observe that the parameters of the ELECTRA-small model, which is negatively impacted, are changed considerably compared to other PLMs having parameters more than 100M.","We observe that the parameters of the Electra-small model, which is negatively impacted, are changed considerably compared to other PLMs having parameters more than 100M.","Modify,Grammar",Grammar
103,110-ARR,110-ARR_v2_67@1,110-ARR_v1_66@1,"To confirm whether the issue occurs, we compare the performance of BERT, RoBERTa, and ELECTRA-large on 7 GLUE benchmark datasets (Wang et al., 2018) with their IM 2 counterparts.","To confirm whether the issue occurs, we compare the performance of BERT, RoBERTa, and Electra-large on 7 GLUE benchmark datasets (Wang et al., 2018).","Modify,Fact/Evidence",Fact/Evidence
104,110-ARR,110-ARR_v2_68@1,110-ARR_v1_67@0,"We find no significant difference in performance for tasks with large datasets, such as MNLI, QNLI, QQP, and SST2.","We find no significant difference in performance for tasks with large datasets, such as the MNLI, QNLI, QQP, and SST2.","Modify,Grammar",Grammar
105,110-ARR,110-ARR_v2_68@2,110-ARR_v1_67@1,"On the contrary, tasks with small datasets, like MRPC and RTE, are slightly improved.","On the contrary, tasks with small datasets, like the MRPC and RTE, are slightly improved.","Modify,Grammar",Grammar
106,110-ARR,110-ARR_v2_68@5,110-ARR_v1_67@4,The result suggests that meaning-matching is a safe intermediate task that ensures a positive transfer with target downstream tasks.,The result suggests that the meaningmatching is a safe intermediate task that ensures positive transfer with target downstream tasks.,"Modify,Grammar",Grammar
107,110-ARR,110-ARR_v2_70@0,110-ARR_v1_69@0,"Finally, we conduct experiments on the NegNLI benchmark dataset (Hossain et al., 2020), where negation plays an important role for NLI tasks.","Finally, we conduct experiments on the NegNLI benchmark dataset (Hossain et al., 2020) where negation plays an important role for NLI tasks.","Modify,Grammar",Grammar
108,110-ARR,110-ARR_v2_71@0,110-ARR_v1_70@0,"For both SNLI and MNLI, we observe that our approach outperforms BERTNOT in the NegNLI datasets, while yielding a comparable performance in the original development datasets.","For both SNLI and MNLI, we observe that our approach outperforms BERTNOT in NegNLI datasets, while yielding a comparable performance in the original development datasets.","Modify,Grammar",Grammar
109,110-ARR,110-ARR_v2_73@3,110-ARR_v1_72@3,"Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2021;Gupta et al., 2021;Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2020;Gupta et al., 2021;Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
110,110-ARR,110-ARR_v2_74@1,110-ARR_v1_73@1,Ettinger (2020) check the ability of PLMs to understand the meaning of negation in given contexts.,Ettinger (2020) check the ability of PLMs to understand of the meaning of negation in given contexts.,"Modify,Grammar",Grammar
111,110-ARR,110-ARR_v2_75@1,110-ARR_v1_74@1,"In their remedy, they augment the language modelling objective with an unlikelihood objective (Welleck et al., 2020) based on negated sentences from the training corpus.","In their remedy, they augment the language modelling objective with an unlikelihood objective (Welleck et al., 2019) based on negated sentences from the training corpus.","Modify,Fact/Evidence",Fact/Evidence
112,110-ARR,110-ARR_v2_75@3,110-ARR_v1_74@3,"In this method, the dependency parse of the sentences, POS tags, and morphological information of each word are taken as input, and the negation of sentences is done using sets of dependency tree regular expression patterns, such as Semgrex (Chambers et al., 2007).","In this method, the dependency parse of the sentences, POS tags and morphological information of each word are taken as input and the negation of sentences is done using sets of dependency tree regular expression patterns, such as Semgrex (Chambers et al., 2007).","Modify,Grammar",Grammar
113,110-ARR,110-ARR_v2_76@0,110-ARR_v1_75@0,"Previous studies (e.g., Kassner and Schütze (2020)) have mostly limited the scope of the logical negation property only to the negation expressions (e.g., ""no"" and ""not"").","Previous studies, e.g., Kassner and Schütze (2020), have mostly limited the scope of the logical negation property only to the negation expressions (e.g., ""no"" and ""not"").","Modify,Grammar",Grammar
114,110-ARR,110-ARR_v2_76@1,110-ARR_v1_75@1,"However, the core spirit of this property is the opposite meaning, which is not only limited to the negation.","However, the core spirit of the property is opposite-meaning, which is not only limited to negation.","Modify,Grammar",Grammar
115,110-ARR,110-ARR_v2_76@2,110-ARR_v1_75@2,Welleck et al. (2020) consider negating sentences using dependency tree regular expression patterns.,Welleck et al. (2019) consider negating sentences using dependency tree regular expression patterns.,"Modify,Fact/Evidence",Fact/Evidence
116,110-ARR,110-ARR_v2_76@3,110-ARR_v1_75@3,"This widens the scope of negation, as it is not only limited to the negation expressions ""no"" and ""not"".","This widens the scope of negation, as it is not only limited to negation expressions ""no"" and ""not"".","Modify,Grammar",Grammar
117,110-ARR,110-ARR_v2_76@4,110-ARR_v1_75@4,"However, their approach relies on other components, such as Semgrex, and dependency and POS parsers, which could impact the quality of the data, hence impact the models' performance.","However, their approach relies on other components, such as Semgrex, and dependency and POS parsers which could impact the quality of the data, hence impact the models' performance.","Modify,Grammar",Grammar
118,110-ARR,110-ARR_v2_76@5,110-ARR_v1_75@5,"In this work, we consider other perturbation methods to generate the opposite-meaning sentences to investigate whether PLMs satisfy the logical negation property, and we propose a remedy, called intermediate-training on meaning-matching (IM 2 ), which hardly employs additional linguistic components.","In this work, we consider other perturbation methods to generate the opposite-meaning sentences to investigate whether PLMs satisfy the logical negation property, and we propose a remedy called intermediate-training on meaning-matching (IM 2 ) that hardly employs additional linguistic components.","Modify,Clarity",Clarity
119,110-ARR,110-ARR_v2_79@0,110-ARR_v1_78@0,We hypothesise that the distributional hypothesis is an insufficient basis for understanding the semantic meaning of texts.,We hypothesise that the distributional hypothesis results in PLMs' lack of understanding of the true meaning of texts.,"Modify,Clarity",Clarity
120,110-ARR,110-ARR_v2_8@1,110-ARR_v1_8@1,"Hosseini et al. (2021) recently employed data augmentation and unlikelihood training (Welleck et al., 2020) to prevent models from generating unwanted words, given the augmented negated data during masked language modelling (MLM).","Hosseini et al. (2021) recently employed data augmentation and unlikelihood training (Welleck et al., 2019) to prevent models from generating unwanted words, given the augmented negated data during masked language modelling (MLM).","Modify,Fact/Evidence",Fact/Evidence
121,110-ARR,110-ARR_v2_8@4,110-ARR_v1_8@4,"Second, the data augmentation method is contingent on many additional linguistic compo-nents, which causes the dependency of a model's performance on certain modules and precludes applying the method to other languages where such resources are unavailable.","Second, the data augmentation method is contingent on many additional linguistic components, which causes the dependency of model's performance on certain modules and precludes applying the method to other languages where such resources are unavailable.","Modify,Grammar",Grammar
122,110-ARR,110-ARR_v2_9@1,110-ARR_v1_10@0,"Next, we propose a remedy, called intermediate-training on meaning-matching (IM 2 ), which hardly employs additional linguistic components.","Next, we propose a remedy, called intermediatetraining on meaning-matching (IM 2 ), that hardly employs additional linguistic components.","Modify,Clarity",Clarity
123,110-ARR,110-ARR_v2_10@0,110-ARR_v1_11@0,"Our main contributions are as follows: (i) We extend the investigation of the LNP from negation to lexical semantics (Section 2), (ii) we reveal that PLMs are prone to violate the LNP (Section 3), (iii) we propose a novel remedy, named IM 2 , which is decoupled from the distributional hypothesis but learns meaning-text correspondence instead (Section 4), (iv) through experiments, we ascertain that the proposed approach improves the understanding of negation and lexical semantic information (Sections 5.1 and 5.2), and (v) we verify that meaningmatching is a stable and safe intermediate task that produces a similar or better performance in multiple downstream tasks (Sections 5.3 and 5.4).","Our main contributions are as follows: (i) We extend the investigation of the LNP from negation to lexical semantics (Section 2), (ii) we reveal that PLMs are prone to violate the LNP (Section 3), (iii) we propose a novel remedy named IM 2 that is decoupled from the distributional hypothesis but learns meaning-text correspondence instead (Section 4), (iv) through experiments, we ascertain that the proposed approach improves the understanding of negation and lexical semantic information (Sections 5.1 and 5.2), and (v) we verify that meaningmatching is a stable and safe intermediate task that produces a similar or better performance in multiple downstream tasks (Sections 5.3 and 5.4).","Modify,Clarity",Clarity
124,110-ARR,110-ARR_v2_2@5,110-ARR_v1_2@5,"To alleviate the issue, we propose a novel intermediate training task, named meaning-matching, designed to directly learn a meaning-text correspondence, instead of relying on the distributional hypothesis.","To alleviate the issue, we propose a novel intermediate training task, named meaningmatching, designed to directly learn a meaningtext correspondence, instead of relying on the distributional hypothesis.","Modify,Grammar",Grammar
125,111-ARR,111-ARR_v2_89@0,,Conclusion,,"Add,Other",Other
126,111-ARR,111-ARR_v2_116@3,,"In Table 9 we see a stronger correlation of human annotations with LaBSE compared to Sentence-BERT, especially for languages like Bengali, Kannada for which Sentence-BERT did not see parallel data.",,"Add,Fact/Evidence",Fact/Evidence
127,111-ARR,111-ARR_v2_117@1,,"Overall, LaBSE correlates more strongly than Sentence-BERT with our annotated data.",,"Add,Fact/Evidence",Fact/Evidence
128,111-ARR,111-ARR_v2_121@2,,"In As we increase the threshold L, we see this percentage substantially reduces, indicating our chosen thresholds are within the range of variation in LaBSE scores for semantically similar sentences.",,"Add,Fact/Evidence",Fact/Evidence
129,111-ARR,111-ARR_v2_125@1,,"20 A potential tool for fluency evaluation in future work is LAMBRE (Pratapa et al., 2021).",,"Add,Claim",Claim
130,111-ARR,111-ARR_v2_125@2,,"However, the original paper does not evaluate performance on Indic languages and the grammars for Indic languages would need to collected / built.",,"Add,Claim",Claim
131,111-ARR,111-ARR_v2_141@0,,"In Figure 9 we measure the lexical overlap between paraphrases used in our DIFFUR training strategy for six different languages (Hindi, Bengali, Kannada, Telugu, Swahili and Spanish).",,"Add,Fact/Evidence",Fact/Evidence
132,111-ARR,111-ARR_v2_141@1,,"The lexical overlap is measured using the unigram F1 score, using the implementation from the SQuAD evaluation script (Rajpurkar et al., 2016).",,"Add,Fact/Evidence",Fact/Evidence
133,111-ARR,111-ARR_v2_141@2,,The wide spread of the histogram and sufficient percentage of low overlap pairs confirm the lexical diversity of the paraphrases used.,,"Add,Fact/Evidence",Fact/Evidence
134,111-ARR,111-ARR_v2_141@3,,"As shown in prior work (Krishna et al., 2020), high lexical diversity of paraphrases is helpful for changing the input style.",,"Add,Fact/Evidence",Fact/Evidence
135,111-ARR,,111-ARR_v1_70@0,,We evaluate models on (1) formality transfer;,"Delete,Fact/Evidence",Fact/Evidence
136,111-ARR,,111-ARR_v1_71@0,,(2) increasing the amount of code-mixing with English.,"Delete,Fact/Evidence",Fact/Evidence
137,111-ARR,,111-ARR_v1_78@0,,"Multilingual style transfer is mostly unexplored in prior work: a 35 paper survey by Briakou et al. (2021b) found only one work in Chinese, Russian, Latvian, Estonian, French (Shang et al., 2019;Tikhonov and Yamshchikov, 2018;Korotkova et al., 2019;Niu et al., 2018).","Delete,Fact/Evidence",Fact/Evidence
138,111-ARR,,111-ARR_v1_78@1,,"Briakou et al. (2021b) further introduced XFORMAL, the first formality transfer evaluation dataset in French, Brazilian Portugese and Italian.","Delete,Fact/Evidence",Fact/Evidence
139,111-ARR,,111-ARR_v1_78@2,,"16 Hindi formality has been studied in linguistics, focusing on politeness (Kachru, 2006;Agnihotri, 2013;Kumar, 2014) and codemixing (Bali et al., 2014).","Delete,Fact/Evidence",Fact/Evidence
140,111-ARR,,111-ARR_v1_78@3,,"Due to its prevalence in India, English-Hindi code-mixing has seen work in language modeling (Pratapa et al., 2018;Samanta et al., 2019) and core NLP tasks (Khanuja et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
141,111-ARR,,111-ARR_v1_78@4,,"To the best of our knowledge, we are the first to study style transfer for Indic languages.","Delete,Claim",Claim
142,111-ARR,,111-ARR_v1_91@0,,Results: Our results on Hindi are presented in Table 6 and other languages in Table 7.,"Delete,Fact/Evidence",Fact/Evidence
143,111-ARR,,111-ARR_v1_130@0,,"In the baseline Hindi UR model, we notice high COPY rates (45.4%), resulting in lower ACC scores.","Delete,Fact/Evidence",Fact/Evidence
144,111-ARR,,111-ARR_v1_131@3,,"Also see Figure 9 for a comparison across λ values, and Section 5 for detailed metric descriptions.","Delete,Fact/Evidence",Fact/Evidence
145,111-ARR,,111-ARR_v1_131@13,,"The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5.","Delete,Fact/Evidence",Fact/Evidence
146,111-ARR,,111-ARR_v1_131@14,,"We see the DIFFUR models outperform other systems across the λ range, and get best performance with the DIFFUR-MLT variant.","Delete,Fact/Evidence",Fact/Evidence
147,111-ARR,,111-ARR_v1_131@15,,"We also see that DIFFUR models, especially with DIFFUR-MLT, lead to better style transfer control (bottom plot, closer to x = 1 is better), giving large style variation with λ without loss in semantics (X-axis).","Delete,Fact/Evidence",Fact/Evidence
148,111-ARR,,111-ARR_v1_131@17,,"The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5.","Delete,Fact/Evidence",Fact/Evidence
149,111-ARR,,111-ARR_v1_131@18,,Note that Gujarati is a zero-shot language for DIFFUR models -no Gujarati paraphrase data was seen during training.,"Delete,Fact/Evidence",Fact/Evidence
150,111-ARR,,111-ARR_v1_131@19,,"We see that while the vanilla DIFFUR model performs poorly, the DIFFUR-INDIC is competitive with baselines and the DIFFUR-MLT variant significantly outperforms other systems.","Delete,Fact/Evidence",Fact/Evidence
151,111-ARR,,111-ARR_v1_131@20,,"We also see that the DIFFUR-MLT variant lead to better style transfer control (bottom plot, closer to x = 1 is better), giving style variation with λ without loss in semantics (X-axis).","Delete,Fact/Evidence",Fact/Evidence
152,111-ARR,111-ARR_v2_43@4,,In Appendix K we confirm that our backtranslated paraphrases are lexically diverse from the input.,,"Add,Fact/Evidence",Fact/Evidence
153,111-ARR,111-ARR_v2_49@0,,"3. The length of s diff acts as a proxy for the amount of style transfer, which is controlled using λ during inference (Section 3).",,"Add,Fact/Evidence",Fact/Evidence
154,111-ARR,,111-ARR_v1_19@0,,"Denoising: To learn a style extractor, the Universal Rewriter uses the idea that two non-overlapping spans of text in the same document are likely to have the same style.","Delete,Fact/Evidence",Fact/Evidence
155,111-ARR,111-ARR_v2_54@3,,"We initialize the model with the UR-INDIC checkpoint, and fine-tune it on these two losses together, giving each loss equal weight.",,"Add,Fact/Evidence",Fact/Evidence
156,111-ARR,111-ARR_v2_69@1,,"Since each of our individual metrics can only take values 0 or 1 at an instance level, our aggregation acts like a Boolean AND operation.",,"Add,Fact/Evidence",Fact/Evidence
157,111-ARR,111-ARR_v2_77@1,,"DIFFUR-MLT gives best overall performance (r-AGG / a-AGG), with a good trade-off between style accuracy (ACC), semantic similarity (SIM), langID score (LANG), and low input copy rates (COPY); metrics defined in Section 5, other language results in Appendix I.",,"Add,Fact/Evidence",Fact/Evidence
158,111-ARR,111-ARR_v2_79@0,,Experimental Setup,,"Add,Other",Other
159,111-ARR,111-ARR_v2_83@6,,"For code-mixing addition, we use Hindi/English code-mixed exemplars in Devanagari (shown in Appendix D).",,"Add,Fact/Evidence",Fact/Evidence
160,111-ARR,111-ARR_v2_84@0,,Main Results,,"Add,Other",Other
161,111-ARR,111-ARR_v2_85@0,,"Each proposed method improves over prior work, DIFFUR-MLT works best.",,"Add,Fact/Evidence",Fact/Evidence
162,111-ARR,111-ARR_v2_85@1,,"We present our On Gujarati, the DIFFUR-INDIC fails to get good performance (36.0 r-AGG) since it did not see Gujarati paraphrase data, but this performance is recovered using DIFFUR-MLT (75.0).",,"Add,Fact/Evidence",Fact/Evidence
163,111-ARR,111-ARR_v2_85@2,,In Table 4 we see human evaluations support our automatic evaluation for formality transfer.,,"Add,Fact/Evidence",Fact/Evidence
164,111-ARR,111-ARR_v2_85@3,,In Figure 4: Outputs and qualitative analysis of our best performing model for several attribute transfer tasks (λ is transfer magnitude).,,"Add,Fact/Evidence",Fact/Evidence
165,111-ARR,111-ARR_v2_85@4,,We notice lower quality qualitatively for ** marked styles; see Appendix J for more outputs.,,"Add,Fact/Evidence",Fact/Evidence
166,111-ARR,111-ARR_v2_86@0,,ACC scores.,,"Add,Other",Other
167,111-ARR,111-ARR_v2_87@1,,In Appendix I we show a breakdown by individual metrics for other languages and plot variations with λ.,,"Add,Fact/Evidence",Fact/Evidence
168,111-ARR,111-ARR_v2_31@6,111-ARR_v1_26@6,"Moreover, token-level noise does not differentiate between content or function words, and cannot do syntactic changes like content reordering (Goyal and Durrett, 2020).","Moreover, token-level noise does not differentiate between content / function words, and cannot do syntactic changes like content reordering (Goyal and Durrett, 2020).","Modify,Grammar",Grammar
169,111-ARR,111-ARR_v2_3@2,111-ARR_v1_3@2,"Moreover, our method is better at controlling the style transfer magnitude using an input scalar knob.","Moreover, our method is better able to control the amount of style transfer using an input scalar knob.","Modify,Clarity",Clarity
170,111-ARR,111-ARR_v2_31@11,111-ARR_v1_27@3,"This has also been observed recently in Kreutzer et al. (2022), who audit 100 sentences in each language, and report 50% sentences in Marathi and 20% sentences in Hindi have the wrong language.","This has also been observed recently in Caswell et al. (2021), who audit 100 sentences in each language, and report 50% sentences in Marathi and 20% sentences in Hindi have the wrong language.","Modify,Fact/Evidence",Fact/Evidence
171,111-ARR,111-ARR_v2_3@3,111-ARR_v1_3@3,"We report promising qualitative results for several attribute transfer tasks (sentiment transfer, simplification, gender neutralization, text anonymization) all without retraining the model.","We report promising qualitative results for several attribute transfer directions, including sentiment transfer, text simplification, gender neutralization and text anonymization, all without retraining the model.","Modify,Clarity",Clarity
172,111-ARR,111-ARR_v2_3@4,111-ARR_v1_3@4,"Finally, we find model evaluation to be difficult due to the lack of datasets and metrics for many languages.",Finally we found model evaluation to be difficult due to the lack of evaluation datasets and metrics for many languages.,"Modify,Clarity",Clarity
173,111-ARR,111-ARR_v2_42@0,111-ARR_v1_38@0,"Paraphrases as a ""noise"" function: Instead of using random token-level noise (Issue #1 in Section 3.1), we paraphrase sentences to ""noise"" them during training.","Paraphrases as a ""noise"" function: Instead of using random token-level noise (issue #1 in Section 3.1), we paraphrase sentences to ""noise"" them during training.","Modify,Grammar",Grammar
174,111-ARR,111-ARR_v2_44@0,111-ARR_v1_40@0,"Using style vector differences for control: To fix the training / inference mismatch for style extraction (Issue #2 in Section 3.1), we propose using style vector differences between the output and input as the stylistic control.","Using style vector differences for control: To fix the training / inference mismatch for style extraction (issue #2 in Section 3.1), we propose using style vector differences between the output and input as the stylistic control.","Modify,Grammar",Grammar
175,111-ARR,111-ARR_v2_3@5,111-ARR_v1_3@5,"To facilitate future research we crowdsource formality annotations for 4000 sentence pairs in four Indic languages, and use this data to design our automatic evaluations.","To facilitate further research in formality transfer for Indic languages, we crowdsource annotations for 4000 sentence pairs in four languages, and use this dataset 1 to design our automatic evaluation suite.","Modify,Fact/Evidence",Fact/Evidence
176,111-ARR,111-ARR_v2_2@0,111-ARR_v1_2@0,Style transfer is the task of rewriting a sentence into a target style while approximately preserving content.,Style transfer is the task of rewriting an input sentence into a target style while approximately preserving its content.,"Modify,Clarity",Clarity
177,111-ARR,111-ARR_v2_52@0,111-ARR_v1_46@0,"To address the issue of no translation data (Issue #4 in Section 3.1), we train Indic variants of our models.","To address the issue of no translation data (issue #4 in Section 3.1), we train Indic variants of our models.","Modify,Grammar",Grammar
178,111-ARR,111-ARR_v2_54@0,111-ARR_v1_48@0,One issue with our DIFFUR-INDIC setup is usage of a stop-grad(•) to avoid verbatim copying from the input.,"One issue with our DIFFUR-INDIC setup is usage of a stop-grad(•), to avoid verbatim copying from the input.","Modify,Grammar",Grammar
179,111-ARR,111-ARR_v2_54@2,111-ARR_v1_48@2,To prevent this we simply multi-task between the exemplardriven denoising UR objective (Section 3) and the DIFFUR objective.,"To prevent this from happening, we simply do multi-task learning between the original Universal Rewriter objective (Section 3) and our DIFFUR-INDIC objective, using an equal number of minibatches for each objective.","Modify,Fact/Evidence",Fact/Evidence
180,111-ARR,111-ARR_v2_0@0,111-ARR_v1_0@0,Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings,Few-shot Controllable Style Transfer for Low-Resource Multilinugal Settings,"Modify,Grammar",Grammar
181,111-ARR,111-ARR_v2_70@0,111-ARR_v1_62@1,"In other words, we are measuring the fraction of outputs which simultaneously transfer style, have a semantic similarity of at least L (our threshold in Section 5.3), and have the same language as the input.","In other words, we measure the fraction of outputs which simultaneously transfer style, have a semantic similarity of at least L (our threshold in Section 5.3), and have the same language as the input.","Modify,Grammar",Grammar
183,111-ARR,111-ARR_v2_76@4,111-ARR_v1_68@3,"More experiment details (inter-annotator agreement, compensation, instructions) are provided in Appendix E.4.","This lets us evaluate r-ACC, SIM, r-AGG, CALIB with respect to human annotations instead of classifier predictions; details in Appendix E.4.","Split+Modify,Fact/Evidence",Fact/Evidence
185,111-ARR,111-ARR_v2_88@2,111-ARR_v1_72@1,More outputs are provided in Appendix J.,"Besides formality transfer and code-mixing addition, we transfer several other attributes: sentiment (Li et al., 2018), simplicity (Xu et al., 2015), anonymity (Anandan et al., 2012) and gender neutrality (Reddy and Knight, 2016); more outputs in Appendix J.","Split+Modify,Clarity",Clarity
186,111-ARR,111-ARR_v2_90@1,111-ARR_v1_73@1,"Our methods outperform prior work in formality transfer & code-mixing for 7 languages, with promising qualitative results for several other attribute transfer tasks.","Our methods outperform prior work in formality transfer & codemixing for 7 languages, with promising qualitative results.","Modify,Claim",Claim
187,111-ARR,111-ARR_v2_90@2,111-ARR_v1_73@2,"Future work includes further improving systems for some attributes, and studying style transfer for languages where little / no translation data is available.","Future work includes further improving systems for some attributes, and considering languages where little / no translation data is available.","Modify,Clarity",Clarity
188,111-ARR,111-ARR_v2_92@3,111-ARR_v1_75@3,"The Google 2020 environment report mentions, 15 ""TPUs are highly efficient chips which have been specifically designed for machine learning applications"".","The Google 2020 environment report mentions, 13 ""TPUs are highly efficient chips which have been specifically designed for machine learning applications"".","Modify,Fact/Evidence",Fact/Evidence
189,111-ARR,111-ARR_v2_80@0,111-ARR_v1_75@5,"In our experiments, we compare the following models (training details are provided Appendix A):",We compare the following models:,"Modify,Fact/Evidence",Fact/Evidence
190,111-ARR,111-ARR_v2_2@1,111-ARR_v1_2@1,"While most prior literature assumes access to a large style-labelled corpus, recent work (Riley et al., 2021) has attempted ""few-shot"" style transfer using just 3-10 sentences at inference for style extraction.","While most prior literature assumes access to large stylelabelled corpora, recent work (Riley et al., 2021) has attempted ""few-shot"" style transfer using only 3-10 sentences at inference for extracting the target style.","Modify,Clarity",Clarity
191,111-ARR,111-ARR_v2_16@0,111-ARR_v1_10@1,"Most prior work either assumes access to supervised data with parallel sentences between the two styles (Jhamtani et al., 2017), or access to a large corpus of unpaired sentences with style labels (Prabhumoye et al., 2018;Subramanian et al., 2019).","Most prior work either assumes access to supervised data with parallel sentences between the two styles (Jhamtani et al., 2017), or access to large corpus of unpaired sentences with style labels (Prabhumoye et al., 2018;Subramanian et al., 2019).","Modify,Grammar",Grammar
192,111-ARR,111-ARR_v2_16@5,111-ARR_v1_10@6,"In this work, we take the first steps studying style transfer in seven languages 2 with nearly 1.5 billion speakers in total.","In this work, we take the first steps studying style transfer in seven languages 2 with nearly 1.5 billion speakers.","Modify,Clarity",Clarity
193,111-ARR,111-ARR_v2_16@7,111-ARR_v1_10@8,"Unfortunately, we find it often copies the inputs verbatim (Section 3.1), without changing their style.","Unfortunately, we found it often copied input sentences verbatim (Section 3.1) without transferring their style.","Modify,Clarity",Clarity
194,111-ARR,111-ARR_v2_17@1,111-ARR_v1_11@1,"To further boost performance we propose DIFFUR, 3 a novel algorithm using the recent finding that paraphrasing leads to stylistic changes (Krishna et al., 2020).","To further boost performance we propose DIFFUR, 3 an algorithm using the recent finding that paraphrasing leads to stylistic changes (Krishna et al., 2020).","Modify,Clarity",Clarity
195,111-ARR,111-ARR_v2_136@1,111-ARR_v1_125@1,"Consistent with Krishna et al. (2020), we find that top-p decoding usually gets higher style accuracy (r-ACC, a-ACC) and output diversity (1-g, COPY) scores, but lower semantic similarity (SIM) scores.","Consistent with Krishna et al. (2020), we find that top-p decoding usually gets higher style accuracy (r-ACC, a-ACC) and output diversity (1-g, COPY) scores, but lower similarity (SIM) scores.","Modify,Clarity",Clarity
196,111-ARR,111-ARR_v2_86@2,111-ARR_v1_130@2,"We find the lowest COPY (and lowest 1-g) for models with +BT (1%), which is due to two translation steps.","We find the lowest COPY (and lowest 1-g) for models with +BT (1%), which is due to two steps of translation.","Modify,Clarity",Clarity
197,111-ARR,111-ARR_v2_86@3,111-ARR_v1_130@3,"However, this lowers semantic similarity (also seen in Table 4) lowering the overall score (60.0 vs 78.1) compared to DIFFUR-MLT.","However, this lowers semantic similarity (also seen in Table 3) lowering the overall score compared to DIFFUR-MLT (60.0 vs 78.1 r-AGG).","Modify,Fact/Evidence",Fact/Evidence
198,111-ARR,111-ARR_v2_2@2,111-ARR_v1_2@2,"In this work, we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available.",In this work we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available.,"Modify,Grammar",Grammar
199,111-ARR,111-ARR_v2_2@3,111-ARR_v1_2@3,"We notice that existing few-shot methods perform this task poorly, often copying inputs verbatim.","We find that existing fewshot methods perform this task poorly, with a strong tendency to copy inputs verbatim.","Modify,Clarity",Clarity
200,111-ARR,111-ARR_v2_22@1,111-ARR_v1_16@1,"At a high level, the UR model extracts a style vector s from an exemplar sentence e, which reflects the desired target style.","The UR model extracts a style vector s from an exemplar sentence e, which reflects the desired target style.","Modify,Clarity",Clarity
201,111-ARR,111-ARR_v2_24@2,111-ARR_v1_19@1,"Concretely, let x 1 and x 2 be two non-overlapping spans.","Concretely, let x 1 and x 2 be two non-overlapping spans in mC4.","Modify,Fact/Evidence",Fact/Evidence
202,111-ARR,111-ARR_v2_3@1,111-ARR_v1_3@1,"When compared to prior work, our model achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages.","When compared to prior work using automatic and human evaluations, our model achieves 2-3x better performance and output diversity in formality transfer and code-mixing addition across seven languages.","Modify,Claim",Claim
203,114-ARR,,114-ARR_v1_46@2,,"Given that trigger-dependent types often have indicative triggers, we build a mechanism called word saliency embeddings (WSEs) in the model for T trigger to capture such regularities.","Delete,Fact/Evidence",Fact/Evidence
204,114-ARR,,114-ARR_v1_46@3,,"Specifically, we first quantify each word's saliency value 3 as 0 or 1 based on λ, i.e., the threshold we used previously for distinguishing event types, and then use a separate embedding vector to distinguish 0 and 1, similar to word embeddings.","Delete,Fact/Evidence",Fact/Evidence
205,114-ARR,,114-ARR_v1_46@4,,Such embeddings are incorporated into the model 4 to capture a regularity that words with high saliency values are more likely to be triggers.,"Delete,Fact/Evidence",Fact/Evidence
206,114-ARR,,114-ARR_v1_46@5,,"Note WSEs are also incorporated in the model for the T context , which on the other hand seeks to learn the opposite regularity that words with high saliency values may not be triggers.","Delete,Fact/Evidence",Fact/Evidence
207,114-ARR,,114-ARR_v1_54@1,,"We use Adam (Kingma and Ba, 2015) with default hyper-parameters for parameter update.","Delete,Fact/Evidence",Fact/Evidence
208,114-ARR,,114-ARR_v1_55@0,,Experimental Setups,"Delete,Other",Other
209,114-ARR,,114-ARR_v1_56@0,,Datasets.,"Delete,Other",Other
210,114-ARR,,114-ARR_v1_56@1,,"We conduct experiments on ACE 2005 (LDC, 2005) and MAVEN documents.","Delete,Fact/Evidence",Fact/Evidence
211,114-ARR,,114-ARR_v1_56@2,,"We adopt a common split for evaluation following previous works (Li et al., 2013;Wadden et al., 2019).","Delete,Fact/Evidence",Fact/Evidence
212,114-ARR,,114-ARR_v1_56@3,,MAVEN is a newly released corpus defining 168 more fine-grained event types .,"Delete,Fact/Evidence",Fact/Evidence
213,114-ARR,,114-ARR_v1_56@4,,"Because the MAVEN test set is not publicly available and our study is concerned with per-type performance, we instead use the MAVEN development set for assessment and divide the original MAVEN training set as 9:1 for training and validating.","Delete,Fact/Evidence",Fact/Evidence
214,114-ARR,,114-ARR_v1_56@5,,Table 1 displays the comprehensive data statistics for the two datasets.,"Delete,Fact/Evidence",Fact/Evidence
215,114-ARR,,114-ARR_v1_57@0,,Evaluation Metrics.,"Delete,Other",Other
216,114-ARR,,114-ARR_v1_67@5,,"(SL), which only differentiates event types for training, outperforms BERTEns by 1.6% in F1.","Delete,Fact/Evidence",Fact/Evidence
217,114-ARR,114-ARR_v2_67@2,,"For example, we may further subdivide a CD type TRANSFER_MONEY into finergrained ones like LOAN and PURCHASE.",,"Add,Claim",Claim
218,114-ARR,114-ARR_v2_67@3,,"We provide linguistic/lexical insights by comparing the hierarchy levels of TD/CD types on WordNet (Miller, 1992).",,"Add,Fact/Evidence",Fact/Evidence
219,114-ARR,114-ARR_v2_4@0,114-ARR_v1_4@0,"Event detection (ED) is the first and a crucial step of event extraction, which aims to identify events of certain types in plain texts (Ahn, 2006;Nguyen and Grishman, 2015;Mitamura et al., 2017).","Event detection (ED), the first and a crucial step of event extraction, aims to identify events of certain types in texts (Ahn, 2006;Nguyen and Grishman, 2015;Mitamura et al., 2017).","Modify,Clarity",Clarity
220,114-ARR,114-ARR_v2_4@2,114-ARR_v1_4@2,"Tasking the ACE benchmark as an example, we note the state-of-the-art ED model (Wadden et al., 2019) can strike 90% in F1 for the type DIVORCE, yet only 50% for the type START-POSITION, and it is more surprising that the training set of DIVORCE is eight times smaller than that of START-POSITION.","Tasking the ACE benchmark as an example, we note the state-of-the-art ED model (Wadden et al., 2019) can strike 90% in F1 for the type DIVORCE, yet only 50% for the type START-POSITION; it is more surprising that the training set of DIVORCE is 8 times smaller than that of START-POSITION.","Modify,Clarity",Clarity
221,114-ARR,114-ARR_v2_52@3,114-ARR_v1_60@3,"After 5 epochs, it achieves 74.8% in F1 on the ACE 2005 development set, matching the state-of-the-art performance (Liu et al., 2019c).","After 5 epochs, it achieves 74.8% in F1 on the ACE 2005 development set, matching the state-of-the-art performance .","Modify,Fact/Evidence",Fact/Evidence
222,114-ARR,114-ARR_v2_52@7,114-ARR_v1_60@8,"To allow for further investigation, we have made our code publicly available at https://github.com/ jianliu-ml/SaliencyED.","To allow for further investigation, we have made our code publicly available at http://anomynous.","Modify,Fact/Evidence",Fact/Evidence
223,114-ARR,114-ARR_v2_5@0,114-ARR_v1_5@0,In this study we take a fresh look at above problem and for the first time attribute the skewed performance to the contextual patterns of events.,This study takes a fresh look at the problem by attributing the skewed performance to the contextual patterns of events.,"Modify,Clarity",Clarity
224,114-ARR,114-ARR_v2_5@2,114-ARR_v1_5@2,"Intuitively, they demonstrate distinct patterns: the DI-VORCE event is more trigger-dependent, and the trigger word (i.e., ""divorced"") is very indicative of the event's occurrence; by contrast, the START-POSITION event is more context-dependent -the event semantic is primarily expressed by contexts rather than the trigger ""become"", which is a merely light verb.","Intuitively, they have distinct patterns: the DI-VORCE event is more trigger-dependent, because the trigger word (divorced) is very indicative of the event's occurrence; by contrast, the START-POSITION event is more context-dependent -the event semantic is primarily expressed by contexts rather than the trigger (become), which is a merely light verb.","Modify,Clarity",Clarity
225,114-ARR,114-ARR_v2_6@0,114-ARR_v1_6@0,"To address the first question, we introduce a brandy new concept called trigger saliency attribution, which can explicitly quantify an event's contextual pattern.",We introduce a brandy new concept called trigger saliency attribution that can explicitly quantify an event's contextual pattern.,"Modify,Clarity",Clarity
226,114-ARR,114-ARR_v2_6@1,114-ARR_v1_6@1,"Figure 2 illustrates the key idea: to determine how much an event is trigger-dependent or context-dependent, we measure the trigger's contribution to expressing overall the event semantic.","As shown in Figure 2, to determine how much an event depends on triggers/contexts, the key notion is to measure the trigger's contribution to expressing overall the event semantic.","Modify,Clarity",Clarity
227,114-ARR,114-ARR_v2_6@2,114-ARR_v1_6@2,"Specifically, we first assign each sentence a global event label that represents the overall event semantic.","To this end, we first assign each sentence a global event label that represents the overall event semantic.","Modify,Clarity",Clarity
228,114-ARR,114-ARR_v2_6@3,114-ARR_v1_6@3,"Then, inspired by the feature attribution method (Simonyan et al., 2014;Sundararajan et al., 2017), we regard each word as a feature and compute its contribution (i.e., saliency value) for predicting the global event label.","Then, inspired by the feature attribution method (Simonyan et al., 2014;Sundararajan et al., 2017), we regard each word as a feature and compute its saliency value (i.e., contribution) for predicting the global event label.","Modify,Clarity",Clarity
229,114-ARR,114-ARR_v2_6@4,114-ARR_v1_6@4,"Finally, by examining the ground-truth trigger's saliency value, we can tell how much an event depends on triggers or contexts: a higher value, for example, indicates that the trigger contributes more to the event, implying the event is more trigger-dependent.","Finally, by examining the ground-truth trigger's saliency value, we can determine how much an event depends on triggers or contexts: a higher value, for example, indicates that the trigger contributes more to the event, implying the event is more trigger-dependent.","Modify,Clarity",Clarity
230,114-ARR,114-ARR_v2_7@0,114-ARR_v1_7@0,"To answer the second question, we develop a new training mechanism based on trigger saliency attribution, which uses saliency as evidence to enhance learning.","We also develop a new training mechanism based on trigger saliency attribution, which uses saliency as evidence to enhance learning.","Modify,Clarity",Clarity
231,114-ARR,114-ARR_v2_7@1,114-ARR_v1_7@1,"Our method is simple and straightforward -instead of using a single model to detect all event types, we group event types with similar patterns together (assessed by trigger saliency attribution) and develop separate models for each group.","Our method is simple yet effective -instead of using a single model to detect all event types, we group event types with similar patterns together (assessed by trigger saliency attribution) and develop separate models for each group.","Modify,Clarity",Clarity
233,114-ARR,114-ARR_v2_7@2,114-ARR_v1_7@3,"This strategy enables different models to capture distinct patterns -for example, the model for context-dependent type can focus on mining contextual information for learning.","The model for context-dependent types, for example, can focus on mining contextual information for learning.","Merge+Modify,Clarity",Clarity
234,114-ARR,114-ARR_v2_7@3,114-ARR_v1_7@4,"To further boost learning, we also propose two saliency-exploration strategy to augment the above framework, which can explicitly integrate saliency information into learning and produce improved performance particularly for context-dependent types ( § 6.2).","Furthermore, we augment the above framework with two saliency-exploration strategy, which can explicitly integrate saliency information into learning and produce improved performance particularly for context-dependent types ( § 6.2).","Modify,Clarity",Clarity
235,114-ARR,114-ARR_v2_8@0,114-ARR_v1_8@0,"To verify the effectiveness of our approach, we have conducted extensive experiments on two ED benchmarks (i.e., ACE 2005(LDC, 2005 and MAVEN ).","We have conducted extensive experiments on two ED benchmarks (i.e., ACE 2005(LDC, 2005 and MAVEN ) to verify the effectiveness of our approach.","Modify,Clarity",Clarity
236,114-ARR,114-ARR_v2_8@1,114-ARR_v1_8@1,"According to the results: (i) Our trigger saliency attribution method can capture the underlying pattern and well explain the skewed performance, obtaining Spearman's correlation coefficients of 0.72 and 0.61 with per-type F1 on ACE 2005 and MAVEN respectively; (ii) Our new training regime based on saliency demonstrates improved results on the two benchmarks.","From the results: (i) Our trigger saliency attribution method does capture the underlying pattern and can well explain the skewed performance, obtaining Spearman's correlation coefficients of 0.72 and 0.61 with per-type F1 on ACE 2005 and MAVEN respectively; (ii) Our new training regime based on saliency demonstrates improved results on the two benchmarks.","Modify,Clarity",Clarity
237,114-ARR,114-ARR_v2_8@3,114-ARR_v1_8@3,"Finally, in ablation studies, we compare and highlight many significant characteristics (e.g., linguistic and lexical patterns) of triggerdependent and context-dependent event types; our work may inspire future research into their patterns.","Finally, we compare and emphasize several significant aspects (e.g., linguistic and lexical patterns) of trigger-dependent and contextdependent event types, and our work may inspire future research into their differences.","Modify,Clarity",Clarity
238,114-ARR,114-ARR_v2_20@0,114-ARR_v1_19@0,"• We highlight several diverse patterns of trigger-dependent and context-dependent event types, and our findings may stimulate future research into their differences.","• We highlight many distinct patterns of triggerdependent and context-dependent event types, and our findings suggest that the traditional ""one model fits all types"" paradigm may need to be revised.","Modify,Claim",Claim
239,114-ARR,114-ARR_v2_27@0,114-ARR_v1_27@0,"FA have been used to interpret model predictions in applications including image classification (Simonyan et al., 2014), machine translation (Ding et al., 2017), text classification , and others (Bastings and Filippova, 2020).","FA have been used to interpret model predictions in applications including image classification (Simonyan et al., 2014), machine translation (Ding et al., 2017), text classification (Chen et al., 2018), and others (Bastings and Filippova, 2020).","Modify,Fact/Evidence",Fact/Evidence
240,115-ARR,,115-ARR_v1_65@0,,Dataset,"Delete,Other",Other
241,115-ARR,,115-ARR_v1_66@0,,We conduct experiments on a publicly available multi-modal sarcasm detection benchmark dataset collected by Cai et al. (2019).,"Delete,Fact/Evidence",Fact/Evidence
242,115-ARR,,115-ARR_v1_66@1,,This dataset contains English tweets expressing sarcasm as Positive examples and those expressing non-sarcasm as Negative examples.,"Delete,Fact/Evidence",Fact/Evidence
243,115-ARR,,115-ARR_v1_66@2,,Each example in the dataset consists of a text and an associated image.,"Delete,Fact/Evidence",Fact/Evidence
244,115-ARR,,115-ARR_v1_66@3,,The statistics of the dataset are shown in Table 1.,"Delete,Fact/Evidence",Fact/Evidence
245,115-ARR,115-ARR_v2_75@4,,"2) We conduct significance tests of our CMGCN over the baseline models, the results show that our CMGCN significantly outperforms the baseline models in terms of most of the evaluation metrics (with p−value < 0.05).",,"Add,Fact/Evidence",Fact/Evidence
246,115-ARR,115-ARR_v2_88@0,,"As described in Section 3.3, the weights of edges in the cross-modal graph are computed based on both word similarities and affective clues between textual words and the attribute-object pairs of the image regions, and the dependency tree of the textmodality.",,"Add,Fact/Evidence",Fact/Evidence
247,115-ARR,115-ARR_v2_88@1,,The approach can be easily generalized to other sentiment-related multi-modal learning scenarios.,,"Add,Claim",Claim
248,115-ARR,115-ARR_v2_88@2,,"Nevertheless, the cross-graph solution might not be generalized well to other multi-modal tasks or data genres, if there is a lack of affective knowledge or a difficulty in deriving dependency trees in low-resource settings.",,"Add,Claim",Claim
249,115-ARR,115-ARR_v2_88@3,,"Therefore, future research can consider exploiting alternatively approaches to automatically learn the weights of edges in the cross-modal graph without relying on external knowledge sources.",,"Add,Claim",Claim
250,115-ARR,,115-ARR_v1_14@1,,"Different from text-based sarcasm detection, multimodal sarcasm detection aims to identify the sarcastic expression among different modalities (Schifanella et al., 2016;Castro et al., 2019).","Delete,Fact/Evidence",Fact/Evidence
251,115-ARR,,115-ARR_v1_14@2,,Schifanella et al. (2016) firstly tackled the multi-modal sarcasm detection task with text and image modalities by manually designed features.,"Delete,Fact/Evidence",Fact/Evidence
252,115-ARR,,115-ARR_v1_14@3,,Cai et al. (2019) created a new dataset and proposed a hierarchical fusion model for multi-modal sarcasm detection.,"Delete,Fact/Evidence",Fact/Evidence
253,115-ARR,,115-ARR_v1_14@5,,Pan et al. (2020) proposed inter-modality attention and coattention to learn the contradiction of sarcasm.,"Delete,Fact/Evidence",Fact/Evidence
254,115-ARR,115-ARR_v2_68@0,115-ARR_v1_71@0,"2) Text-modality methods: These models use only textual information, including TextCNN (Kim, 2014), a deep learning model based on CNN for text classification; Bi-LSTM, a bidirectional LSTM network for text classification; SIARN (Tay et al., 2018), adopting inner-attention for textual sarcasm detection; SMSD (Xiong et al., 2019), exploring a self-matching network to capture textual incongruity information; and BERT (Devlin et al., 2019), the vanilla pre-trained uncased BERT-base taking '[CLS] text [SEP]' as input.","2) Text-modality methods These models use only textual information, including TextCNN (Kim, 2014), a deep learning model based on CNN for text classification; Bi-LSTM, a bidirectional LSTM network for text classification; SIARN (Tay et al., 2018), adopting inner-attention for textual sarcasm detection; SMSD (Xiong et al., 2019), exploring a self-matching network to capture textual incongruity information; and BERT (Devlin et al., 2019), the vanilla pre-trained uncased BERT-base taking '[CLS] text [SEP]' as input.","Modify,Grammar",Grammar
255,115-ARR,115-ARR_v2_69@0,115-ARR_v1_72@0,3) Multi-modal methods: These models take both text-and image-modality information.,3) Multi-modal methods These models take both text-and image-modality information.,"Modify,Grammar",Grammar
256,115-ARR,115-ARR_v2_69@1,115-ARR_v1_72@1,"Including HFM (Cai et al., 2019), a hierarchical multimodal features fusion model for multi-modal sarcasm detection; D&R Net (Xu et al., 2020), a Decomposition and Relation Network modeling both crossmodality contrast and semantic association; Res-BERT (Pan et al., 2020), concatenating image features and BERT-based text features for sarcasm prediction; Att- BERT (Pan et al., 2020), exploring an inter-modality attention and a co-attention to model the incongruity of multi-modal sarcasm detection; and InCrossMGs (Liang et al., 2021a), a graph-based model to leverage the sarcastic relations from both intra-and inter-modal perspectives.","Including HFM (Cai et al., 2019), a hierarchical multimodal features fusion model for multi-modal sarcasm detection; D&R Net , a Decomposition and Relation Network modeling both crossmodality contrast and semantic association; Res-BERT (Pan et al., 2020), concatenating image features and BERT-based text features for sarcasm prediction; Att- BERT (Pan et al., 2020), exploring an inter-modality attention and a co-attention to model the incongruity of multi-modal sarcasm detection; and InCrossMGs , a graph-based model to leverage the sarcastic relations from both intra-and inter-modal perspectives.","Modify,Fact/Evidence",Fact/Evidence
257,115-ARR,115-ARR_v2_5@4,115-ARR_v1_5@4,"This post however contains a sarcastic expression with negative sentiment, because it is accompanied by an image with ""thunderstorm clouds"".","This post however contains a sarcastic expression with negative sentiment, because it is accompanied by an image with thunderstorm clouds.","Modify,Grammar",Grammar
258,115-ARR,115-ARR_v2_86@0,115-ARR_v1_90@0,Conclusion and Future Work,Conclusion,"Modify,Other",Other
259,115-ARR,115-ARR_v2_87@1,115-ARR_v1_91@1,"Specifically, unlike previous research efforts that simply consider the visual information of the whole image, we attempt to recognize the important visual regions via object detection results, and further devise a novel cross-modal graph to explicitly establish the connections of scattered visual regions and the associated textual tokens.","Specifically, unlike previous research efforts that simply consider the visual information of the whole image, we attempt to recognize the important visual regions via object detection results.","Modify,Fact/Evidence",Fact/Evidence
260,115-ARR,115-ARR_v2_87@4,115-ARR_v1_91@4,"To the best of our knowledge, it is the first study of utilizing a cross-modal graph to extract intricate multi-modal sarcastic relations via object detection and sentiment cues from external knowledge bases.","To our knowledge, it is the first study of utilizing a cross-modal graph to extract intricate multi-modal sarcastic relations via object detection and sentiment cues from external knowledge bases.","Modify,Clarity",Clarity
261,115-ARR,115-ARR_v2_6@0,115-ARR_v1_6@0,"To perform multi-modal sarcasm detection on data composed of text and image, several related research efforts attempt to concatenate the textual and visual features to fuse sarcastic information (Schifanella et al., 2016), employ attention mechanism to implicitly fuse the features of different modalities based on external knowledge (Cai et al., 2019;Xu et al., 2020;Pan et al., 2020), or build interactive graphs to model the relations of different modalities (Liang et al., 2021a).","To perform multi-modal sarcasm detection on data composed of text and image, several related research efforts attempt to concatenate the textual and visual features to fuse sarcastic information (Schifanella et al., 2016), employ attention mechanism to implicitly fuse the features of different modalities based on external knowledge (Cai et al., 2019;Pan et al., 2020), or build interactive graphs to model the relations of different modalities .","Modify,Fact/Evidence",Fact/Evidence
262,115-ARR,115-ARR_v2_2@1,115-ARR_v1_2@1,"In this paper, we investigate multimodal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic relations between textual and visual modalities.","Different from existing research efforts that either simply consider the visual cues from the whole image or implicitly extract the sarcastic relations between different modalities purely via attention mechanism, in this paper, we investigate multi-modal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic relations between textual and visual modalities.","Modify,Claim",Claim
263,115-ARR,115-ARR_v2_6@5,115-ARR_v1_6@5,"As such, it is essential to focus on drawing the intricate sentiment connections between text and image modalities, allowing a good exploitation of the contradictory sentiment information between modalities for learning sarcastic clues.","As such, it is essential to focus on drawing the intricate sentiment connections between modalities, allowing a good exploitation of the contradictory sentiments between modalities for learning sarcastic clues.","Modify,Clarity",Clarity
264,115-ARR,115-ARR_v2_7@3,115-ARR_v1_7@3,"Then, we explore a novel solution to assign weights to the edges of the cross-modal graph by means of computing the word similarities between the object descriptors of the attribute-object pairs and textual words based on the WordNet (Miller, 1992).","Then, we explore a novel solution to assign weights to the edges of the cross-modal graph by means of computing the word similarities between the object descriptors of the attribute-object pairs and textual words based on the WordNet (Miller, 1995).","Modify,Fact/Evidence",Fact/Evidence
265,115-ARR,115-ARR_v2_14@1,115-ARR_v1_16@1,"Further, there are also some research studies explored graph models to deal with the multi-modal tasks, such as multi-modal sentiment detection (Yang et al., 2021), multi-modal named entity recognition , cross-modal video moment retrieval (Zeng et al., 2021), multi-modal neural machine translation (Yin et al., 2020), and multimodal sarcasm detection (Liang et al., 2021a).","Correspondingly, there are also some multi-modal studies, such as multi-modal sentiment detection , multi-modal named entity recognition , and multi-modal sarcasm detection .","Modify,Fact/Evidence",Fact/Evidence
266,115-ARR,115-ARR_v2_17@0,115-ARR_v1_19@0,"As demonstrated in Figure 2, the architecture of the proposed CMGCN contains four main components: 1) Text-modality representation, which employs the pre-trained uncased BERT-base model (Devlin et al., 2019) as the text encoder to capture the hidden representation of the text-modality; 2) Image-modality representation, which deploys the pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2021) as the image encoder to capture the hidden representation of the image-modality with respect to each bounding box (visual region); 3) Cross-modal graph, which constructs a crossmodal graph for each multi-modal example based on the external affective knowledge source and the hidden representations of text and image modalities; 4) Multi-modal fusion, which fuses the representations from image and text modalities to capture the sarcastic features by means of a GCN structure and an attention mechanism.","As demonstrated in Figure 2, the architecture of the proposed CMGCN contains four main components: 1) Text-modality representation, which employs the pre-trained uncased BERT-base model (Devlin et al., 2019) as the text encoder to capture the hidden representation of the text-modality; 2) Image-modality representation, which deploys the pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2021) as the image encoder to capture the hidden representation of the image-modality with respect to each bounding box (visual region); 3) Cross-modal graph, which constructs a crossmodal graph for each multi-modal example based on the hidden representations of text and image modalities; 4) Multi-modal fusion, which fuses the representations from image and text modalities to capture the sarcastic features by means of a GCN structure and an attention mechanism.","Modify,Fact/Evidence",Fact/Evidence
267,118-ARR,,118-ARR_v1_75@0,,"4. Conll03-Typos , which is generated from Conll2003 (Sang and De Meulder, 2003).","Delete,Fact/Evidence",Fact/Evidence
268,118-ARR,,118-ARR_v1_75@1,,"The entities in the test set is replaced by typos version(character modify, insert, and delete operation).","Delete,Fact/Evidence",Fact/Evidence
269,118-ARR,,118-ARR_v1_76@0,,"5. Conll03-OOV , which is generated from Conll2003 (Sang and De Meulder, 2003).","Delete,Fact/Evidence",Fact/Evidence
270,118-ARR,,118-ARR_v1_76@1,,The entities in the test set is replaced by another out-of-vocabulary entity in test set.,"Delete,Fact/Evidence",Fact/Evidence
271,118-ARR,,118-ARR_v1_77@0,,Table 2 reports the static results of the OOV problem on the test sets of each dataset.,"Delete,Fact/Evidence",Fact/Evidence
272,118-ARR,,118-ARR_v1_77@1,,"As shown in the table, the test set of these data sets comprises a substantial amount of OOV entities.","Delete,Fact/Evidence",Fact/Evidence
273,118-ARR,118-ARR_v2_8@3,,Our codes 1 are publicly available.,,"Add,Fact/Evidence",Fact/Evidence
274,118-ARR,118-ARR_v2_64@2,,The resulting Mutual Information based Named Entity Recognition model is visualized in Figure 1.,,"Add,Fact/Evidence",Fact/Evidence
275,118-ARR,118-ARR_v2_85@3,,The results are obtained by testing MINER (Bert large) on TwitterNER .,,"Add,Fact/Evidence",Fact/Evidence
276,118-ARR,118-ARR_v2_85@4,,"We fix β = 1e03, and the orange line is f1 score when γ = 0.",,"Add,Fact/Evidence",Fact/Evidence
277,118-ARR,118-ARR_v2_85@5,,The results are obtained by testing MINER (Bert large) on TwitterNER .,,"Add,Fact/Evidence",Fact/Evidence
278,118-ARR,118-ARR_v2_85@6,,"We fix γ = 1e04, and the orange line is f1 score when β = 0.",,"Add,Fact/Evidence",Fact/Evidence
279,118-ARR,118-ARR_v2_4@0,118-ARR_v1_4@0,"Named Entity Recognition (NER) aims to identify and classify entity mentions from unstructured text, e.g., extracting location mention ""Berlin"" from the sentence ""Berlin is wonderful in the winter"".","Named Entity Recognition(NER) aims to identify and classify entity mentions from unstructured text, e.g., extracting location mention ""Berlin"" from sentence ""Berlin is wonderful in the winter"".","Modify,Grammar",Grammar
280,118-ARR,118-ARR_v2_48@0,118-ARR_v1_47@0,"Motivated by IB (Tishby et al., 2000;Federici et al., 2020), we can subdivide I(X; Z) into two components by using the chain rule of mutual information(MI):","Motivated by IB (Tishby et al., 2000;Federici et al., 2020), we can subdividing I(X; Z) into two components by using the chain rule of mutual information(MI):","Modify,Grammar",Grammar
281,118-ARR,118-ARR_v2_72@0,118-ARR_v1_69@0,"In this section, we verify the performance of the proposed method on five OOV datasets, and compared it with other methods.","In this section, we verified the performance of the proposed method on five OOV datasets, and compared it with other methods.","Modify,Grammar",Grammar
282,118-ARR,118-ARR_v2_77@2,118-ARR_v1_78@2,"However, the work is neither open source nor reported on the same dataset, so this method cannot be compared with MINER.","However, the work is neither open source nor reported on the same data set, so this method is not compared with MINER.","Modify,Clarity",Clarity
283,118-ARR,118-ARR_v2_80@0,118-ARR_v1_81@0,"To verify the universality of our method, we measured its performance on various pre-trained models, i.e., Bert (Devlin et al., 2018), Roberta (Liu et al., 2019), Albert (Lan et al., 2019).","To verify the universality of our method, we measured its performance in various pre-trained models, i.e., Bert (Devlin et al., 2018), Roberta (Liu et al., 2019), Albert (Lan et al., 2019).","Modify,Grammar",Grammar
284,118-ARR,118-ARR_v2_82@2,118-ARR_v1_83@2,The output dim of the information bottleneck layer is 50.,The output dim of information bottleneck layer is 50.,"Modify,Grammar",Grammar
285,118-ARR,118-ARR_v2_82@4,118-ARR_v1_83@4,"On the other hand, we count the length distribution of entity length in different datasets, and finally choose 4 as the maximum enumerated entity length.","On the other hand, we count the length distribution of entity length in different datasets, and finally chose 4 as the maximum enumerated entity length.","Modify,Grammar",Grammar
286,118-ARR,118-ARR_v2_82@5,118-ARR_v1_83@5,The values of β and γ differ for different datasets.,The values of β and γ are different for different data sets.,"Modify,Clarity",Clarity
287,118-ARR,118-ARR_v2_82@7,118-ARR_v1_83@7,The model is trained in an NVIDIA GeForce RTX 2080Ti GPU.,The model is trained in a NVIDIA GeForce RTX 2080Ti GPU.,"Modify,Grammar",Grammar
288,118-ARR,118-ARR_v2_84@1,118-ARR_v1_85@1,"As shown in table 3, we conducted the following comparison and analysis:","As shown in table 3, we have the following observations and analysis:","Modify,Clarity",Clarity
289,118-ARR,118-ARR_v2_85@0,118-ARR_v1_86@0,"1) Our baseline model, i.e., SpanNER, does an excellent job of predicting OOV entities.","1) Our baseline model, i.e., SpanNER, does a good job at predicting OOV entities.","Modify,Other",Other
290,118-ARR,118-ARR_v2_90@4,118-ARR_v1_91@4,It probes the effectiveness of our proposed training objectives that enhances representation via deep understanding of context and entity surface forms and discourages representation from rote memorizing entity names or exploiting biased cues in data.,It probes the effectiveness of our proposed training objectives that enhances representation via deep understanding of context and entity surface forms and discourages representation from rotate memorizing entity names or exploiting biased cues in data.,"Modify,Grammar",Grammar
291,118-ARR,118-ARR_v2_90@5,118-ARR_v1_91@5,"As the coefficient rate increases continuously, the performance shows a declining trend, which means the over-constraint of L gi or L si will hurt the generalizing ability of predicting the OOV entities.","When the coefficient rate increases continuously, the performance shows a decline trend, which means the over-constraint of L gi or L si will hurt the generalizing ability of predicting the OOV entities.","Modify,Clarity",Clarity
292,118-ARR,118-ARR_v2_92@4,118-ARR_v1_93@4,"Take the attention weights of the entity ""State Street"" as an example, it is obvious that baseline model, i.e., SpanNER, focus on entity words themselves.","Take the attention weights of entity ""State Street"" as a example, it is obvious that baseline model, i.e., SpanNER, focus on entity words themselves.","Modify,Grammar",Grammar
293,118-ARR,118-ARR_v2_92@5,118-ARR_v1_93@5,"While the scores of our model are more average, it means that our method concerns more context information.","While the scores of our model is more average, means that our method concern more context information.","Modify,Grammar",Grammar
294,118-ARR,118-ARR_v2_95@0,118-ARR_v1_96@0,This group of methods makes it easier to predict OOV entities using external knowledge.,This of methods makes it easier to predict OOV entities using external knowledge.,"Modify,Grammar",Grammar
295,118-ARR,118-ARR_v2_95@1,118-ARR_v1_96@1,Zhang and Yang (2018) utilize a dictionary to list numerous entity mentions.,Zhang and Yang (2018) Use a dictionary to list numerous entity mentions.,"Modify,Clarity",Clarity
296,118-ARR,118-ARR_v2_95@3,118-ARR_v1_96@3,"To diminish the model's dependency on OOV embedding, introduce partof-speech tags.","To diminish the model's dependency on OOV embedding, introduces partof-speech tags.","Modify,Grammar",Grammar
297,118-ARR,118-ARR_v2_99@2,118-ARR_v1_100@2,Pre-trained models contextualized word embeddings via pretraining on large background corpora.,Pre-trained models contextualized word embbeddings via pretraining on large background corpora.,"Modify,Grammar",Grammar
298,118-ARR,118-ARR_v2_99@3,118-ARR_v1_100@3,"Furthermore, contextualized word embeddings can be provided by the pre-trained models, which are pre-trained on large background corpora (Peters et al., 2018;Devlin et al., 2018;Liu et al., 2019).","Furthermore, contextualized word embeddings can be provided by the pre-trained models which are pre-trained on large background corpora (Peters et al., 2018;Devlin et al., 2018;Liu et al., 2019).","Modify,Grammar",Grammar
299,118-ARR,118-ARR_v2_99@4,118-ARR_v1_100@4,Yan et al. (2021) shows that BERT is not always better at capturing context as compared to Gloe-based BiLSTM-CRFs.,Yan et al. (2021) shows that BERT are not always better at capturing context as compared to Gloe-based BiLSTM-CRFs.,"Modify,Grammar",Grammar
300,118-ARR,118-ARR_v2_99@5,118-ARR_v1_100@5,Their higher performance could be the result of learning the subword structure better.,Their higher performance could be the results of learning the subword structure better.,"Modify,Grammar",Grammar
301,118-ARR,118-ARR_v2_101@0,118-ARR_v1_102@0,"Based on the recent studies of NER, we analyze how to improve the OOV entity recognition.","Based on the recent studies of NER, we analyzed how to improve the OOV entity recognition.","Modify,Grammar",Grammar
302,118-ARR,118-ARR_v2_7@4,118-ARR_v1_7@4,"The strategy is learning a static OOV embedding representation, but not directly utilizing the context.","The strategy is learning a static OOV embedding representation, but not directly utilize the context.","Modify,Grammar",Grammar
303,118-ARR,118-ARR_v2_7@6,118-ARR_v1_7@6,"Unfortunately, Agarwal et al. (2021) shows that the higher performance of pretrained models could be the results of learning the subword structure better.","Unfortunately, Yan et al. (2021) shows that the higher performance of pretrained models could be the results of learning the subword structure better.","Modify,Fact/Evidence",Fact/Evidence
304,118-ARR,118-ARR_v2_8@2,118-ARR_v1_9@0,"Specifically, MINER contains two mutual information based learning objectives: i) generalizing information maximization, which aims to maximize the mutual information between representations and well-generalizing features, i.e., context and entity surface forms; ii) superfluous information minimization, which prevents the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.","Specifically, MINER contains two mutual information based learning objectives: i) generalizing information maximization, which aims to maximize the mutual information between representations and well-generalizing features, i.e., context and entity surface forms; ii) superfluous information minimization, which prevents the model from rote memorizing the entity names or exploiting biased cus via eliminating entity name information.","Modify,Grammar",Grammar
305,118-ARR,118-ARR_v2_10@0,118-ARR_v1_11@0,"1. We propose a novel learning framework, i.e., MINER, from an information theory perspective, aiming to improve the robustness of entity changes by eliminating entity-specific and maximizing wellgeneralizing information.","1. We propose a novel learning framework, i.e., MINER, from an information theory perspective, aiming to improve the robustness of entity changes by eliminating entity-specific and maximize wellgeneralizing information.","Modify,Grammar",Grammar
306,118-ARR,118-ARR_v2_13@1,118-ARR_v1_14@1,"Subsequently, the analysis of possible issues was provided when applying it to OOV entity recognition.","Subsequently, the analysis of possible issues when applying it to OOV entity recognition was provided.","Modify,Clarity",Clarity
307,118-ARR,118-ARR_v2_14@1,118-ARR_v1_15@1,It formulates the goal of representation learning as an information trade-off between predictive power and representation compression.,It formulates the goal of representation learning as an information trade-off between representation compression and predictive power.,"Modify,Clarity",Clarity
308,118-ARR,118-ARR_v2_2@3,118-ARR_v1_2@3,"The proposed approach contains two mutual information-based training objectives: i) generalizing information maximization, which enhances representation via deep understanding of context and entity surface forms; ii) superfluous information minimization, which discourages representation from rote memorizing entity names or exploiting biased cues in data.","The proposed approach contains two mutual information based training objectives: i) generalizing information maximization, which enhances representation via deep understanding of context and entity surface forms; ii) superfluous information minimization, which discourages representation from rotate memorizing entity names or exploiting biased cues in data.","Modify,Grammar",Grammar
309,118-ARR,118-ARR_v2_16@1,118-ARR_v1_17@1,The trade-off between the two MI terms is controlled by the Lagrange multiplier β.,The trade-off between the two MI terms is controlled by a Lagrange multiplier β.,"Modify,Grammar",Grammar
310,118-ARR,118-ARR_v2_18@3,118-ARR_v1_19@3,"Consequently, neural networks tend to use any accessible signal to do so (Ilyas et al., 2019), which is referred to as a shortcut learning problem (Geirhos et al., 2020).","Consequently, neural networks tend to use any accessible signal to do so (Ilyas et al., 2019), which is referred to as a shotcut learning problem (Geirhos et al., 2020).","Modify,Grammar",Grammar
311,118-ARR,118-ARR_v2_19@1,118-ARR_v1_20@1,"In Section 4, we demonstrate how we extend IB to the NER task and address these issues.","In Section 4, we demonstrate how we extend BN to the NER task and address these issues.","Modify,Fact/Evidence",Fact/Evidence
312,118-ARR,118-ARR_v2_21@2,118-ARR_v1_23@1,"2) Compared with sequence labeling, SpanNER does better in sentences with more OOV words (Fu et al., 2021).","2) compared with sequence labeling, SpanNER does better in sentences with more OOV words (Fu et al., 2021).","Modify,Grammar",Grammar
357,124-ARR,,124-ARR_v1_29@0,,We also show explorations with other review combination methods in Appendix C.1.,"Delete,Fact/Evidence",Fact/Evidence
358,124-ARR,,124-ARR_v1_31@5,,"Hence, all our future usage of the word ""Transformers"" refers to bart-large-cnn in the Transformers library .","Delete,Fact/Evidence",Fact/Evidence
359,124-ARR,,124-ARR_v1_71@0,,We show category examples in Table 9.,"Delete,Fact/Evidence",Fact/Evidence
360,124-ARR,,124-ARR_v1_72@0,,"The additional rules for annotation are as follows: First, instead of only labeling the individual sentences per se, the annotators are given a complete paragraph of meta-review to label the sentences with context information.","Delete,Fact/Evidence",Fact/Evidence
361,124-ARR,,124-ARR_v1_72@1,,"For example, if the area chair writes a sentence providing some extra background knowledge in the discussion of the weakness of the submission, that sentence itself can be considered as ""misc"".","Delete,Fact/Evidence",Fact/Evidence
362,124-ARR,,124-ARR_v1_72@2,,"However, it should be labeled as ""weakness"" to be consistent in context.","Delete,Fact/Evidence",Fact/Evidence
363,124-ARR,,124-ARR_v1_72@3,,"Second, not every sentence can be strictly classified into a single category.","Delete,Fact/Evidence",Fact/Evidence
364,124-ARR,,124-ARR_v1_72@4,,"When a sentence contains information from multiple categories, the annotators should consider its main point and primary purpose.","Delete,Fact/Evidence",Fact/Evidence
365,124-ARR,,124-ARR_v1_73@0,,"Furthermore, there are still some cases where the main point of the sentence is hard to differentiate from multiple categories.","Delete,Fact/Evidence",Fact/Evidence
366,124-ARR,,124-ARR_v1_73@3,,"We use the sign "" ? ="" because there are some rare cases where a sentence contains both ""strength"" and ""weakness"" while there is no obvious emphasis on either, and it is hard to tell whether ""strength"" should have a priority over ""weakness"" or the other way round.","Delete,Fact/Evidence",Fact/Evidence
367,124-ARR,,124-ARR_v1_73@4,,"We then label this sentence based on the final decision: if this submission is accepted, we label the sentence as ""strength"", and vice versa.","Delete,Fact/Evidence",Fact/Evidence
368,124-ARR,,124-ARR_v1_74@0,,We further analyze the category distribution in borderline papers.,"Delete,Fact/Evidence",Fact/Evidence
369,124-ARR,,124-ARR_v1_74@1,,"As shown in Table 10, for submissions within the score range of [4.5,6), there are 713 accepted submissions and 2,588 rejected submissions.","Delete,Fact/Evidence",Fact/Evidence
370,124-ARR,,124-ARR_v1_74@2,,"One clear difference is the percentage of ""strength"" and ""weakness"".","Delete,Fact/Evidence",Fact/Evidence
371,124-ARR,,124-ARR_v1_74@3,,"Another difference is the percentage of ""ac disagreement"", where the accepted papers have four times the value than rejected ones.","Delete,Fact/Evidence",Fact/Evidence
372,124-ARR,,124-ARR_v1_74@4,,"This suggests that for the accepted borderline papers, the area chair tends to share different opinions with reviewers, and thus deciding to accept the borderline submissions.","Delete,Claim",Claim
373,124-ARR,,124-ARR_v1_75@0,,"We further analyze the occurrence of each category for accepted papers and rejected papers separately across different score ranges, as shown in Table 11.","Delete,Fact/Evidence",Fact/Evidence
374,124-ARR,,124-ARR_v1_75@1,,"For accepted papers, as the score increases, the percentage of meta-reviews having ""weakness"" and ""suggestion"" drops because the high-score submissions are more likely to be accepted.","Delete,Fact/Evidence",Fact/Evidence
375,124-ARR,,124-ARR_v1_75@2,,"Even the percentage of ""decision"" drops following the same trend.","Delete,Fact/Evidence",Fact/Evidence
376,124-ARR,,124-ARR_v1_76@1,,"For the concat, we simply concatenate all reviews one after another according to their reviewers' sequence.","Delete,Fact/Evidence",Fact/Evidence
377,124-ARR,,124-ARR_v1_76@2,,"For merge, we can obtain the merged content as follows: From all review inputs, we use the longest one as a backbone.","Delete,Fact/Evidence",Fact/Evidence
378,124-ARR,,124-ARR_v1_83@0,,"For preprocessing, besides filtering based on metareview length, we also remove submissions with only one or two reviews, since the majority of the submissions have more than 3 reviews.","Delete,Fact/Evidence",Fact/Evidence
379,124-ARR,,124-ARR_v1_85@2,,"For the rest of the hyperparameters, we use the pretrained model's default values.","Delete,Fact/Evidence",Fact/Evidence
380,124-ARR,124-ARR_v2_2@7,,"† Chenhui, Liying, and Ran are under the Joint PhD Program between Alibaba and their corresponding universities.",,"Add,Fact/Evidence",Fact/Evidence
381,124-ARR,124-ARR_v2_3@1,,1 Our code and data are released at https://github.,,"Add,Fact/Evidence",Fact/Evidence
382,124-ARR,124-ARR_v2_6@1,,"The more-to-less text generation tasks output a concise piece of text from some more abundant input, such as text summarization (Tan et al., 2017;Kryściński et al., 2018).",,"Add,Fact/Evidence",Fact/Evidence
383,124-ARR,124-ARR_v2_11@0,,(2) We propose a new task of controllable generation focusing on controlling the passage macro structures.,,"Add,Fact/Evidence",Fact/Evidence
384,124-ARR,124-ARR_v2_11@1,,It offers stronger generation flexibility and applicability for practical use cases.,,"Add,Claim",Claim
385,124-ARR,124-ARR_v2_16@3,,Table 2 shows the statistics of data collected from each year.,,"Add,Fact/Evidence",Fact/Evidence
386,124-ARR,124-ARR_v2_16@4,,"Initially, 7,894 submissions are collected.",,"Add,Fact/Evidence",Fact/Evidence
387,124-ARR,124-ARR_v2_16@5,,"After filtering, 7,089 meta-reviews are retained with their corresponding 23,675 reviews.",,"Add,Fact/Evidence",Fact/Evidence
388,124-ARR,,124-ARR_v1_9@0,,"We will release our full dataset, code, and detailed settings to the community.","Delete,Claim",Claim
389,124-ARR,124-ARR_v2_41@2,,"Due to long inputs (see Table 17), we experiment with different source truncation lengths of 1024, 2048, and 3072 tokens.",,"Add,Fact/Evidence",Fact/Evidence
390,124-ARR,124-ARR_v2_41@3,,We cannot explore truncation length of more than 3072 tokens due to the limitation of GPU space.,,"Add,Fact/Evidence",Fact/Evidence
391,124-ARR,124-ARR_v2_41@4,,"Our learning rate is 5e-5, and we use Adam optimizer with momentum β 1 = 0.9, β 2 = 0.999 without any warm-up steps or weight decay.",,"Add,Fact/Evidence",Fact/Evidence
392,124-ARR,124-ARR_v2_41@5,,"We set the seed to be 0, and train the model for 3 epochs with gradient accumulation step of 1.",,"Add,Fact/Evidence",Fact/Evidence
393,124-ARR,124-ARR_v2_41@6,,"For decoding, we use a beam size of 4 and length penalty of 2.",,"Add,Fact/Evidence",Fact/Evidence
394,124-ARR,124-ARR_v2_45@0,,Review Combination Results,,"Add,Other",Other
395,124-ARR,124-ARR_v2_46@0,,"We also show uncontrolled generation results for different review combination methods in Table 6, with source truncation of 2048.",,"Add,Fact/Evidence",Fact/Evidence
396,124-ARR,124-ARR_v2_46@2,,"Rateconcat has the best overall performance, which is the setting we used for the main results.",,"Add,Fact/Evidence",Fact/Evidence
397,124-ARR,124-ARR_v2_46@3,,"Never- theless, it is not significantly better than merge.",,"Add,Fact/Evidence",Fact/Evidence
398,124-ARR,124-ARR_v2_46@4,,"It is also interesting to see that for merge, providing additional rating information (rate-merge) slightly worsens the performance.",,"Add,Fact/Evidence",Fact/Evidence
399,124-ARR,124-ARR_v2_46@5,,We will leave the investigation of better review combination methods for future work.,,"Add,Claim",Claim
400,124-ARR,124-ARR_v2_61@1,,"For each test instance, we provide the judges with the input reviews and randomly ordered generations from different models, and ask them to individually evaluate the generations based on the following criteria: (1) Fluency: is the generation fluent, grammatical, and without unnecessary repetitions? (2) Content Relevance: does the generation reflect the review content well, or does it produce general but trivial sentences? (3) Structure Similarity: how close does the generation structure resemble the gold structure (i.e., the control sequence)? ( 4) Decision Correctness: does the generation correctly predicts the gold human decision?",,"Add,Fact/Evidence",Fact/Evidence
401,124-ARR,124-ARR_v2_66@2,,Our work is the first fully-annotated dataset in this domain for the structure-controllable generation task.,,"Add,Claim",Claim
402,124-ARR,124-ARR_v2_70@0,,Ethical Concerns,,"Add,Other",Other
403,124-ARR,124-ARR_v2_71@0,,We have obtained approval from ICLR organizers to use the data collected from ICLR 2018-2021 on OpenReview.,,"Add,Fact/Evidence",Fact/Evidence
404,124-ARR,124-ARR_v2_72@1,,"Note that due to limited GPU space, we cannot fit 2048 input tokens for T5.",,"Add,Fact/Evidence",Fact/Evidence
405,124-ARR,124-ARR_v2_72@2,,"Thus, for fair comparison, all results shown are from source truncation of 1024.",,"Add,Fact/Evidence",Fact/Evidence
406,124-ARR,124-ARR_v2_73@1,,"Specifically, we define the task as a sequence labeling problem and apply the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks with a conditional random field (CRF) (Lafferty et al., 2001) (i.e., LSTM-CRF (Lample et al., 2016)) model on the annotated MReD dataset.",,"Add,Fact/Evidence",Fact/Evidence
407,124-ARR,124-ARR_v2_74@0,,The same data split as the meta-review generation task is used.,,"Add,Fact/Evidence",Fact/Evidence
408,124-ARR,124-ARR_v2_74@1,,"We adopt the standard IOBES tagging scheme (Ramshaw, 1995;Ratinov and Roth, 2009), and fine-tune BERT (Devlin et al., 2019) and RoBERTa models in Hugging Face.",,"Add,Fact/Evidence",Fact/Evidence
409,124-ARR,124-ARR_v2_74@2,,"All models are trained for 30 epochs with an early stop of 20, and each epoch takes about 30 minutes.",,"Add,Fact/Evidence",Fact/Evidence
410,124-ARR,124-ARR_v2_74@3,,We select the best model parameters based on the best micro F 1 score on the development set and apply it to the test set for evaluation.,,"Add,Fact/Evidence",Fact/Evidence
411,124-ARR,124-ARR_v2_75@0,,All models are run with single V100 GPUs.,,"Add,Fact/Evidence",Fact/Evidence
412,124-ARR,124-ARR_v2_75@1,,"We use Adam (Kingma and Ba, 2014) with an initial learning rate of 2e-5.",,"Add,Fact/Evidence",Fact/Evidence
413,124-ARR,124-ARR_v2_75@2,,We report the F 1 scores for each category as well as the overall micro F 1 and macro F 1 scores in Table 14.,,"Add,Fact/Evidence",Fact/Evidence
414,124-ARR,124-ARR_v2_79@2,,"Nevertheless, the pattern is less evident in the source (reviews) baselines.",,"Add,Claim",Claim
415,124-ARR,124-ARR_v2_19@2,124-ARR_v1_16@2,"Each meta-review sentence is independently labeled by 2 different annotators, and a third expert annotator resolves any disagreement between the first two annotators.","Each meta-review sentence is independently labeled by 2 different annotators, and a third annotator resolves any disagreement between the first two annotators.","Modify,Clarity",Clarity
416,124-ARR,124-ARR_v2_19@3,124-ARR_v1_16@3,"We label 45,929 sentences from 7,089 meta-reviews in total, and the Cohen's kappa is 0.778 between the first two annotators, showing that the annotation is of quite high quality.","We label 45,929 sentences from 7,089 meta-reviews in total, and the Cohen's kappa is 0.778 between the two annotators, showing that the annotation is of quite high quality.","Modify,Clarity",Clarity
417,124-ARR,124-ARR_v2_22@1,124-ARR_v1_19@1,"The number of sentences in different categories are shown in Figure 1, breakdown by the decision (i.e., accept or reject).","The sentence numbers in different categories are shown in Figure 1, breakdown by the decision (i.e., accept or reject).","Modify,Clarity",Clarity
418,124-ARR,124-ARR_v2_27@2,124-ARR_v1_24@2,"To achieve such flexibility, the task of structure-controllable text generation is defined as: given the text input (i.e., reviews) and a control sequence of the output structure, a model should generate a meta-review that is derivable from the reviews and presents the required structure.","To achieve such flexibility, the task of structure-controllable text generation is defined as: given the text input (i.e., reviews) and a control sequence of the output structure, a model should generate a meta-review which is derivable from the reviews and presents the required structure.","Modify,Clarity",Clarity
419,124-ARR,124-ARR_v2_31@1,124-ARR_v1_28@1,"One simple method, concat, is to concatenate all inputs one after another (Fabbri et al., 2019).","One simple method to combine multiple inputs for encoder-decoder models is to concatenate all inputs one after another (Fabbri et al., 2019).","Modify,Clarity",Clarity
420,124-ARR,124-ARR_v2_31@2,124-ARR_v1_28@2,"Besides the text inputs, the review rating, which cannot be found in the review passages but exists in the field of rating score, is also crucial information for writing meta-reviews.","Beside the text inputs, the review rating is also crucial information for writing meta reviews, which cannot be found in the review passages but exists in the field of rating score.","Modify,Clarity",Clarity
421,124-ARR,124-ARR_v2_32@2,124-ARR_v1_30@2,Sent-ctrl uses one control label per target sentence and controls generation on the sentence-level.,Sent-ctrl uses one control label per target sentence and controls generation on a sentence-level.,"Modify,Grammar",Grammar
422,124-ARR,124-ARR_v2_33@4,124-ARR_v1_31@4,"More specifically, we use the Py-Torch implementation in the open-source library Hugging Face Transformers (Wolf et al., 2020) and its hosted pretrained models 3 .","More specifically, we use the pytorch implementation in the open-source library Hugging Face Transformers (Wolf et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
423,124-ARR,124-ARR_v2_36@5,124-ARR_v1_34@5,"After ranking with each of the above models, we select sentences as output with different strategies according to the controlled and uncontrolled settings.","After ranking with the above models, we select sentences as output with different strategies according to the controlled and uncontrolled settings.","Modify,Clarity",Clarity
424,124-ARR,124-ARR_v2_36@8,124-ARR_v1_34@8,"To do so, we employ an LSTM-CRF (Lample et al., 2016) tagger trained on the labeled meta-reviews to predict the labels of each input review sentence.","To do so, we employ an LSTM-CRF (Lample et al., 2016) tagger trained on the labeled meta-reviews to predict the sentence labels of each input review.","Modify,Clarity",Clarity
425,124-ARR,124-ARR_v2_43@5,124-ARR_v1_39@5,"On the other hand, for bart-large-cnn, sent-ctrl is the best, followed by seg-ctrl.","On the other hand, for the Transformers, sent-ctrl is the best, followed by seg-ctrl.","Modify,Fact/Evidence",Fact/Evidence
426,124-ARR,124-ARR_v2_44@1,124-ARR_v1_39@8,"This is also validated by the ""Target Generic"" baseline's consistent improvement over the ""Source Generic"" baseline, which shows that generic sentences from meta-reviews can suit generation better than those in reviews.","This is again validated by the ""Target Generic"" baseline's significant improvement over the ""Source Generic"" baseline, which shows that generic sentences from meta-reviews can suit generation much better than those in reviews.","Modify,Clarity",Clarity
427,124-ARR,124-ARR_v2_63@1,124-ARR_v1_54@1,"Sentctrl also has better fluency and decision correctness, suggesting that having a better output structure can benefit readability and decision generation.","Sentctrl also has better fluency and decision correctness, suggesting that having a better output structure can benefit the readability and decision generation.","Modify,Grammar",Grammar
428,124-ARR,124-ARR_v2_65@2,124-ARR_v1_56@2,"In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely, structurecontrollable text generation.","In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely structurecontrollable text generation.","Modify,Grammar",Grammar
429,124-ARR,124-ARR_v2_66@3,124-ARR_v1_57@2,"There are also some datasets and annotation schemes on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain and cannot be easily transferred to our task.","There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.","Modify,Claim",Claim
430,124-ARR,124-ARR_v2_7@0,124-ARR_v1_5@0,"To some extent, the existing task settings are not so adequate because they do not have a deep understanding of the domains they are working on, i.e., domain knowledge.","To some extent, the existing task settings are not so adequate because they do not have deep understanding of the domains they are working on, i.e., domain knowledge.","Modify,Grammar",Grammar
431,124-ARR,124-ARR_v2_7@1,124-ARR_v1_5@1,"Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail (Nallapati et al., 2016) is composed of the training pairs of news content and human-written summary bullets.","Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail (Nallapati et al., 2016) is composed of the training pairs of news content and news titles.","Modify,Fact/Evidence",Fact/Evidence
432,124-ARR,124-ARR_v2_31@7,124-ARR_v1_76@4,"Then, for each paragraph embedding in the nonbackbone reviews, we calculate a cosine similarity score with each backbone paragraph embedding.","Then, for each paragraph embedding in the non-backbone reviews, we calculate a cosine similarity score with each backbone paragraph embedding, and insert it after the backbone paragraph with which it has the highest similarity score.","Split+Modify,Grammar",Grammar
433,124-ARR,124-ARR_v2_31@8,124-ARR_v1_76@4,We then insert each non-backbone paragraph after the backbone paragraph with which it has the highest similarity score.,"Then, for each paragraph embedding in the non-backbone reviews, we calculate a cosine similarity score with each backbone paragraph embedding, and insert it after the backbone paragraph with which it has the highest similarity score.","Split+Modify,Clarity",Clarity
434,124-ARR,124-ARR_v2_7@2,124-ARR_v1_5@2,"However, it does not tell why a particular piece of news content should have that corresponding summary, for example for the same earnings report, why one media emphasizes its new business success in the summary, but another emphasizes its net income.","However, it does not tell why a particular piece of news content should have that corresponding title, for example for the same earnings report, why one media emphasizes its new business success in the title, but another emphasizes its net income.","Modify,Clarity",Clarity
435,124-ARR,124-ARR_v2_31@11,124-ARR_v1_76@6,"Additionally, we provide a longest-review baseline, which does not combine reviews but only uses the longest review as the input.","Additionally, we provide a baseline setting longestreview, which does not combine reviews but only uses the longest review as the input.","Modify,Clarity",Clarity
436,124-ARR,124-ARR_v2_31@10,124-ARR_v1_76@7,We further add rating sentences in front of the results of merge to obtain rate-merge.,"Moreover, we add rating sentences in front of the results of concat and merge to obtain rate-concat and rate-merge, respectively.","Modify,Fact/Evidence",Fact/Evidence
437,124-ARR,124-ARR_v2_46@1,124-ARR_v1_77@0,"The longest-review setting has the worst performance, thus validating that the review combination methods are necessary in order not to omit important information.","As shown in Table 12, the longest-review setting has the worst performance, thus validating that the review combination methods are necessary in order not to omit important information.","Modify,Fact/Evidence",Fact/Evidence
438,124-ARR,124-ARR_v2_75@7,124-ARR_v1_79@6,"RoBERTabase is the best performing model, therefore we use this model to predict review sentence labels.","RoBERTabase is the best performing model, therefore we use this model for review sentence label prediction.","Modify,Clarity",Clarity
439,124-ARR,124-ARR_v2_76@0,124-ARR_v1_80@0,"Besides the baselines of ""Source Generic"" and ""Target Generic"", we explore subsets of papers with high scores (average reviewers' rating ⩾ 7) or low scores (average reviewers' rating ⩽ 3) to obtain 4 generic baselines: ""Source High Score"", ""Source Low Score"", ""Target High Score"", ""Target Low Score"".","Besides the baselines of ""Source Generic"" and ""Target Generic"", we explore subsets of papers with high scores (average reviewers' rating 7) or low scores (average reviewers' rating 3) to obtain 4 additional generic baselines: ""Source High Score"", ""Source Low Score"", ""Target High Score"", ""Target Low Score"".","Modify,Fact/Evidence",Fact/Evidence
440,124-ARR,124-ARR_v2_77@0,124-ARR_v1_80@1,"We use ""Target High Score"" as an example to explain how we obtain the generic sentences: From the training subset of high score papers, We first separate all meta-review sentences into the corresponding label categories, obtaining a total of 9 groups of sentences.","We use ""Target Generic"" as an example to explain how we obtain the generic sentences: We first group all meta-review sentences from the training set according to their label categories, and then re-arrange the sentences in each category using TextRank (our best performing extractive model).","Split+Modify,Fact/Evidence",Fact/Evidence
441,124-ARR,124-ARR_v2_77@1,124-ARR_v1_80@1,"Then, we re-arrange the sentences in each group using TextRank (our best extractive model).","We use ""Target Generic"" as an example to explain how we obtain the generic sentences: We first group all meta-review sentences from the training set according to their label categories, and then re-arrange the sentences in each category using TextRank (our best performing extractive model).","Split+Modify,Clarity",Clarity
442,124-ARR,124-ARR_v2_79@0,124-ARR_v1_80@3,"All generic sentence baselines can be obtained in a similarly procedure as outlined above, and we show results in Table 15.","Similarly, different sets of generic sentences can be obtained for the other 5 baselines.","Merge+Modify,Fact/Evidence",Fact/Evidence
443,124-ARR,124-ARR_v2_78@0,124-ARR_v1_81@0,"After obtaining the generic sentence sets, we can create baseline generations using the sent-ctrl sequence on the corresponding high score paper test data.","After obtaining the generic sentence sets, we can create baseline generations using the sent-ctrl sequence.","Modify,Fact/Evidence",Fact/Evidence
444,124-ARR,124-ARR_v2_79@0,124-ARR_v1_82@0,"All generic sentence baselines can be obtained in a similarly procedure as outlined above, and we show results in Table 15.",We show results in Table 15.,"Merge+Modify,Clarity",Clarity
445,124-ARR,124-ARR_v2_79@1,124-ARR_v1_82@1,"Both ""Target High Score"" and ""Target Low Score"" perform much better than the ""Target Genric"" baseline, suggesting that papers with very high or low scores tend to have more typical patterns in their meta-reviews.","The low score baselines perform the best amongst both source and target baselines, suggesting that the sentences from low score submissions are more typical for both reviews and meta-reviews.","Modify,Claim",Claim
446,124-ARR,124-ARR_v2_41@0,124-ARR_v1_85@0,"For bart-large-cnn, we first load the pretrained model and then fine-tune it on MReD. All experiments are conducted on single V100 GPUs, using a batch size of 1 in order to fit the large pretrained model on a single GPU.","For the Transformers, we first load the pretrained model and then fine-tune it on MReD. All experiments are conducted on single V100 GPUs, using a batch size of 1 in order to fit the large pretrained model on a single GPU.","Modify,Fact/Evidence",Fact/Evidence
447,124-ARR,124-ARR_v2_41@1,124-ARR_v1_85@1,"During fine-tuning, we set the hyperparameters of ""minimum_target_length"" to 20, and ""maximum_target_length"" to 400, according to our filter range on the meta-review lengths.","During finetuning, we set the Transformers' hyperparameters of ""minimum_target_length"" to 20, and ""maxi-mum_target_length"" to 400, according to our filter range on the meta-review lengths.","Modify,Clarity",Clarity
448,124-ARR,124-ARR_v2_80@1,124-ARR_v1_86@1,We further investigate the performance of different source truncation lengths under the setting of rate-concat.,We further investigate the performance of different source truncation lengths using rate-concat.,"Modify,Clarity",Clarity
449,124-ARR,124-ARR_v2_82@1,124-ARR_v1_88@1,"We use max-pooling to aggregate attention for same-sentence input tokens, because summation unfairly gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out.","We use max-pooling to aggregate attention for same-sentence input tokens, because summation gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out.","Modify,Other",Other
450,124-ARR,124-ARR_v2_84@1,124-ARR_v1_91@1,"We then calculate the normalized token-level edit distance between the judge-annotated label sequence and the given control sequence, where each label is considered as a single token, and finally deduct this value from 1.","We then calculate the normalized token-level edit distance between the judge-annotated label sequence and the given control sequence, then deduct this value from 1.","Modify,Fact/Evidence",Fact/Evidence
451,124-ARR,124-ARR_v2_85@1,124-ARR_v1_92@1,"More specifically, we give 0 if the generation produces either contradictory decisions or a wrong decision, or if the generation does not show enough hints for rejection or acceptance.","More specifically, we give 0 if the generation produces contradictory decisions and a wrong decision, or the generation does not show enough hints for rejection or acceptance.","Modify,Grammar",Grammar
452,124-ARR,124-ARR_v2_8@2,124-ARR_v1_6@2,"Thus from the same input text, the trained generator can generate varied outputs according to the given control signals.","Thus from the same input text, the trained generator can generate varied outputs according to the given control signal.","Modify,Grammar",Grammar
453,124-ARR,124-ARR_v2_2@2,124-ARR_v1_2@2,"A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with a deep understanding of the domain knowledge.","A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with deep understanding of the domain knowledge.","Modify,Grammar",Grammar
454,124-ARR,124-ARR_v2_8@9,124-ARR_v1_6@9,"Our MReD is obviously different from the previous text generation/summarization datasets because, given the rich annotations of individual meta-review sentences, a model is allowed to learn more sophisticated generation behaviors to control the structure of the generated passage.","Our MReD is obviously different from previous text generation/summarization datasets because, given the rich annotations of individual meta-review sentences, a model is allowed to learn more sophisticated generation behaviors to control the structure of the generated passage.","Modify,Grammar",Grammar
455,124-ARR,124-ARR_v2_8@10,124-ARR_v1_6@10,"Our proposed task is also noticeably different from the existing controllable text generation tasks (e.g., text style transfer on sentiment polarity (Shen et al., 2017;Liao et al., 2018) and formality (Shang et al., 2019)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.","Our proposed task is also noticeably different from existing controllable text generation tasks (e.g., text style transfer on sentiment polarity (Shen et al., 2017;Liao et al., 2018) and formality (Shang et al., 2019)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.","Modify,Grammar",Grammar
456,124-ARR,124-ARR_v2_14@1,124-ARR_v1_11@1,"Unlike the previous datasets that mainly focus on domains like news, the domain for meta-reviews is worth-studying because it contains essential and high-density opinions.","Unlike previous datasets that mainly focus on domains like news, meta-review is a worthstudying domain containing essential and highdensity opinions.","Modify,Clarity",Clarity
457,124-ARR,124-ARR_v2_16@0,124-ARR_v1_13@0,"We collect the meta-review related data of ICLR from an online peer-reviewing platform, i.e., Open-Review 2 from 2018 to 2021.",We collect the meta-review related data from an online peer reviewing platform for ICLR 2 from 2018 to 2021.,"Modify,Fact/Evidence",Fact/Evidence
458,124-ARR,124-ARR_v2_16@2,124-ARR_v1_13@2,"To prepare our dataset for controllable text generation, for each submission, we collect all of its corresponding official reviews with reviewer ratings and confidence scores, the final meta-review decision, and the meta-review passage.","To prepare our dataset for controllable text generation, for each submission, we collect multiple reviews with reviewer ratings and confidence scores, the final meta-review decision, and the meta-review passage.","Modify,Fact/Evidence",Fact/Evidence
459,125-ARR,,125-ARR_v1_25@0,,"We want to point out one advantage of using generation-based models under the low-resource scenario compared to previous classification-based event extraction models -generation-based models do not require named entity annotations (Sha et al., 2018;Lin et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
460,125-ARR,,125-ARR_v1_25@1,,"The pre-trained decoder inherently identifies reasonable entity spans, which makes generation-based models become a good choice when annotations are expensive.","Delete,Claim",Claim
461,125-ARR,,125-ARR_v1_48@1,,"In addition to the previously mentioned EE models: OneIE (Lin et al., 2020), BERT_QA (Du and Cardie, 2020), TANL (Paolini et al., 2021), and Text2Event (Lu et al., 2021), we also consider the following baselines focusing on the high-resource setting.","Delete,Fact/Evidence",Fact/Evidence
462,125-ARR,,125-ARR_v1_48@7,,"Therefore, the advantage of DEGREE over DEGREE(PIPE) becomes less obvious.","Delete,Claim",Claim
463,125-ARR,,125-ARR_v1_48@8,,This result justifies our hypothesis that DEGREE has better performance for the lowresource setting because of its ability to better capture dependencies.,"Delete,Claim",Claim
464,125-ARR,,125-ARR_v1_49@0,,Results for event argument extraction.,"Delete,Other",Other
465,125-ARR,,125-ARR_v1_49@1,,"In Table 3, we additionally study the performance for event argument extraction task, where the model makes argument predictions with the gold trigger provided.","Delete,Fact/Evidence",Fact/Evidence
466,125-ARR,,125-ARR_v1_49@2,,"Interestingly, DEGREE(EAE) achieves pretty strong performance and outperforms other baselines with a large margin.","Delete,Fact/Evidence",Fact/Evidence
467,125-ARR,,125-ARR_v1_49@3,,"Combining the results in Table 2, we hypothesize that event argument extraction is a more challenging task than event trigger detection and it requires more training examples to learn well.","Delete,Claim",Claim
468,125-ARR,,125-ARR_v1_49@4,,"Hence, our proposed model, which takes the advantage of using label semantics to better capture dependencies, achieves a new state-of-the-art for event argument extraction.","Delete,Claim",Claim
469,125-ARR,,125-ARR_v1_87@0,,Limitations.,"Delete,Other",Other
470,125-ARR,,125-ARR_v1_87@2,,We believe this assumption holds for most of common NLP tasks.,"Delete,Claim",Claim
471,125-ARR,,125-ARR_v1_87@3,,"However, for some specific domains, such as the biomedical domain, acquiring this information can be a bit difficult (e.g., needs to hire experts to write down templates), which increases the cost of training DEGREE.","Delete,Claim",Claim
472,125-ARR,,125-ARR_v1_87@4,,"In addition, our proposed model is based on pre-trained language models.","Delete,Fact/Evidence",Fact/Evidence
473,125-ARR,,125-ARR_v1_87@5,,DEGREE performs well because it is able to leverage the prompts and the pre-trained knowledge.,"Delete,Claim",Claim
474,125-ARR,,125-ARR_v1_87@6,,"However, if the downstream domain is far from the pre-trained corpus, the advantage of leveraging knowledge becomes restricted.","Delete,Claim",Claim
475,125-ARR,,125-ARR_v1_87@8,,"DE-GREE achieves a good performance on two datasets (ACE 2005 and ERE-EN), which are more related to news-styled passages.","Delete,Claim",Claim
476,125-ARR,,125-ARR_v1_87@9,,"When considering other downstream domains, it is possible that the improvement is not as significant as it is for the two datasets we use in the paper.","Delete,Claim",Claim
477,125-ARR,,125-ARR_v1_87@10,,"The reason is the gap between the downstream domain knowledge and the pre-trained knowledge, as mentioned in the previous paragraph.","Delete,Claim",Claim
478,125-ARR,,125-ARR_v1_88@0,,Potential risks.,"Delete,Other",Other
479,125-ARR,125-ARR_v2_4@3,,"Fincke et al., 2021) usually divides EE into two subtasks: (1) event detection, which identifies event triggers and their types, and (2) event argument extraction, which extracts the arguments and their roles for given event triggers.",,"Add,Fact/Evidence",Fact/Evidence
480,125-ARR,125-ARR_v2_4@4,,"EE has been shown to benefit a wide range of applications, e.g., building knowledge graphs , question answering (Berant et al., 2014;, and other downstream studies (Han et al., 2019a;Hogenboom et al., 2016;.",,"Add,Fact/Evidence",Fact/Evidence
481,125-ARR,125-ARR_v2_12@9,,"For example, in our experiments, we take the information from the annotation guideline, which is provided along with the dataset.",,"Add,Fact/Evidence",Fact/Evidence
482,125-ARR,125-ARR_v2_16@0,,Our code and models can be found at https: //github.com/PlusLabNLP/DEGREE.,,"Add,Fact/Evidence",Fact/Evidence
483,125-ARR,125-ARR_v2_23@0,,We list three EAE templates in Table 1.,,"Add,Fact/Evidence",Fact/Evidence
484,125-ARR,125-ARR_v2_32@0,,Efficiency Considerations.,,"Add,Other",Other
485,125-ARR,125-ARR_v2_32@1,,"DEGREE requires to enumerate all event types during inference, which could cause efficiency considerations when extending to applications that contain many event types.",,"Add,Claim",Claim
486,125-ARR,125-ARR_v2_32@2,,"This issue is minor for our experiments on the two datasets (ACE 2005 and ERE-EN), which are relatively small scales in terms of the number of event types.",,"Add,Claim",Claim
487,125-ARR,125-ARR_v2_32@4,,We leave the work on benchmarking and improving the efficiency of DEGREE in the scenario considering more diverse and comprehensive types of events as future work.,,"Add,Claim",Claim
488,125-ARR,125-ARR_v2_46@1,,"We consider the following classification-based models: (1) OneIE , the current state-of-the-art (SOTA) EE model trained with designed global features.",,"Add,Fact/Evidence",Fact/Evidence
489,125-ARR,125-ARR_v2_46@2,,"( 2) BERT_QA (Du and Cardie, 2020), which views EE tasks as a sequence of extractive question answering problems.",,"Add,Fact/Evidence",Fact/Evidence
490,125-ARR,125-ARR_v2_46@3,,"Since it learns a classifier to indicate the position of the predicted span, we view it as a classification model.",,"Add,Fact/Evidence",Fact/Evidence
491,125-ARR,125-ARR_v2_46@4,,"We also consider the following generation-based models: (3) TANL (Paolini et al., 2021), which treats EE tasks as translation tasks between augmented natural languages.",,"Add,Fact/Evidence",Fact/Evidence
492,125-ARR,125-ARR_v2_52@0,,"Finally, we perform additional experiments on few-shot and zero-shot experiments.",,"Add,Fact/Evidence",Fact/Evidence
493,125-ARR,125-ARR_v2_52@1,,The results can be found in Appendix E.,,"Add,Fact/Evidence",Fact/Evidence
494,125-ARR,125-ARR_v2_65@4,,"The output sequence designs of TANL and Temp-Gen hinder the models from fully leveraging label semantics, unlike DEGREE that generates natural sentences.",,"Add,Claim",Claim
495,125-ARR,125-ARR_v2_70@1,,This assumption may holds for most situations.,,"Add,Claim",Claim
496,125-ARR,125-ARR_v2_70@2,,"We leave the automation of template construction for future work, which can further ease the needed efforts when deploying DEGREE in a large-scale corpus.",,"Add,Claim",Claim
497,125-ARR,125-ARR_v2_94@0,,"In order to further test our models' generaliability, we additionally conduct zero-shot and fewshot experiments on the ACE05-E dataset with DEGREE(ED) and DEGREE(EAE).",,"Add,Fact/Evidence",Fact/Evidence
498,125-ARR,125-ARR_v2_95@0,,Settings.,,"Add,Other",Other
499,125-ARR,125-ARR_v2_95@1,,"We first select the top n common event types as ""seen"" types and use the rest as ""unseen/rare"" types, where the top common types are listed in Table 12.",,"Add,Fact/Evidence",Fact/Evidence
500,125-ARR,125-ARR_v2_95@2,,"To simulate a zero-shot scenario, we remove all events with ""unseen/rare"" types from the training data.",,"Add,Fact/Evidence",Fact/Evidence
501,125-ARR,125-ARR_v2_95@3,,"To simulate a few-shot scenario, we keep only k event examples for each ""unseen/rare"" type (denoted as k-shot).",,"Add,Fact/Evidence",Fact/Evidence
502,125-ARR,125-ARR_v2_95@4,,"During the evaluation, we calculate micro F1-scores only for these ""unseen/rare"" types.",,"Add,Fact/Evidence",Fact/Evidence
503,125-ARR,125-ARR_v2_23@1,125-ARR_v1_20@0,The full list of EAE templates and the construction details can be found in Appendix A.,The full list of EAE templates and the constructing details can be found in Appendix A.,"Modify,Clarity",Clarity
504,125-ARR,125-ARR_v2_2@7,125-ARR_v1_2@7,"Moreover, DEGREE is capable of using additional weaklysupervised information, such as the description of events encoded in the prompts.","In addition, the proposed model is capable of using additional weakly-supervised information, such as the description of events.","Modify,Fact/Evidence",Fact/Evidence
505,125-ARR,125-ARR_v2_26@1,125-ARR_v1_22@1,"For the case that there are multiple triggers for the given event type in the input passage, DEGREE is trained to generate the output text that contains multiple E2E template such that each E2E template corresponds to one trigger and its argument roles.","For the case that there are multiple triggers for the given event type, DEGREE will generate the E2E template multiple times such that each E2E template corresponds to each trigger and its argument roles.","Modify,Fact/Evidence",Fact/Evidence
506,125-ARR,125-ARR_v2_28@1,125-ARR_v1_23@2,"After we obtain the generated sentences, we compare the outputs with E2E template to determine the predicted triggers and arguments in string format.","Then, we compare the generated output with the placeholders in E2E template to determine the predicted trigger spans and predicted argument spans.","Modify,Fact/Evidence",Fact/Evidence
507,125-ARR,125-ARR_v2_28@2,125-ARR_v1_23@3,"Finally, we apply string matching to convert the predicted string to span offsets in the passage.","Finally, we apply string matching to convert the word spans to the offsets in the passage.","Modify,Fact/Evidence",Fact/Evidence
508,125-ARR,125-ARR_v2_28@3,125-ARR_v1_23@4,"If the predicted string appears in the passage multiple times, we choose all span offsets that match for trigger predictions and choose the one closest to the given trigger span for argument predictions.","If the predicted span appears in the passage multiple times, we choose all that match for trigger predictions and choose the one being closest to the given trigger span for argument predictions.","Modify,Clarity",Clarity
509,125-ARR,125-ARR_v2_2@8,125-ARR_v1_2@8,"Finally, DE-GREE learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them.","Finally, learning triggers and argument roles in an end-toend manner encourages the model to better utilize the shared knowledge and dependencies between them.","Modify,Clarity",Clarity
510,125-ARR,125-ARR_v2_30@6,125-ARR_v1_24@7,"For example, DEGREE knows the relationship between the role Attacker and the role Target (who is attacking and who is attacked) due to E2E template.","For example, DEGREE knows the relation between the role Attacker and the role Target (who is attacking and who is attacked) because of the word ""attacked"" in E2E template.","Modify,Fact/Evidence",Fact/Evidence
511,125-ARR,125-ARR_v2_30@7,125-ARR_v1_24@8,This guidance helps DEGREE learn the dependencies between entities.,This guidance makes DEGREE learn the dependencies between entities well with less training data.,"Modify,Claim",Claim
512,125-ARR,125-ARR_v2_30@9,125-ARR_v1_24@10,This not only uses label semantics better but also makes the model easier to leverage the knowledge from the pre-trained decoder.,This not only utilizes label semantics better but also makes the model easier to leverage the knowledge from the pre-trained decoder.,"Modify,Clarity",Clarity
513,125-ARR,125-ARR_v2_2@9,125-ARR_v1_2@9,Our experimental results demonstrate the strong performance of DEGREE for low-resource event extraction.,Our experimental results and ablation studies demonstrate the strong performance of DEGREE for low-resource event extraction.,"Modify,Fact/Evidence",Fact/Evidence
514,125-ARR,125-ARR_v2_31@4,125-ARR_v1_26@4,"In fact, several prior works Du and Cardie, 2020;) also use constructed templates as weakly-supervised signals to improve models.","In fact, several prior works Du and Cardie, 2020;Li et al., 2020) also use constructed templates as weakly-supervised signals to improve models.","Modify,Fact/Evidence",Fact/Evidence
515,125-ARR,125-ARR_v2_2@0,125-ARR_v1_2@0,"Event extraction requires high-quality expert human annotations, which are usually expensive.","Due to the high cost of human annotations, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge.","Split+Modify,Claim",Claim
516,125-ARR,125-ARR_v2_2@1,125-ARR_v1_2@0,"Therefore, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge.","Due to the high cost of human annotations, learning a data-efficient event extraction model that can be trained with only a few labeled examples has become a crucial challenge.","Split+Modify,Clarity",Clarity
517,125-ARR,125-ARR_v2_4@0,125-ARR_v1_4@0,"Event extraction (EE) aims to extract events, each of which consists of a trigger and several participants (arguments) with their specific roles, from a given passage.","Event extraction (EE) aims to extract different types of events, each of which includes a trigger and several participants (arguments) with specific roles, from the given passage.","Modify,Clarity",Clarity
518,125-ARR,125-ARR_v2_44@1,125-ARR_v1_39@1,"We generate different proportions (1%, 2%, 3%, 5%, 10%, 20%, 30%, and 50%) of training data to study the influence of the size of the training set and use the original development set and test set for evaluation.","We generate different proportions (1%, 2%, 3%, 5%, 10%, 20%, 30%, and 50%) of training data to study the influence of the size of training set and use the original dev set and test set for evaluation.","Modify,Clarity",Clarity
519,125-ARR,125-ARR_v2_44@2,125-ARR_v1_39@2,Appendix C lists more details about the split generation process and the data statistics.,Appendix C lists more details about the split generating process and the data statistics.,"Modify,Clarity",Clarity
520,125-ARR,125-ARR_v2_46@6,125-ARR_v1_40@4,Note that the outputs of both generation-based baselines are not natural sentences.,Notice that the outputs of both generation-based baselines are not natural sentences.,"Modify,Clarity",Clarity
521,125-ARR,125-ARR_v2_4@1,125-ARR_v1_4@1,"For example, in Figure 1, a Justice:Execute event is triggered by the word ""execution"" and this event contains three argument roles, including an Agent (Indonesia) who carries out the execution, a Person who is executed (convicts), and a Place where the event occurs (not mentioned in the passage).","For example, in Figure 1, a Justice:Execute event is triggered by the word ""execution"" and this event contains three argument roles, including an Agent (Indonesia) who carries out the execution, a Person been executed (convicts), and a Place where the event occurs (not mentioned in the passage).","Modify,Grammar",Grammar
522,125-ARR,125-ARR_v2_48@0,125-ARR_v1_42@0,Table 2 shows the trigger classification F1-scores and the argument classification F1-scores in three data sets with different proportions of training data.,Table 1 shows the trigger classification F1-scores and the argument classification F1-scores across three datasets with different proportions of training data.,"Modify,Grammar",Grammar
523,125-ARR,125-ARR_v2_49@2,125-ARR_v1_43@2,"For example, when only 1% of the training data is available, DEGREE and DEGREE(PIPE) achieve more than 15 points of improvement in trigger classification F1 scores and more than 5 points in argument classification F1 scores.","For example, when only 1% of training data is available, DEGREE and DEGREE(PIPE) achieve more than 15 points of trigger classification F1scores improvement and more than 5 points of argument classification F1-scores.","Modify,Clarity",Clarity
524,125-ARR,125-ARR_v2_49@4,125-ARR_v1_43@4,"The generation-based model with carefully designed prompts is able to utilize the label semantics and the additional weakly supervised signals, thus helping learning under the low-resource regime.","The generationbased model with carefully designed prompts is able to utilize the label semantics and the additional weakly-supervised signals, thus, helps the learning under the low-resource regime.","Modify,Grammar",Grammar
525,125-ARR,125-ARR_v2_50@0,125-ARR_v1_44@0,Another interesting finding is that DEGREE and DEGREE(PIPE) seem to be more beneficial for predicting arguments than for predicting triggers.,Another interesting finding is that DEGREE and DEGREE(PIPE) seem to be more beneficial to argument prediction than trigger prediction.,"Modify,Clarity",Clarity
526,125-ARR,125-ARR_v2_50@1,125-ARR_v1_44@1,"For example, OneIE, the strongest baseline, requires 20% of training data to achieve competitive performance on trigger prediction to DEGREE and DEGREE(PIPE); however, it requires about 50% of training data to achieve competitive performance in predicting arguments.","For instance, OneIE, the strongest baseline, requires 20% of training data to achieve competitive performance on trigger prediction to DEGREE and DEGREE(PIPE); however, it requires about 50% of training data to achieve competitive performance on argument prediction.","Modify,Clarity",Clarity
527,125-ARR,125-ARR_v2_51@0,125-ARR_v1_45@0,"Furthermore, we observe that DEGREE is slightly better than DEGREE(PIPE) under the lowresource setting.","Finally, we observe that DEGREE is slightly better than DEGREE(PIPE) under the low-resource setting.","Modify,Clarity",Clarity
528,125-ARR,125-ARR_v2_51@1,125-ARR_v1_45@1,This provides empirical evidence on the benefit of jointly predicting triggers and arguments in a low-resource setting.,We hypothesize that DEGREE jointly predicts triggers and arguments and therefore can better take advantage of the output dependencies.,"Modify,Claim",Claim
529,125-ARR,125-ARR_v2_54@0,125-ARR_v1_47@0,"Although we focus on data-efficient learning for low-resource event extraction, to better understand the advantages and disadvantages of our model, we additionally study DEGREE in the high-resource setting for controlled comparisons.","While we focus on data-efficient learning for lowresource event extraction, to better understand the advantages and disadvantages of our model and make sure that it is indeed more data-efficient, rather than simply a stronger model, we additionally study DEGREE in the high-resource setting for controlled comparisons.","Modify,Fact/Evidence",Fact/Evidence
530,125-ARR,125-ARR_v2_56@0,125-ARR_v1_50@0,Ablation Studies,Ablation Study,"Modify,Grammar",Grammar
531,125-ARR,125-ARR_v2_5@0,125-ARR_v1_5@0,"Most prior works on EE rely on a large amount of annotated data for training (Nguyen and Grishman, 2015;Nguyen et al., 2016;Han et al., 2019b;Du and Cardie, 2020;Paolini et al., 2021).","Several previous EE approaches rely on a large amount of annotated data for training (Nguyen and Grishman, 2015;Nguyen et al., 2016;Du and Cardie, 2020;Paolini et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
532,125-ARR,125-ARR_v2_5@1,125-ARR_v1_5@1,"However, high-quality event annotations are expensive to obtain.","However, these high-quality event annotations are expensive to be obtained.","Modify,Clarity",Clarity
533,125-ARR,125-ARR_v2_5@2,125-ARR_v1_5@2,"For example, the ACE 2005 corpus (Doddington et al., 2004), one of the most widely used EE datasets, requires two rounds of annotations by linguistics experts.","For example, the ACE 2005 corpus (Doddington et al., 2004), one of the most common EE datasets, requires two rounds of annotations by linguistics experts.","Modify,Clarity",Clarity
534,125-ARR,125-ARR_v2_65@2,125-ARR_v1_59@6,Their predicted target-augmented language embed labels into the input passage via using brackets and vertical bar symbols.,"Their predicted targetaugmented language embed labels into the input passage via using brackets and vertical bar symbols, hindering the model from fully leveraging label semantics.","Modify,Claim",Claim
535,125-ARR,125-ARR_v2_65@6,125-ARR_v1_59@8,"They solve event extraction with a pipeline, which prevents knowledge sharing across subtasks.","Yet, similar to TANL, they solve event extraction with a pipeline, which prevents knowledge sharing across subtasks.","Modify,Fact/Evidence",Fact/Evidence
536,125-ARR,125-ARR_v2_66@1,125-ARR_v1_60@1,It has been a growing interest in event extraction in a scenario with less data.,It has been a rising interest in event extraction under less data scenario.,"Modify,Clarity",Clarity
537,125-ARR,125-ARR_v2_68@0,125-ARR_v1_62@0,Conclusion & Future Work,Conclusion,"Modify,Other",Other
538,125-ARR,125-ARR_v2_5@4,125-ARR_v1_5@4,"Therefore, how to learn a data-efficient EE model trained with only a few annotated examples is a crucial challenge.","Therefore, how to learn a data-efficient EE model trained with only a few annotated examples is a crucial research question.","Modify,Clarity",Clarity
539,125-ARR,125-ARR_v2_6@0,125-ARR_v1_6@0,"In this paper, we focus on low-resource event extraction, where only a small amount of training examples are available for training.","In this paper, we focus on low-resource event extraction, where only a small amount of training examples are available during training.","Modify,Clarity",Clarity
540,125-ARR,125-ARR_v2_2@2,125-ARR_v1_2@1,"In this paper, we focus on low-resource end-to-end event extraction and propose DE-GREE, a data-efficient model that formulates event extraction as a conditional generation problem.","In this paper, we focus on low-resource end-toend event extraction.","Merge+Modify,Grammar",Grammar
541,125-ARR,125-ARR_v2_12@2,125-ARR_v1_6@3,DEGREE enjoys the following advantages to learn well with less training data.,DEGREE enjoys the following three advantages to learn well with less training data.,"Modify,Fact/Evidence",Fact/Evidence
542,125-ARR,125-ARR_v2_95@6,125-ARR_v1_83@4,"We consider the following baselines: (1) BERT_QA (Du and Cardie, 2020) (2) OneIE (3) Matching baseline, a proposed baseline that makes trigger predictions by performing string matching between the input passage and the event keywords.","We consider the following baselines: (1) BERT_QA (Du and Cardie, 2020) (2) OneIE (Lin et al., 2020) (3) Matching baseline, a proposed baseline that makes trigger predictions by performing string matching between the input passage and the event keywords.","Modify,Fact/Evidence",Fact/Evidence
543,125-ARR,125-ARR_v2_70@0,125-ARR_v1_87@1,"DEGREE assumes that some weakly-supervised information (the description of events, similar keywords, and human-written templates) is accessible or not expensive for the users to craft.","DEGREE assumes that some weakly-supervised information (the description of events, similar keywords, and human-written templates) is accessible and not expensive.","Modify,Claim",Claim
544,125-ARR,125-ARR_v2_71@0,125-ARR_v1_88@1,"DEGREE fine-tunes the pre-trained generative language model (Lewis et al., 2020).","DEGREE fine-tunes the pretrained generative language (Lewis et al., 2020).","Modify,Clarity",Clarity
545,125-ARR,125-ARR_v2_10@2,125-ARR_v1_10@2,"The input of DEGREE consists of the given passage and our design prompt that contains an event type description, event keywords, and a E2E template.","The input of DEGREE consists of the given passage and our design prompt that contains a event type description, some event keywords, and a E2E template.","Modify,Grammar",Grammar
546,125-ARR,125-ARR_v2_2@2,125-ARR_v1_2@2,"In this paper, we focus on low-resource end-to-end event extraction and propose DE-GREE, a data-efficient model that formulates event extraction as a conditional generation problem.","We propose DEGREE, a model that formulates event extraction as a conditional generation problem.","Merge+Modify,Claim",Claim
547,125-ARR,125-ARR_v2_12@5,125-ARR_v1_12@2,"In addition, the sentence structure of the template and the word ""attacked"" depict the semantic relation between the role Attacker and the role Target.","Also, the word ""attacked"" in the prompt depicts the relationship between the role Attacker and the role Target.","Modify,Claim",Claim
548,125-ARR,125-ARR_v2_12@6,125-ARR_v1_12@3,"With these kinds of guidance, DEGREE can make more accurate predictions with less training examples.","With these kinds of guidance, DEGREE can make accurate predictions without many training examples.","Modify,Clarity",Clarity
549,125-ARR,125-ARR_v2_12@7,125-ARR_v1_12@4,"Second, the prompts can incorporate additional weaksupervision signal about the task, such as the description of the event and similar keywords.","Second, the prompts can be further extended to include additional weakly-supervised information about the task, such as the description of the event and similar keywords.","Modify,Clarity",Clarity
550,125-ARR,125-ARR_v2_2@3,125-ARR_v1_2@3,"Given a passage and a manually designed prompt, DEGREE learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern.","Given a passage and a manually designed prompt, DEGREE learns to summarize the event happening in the passage into a natural sentence that follows a predefined pattern.","Modify,Clarity",Clarity
551,125-ARR,125-ARR_v2_12@10,125-ARR_v1_12@5,This information facilitates DEGREE to learn under a low-resource situation.,1 This information facilitates DEGREE to learn under the low-resource situation.,"Modify,Grammar",Grammar
552,125-ARR,125-ARR_v2_12@12,125-ARR_v1_12@7,Leveraging the shared knowledge and dependencies between the two tasks makes our model more data-efficient.,Utilizing the shared knowledge and dependencies between the two tasks makes DEGREE more dataefficient.,"Modify,Clarity",Clarity
553,125-ARR,125-ARR_v2_13@0,125-ARR_v1_13@0,Existing works on EE usually have only one or two of above-mentioned advantages.,Prior approaches on EE usually have only one or two above-mentioned advantages.,"Modify,Clarity",Clarity
554,125-ARR,125-ARR_v2_13@3,125-ARR_v1_13@3,"As a result, our model DEGREE can achieve significantly better performance than prior approaches on low-resource event extraction, as we will demonstrate in Section 3.","As a result, DEGREE can achieve significantly better performance than prior approaches on low-resource event extraction, as we will demonstrate in Section 3.","Modify,Clarity",Clarity
555,125-ARR,125-ARR_v2_2@4,125-ARR_v1_2@4,The final event predictions are then extracted from the generated sentence with a deterministic algorithm.,The final event structure predictions are then extracted from the generated sentence with a deterministic algorithm.,"Modify,Clarity",Clarity
556,125-ARR,125-ARR_v2_18@1,125-ARR_v1_16@1,"Unlike previous works , which separate event extraction into two pipelined tasks (event detection and event argument extraction), DEGREE is designed for the end-to-end event extraction and predict event triggers and arguments at the same time.","Unlike previous works (Wadden et al., 2019;Lin et al., 2020), which separate event extraction into two pipelined tasks (event detection and event argument extraction), DEGREE is designed for the end-to-end event extraction and makes trigger predictions and argument predictions at the same time.","Modify,Fact/Evidence",Fact/Evidence
557,125-ARR,125-ARR_v2_19@0,125-ARR_v1_17@0,The DEGREE Model,DEGREE,"Modify,Other",Other
558,125-ARR,125-ARR_v2_20@4,125-ARR_v1_18@4,"By designing appropriate prompts, we encourage DEGREE to better capture the dependencies between entities and, therefore, to reduce the number of training examples needed.","By designing appropriate prompts, we encourage DEGREE to better capture the dependencies between entities and therefore reduce the number of needed training examples.","Modify,Grammar",Grammar
559,125-ARR,125-ARR_v2_2@5,125-ARR_v1_2@5,DEGREE has three advantages to learn well with less training data.,DEGREE has the following advantages to learn well with less training data.,"Modify,Fact/Evidence",Fact/Evidence
560,125-ARR,125-ARR_v2_2@6,125-ARR_v1_2@6,"First, our designed prompts provide semantic guidance for DEGREE to leverage label semantics and thus better capture the event arguments.","First, with our design of prompts, DEGREE obtains semantic guidance by leveraging label semantics and thus better captures the argument roles.","Modify,Clarity",Clarity
642,13-ARR,,13-ARR_v1_19@6,,We denote the O mask as the mask for an object 1.,"Delete,Fact/Evidence",Fact/Evidence
643,13-ARR,,13-ARR_v1_20@0,,"We first sample the trajectories in the Matterport (Chang et al., 2017) Environment.","Delete,Fact/Evidence",Fact/Evidence
644,13-ARR,,13-ARR_v1_20@1,,"We randomly sample the starting and ending positions, and collect tracks with lengths of less than 8 hops.","Delete,Fact/Evidence",Fact/Evidence
645,13-ARR,,13-ARR_v1_20@2,,Then we obtain the corresponding actions of each trajectory by firstperson movement.,"Delete,Fact/Evidence",Fact/Evidence
646,13-ARR,,13-ARR_v1_20@3,,"If the agent chooses the front navigable position to move, we generate a 'forward' action.","Delete,Fact/Evidence",Fact/Evidence
647,13-ARR,,13-ARR_v1_20@4,,"If the agent chooses the back navigable position to move, we generate an 'around' action.","Delete,Fact/Evidence",Fact/Evidence
648,13-ARR,,13-ARR_v1_20@5,,"Otherwise, if the agent selects the right front navigable position to move for the next step, we generate an action sequence like {'right', 'forward'}, which is used to fill actionable verbs during instruction generation.","Delete,Fact/Evidence",Fact/Evidence
649,13-ARR,,13-ARR_v1_21@1,,"ProbES introduces CLIP, a powerful vision-language alignment model learned from a large-scale image-caption dataset.","Delete,Fact/Evidence",Fact/Evidence
650,13-ARR,,13-ARR_v1_22@0,,"To generate structured augmentation data, we fullfill the templates with phrases that describe the sampled trajectory and actions.","Delete,Fact/Evidence",Fact/Evidence
651,13-ARR,,13-ARR_v1_22@1,,"A trajectory is denoted as {v 1 , v 2 , ..., v n }, where v i represents an observation viewpoint.","Delete,Fact/Evidence",Fact/Evidence
652,13-ARR,13-ARR_v2_25@5,,"For the vision stream, since the trajectory is represented as a sequence of panoramic image regions, which is different from VLMs pretrained on image-caption pairs, we also update the visual embedding during prompt tuning.",,"Add,Fact/Evidence",Fact/Evidence
653,13-ARR,13-ARR_v2_25@6,,The visual embedding contains image embedding and location embedding.,,"Add,Fact/Evidence",Fact/Evidence
654,13-ARR,13-ARR_v2_33@1,,"Rec indicates using Recurrent VLN-Bert (Hong et al., 2021) with different backbones or parameter initialization.",,"Add,Fact/Evidence",Fact/Evidence
655,13-ARR,13-ARR_v2_38@2,,"Our model outperforms the model fine-tuned on R2R dataset by 1.1% in unseen split, indicating that ProbES improves the generalization ability of the navigation model.",,"Add,Claim",Claim
656,13-ARR,13-ARR_v2_38@4,,"Table 6 introduces comprehensive ablation experiments showing the impact of key steps in the strategy of generating instructions, and the experiments are performed in the baseline model: IL+RL from En-vDrop .",,"Add,Fact/Evidence",Fact/Evidence
657,13-ARR,13-ARR_v2_38@5,,Class indicates classes we use to feed into CLIP.,,"Add,Fact/Evidence",Fact/Evidence
658,13-ARR,13-ARR_v2_38@6,,M and P/O represent classes from Matterport and Place365/Objects365 datasets respectively.,,"Add,Fact/Evidence",Fact/Evidence
659,13-ARR,13-ARR_v2_38@7,,G T emplate denotes the strategy used to generate templates. 'ours' denote the strategy shown in Sec 3.2.,,"Add,Fact/Evidence",Fact/Evidence
660,13-ARR,13-ARR_v2_38@8,,"For S T emplate , 'random' and 'match' indicate sampling a template randomly and choosing a template with the same number of masks as the number of viewpoints.",,"Add,Fact/Evidence",Fact/Evidence
661,13-ARR,13-ARR_v2_2@5,13-ARR_v1_2@5,"Unlike the conventional approach of fine-tuning, we introduce prompt-based learning to achieve fast adaptation for language embeddings, which substantially improves the learning efficiency by leveraging prior knowledge.","Unlike the conventional approach of fine-tuning, we introduce prompt tuning to achieve fast adaptation for language embeddings, which substantially improves the learning efficiency by leveraging prior knowledge.","Modify,Clarity",Clarity
662,13-ARR,13-ARR_v2_2@6,13-ARR_v1_2@6,"By automatically synthesizing trajectoryinstruction pairs in any environment without human supervision and efficient prompt-based learning, our model can adapt to diverse visionlanguage navigation tasks, including VLN and REVERIE.","By automatically synthesizing trajectoryinstruction pairs in any environment without human supervision and instruction prompt tuning, our model can adapt to diverse visionlanguage navigation tasks, including VLN and REVERIE.","Modify,Claim",Claim
663,13-ARR,13-ARR_v2_2@7,13-ARR_v1_2@7,Both qualitative and quantitative results show that our ProbES significantly improves the generalization ability of the navigation model * .,Both qualitative and quantitative results show that our ProbES significantly improves the generalization ability of the navigation model.,"Modify,Grammar",Grammar
664,13-ARR,13-ARR_v2_25@4,13-ARR_v1_26@4,"Then in the prompt tuning process, we only train E p and fix the parameters of E x for the language stream.","Then in the prompt tuning process, we only train E p and fix the parameters of E x .","Modify,Fact/Evidence",Fact/Evidence
665,13-ARR,13-ARR_v2_26@0,13-ARR_v1_26@5,"We sample hard negative paths based on distance in the environment for an instruction-trajectory pair, and the model is trained to choose the best path among them.","Similar to VLN-Bert (Devlin et al., 2018), we sample 3 hard negative paths using beam search for an instruction-trajectory pair, and the model is trained to choose the best path among them.","Modify,Fact/Evidence",Fact/Evidence
666,13-ARR,13-ARR_v2_30@0,13-ARR_v1_31@0,"We experiment with our proposed ProbES on two downstream tasks: goal-oriented navigation task (R2R (Anderson et al., 2018)), and objectoriented navigation task (REVERIE ).","We experiment with our proposed ProbES on two downstream tasks: goal-oriented navigation task (R2R ), and objectoriented navigation task (REVERIE ).","Modify,Fact/Evidence",Fact/Evidence
667,13-ARR,13-ARR_v2_4@2,13-ARR_v1_4@2,"The vision-language navigation (VLN) task (Anderson et al., 2018) is proposed where an agent is required to navigate in a photo-realistic environment stepby-step following a natural language instruction.",The vision-language navigation (VLN) task is proposed where an agent is required to navigate in a photo-realistic environment stepby-step following a natural language instruction.,"Modify,Fact/Evidence",Fact/Evidence
668,13-ARR,13-ARR_v2_33@0,13-ARR_v1_34@0,"We compare ProbES with previous state-of-the-art methods on the R2R dataset in the generative setting, which predicts actions sequentially, as shown in Table 2.","We compare ProbES with previous state-of-the-art methods on the R2R dataset in the generative setting, as shown in Table 2.","Modify,Fact/Evidence",Fact/Evidence
669,13-ARR,13-ARR_v2_34@0,13-ARR_v1_35@0,"We compare ProbES with VLN-BERT in the discriminative setting, which outputs scores for instruction-trajectory pairs, as in Table 4.",We compare ProbES with VLN-BERT in the discriminative setting as in Table 4.,"Modify,Fact/Evidence",Fact/Evidence
670,13-ARR,13-ARR_v2_4@3,13-ARR_v1_5@0,Recent tasks focus on target objects localization that asks an agent to identify an object in an unseen room.,"To solve a more practical problem, the REVERIE task focuses on target objects localization that asks an agent to identify an object in an unseen room.","Modify,Claim",Claim
671,13-ARR,13-ARR_v2_7@5,13-ARR_v1_7@12,"We evaluate ProbES on R2R (Anderson et al., 2018) and REVERIE datasets by discriminative and generative settings.",We evaluate ProbES on R2R and REVERIE datasets by discriminative and generative settings.,"Modify,Fact/Evidence",Fact/Evidence
672,13-ARR,13-ARR_v2_15@2,13-ARR_v1_12@2,"Inspired by BERT (Devlin et al., 2019), much work has extended it to process visual tokens and pretrain on large-scale image-text pairs for learning generic visio-linguistic representations.","Inspired by BERT (Devlin et al., 2018), much work has extended it to process visual tokens and pretrain on large-scale image-text pairs for learning generic visio-linguistic representations.","Modify,Fact/Evidence",Fact/Evidence
673,133-ARR,,133-ARR_v1_66@2,,We make the source code available upon acceptance of the paper.,"Delete,Fact/Evidence",Fact/Evidence
674,133-ARR,133-ARR_v2_22@0,,Models for Text Categorization,,"Add,Other",Other
675,133-ARR,133-ARR_v2_23@0,,"We formally introduce the three families of models for text categorization, namely the BoW-based, graph-based, and sequence-based models.",,"Add,Fact/Evidence",Fact/Evidence
676,133-ARR,133-ARR_v2_23@1,,"Table 1 summarizes the key properties of the approaches: whether they require a synthetic graph, whether word position is reflected in the model, whether the model can deal with arbitrary length text, and whether the model is capable of inductive learning.",,"Add,Fact/Evidence",Fact/Evidence
677,133-ARR,133-ARR_v2_24@0,,BoW-Based Text Categorization,,"Add,Other",Other
678,133-ARR,133-ARR_v2_25@0,,"Under pure BoW-based text categorization, we denote approaches that are not order-aware and operate only on the multiset of words from the input document.",,"Add,Fact/Evidence",Fact/Evidence
679,133-ARR,133-ARR_v2_72@0,,The focus of this work is text classification.,,"Add,Fact/Evidence",Fact/Evidence
680,133-ARR,133-ARR_v2_72@1,,Potential risks that apply to text classification in general also apply to this work.,,"Add,Claim",Claim
681,133-ARR,133-ARR_v2_72@2,,"Nonetheless, we present alternatives to commonly used pretrained language models, which suffer from various sources of bias due to the large and poorly manageable data used for pretraining (Bender et al., 2021).",,"Add,Fact/Evidence",Fact/Evidence
682,133-ARR,133-ARR_v2_72@3,,"In contrast, the presented alternatives render full control over the training data and, thus, contribute to circumvent the biases otherwise introduced during pretraining.",,"Add,Claim",Claim
683,133-ARR,133-ARR_v2_76@3,,"We further motivate the choice of using wide layers with results from multi-label text classification (Galke et al., 2017), which has shown that a (wide) MLP outperforms all tested classical baselines such as SVMs, k-Nearest Neighbors, and logistic regression.",,"Add,Fact/Evidence",Fact/Evidence
684,133-ARR,133-ARR_v2_76@4,,"Follow-up work (Mai et al., 2018) then found that also CNN and LSTM do not substantially improve over the wide MLP.",,"Add,Fact/Evidence",Fact/Evidence
685,133-ARR,133-ARR_v2_16@9,133-ARR_v1_16@9,"Semantic hyperedges for word-word connections are derived from topic models (Blei et al., 2001).","Semantic hyperedges for word-word connections are derived from topic models (Blei et al., 2003).","Modify,Fact/Evidence",Fact/Evidence
686,133-ARR,133-ARR_v2_2@5,133-ARR_v1_2@5,"Finally, since Transformers need to compute O(L 2 ) attention weights with sequence length L, the MLP models show higher training and inference speeds on datasets with long sequences.","Finally, since Transformers need to compute O(L 2 ) attention weights with L sequence length, the MLP models show higher training and inference speeds on datasets with long sequences.","Modify,Clarity",Clarity
687,133-ARR,133-ARR_v2_18@4,133-ARR_v1_18@3,"Also Text-RCNN (Lai et al., 2015), a model combining recurrence and convolution uses only the 4 major categories in the 20ng dataset.","Also Tex-tRCNN (Lai et al., 2015), a model combining recurrence and convolution uses only the 4 major categories in the 20ng dataset.","Modify,Grammar",Grammar
688,133-ARR,133-ARR_v2_18@5,133-ARR_v1_18@4,The results of Text-RCNN are identical with BLSTM-2DCNN.,The results of Text-RCNN is identical with BLSTM-2DCNN.,"Modify,Grammar",Grammar
689,133-ARR,133-ARR_v2_18@6,133-ARR_v1_18@5,"For the MR dataset, BLSTM-2DCNN provides no information on the specific split of the dataset.","For the MR dataset, BLSTM-2DCNN provides no information on the specific splitting of the dataset.","Modify,Grammar",Grammar
690,133-ARR,133-ARR_v2_19@0,133-ARR_v1_19@0,"Sequence models: Transformers Surprisingly, only few works consider Transformer models for text categorization.","Sequence models: Transformers Surprisingly, only few works consider Transformer models for topical text classification.","Modify,Clarity",Clarity
691,133-ARR,133-ARR_v2_20@3,133-ARR_v1_20@3,"TinyBERT (Jiao et al., 2020) and Mo-bileBERT (Sun et al., 2020) would be similarly suitable alternatives, among others.","TinyBERT (Jiao et al., 2020) and Mo-bileBERT would be similarly suitable alternatives, among others.","Modify,Fact/Evidence",Fact/Evidence
692,133-ARR,133-ARR_v2_4@1,133-ARR_v1_4@1,"Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021;Li et al., 2020;Zhou et al., 2020;Kowsari et al., 2019;Kadhim, 2019).","Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021;Zhou et al., 2020;Kowsari et al., 2019;Kadhim, 2019).","Modify,Fact/Evidence",Fact/Evidence
693,133-ARR,133-ARR_v2_26@0,133-ARR_v1_22@0,"As BoW-based model, we consider a one hidden layer WideMLP (i. e., two layers in total).","As BoW-based models, we consider a one hidden layer WideMLP (i. e., two layers in total).","Modify,Grammar",Grammar
694,133-ARR,133-ARR_v2_26@1,133-ARR_v1_22@1,"We experiment with pure BoW, TF-IDF weighted, and averaged GloVe input representations.","We further experiment with pure BoW, TF-IDF weighted, or averaged GloVe input representations and two hidden layers WideMLP-2.","Split+Modify,Clarity",Clarity
695,133-ARR,133-ARR_v2_26@2,133-ARR_v1_22@1,We also use a two hidden layers WideMLP-2.,"We further experiment with pure BoW, TF-IDF weighted, or averaged GloVe input representations and two hidden layers WideMLP-2.","Split+Modify,Clarity",Clarity
696,133-ARR,133-ARR_v2_26@3,133-ARR_v1_22@2,"We list the numbers for fastText, SWEM, and logistic regression from Ding et al. (2020) in our comparison.","We also list the numbers for fastText, SWEM, and logistic regression from Ding et al. (2020) in our comparison.","Modify,Clarity",Clarity
697,133-ARR,133-ARR_v2_28@1,133-ARR_v1_24@1,"For instance, in TextGCN the graph is set up in two parts: word-word connections are modeled by pointwise mutual information and word-document edges resemble that the word occurs in the document.","For instance, TextGCN the graph is set up in two parts: word-word connections modeled by pointwise mutual information and word-document edges resemble that the word occurs in the document.","Modify,Grammar",Grammar
698,133-ARR,133-ARR_v2_28@6,133-ARR_v1_24@6,A detailed discussion of the connection between TextGCN and MLP is provided in Appendix B.,A detailed discussion of the connection between TextGCN and MLP is provided in the Appendix B.,"Modify,Grammar",Grammar
699,133-ARR,133-ARR_v2_37@2,133-ARR_v1_33@2,"The mean sequence length is 551 words with a standard deviation (SD) of 2,047.","The mean sequence length is 551 words with a standard deviation of 2,047.","Modify,Clarity",Clarity
700,133-ARR,133-ARR_v2_5@1,133-ARR_v1_5@1,"Among them are Deep Averaging Networks (DAN) (Iyyer et al., 2015), a deep Multi-Layer Perceptron (MLP) model with n layers that relies on averaging the BoW, Simple Word Embedding Models (SWEM) (Shen et al., 2018) that explores different pooling strategies for pretrained word embeddings, and fastText (Bojanowski et al., 2017), which uses a linear layer on top of pretrained word embed-dings.","Among them are Deep Averaging Networks (DAN) (Iyyer et al., 2015), a deep Multi-Layer Perceptron (MLP) model with n layers that relies on averaging the BoW, Simple Word Embedding Models (SWEM) (Shen et al., 2018) that explores different pooling strategies for pretrained word embeddings, and fastText , which uses a linear layer on top of pretrained word embed-dings.","Modify,Fact/Evidence",Fact/Evidence
701,133-ARR,133-ARR_v2_43@7,133-ARR_v1_39@7,We use this augmentation strategy to increase the number of training examples by a factor of two (BERT w/ shuf. augm.).,We use this augmentation strategy to increase the number of training examples by a factor of two (BERT w/ shuffle augment).,"Modify,Clarity",Clarity
702,133-ARR,133-ARR_v2_6@1,133-ARR_v1_6@1,"Besides TextGCN, there are follow-up works like HeteGCN (Ragesh et al., 2021), TensorGCN (Liu et al., 2020), and HyperGAT (Ding et al., 2020), which we collectively call graph-based models.","Besides TextGCN, there are follow-up works like HeteGCN (Ragesh et al., 2021), TensorGCN , and HyperGAT (Ding et al., 2020), which we collectively call graph-based models.","Modify,Fact/Evidence",Fact/Evidence
703,133-ARR,133-ARR_v2_61@1,133-ARR_v1_57@1,"In such cases, the performance of graph neural networks is the state of the art (Kipf and Welling, 2017;Velickovic et al., 2018) and are superior to MLPs that use only the node features and not the graph structure (Shchur et al., 2018).","In such cases, the performance of graph neural networks is the state of the art (Kipf and Welling, 2017;Veličković et al., 2018) and are superior to MLPs that use only the node features and not the graph structure (Shchur et al., 2018).","Modify,Grammar",Grammar
704,133-ARR,133-ARR_v2_66@3,133-ARR_v1_62@3,"In computer vision, Tolstikhin et al. ( 2021) and Melas-Kyriazi (2021) proposed attention-free MLP models that are on par with the Vision Transformer .","In computer vision, Tolstikhin et al. (2021 and Melas-Kyriazi (2021) attentionfree MLP models are on par with the Vision Transformer (Dosovitskiy et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
705,133-ARR,133-ARR_v2_76@0,133-ARR_v1_70@0,"Depth vs. Width In text classification, width seems more important than depth.","Depth vs width In text classification, width seems more important than depth.","Modify,Grammar",Grammar
706,133-ARR,133-ARR_v2_78@0,133-ARR_v1_72@0,"In our experiments, we did not observe any improvement with more hidden layers (WideMLP-2), as suggested by Iyyer et al. (2015), but it might be beneficial for other, more challenging datasets.","In our experiments, we did not observe any improvement with more hidden layers (WideMLP-2), as suggested by Iyyer et al. (2015), but it might help for other, more challenging, datasets.","Modify,Clarity",Clarity
707,133-ARR,133-ARR_v2_81@1,133-ARR_v1_74@1,"A single layer Text-GCN is a BoW-MLP, except for the document embedding.","A single layer Text-GCN is a bow MLP, except for the document embedding.","Modify,Grammar",Grammar
708,133-ARR,133-ARR_v2_82@0,133-ARR_v1_75@0,The basic GCN equation H = σ( ÂXW ) reveals that the order of transformation and neighborhood aggregation is irrelevant.,The basic GCN equation reveals that the order of transformation and neighborhood aggregation is equivalent.,"Modify,Claim",Claim
709,133-ARR,133-ARR_v2_82@2,133-ARR_v1_75@2,"Truly new documents, as in inductive learning scenarios, would need a special treatment such as using an all zero embedding vector.",Truly new documents would need a special treatment such as using an all zero embedding vector.,"Modify,Clarity",Clarity
710,133-ARR,133-ARR_v2_83@1,133-ARR_v1_76@1,"On bag-of-words inputs, the first layer W (1) x + b (1) can be replaced by an equivalent embedding layer with weighting (e. g., TF-IDF or length normalization) being applied during aggregation of the embedding vectors.","The first layer can be replaced by an embedding layer such that H = XE, where X is the weighted term-document matrix.","Modify,Fact/Evidence",Fact/Evidence
711,133-ARR,133-ARR_v2_2@1,133-ARR_v1_2@1,We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and Hete-GCN in an inductive text classification setting and is comparable with HyperGAT.,"We show that a simple multi-layer perceptron (MLP) using a ""Bag of Words"" (BoW) outperforms the recent graph-based models TextGCN and Het-eGCN in an inductive text classification setting and is comparable with HyperGAT.","Modify,Clarity",Clarity
712,133-ARR,133-ARR_v2_88@2,133-ARR_v1_81@2,"Only with a second layer, TextGCN considers the embedding of other documents whose words are connected to the present documents' words.","Only with a second layer, TextGCN considers the embedding of other documents whose words are related to the present documents' words.","Modify,Clarity",Clarity
713,133-ARR,133-ARR_v2_12@6,133-ARR_v1_12@6,"We check whether modified versions of the datasets have been used (e. g., fewer classes), to avoid bias and wrongfully giving advantages.","We check whether modified versions of the datasets have been used (e. g., less classes), to avoid bias and wrongfully giving advantages.","Modify,Grammar",Grammar
714,133-ARR,133-ARR_v2_13@4,133-ARR_v1_13@4,"In fastText (Bojanowski et al., 2017;Joulin et al., 2017) a linear layer on top of pretrained embeddings is used for classification.",In fastText a linear layer on top of pretrained embeddings is used for classification.,"Modify,Fact/Evidence",Fact/Evidence
715,133-ARR,133-ARR_v2_16@1,133-ARR_v1_16@1,"Examples of GNN-based methods operating on a word-document co-occurence graph are TextGCN (Yao et al., 2019) and its successor TensorGCN (Liu et al., 2020) as well as Hete-GCN (Ragesh et al., 2021), HyperGAT (Ding et al., 2020), andDADGNN (Liu et al., 2020).","Examples of GNN-based methods operating on a word-document co-occurence graph are TextGCN (Yao et al., 2019) and its successor TensorGCN as well as Hete-GCN (Ragesh et al., 2021), HyperGAT (Ding et al., 2020), and DADGNN .","Modify,Fact/Evidence",Fact/Evidence
893,14-ARR,14-ARR_v2_25@0,,"• Grammatical error (Gram): Erroneous usage of past/current tense and mistakes in misplaced modifiers. • Event mismatch (Event): Stories that are offtopic, which present events that are not relevant to the image stream. • Object mismatch (Obj): Irrelevant nouns that do not appear in the images and are not semantically related.",,"Add,Fact/Evidence",Fact/Evidence
894,14-ARR,14-ARR_v2_69@5,,"• Stretch-VST (Hsu et al., 2021b): a modification of KGStory that produces more sentences in the story while maintaining quality.",,"Add,Fact/Evidence",Fact/Evidence
895,14-ARR,14-ARR_v2_69@6,,Appropriate knowledge added to the story results in a more detailed story.,,"Add,Claim",Claim
896,14-ARR,14-ARR_v2_16@0,14-ARR_v1_18@0,The construction of VHED is shown in Figure 2.,The construction of VHED is shown in Fig. 2.,"Modify,Clarity",Clarity
897,14-ARR,14-ARR_v2_28@2,14-ARR_v1_28@2,"SIMCSE uses contrastive learning with dropout as augmentation, then trained on natural langauge inference datasets to obtain better sentence embeddings from BERT (Devlin et al., 2019).","SIMCSE uses contrastive learning with dropout as augmentation, then trained on natural langauge inference datasets to obtain better sentence embeddings from BERT (Devlin et al., 2018).","Modify,Fact/Evidence",Fact/Evidence
898,14-ARR,14-ARR_v2_29@2,14-ARR_v1_29@2,"We hypothesize that utilizing this feature makes it possible to extract more information, making it easier for the model to learn human judgment.","We hypothesize that thus doing makes it possible to extract more information, making it easier for the model to learn human judgment for story pairs.","Modify,Clarity",Clarity
899,14-ARR,14-ARR_v2_29@3,14-ARR_v1_29@3,"However, due to the small amount of data available, high variance is likely (Mosbach et al., 2020) to occur during inference.","However, due to the small amount of data available, high variance is likely (Mosbach et al., 2021) to occur during inference.","Modify,Fact/Evidence",Fact/Evidence
900,14-ARR,14-ARR_v2_29@4,14-ARR_v1_29@4,"Hence, we used all data from VHED, including human agreement=3 to increase the stability of our model following Mosbach et al. (2020).","Hence, we used all data from VHED, including human agreement=3 to increase the stability of our model following Mosbach et al. (2021).","Modify,Fact/Evidence",Fact/Evidence
901,14-ARR,14-ARR_v2_33@3,14-ARR_v1_33@3,"Given the story pair (x 1 , x 2 ), the automatic metric being assessed predicts the corresponding story quality scores (s 1 , s 2 ) which we compare to the averaged ranks y 1 and y 2 of x 1 and x 1 from human evaluation.","Given the story pair (x 1 , x 2 ), the autometric being assessed predicts the corresponding story quality scores (s 1 , s 2 ) which we compare to the averaged ranks y 1 and y 2 of x 1 and x 1 from human evaluation.","Modify,Clarity",Clarity
902,14-ARR,14-ARR_v2_40@1,14-ARR_v1_39@1,"We also implement the more recent BERT-Score, BLEURT, and UNION as baseline metrics.","We also considered the more recent BERT-Score, BLEURT, and UNION as baseline metrics.","Modify,Clarity",Clarity
903,14-ARR,14-ARR_v2_40@2,14-ARR_v1_39@2,"In addition to the above automatic metrics, we also include a random baseline, denoted as Random in Table 4, to provide a random score for each story as the lower bound.","In addition to the above automatic metrics, we also included a random baseline to provide a random score for each story, shown as Random in Table 4, as the lower bound.","Modify,Clarity",Clarity
904,14-ARR,14-ARR_v2_0@0,14-ARR_v1_0@0,Learning to Rank Visual Stories from Human Ranking Data,Learning to Rank Visual Stories From Human Ranking Data,"Modify,Grammar",Grammar
905,14-ARR,14-ARR_v2_43@1,14-ARR_v1_42@1,"This algorithm only applies when evaluating story pairs containing references, i.e., reference-machine pairs in this paper.","The Reference Absent Algorithm only applies when evaluating story pairs containing references, i.e., reference-machine pairs in this paper.","Modify,Clarity",Clarity
906,14-ARR,14-ARR_v2_47@10,14-ARR_v1_47@10,"We also find that Vrank ranks correctly when machine is better than reference, showing that Vrank yields 26.5% recall when the other metrics have 0 recall without Eq. 3 and ∼18% with Eq. 3.",Another analysis to study ability of Vrank to rank correctly when machine is better than reference shows that Vrank yields 26.5% recall when the other metrics have 0 recall without Eq. 3 and ∼18% with Eq. 3.,"Modify,Clarity",Clarity
907,14-ARR,14-ARR_v2_50@2,14-ARR_v1_49@2,It is crucial for automatic metrics to also recognize errors to judge generated text.,It is crucial for automatic metrics to also recognize such errors to judge generated text.,"Modify,Clarity",Clarity
908,14-ARR,14-ARR_v2_54@1,14-ARR_v1_53@1,"To determine whether Vrank generalizes to textual stories, we selected MANS dataset (Guan et al., 2021), an imagefree storytelling dataset in which the stories are derived from the ROCStories corpus .","To determine whether Vrank generalizes to textual stories, we selected as the benchmark the MANS dataset (Guan et al., 2021), an image-free storytelling dataset in which the stories are derived from the ROCStories corpus.","Modify,Clarity",Clarity
909,14-ARR,14-ARR_v2_54@2,14-ARR_v1_53@2,"MANS includes 200 story prompts, where each prompt includes five model-generated stories and a reference.","This dataset includes 200 story prompts, where each prompt includes five model-generated stories and a reference.","Modify,Clarity",Clarity
910,14-ARR,14-ARR_v2_61@0,14-ARR_v1_60@0,"After applying Vrank to assess five recent VIST models, we present the results in Figure 4: the models are gradually approaching human-level writing, outlining an exciting development of NLG in VIST.","After applying Vrank to assess five recent VIST models, we present the results in Fig. 4: the models are gradually approaching human-level writing, outlining an exciting development of NLG in VIST.","Modify,Clarity",Clarity
911,14-ARR,14-ARR_v2_61@2,14-ARR_v1_60@2,We also show the correlation between different error types in Figure 5.,We also show the correlation between different error types in Fig. 5.,"Modify,Clarity",Clarity
912,14-ARR,14-ARR_v2_61@4,14-ARR_v1_60@4,"Ranking Gap Distribution The ranking gap distribution is shown in Figure 6, in which both the ranking gaps and the number of stories are normalized.","Ranking Gap Distribution The ranking gap distribution is shown in Fig. 6, in which both the ranking gaps and the number of stories are normalized.","Modify,Clarity",Clarity
913,14-ARR,14-ARR_v2_65@2,14-ARR_v1_64@2,"The batch size is set as 32 and the random seed for training can be set as 7,777 for reproduction.",The batch size is set as 32 and the random seed for training can be set as 7777 for reproduction.,"Modify,Grammar",Grammar
914,14-ARR,14-ARR_v2_8@1,14-ARR_v1_9@1,We then re-purposed VHED to create a better metric Vrank for VIST to rank visual stories.,We then re-purposed VHED to create a better metric for VIST named Vrank (VIST Ranker).,"Modify,Fact/Evidence",Fact/Evidence
915,14-ARR,14-ARR_v2_9@7,14-ARR_v1_10@7,"Indeed, 38% of machine-generated stories are better than the references, which suggests that the afore-mentioned assumption may need to be revisited .","Indeed, 38% of machine-generated stories are better than the references, which suggests that the afore-mentioned assumption may need to be revisited (Clark et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
917,14-ARR,14-ARR_v2_9@13,14-ARR_v1_10@12,"Moreover, Vrank can rank machine and human stories decently and is better at detecting story errors.","In conclusion, Vrank excels in the above assessments and able to follow human behaviors in ranking, rank machine and human stories decently and is better at detecting story errors.","Split+Modify,Clarity",Clarity
918,14-ARR,14-ARR_v2_9@14,14-ARR_v1_11@0,"Specifically, we make three major contributions:",The contributions of this paper are threefold:,"Modify,Claim",Claim
919,141-ARR,,141-ARR_v1_7@0,,The correct answer should be Saint Vincent and the Grenadines instead of United States although both entities have co-occurring contexts with Moonhole.,"Delete,Fact/Evidence",Fact/Evidence
920,141-ARR,,141-ARR_v1_73@2,,The proof can be found in the Appendix.,"Delete,Fact/Evidence",Fact/Evidence
921,141-ARR,,141-ARR_v1_73@8,,"Then µ(r m (s, a)) takes the m-th entry of F( hs , ha ).","Delete,Fact/Evidence",Fact/Evidence
922,141-ARR,,141-ARR_v1_79@6,,The number of final clauses to define the query relation is H = 5 and the number of candidate bridging contexts for each hop is set to K = 5.,"Delete,Fact/Evidence",Fact/Evidence
923,141-ARR,,141-ARR_v1_79@7,,"The dimension of predicate embeddings and biGRU layer is 100 and 200, respectively.","Delete,Fact/Evidence",Fact/Evidence
924,141-ARR,,141-ARR_v1_79@8,,"For training, we adopt Adam optimization with learning rate initialized at 0.001.","Delete,Fact/Evidence",Fact/Evidence
925,141-ARR,,141-ARR_v1_79@9,,The batch size is set to 10.,"Delete,Fact/Evidence",Fact/Evidence
926,141-ARR,,141-ARR_v1_79@10,,"For all the experiments, we use the development dataset to evaluate the results because the test data is not publicly available.","Delete,Fact/Evidence",Fact/Evidence
927,141-ARR,,141-ARR_v1_80@0,,Experimental Result,"Delete,Other",Other
928,141-ARR,,141-ARR_v1_81@0,,"Weber et al. ( 2019) only selects four different query relations from WikiHop, namely Publisher, Developer, Country and Record_label, to evaluate their model.","Delete,Fact/Evidence",Fact/Evidence
929,141-ARR,,141-ARR_v1_81@1,,"For fair comparison, we first follow their setting to compare on these specific domains.","Delete,Fact/Evidence",Fact/Evidence
930,141-ARR,,141-ARR_v1_81@2,,"Besides BIDAF (Seo et al., 2017) and FastQA (Weissenborn et al., 2017), we also consider another three representative deep learning baselines : EPAr (Yichen Jiang and Bansal, 2019), HDEG (Tu et al., 2019), DynSAN (Zhuang and Wang, 2019) 2 , and a differentiable reasoning model DrMD adapted from (Dhingra et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
931,141-ARR,,141-ARR_v1_81@3,,HDEG is a graph-based DNN.,"Delete,Fact/Evidence",Fact/Evidence
932,141-ARR,,141-ARR_v1_81@4,,EPAr and DynSAN are memory-based DNNs.,"Delete,Fact/Evidence",Fact/Evidence
933,141-ARR,,141-ARR_v1_81@5,,"DrMD is implemented following (Dhingra et al., 2020), except that we remove pre-defined entities and only consider mention interactions given our settings.","Delete,Fact/Evidence",Fact/Evidence
934,141-ARR,,141-ARR_v1_81@6,,BERT is a baseline model that concatenates query subject (or a candidate entity) with each context in the form of we feed the hidden representations corresponding to the query subject (or candidates) into an attention model to generate a single vector to be fed into a classifier.,"Delete,Fact/Evidence",Fact/Evidence
935,141-ARR,,141-ARR_v1_81@7,,"For all the baselines, we train the models on each query relation separately to test the reasoning capability, same as our setting.","Delete,Fact/Evidence",Fact/Evidence
936,141-ARR,,141-ARR_v1_81@8,,"Table 1 lists the results for MedHop and four query relations from WikiHop according to (Weber et al., 2019).","Delete,Fact/Evidence",Fact/Evidence
937,141-ARR,,141-ARR_v1_81@9,,"Clearly, DILR substantially outperforms all the baselines on MedHop, demonstrating the importance of the reasoning capabilities for interaction-intensive medical dataset.","Delete,Claim",Claim
938,141-ARR,,141-ARR_v1_81@10,,"On the four query relations from WikiHop, we still obtain the best performances.","Delete,Fact/Evidence",Fact/Evidence
939,141-ARR,,141-ARR_v1_81@13,,The results in terms of accuracy are shown in Table 2.,"Delete,Fact/Evidence",Fact/Evidence
940,141-ARR,,141-ARR_v1_81@15,,"As shown in Table 2, there are 38 relations (D1) containing less than 1,000 training examples, 7 relations (D2) with training examples ranging from 1,000 to 4,000 and 2 relations (D3) having more than 4,000 training examples.","Delete,Fact/Evidence",Fact/Evidence
941,141-ARR,,141-ARR_v1_81@21,,The Detailed comparison on each query relation can be found in Appendix.,"Delete,Fact/Evidence",Fact/Evidence
942,141-ARR,141-ARR_v2_7@0,,"In this example, the underlined entities are used to infer the correct answer, i.e., ""country(Moonhole, Saint Vincent and the Grenadines)"", but are not explicitly annotated for relational reasoning.",,"Add,Fact/Evidence",Fact/Evidence
943,141-ARR,141-ARR_v2_26@3,,"Here r denotes a predicate, i.e., a relation between X 0 and X l+1 .",,"Add,Fact/Evidence",Fact/Evidence
944,141-ARR,141-ARR_v2_84@0,,This completes the proof.,,"Add,Claim",Claim
945,141-ARR,141-ARR_v2_21@0,141-ARR_v1_20@0,"The hypothesis H is a logic program consisting of definite clauses b 1 ∧ ... ∧ b N ⇒ h where b 1 , ..., b N and h are logic atoms.","The hypothesis H is a logic program consisting of definite clauses b 1 ∧ ... ∧ b k ⇒ h where b 1 , ..., b k and h are logic atoms.","Modify,Fact/Evidence",Fact/Evidence
946,141-ARR,141-ARR_v2_29@0,141-ARR_v1_29@0,"Overall, DILR simulates a multi-hop reasoning process considering different number of inference steps.","Overall, DILR simulates multi-hop reasoning processes considering different number of inference steps.","Modify,Grammar",Grammar
947,141-ARR,141-ARR_v2_31@0,141-ARR_v1_31@0,"To avoid inevitable errors brought by off-the-shelf NER tools for named entity extraction, we propose to extract relevant information using an attentive reader.","To avoid inevitable errors brought by the NER tools for named entity extraction, we propose to learn to extract relevant information using an attentive reader.","Modify,Clarity",Clarity
948,141-ARR,141-ARR_v2_33@0,141-ARR_v1_33@0,"Given a query subject s with n s tokens, a candidate a with n a tokens, and a context c of length n c , we denote by S ∈ R ns×D , A ∈ R na×D and C ∈ R nc×D their word features after a biGRU layer, respectively.","Given a query subject s with n s tokens, a candidate a with n a tokens, and a context c of length n c , we denote by S ∈ R ns×D , A ∈ R na×D and C ∈ R nc×D as their word features after a biGRU layer, respectively.","Modify,Grammar",Grammar
949,141-ARR,141-ARR_v2_4@1,141-ARR_v1_4@1,"However, when the background knowledge is expressed in natural languages, as shown in the multi-hop reading comprehension problem with triplet-form questions (Welbl et al., 2018), it becomes difficult to conduct complex reasoning because the entities and relations are not explicitly labeled in the documents.","However, when the background knowledge is expressed in natural languages, as shown in the multi-hop reading comprehension problem with triplet-form questions (Welbl et al., 2018), it becomes difficult to conduct complex reasoning.","Modify,Claim",Claim
950,141-ARR,141-ARR_v2_35@1,141-ARR_v1_35@1,We obtain the normalized similarity score α s ij between the i-th token in the subject and the j-th token in the context via a softmax operation on each row of B s .,We obtain the normalized similarity score α s ij between i-th token in the subject and j-th token in the context via a softmax operation on each row of B s .,"Modify,Grammar",Grammar
951,141-ARR,141-ARR_v2_37@2,141-ARR_v1_37@2,"We denote by s = β s S, and a = β a A the feature representation of the query subject and the candidate entity, respectively.","We denote by s = β s S, and a = β a A the query subject and candidate representations, respectively.","Modify,Clarity",Clarity
952,141-ARR,141-ARR_v2_38@0,141-ARR_v1_38@0,"For (l + 1)-hop reasoning (l ≥ 0), it is desired to relocate to intermediate (bridging) entities that are related to the l-hop entities.","For (l + 1)-hop reasoning (l ≥ 0), it is desired to relocate to intermediate (bridging) entities related to the l-hop entities.","Modify,Clarity",Clarity
953,141-ARR,141-ARR_v2_2@0,141-ARR_v1_2@0,Multi-hop reading comprehension requires an ability to reason across multiple documents.,Multi-hop reading comprehension requires the ability to reason across multiple documents.,"Modify,Grammar",Grammar
954,141-ARR,141-ARR_v2_40@1,141-ARR_v1_40@1,We use α l+1 ij to denote a normalized attention score between the i-th and the j-th context tokens after applying a softmax operator over each row of B l+1 .,We use α l+1 ij to denote a normalized attention score between i-th and j-th context tokens after applying a softmax operator over each row of B l+1 .,"Modify,Grammar",Grammar
955,141-ARR,141-ARR_v2_54@0,141-ARR_v1_55@0,The multi-hop reasoner aims to conduct complex reasoning by first generating probable logic clauses and then evaluating each clause by instantiating the variables with relevant contexts obtained from the attentive reader.,The multi-hop reasoner aims to conduct complex reasoning by first generating probable logic clauses and then evaluating each clause by instantiating the variables.,"Modify,Fact/Evidence",Fact/Evidence
956,141-ARR,141-ARR_v2_54@2,141-ARR_v1_55@2,An illustration of the procedure is shown in Figure 1 and is elaborated in the following sub-section.,An illustration of the procedure is shown in Figure 1 and will be elaborated in the next section.,"Modify,Clarity",Clarity
957,141-ARR,141-ARR_v2_54@3,141-ARR_v1_55@3,"The clause evaluation process is then to instantiate variables in each atom with constants such as query subjects, candidate entities or bridging entities.","Then the clause evaluation process will ground each atom with query subjects, candidate entities or bridging entities.","Modify,Fact/Evidence",Fact/Evidence
958,141-ARR,141-ARR_v2_54@4,141-ARR_v1_55@4,"The outputs from the attentive reader, i.e., hs , ha and {h l k }'s (l > 0), can be used as feature representations for these constants to compute the atom scores for clause evaluation and updates.","The outputs from the attentive reader, i.e., hs , ha and {h l k }'s (l > 0), can be regarded as these constant representations to compute the atom scores for clause evaluation and updates.","Modify,Clarity",Clarity
959,141-ARR,141-ARR_v2_59@0,141-ARR_v1_58@0,"The clause generation process is divided into two stages: 1) to generate clauses defining invented predicates using only the existential predicates, and 2) to generate final clauses defining query relation q using only the invented predicates.",The clause generation process is divided into two stages: 1) generate clauses defining invented predicates using only the existential predicates; 2) generate final clauses defining query relation using only the invented predicates.,"Modify,Clarity",Clarity
960,141-ARR,141-ARR_v2_61@1,141-ARR_v1_60@2,"We use sparsemax, a sparse version of softmax (Martins and Astudillo, 2016), to select only a small number of predicates.","We use sparsemax which is a sparse version of softmax (Martins and Astudillo, 2016) to select only a small number of predicates.","Modify,Clarity",Clarity
961,141-ARR,141-ARR_v2_61@2,141-ARR_v1_60@3,"Intuitively, to learn to define a l-hop invented predicate r l m , ( 5) and ( 6) sequentially produce F t (X t , X t+1 ) at each step t ∈ {0, ..., l} to form the clause body by attending over all the existential predicates with attention weight S l t .","Intuitively, to learn to define a l-hop invented predicate r l m , ( 5) and (6) will sequentially produce F t (X t , X t+1 ) at each step t ∈ {0, ..., l} to form the clause body by attending over all the existential predicates with attention weight S l t .","Modify,Grammar",Grammar
962,141-ARR,141-ARR_v2_62@2,141-ARR_v1_62@2,"Given an embedding u q ∈ R D for the target relation q, we use a multi-head attention mechanism to compute a probability distribution s h over all the invented predicates for each head h ∈ {1, ..., H} to produce the h-th final clause:","Given an embedding u q ∈ R D for the target relation q, we use a multi-head attention mechanism to compute a probability distribution s i over all the invented predicates for each head i ∈ {1, ..., H} to produce the i-th final clause:","Modify,Fact/Evidence",Fact/Evidence
963,141-ARR,141-ARR_v2_64@1,141-ARR_v1_64@1,"For example, if s h selects r 0 1 and r 1 2 , the final clause becomes r 0 1 (X, Y ) ∧ r 1 2 (X, Y ) ⇒ q(X, Y ), which involves at most 1 inference step because r 1 2 is a 1-hop invented predicate.","For example, if s i selects r 0 1 and r 1 2 , the final clause becomes r 0 1 (X, Y ) ∧ r 1 2 (X, Y ) ⇒ q(X, Y ), which involves at most 1 inference step (r 12 ).","Modify,Fact/Evidence",Fact/Evidence
964,141-ARR,141-ARR_v2_66@0,141-ARR_v1_66@0,"Instantiation The clauses generated using the attentive memories need to be tested and refined against the given positive and negative examples, known as learning from entailment that tries to maximize the truth probabilities of positive examples and minimize those of negative examples.","Instantiation The clauses generated using the attentive memories will be tested and refined against the given positive and negative examples, known as learning from entailment that tries to maximize the truth probabilities of positive examples and minimize those of negative examples.","Modify,Clarity",Clarity
965,141-ARR,141-ARR_v2_66@1,141-ARR_v1_66@1,"The positive examples correspond to q(s, a) and the negative examples correspond to {q(s, a − )}'s, where s, a and a − refers to the query subject, correct answer and incorrect candidate, respectively.","The positive examples correspond to q(s, a) and the negative examples correspond to {q(s, a j )}'s, where s, a and a j refers to the query subject, correct answer and incorrect candidate, respectively.","Modify,Fact/Evidence",Fact/Evidence
966,141-ARR,141-ARR_v2_66@2,141-ARR_v1_66@2,"To obtain the truth probabilities of these atoms, we first instantiate the variables for each generated clause with constant contexts, e.g., X = s and Y = a (or Y = a − ) in q(X, Y ).","To obtain the truth probabilities of these atoms, we first instantiate the variables for each generated clause, e.g., X = s and Y = a (or Y = a j ) in q(X, Y ).","Modify,Fact/Evidence",Fact/Evidence
967,141-ARR,141-ARR_v2_66@4,141-ARR_v1_66@4,"Specifically, to instantiate each X l , we pick top-K contexts (documents) {c l 1 , ..., c l K } ⊆ C, namely X l = c l k , 1 ≤ k ≤ K with highest probabilities according to p l k computed via (4).","To avoid inaccurate selection, for each X l , we pick K contexts {c l 1 , ..., c l K } with highest probabilities according to p l k in (4).","Modify,Fact/Evidence",Fact/Evidence
968,141-ARR,141-ARR_v2_66@5,141-ARR_v1_66@5,"Neural Logic Operator Given a definite clause b 1 ∧ ... ∧ b N ⇒ h consisting of grounded atoms (e.g., b 1 = r 1 (s, a)), we could obtain the value for its head atom as µ(h) = µ(b 1 ∧ ... ∧ b N ).","Neural Logic Operator Given a definite clause b 1 ∧ ... ∧ b K ⇒ h consisting of grounded atoms (e.g., b 1 = r 1 (s, a)), we could obtain the value for its head atom as µ(h) = µ(b 1 ∧ ... ∧ b k ).","Modify,Fact/Evidence",Fact/Evidence
969,141-ARR,141-ARR_v2_85@1,141-ARR_v1_73@1,"When N = 2, the RHS of the inequality equals to 1/4 • µ n̸ =min , which makes G ∧ closer to µ min when µ n̸ =min is smaller.","When K = 2, the RHS of the inequality equals to 1/4 • µ k̸ =min , which makes G ∧ closer to µ min when µ k̸ =min is smaller.","Modify,Fact/Evidence",Fact/Evidence
970,141-ARR,141-ARR_v2_85@3,141-ARR_v1_73@4,"Moreover, It avoids exponential decay in the output when N > 1.","Moreover, It avoids exponential decay in the output when K > 1.","Modify,Fact/Evidence",Fact/Evidence
971,141-ARR,141-ARR_v2_8@4,141-ARR_v1_8@3,"However, DNNs only implicitly encode relevant contexts and fail to explicitly uncover the underlying relational compositions for complex inference.","However, DNNs only implicitly encode relevant contexts but fail to explicitly uncover the underlying relational compositions for complex inference.","Modify,Clarity",Clarity
972,141-ARR,141-ARR_v2_90@4,141-ARR_v1_75@4,We use a max operator to generate the maximum score over all possible instantiations in Z l to represent the final truth probability of each invented predicate.,We use a max operator to generate the maximum score over all possible bridging entities to represent the final truth probability of each invented predicate.,"Modify,Fact/Evidence",Fact/Evidence
973,141-ARR,141-ARR_v2_92@2,141-ARR_v1_77@1,"Here we organize the dataset according to subject-candidate pairs: (s, a).","Here we organize the dataset according to subject-candidate pairs: (s n , a n ).","Modify,Fact/Evidence",Fact/Evidence
974,141-ARR,141-ARR_v2_8@5,141-ARR_v1_8@4,"For instance, in the above example, DNNs may encode Bequia and Gladys Johnson into 1-hop features, given the fact that both entities co-occur with the query Moonhole.","With the above example, DNNs may encode Bequia and Gladys Johnson into 1-hop features, given both entities co-occur with the query Moonhole.","Modify,Clarity",Clarity
975,141-ARR,141-ARR_v2_92@3,141-ARR_v1_77@2,"We associate the ground-truth label y = 1 with (s, a) if a is the correct answer, otherwise, y = 0.","We associate the ground-truth label y n = 1 with (s n , a n ) if a n is the correct answer, otherwise, y n = 0.","Modify,Fact/Evidence",Fact/Evidence
976,141-ARR,141-ARR_v2_94@5,141-ARR_v1_79@5,"We define M = 10 relations as existential predicates and M l = 5 invented predicates for each hop with (Weber et al., 2019).","We define M = 10 relations as existential predicates and M l = 5 invented predicates for each hop with l = 0, 1, 2.","Modify,Fact/Evidence",Fact/Evidence
977,141-ARR,141-ARR_v2_8@7,141-ARR_v1_8@6,"In contrast, human beings would easily produce the correct answer given the knowledge ""if A is in B and B is part of country C, then A is in country C"" and by examining the relations between each entity pair co-occurred in the context.","However, a human would easily produce the correct answer given the knowledge ""if A is in B and B is part of country C, then A is in country C"" and by examining the relations between each entity pair co-occurred in the context.","Modify,Clarity",Clarity
978,141-ARR,141-ARR_v2_95@0,141-ARR_v1_81@14,"For a more thorough analysis, we take the entire WikiHop dataset and group the query relations in terms of the number of training instances.","For a more thorough analysis, we group the query relations in terms of the number of training instances.","Modify,Fact/Evidence",Fact/Evidence
979,141-ARR,141-ARR_v2_94@6,141-ARR_v1_81@17,"Clearly, DILR gives the best performances across all the baselines, demonstrating the advantage of combining deep attentive learning with logic reasoning.","Our model achieves the best performances over all data groups, demonstrating the advantage of combining deep attentive learning with logic reasoning.","Split+Modify,Clarity",Clarity
980,141-ARR,141-ARR_v2_95@3,141-ARR_v1_81@17,"Clearly, our model achieves the best performances over all data groups.","Our model achieves the best performances over all data groups, demonstrating the advantage of combining deep attentive learning with logic reasoning.","Split+Modify,Clarity",Clarity
981,141-ARR,141-ARR_v2_94@8,141-ARR_v1_81@20,"Even with well-trained contextualized word embeddings (DILR-BERT), our model still brings consistent performance gains.","Even with well-trained contextualized word embeddings, DILR still brings consistent performance gains.","Modify,Fact/Evidence",Fact/Evidence
982,141-ARR,141-ARR_v2_97@3,141-ARR_v1_83@3,"Clearly, ≤ 0 Hop and ≤ 3 Hop produce lower accuracies because ≤ 0 Hop fails to model the bridging entities and ≤ 3 Hop could overfit the model given most of the questions only involve at most 2 reasoning hops.","Clearly, ≤ 0 Hop and ≤ 3 Hop produce lower accuracies due to either missing bridging entities or overfitting with excessive inference steps.","Modify,Fact/Evidence",Fact/Evidence
983,141-ARR,141-ARR_v2_9@1,141-ARR_v1_9@1,"To answer the previous query, ILP could generate a rule as located_in(X, Z) ∧ country(Z, Y ) ⇒ country(X, Y ).","To answer the previous query, ILP could generate this rule: located_in(X, Z)∧ country(Z, Y ) ⇒ country(X, Y ).","Modify,Clarity",Clarity
984,141-ARR,141-ARR_v2_9@6,141-ARR_v1_9@6,"However, their work relies on the degree of precision for pre-extracted NERs and is limited by the number of rule templates.","However, their work relies on the accuracies of pre-extracted NERs and is limited by the number of rule templates.","Modify,Clarity",Clarity
985,141-ARR,141-ARR_v2_10@0,141-ARR_v1_10@0,"To address the aforementioned limitations, we propose a novel end-to-end integration of deep learning and logic reasoning termed Deep Inductive Logic Reasoning (DILR).","To address these limitations, we propose a novel end-to-end combination of deep learning and logic reasoning termed Deep Inductive Logic Reasoning (DILR).","Modify,Clarity",Clarity
986,141-ARR,141-ARR_v2_10@1,141-ARR_v1_10@1,"It consists of two components: 1) a hierarchical attentive reader that filters query-related and candidate-related information from given documents, and 2) a multihop reasoner that conducts inductive logic reasoning by attentively selecting proper predicates to form candidate rules and refines them upon given examples.",It consists of two components: 1) a hierarchical attentive reader that filters query-related and candidate-related information from given documents; 2) a multi-hop reasoner that conducts inductive logic reasoning by attentively selecting proper predicates to form candidate rules and refines them upon given examples.,"Modify,Grammar",Grammar
987,141-ARR,141-ARR_v2_14@1,141-ARR_v1_14@1,"To explicitly incorporate entity connections, De Cao et al. (2019), Ding et al. (2019), Qiu et al. (2019), Tang et al. (2020), Song et al. (2018) and Tu et al. (2019) proposed to build entity graphs and apply Graph Neural Networks for information propagation.","To explicitly incorporate entity connections, De Cao et al. ( 2019), Ding et al. (2019), Qiu et al. (2019), Tang et al. (2020), Song et al. (2018) and Tu et al. (2019) build entity graphs and apply Graph Neural Networks for information propagation.","Modify,Clarity",Clarity
988,141-ARR,141-ARR_v2_14@2,141-ARR_v1_14@2,Kundu et al. (2019) formalized reasoning as a path-finding problem with neural encoding to rank candidate paths.,Kundu et al. (2019) formalizes reasoning as a path-finding problem with neural encoding to rank candidate paths.,"Modify,Grammar",Grammar
989,141-ARR,141-ARR_v2_14@3,141-ARR_v1_14@3,Path modeling was also adopted in using pointer networks.,Path modeling is also adopted in using pointer networks.,"Modify,Grammar",Grammar
990,141-ARR,141-ARR_v2_14@4,141-ARR_v1_14@4,"However, these approaches only focus on local information without the ability to generalize, and some of them rely on off-the-shelf NER tools.","However, these approaches only focus on local information without the ability to generalize, and some of them rely on NER tools.","Modify,Clarity",Clarity
991,141-ARR,141-ARR_v2_14@5,141-ARR_v1_14@5,Dhingra et al. (2020) proposed to convert texts into a virtual knowledge base for retrieval using a pre-constructed entity database.,"Dhingra et al. (2020) converts texts into a virtual knowledge based for retrieval, but requires an entity database.","Modify,Clarity",Clarity
992,141-ARR,141-ARR_v2_18@1,141-ARR_v1_17@1,"Formally, for each RC problem, we are given a set of documents C = {c 1 , ..., c K }, a structured query in the form of a relational triplet (s, q, ?), where s denotes the subject of the relation q, and a list of candidate answers A = {a 1 , ..., a n }.","Formally, for each RC problem, we are given a set of documents D = {D 1 , ..., D n }, a structured query in the form of a relational triplet (s, q, ?) where s denotes the subject of the relation q, and a list of candidate answers A = {a 1 , ..., a m }.","Modify,Fact/Evidence",Fact/Evidence
993,15-ARR,,15-ARR_v1_31@2,,The USA part contains the majority of the arguments as we could reuse an existing dataset.,"Delete,Fact/Evidence",Fact/Evidence
994,15-ARR,15-ARR_v2_4@5,,Categories that tend to conflict are placed on opposite sites.,,"Add,Fact/Evidence",Fact/Evidence
995,15-ARR,15-ARR_v2_4@6,,"Illustration adapted from (Schwartz et al., 2012).",,"Add,Fact/Evidence",Fact/Evidence
996,15-ARR,15-ARR_v2_13@1,,Our consolidated value taxonomy (Section 3) is thus based on these schemes.,,"Add,Fact/Evidence",Fact/Evidence
997,15-ARR,15-ARR_v2_15@1,,We give an overview for completeness.,,"Add,Fact/Evidence",Fact/Evidence
998,15-ARR,15-ARR_v2_21@5,,"Formally, values are connected specifically with the argument's premise.",,"Add,Claim",Claim
999,15-ARR,15-ARR_v2_21@6,,"However, automatic models might still improve when incorporating the textual conclusion as context for the textual premise.",,"Add,Claim",Claim
1000,15-ARR,15-ARR_v2_23@0,,The taxonomy levels are chosen based on usefulness in social science research.,,"Add,Fact/Evidence",Fact/Evidence
1001,15-ARR,15-ARR_v2_23@1,,"The values at Level 1 are intended to be the items in surveys (Schwartz, 1994), which is why we also suggest to use them for dataset annotation.",,"Add,Fact/Evidence",Fact/Evidence
1002,15-ARR,15-ARR_v2_23@2,,"Moreover, Level 1 values can still be classified into being either instrumental or terminal.",,"Add,Claim",Claim
1003,15-ARR,15-ARR_v2_23@3,,"One could, however, create arbitrarily coarse-and fine-grained levels.",,"Add,Claim",Claim
1004,15-ARR,15-ARR_v2_23@5,,"The grouping of values at higher levels allows for classifications at coarser levels of granularity, enabling investigations such as, whether a specific set of arguments focus on persons or society mainly, or whether they imply a rather anxietyfree or a rather anxiety-avoiding background (cf. Figure 1).",,"Add,Claim",Claim
1005,15-ARR,15-ARR_v2_23@6,,"Also, the circular organization of the taxonomy enables the analysis of major ""directions"" in a collection of arguments, which can, for example, be used to study value differences in argumentation datasets of different cultures.",,"Add,Claim",Claim
1006,15-ARR,15-ARR_v2_23@8,,These links allow comparing value distributions identified in regional datasets with survey data.,,"Add,Fact/Evidence",Fact/Evidence
1007,15-ARR,15-ARR_v2_27@4,,"Note that this data is not intended to represent the respective culture, but to train and benchmark classifiers across sources.",,"Add,Fact/Evidence",Fact/Evidence
1008,15-ARR,15-ARR_v2_50@11,,"We found most disagreement arose from the complexity of annotating 54 values at once, with annotators sometimes confusing values despite the descriptions.",,"Add,Fact/Evidence",Fact/Evidence
1009,15-ARR,15-ARR_v2_50@12,,"For follow-up datasets, one could likely reduce such problems by training annotators on the arguments of our dataset with highest disagreement.",,"Add,Claim",Claim
1010,15-ARR,15-ARR_v2_54@4,,By definition this baseline achieves at least as high-and in most cases higher-F 1 -scores than label-wise random guessing according to the label frequency.,,"Add,Fact/Evidence",Fact/Evidence
1011,15-ARR,15-ARR_v2_56@2,,The conclusions were selected so that the different sets contain roughly the specified percentage of arguments.,,"Add,Fact/Evidence",Fact/Evidence
1012,15-ARR,15-ARR_v2_56@3,,"Unfortunately, this process led to different value distributions in the different sets.",,"Add,Fact/Evidence",Fact/Evidence
1013,15-ARR,15-ARR_v2_56@4,,"However, we deemed the conclusion-wise split more important for our experiments, as we want to test whether classifiers generalize to unseen conclusions.",,"Add,Fact/Evidence",Fact/Evidence
1014,15-ARR,15-ARR_v2_57@4,,"The comparably bad performance at higher levels is somewhat surprising, as it indicates that the categories at these higher levels are harder to separate by state-of-the-art language-based approaches.",,"Add,Claim",Claim
1015,15-ARR,15-ARR_v2_57@5,,"Maybe hierarchical classification approaches (e.g., Babbar et al., 2013) can address this comparably weak performance by utilizing signals at each level of the hierarchy simultaneously.",,"Add,Claim",Claim
1016,15-ARR,15-ARR_v2_58@4,,"Moreover, Figure 3 indicates some correlation of value frequency (grey bars) with classifier performance (colored lines).",,"Add,Fact/Evidence",Fact/Evidence
1017,15-ARR,15-ARR_v2_58@5,,One reason for this correlation could be that the dataset is too small for training reliable classifiers on the infrequent values.,,"Add,Claim",Claim
1018,15-ARR,15-ARR_v2_58@6,,"Another reason might be that there is a more developed vocabulary concerning frequent values, making it easier for classifiers to identify these values.",,"Add,Claim",Claim
1019,15-ARR,15-ARR_v2_68@2,,"Clearly expressing values behind arguments could avoid misunderstandings between humans and automated argumentation systems (Kiesel et al., 2021).",,"Add,Fact/Evidence",Fact/Evidence
1020,15-ARR,15-ARR_v2_69@1,,"Combined with Internet archive data, one could even analyse references to values over time.",,"Add,Claim",Claim
1021,15-ARR,15-ARR_v2_69@2,,We thus hope that this work can serve as a first step towards a better understanding of how the public sees and saw human values in everyday (digital) life.,,"Add,Claim",Claim
1022,15-ARR,15-ARR_v2_21@2,15-ARR_v1_20@3,"The term ""behind"" reflects the fact that many arguments do not explicate values; for example, in the argument ""no matter they felt forced to commit it: anyone who commits a crime should be prosecuted"" no value is mentioned literally.","The term ""behind"" reflects the fact that many arguments do not explicate values; e.g., in the argument ""no matter they felt forced to commit it: anyone who commits a crime should be prosecuted"" no value is mentioned literally.","Modify,Clarity",Clarity
1023,15-ARR,15-ARR_v2_21@7,15-ARR_v1_20@6,The task studied in this paper is to draw this connection between arguments and values automatically.,The task at hand is to draw this connection automatically.,"Modify,Clarity",Clarity
1024,15-ARR,15-ARR_v2_22@2,15-ARR_v1_21@2,We also asked the annotators to comment on supposedly missing values (see Section 4).,We also asked the annotators to comment on supposedly missing values (cf. Section 4).,"Modify,Clarity",Clarity
1025,15-ARR,15-ARR_v2_22@3,15-ARR_v1_21@3,"For most of the additional 48 value descriptions that we received (be humane, be fair, be modern, etc.), we identified existing values or value combinations in the taxonomy that subsume them, suggesting to extend the value descriptions rather than adding new values.","For most of the additional 48 value descriptions that we received (be humane, be fair, be modern, etc.) we were able to identify in the proposed taxonomy existing values or value combinations that subsume them, suggesting to extend the value descriptions rather than adding new values.","Modify,Clarity",Clarity
1026,15-ARR,15-ARR_v2_22@4,15-ARR_v1_21@4,Only two of the added values are not directly related to the universal needs that Schwartz (1994) based the value categories on.,Only two of the added values are not directly related to the universal needs where Schwartz (1994) based the value categories on.,"Modify,Grammar",Grammar
1027,15-ARR,15-ARR_v2_22@6,15-ARR_v1_21@6,"We adopt a uniform naming scheme where the value names reflect the distinction of Rokeach (1973) into instrumental (be . . . ) and terminal (have . . . ) values, and are easy to embed in sentences, for example, ""it is good to be creative.""","We adopt a uniform naming scheme where the value names reflect the distinction of Rokeach (1973) into instrumental (be . . . ) and terminal (have . . . ) values, that can be easily embedded in sentences, for example, ""it is good to be creative.""","Modify,Clarity",Clarity
1028,15-ARR,15-ARR_v2_23@7,15-ARR_v1_22@1,"In addition, for the 41 values with a link to the World Values Survey (the WVS column in Table 1, Haerpfer et al., 2020), the corresponding dataset contains information on people's value priorities (i.e., value systems) collected rigorously for 51 territories, with the earliest survey from 1981 and the latest from 2020.","For example, for the 41 values with a link to the World Values Survey (the WVS column in Table 1, Haerpfer et al., 2020), the corresponding dataset contains information on people's value priorities (i.e., value systems) collected rigorously for 51 territories, with the earliest survey from 1981 and the latest from 2020.","Modify,Clarity",Clarity
1029,15-ARR,15-ARR_v2_25@2,15-ARR_v1_29@2,"The dataset, taxonomy description, and annotation interface are available online as Webis-ArgValues-22.","The dataset, a taxonomy description, and the annotation interface are available online.","Modify,Fact/Evidence",Fact/Evidence
1030,15-ARR,15-ARR_v2_27@1,15-ARR_v1_31@1,"Each argument consists of one premise, one conclusion, and a stance attribute indicating whether the premise is in favor of (pro) or against (con) the conclusion.","Each argument consists of one premise, one conclusions, and a stance attribute indicating whether the premise is in favor of (pro) or against (con) the conclusion.","Modify,Grammar",Grammar
1031,15-ARR,15-ARR_v2_27@2,15-ARR_v1_31@3,"As existing argument datasets are almost exclusively from a Western background, we had to collect new suitable arguments for the non-US parts, drastically limiting their size.","However, as existing argument datasets are almost exclusively from a Western background, we had to collect new suitable arguments for the other parts.","Modify,Claim",Claim
1032,15-ARR,15-ARR_v2_4@3,15-ARR_v1_4@3,"Some values tend to conflict and others to align (see Figure 1), which can cause disagreement on the best course forward, but also the support, if not formation, of political parties that promote the respective highly revered values.","Some values tend to conflict and others to align (cf. Figure 1), which can cause disagreement on the best course forward, but also the support, if not formation, of political parties that promote the respective highly revered values.","Modify,Clarity",Clarity
1033,15-ARR,15-ARR_v2_5@0,15-ARR_v1_5@0,"Due to their outlined importance, human values are studied both in the social sciences (Schwartz, 1994) and in formal argumentation (Bench-Capon, 2003) for decades.","Due to their outlined importance, human values are studied both in the social sciences (Schwartz, 1994) and formal argumentation (Bench-Capon, 2003) since decades.","Modify,Grammar",Grammar
1034,15-ARR,15-ARR_v2_50@10,15-ARR_v1_45@1,"Despite the difficulty of the annotation task, the crowdworker annotators reached an average value-wise agreement α of 0.49 (Krippendorff, 2004).","Despite the difficulty of the annotation task, the annotators reached an average value-wise agreement α of 0.49 (Krippendorff, 2004).","Modify,Clarity",Clarity
1035,15-ARR,15-ARR_v2_50@13,15-ARR_v1_45@2,"One step we implemented for quality assurance is that we manually checked the 48 arguments (<1%) to which MACE assigned more than 10 values, reducing their values to the most prevalent 5-7 ones.","Moreover, we manually checked the 48 arguments (<1%) to which MACE assigned more than 10 values, reducing their values to the most prevalent 5-7 ones.","Modify,Fact/Evidence",Fact/Evidence
1036,15-ARR,15-ARR_v2_50@14,15-ARR_v1_45@3,"The right side of Table 1 shows the frequency of each value in each dataset part, revealing that each value occurs at least once.",The right side of Table 1 shows the frequency of each value in each dataset part.,"Modify,Fact/Evidence",Fact/Evidence
1037,15-ARR,15-ARR_v2_56@1,15-ARR_v1_52@1,"The approaches are trained on the arguments from 60 unique conclusions (4240 arguments, ~85%), validated on 4 (277, ~5%), and tested on 7 (503, ~10%).","The approaches are trained on the arguments from 60 unique conclusions (4240 arguments), validated on 4 (277), and tested on 7 (503).","Modify,Fact/Evidence",Fact/Evidence
1038,15-ARR,15-ARR_v2_56@5,15-ARR_v1_52@2,"Only one very rare value, be neat and tidy (0.2% of arguments in USA part), does not occur in the test set.","Only one very rare value, be neat and tidy (0.2% of arguments in USA part), does not occur in this test set and is thus excluded from evaluation.","Split+Modify,Clarity",Clarity
1039,15-ARR,15-ARR_v2_56@6,15-ARR_v1_52@2,We thus exclude this value from evaluation.,"Only one very rare value, be neat and tidy (0.2% of arguments in USA part), does not occur in this test set and is thus excluded from evaluation.","Split+Modify,Clarity",Clarity
1040,15-ARR,15-ARR_v2_57@6,15-ARR_v1_53@2,"Moreover, while a F 1 -score of 0.25 at Level 1 is encouraging for largely out-of-the-box approaches, clearly more work is needed.","While a F 1 -score of 0.25 is encouraging for largely out-of-the-box approaches, clearly more work is needed.","Modify,Clarity",Clarity
1041,15-ARR,15-ARR_v2_57@7,15-ARR_v1_53@3,"Though a recall of 0.19 may be acceptable for applications that not rely on completeness, a precision of 0.40 is clearly too low for practical uses.","Though a recall of 0.19 may be acceptable for applications that not rely on completeness, a precision of 0.40 seems low for practical uses.","Modify,Other",Other
1042,15-ARR,15-ARR_v2_58@7,15-ARR_v1_54@4,The results are distributed alongside the dataset for follow-up analyses.,The complete results are distributed alongside the dataset.,"Modify,Fact/Evidence",Fact/Evidence
1043,15-ARR,15-ARR_v2_60@2,15-ARR_v1_56@2,"However, the 1-Baseline is equally affected by this lack, thus providing for a comparison with the previous setting.","However, the 1-Baseline is equally effected by this lack, thus providing for a comparison with the previous setting.","Modify,Grammar",Grammar
1044,15-ARR,15-ARR_v2_6@0,15-ARR_v1_7@0,"To understand the pragmatics of this argument, a reader has to acknowledge the belief (Point 1 in the definition above) that the ""end state"" (2) of having a comfortable life is desirable in general (3).","To understand the pragmatics of this statement, a reader has to acknowledge the belief (Point 1 in the definition above) that the ""end state"" (2) of having a comfortable life is desirable in general (3).","Modify,Clarity",Clarity
1045,15-ARR,15-ARR_v2_62@0,15-ARR_v1_57@4,"These findings constitute first evidence that using a cross-cultural value taxonomy could result in robust methods for identifying the values behind arguments, even though more data and research seem necessary to get there.",These findings constitute first evidence that using a cross-cultural value taxonomy could result in robust methods for identifying the values behind arguments.,"Modify,Claim",Claim
1046,15-ARR,15-ARR_v2_6@2,15-ARR_v1_7@2,"Within computational linguistics, human values thus provide the context to categorize, compare, and evaluate argumentative statements, creating several possibilities: to inform social science research on values through large-scale datasets; to assess argumentation with respect to scope and strength; to generate or select arguments based on the value system of a target audience; and to identify opposing and shared values on both sides of a controversial topic.","Within computational linguistics, human values thus provide the context to categorize, compare, and evaluate argumentative statements, creating several possibilities: to inform social science research on values through large-scale datasets; to assess argumentations with respect to scope and strength; and to generate or select arguments based on the value system of a target audience.","Modify,Claim",Claim
1047,15-ARR,15-ARR_v2_8@0,15-ARR_v1_9@0,"As a first endeavor on the automatic identification of values in written arguments, this paper makes three contributions: (1) a consolidated multilevel taxonomy of 54 human values taken from four authoritative cross-cultural social science studies (Section 3); (2) a dataset of 5270 arguments from the US (most arguments), Africa, China, and India, each of which manually annotated for all values by three annotators, corresponding to about 850k human judgments (Section 4); and (3) first classification results per taxonomy level, establishing a baseline and revealing promising results both within and across cultures (Section 5).","As a first endeavour on the automatic identification of values in written arguments, this paper makes three contributions: (1) a consolidated multilevel taxonomy of 54 human values taken from four authoritative cross-cultural social science studies (Section 3); (2) a dataset of 5270 arguments from the US (most arguments), Africa, China, and India, each of which manually annotated for all values by three annotators, corresponding to about 850k human judgments (Section 4); and (3) first classification results per taxonomy level, establishing a baseline and revealing promising results both within and across cultures (Section 5).","Modify,Grammar",Grammar
1048,15-ARR,15-ARR_v2_12@2,15-ARR_v1_13@2,"The paper at hand follows these definitions and targets the personal values behind arguments, that is, the values that the arguments, mostly implicitly, resort to.","The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.","Modify,Fact/Evidence",Fact/Evidence
1049,15-ARR,15-ARR_v2_15@0,15-ARR_v1_14@0,"Other schemes, however, pertain to specific purposes, making them less suited for our study.",Several of the value schemes proposed in the literature pertain to specific purposes.,"Modify,Claim",Claim
1050,15-ARR,15-ARR_v2_13@0,15-ARR_v1_15@0,Several proposed value schemes are domainindependent and hence suited to analyze generic argumentation.,Other proposed value schemes are more generic.,"Modify,Claim",Claim
1051,15-ARR,15-ARR_v2_14@0,15-ARR_v1_15@2,"Specifically for cross-cultural analysis, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble.","Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble.","Modify,Grammar",Grammar
1052,15-ARR,15-ARR_v2_14@4,15-ARR_v1_15@6,"However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further in this paper.","However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.","Modify,Grammar",Grammar
1053,15-ARR,15-ARR_v2_17@3,15-ARR_v1_17@3,This paper presents a first step towards the large-scale automatic application of these works as it takes values to argument mining.,This paper present a first step towards the large-scale automatic application of these works as it takes values to argument mining.,"Modify,Grammar",Grammar
1054,15-ARR,15-ARR_v2_18@0,15-ARR_v1_18@0,"Values overlap with idea of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993).","Partly, values overlap with ideas of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem definition, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993).","Modify,Fact/Evidence",Fact/Evidence
1055,15-ARR,15-ARR_v2_18@1,15-ARR_v1_18@1,"In frames, values can define the costs and benefits of options (Entman, 1993), while common value systems are used for evaluation.","In frames, values can define the costs and benefits of options (Entman, 1993) whereas common value systems are used for evaluation.","Modify,Clarity",Clarity
1056,15-ARR,15-ARR_v2_18@2,15-ARR_v1_18@2,"Framing has often been studied computationally for news (Naderi and Hirst, 2015;Chen et al., 2021), but also for political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019).","Frames have been studied computationally for news (Naderi and Hirst, 2015), political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
1057,15-ARR,15-ARR_v2_18@3,15-ARR_v1_18@3,"In the latter, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification.","In argumentation, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification.","Modify,Clarity",Clarity
1058,151-ARR,,151-ARR_v1_20@1,,The input can be formalized as:,"Delete,Fact/Evidence",Fact/Evidence
1059,151-ARR,,151-ARR_v1_35@3,,"Therefore, we propose monotonic regional attention (MRA).","Delete,Claim",Claim
1060,151-ARR,,151-ARR_v1_59@0,,Experimental Settings,"Delete,Other",Other
1061,151-ARR,,151-ARR_v1_60@0,,"Benchmarks Following previous work (Rei et al., 2020;Yuan et al., 2021), we examine the effectiveness of the propose method on WMT 2019 Metrics tasks (Ma et al., 2019).","Delete,Fact/Evidence",Fact/Evidence
1062,151-ARR,,151-ARR_v1_68@5,,"Moreover, we also testify the performance of UniTE-MUP on WMT 2020 QE tasks via finetuning.","Delete,Fact/Evidence",Fact/Evidence
1063,151-ARR,,151-ARR_v1_79@0,,"Considering the English-targeted model, we select Czech (Cz), German (De), Japanese (Ja), Russian (Ru), and Chinese (Zh) as source languages, and English (En) as target.","Delete,Fact/Evidence",Fact/Evidence
1064,151-ARR,,151-ARR_v1_79@1,,"For each translation direction, we collect 1 million samples, finally yielding 5 million examples in total for unified pretraining.","Delete,Fact/Evidence",Fact/Evidence
1065,151-ARR,,151-ARR_v1_79@2,,"As to the multilingual-targeted model, we further collect 1 million synthetic data for each language direction of En-Cz, En-De, En-Ja, En-Ru, and En-Zh.","Delete,Fact/Evidence",Fact/Evidence
1066,151-ARR,,151-ARR_v1_79@3,,"Finally, we construct 10 million examples for the pretraining of the multilingual version by adding the data of the English-targeted model.","Delete,Fact/Evidence",Fact/Evidence
1067,151-ARR,,151-ARR_v1_79@4,,"Note that, for a fair comparison, we filter out all pretraining examples that are involved in benchmarks.","Delete,Fact/Evidence",Fact/Evidence
1068,151-ARR,,151-ARR_v1_80@0,,"After trying several pooling methods which derive sequence-level representations, we use the representations located at the start of sequence as the input of feedforward network (Ranasinghe et al., 2020b).","Delete,Fact/Evidence",Fact/Evidence
1069,151-ARR,,151-ARR_v1_80@1,,"The feedforward network consists of 3 linear transitions, where the dimensionalities of corresponding outputs are 3,072, 1,024, and 1, respectively.","Delete,Fact/Evidence",Fact/Evidence
1070,151-ARR,,151-ARR_v1_80@2,,"Between any two adjacent linear modules in feedforward, hyperbolic tangent function is arranged as activation.","Delete,Fact/Evidence",Fact/Evidence
1071,151-ARR,,151-ARR_v1_82@1,,"As seen, our approach can give better performance than strong QE baselines.","Delete,Claim",Claim
1072,151-ARR,,151-ARR_v1_83@0,,All the models reported in this paper were finetuned on a single Nvidia V100 (32GB) GPU.,"Delete,Fact/Evidence",Fact/Evidence
1073,151-ARR,,151-ARR_v1_83@1,,"Specifically for UniTE-UP and UniTE-MUP, the pretraining is arranged on 4 Nvidia V100 (32GB) GPUs.","Delete,Fact/Evidence",Fact/Evidence
1074,151-ARR,,151-ARR_v1_83@2,,"Our framework is built upon COMET repository (Rei et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
1075,151-ARR,,151-ARR_v1_83@3,,"For the contribution to the research community, we will release both the source code of UniTE framework and the well-trained evaluation models including UniTE-UP and UniTE-MUP checkpoints as described in this paper upon the acceptance.","Delete,Fact/Evidence",Fact/Evidence
1076,151-ARR,151-ARR_v2_42@3,,We prevent specified interactions in SRC+REF training via modifying the attention mask with regional properties.,,"Add,Fact/Evidence",Fact/Evidence
1077,151-ARR,151-ARR_v2_42@4,,"We show the hard (left) and soft design (right, no h → s) in this figure.",,"Add,Fact/Evidence",Fact/Evidence
1078,151-ARR,151-ARR_v2_56@1,,"Following the official report, the Pearson's correlation is used for evaluation.",,"Add,Fact/Evidence",Fact/Evidence
1079,151-ARR,151-ARR_v2_63@0,,High-resource Zero-shot Avg.,,"Add,Other",Other
1080,151-ARR,151-ARR_v2_72@0,,Ranking-based Data Labeling,,"Add,Other",Other
1081,151-ARR,151-ARR_v2_73@0,,"To verify the effectiveness of ranking-based labeling, we collect the results of models applying different pseudo labeling strategies.",,"Add,Fact/Evidence",Fact/Evidence
1082,151-ARR,151-ARR_v2_73@1,,"After deriving the original scores from the well-trained UniTE-MRA checkpoint, we use Z-score and proposed ranking-based normalization methods to label synthetic data.",,"Add,Fact/Evidence",Fact/Evidence
1083,151-ARR,151-ARR_v2_73@2,,"For both methods, we also apply an ensembling strategy to assign training examples with averaged scores deriving from 3 UniTE-MRA checkpoints.",,"Add,Fact/Evidence",Fact/Evidence
1084,151-ARR,151-ARR_v2_73@3,,"Results show that, Z-score normalization reveals a performance drop when applying score ensembling with multiple models.",,"Add,Fact/Evidence",Fact/Evidence
1085,151-ARR,151-ARR_v2_73@4,,"Our proposed ranking-based normalization can boost the UniTE-UP model training, and its ensembling approach can further improve the performance.",,"Add,Claim",Claim
1086,151-ARR,151-ARR_v2_20@0,151-ARR_v1_20@0,"By receiving a data example composing of hypothesis, source, and reference segment, UniTE first modifies it into concatenated sequence following the given setting as REF , SRC , or SRC+REF :","By receiving a data example composing of hypothesis, source, and reference segment, UniTE first modifies it into concatenated sequence following the given setting as REF , SRC , or SRC+REF .","Modify,Grammar",Grammar
1087,151-ARR,151-ARR_v2_26@1,151-ARR_v1_26@1,"According to Ranasinghe et al. (2020b), we use the first output representation as the input of feedforward layer.","According to common practice (Ranasinghe et al., 2020b), we use the first output representation as the input of feedforward layer (see Appendix B).","Modify,Fact/Evidence",Fact/Evidence
1088,151-ARR,151-ARR_v2_27@0,151-ARR_v1_27@0,"Compared to existing methods (Zhang et al., 2020;Rei et al., 2020) which take sentence-level representations for evaluation, the advantages of our architecture design are as follows.","Compared to existing methods (Rei et al., 2020;Zhang et al., 2020) which only take the outputted representations of the topmost layer for evaluation, the advantages of our architecture design are as follows.","Modify,Fact/Evidence",Fact/Evidence
1089,151-ARR,151-ARR_v2_27@1,151-ARR_v1_27@1,"First, our UniTE model can benefit from layer-coordinated semantical interactions inside every one of PLM layers, which is proven effective on capturing diverse linguistic features (He et al., 2018;Lin et al., 2019;Jawahar et al., 2019;Tenney et al., 2019;Rogers et al., 2020).","First, our UniTE model can benefit from layer-coordinated semantical interactions inside every one layer of PLM, which is proven effective on capturing diverse linguistic features (He et al., 2018;Lin et al., 2019;Jawahar et al., 2019;Tenney et al., 2019;Rogers et al., 2020).","Modify,Clarity",Clarity
1090,151-ARR,151-ARR_v2_2@8,151-ARR_v1_2@7,Both source code and associated models are available at https://github.com/NLP2CT/UniTE.,Both source code and associated models will be released upon the acceptance of this paper.,"Modify,Fact/Evidence",Fact/Evidence
1091,151-ARR,151-ARR_v2_32@0,151-ARR_v1_31@0,"For training, we encourage the model to reduce the mean squared error with respect to given score q:","During training, we encourage the model to reduce the mean squared error between model prediction and given score q:","Modify,Clarity",Clarity
1092,151-ARR,151-ARR_v2_35@0,151-ARR_v1_33@0,"However, for the pretraining of most PLMs (e,g., XLM-R, Conneau et al., 2020), the input patterns are designed to receive two segments at most.","However, the input patterns for most multilingual PLMs (e,g., XLM-R, Conneau et al., 2020) are designed to receive two segments at most during pretraining.","Modify,Clarity",Clarity
1093,151-ARR,151-ARR_v2_35@2,151-ARR_v1_33@2,"Moreover, previous study (Takahashi et al., 2020) shows that directly training over SRC+REF by following such design leads to worse performance than REF scenario.","Moreover, previous studies (Takahashi et al., 2020) show that directly training over SRC+REF by following such design leads to slightly worse performance than REF scenarios.","Modify,Grammar",Grammar
1094,151-ARR,151-ARR_v2_4@0,151-ARR_v1_4@0,"Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020;Mathur et al., 2020a;Zhao et al., 2020;Kocmi et al., 2021).","Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020;Mathur et al., 2020;Zhao et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
1096,151-ARR,151-ARR_v2_2@1,151-ARR_v1_2@0,"According to the input format, it is mainly separated into three tasks, i.e., reference-only, source-only and source-reference-combined.","Translation quality evaluation plays a crucial role in machine translation, and is mainly separated into three tasks according to different input formats, i.e., reference-only, sourceonly and source-reference-combined.","Split+Modify,Clarity",Clarity
1097,151-ARR,151-ARR_v2_37@0,151-ARR_v1_35@0,"To fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks.","In order to fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks.","Modify,Clarity",Clarity
1098,151-ARR,151-ARR_v2_37@3,151-ARR_v1_36@0,Considering the conventional attention module:,The conventional attention module can be expressed as:,"Modify,Clarity",Clarity
1099,151-ARR,151-ARR_v2_39@1,151-ARR_v1_39@1,"2 As to monotonic regional attention (MRA), we simply add a mask M to the softmax logits to control attention flows:","2 As to MRA, we simply add a mask M to the softmax logits to control attention flows:","Modify,Clarity",Clarity
1100,151-ARR,151-ARR_v2_43@0,151-ARR_v1_44@0,"To give more fine-grained designs, we propose two approaches for UniTE-MRA, which apply the MRA mechanism into UniTE model (Figure 2):","To give more fine-grained designs, we propose two approaches for UniTE-MRA, which apply the MRA mechanism into UniTE model:","Modify,Fact/Evidence",Fact/Evidence
1101,151-ARR,151-ARR_v2_4@1,151-ARR_v1_4@1,"Based on the input contexts, translation evaluation can be mainly categorized into three classes: 1) reference-only evaluation ( REF ) approaches like BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi-2 (Lo, 2019) and TransQuest (Ranasinghe et al., 2020b), which are also referred as quality estimation (QE).","According to the input contexts, translation evaluation can be mainly categorized into three classes: 1) reference-only evaluation ( REF ) approaches like BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi-2 (Lo, 2019) and TransQuest (Ranasinghe et al., 2020b), which are also referred as quality estimation (QE).","Modify,Clarity",Clarity
1102,151-ARR,151-ARR_v2_45@0,151-ARR_v1_46@0,"Note that, although the processing in source and reference may be affected because their positions are not indexed from the start, related studies on positional embeddings reveal that, PLM can well capture relative positional information (Wang and Chen, 2020), which dispels this concern.","Note here that, although the processing in source and reference segments may be affected because their position embeddings are not indexed from the start, related studies on positional embeddings reveal that PLM models can well capture relative positional information (Wang and Chen, 2020), which dispels this concern.","Modify,Clarity",Clarity
1103,151-ARR,151-ARR_v2_47@0,151-ARR_v1_48@0,"To further bridge the modeling gap between PLM and the joint training of UniTE mentioned in §3.1, we propose a unified pretraining strategy including the following main stages: 1) collecting and downgrading synthetic data; 2) labeling examples with a novel ranking-based strategy; 3) multi-task learning for unified pretraining and finetuning.","To further bridge the modeling gap between PLM and the joint training of UniTE as we mentioned in §3.1, we propose a unified pretraining strategy including the following main stages: 1) collecting and downgrading synthetic data; 2) labeling examples with a novel ranking-based strategy; 3) multi-task learning for unified pretraining and finetuning.","Modify,Clarity",Clarity
1104,151-ARR,151-ARR_v2_48@1,151-ARR_v1_49@1,"To further improve the diversity of synthetic data quality, we follow existing experiences (Sellam et al., 2020a;Wan et al., 2021) to apply the word and span dropping strategy to downgrade a portion of hypotheses.","In order to further improve the diversity of synthetic data quality, we follow Sellam et al. (2020a) to apply the word and span dropping strategy to downgrade a portion of hypotheses.","Modify,Fact/Evidence",Fact/Evidence
1105,151-ARR,151-ARR_v2_49@0,151-ARR_v1_50@0,"Specifically, we first use available approaches to derive the predicted score qi for each item, yielding labeled synthetic quadruple examples formed as D = { h i , s i , r i , qi } N i=1 .","To be concrete, for each data item, we first use existing evaluation approach to give prediction qi for each item, yielding labeled synthetic quadruple examples formed as D = { h i , s i , r i , qi } N i=1 .","Modify,Clarity",Clarity
1106,151-ARR,151-ARR_v2_49@1,151-ARR_v1_50@1,"Then, we tag each example with its rank index qi referring to qi :","After that, we descendingly tag each example with their rank index qi referring to qi :","Modify,Fact/Evidence",Fact/Evidence
1107,151-ARR,151-ARR_v2_53@4,151-ARR_v1_55@4,"For example, different methods may give scores with different distributions.","For example, different existing methods may output scores with different distributions.","Modify,Clarity",Clarity
1108,151-ARR,151-ARR_v2_4@2,151-ARR_v1_4@2,"These methods estimate the quality of the hypothesis based on the source sentence without using references; 3) source-reference-combined evaluation ( SRC+REF ) works like COMET (Rei et al., 2020), where the evaluation exploits information from both source and reference.","These methods estimate the quality of the hypothesis based on the source sentence without using references; 3) source-reference-combined evaluation ( SRC+REF ) works like COMET (Rei et al., 2020), where the model exploits information from both source and reference for the evaluation.","Modify,Clarity",Clarity
1109,151-ARR,151-ARR_v2_54@3,151-ARR_v1_60@1,"For the former, we follow the common practice in COMET 3 (Rei et al., 2020) to collect and preprocess the dataset.","We follow the common practice in COMET 3 (Rei et al., 2020) to collect and preprocess the dataset.","Modify,Clarity",Clarity
1110,151-ARR,151-ARR_v2_56@0,151-ARR_v1_60@4,"For SRC scenario, we further conduct results on WMT 2020 QE task (Specia et al., 2020) referring to Ranasinghe et al. (2020a) for data collection and preprocessing.","For SRC scenario, we further evaluate our method on WMT 2020 QE tasks (Specia et al., 2020), where we follow Ranasinghe et al. (2020a) for data collection and preprocessing.","Modify,Clarity",Clarity
1111,151-ARR,151-ARR_v2_58@0,151-ARR_v1_61@1,"The data is constructed from WMT 2021 News Translation task, where we collect the training sets from five translation tasks.","The data is constructed from WMT 2021 Cz-En, De-En, Ja-En, Ru-En, and Zh-En machine translation training sets.","Split+Modify,Clarity",Clarity
1112,151-ARR,151-ARR_v2_58@1,151-ARR_v1_61@1,"Among those tasks, the target sentences are all in English (En), and the source languages are Czech (Cs), German (De), Japanese (Ja), Russian (Ru), and Chinese (Zh).","The data is constructed from WMT 2021 Cz-En, De-En, Ja-En, Ru-En, and Zh-En machine translation training sets.","Split+Modify,Clarity",Clarity
1113,151-ARR,151-ARR_v2_58@2,151-ARR_v1_61@2,"Specifically, we follow Sellam et al. (2020a) to use TRANSFORMER-base (Vaswani et al., 2017) MT models to generate translation candidates, and use the checkpoints trained via UniTE-MRA approach for synthetic data labeling.","Specifically, we follow Sellam et al. (2020a) to use Transformer-base (Vaswani et al., 2017) model to generate translation candidates, and use the checkpoints trained via UniTE-MRA approach for synthetic data labeling.","Modify,Clarity",Clarity
1114,151-ARR,151-ARR_v2_58@3,151-ARR_v1_61@3,"We pretrain two kinds of models, one is pretrained on English-targeted language directions, and the other is a multilingual version trained using bidirectional data.","We pretrain two kinds of models, one is the English version which is pretrained on English-targeted language directions, the other is a multilingual version trained using bidirectional data.","Modify,Clarity",Clarity
1115,151-ARR,151-ARR_v2_59@1,151-ARR_v1_62@1,"For SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al., 2020), PRISM-src (Thompson and Post, 2020) and multilingual-to-multilingual MTran-sQuest (Ranasinghe et al., 2020b).","As to SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al., 2020), PRISMsrc (Thompson and Post, 2020) and multilingual-tomultilingual version of MTransQuest (Ranasinghe et al., 2020b).","Modify,Clarity",Clarity
1116,151-ARR,151-ARR_v2_59@2,151-ARR_v1_62@2,"For SRC+REF , we use XLM-R+Concat (Takahashi et al., 2020) and COMET (Rei et al., 2020) as strong baselines.","For SRC+REF scenario, we use XLM-R+Concat (Takahashi et al., 2020) and COMET (Rei et al., 2020) as strong baselines.","Modify,Clarity",Clarity
1117,151-ARR,151-ARR_v2_62@1,151-ARR_v1_64@1,"Among all involved baselines, for REF methods, BARTScore (Yuan et al., 2021) performs better than other statistical and model-based metrics.","For REF methods, BARTScore (Yuan et al., 2021) performs better than other statistical and model-based metrics.","Modify,Clarity",Clarity
1118,151-ARR,151-ARR_v2_63@2,151-ARR_v1_64@2,"Further, COMET (Rei et al., 2020) performs better than XLM-R+Concat (Takahashi et al., 2020) on SRC+REF scenario.","MTransQuest (Ranasinghe et al., 2020b) gives dominant performance on SRC scenario, and COMET (Rei et al., 2020) performs better than XLM-R+Concat on SRC+REF scenario.","Modify,Fact/Evidence",Fact/Evidence
1119,151-ARR,151-ARR_v2_63@3,151-ARR_v1_65@0,"As for our methods, we can see that, UniTE-MRA achieves better results on all tasks, demonstrating the effectiveness of monotonic attention flows for cross-lingual interactions.","As for our methods, UniTE-MRA approach achieves better results on all tasks, demonstrating the effectiveness of monotonic attention flow for cross-lingual interactions.","Modify,Clarity",Clarity
1120,151-ARR,151-ARR_v2_63@4,151-ARR_v1_65@1,"Moreover, the proposed model UniTE-UP, which unifies REF , SRC , and SRC+REF learning on both pretraining and finetuning, yields better results on all evaluation settings.","Moreover, our proposed model UniTE-UP, which unifies both pretraining and finetuning, can yield better results following all evaluation settings.","Modify,Fact/Evidence",Fact/Evidence
1121,151-ARR,151-ARR_v2_63@5,151-ARR_v1_65@2,"Most importantly, UniTE-UP is a single model which surpasses all the different state-of-the-art models on three tasks, showing its dominance on both convenience and effectiveness.","Most importantly, UniTE-UP is a single model which surpasses all the different state-of-the-art models on three tasks.","Modify,Claim",Claim
1122,151-ARR,151-ARR_v2_64@1,151-ARR_v1_68@2,"Besides, the UniTE-UP also gives dominant results, revealing an improvement of 0.6, 0.3 and 0.9 averaged Kendall's τ correlation scores, respectively.","Besides, it is encouraging to see that the UniTE-UP can also give dominant results, revealing an improvement of 0.6, 0.3 and 0.9 averaged Kendall's τ correlation scores, respectively.","Modify,Claim",Claim
1123,151-ARR,151-ARR_v2_64@2,151-ARR_v1_68@3,"However, we find that UniTE-MUP outperforms strong baselines but slightly worse than UniTE-UP on English-targeted translation directions (see Table 3).",Our further comparison indicates that UniTE-MUP also outperforms previous strong baselines but slightly worse than UniTE-UP on English-targeted translation directions.,"Modify,Clarity",Clarity
1124,151-ARR,151-ARR_v2_67@1,151-ARR_v1_70@1,All experiments are conducted by following English-targeted setting.,All experiments follow English-targeted setting on SRC+REF task.,"Modify,Fact/Evidence",Fact/Evidence
1125,151-ARR,151-ARR_v2_69@0,151-ARR_v1_72@0,"To investigate the effectiveness of MRA, we further collect experiments in Table 5.","To investigate the effectiveness of MRA, we further collect experiments for comparison.","Modify,Fact/Evidence",Fact/Evidence
1126,151-ARR,151-ARR_v2_69@1,151-ARR_v1_72@1,"As seen, MRA can give performance improvements than full attention, and preventing the interactions between hypothesis and source segment can improve the performance most.","As seen in Table 3, MRA can give performance improvements than full attention, and preventing the interactions between hypothesis and source segment can improve the performance most.","Modify,Fact/Evidence",Fact/Evidence
1127,151-ARR,151-ARR_v2_69@6,151-ARR_v1_73@1,Wang and Chen (2020) found that the positional embeddings in PLM are engaged with strong adjacent information.,As Wang and Chen (2020) found that the positional embeddings in PLM are engaged with strong adjacent information.,"Modify,Clarity",Clarity
1128,151-ARR,151-ARR_v2_71@1,151-ARR_v1_76@1,"As seen, when using the unified pretraining checkpoint to finetune over the specific task, performance over three models reveals performance drop con- sistently, indicating that the unified finetuning is helpful for model learning.","As seen, when using the unified pretraining checkpoint to finetune over the specific task, performance over three models reveals performance drop consistently, indicating the unified finetuning is helpful for model learning, and SRC , REF and SRC+REF tasks are complementary to each other during learning.","Split+Modify,Grammar",Grammar
1129,151-ARR,151-ARR_v2_71@2,151-ARR_v1_76@1,"This also verifies our hypothesis, that the cores of REF , SRC , and SRC+REF tasks are identical to each other.","As seen, when using the unified pretraining checkpoint to finetune over the specific task, performance over three models reveals performance drop consistently, indicating the unified finetuning is helpful for model learning, and SRC , REF and SRC+REF tasks are complementary to each other during learning.","Split+Modify,Claim",Claim
1130,151-ARR,151-ARR_v2_71@3,151-ARR_v1_76@1,"Moreover, unified pretraining and finetuning are complementary to each other.","As seen, when using the unified pretraining checkpoint to finetune over the specific task, performance over three models reveals performance drop consistently, indicating the unified finetuning is helpful for model learning, and SRC , REF and SRC+REF tasks are complementary to each other during learning.","Split+Modify,Clarity",Clarity
1131,151-ARR,151-ARR_v2_5@2,151-ARR_v1_5@2,"We believe that it is valuable, as well as feasible, to unify the capabilities of all MT evaluation tasks ( REF , SRC and SRC+REF ) into one model.","Therefore, we believe that it is valuable to unify the capabilities of all MT evaluation tasks ( REF , SRC and SRC+REF ) into one model.","Modify,Clarity",Clarity
1132,151-ARR,151-ARR_v2_5@4,151-ARR_v1_5@4,"To achieve this, two important challenges need to be addressed: 1) How to design a model framework that can unify all translation evaluation tasks? 2) How to make the powerful PLMs better adapt to the unified evaluation model?","To achieve this idea, the following two important challenges need to be addressed: 1) how to design a model framework that can unify all translation evaluation tasks? 2) Considering the powerful capabilities of the PLM, how to make the PLM better adapt to the unified evaluation model?","Modify,Clarity",Clarity
1133,151-ARR,151-ARR_v2_2@2,151-ARR_v1_2@1,"Recent methods, despite their promising results, are specifically designed and optimized on one of them.","Recent methods, despite their promising results, are specifically designed and optimized on one of these three tasks.","Modify,Clarity",Clarity
1134,151-ARR,151-ARR_v2_6@1,151-ARR_v1_6@1,"To solve the first challenge as mentioned above, based on the multilingual PLM, we utilize layerwise coordination which concatenates all input segments into one sequence as the unified input form.","To solve the first challenge as mentioned above, based on the multilingual PLM (Conneau et al., 2020), we utilize layerwise coordination which concatenates all input segments into one sequence as the unified input form.","Modify,Fact/Evidence",Fact/Evidence
1135,151-ARR,151-ARR_v2_6@3,151-ARR_v1_6@3,"For the second challenge, a multi-task learning-based unified pretraining is proposed.","For the second challenge, a multi-task learning-xbased unified pretraining is proposed.","Modify,Grammar",Grammar
1136,151-ARR,151-ARR_v2_6@6,151-ARR_v1_6@6,"Finally, the multilingual PLM is continuously pretrained on synthetic dataset with multi-task learning manner.","Finally, The multilingual PLM is continuously pretrained on synthetic dataset with multi-task learning.","Modify,Clarity",Clarity
1137,151-ARR,151-ARR_v2_6@7,151-ARR_v1_6@7,"Besides, our proposed models, named UniTE-MRA and UniTE-UP respectively, can benefit from finetuning with human-annotated data over three tasks at once, not requiring extra task-specific training.","Besides, our proposed models, named as UniTE-MRA and UniTE-UP respectively, can benefit from finetuning with human-annotated data over three tasks at once, not requiring extra taskspecific training.","Modify,Grammar",Grammar
1138,151-ARR,151-ARR_v2_2@3,151-ARR_v1_2@2,"This limits the convenience of these methods, and overlooks the commonalities among tasks.",This limits the convenience of these methods and overlooks commonalities among tasks.,"Modify,Grammar",Grammar
1139,151-ARR,151-ARR_v2_7@1,151-ARR_v1_7@1,"Compared to various strong baseline systems on each task, UniTE, which unifies REF , SRC and SRC+REF tasks into one single model, achieves consistently absolute improvements of Kendall's τ correlations at 1.1, 2.3 and 1.1 scores on English-targeted translation directions of WMT 2019 Metric Shared task (Fonseca et al., 2019), respectively.","Compared to different strong baseline systems on each task, UniTE, which unifies REF , SRC and SRC+REF tasks into one single model, achieves consistently absolute improvements of Kendall's τ correlations at 1.1, 2.3 and 1.1 scores on English-targeted translation directions of WMT 2019 Metric Shared task (Fonseca et al., 2019), respectively.","Modify,Clarity",Clarity
1140,151-ARR,151-ARR_v2_11@4,151-ARR_v1_11@4,"However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020;Rei et al., 2020;Mathur et al., 2020a).","However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020;Rei et al., 2020;Mathur et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
1141,151-ARR,151-ARR_v2_12@0,151-ARR_v1_12@0,"Consequently, with the rapid development of PLMs, researchers have been paying their attention to model-based approaches.","Consequently, with the rapid development of PLMs, researchers have been paying their attention in model-based approaches.","Modify,Grammar",Grammar
1142,151-ARR,151-ARR_v2_12@1,151-ARR_v1_12@1,"The basic idea of these studies is to collect sentence representations for similarity calculation (BERTScore, Zhang et al., 2020) or evaluating probabilistic confidence (PRISM-ref, Thompson and Post, 2020;BARTScore, Yuan et al., 2021).","The basic idea of these studies is to collect sentences representations for similarity calculation (BERTScore, Zhang et al., 2020) or evaluating probabilistic confidence (PRISM-ref, Thompson and Post, 2020;BARTScore, Yuan et al., 2021).","Modify,Grammar",Grammar
1143,151-ARR,151-ARR_v2_2@5,151-ARR_v1_2@4,"Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task learning.","Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task training.","Modify,Clarity",Clarity
1144,151-ARR,151-ARR_v2_2@6,151-ARR_v1_2@5,We testify our framework on WMT 2019 Metrics and WMT 2020 Quality Estimation benchmarks.,We empirically testify our framework on WMT 2019 Metrics and WMT 20 Quality Estimation benchmarks.,"Modify,Fact/Evidence",Fact/Evidence
1145,152-ARR,152-ARR_v2_78@2,,Our proposed methods increases corpus size by a slightly larger factor because sentences that contain rare entity types are resampled multiple times.,,"Add,Fact/Evidence",Fact/Evidence
1146,152-ARR,152-ARR_v2_78@3,,"Although increased training corpus size leads to increased training time, note that our methods are especially suitable for scenarios where the annotated corpus is small and hence the training time is still relatively short.",,"Add,Claim",Claim
1147,152-ARR,152-ARR_v2_78@4,,"Each NER model (Shallow, Bi-LSTM, BERT) is associated with a cluster of vectors sharing the same starting point in the space, which represents the performance on the original corpus.",,"Add,Fact/Evidence",Fact/Evidence
1148,152-ARR,152-ARR_v2_82@0,,Note that data augmentation and sentence-level resampling (and resampling methods in general) are complementary methods for improving NER model training.,,"Add,Claim",Claim
1149,152-ARR,152-ARR_v2_82@1,,"Data augmentation improves the semantic richness of training instances by expanding the coverage of training data in the input feature space, while sentence-level resampling refines the importance weighting of training instances by bridging the gap between the training objective and evaluation metrics.",,"Add,Claim",Claim
1150,152-ARR,152-ARR_v2_82@2,,"Therefore, they work in orthogonal directions.",,"Add,Claim",Claim
1151,152-ARR,152-ARR_v2_82@3,,This points to a promising direction for future work: to explore the two line of methods in combination rather than in competition.,,"Add,Claim",Claim
1152,152-ARR,152-ARR_v2_21@4,152-ARR_v1_20@4,"By direct analogy, sentence importance score measures the utility of a sentence with respect to the entities it contains.","By direct analogy, a sentence importance score measures the utility of a sentence respect to the entity tokens it contains.","Modify,Grammar",Grammar
1153,152-ARR,152-ARR_v2_4@0,152-ARR_v1_4@0,"In natural language processing, named entity recognition (NER) is an important task both on its own and for numerous downstream tasks such as entity linking and question answering.","In natural language processing, named entity recognition (NER) is an important task both on its own and supports numerous downstream tasks such as entity linking and question answering.","Modify,Clarity",Clarity
1154,152-ARR,152-ARR_v2_33@1,152-ARR_v1_32@1,By introducing the rareness of an entity type we propose another function called the smoothed resampling incorporating count and rareness (sCR):,By introducing rareness of an entity type we propose another function called smoothed resampling incorporating count and rareness (sCR):,"Modify,Grammar",Grammar
1155,152-ARR,152-ARR_v2_38@0,152-ARR_v1_38@0,We use √ l s instead of l s to slow down the decrease of f sCRD s when a sentence is too long.,We use √ l s instead of l s because to slow down the decrease of f sCRD s when a sentence is too long.,"Modify,Grammar",Grammar
1156,152-ARR,152-ARR_v2_40@0,152-ARR_v1_40@0,"Here, c(t, s) applies a sublinear increasing function on c(t, s) to implement the diminishing marginal utility when a sentence contains many tokens with the same type.","Here, c(t, s) applies a sublinearly increasing function on c(t, s) to implement the diminishing marginal utility when a sentence contains many tokens with the same type.","Modify,Grammar",Grammar
1157,152-ARR,152-ARR_v2_71@3,152-ARR_v1_69@3,"Similar observation was made by previous work (Devlin et al., 2018).","Similar observations was made by previous work (Devlin et al., 2018).","Modify,Grammar",Grammar
1158,152-ARR,152-ARR_v2_72@1,152-ARR_v1_70@1,These benefits become less salient on large corpus (CoNLL).,These benefit becomes less salient on large corpus (CoNLL).,"Modify,Grammar",Grammar
1159,152-ARR,152-ARR_v2_79@0,152-ARR_v1_77@0,Conclusion and Future Work,Conclusion,"Modify,Other",Other
1160,152-ARR,152-ARR_v2_83@0,152-ARR_v1_80@0,Various other avenues exist for future work.,There are multiple avenues for future work.,"Modify,Clarity",Clarity
1161,152-ARR,152-ARR_v2_83@1,152-ARR_v1_80@1,"First, further theoretical and empirical research can explore more effective resampling functions that deliver consistently better performance across corpora and base NER models.","First, further theoretical and empirical research can explore more effective resampling functions that deliver consistently better performance across corpora and base models.","Modify,Clarity",Clarity
1162,152-ARR,152-ARR_v2_83@2,152-ARR_v1_80@2,"Second, more corpora and models can be examined under these resampling strategies to evaluate their generalizability.","Second, more corpora and models can be examined under these resampling strategies.","Modify,Claim",Claim
1163,152-ARR,152-ARR_v2_83@4,152-ARR_v1_80@4,Future research may seek for corpora-level statistics that can assist practitioners in selecting the appropriate resampling methods.,Future research may seek for corpora-level statistics that can assist practitioners in the process of selecting the appropriate resampling method(s).,"Modify,Clarity",Clarity
1164,152-ARR,152-ARR_v2_6@4,152-ARR_v1_5@4,"Data augmentation was shown to be effective by enriching entity-bearing sentences through methods like segment shuffling and mention replacement (Dai and Adel, 2020;Issifu and Ganiz, 2021;Wang and Henao, 2021).","Data augmentation was shown to be effective by enriching entity-bearing sentences through methods like segment shuffling and mention replacement (Dai and Adel, 2020).","Modify,Fact/Evidence",Fact/Evidence
1165,152-ARR,152-ARR_v2_13@0,152-ARR_v1_11@0,"Researchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Garcia, 2009).","Researchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Ma, 2013).","Modify,Fact/Evidence",Fact/Evidence
1166,155-ARR,,155-ARR_v1_85@4,,Table 5: The defense results of different AT methods against two combinatorial optimization attacks.,"Delete,Fact/Evidence",Fact/Evidence
1167,155-ARR,,155-ARR_v1_85@5,,We remove ASR % due to the space limit.,"Delete,Fact/Evidence",Fact/Evidence
1168,155-ARR,,155-ARR_v1_98@3,,We can also conclude that DeBERTa is significantly more robust than RoBERTa.,"Delete,Claim",Claim
1169,155-ARR,155-ARR_v2_48@1,155-ARR_v1_50@1,Take PGD-K as an instance.,Take PGD-K for instance.,"Modify,Grammar",Grammar
1170,155-ARR,155-ARR_v2_4@0,155-ARR_v1_4@0,"Deep neural networks (DNNs) have achieved great success on many natural language processing (NLP) tasks (Kim, 2014;Vaswani et al., 2017;Devlin et al., 2019).","Deep neural networks (DNNs) outperform humans on many natural language processing (NLP) tasks (Kim, 2014;Vaswani et al., 2017;Devlin et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
1171,155-ARR,155-ARR_v2_4@1,155-ARR_v1_4@1,"However, recent studies (Szegedy et al., 2013;Goodfellow et al., 2015) have shown that DNNs are vulnerable to crafted adversarial examples .","However, recent studies have shown that DNNs are vulnerable to crafted adversarial examples (Szegedy et al., 2013;Goodfellow et al., 2014).","Modify,Fact/Evidence",Fact/Evidence
1172,155-ARR,155-ARR_v2_72@0,155-ARR_v1_74@0,"Regarding the training settings and hyperparameters, the optimizer is AdamW (Loshchilov and Hutter, 2019); the learning rate is 2e −5 ; the number of epochs is 10; the batch size is 64 for SST-2 and 24 for IMDb; the maximum sentence length kept for all the models is 40 for SST-2 and 200 for IMDb.","Regarding the training settings and hyperparameters, the optimizer is AdamW (Loshchilov and Hutter, 2019); the learning rate is 2e −5 ; the number of epochs is 10; the batch size is 64 for SST-2 and 24 for IMDb.","Modify,Fact/Evidence",Fact/Evidence
1173,155-ARR,155-ARR_v2_2@4,155-ARR_v1_2@4,"Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate friendly adversarial data.","Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate ""friendly"" adversarial data.","Modify,Grammar",Grammar
1174,155-ARR,155-ARR_v2_21@3,155-ARR_v1_22@3,Goodfellow et al. (2015) proposed fast gradient sign method (FGSM) to obtain δ by one step:,Goodfellow et al. (2014) proposed fast gradient sign method (FGSM) to obtain δ by one step:,"Modify,Fact/Evidence",Fact/Evidence
1175,155-ARR,155-ARR_v2_2@5,155-ARR_v1_2@5,"On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training on friendly adversarial data so that we can save a large number of search steps.","On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training (e.g., FGM) on friendly adversarial data so that we can save a large number of search steps.","Modify,Clarity",Clarity
1176,155-ARR,155-ARR_v2_28@1,155-ARR_v1_30@1,Miyato et al. (2017) find that adversarial and virtual adversarial training have good regularization performance.,Miyato et al. (2016) find that adversarial and virtual adversarial training have good regularization performance.,"Modify,Fact/Evidence",Fact/Evidence
1177,155-ARR,155-ARR_v2_30@0,155-ARR_v1_32@0,"Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2018;Li et al., 2019;Ren et al., 2019;Jin et al., 2019;Zang et al., 2020;Li et al., 2020;Garg and Ramakrishnan, 2020;Si et al., 2021).","Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017;Li et al., 2018;Ren et al., 2019;Jin et al., 2019;Zang et al., 2020;Li et al., 2020;Garg and Ramakrishnan, 2020;Si et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
1178,155-ARR,155-ARR_v2_2@6,155-ARR_v1_2@6,Comprehensive experiments across two widely used datasets and three pretrained language models demonstrate that GAT can obtain stronger robustness via fewer steps.,Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via less steps.,"Modify,Grammar",Grammar
1179,156-ARR,,156-ARR_v1_96@0,,"As the results shown in Table 4, the combination of TurnAPE and RoleAPE achieve the best performance.","Delete,Fact/Evidence",Fact/Evidence
1180,156-ARR,,156-ARR_v1_96@1,,"Both absolute and relative position embeddings improve model performance, nevertheless, including them at the same time can be harmful.","Delete,Claim",Claim
1181,156-ARR,,156-ARR_v1_108@0,,"In this paper, different ethical restrictions deserve discussion.","Delete,Claim",Claim
1182,156-ARR,,156-ARR_v1_109@0,,All data used in our pre-training are available online and other dialog corpus in this paper are publicly available sources.,"Delete,Fact/Evidence",Fact/Evidence
1183,156-ARR,,156-ARR_v1_109@1,,We strictly followed the platform's policies and rules when crawling data from web platforms.,"Delete,Fact/Evidence",Fact/Evidence
1184,156-ARR,,156-ARR_v1_109@2,,We did not employ any author-specific information in our research.,"Delete,Fact/Evidence",Fact/Evidence
1185,156-ARR,,156-ARR_v1_110@0,,"Our corpus may includes some bias, such as political bias and social bias, and our model might have inherited some forms of these bias.","Delete,Claim",Claim
1186,156-ARR,,156-ARR_v1_110@1,,"In order to limit these bias as much as possible, we filter controversial articles and removed data with offensive information when possible.","Delete,Fact/Evidence",Fact/Evidence
1187,156-ARR,,156-ARR_v1_112@0,,"We demonstrate the responses generated from our model as well as other baseline models in Table 7, 8 and 9, respectively.","Delete,Fact/Evidence",Fact/Evidence
1188,156-ARR,,156-ARR_v1_112@1,,The results in Table 8 and 9 show that our model accurately outputs the knowledge information contained in context although we do not model knowledge explicitly.,"Delete,Fact/Evidence",Fact/Evidence
1189,156-ARR,156-ARR_v2_20@1,,The n-stream self-attention mechanism incorporates n extra self-attention predicting streams besides main stream to predict next n continuous future tokens respectively at each time step.,,"Add,Fact/Evidence",Fact/Evidence
1190,156-ARR,,156-ARR_v1_10@0,,"Our pre-trained models and source code will be released, hoping to facilitate further research progress in dialogue generation.","Delete,Claim",Claim
1191,156-ARR,156-ARR_v2_68@2,,We obtain 215 million 1 training samples (42GB in total) for pre-training.,,"Add,Fact/Evidence",Fact/Evidence
1192,156-ARR,156-ARR_v2_69@0,,"To accelerate the training process and accommodate GPU memory limitations, we adopt two methods.",,"Add,Fact/Evidence",Fact/Evidence
1193,156-ARR,156-ARR_v2_69@1,,"First, we sort the samples according to the length of the context.",,"Add,Fact/Evidence",Fact/Evidence
1194,156-ARR,156-ARR_v2_69@2,,Samples with similar length (i.e. number of tokens in context) are assembled into a batch to minimize the amount of padding.,,"Add,Fact/Evidence",Fact/Evidence
1195,156-ARR,156-ARR_v2_69@3,,"Secondly, due to the uneven distribution of sample lengths, we divide the Reddit corpus into two sub-datasets: Reddit-Short and Reddit-Long according to the length of context and response.",,"Add,Fact/Evidence",Fact/Evidence
1196,156-ARR,156-ARR_v2_109@0,,This paper proposes a new pre-training framework for dialogue response generation called Di-alogVED.,,"Add,Fact/Evidence",Fact/Evidence
1197,156-ARR,156-ARR_v2_109@1,,"The latent variable is incorporated into the sequence-to-sequence framework based on Transformer, and obtains a robust and diverse response generation model through 4 training targets.",,"Add,Fact/Evidence",Fact/Evidence
1198,156-ARR,156-ARR_v2_109@3,,Extensive experiments prove the effectiveness of our model.,,"Add,Claim",Claim
1199,156-ARR,156-ARR_v2_109@4,,Additional human evaluation demonstrates the advantages of our proposed model.,,"Add,Claim",Claim
1200,156-ARR,156-ARR_v2_86@0,156-ARR_v1_83@0,"As shown in Table 2 and Table 3, our model Di-alogVED is very competitive compared to PLATO and other models.","As shown in Table 2 and Table 6 (in Appendix A), our model DialogVED is very competitive compared to PLATO and other models.","Modify,Fact/Evidence",Fact/Evidence
1201,156-ARR,156-ARR_v2_105@1,156-ARR_v1_103@1,"These models introduce discourse-level diversity and are able to generate diverse dialog responses (Serban et al., 2017;Zhao et al., 2017a.","These models introduce discourse-level diversity and are able to generate diverse dialog responses (Serban et al., 2017;Zhao et al., 2017aZhao et al., , 2018.","Modify,Fact/Evidence",Fact/Evidence
1202,156-ARR,156-ARR_v2_87@2,156-ARR_v1_111@2,"However, DialogVED equipped with beam search or greedy search, can still easily beat PLATO even though it has a post-generation ranking component.","However, DialogVED equipped with beam or search, can still easily beat PLATO even though it has a post-generation ranking component.","Modify,Clarity",Clarity
1203,156-ARR,156-ARR_v2_10@0,156-ARR_v1_10@1,"The main contributions of this paper can be summarized as follows: 1) We propose a pretrained dialog model, which incorporates continuous latent variables into the enhanced encoder-decoder pre-training framework; 2) We explore the impact of latent variable sizes, different decoding strategies, and position embeddings for turns and roles in our model; 3) Extensive experiments show that the proposed model achieves the new state-of-theart (SOTA) in multiple downstream tasks, and our model has better performance both on relevance and diversity than previous SOTA in response generation.","The main contributions of this paper can be summarized as follows: 1) We propose a pretrained dialog model, which incorporates continuous latent variables into the enhanced encoder-decoder pre-training framework; We explore the impact of latent variable sizes, different decoding strategies, and position embeddings for turns and roles in our model; Extensive experiments show that the proposed model achieves the new state-of-the-art (SOTA) in multiple downstream tasks, and our model has better performance both on relevance and diversity than previous SOTA in response generation.","Modify,Grammar",Grammar
1204,157-ARR,,157-ARR_v1_30@0,,Figure 2 shows the relevance of different content (columns) for the various stakeholders (the rows) for a 10K filing document.,"Delete,Fact/Evidence",Fact/Evidence
1205,157-ARR,,157-ARR_v1_30@1,,Groups of stakeholders are made that form the personas interested in the different parts of the document.,"Delete,Fact/Evidence",Fact/Evidence
1206,157-ARR,,157-ARR_v1_30@2,,The different columns are also grouped together as to indicate what kind of information those sections will contain.,"Delete,Fact/Evidence",Fact/Evidence
1207,157-ARR,,157-ARR_v1_53@0,,Table 5: Results from the human experiment on using the Default Reading experience with DYNAMICTOC.,"Delete,Other",Other
1208,157-ARR,157-ARR_v2_30@0,,We can map these columns to the aspects we get from the Aspect Detection Module and determine if a particular persona is interested in that paragraph or not.,,"Add,Fact/Evidence",Fact/Evidence
1209,157-ARR,157-ARR_v2_29@0,157-ARR_v1_28@0,"We consulted a domain expert (financial domain; specifically for SEC 10-K filings) to create a matrix of personas, who read such documents, and what kind of information they are interested in.",We consulted a domain expert (financial domain; specifically for SEC 10-K filings) to get an understanding of the parties or personas who read such documents and what kind of information are they generally interested in.,"Modify,Clarity",Clarity
1210,157-ARR,157-ARR_v2_29@1,157-ARR_v1_28@1,Figure 2 lists out the various stakeholders of a general 10-K filing against the different sections of the document each stakeholder is interested in.,We constructed a matrix listing out the various stakeholders of a general 10-K filing against the different sections of the document each stakeholder is interested in.,"Modify,Fact/Evidence",Fact/Evidence
1211,157-ARR,157-ARR_v2_29@2,157-ARR_v1_28@2,"The stakeholders are grouped together to form the personas used in DYNAMICTOC, viz. employees, business partners, investors and lendors, financial bodies and advisory and regulatory firms.","The stakeholders were grouped together to form the personas used in DYNAMICTOC, viz. employees, business partners, investors and lendors, financial bodies and advisory regulatory firms.","Modify,Grammar",Grammar
1212,157-ARR,157-ARR_v2_29@3,157-ARR_v1_28@3,"Similarly, the columns (headings) are grouped together according to similarity to create a mapping of topics of interest for each persona.","Similarly, the columns (headings) were grouped together according to similarity to create a mapping of topics of interest for each persona.","Modify,Grammar",Grammar
1213,157-ARR,157-ARR_v2_30@1,157-ARR_v1_29@0,"For this, the aspects obtained from the unsupervised technique are compared against the simplified column values from the constructed matrix.",The aspects we got from our unsupervised technique were compared against the simplified column values from the constructed matrix.,"Modify,Clarity",Clarity
1214,157-ARR,157-ARR_v2_30@2,157-ARR_v1_29@1,The columns with the greatest similarity (above a threshold) are associated with each persona.,The columns with the greatest similarity (above a threshold) were associated with each persona.,"Modify,Grammar",Grammar
1215,157-ARR,157-ARR_v2_30@3,157-ARR_v1_29@2,"For getting the personas interested in each paragraph, the paragraphs are first tagged for aspect.","For getting the personas interested in each paragraph, the paragraphs were first tagged for aspect.","Modify,Grammar",Grammar
1216,157-ARR,157-ARR_v2_30@4,157-ARR_v1_29@3,"From the resultant vector (which represents the confidence score of the text for each aspect), the combined score for each persona is calculated using the scores of its constituent aspects.","From the resultant vector (which represents the confidence score of the text for each aspect), the combined score for each persona was calculated using the scores of its constituent aspects.","Modify,Grammar",Grammar
1217,157-ARR,157-ARR_v2_31@1,157-ARR_v1_31@1,"Note that for financial documents, we were able to gather domain knowledge and leverage it to obtain the persona space.","The thing to note here is, for financial documents, we were able to get some domain knowledge and leveraged it to obtain the persona space.","Modify,Clarity",Clarity
1218,157-ARR,157-ARR_v2_31@2,157-ARR_v1_31@2,But the proposed technique is generalizable to other domains as well.,But the technique we are proposing is generalizable to other domains as well.,"Modify,Clarity",Clarity
1219,157-ARR,157-ARR_v2_34@5,157-ARR_v1_34@5,We use ELI5 dataset for training the model.,"We use ELI5 (Fan et al., 2019) dataset for training the model.","Modify,Fact/Evidence",Fact/Evidence
1220,157-ARR,157-ARR_v2_55@1,157-ARR_v1_60@1,"Financial documents are high value documents for businesses, and are often long and complex.","Financial documents and contracts are high value documents for business entities, and are often long and complex.","Modify,Claim",Claim
1221,157-ARR,157-ARR_v2_55@2,157-ARR_v1_60@2,The default ToC-based reading experience is quite limited and document consumption can be enhanced using intelligent technologies.,The default ToC-based reading experience is quite limited and there are immense opportunities to enhance the document consumption using intelligent technologies.,"Modify,Clarity",Clarity
1222,157-ARR,157-ARR_v2_55@3,157-ARR_v1_60@3,DY-NAMICTOC is one of the first works to pursue this exciting research direction.,"We believe that DYNAMICTOC is one of the first works to pursue this exciting research direction, and would enable further exploration in the area.","Modify,Claim",Claim
1223,157-ARR,157-ARR_v2_55@4,157-ARR_v1_61@0,DYNAMICTOC would benefit from in-domain learning of aspect keywords and questions.,"For the future direction, DYNAMICTOC would benefit from a better supervised in-domain learning of aspect keywords and questions.","Modify,Clarity",Clarity
1224,157-ARR,157-ARR_v2_55@5,157-ARR_v1_61@1,Evaluation of paragraph segmentation and mapping of personas to the aspects are future directions.,We would also like to work on evaluation of the paragraph segmentation and mapping of personas to the aspects in future.,"Modify,Clarity",Clarity
1225,157-ARR,157-ARR_v2_55@6,157-ARR_v1_61@2,A better understanding of personas would generalize the work to different domains.,A better understanding of personas or entities interested in consuming the document would help to generalize the work to different domains.,"Modify,Claim",Claim
1226,157-ARR,157-ARR_v2_7@2,157-ARR_v1_7@2,"Keyword detection (Liu et al., 2009;Tixier et al., 2016) & topic modeling (Blei et al., 2001) works aim is to describe the document by a few important words or topics for concise representation.","Keyword detection (Liu et al., 2009;Tixier et al., 2016) & topic modeling (Blei et al., 2003) works aim is to describe the document by a few important words or topics for concise representation.","Modify,Fact/Evidence",Fact/Evidence
1227,157-ARR,157-ARR_v2_7@4,157-ARR_v1_7@4,"Another task is compact and informative headline generation from a document (Dorr et al., 2003;Lopyrev, 2015).","Another task is compact and informative headline generation from a document (David and Zajic, 2003;Lopyrev, 2015).","Modify,Fact/Evidence",Fact/Evidence
1228,157-ARR,157-ARR_v2_8@4,157-ARR_v1_8@4,"Early unsupervised systems are dominated by Latent Dirichlet Allocation (LDA)-based topic models (García-Pablos et al., 2018;Shi et al., 2018;Álvarez-López et al., 2016).","Early unsupervised systems are dominated by Latent Dirichlet Allocation (LDA)-based topic models (García-Pablos et al., 2018;Shi et al., 2018;Alvarez-López et al., 2016).","Modify,Grammar",Grammar
2726,25-ARR,,25-ARR_v1_19@2,,"This is despite evidence of adjectival scale labels being problematic in terms of bias resulting from positively and negatively worded items not being true opposites of one another, and items intended to have neutral intensity in fact proving to have specific conceptual meanings.","Delete,Claim",Claim
2727,25-ARR,,25-ARR_v1_46@0,,Comparison with Automatic Evaluation Metrics,"Delete,Other",Other
2728,25-ARR,,25-ARR_v1_48@0,,As can be seen from Tables Tables 6 unfortu-9 Raw average scores for models in the Ice-breaker run are additionally provided in Table 10 in Appendix A.4.,"Delete,Fact/Evidence",Fact/Evidence
2729,25-ARR,25-ARR_v2_42@2,,Models are consistent with Table 3.,,"Add,Fact/Evidence",Fact/Evidence
2730,25-ARR,25-ARR_v2_48@0,,Evaluating with Prescribed Topics,,"Add,Other",Other
2731,25-ARR,25-ARR_v2_49@6,,Raw average scores for models in the Ice-breaker run are additionally provided in Table 11 in Appendix A.4.,,"Add,Fact/Evidence",Fact/Evidence
2732,25-ARR,25-ARR_v2_50@1,,It also uses the brevity penalty to penalize short outputs.,,"Add,Fact/Evidence",Fact/Evidence
2733,25-ARR,25-ARR_v2_51@1,,"It computes the precision and recall using longest common subsequence (LSC) instead of n-gram, and the F1 score of precision and recall is reported as the final score.",,"Add,Fact/Evidence",Fact/Evidence
2734,25-ARR,25-ARR_v2_52@1,,"It computes the unigram precision and recall, and have a different mechanism of choosing the brevity penalty.",,"Add,Fact/Evidence",Fact/Evidence
2735,25-ARR,25-ARR_v2_53@1,,The minimum of precision and recall is reported as the final GLEU score.,,"Add,Fact/Evidence",Fact/Evidence
2736,25-ARR,25-ARR_v2_56@0,,Reference-free Metrics,,"Add,Other",Other
2737,25-ARR,25-ARR_v2_57@0,,The following introduces two reference-free automatic metrics we employed: FED and USR.,,"Add,Fact/Evidence",Fact/Evidence
2738,25-ARR,25-ARR_v2_57@1,,Their scores are computed using the conversations collected in our experiment.,,"Add,Fact/Evidence",Fact/Evidence
2739,25-ARR,25-ARR_v2_59@1,,"It consists of three sub-metrics: USR-MLM is to evaluate the understandability and naturalness, USR-DR(c) and USR-DR(f) are to evaluate the interestingness and consistency.",,"Add,Fact/Evidence",Fact/Evidence
2740,25-ARR,25-ARR_v2_59@2,,The sub-metric scores then produce an overall score through a regression model.,,"Add,Fact/Evidence",Fact/Evidence
2741,25-ARR,25-ARR_v2_60@0,,Correlation between Automatic Metrics and Human Evaluation,,"Add,Other",Other
2742,25-ARR,25-ARR_v2_35@2,25-ARR_v1_35@2,"Table 2 shows subsequent proportions (%) of workers, and the detailed instructions are introduced in Figure 5 in Appendix A.4.",Table 2 shows subsequent proportions (%) of workers.,"Modify,Fact/Evidence",Fact/Evidence
2743,25-ARR,25-ARR_v2_61@0,25-ARR_v1_47@0,"We compute the correlation between commonly applied automatic metrics and our human evaluation methods, including word-overlap-based metrics and reference-free metrics, as shown in Tables 5 and 6 respectively.","In this section, we compute the correlation between commonly applied automatic metrics and our human evaluation methods, including word-overlapbased metrics and reference-free metrics, as shown in Tables 6 and 5 respectively.","Modify,Clarity",Clarity
2744,25-ARR,25-ARR_v2_8@0,25-ARR_v1_8@0,"In terms of the live evaluation, competitions such as ConvAI2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.","In terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.","Modify,Grammar",Grammar
2745,25-ARR,25-ARR_v2_17@0,25-ARR_v1_17@0,"A continuous (0-100) rating scale is employed with three main motivation points (Graham et al., 2013;Novikova et al., 2018;Li et al., 2019;Santhanam and Shaikh, 2019;Barrault et al., 2020;Howcroft et al., 2020).","A continuous (0-100) rating scale is employed with three main motivation points (Graham et al., 2013;Mille et al., 2020;Barrault et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
3457,334-ARR,,334-ARR_v1_51@9,,"This provides interesting future direction, where legal knowledge is incorporated into the prediction model.","Delete,Claim",Claim
3458,334-ARR,334-ARR_v2_54@9,,We also performed quantitative analysis on the model output to better understand the performance.,,"Add,Fact/Evidence",Fact/Evidence
3459,334-ARR,334-ARR_v2_54@10,,"Our model outputs a probabilistic score in the range {0, 1}.",,"Add,Fact/Evidence",Fact/Evidence
3460,334-ARR,334-ARR_v2_55@0,,"A score closer to 0 indicates our model is confident that bail would be denied, while a score closer to 1 means bail granted.",,"Add,Fact/Evidence",Fact/Evidence
3461,334-ARR,334-ARR_v2_55@2,,"We observe the correct bail granted predictions are shifted towards 1, and the correct bail denied predictions are shifted towards 0.",,"Add,Fact/Evidence",Fact/Evidence
3462,334-ARR,334-ARR_v2_55@3,,"Additionally, the incorrect samples are concentrated near the middle (≈ 0.5), which shows that our model was able to identify these as borderline cases.",,"Add,Fact/Evidence",Fact/Evidence
3463,334-ARR,334-ARR_v2_13@0,334-ARR_v1_11@0,Hindi Legal Documents Corpus,Hindi Legal Document Corpus,"Modify,Grammar",Grammar
3464,334-ARR,334-ARR_v2_14@0,334-ARR_v1_12@0,"Hindi Legal Documents Corpus (HLDC) is a corpus of 912,568 Indian legal case documents in the Hindi language.",Hindi Legal Document Corpus (HLDC) is a corpus of about 900K Indian legal case documents in the Hindi language.,"Modify,Fact/Evidence",Fact/Evidence
3465,334-ARR,334-ARR_v2_14@1,334-ARR_v1_13@0,The corpus is created by downloading data from the e-Courts website (a publicly available website: https:// districts.ecourts.gov.in/).,The corpus is created by scraping data from the e-Courts website (a publicly available website: https:// districts.ecourts.gov.in/).,"Modify,Clarity",Clarity
3466,334-ARR,334-ARR_v2_14@3,334-ARR_v1_13@2,We download case documents pertaining to the district courts located in the Indian northern state of Uttar Pradesh (U.P.).,We scrape case documents pertaining to the district courts located in the Indian northern state of Uttar Pradesh (U.P.).,"Modify,Clarity",Clarity
3467,334-ARR,334-ARR_v2_2@9,334-ARR_v1_2@9,Experiments with different models are indicative of the need for further research in this area.,Results on different models are indicative of the need for further research in this area.,"Modify,Clarity",Clarity
3468,334-ARR,334-ARR_v2_14@15,334-ARR_v1_13@14,The first step in HLDC creation is the downloading of documents from the e-Courts website.,The first step in HLDC creation is the scraping of documents from the e-Courts website.,"Modify,Clarity",Clarity
3469,334-ARR,334-ARR_v2_2@0,334-ARR_v1_2@0,Many populous countries including India are burdened with a considerable backlog of legal cases.,"Populous countries (e.g., India) are burdened with a considerable backlog of legal cases.","Modify,Clarity",Clarity
3470,334-ARR,334-ARR_v2_15@3,334-ARR_v1_14@3,"The header contains the meta-information related to the case, for example, case number, court identifier, and applicable sections of the law.","The header contains the meta-information related to the case, for example, case number, court identifier, applicable sections of the law, etc.","Modify,Clarity",Clarity
3471,334-ARR,334-ARR_v2_4@2,334-ARR_v1_4@2,"For example, if a system could readily extract the required information from a legal document for a legal practitioner, then it would help expedite the legal process.","For example, if a system could readily extract the required information from a legal document for a legal practitioner, then it would help them expedite the legal process.","Modify,Clarity",Clarity
3472,334-ARR,334-ARR_v2_18@1,334-ARR_v1_15@18,"Body is further segmented into Facts and Arguments, Judge's summary and Case Result.","The body is further segmented into Facts and Arguments, Judge's summary and Case Result.","Modify,Grammar",Grammar
3473,334-ARR,334-ARR_v2_4@4,334-ARR_v1_4@4,"For example, legal documents are typically quite long (tens of pages), highly unstructured and noisy (spelling and grammar mistakes since these are typed), use domainspecific language and jargon; consequently, pre-trained language models do not perform well on these (Malik et al., 2021b).","For example, legal documents are typically quite long (tens of pages), legal documents are highly unstructured and noisy (spelling and grammar mistakes, since these are typed), language in legal documents are domain-specific, and pre-trained language models do not perform well on these (Malik et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3474,334-ARR,334-ARR_v2_23@7,334-ARR_v1_19@7,"As observed in previous work (Malik et al., 2021b), anonymization of a judge's name is important as there is a correlation between a case outcome and a judge name.","As observed in previous work (Malik et al., 2021), anonymization of a judge's name is important as there is a correlation between a case outcome and a judge name.","Modify,Fact/Evidence",Fact/Evidence
3475,334-ARR,334-ARR_v2_23@12,334-ARR_v1_19@12,"Moreover, the Bail corpus and corresponding bail prediction systems can promote the development of explainable systems (Malik et al., 2021b), we leave research on such explainable systems for future work.","Moreover, the Bail corpus and corresponding bail prediction systems can promote the development of explainable systems (Malik et al., 2021), we leave research on such explainable systems for future work.","Modify,Fact/Evidence",Fact/Evidence
3476,334-ARR,334-ARR_v2_4@5,334-ARR_v1_5@0,"Thus, to develop legal text processing systems and address the challenges associated with the legal domain, there is a need for creating specialized legal domain corpora.","To develop legal text processing systems and address the challenges associated with the legal domain, there is a need for creating specialized legal domain corpora.","Modify,Clarity",Clarity
3477,334-ARR,334-ARR_v2_2@1,334-ARR_v1_2@1,Development of automated systems that could process legal documents and augment legal practitioners can mitigate this.,This calls for the development of automated systems that could process legal documents and augment legal practitioners.,"Modify,Claim",Claim
3479,334-ARR,334-ARR_v2_5@1,334-ARR_v1_5@1,"For example, Chalkidis et al. (2019) have developed an English corpus of European Court of Justice documents, while Malik et al. (2021b) have developed an English corpus of Indian Supreme Court documents.","In recent times, there have been efforts to develop such corpora for example, Chalkidis et al. (2019) have developed an English corpus of European Court of Justice documents, Malik et al. (2021) have developed an English corpus of Indian Supreme Court documents, Xiao et al. (2018) have developed Chinese Legal Document corpus.","Split+Modify,Fact/Evidence",Fact/Evidence
3481,334-ARR,334-ARR_v2_50@5,334-ARR_v1_45@5,Testing is done on a different set of 17 districts not present in train set.,Testing is done on a different set of 17 districts not present during training.,"Modify,Clarity",Clarity
3482,334-ARR,334-ARR_v2_52@4,334-ARR_v1_48@4,"Another thing to note from the results is that, in general, summarization based models perform better than Doc2Vec and transformer-based models, highlighting the importance of the summarization step in the bail prediction task.","Another thing to note from the results is that, in general, summarization models perform better than Doc2Vec and transformer-based models, highlighting the importance of the summarization step in the bail prediction task.","Modify,Clarity",Clarity
3483,334-ARR,334-ARR_v2_5@4,334-ARR_v1_5@3,"Hindi uses Devanagari script (Wikipedia contributors, 2021) for the writing system.","Hindi uses Devanagri (Wikipedia contributors, 2021) script for the writing system.","Modify,Clarity",Clarity
3484,334-ARR,334-ARR_v2_54@1,334-ARR_v1_51@1,"After examining the miss-classified examples, we observed the following.",We observe a couple of things looking at the misclassified examples.,"Modify,Clarity",Clarity
3485,334-ARR,334-ARR_v2_54@6,334-ARR_v1_51@6,"In some instances, we also observed that even if the facts of the cases are similar the judgements can differ.","In some instances, we also observe that even if the facts of the cases are similar the judgements can differ.","Modify,Grammar",Grammar
3486,334-ARR,334-ARR_v2_5@6,334-ARR_v1_5@6,Most of the lower (district) courts in northern India use Hindi as the official language.,Most of the lower (district) courts in Northern India use Hindi as the official language.,"Modify,Grammar",Grammar
3487,334-ARR,334-ARR_v2_5@7,334-ARR_v1_5@7,"However, most of the legal NLP systems that currently exist in India have been developed on English, and these do not work on Hindi legal documents (Malik et al., 2021b).","However, most of the legal NLP systems that currently exist in India have been developed on English, and these do not work on Hindi legal documents (Malik et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3488,334-ARR,334-ARR_v2_5@8,334-ARR_v1_5@8,"To address this problem, in this paper, we release a large corpus of Hindi legal documents (HINDI LEGAL DOCUMENTS CORPUS or HLDC) that can be used for developing NLP systems that could augment the legal practitioners by automating some of the legal processes.","To address this problem, in this paper, we release a large corpus of Hindi legal documents (Hindi Legal Document Corpus) that can be used for developing NLP systems that could augment the legal practitioners by automating some of the legal processes.","Modify,Clarity",Clarity
3489,334-ARR,334-ARR_v2_58@8,334-ARR_v1_56@3,"Finally, all sentences labelled as judge's opinion either during reverse iteration or during paragraph level extension are extracted out as judge's summary and rest of the sentences form facts and opinions for further modelling.","Finally, all labelled as judge's opinion either during reverse iteration or during paragraph level extension are extracted out as judge's summary and rest of the sentences form facts and opinions for further modelling.","Modify,Clarity",Clarity
3490,334-ARR,334-ARR_v2_2@2,334-ARR_v1_2@2,"However, there is a dearth of high-quality corpora that is needed to develop such data-driven systems.","To develop such data-driven systems, there is a dearth of high-quality corpora.","Modify,Clarity",Clarity
3491,334-ARR,334-ARR_v2_6@0,334-ARR_v1_6@0,"India follows a Common Law system and has a three-tiered court system with District Courts (along with Subordinate Courts) at the lowest level (districts), followed by High Courts at the state level, and the Supreme Court of India at the highest level.","India follows a Common Law system and has a three-tiered court system with District Courts (along with Subordinate Courts) at the lowest levels of districts, followed by High Courts at the state level and the Supreme Court at the highest level.","Modify,Clarity",Clarity
3492,334-ARR,334-ARR_v2_6@1,334-ARR_v1_6@1,"In terms of number of cases, district courts handle the majority.","In terms of the number of cases, district courts handle the majority of the cases.","Modify,Clarity",Clarity
3493,334-ARR,334-ARR_v2_6@2,334-ARR_v1_6@2,"According to India's National Judicial Data Grid, as of November 2021, there are approximately 40 million cases pending in District Courts (National Judicial Data Grid, 2021) as opposed to 5 million cases pending in High Courts.","According to India's National Judicial Data Grid, as of November 2021, there are approximately 40 million cases pending in District courts (National Judicial Data Grid, 2021) as opposed to 5 million cases pending in High Courts.","Modify,Grammar",Grammar
3494,334-ARR,334-ARR_v2_6@3,334-ARR_v1_6@3,These statistics show an immediate need for developing models that could address the problems at the grass-root levels of the Indian legal system.,These statistics show an immediate need for developing systems that could address the problems at the grass-root levels of the Indian legal system.,"Modify,Clarity",Clarity
3495,334-ARR,334-ARR_v2_2@3,334-ARR_v1_2@3,The problem gets even more pronounced in the case of low resource languages such as Hindi.,"The problem gets even more pronounced in the case of low resource language (e.g., Hindi).","Modify,Clarity",Clarity
3496,334-ARR,334-ARR_v2_6@4,334-ARR_v1_6@4,"Out of the 40 million pending cases, approximately 20 million are from courts where the official language is Hindi (National Judicial Data Grid, 2021).","Out of 40 million pending cases, approximately 20 million cases are from courts where the official language is Hindi (National Judicial Data Grid, 2021).","Modify,Grammar",Grammar
3497,334-ARR,334-ARR_v2_6@5,334-ARR_v1_6@5,"In this resource paper, we create a large corpus of 912,568 Hindi legal documents.","In this resource paper, we create a large corpus of about 900K Hindi legal documents.","Modify,Fact/Evidence",Fact/Evidence
3498,334-ARR,334-ARR_v2_6@6,334-ARR_v1_6@6,"In particular, we collect documents from the state of Uttar Pradesh, the most populous state of India with a population of approximately 237 million (PopulationU, 2021).","In particular, we collect documents from the state of Uttar Pradesh (U.P.), the most populous state of India with a population of approximately 237 million (Popula-tionU, 2021).","Modify,Clarity",Clarity
3499,334-ARR,334-ARR_v2_6@7,334-ARR_v1_6@7,"The Hindi Legal Documents Corpus (HLDC) can be used for a number of legal applications, and as a use case, in this paper, we propose the task of Bail Prediction.","The Hindi Legal Document Corpus (HLDC) can be used for a number of legal applications, and in this paper, we propose the task of Bail Prediction.","Modify,Clarity",Clarity
3500,334-ARR,334-ARR_v2_2@4,334-ARR_v1_2@4,"In this resource paper, we introduce the Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents in Hindi.","In this resource paper, we introduce the Hindi Legal Documents Corpus (HLDC), a corpus of 900K legal documents in Hindi.","Modify,Fact/Evidence",Fact/Evidence
3501,334-ARR,334-ARR_v2_2@5,334-ARR_v1_2@5,Documents are cleaned and structured to enable the development of downstream applications.,The documents are cleaned and structured to enable the development of downstream applications.,"Modify,Grammar",Grammar
3502,334-ARR,334-ARR_v2_12@0,334-ARR_v1_10@0,"Majority of the work in the legal domain has focused on the higher court (Malik et al., 2021b;Strickson and De La Iglesia, 2020;Zhong et al., 2020b); however, the lower courts handle the maximum number of cases.","Majority of the work in the legal domain has focused on the higher court (Malik et al., 2021;Strickson and De La Iglesia, 2020;Zhong et al., 2020b); however, the lower courts handle the maximum number of cases.","Modify,Fact/Evidence",Fact/Evidence
3503,334-ARR,334-ARR_v2_2@6,334-ARR_v1_2@6,"Further, as a use-case for the corpus, we introduce the task of bail prediction.","Further, as a usecase for the corpus, we introduce the task of Bail Prediction.","Modify,Grammar",Grammar
3504,334-ARR,334-ARR_v2_12@3,334-ARR_v1_10@3,"The competition has two sub-tasks, a legal information retrieval task and an entailment identification task between law articles and queries.","The competition had two sub-tasks, a legal information retrieval task and an entailment identification task between law articles and queries.","Modify,Grammar",Grammar
5718,86-ARR,,86-ARR_v1_70@0,,"Inference Promotion: We can achieve 11.73 and 2.06 absolute inference accuracy improvements compared to the baselines for the NLI and CQA task, respectively.","Delete,Fact/Evidence",Fact/Evidence
5719,86-ARR,,86-ARR_v1_70@1,,"For the NLI task, with our MPII framework, the Transformer baseline model can improve over 5 absolute accuracy score.","Delete,Fact/Evidence",Fact/Evidence
5720,86-ARR,,86-ARR_v1_70@2,,"The ablation study shows the contribution comes from not only the mutual interaction of inference and interpretation in the Stepwise Integration Mechanism (SIM), but also the adversarial mutual information training objective introduced in the Adversarial Fidelity Regularization (AFiRe).","Delete,Fact/Evidence",Fact/Evidence
5721,86-ARR,,86-ARR_v1_70@3,,"Moreover, with parameters initialized with the pretrained BART model, the accuracy can be further improved by a 4.53 absolute score.","Delete,Fact/Evidence",Fact/Evidence
5722,86-ARR,,86-ARR_v1_70@4,,"For the CQA task, we observe that better performance is still achieved compared with the CAGE baseline model.","Delete,Fact/Evidence",Fact/Evidence
5723,86-ARR,,86-ARR_v1_70@5,,"If we remove the AFiRe, a significant inference degradation would be witnessed.","Delete,Claim",Claim
5724,86-ARR,,86-ARR_v1_70@6,,It also indicates the effectiveness of AFiRe for utilizing interpretability to improve the inference ability.,"Delete,Claim",Claim
5725,86-ARR,,86-ARR_v1_71@0,,Interpretation Promotion: The quality of generated interpretation can also be significantly improved with our mutual promotion method on both NLI and CQA tasks.,"Delete,Fact/Evidence",Fact/Evidence
5726,86-ARR,,86-ARR_v1_71@1,,"For NLI task, combined with our MPII, the Transformer baseline model can provide more accurate, fluent and diverse interpretation with much better results in all metrics.","Delete,Fact/Evidence",Fact/Evidence
5727,86-ARR,,86-ARR_v1_71@2,,"Similar with the inference results, the ablation study shows that both SIM and AFiRe contribute to the performance improvement.","Delete,Fact/Evidence",Fact/Evidence
5728,86-ARR,,86-ARR_v1_71@3,,"With the pretrained BART model, we further improve the BLEU and Inter-Rep performance and get comparable PPL compared with the e-INFERSENT model.","Delete,Fact/Evidence",Fact/Evidence
5729,86-ARR,,86-ARR_v1_71@4,,"For CQA task, our method performs better in terms of BLEU score and the diversity of generated explanations.","Delete,Fact/Evidence",Fact/Evidence
5730,86-ARR,,86-ARR_v1_71@5,,"We notice that the BLEU scores are pretty low for CQA task, which may stem from the free form of expression for explanations in the dataset, i.e. several different explanations share the same commonsense knowledge.","Delete,Claim",Claim
5731,86-ARR,86-ARR_v2_73@1,,"The input of the model is ""[CLS] a couple standing on what looks like a peer or boardwalk [SEP] a couple hugging each other at the park"", of which the ground truth label is ""contradiction"".",,"Add,Fact/Evidence",Fact/Evidence
5732,86-ARR,86-ARR_v2_75@1,,"For the second example, our MPII and MPII with AFiRe removed still capture the entailment relation well, and explain that ""at the beach"" and ""at restaurant"" can not be done at the same time.",,"Add,Fact/Evidence",Fact/Evidence
5733,86-ARR,86-ARR_v2_75@2,,"As we can see, these explanations generated by our method are also fluent.",,"Add,Fact/Evidence",Fact/Evidence
5734,86-ARR,86-ARR_v2_76@4,,"Our MPII still explains well, but fails to explain properly with AFiRe removed, even if the explanation contains the correct answer, which reveals the importance of AFiRe for promotion of interpretation.",,"Add,Claim",Claim
5735,86-ARR,,86-ARR_v1_16@1,,Both prediction label and explanation token are generated at every decoding step.,"Delete,Fact/Evidence",Fact/Evidence
5736,86-ARR,,86-ARR_v1_16@2,,Two fusion gates are attached to enable deep interaction of their hidden representations.,"Delete,Fact/Evidence",Fact/Evidence
5737,86-ARR,86-ARR_v2_21@1,86-ARR_v1_22@1,"ReLU(•) here denotes the ReLU activation function (Nair and Hinton, 2010), σ(•) represents the sigmoid function.","ReLU(.) here denotes the ReLU activation function (Nair and Hinton, 2010), σ(.) represents the sigmoid function.","Modify,Grammar",Grammar
5738,86-ARR,86-ARR_v2_35@2,86-ARR_v1_38@0,"The step-by-step explanation helps the model to do better inference, and the stepwise inference in turn guides the generation of better explanation.","Step-by-step interpretation helps the model to better inference, stepwise inference in turn guides the generation of better explanation.","Modify,Clarity",Clarity
5739,86-ARR,86-ARR_v2_40@0,86-ARR_v1_44@0,"Because of the intractability of directly estimating the mutual information in high-dimensional space, we approximate the optimization objective with a Variational Information Maximization lower bound (Chen et al., 2016b;Zhang et al., 2018;Poole et al., 2019):","Because of the intractability of directly estimating the mutual information in high-dimensional space, we approximate the optimization objective with a Variational Information Maximization (Chen et al., 2016b;Zhang et al., 2018) lower bound (Poole et al., 2019):","Modify,Clarity",Clarity
5740,86-ARR,86-ARR_v2_42@1,86-ARR_v1_46@1,"P θ (L, E|X) and Q ϕ (X|L, E) denote the forward network (generating L, E conditioned on X) and the backward network (generating X conditioned on L, E) respectively.","P θ (L, E|X) and Q ϕ (X|L, E) denote the forward network (generating L, R conditioned on X) and the backward network (generating X conditioned on L, E) respectively.","Modify,Fact/Evidence",Fact/Evidence
5741,86-ARR,86-ARR_v2_4@1,86-ARR_v1_4@1,"In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretations to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2019;Thorne et al., 2019;Kumar and Talukdar, 2020).","In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2018;Thorne et al., 2019;Kumar and Talukdar, 2020).","Modify,Fact/Evidence",Fact/Evidence
5742,86-ARR,86-ARR_v2_51@0,86-ARR_v1_55@0,"Besides, we add an objective term P θ (L, E|X) of maximize the negative likelihood of P θ to balance the positive samples as teacher-forcing algorithm (Li et al., 2017).","We also add an objective term P θ (L, E|X) of maximum the negative likelihood of P θ to balance the positive samples as teacher-forcing algorithm (Li et al., 2017).","Modify,Other",Other
5743,86-ARR,86-ARR_v2_57@0,86-ARR_v1_61@0,"We use six datasets as our testbeds: SNLI (Bowman et al., 2015), e-SNLI (Camburu et al., 2018), CQA (Talmor et al., 2019), CoS-E (Rajani et al., 2019, MultiNLI (Williams et al., 2018), and SICK-E (Marelli et al., 2014).","We use six datasets as our testbeds: SNLI (Bowman et al., 2015), e-SNLI (Camburu et al., 2018), CQA (Talmor et al., 2018), CoS-E (Rajani et al., 2019), MultiNLI (Williams et al., 2018), and SICK-E (Marelli et al., 2014).","Modify,Fact/Evidence",Fact/Evidence
5744,86-ARR,86-ARR_v2_58@3,86-ARR_v1_62@3,"SICK-e (Sentences Involving Compositional Knowledge for entailment) provides sentence pairs that are rich in the lexical, syntactic and semantic phenomena.","SICKe(Sentences Involving Compositional Knowledge for entailment) provides sentence pairs that are rich in the lexical, syntactic and semantic phenomena.","Modify,Grammar",Grammar
5745,86-ARR,86-ARR_v2_58@4,86-ARR_v1_62@4,The latter two datasets are used for out-of-domain evaluation.,The latter two datasets are used for out-of-domain test.,"Modify,Clarity",Clarity
5746,86-ARR,86-ARR_v2_60@3,86-ARR_v1_64@3,"The Transformer model (Vaswani et al., 2017) adds a MLP layer for making predictions.","The Transformer model (Vaswani et al., 2017) adds a MLP layer for generating sentencelevel interpretations.","Modify,Fact/Evidence",Fact/Evidence
5747,86-ARR,86-ARR_v2_67@0,86-ARR_v1_73@0,"As shown in Table 2, we evaluate our method with the Transformer baseline model on two outof-domain datasets: MultiNLI and SICK-E. The results show that our mutual promotion method enables the Transformer model to be more robust, and achieves about 3 absolute accuracy improvement on both of the out-of-domain datasets without fine-tuning.","As shown in Table 2, we evaluate our method with the Transformer baseline model on two outof-domain datasets: MultiNLI and SICK-E. The results show that our mutual promotion method enables the Transformer model to be more robust, and achieve more than 3 accuracy improvement on both of the out-of-domain datasets without fine-tuning.","Modify,Fact/Evidence",Fact/Evidence
5748,86-ARR,86-ARR_v2_67@2,86-ARR_v1_74@1,The ablation results demonstrate both the adversarial mutual information training strategy in AFiRe and deep integration in SIM is very effective to improve the model's generalization and robustness.,The ablation results demonstrate the adversarial mutual information training strategy in AFiRe is very effective to improve the model's generalization and robustness.,"Modify,Fact/Evidence",Fact/Evidence
5749,86-ARR,86-ARR_v2_8@1,86-ARR_v1_8@1,"Considering readability and comprehensibility for humans, some works turn to generate token-level explanations (Liu et al., 2019;Thorne et al., 2019), which are nevertheless prone to cause ambiguity.","Considering readability and comprehensibleness for human, some works turn to generate token-level explanations (Liu et al., 2018;Thorne et al., 2019), which nevertheless prone to cause ambiguity.","Modify,Fact/Evidence",Fact/Evidence
5750,86-ARR,86-ARR_v2_73@4,86-ARR_v1_80@1,"From the clear split of the red and blue lines when ""does"" and ""not"" are generated, we can see that the prediction is very sensitive to explanation, which demonstrates the faithfulness (Kumar and Talukdar, 2020).","From the clear split of the red and blue lines when 'does' and 'not' are generated, we can see that the prediction is very sensitive to explanation, which demonstrates the faithfulness (Kumar and Talukdar, 2020).","Modify,Grammar",Grammar
5751,86-ARR,86-ARR_v2_76@1,86-ARR_v1_82@1,"For the first example, CAGE makes wrong prediction, and generates explanation that obviously conflicts with common knowledge.","For the first exapmle, CAGE makes wrong prediction, and generates explanation that obviously conflicts with common knowledge.","Modify,Grammar",Grammar
5752,86-ARR,86-ARR_v2_79@0,86-ARR_v1_86@0,"With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2019;Thorne et al., 2019;Kumar and Talukdar, 2020).","With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2018;Thorne et al., 2019;Kumar and Talukdar, 2020).","Modify,Fact/Evidence",Fact/Evidence
5753,86-ARR,86-ARR_v2_8@6,86-ARR_v1_8@6,"Intuitively, human language sentence-level interpretations containing reasoning logic are the best form for human to understand.","Intuitively, human language sentence-level interpretations containing reasoning logic is the best form for human to understand.","Modify,Grammar",Grammar
5754,86-ARR,86-ARR_v2_9@0,86-ARR_v1_9@0,"With annotated natural language interpretation datasets available (Camburu et al., 2018;Rajani et al., 2019), methods of generating sentence-level interpretation have been explored recently.","With the annotated natural language interpretation datasets available (Camburu et al., 2018;Rajani et al., 2019), methods of generating sentencelevel interpretation have been explored recently.","Modify,Grammar",Grammar
5755,86-ARR,86-ARR_v2_9@2,86-ARR_v1_9@2,"Kumar and Talukdar (2020) proposed to first generate sentence-level interpretations with deep pre-trained language models (such as BERT and GPT), then fed those interpretations as extra knowledge to help improve inference performance.","Kumar and Talukdar (2020) proposed to first generate sentence-level interpretation with deep pre-trained language models (such as BERT and GPT), then fed those interpretation as extra knowledge to help improve inference performance.","Modify,Grammar",Grammar
5756,86-ARR,86-ARR_v2_12@0,86-ARR_v1_12@0,"• Different from the previous works that only include one-side promotion, we mutually promote the inference and sentence-level interpretation from both the model-level and the optimization-level. • We propose a Stepwise Integration Mechanism to tightly fuse latent prediction and interpretation information at every decoding step, and an Adversarial Fidelity Regularization to further improve the fidelity with the adversarial training strategy. • Experiment results show that our method achieves significant improvement in both inference accuracy and interpretation quality compared with baseline models.","• Different from the previous works that only include one-side promotion, we mutually promote the inference and sentence-level interpretation from both the model-level and the optimization-level. • We propose a Stepwise Integration Mechanism to tightly fuse latent prediction and interpretation information at every decoding step, and an Adversarial Fidelity Regularization to further improve the fidelity with the adversarial training strategy. • Experiment results show that our method achieve significant improvement in both inference accuracy and interpretation quality compared with baseline models.","Modify,Grammar",Grammar
5757,86-ARR,86-ARR_v2_14@1,86-ARR_v1_14@1,"Utilizing the autoregressive nature of Transformer decoder, SIM enables deep interaction at every decoding step between inference and interpretation.","Utilizing the autoregressive nature of Transformer decoder, SIM allows deep interaction at every decoding step between inference and interpretation.","Modify,Clarity",Clarity
5758,86-ARR,86-ARR_v2_14@2,86-ARR_v1_14@2,"With the adversarial training strategy, AFiRe enables further integration of latent semantic information between inference and interpretation, and also improves the quality of explanation sentences by bringing them closer to human expressions.","With the adversarial training strategy, AFiRe allows further integration of latent semantic information between inference and interpretation, and also improves the quality of explanation sentences, bringing them closer to human expressions.","Modify,Clarity",Clarity
5759,89-ARR,,89-ARR_v1_26@6,,Our results alert investors who use text-based stock prediction models to deploy defense systems to guard against loss caused by potential adversarial attack.,"Delete,Claim",Claim
5760,89-ARR,,89-ARR_v1_29@0,,"In summary, we show that financial forecast models are vulnerable to adversarial attack even if it is subject to certain physical constraints.","Delete,Claim",Claim
5761,89-ARR,,89-ARR_v1_12@4,,"A directional financial forecast model takes domains of tweets and numerical factors as input, and yields prediction for stocks' directional movement y ∈ {−1, 1}:","Delete,Fact/Evidence",Fact/Evidence
5762,89-ARR,89-ARR_v2_19@1,89-ARR_v1_20@1,"Specifically, the boolean variables (for tweet and word selection) are relaxed into the continuous space so that they can be optimized by gradientbased methods over a convex hull.","Specifically, the boolean variables (for tweet and word selection) would be relaxed into the continuous space so that they can be optimized by gradient-based methods over a convex hull.","Modify,Clarity",Clarity
5763,89-ARR,89-ARR_v2_21@1,89-ARR_v1_22@1,"We evaluate our adversarial attack on a stock prediction dataset consisting of 10,824 instances including relevant tweets and numerical features of 88 stocks from 2014 to 2016 (Xu and Cohen, 2018).","We evaluate our adversarial attack on a stock prediction dataset consisting of 10824 instances including relevant tweets and numerical features of 88 stocks from 2014 to 2016 (Xu and Cohen, 2018).","Modify,Grammar",Grammar
5764,89-ARR,89-ARR_v2_4@0,89-ARR_v1_4@0,"The advance of deep learning based language models are playing a more and more important role in the financial context, including convolutional neutral network (CNN) (Ding et al., 2015), recurrent neutral network (RNN) (Minh et al., 2018), long short-term memory network (LSTM) (Hiew et al., 2019;Sawhney et al., 2021;Hochreiter and Schmidhuber, 1997), graph neutral network (GNN) (Sawhney et al., 2020a,b), transformer , autoencoder (Xu and Cohen, 2018), etc.","The advance of deep learning based language models are playing a more and more important role in the financial context, including convolutional neutral network (CNN) (Ding et al., 2015), recurrent neutral network (RNN) (Minh et al., 2018), long short-term memory network (LSTM) (Hiew et al., 2019;Sawhney et al., 2021;Hochreiter and Schmidhuber, 1997), graph neutral network (GNN) (Sawhney et al., 2020a,b), transformer (Yang et al., 2020), autoencoder (Xu and Cohen, 2018), etc.","Modify,Fact/Evidence",Fact/Evidence
5765,89-ARR,89-ARR_v2_21@3,89-ARR_v1_22@3,We apply our attack to instances on which the victim models make correct predictions.,We apply our attack to instances on which the victim models make correct prediction.,"Modify,Grammar",Grammar
5766,89-ARR,89-ARR_v2_24@2,89-ARR_v1_25@2,"For both JO and AGO, ASR increases by roughly 10% and F1 drops by 0.1 on average in comparison to the random attack.","As we can see, for both JO and AGO, ASR increases by roughly 10% and F1 drops by 0.1 on average in comparison to random attack.","Modify,Clarity",Clarity
5767,89-ARR,89-ARR_v2_24@6,89-ARR_v1_25@6,It appears that the attack performance becomes saturated if we keep increasing the attack budgets.,It appears that the attack performance becomes saturated if we keep increasing the attack budget.,"Modify,Grammar",Grammar
5768,89-ARR,89-ARR_v2_24@7,89-ARR_v1_25@7,"In fact, the attack with budget of one tweet and one word is the most cost effective, provided that it introduces minimum perturbation but achieves a relatively similar ASR.","In fact, the attack with budget of one tweet and one word is most cost effective, provided that it introduces minimum perturbation but achieves relatively similar ASR.","Modify,Grammar",Grammar
5769,89-ARR,89-ARR_v2_26@3,89-ARR_v1_26@4,"For each simulation, the investor has $10K (100%) to invest; the results show that the proposed attack method with a retweet with only a single word replacement can cause the investor an additional $3.2K (75%-43%) loss to their portfolio after about 2 years.","For each simulation, net values are set as 100% at the beginning.","Merge+Modify,Fact/Evidence",Fact/Evidence
5770,89-ARR,89-ARR_v2_26@3,89-ARR_v1_26@5,"For each simulation, the investor has $10K (100%) to invest; the results show that the proposed attack method with a retweet with only a single word replacement can cause the investor an additional $3.2K (75%-43%) loss to their portfolio after about 2 years.",The results show that even replacement of a single word in one tweet can cause a 32% (75%-43%) additional loss to the portfolio.,"Merge+Modify,Fact/Evidence",Fact/Evidence
5771,89-ARR,89-ARR_v2_28@0,89-ARR_v1_29@1,This work demonstrates that our adversarial attack method consistently fools various financial forecast models even with physical constraints that the raw tweet can not be modified.,The experiments demonstrate that our adversarial attack method consistently fools various models.,"Modify,Claim",Claim
5772,89-ARR,89-ARR_v2_28@1,89-ARR_v1_29@2,"Adding a retweet with only one word replaced, the attack can cause 32% additional loss to our simulated investment portfolio.","Moreover, with replacement of a single word on one tweet, the attack can cause 32% additional loss to our simulated portfolio.","Modify,Fact/Evidence",Fact/Evidence
5773,89-ARR,89-ARR_v2_28@2,89-ARR_v1_29@3,"Via studying financial model's vulnerability, our goal is to raise financial community's awareness of the AI model's risks, so that in the future we can develop more robust human-in-theloop AI architecture (Wang et al., 2019) to cope with this and other real-world attacks, including black-box attack, unknown input domains, etc.","Through studying vulnerability of financial forecast models, our goal is to raise financial community's awareness of the model robustness.","Merge+Modify,Clarity",Clarity
5774,89-ARR,89-ARR_v2_28@2,89-ARR_v1_29@4,"Via studying financial model's vulnerability, our goal is to raise financial community's awareness of the AI model's risks, so that in the future we can develop more robust human-in-theloop AI architecture (Wang et al., 2019) to cope with this and other real-world attacks, including black-box attack, unknown input domains, etc.","In the future, we plan to introduce more real-world constraints, including black-box attack, unknown input domains, etc.","Merge+Modify,Claim",Claim
5775,89-ARR,89-ARR_v2_2@0,89-ARR_v1_2@0,"More and more investors and machine learning models rely on social media (e.g., Twitter and Reddit) to gather real-time information and sentiment to predict stock price movements.","More and more investors and machine learning models rely on social media (e.g., Twitter and Reddit) to gather information and predict movements stock prices.","Modify,Claim",Claim
5776,89-ARR,89-ARR_v2_40@1,89-ARR_v1_41@1,"An computationally efficient fashion is to optimize over a convex hull constructed with linear combination of candidate set, and the optimal replacement goes with word with highest weight (Dong et al., 2021).","An computationally efficient fashion is to optimize over a convex hull constructed with linear combination of candidate set, and optimal replacement goes with word with highest weight (Dong et al., 2021).","Modify,Grammar",Grammar
5777,89-ARR,89-ARR_v2_5@1,89-ARR_v1_5@1,"The perturbation can be at the sentence level (e.g., Xu et al., 2021;Iyyer et al., 2018;Ribeiro et al., 2018), the word level (e.g., Zhang et al., 2019;Alzantot et al., 2018;Zang et al., 2020;Jin et al., 2020;Lei et al., 2019;Zhang et al., 2021;Lin et al., 2021), or both (Chen et al., 2021).","The perturbation can be done at the sentence level (e.g., Xu et al., 2021;Iyyer et al., 2018;Ribeiro et al., 2018), the word level (e.g., Zhang et al., 2019;Alzantot et al., 2018;Zang et al., 2020;Jin et al., 2020;Lei et al., 2018;Zhang et al., 2021;Lin et al., 2021), or both (Chen et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
5778,89-ARR,89-ARR_v2_5@2,89-ARR_v1_5@2,"We are interested in whether such adversarial attack vulnerability also exists in stock prediction models, as these models embrace more and more human-generated media data (e.g., Twitter, Reddit, Stocktwit, Yahoo News (Xu and Cohen, 2018;Sawhney et al., 2021)).","We are interested in whether such adversarial attack vulnerability also exists in stock prediction models, as these models embrace more and more user-generated public data (e.g., Twitter, Reddit, or Stocktwit (Xu and Cohen, 2018;Sawhney et al., 2021)).","Modify,Fact/Evidence",Fact/Evidence
5779,89-ARR,89-ARR_v2_5@3,89-ARR_v1_5@3,The adversarial robustness is a more critical issue in the context of stock prediction as anyone can post perturbed tweets or news to influence forecasting models.,The adversarial robustness may be a more critical topic in the context of stock prediction as anyone can post perturbed tweets to influence forecast models.,"Modify,Clarity",Clarity
5780,89-ARR,89-ARR_v2_5@4,89-ARR_v1_5@4,"For example, a fake news (""Two Explosions in the White House and Barack Obama is Injured"") posted by a hacker using the Associated-Press's Twitter account on 04/23/2013 erased $136 billion market value in just 60 seconds (Fisher, 2013).","As one example, a fake news (""Two Explosions in the White House and Barack Obama is Injured"") posted by a hacker using the AssociatedPress's Twitter account on 04/23/2013 erased $136 billion in stock market in just 60 seconds (Fisher, 2013).","Modify,Clarity",Clarity
5781,89-ARR,89-ARR_v2_5@5,89-ARR_v1_5@5,"Although the event doesn't fall into the category of adversarial attack, it rings the alarm for traders who use (social) media information for their trading decisions.","Although the event doesn't fall into the category of adversarial attack, it rings the alarm for traders who take information from social media to back their trading decision.","Modify,Clarity",Clarity
5782,89-ARR,89-ARR_v2_62@7,89-ARR_v1_63@7,The results show that loss smoothing does not contribute to attack performance in our experiment as it does in Srikant et al. (2021).,"The results show that loss smoothing does not contribute to attack performance in our experiment as it does in (Srikant et al., 2021).","Modify,Grammar",Grammar
5783,89-ARR,89-ARR_v2_63@2,89-ARR_v1_64@2,We then run Kmeans clustering on these 18 corpora based on the feature matrix from LIWC.,We then run Kmeans clustering these 18 corpora based on the feature matrix from LIWC.,"Modify,Grammar",Grammar
5784,89-ARR,89-ARR_v2_2@1,89-ARR_v1_2@1,"Although text-based models are known to be vulnerable to adversarial attacks, whether stock prediction models have similar vulnerability is underexplored.","Although textbased models are known to be vulnerable to adversarial attacks, whether stock prediction models have similar vulnerability given necessary constraints is underexplored.","Modify,Clarity",Clarity
5785,89-ARR,89-ARR_v2_6@1,89-ARR_v1_6@1,"Many attacks modify benign text directly (manipulation attack) and use them as model input; However, in our case, adversarial retweets enter the model along with benign tweets (concatenation attack), which is more realistic as malicious Twitter users can not modify others' tweets.","Many attack modifies benign text directly (manipulation attack) and use them as model input; However, in our case, adversarial retweets enter the model along with benign tweets (concatenation attack), which is more realistic as malicious Twitter users can not modify others' tweets.","Modify,Grammar",Grammar
5786,89-ARR,89-ARR_v2_6@2,89-ARR_v1_6@2,"In other words, we formulate the task as a text-concatenating attack (Jia and Liang, 2017;Le et al., 2021): we implement the attack by injecting new tweets instead of manipulating existing benign tweets.","In other words, we formulate the task as text-concatenating attack (Jia and Liang, 2017;Le et al., 2021): we implement the attack by injecting new tweets instead of manipulating existing benign tweets.","Modify,Grammar",Grammar
5787,89-ARR,89-ARR_v2_6@3,89-ARR_v1_6@3,"Our task is inspired and mimics the retweet function on social media, and uses it to feed the adversarial samples into the dataset.","Our task is inspired and mimics the retweet function on social media, and use it to feed the adversarial samples into the dataset.","Modify,Grammar",Grammar
5788,89-ARR,89-ARR_v2_6@4,89-ARR_v1_6@4,"Despite various algorithms are proposed to generate manipulation attack, literature of concatenation attack on classification models is rare, with exceptions Le et al. (2021), Song et al. (2021) and Wang et al. (2020).","Despite various algorithms are proposed to generate manipulation attack, literature of concatenation attack on classification model is rare, with exceptions Le et al. (2021), Song et al. (2021) and Wang et al. (2020).","Modify,Grammar",Grammar
5789,89-ARR,89-ARR_v2_6@5,89-ARR_v1_6@5,Our paper provides extra evidence of their difference by investigating their performances in the financial domain.,Our paper provides extra evidence of their difference by investigating their performances in the domain of finance.,"Modify,Clarity",Clarity
5790,89-ARR,89-ARR_v2_7@0,89-ARR_v1_7@0,The main challenge is to craft new and effective adversarial tweets.,The main challenge is to craft new adversarial tweets.,"Modify,Claim",Claim
5791,89-ARR,89-ARR_v2_7@1,89-ARR_v1_7@1,We solve the task by aligning the semantics with benign tweets so that the potential human and machine readers can't detect our adversarial tweets.,"While the adversarial tweets can be arbitrary given that they are newly posted, we solve the task by aligning the semantics with benign tweets so that potential human and machine readers can not detect our adversarial tweets.","Modify,Claim",Claim
5792,89-ARR,89-ARR_v2_7@3,89-ARR_v1_7@3,"Specific tweets are first selected, which are used as the target of perturbation on a limit number of words within the tweets.","Specific tweets are first selected, which are used as target of perturbation on a limit number of words within the tweets.","Modify,Grammar",Grammar
5793,89-ARR,89-ARR_v2_7@6,89-ARR_v1_7@6,"More astonishingly, the attack can cause additional loss of 23% to 32% if an investor trades on the predictions of the victim models (Fig. 4).","More astonishingly, the attack can cause additional loss of 23% to 32% if the investor trades on predictions of the victim models (Fig. 4).","Modify,Grammar",Grammar
5794,89-ARR,89-ARR_v2_10@3,89-ARR_v1_10@3,"Secondly, adversarial tweets are optimized to be semantically similar to the original tweets so that they are not counterfactual and very likely to fool human sanity checks as well as the Twitter's content moderation system.","Secondly, adversarial tweets are optimized to be semantically similar to original tweets so that they are not counterfactual and very likely fool human sanity checks as well as the Twitter's content moderator mechanisms.","Modify,Clarity",Clarity
5795,89-ARR,89-ARR_v2_11@1,89-ARR_v1_11@1,The challenge of our attack method centers around how to select the optimal tweets and the token perturbations with the constraints of semantic similarity.,The challenge of our attack method centers around how to select the optimal tweets and the token perturbations with constraints of semantic similarity.,"Modify,Grammar",Grammar
5796,89-ARR,89-ARR_v2_11@3,89-ARR_v1_11@3,"In the first step, a set of optimal tweets is first selected as the target tweets to be perturbed and retweeted.","In the first step, a set of optimal tweets is first selected as target tweets to be perturbed and retweeted.","Modify,Grammar",Grammar
5797,89-ARR,89-ARR_v2_11@4,89-ARR_v1_11@4,"For each selected tweet in the pool, the word selection problem is then solved to find one or more optimal words to apply perturbation.","For each selected tweet in the pool, the word selection problem is then solved to find one or more best words to apply perturbation.","Modify,Clarity",Clarity
5798,89-ARR,89-ARR_v2_11@5,89-ARR_v1_11@5,Word and tweet budgets are also introduced to quantify the strength of the perturbation.,Word and tweet budgets are also introduced to quantifies the strength of perturbation.,"Modify,Grammar",Grammar
5799,89-ARR,89-ARR_v2_12@0,89-ARR_v1_12@0,"We consider the word replacement and deletion for word perturbation (Garg and Ramakrishnan, 2020;Li et al., 2020).","We consider word replacement and deletion for word perturbation (Garg and Ramakrishnan, 2020;Li et al., 2020).","Modify,Grammar",Grammar
5800,89-ARR,89-ARR_v2_12@2,89-ARR_v1_12@2,"A synonym as replacement is widely adopted in the word-level attack since it is a natural choice to preserve semantics (Zang et al., 2020;Dong et al., 2021;Zhang et al., 2019;Jin et al., 2020).","Synonym as replacement is widely adopted in the word-level attack since it is a natural choice to preserve semantics (Zang et al., 2020;Dong et al., 2021;Zhang et al., 2019;Jin et al., 2020).","Modify,Grammar",Grammar
5801,89-ARR,89-ARR_v2_17@1,89-ARR_v1_18@1,"Consequently, given attack loss L, generation of adversarial retweets can be formulated as the optimization program min m,z,u and c) 1 T u i,j = 1, ∀i, j, where b s and b w denote the tweet and word budgets.","Consequently, given attack loss L, generation of adversarial retweets can be formulated as the optimization program min m,z,u and c) 1 T u i,j = 1, ∀i, j, where b s and b w denote the tweet and word budget.","Modify,Grammar",Grammar
5802,89-ARR,89-ARR_v2_17@2,89-ARR_v1_18@2,It is worth to stress that perturbation is only applied to the date (t) when the attack is implemented to preserve the temporal order.,It is worth to stress that perturbation is only applied to the date (t) when the attack is implemented to preserve temporal order.,"Modify,Grammar",Grammar
5993,1-12,1-12_v2_16@3,,"Since the N-terminal hydrophobic domain of plasmepsin V is not cleaved <REF-12> , it is likely a transmembrane signal anchor.",,"Add,Claim",Claim
5994,1-12,1-12_v2_16@4,,Both TMHMM and TopPred predict it to insert into the ER membrane with the N-terminus in the lumen such that the subsequent soluble region containing the active site would be in the cytoplasm.,,"Add,Fact/Evidence",Fact/Evidence
5995,1-12,1-12_v2_28@3,1-12_v1_28@3,Membrane proteins would be transported by vesicular transport from the PVM to Maurer's clefts where their release from the PI3P patches could be triggered by a PI3P-phosphatase.,Membrane proteins would be transported by vesicular transport from the PVM to Maurer’s clefts where their release from the PI3P patches could be triggered by a PI3P-phosphatase.,"Modify,Grammar",Grammar
5996,1-12,1-12_v2_32@6,1-12_v1_32@6,"The authors' interpretation of the data was that proteins must be unfolded in order to be transported across the PVM into the erythrocyte, that the PVM therefore contained a protein-conducting channel with similar requirements for transport as the Sec61 channel in the ER membrane, and that the time window after synthesis during which proteins were transport-competent was limited.","The authors’ interpretation of the data was that proteins must be unfolded in order to be transported across the PVM into the erythrocyte, that the PVM therefore contained a protein-conducting channel with similar requirements for transport as the Sec61 channel in the ER membrane, and that the time window after synthesis during which proteins were transport-competent was limited.","Modify,Grammar",Grammar
5997,1-12,1-12_v2_10@4,1-12_v1_10@4,- The uncleaved targeting signal binds PI3P at the ER membrane with the same specificity required for protease cleavage and host cell targeting; the cleaved signal no longer binds PI3P <REF-13> .,- The uncleaved targeting signal binds phosphoinositol-3-phosphate (PI3P) at the ER membrane with the same specificity required for protease cleavage and host cell targeting; the cleaved signal no longer binds PI3P <REF-13> .,"Modify,Clarity",Clarity
5998,1-12,1-12_v2_10@5,1-12_v1_10@5,"- A putative 'translocator' complex resides in the PV membrane (PVM); it consists of 5 proteins that coprecipitate some of the proteins bearing a host cell targeting signal, but a function of the complex has not been demonstrated in any way, nor has it been investigated whether the association of the complex and the PEXEL proteins is mediated by the PEXEL signal <REF-14> , <REF-15> .","- A putative ‘translocator’ complex resides in the PV membrane (PVM); it consists of 5 proteins that coprecipitate some of the proteins bearing a host cell targeting signal, but a function of the complex has not been demonstrated in any way, nor has it been investigated whether the association of the complex and the PEXEL proteins is mediated by the PEXEL signal <REF-14> , <REF-15> .","Modify,Grammar",Grammar
5999,1-12,1-12_v2_12@1,1-12_v1_12@1,"This cannot be right: N-acetylation is a cytosolic modification, based on the biochemical and sequence data characterizing the protease plasmepsin V, its active site is almost certainly on the cytoplasmic face of the membrane, and the only possible location for PI3P at the ER membrane is in the cytosolic leaflet.","This cannot be right: N-acetylation is a cytosolic modification, the active site of plasmepsin V is almost certainly on the cytoplasmic face of the ER membrane (based on the biochemical & sequence data characterizing the protease), and the only possible location for PI3P at the ER membrane is in the cytosolic leaflet and the only possible location for PI3P at the ER membrane is in the cytosolic leaflet.","Modify,Clarity",Clarity
6000,1-12,1-12_v2_16@2,1-12_v1_16@2,"Russo and colleagues showed later (2010) <REF-10> that fusion of this region of plasmepsin V to a fluorescent reporter protein resulted in ring-shaped staining around the parasite cytoplasm indicative of location in the PV or at the parasite plasma membrane, and suggesting entry of the protein into the ER and transport to the cell surface.","Russo and colleagues (2010) <REF-10> showed later that this region of plasmepsin V was not able to target the protein to the ER, which demonstrates that it is not a signal sequence.","Modify,Fact/Evidence",Fact/Evidence
6001,1-12,1-12_v2_16@5,1-12_v1_16@3,In the same paper Russo and colleagues demonstrated that the C-terminal hydrophobic region of plasmepsin V was required for ER retention of the protein <REF-10> .,In the same paper Russo and colleagues demonstrated that the C-terminal hydrophobic region of plasmepsin V was required to anchor the protein to the ER membrane.,"Modify,Fact/Evidence",Fact/Evidence
6002,1-12,1-12_v2_16@6,1-12_v1_16@4,"Both Russo and colleagues and Klemba and Goldberg describe this region as a transmembrane domain, but Klemba and Goldberg show that 50% of plasmepsin V can be extracted from the membrane by carbonate, pH 11.0 <REF-12> .","Both Russo and colleagues and Klemba and Goldberg describe this region as a transmembrane domain, but Klemba & Goldberg show that 50% of plasmepsin V can be extracted from the membrane by carbonate, pH 11.0 <REF-12> .","Modify,Grammar",Grammar
6003,1-12,1-12_v2_16@8,1-12_v1_16@6,"Altogether these data suggest that plasmepsin V is tethered to the ER membrane by hydrophobic regions at both termini, and that its active site is in the cytoplasm.",Altogether these data suggest that plasmepsin V is a carboxy-terminally membrane-anchored or membrane-associated protein with its entire N-terminal domain including the active site in the cytoplasm.,"Modify,Claim",Claim
6004,1-21,1-21_v2_13@3,,"Namely, whereas the bacterial populations differed with regards the side scatter parameter, forward scatter showed no change in its distribution.",,"Add,Fact/Evidence",Fact/Evidence
6005,1-21,1-21_v2_18@3,,An alternative explanation is that phage binding decreases resource uptake by bacterial cells.,,"Add,Claim",Claim
6006,1-21,1-21_v2_18@4,,"However, this seems unlikely in our experiment.",,"Add,Claim",Claim
6007,1-21,1-21_v2_18@5,,"Because bacteria were exposed to inactivated phages only, the total number of viral particles is predicted to stay constant (or possibly degrade) throughout the experiment.",,"Add,Claim",Claim
6008,1-21,1-21_v2_18@6,,"When bacteria divide, the number of phages bound to a daughter cell should be roughly half the number on the mother cell; thus, the number of bound phages per cell will decrease exponentially with cell divisions.",,"Add,Claim",Claim
6009,1-21,1-21_v2_18@7,,"Using the density of phages and bacteria employed in our experiment, we predict that there will be, on average, less than one phage individual per bacterial cell after 9 to 10 cell divisions, which based on the mean doubling time presented in Figure 1 , is reached in the first 48 hours of the experiment.",,"Add,Claim",Claim
6010,1-21,1-21_v2_18@8,,Our results can explain previous observations on phage-associated increases in population size in P. fluorescens <REF-32> .,,"Add,Fact/Evidence",Fact/Evidence
6011,1-21,1-21_v2_18@9,,"Specifically, we predict that a significant number of phage in the experiments of Gomez and Buckling <REF-32> did not kill their bacterial hosts before some of the latter were able to accelerate their cell cycle and produce daughter cells.",,"Add,Claim",Claim
6012,1-21,1-21_v2_18@14,,"This response is expected to result in smaller individual size, because energy allocated to growth is directed to reproduction when the stressor is present.",,"Add,Claim",Claim
6013,1-21,1-21_v2_28@9,,Observations of c. 50 cells using TEM (Zeis EM10) showed no bound phages after the centrifugation treatment.,,"Add,Fact/Evidence",Fact/Evidence
6014,1-21,1-21_v2_30@3,,"KB medium containing UV-inactivated phages was obtained through centrifugation of inactivated phage, which were further added into pure KB, so that the medium used in the treatments only differs from the control by the presence of phages.",,"Add,Fact/Evidence",Fact/Evidence
6015,1-21,1-21_v2_36@0,,Measures of OD will be affected by changes in particle size.,,"Add,Claim",Claim
6016,1-21,1-21_v2_36@1,,"At equal bacterial density, a population of smaller cells will yield a lower OD value, because fewer particles will block less of the incoming light.",,"Add,Claim",Claim
6017,1-21,1-21_v2_36@2,,"The practical conclusion is that whenever bacteria get smaller, we understimate their count, and thus their growth rate.",,"Add,Claim",Claim
6018,1-21,1-21_v2_36@3,,"Because this means that we are more conservative about the impact of phage exposure on growth rate (i.e., if there were any bias in our results, it would be an underestimation of the increase in growth rate), we did not correct for this effect.",,"Add,Claim",Claim
6019,1-21,1-21_v2_16@0,1-21_v1_16@0,"We did not observe any difference in the impact of live phage on bacterial populations exposed to the different treatments (KW, df = 2, P = 0.153), suggesting that the inducible response does not alter bacteria resistance to phage predation.","We did not observe any difference in the impact of live phage on bacterial populations exposed to the different treatments (KW, df=2, P = 0.153), suggesting that the inducible response does not alter bacteria resistance to phage predation.","Modify,Grammar",Grammar
6020,1-21,1-21_v2_18@1,1-21_v1_18@1,We hypothesize that this response increases the survival chances of bacterial progeny under natural conditions and demonstrate that this behaviour comes at a fitness cost of reduced size of daughter cells.,We hypothesize that this response increases the survival chances of progeny under natural conditions and demonstrate that this behaviour comes at a fitness cost of reduced size of daughter cells.,"Modify,Claim",Claim
6021,1-21,1-21_v2_18@10,1-21_v1_19@0,"Furthermore, our results support and extend both theoretical <REF-33> and empirical <REF-34> , <REF-35> predictions that victims may lessen the fitness impact of their natural enemies through early reproduction, to cases where phenotypic responses are plastic and temporary.","These results support and extend both theoretical <REF-32> and empirical <REF-33> , <REF-34> predictions that victims may lessen the fitness impact of their natural enemies through early reproduction, to cases where phenotypic responses are plastic and temporary.","Modify,Clarity",Clarity
6022,1-21,1-21_v2_18@11,1-21_v1_19@1,Increased allocation to reproduction in stressful environments–termed “fecundity compensation” or “terminal investment” <REF-31> –although never studied in bacteria-phage associations to our knowledge–has been extensively studied for other host-parasite (or organism-stressor) interactions.,Increased allocation to reproduction in stressful environments–termed “fecundity compensation” or “terminal investment” <REF-31> – although never studied in bacteria-phage associations to our knowledge – has been extensively studied for other host-parasite (or organism - stressor) interactions.,"Modify,Grammar",Grammar
6023,1-21,1-21_v2_21@3,1-21_v1_22@3,"Assuming that the physiological mechanisms involved in fission rate increases are the same in the two experiments, this suggests that rapid multiplication is not adaptive for the bacterium, and indeed we report no advantage of being exposed to inactived phage in terms of a lessened population impact during live phage exposure.","Assuming that the physiological mechanisms involved in fission rate increases are the same in the two experiments, this suggests that rapid multiplication is not adaptive for the bacterium.","Modify,Fact/Evidence",Fact/Evidence
6024,1-21,1-21_v2_21@7,1-21_v1_22@7,"Future studies should therefore focus on the possible adaptive nature of this response for both bacterium and phage, by investigating in greater depth how it affects the mechanisms of infection, recovery, and resistance.",Future studies should therefore focus on the possible adaptive nature of this response for both bacterium and phage.,"Modify,Claim",Claim
6025,1-21,1-21_v2_28@4,1-21_v1_29@4,Inactivated phage were allowed 4 h to attach to the bacterial outer membrane.,Inactivated phage were allowed 4h to attach to the bacterial outer membrane.,"Modify,Grammar",Grammar
6026,1-21,1-21_v2_28@7,1-21_v1_29@7,"PCR was done using TPV1f (GATGTGAGAAAGCGATACACGG) and TPV1r (GAGAGAAGCGGGAGAGTGAA) sequences developed for this study, which selectively amplify a 550 bp fragment of the phage DNA and a 1200 bp fragment of the bacterial DNA (see Supplementary Figure 1 for detailed protocols).","PCR was done using TPV1f (GATGTGAGAAAGCGATACACGG) and TPV1r (GAGAGAAGCGGGAGAGTGAA) sequences developed for this study, which selectively amplify a 550 bp fragment of the phage DNA and a 1200 bp fragment of the bacterial DNA (see Supplement Figure 1 for detailed protocols).","Modify,Grammar",Grammar
6027,1-21,1-21_v2_28@8,1-21_v1_29@8,"We did not find any evidence that UV-inactivated phage was present in samples putatively containing bacteria only, thus confirming that (i) the DNA of inactivated phage was not incorporated in the bacterial cell and (ii) our centrifugation method removed both bound and unbound phage.","We did not find any evidence that UV-inactivated phage was present in samples putatively containing bacteria only, thus confirming that the DNA of inactivated phage was not incorporated in the bacterial cell.","Modify,Claim",Claim
6028,1-21,1-21_v2_30@1,1-21_v1_31@1,Fixed SBW25 bacteria of the smooth morphotype were first cultivated in 6 ml KB in 30 mL universal glass vials.,Fixed smooth SBW25 bacteria were first cultivated in 6 ml KB in 30 mL universal glass vials.,"Modify,Clarity",Clarity
6029,1-21,1-21_v2_34@1,1-21_v1_35@1,"Exponential phase was determined by conducting a series of windowed linear regressions over the full growth curve, and retaining the part of the curve with the largest slope (computer code given in Supplementary materials part 3 ).","Exponential phase was determined by conducting a series of windowed linear regressions over the full growth curve, and retaining the part of the curve with the largest slope (computer code given in suppl. materials part 3).","Modify,Clarity",Clarity
6030,1-21,1-21_v2_8@8,1-21_v1_8@7,"Terminal investment is well characterised for other host-parasite associations <REF-31> , but to our knowledge has not previously been observed in bacteria subject to phage infection.","Terminal investment is well characterised for other host-parasite associations <REF-31> , but to our knowledge has not previously been observed in bacteria and phage.","Modify,Claim",Claim
6031,1-21,1-21_v2_10@0,1-21_v1_10@0,"Bacteria exposed to UV-inactivated phage display a statistically significant higher growth rate over the first 24 hours post-exposure than non-phage controls (Kruskal-Wallis, df = 3, P = 0.006; Figure 1 ).","Bacteria exposed to UV-inactivated phage display a statistically significant higher growth rate over the first 24 hours post-exposure than non-phage controls (Kruskal-Wallis, df=3, P = 0.006; Figure 1 ).","Modify,Grammar",Grammar
6032,1-21,1-21_v2_10@3,1-21_v1_10@3,"During the fourth day post-exposure, control and treatment bacteria showed no significant differences in doubling time (KW, df = 3, P > 0.05).","During the fourth day post-exposure, control and treatment bacteria showed no significant differences in doubling time (KW, df=3, P > 0.05).","Modify,Grammar",Grammar
6033,1-21,1-21_v2_10@5,1-21_v1_10@5,"There was a marginally significant effect on population growth for bacteria exposed to different phage concentrations (KW, df = 2, P < 0.02), suggesting that the encounter rate between bacteria and phage is important in determining the population-level strength of the fission response.","There was a marginally significant effect on population growth for bacteria exposed to different phage concentrations (KW, df=2, P < 0.02), suggesting that the encounter rate between bacteria and phage is important in determining the population-level strength of the fission response.","Modify,Grammar",Grammar
6034,1-21,1-21_v2_13@0,1-21_v1_13@0,"We hypothesized that faster doubling times would come at a cost to cell size, since cells would have less time to metabolize and convert absorbed nutrients into cell structure twenty-four hours post-exposure, we found that phage-treated bacteria were two to three times smaller (as measured by mean cellular width) than the control (KW, df = 3, P < 0.0001; Figure 2 ).","We hypothesized that faster doubling times would come at a cost to cell size, since cells would have less time to metabolize and convert absorbed nutrients into cell structure Twenty-four hours post-exposure, we found that phage-treated bacteria were two to three times smaller (as measured by mean cellular width) than the control (KW, df=3, P < 0.0001; Figure 2 ).","Modify,Grammar",Grammar
6035,1-21,1-21_v2_13@2,1-21_v1_13@2,Analyses of the distribution of several flow cytometry profiles showed that a difference in cell shape is unlikely to explain this result (see Data File below).,Analyses of the distribution of several flow cytometry profiles showed that a difference in cell shape is unlikely to explain this result (see data associated to this article).,"Modify,Clarity",Clarity
6036,1-21,1-21_v2_13@4,1-21_v1_13@3,"This implies that bacterial shape remained unchanged throughout the experiment, and indeed, additional observations using a transmission electron microscope showed that the cells remained rod-shaped for all treatments.","Finally, observations using a transmission electron microscope showed that the cells remained rod-shaped for all treatments.","Modify,Claim",Claim
6037,1-23,,1-23_v1_19@10,,Data about health hazards linked to FGC were not derived from any of these studies.,"Delete,Fact/Evidence",Fact/Evidence
6038,1-23,,1-23_v1_19@12,,"If a new test or a drug is to be prescribed for a patient, it should pass through a complicated series of tests and randomized comparisons before getting approved.","Delete,Claim",Claim
6039,1-23,,1-23_v1_19@13,,The same occurs with any surgical procedure.,"Delete,Claim",Claim
6040,1-23,,1-23_v1_19@14,,No procedure can be considered superior to another or blamed for complications except after randomized controlled trials comparing the new to standard surgery.,"Delete,Claim",Claim
6041,1-23,,1-23_v1_19@15,,It therefore seems unrealistic to consider data about FGC not derived from randomized or cohort studies are true and conclusive.,"Delete,Claim",Claim
6042,1-23,,1-23_v1_22@0,,"The ban against FGC seems to be gender based, especially because no similar act was taken against male circumcision.","Delete,Claim",Claim
6043,1-23,,1-23_v1_22@1,,"If male circumcision is considered safe by anti FGC groups, they should advise how to render FGC as safe as male circumcision instead of enforcing the ban against it.","Delete,Claim",Claim
6044,1-23,1-23_v2_19@5,,"In the era of evidence based medicine, level I evidence, derived from either systematic reviews or randomized controlled trials (RCTs), to support the ban against FGC is not available.",,"Add,Claim",Claim
6045,1-23,1-23_v2_19@7,,"In fact, the design and implementation of a RCT to address the effects of FGC cannot be justified and seems to be unethical.",,"Add,Claim",Claim
6046,1-23,,1-23_v1_19@8,,Research including reported data about past experiences will always be threatened by the individual’s memory and the influence of exposure status on the recalling process <REF-21> .,"Delete,Fact/Evidence",Fact/Evidence
6047,1-23,,1-23_v1_19@9,,The strongest evidence comes from randomized controlled trials followed by cohort studies.,"Delete,Claim",Claim
6048,1-23,1-23_v2_23@0,1-23_v1_24@0,Conclusions,Final remarks,"Modify,Other",Other
6049,1-23,1-23_v2_17@8,1-23_v1_17@8,Claims for increased Cesarean deliveries in cut women were attributed to obstructed labor most likely due to excessive scarring at the pelvic outlet probably resulting from the imperfect healing of the genital cutting and possible associated infection.,Claims for increased Cesarean deliveries in cut women were attributed to obstructed labor most likely due to excessive scarring at the pelvic outlet.,"Modify,Claim",Claim
6050,1-23,1-23_v2_17@9,1-23_v1_17@9,"However, the high Cesarean rate in this population cannot be attributed solely to obstruction due to excessive outlet scarring; obstructed labor may occur due to a variety of reasons.",The high Cesarean rate in this population cannot be attributed solely to obstruction due to excessive outlet scarring; obstructed labor may occur due to a variety of reasons.,"Modify,Clarity",Clarity
6051,1-23,1-23_v2_19@8,1-23_v1_19@5,"In light of this fact and in the absence of any scientific evidence to support the practice of female circumcision, the available level III evidence, derived from retrospective studies and studies depended on self-reported FGC and its health consequences, should be taken into consideration in spite of their imprecision and low reliability <REF-19> , <REF-20> .",It is the author’s view that none of these studies hold solid evidence to rely upon.,"Merge+Modify,Claim",Claim
6052,1-23,1-23_v2_19@8,1-23_v1_19@6,"In light of this fact and in the absence of any scientific evidence to support the practice of female circumcision, the available level III evidence, derived from retrospective studies and studies depended on self-reported FGC and its health consequences, should be taken into consideration in spite of their imprecision and low reliability <REF-19> , <REF-20> .",These studies were either of retrospective design or studies depended on self-reported FGC and its health consequences.,"Merge+Modify,Clarity",Clarity
6053,1-23,1-23_v2_19@8,1-23_v1_19@7,"In light of this fact and in the absence of any scientific evidence to support the practice of female circumcision, the available level III evidence, derived from retrospective studies and studies depended on self-reported FGC and its health consequences, should be taken into consideration in spite of their imprecision and low reliability <REF-19> , <REF-20> .","Such studies are imprecise and have low reliability <REF-19> , <REF-20> .","Merge+Modify,Clarity",Clarity
6054,1-62,,1-62_v1_16@0,,The 16S rRNA PCR product sequencing was carried out by modified Sanger’s dideoxy chain termination cycle sequencing method <REF-21> .,"Delete,Fact/Evidence",Fact/Evidence
6055,1-62,,1-62_v1_16@1,,"Electropherogram was read by an automated DNA sequencer ( Applied Biosystems ABI3500 XL Genetic Analyzer, Big Dye Terminator version 3.1 Cycle sequencing kit ) for 1.5 kb amplicon of the isolate.","Delete,Fact/Evidence",Fact/Evidence
6056,1-62,,1-62_v1_16@2,,The resulting final DNA sequence of isolate was subjected to BLAST analysis on the NCBI web server.,"Delete,Fact/Evidence",Fact/Evidence
6057,1-62,,1-62_v1_16@3,,The phylogenetic tree of 16S rRNA of the isolate was constructed by using a neighbor-joining (NJ) method with 1000 replicates of bootstrap in MEGA4.1 software <REF-22> .,"Delete,Fact/Evidence",Fact/Evidence
6058,1-62,,1-62_v1_16@4,,The bootstrap consensus tree inferred from 1000 replicates was selected to represent the evolutionary history of the taxa for 16S rRNA sequence analysis.,"Delete,Fact/Evidence",Fact/Evidence
6059,1-62,,1-62_v1_17@1,,The NCBI Blast server was used for the identification of the ATP synthase a -subunit and the phylogenetic analysis of the a -subunit was carried out with the help of MEGA4.1.,"Delete,Fact/Evidence",Fact/Evidence
6060,1-62,,1-62_v1_17@2,,The blastx was used to get the amino acid sequence of the a -subunit.,"Delete,Fact/Evidence",Fact/Evidence
6061,1-62,,1-62_v1_17@3,,"The a -subunit of isolate was compared with that of established alkaliphiles, acidophiles and neutrophiles with the help of ClustalW.","Delete,Fact/Evidence",Fact/Evidence
6062,1-62,,1-62_v1_20@1,,"Out of these, some bacterial colonies were found with pink and orange pigmentation along with no pigmentation i.e. white colonies.","Delete,Fact/Evidence",Fact/Evidence
6063,1-62,,1-62_v1_20@4,,"However, the pH range of growth was pH 7.0 to 12.0 (Supplementary figure 1) .","Delete,Fact/Evidence",Fact/Evidence
6064,1-62,,1-62_v1_33@0,,"Comparison of a -subunit of Stenotrophomonas species DL18 with acidophiles, alkaliphiles and neutrophiles","Delete,Other",Other
6065,1-62,,1-62_v1_37@0,,"However, Ala 198 from the Stenotrophomonas species DL 18 and Ala 196 from T. cyclicum were found to correspond with Gly 208 in E. coli and Gly 170 in Bacillus pseudofirmus OF4 in alignment of the a -subunit as shown in Figure 2 .","Delete,Fact/Evidence",Fact/Evidence
6066,1-62,,1-62_v1_37@1,,"In a similar way, Gly 207 was observed in Stenotrophomonas species DL 18, while in this respective position alanine was found in alkaliphiles and neutrophiles.","Delete,Fact/Evidence",Fact/Evidence
6067,1-62,,1-62_v1_37@2,,But both amino acids correspond from the same amino acid family.,"Delete,Fact/Evidence",Fact/Evidence
6068,1-62,,1-62_v1_38@4,,"However, exchange mutations at Gly 120 and Lys 180 to Lys 120 and Gly 180 showed ATP synthase activity in Bacillus pseudofirmus OF4.","Delete,Fact/Evidence",Fact/Evidence
6069,1-62,,1-62_v1_39@0,,"After comparison with acidophiles, neutrophiles and alkaliphiles, Leu 197 of the Stenotrophomonas species DL 18 was found to be conserved in TMH-4 (Figure 3) .","Delete,Fact/Evidence",Fact/Evidence
6070,1-62,1-62_v2_2@2,,Various studies have reported alkaliphiles from different alkaline habitats other than Lonar Lake with alkaliphile specific amino acid residues in the F 1 F o ATP synthase a-subunit.,,"Add,Fact/Evidence",Fact/Evidence
6071,1-62,1-62_v2_4@14,,"Hence, the cytoplasmic pH needs to be maintained 1.5 to 2.3 pH units below the external environment, which generates an optimal condition for ATP synthesis.",,"Add,Claim",Claim
6072,1-62,1-62_v2_9@0,,Further polymerase chain reaction (PCR) and sequencing of 16S rRNA was carried out for identification of bacterium <REF-11> .,,"Add,Fact/Evidence",Fact/Evidence
6073,1-62,1-62_v2_11@4,,"The amplified PCR product of 1.7 kbp was visualized on 1% agarose gel and the results were documented by Bio-Rad gel documentation system with Quantity One software (Bio-Rad, USA).",,"Add,Fact/Evidence",Fact/Evidence
6074,1-62,1-62_v2_11@6,,"An ATP synthase comparative study was performed by multiple sequence alignment with other strains from different categories i.e. acidophiles, neutrophiles and alkaliphiles, as shown in Figure 1 .",,"Add,Fact/Evidence",Fact/Evidence
6075,1-62,1-62_v2_19@0,,Table 1 lists the bacterial species used in the comparative studies.,,"Add,Fact/Evidence",Fact/Evidence
6076,1-62,,1-62_v1_6@1,,"The c 11 ring from Ilyobacter tartaricus has Na + ion binding specificity, while the c 13 ring from Bacillus pseudofirmus OF4 and the c 15 ring from Spirulina platensis has H + ion binding specificity <REF-6> .","Delete,Fact/Evidence",Fact/Evidence
6077,1-62,,1-62_v1_6@2,,Ion coordination geometry and distances determine the ion specificity.,"Delete,Claim",Claim
6078,1-62,,1-62_v1_6@3,,"Translocated ions bind to conserved carboxylate of aspartate or glutamate (D/E) in the outer α-helices of c-rings <REF-5> , <REF-6> .","Delete,Fact/Evidence",Fact/Evidence
6079,1-62,,1-62_v1_6@4,,"However, ions are further coordinated by a network of residues.","Delete,Claim",Claim
6080,1-62,,1-62_v1_6@5,,The inner and outer α-helices are in a staggered position.,"Delete,Claim",Claim
6081,1-62,1-62_v2_26@1,,"In addition, exchange mutations at a E219-H and a H245-E showed similar ATP synthase activity in E. coli .",,"Add,Fact/Evidence",Fact/Evidence
6082,1-62,1-62_v2_26@2,,"Moreover, the aG218-K substitution effect was suppressed by a H245-G mutation in E. coli.",,"Add,Fact/Evidence",Fact/Evidence
6083,1-62,1-62_v2_26@4,,"In addition, equivalent amino acid residue studies were also reported from the facultative alkaliphile B. pseudofirmus OF4 a -subunit <REF-7> – <REF-9> .",,"Add,Fact/Evidence",Fact/Evidence
6084,1-62,,1-62_v1_6@6,,"Out of these, the inner helices are hydrophobic in nature and in contact with the phospholipids, while outer helices are hydrophilic with a -subunit interaction for ion translocation <REF-2> , <REF-6> .","Delete,Fact/Evidence",Fact/Evidence
6085,1-62,,1-62_v1_6@7,,"The transmembrane electric potential controls the kinetics of rotary motion, which seems to be independent of the ionic gradient <REF-5> .","Delete,Fact/Evidence",Fact/Evidence
6086,1-62,,1-62_v1_6@9,,"For this, there are some crucial amino acid residue adaptations in the a - and c -subunit solving the problem of proton capture from an alkaline environment and subsequent translocation to the binding sites on the c-ring <REF-6> .","Delete,Fact/Evidence",Fact/Evidence
6087,1-62,,1-62_v1_7@0,,"Various studies based on the mechanism of proton binding <REF-7> , <REF-8> , through hydronium ion proton retention and transportation for ATP synthesis in the bacterial system including alkaliphiles <REF-9> , <REF-10> suggest the presence of alkaliphile specific conserved amino acid motifs in transmembrane helix-4 (TMH-4) and TMH-5 of the a -subunit and the inner and the outer helix of the c -subunit <REF-3> , <REF-11> – <REF-14> of the ATP synthase F o subunit as well as Na + /H + antiporter <REF-15> and other cation binding proton transporters including multiple drug transporters <REF-6> , <REF-16> .","Delete,Fact/Evidence",Fact/Evidence
6088,1-62,,1-62_v1_7@1,,The presence of the above mentioned alkaliphile specific sequences of ATP synthase F o subunit along with Na + /H + antiporters and other multiple drug transporters are the major strategies for pH homeostasis of extremely alkaliphilic species.,"Delete,Claim",Claim
6089,1-62,,1-62_v1_8@1,,"The arginine residue of the a -subunit, which transfers protons to the c -subunit, is conserved in almost all bacterial species <REF-3> .","Delete,Fact/Evidence",Fact/Evidence
6090,1-62,,1-62_v1_8@2,,"Recent developments in molecular studies of facultative alkaliphiles suggests the presence of a highly conserved A X A X A X A motif in the amino terminal helix and a P XX E XX P motif in the carboxy terminal helix of the ATP synthase c -subunit in Bacillus pseudofirmus OF4, an established facultative alkaliphile <REF-6> , <REF-17> , <REF-18> .","Delete,Fact/Evidence",Fact/Evidence
6091,1-62,,1-62_v1_2@3,,"Attempts were made to isolate and identify alkaliphiles from their naturally occurring original habitat, i.e. Lonar Lake, India with high alkaline conditions of pH 10.5.","Delete,Claim",Claim
6092,1-62,,1-62_v1_8@5,,"The present study deals with the isolation, identification and analysis of alkaliphile specific amino acid motifs in the a -subunit of ATP synthase.","Delete,Fact/Evidence",Fact/Evidence
6093,1-62,,1-62_v1_11@2,,"After mix culture was obtained by the spread plate method, pure culture of each type of colony was maintained for further studies.","Delete,Fact/Evidence",Fact/Evidence
6094,1-62,,1-62_v1_12@0,,DNA extraction and Polymerase Chain Reaction (PCR),"Delete,Other",Other
6095,1-62,,1-62_v1_13@1,,DNA quantization and quality control for protein contamination was carried out by spectrophotometric absorbance at A 260 and A 280 .,"Delete,Fact/Evidence",Fact/Evidence
6096,1-62,,1-62_v1_13@2,,The small subunit ribosomal RNA (16S rRNA) PCR for identification of bacterium was performed with forward primer (16S20F: 5’ATGTTGATCATGGCTCA3’) and reverse primer (16S1540R: 5’AAGGAGGTGATCCAACCGCA 3’) <REF-20> .,"Delete,Fact/Evidence",Fact/Evidence
6097,1-62,,1-62_v1_13@3,,"Briefly, master mix was prepared for 16S rRNA PCR: 10x PCR Rxn Buffer without MgCl 2 (Invitrogen, P/N Y02028B Lot no. WK1B1b, USA), 1mM MgCl 2 (Invitrogen, P/N Y02016B Lot no. WK2B1a, USA), 200 µM dNTP mix (Merck, India), 100 picomoles of each reverse and forward primers (Integrated DNA technologies, USA), 2.5U of Taq Polymerase enzyme (Invitrogen, USA, 11615-010 Lot No. VKRB1E) and nuclease free water (Merck, India) was added to make up a final volume of 100 µl.","Delete,Fact/Evidence",Fact/Evidence
6098,1-62,,1-62_v1_13@4,,"Following thermal cycling conditions were used for PCR: Initial denaturation at 94°C for 5 min, followed by 30 cycles of denaturation at 94°C for 1 min, primer annealing at 55°C for 1 min and primer extension at 72°C for 2 min.","Delete,Fact/Evidence",Fact/Evidence
6099,1-62,,1-62_v1_13@5,,Thirty cycles of PCR were followed by final extension at 72°C for 5 min followed by cooling at 4°C.,"Delete,Fact/Evidence",Fact/Evidence
6100,1-62,,1-62_v1_14@0,,ATP synthase F o amplification primers were designed based on S. maltophilia K279a as follows: forward primer Steno atp1F: 5’CCTGGCGGATCCTTAGATCTCCG 3’ and reverse primer Steno atp1R: 5’CAGTGAGGATCCTTAGATCTCCGAGGCCAGCT 3’.,"Delete,Fact/Evidence",Fact/Evidence
6101,1-62,,1-62_v1_14@4,,"Results of 16S rRNA and ATP synthase F o amplicons were visualized on 1% agarose gel with 200 ng/ml ethidium bromide (sd fine, India) and results were observed and analyzed with the help of Bio-Rad gel documentation system XR with Bio-Rad Quantity-One 4.6.5 software.","Delete,Fact/Evidence",Fact/Evidence
6102,1-62,,1-62_v1_2@5,,"Although the a-subunit of Stenotrophomonas DL18 showed significant similarity with neutrophiles, the isolated bacterium is an alkaliphile and optimally grows at pH 10.5.","Delete,Fact/Evidence",Fact/Evidence
6103,1-62,1-62_v2_11@5,1-62_v1_17@0,"Further, the DNA sequencing of the ATP F o subunit of the selected isolate was performed via primer walking.","Further, the DNA sequencing of the ATP F o subunit of selected isolates was performed by primer walking method.","Modify,Clarity",Clarity
6104,1-62,1-62_v2_15@0,1-62_v1_19@0,Bacterial identification and ATP synthase F o subunit amplification,Bacterial identification,"Modify,Other",Other
6105,1-62,1-62_v2_8@3,1-62_v1_20@0,"Across the pH gradient, morphologically different pink, orange and white bacterial colonies were observed.","Across the pH gradient, growth of morphologically different bacterial colonies was observed.","Modify,Fact/Evidence",Fact/Evidence
6106,1-62,1-62_v2_16@0,1-62_v1_20@2,The orange pigmented bacterium was identified as Stenotrophomonas sp. based on a NCBI BLAST analysis of the 16S rRNA gene sequence and titled Stenotrophomonas sp. DL18 (GenBank Accession number: JN995612 ).,The orange pigmented bacterium was identified as Stenotrophomonas species based on BLAST analysis of 16S rRNA gene sequence and titled as Stenotrophomonas species DL18 (GenBank Accession number: JN995612 ).,"Modify,Fact/Evidence",Fact/Evidence
6107,1-62,1-62_v2_16@1,1-62_v1_20@3,"Microbiological studies we performed showed that Stenotrophomonas sp. DL18, which optimally grows at pH 9.5, is an aerobic, facultative alkaliphilic with curved rod morphology.",Stenotrophomonas species DL18 optimally grow at the pH 9.0 to 10.0.,"Merge+Modify,Clarity",Clarity
6108,1-62,1-62_v2_16@1,1-62_v1_20@5,"Microbiological studies we performed showed that Stenotrophomonas sp. DL18, which optimally grows at pH 9.5, is an aerobic, facultative alkaliphilic with curved rod morphology.","The Stenotrophomonas species DL18 is known to be an aerobic, facultative alkaliphilic curved rod (Supplementary figure 2) .","Merge+Modify,Fact/Evidence",Fact/Evidence
6109,1-62,1-62_v2_4@0,1-62_v1_4@0,"ATP is molecular currency for a living cell, which is not only growing and dividing but also continuously responding to external environmental stimuli.","ATP is the molecular currency for a living cell, which is not only merely growing and dividing but also continuously responding to external environmental stimuli.","Modify,Clarity",Clarity
6110,1-62,1-62_v2_18@0,1-62_v1_25@0,Comparative studies of ATP synthase a -subunit of Stenotrophomonas species DL18,ATP synthase a -subunit of Stenotrophomonas species DL18,"Modify,Other",Other
6111,1-62,1-62_v2_17@0,1-62_v1_26@0,A BLAST analysis of the ATP synthase a -subunit of the Stenotrophomonas species DL18 suggests maximum identity with Stenotrophomonas species SKA14 (GenBank Accession number: ZP_05136035 ) at the amino acid level (259 amino acid residues were identical from a total of 266 amino acid residues in the SKA14 species).,BLAST analysis of the ATP synthase a -subunit of the Stenotrophomonas species DL 18 suggests maximum identity at the amino acid level (259 identical amino acids from a total of 266 amino acids of Stenotrophomonas species SKA14; GenBank Accession number: ZP_05136035 ).,"Modify,Clarity",Clarity
6112,1-62,1-62_v2_19@1,1-62_v1_26@1,"The amino acid residue arginine, which was found to be conserved in almost all bacterial species in the a -subunit, was observed at position 200 (Arg 200 ) in DL18.","The amino acid residue arginine, which was found to be conserved in almost all bacterial species in the a -subunit, was observed at position 200 (Arg 200 ) (Supplementary figure 3) .","Modify,Fact/Evidence",Fact/Evidence
6113,1-62,1-62_v2_19@2,1-62_v1_26@2,"Moreover, other amino acid residues that were conserved in most of the bacteria include Leu 207 , Arg 210 , Leu 211 , Gly 213 , Asn 214 , Gly 218 , Gln 252 , Ala 253 and Phe 255 ( E. coli numbering system for ATP synthase a -subunit) in the trans-membrane helix-4 ( a TMH-4), a TMH-5 and the corresponding amino acids were also found in the DL18 alignment ( Figure 1 and Figure 2 ).","Moreover, other amino acids that were conserved in most of the bacteria include Leu 207 , Arg 210 , Leu 211 , Gly 213 , Asn 214 , Gly 218 , Gln 252 , Ala 253 , Phe 255 ( E. coli numbering system for ATP synthase a -subunit) in TMH-4 and TMH-5 and the corresponding amino acids were also found in the Stenotrophomonas species DL 18 in alignment ( Figure 1 , Figure 2 ).","Modify,Clarity",Clarity
6114,1-62,1-62_v2_4@1,1-62_v1_4@1,"To survive in extreme conditions, microorganisms devise specific adaptive mechanisms.","To sustain life in extreme conditions, microorganisms devise specific mechanisms for their adaptation.","Modify,Clarity",Clarity
6115,1-62,1-62_v2_4@2,1-62_v1_4@2,"Along with other transporter proteins, ATP synthase is widely considered one of the key molecules for adaptation at alkaline conditions.","Along with other transporter proteins, ATP synthase is widely considered as one of the key molecules for adaptation at alkaline conditions.","Modify,Grammar",Grammar
6116,1-62,1-62_v2_4@3,1-62_v1_5@0,"Hydrolysis of nucleoside triphosphates, specifically ATP, provides the chemical energy to drive a wide variety of cellular reactions.","Hydrolysis of nucleoside tri-phosphates, specifically ATP, provides the chemical energy to drive a wide variety of cellular reactions.","Modify,Grammar",Grammar
6117,1-62,1-62_v2_24@0,1-62_v1_34@0,It was observed that the most conserved arginine residue of the a -subunit (Arg 200 of Stenotrophomonas sp. DL18) was aligned with the expected position of the facultative alkaliphile Bacillus pseudofirmus OF4 (i.e. Arg 172 ).,It was observed that the most conserved arginine residue of the a -subunit (Arg 200 of Stenotrophomonas species DL18) was aligned with the expected position of the facultative alkaliphile Bacillus pseudofirmus OF4 (i.e. Arg 172 ; GenBank Accession number: YP_003426326 ).,"Modify,Fact/Evidence",Fact/Evidence
6118,1-62,1-62_v2_24@2,1-62_v1_34@2,Lys 180 in B. pseudofirmus OF4 was replaced by Gly 208 in the DL18 strain.,Amino acid Lys 180 of Bacillus pseudofirmus OF4 was replaced by Gly 208 in the Stenotrophomonas species DL 18.,"Modify,Clarity",Clarity
6119,1-62,1-62_v2_24@3,1-62_v1_34@3,"In addition, glycine and alanine residues were observed at the same position in other alkaliphiles including Gly 206 in Thioalkalimicrobium cyclicum ALM1, and Ala 230 in Theoalkalivibrio sp. K90mix, as shown in Figure 1 .","In addition, a glycine residue was observed at the same position in other alkaliphiles, T. cyclicum (Gly 206 ; GenBank Accession number: YP_004537849 ), and same amino acid family Ala 230 in Theoalkalivibrio species K90mix (GenBank Accession number: YP_003461818 ) as shown in Figure 2 .","Modify,Fact/Evidence",Fact/Evidence
6120,1-62,1-62_v2_24@5,1-62_v1_34@6,"As reported by Ivey DM et al. , Lys 180 and corresponding amino acids were located in a TMH-4 in Bacillus pseudofirmus OF4 and other alkaliphiles <REF-12> , <REF-13> .","Lys 180 and corresponding amino acids were located in transmembrane helix-4 ( a TMH-4) in Bacillus pseudofirmus OF4 and other alkaliphiles <REF-23> , <REF-24> .","Modify,Fact/Evidence",Fact/Evidence
6121,1-62,1-62_v2_24@6,1-62_v1_34@7,"In the DL18 strain, a histidine residue, which is conserved in other reference species of the same genus (e.g. Stenotrophomonas species K279a, Stenotrophomonas sp. SKA14 and other alkaliphiles including T. cyclicum ALM1 (His 244 ), and Theoalkalivibrio sp. K90mix (His 262 ), was present at position 240 (His 240 ) ( Figure 1 ).","In the Stenotrophomonas species DL 18, a histidine residue, which is conserved in other reference species of the same genus ( S. maltophilia K279a GenBank Accession number: YP_001973793 ; and S . sp. SKA14) and other alkaliphiles T. cyclicum (His244), and Theoalkalivibrio species K90mix (His 262 ), was present at position 240 (His 240 ) (Figure 2) .","Modify,Fact/Evidence",Fact/Evidence
6122,1-62,1-62_v2_4@4,1-62_v1_5@1,ATP synthesis is central to ATP production during oxidative phosphorylation.,ATP synthases are central to ATP production during oxidative phosphorylation.,"Modify,Grammar",Grammar
6123,1-62,1-62_v2_26@0,1-62_v1_34@8,"E. coli K12 DH10B considered a neutrophile, can adapt to slightly alkaline conditions up to pH 8.0 and this may be due to the presence of His 245 .","However, E. coli K12 DH10B (GenBank Accession number: YP_001732559 ), considered as neutrophile, can adapt to slightly alkaline conditions i.e. up to pH 8.0 and this may be due to the presence of His 245 .","Modify,Fact/Evidence",Fact/Evidence
6124,1-62,1-62_v2_25@0,1-62_v1_34@9,It has been proposed that Gly 120 and Lys 180 forms a channel for the proton uptake pathway of the a -subunit through which protons pass onto the neighboring c -subunit in B. pseudofirmus OF4 <REF-7> .,It was proposed that Gly 120 and Lys 180 form a channel residing within the proton uptake pathway of the a -subunit through which protons pass onto the neighboring c -subunit in Bacillus pseudofirmus OF4 <REF-18> .,"Modify,Clarity",Clarity
6125,1-62,1-62_v2_2@0,1-62_v1_2@0,"Lonar Lake, an Indian soda lake with high alkaline conditions of pH 10.5, is well known for its biodiversity of extremophiles including alkaliphiles.","Lonar Lake, an Indian Soda Lake, is well known for its biodiversity of extremophiles including alkaliphiles.","Modify,Fact/Evidence",Fact/Evidence
6126,1-62,1-62_v2_4@6,1-62_v1_5@3,"The F o integral membrane protein complex provides a transmembrane pore for protons, whereas the peripheral protein F <REF-1> is involved in catalysis <REF-1> .","The F o integral membrane protein complex (the subscript ‘o’ denotes its inhibition by the drug oligomycin) provides a transmembrane pore for protons, whereas the peripheral protein F <REF-1> (the subscript ‘1’ indicates that it was the first of several factors isolated from mitochondria) is involved in catalysis <REF-1> .","Modify,Fact/Evidence",Fact/Evidence
6127,1-62,1-62_v2_25@1,1-62_v1_37@3,"However, one study has reported that His 245 along with the Gly 218 and Glu 219 positioning plays a critical role in F o ion translocations in E. coli <REF-8> , and in the present study on Stenotrophomonas sp. DL18, His 240 , Gly 208 and Glu 209 were observed at the same corresponding positions as shown in Figure 1 .","However, one of the studies reported the His 245 along with the Glu 219 positioning plays a critical role in ion translocation in E. coli <REF-25> , while in Stenotrophomonas species DL 18 these were at His 240 and Glu 209 as shown in Figure 2 .","Modify,Fact/Evidence",Fact/Evidence
6128,1-62,1-62_v2_25@2,1-62_v1_38@0,McMillan <REF-14> et al demonstrated the importance of residues with a basic side chain along with its pKa value in ATP synthesis in alkaline environments.,McMillan et al <REF-25> showed the importance of the residue with a basic side chain along with its pKa value in ATP synthesis linked with alkaline environment.,"Modify,Clarity",Clarity
6129,1-62,1-62_v2_25@3,1-62_v1_38@1,"However, that mutation study was carried out in the Bacillus sp. TA2.A1, specifically involving the 180 th residue in the a -subunit.","However, these studies were carried out with a mutation study in the Bacillus pseudofirmus OF4 a -subunit, specifically with the position of the residue at 180.","Modify,Fact/Evidence",Fact/Evidence
6130,1-62,1-62_v2_25@4,1-62_v1_38@2,"In that amino acid substitution study, amino acid residue Lys 180 in the a -subunit was mutated to Gly 180 , His 180 and Arg 180 .","These studies included mutations at Lys 180 position as Gly 180 , His 180 and Arg 180 .","Modify,Fact/Evidence",Fact/Evidence
6131,1-62,1-62_v2_26@3,1-62_v1_38@5,"Hence, these studies in E. coli signified that the positions Gly 218 and His 245 along with Glu 219 had a critical interaction with the F o subunit function.","Similar studies in E. coli showed that the positions Gly 218 and His 245 along with G1u 219 had a critical interaction with F o function <REF-26> , <REF-27> .","Modify,Fact/Evidence",Fact/Evidence
6132,1-62,1-62_v2_27@0,1-62_v1_39@1,"From alignment, Glu 209 of the DL18 strain was conserved in alkaliphiles, neutrophiles and some acidophiles except His 186 in Acidiphilium cryptum ( Figure 1 ).","From alignment, Glu 209 of the Stenotrophomonas species DL 18 was conserved in alkaliphiles, neutrophiles and some acidophiles except His 186 Acidiphilium cryptum .","Modify,Fact/Evidence",Fact/Evidence
6133,1-62,1-62_v2_27@1,1-62_v1_39@2,"In addition, the position of Gly 212 in B. pseudofirmus OF4 corresponds to Ser 212 in Enterococcus hirae ATCC 9790 (neutrophile), His 245 in E. coli and His 240 in the Stenotrophomonas sp. DL18 while glutamate in acidophiles is positioned at Glu 222 in A. ferrooxidans and Glu 225 in A. cryptum as shown in Figure 1 .","In addition, the position of Gly 212 in Bacillus pseudofirmus OF4 corresponds to Ser 212 in E. hirae (neutrophile; GenBank Accession number: YP_006487510 ), His 245 in E. coli and His 240 in the Stenotrophomonas species DL 18 while glutamate in acidophiles, Glu 222 in A. ferrooxidans and Glu 225 in A. cryptum as shown in Figure 3 .","Modify,Fact/Evidence",Fact/Evidence
6134,1-62,1-62_v2_27@2,1-62_v1_39@3,"This showed that channel formation may involve a glycine residue along with other residues, specifically those with acidic, basic and neutral side chains, which play vital roles in ATP synthesis in acidophile, alkaliphiles and neutrophiles.","This showed that the channel formation may involve a glycine residue along with other residues, specifically acidic, basic and neutral side chain, which play vital roles in ATP synthesis in acidophile, alkaliphiles and neutrophiles.","Modify,Clarity",Clarity
6135,1-62,1-62_v2_27@3,1-62_v1_39@4,"Hence, these residues are found to be critical in channel formation <REF-7> – <REF-9> .","Hence, these residues are found to be critical in channel formation.","Modify,Fact/Evidence",Fact/Evidence
6136,1-62,1-62_v2_27@5,1-62_v1_39@5,Gly 208 and His 240 may form the proton translocation channel.,"In the same scenario, the Gly 208 and His 240 may be form the proton translocation channel in Stenotrophomonas species DL 18.","Modify,Clarity",Clarity
6137,1-62,1-62_v2_4@7,1-62_v1_5@4,"F <REF-1> consists of five subunits, α <REF-3> β <REF-3> γ <REF-1> δ <REF-1> ε <REF-1> , with a ring of α- and β-subunits alternating around a single γ-subunit <REF-2> .","F <REF-1> consists of five subunits α <REF-3> β <REF-3> γ <REF-1> δ <REF-1> ε <REF-1> , with a ring of α- and β-subunits alternating around a single γ-subunit <REF-2> .","Modify,Grammar",Grammar
6138,1-62,1-62_v2_4@9,1-62_v1_5@6,"Out of these subunits, a -subunit is a stator and c -ring is a rotor ring through which ions (H + or Na + ) are translocated <REF-3> – <REF-6> .","Out of these subunits, the a -subunit is a stator and c-ring is a rotor ring through which ions (H + or Na + ) are translocated <REF-3> – <REF-6> .","Modify,Grammar",Grammar
6139,1-62,1-62_v2_4@10,1-62_v1_5@7,Each c -chain from the ring consists of two α-helices traversing the membrane and the polar loop extends out of the membrane to interact with the γ-and ε-subunits.,"Each c -chain from the ring consists of two α-helices traversing the membrane, and the polar loop extends out of the membrane to interact with the γ-and ε-subunits.","Modify,Grammar",Grammar
6140,1-62,1-62_v2_4@11,1-62_v1_5@8,A cytoplasmic F <REF-1> catalytic domain is connected by a membrane-embedded F o domain by a central (γε) and peripheral (b <REF-2> δ) stalk <REF-2> – <REF-6> .,A cytoplasmic F <REF-1> catalytic domain is connected with a membrane-embedded F o domain by a central (γε) and peripheral (b <REF-2> δ) stalk <REF-2> – <REF-6> .,"Modify,Grammar",Grammar
6141,1-62,1-62_v2_4@12,1-62_v1_6@0,"In general, downhill ion translocation across the membrane through F o causes rotation of the c -ring, which induces conformational changes in the catalytic β-subunit and results in ATP synthesis.","Downhill ion translocation across the membrane through F o causes rotation of the c-ring, which induces conformational changes in the catalytic β-subunit and results in ATP synthesis.","Modify,Clarity",Clarity
6142,1-62,1-62_v2_2@1,1-62_v1_2@1,Most of the molecular studies on Lonar Lake alkaliphiles are based on identification by 16S ribosomal RNA (16S rRNA).,Most of the molecular studies on Lonar Lake alkaliphiles are based on molecular identification by 16S ribosomal RNA along with numerous applications in the biotechnology industry.,"Modify,Fact/Evidence",Fact/Evidence
6143,1-62,1-62_v2_4@13,1-62_v1_6@8,"However, in alkaline conditions, the external pH is high i.e. above pH 8.0, which poses a major thermodynamic problem for ATP synthesis.","However, alkaliphiles grow at high environmental pH, which poses the thermodynamic problem of synthesizing ATP with ATP synthase.","Modify,Fact/Evidence",Fact/Evidence
6144,1-62,1-62_v2_2@3,1-62_v1_2@2,"As the data on the alkaliphilic nature of bacteria from Lonar Lake is incompletely understood, the present report comprised of isolation and identification of alkaliphiles from Lonar Lake.","However, molecular basis of adaptation of these alkaliphiles to high alkaline conditions is incompletely understood.","Modify,Fact/Evidence",Fact/Evidence
6145,1-62,1-62_v2_5@0,1-62_v1_8@0,Most studies have focused on the proton translocation channel in the a -subunit of ATP synthase <REF-7> – <REF-9> .,Most studies have focused on the proton translocation channel in the a -subunit of ATP synthase.,"Modify,Fact/Evidence",Fact/Evidence
6146,1-62,1-62_v2_5@1,1-62_v1_8@3,"However, similar experimental evidence from other geographic locations such as highly alkaline soda lakes needs further exploration to understand pH homeostasis in facultative alkaliphiles.","However, similar experimental evidence from other geographic locations such as highly alkaline soda lakes need further exploration to understand pH homeostasis in facultative alkaliphiles.","Modify,Grammar",Grammar
6147,1-62,1-62_v2_5@2,1-62_v1_8@4,This study explores the ATP synthase a -subunit of a facultative alkaliphilic aerobe isolated from Lonar Lake.,This study explores the comparison of the ATP synthase a -subunit of facultative alkaliphilic aerobes isolated from Lonar Lake with established and reported alkaliphiles.,"Modify,Fact/Evidence",Fact/Evidence
6148,1-62,1-62_v2_6@0,1-62_v1_9@0,Methods,Materials and methods,"Modify,Other",Other
6149,1-62,1-62_v2_7@0,1-62_v1_10@0,Isolation and identification of bacteria from the soda lake,Isolation and culture of bacteria,"Modify,Other",Other
6150,1-62,1-62_v2_8@0,1-62_v1_11@0,"In the present study, underwater sediment soil samples were collected 350 meters away from the Kamalaja Devi Temple end of Lonar Lake, Buldhana, Maharashtra, India.","Underwater sediment soil samples were collected from 350 meters away from Kamalaja Devi Temple end, Lonar Lake, Buldhana, Maharashtra, India.","Modify,Clarity",Clarity
6151,1-62,1-62_v2_8@4,1-62_v1_13@0,The bacterial genomic DNA of optimally grown bacteria at pH 9.5 with orange pigmentation was isolated by the DNAzol method <REF-10> .,Bacterial genomic DNA was isolated by the DNAzol method <REF-19> .,"Modify,Fact/Evidence",Fact/Evidence
6152,1-62,1-62_v2_2@4,1-62_v1_2@4,"Further, we studied the F1FoATP synthase a- subunit, with reference to alkaliphile specific domains, of one of the facultative alkaliphiles, Stenotrophomonas sp. DL18.","One facultative alkaliphile, Stenotrophomonas species DL18, was studied for F 1 F o ATP synthase a-subunit with reference to alkaliphile-specific domains.","Modify,Clarity",Clarity
6153,1-62,1-62_v2_11@1,1-62_v1_14@1,"Briefly, a PCR reaction mixture of 100 µl was prepared: 10x PCR buffer, 50 mM MgCl 2 , 10 mM dNTP mix, 100 picomoles each of forward and reverse primers, 5 units of Taq DNA polymerase (Chromous Biotech, India) with Pfu and 200 ng of bacterial genomic DNA template in nuclease free water.","Briefly, PCR reaction mixture of 100 µl was prepared as 10x PCR Rxn Buffer (Invitrogen), 50mM MgCl 2 (Invitrogen) 1.8 µl, 10 mM dNTP mix 3.0 µl (Merck), 100 picomoles of each forward and reverse primers (Integrated DNA technologies, USA), 5U Taq DNA polymerase (Invitrogen) with pfu (Chromous biotech, India) and bacterial genomic DNA templates 200 ng, with remaining nuclease free water (Merck, India).","Modify,Fact/Evidence",Fact/Evidence
6154,1-62,1-62_v2_11@3,1-62_v1_14@3,A final extension was carried out at 72°C for 5 min.,Final extension was carried out at 72°C for 5 min and stored at 4°C.,"Modify,Fact/Evidence",Fact/Evidence
6155,1-62,1-62_v2_10@0,1-62_v1_15@0,PCR and DNA sequencing of ATP synthase a -subunit of Stenotrophomonas species DL18,DNA sequencing and analysis,"Modify,Other",Other
6156,10-25,10-25_v2_24@1,,"After the acclimation, juvenile silver pompano with mean body weight of 8.56 ± 0.18 g were randomly distributed into 15 similar 20-L plastic jars filled with 15-L water (10 fish per jar with three replication per treatment).",,"Add,Fact/Evidence",Fact/Evidence
6157,10-25,10-25_v2_29@3,,"Goblet cells, congestion and hemorrhage in the intestinal tissues among treatments was compared.",,"Add,Fact/Evidence",Fact/Evidence
6158,10-25,10-25_v2_55@5,,"Furthermore, this decrease might be due to the feeding habits of carnivorous fish such as silver pompano which is lower amylase activity than protease and lipase enzymes.",,"Add,Claim",Claim
6159,10-25,10-25_v2_3@1,10-25_v1_3@1,The diets were given to juvenile silver pompano (with average body weight of 8.56 ± 0.18 g) and stocked in 15 similar 20-L plastic jars with 10 fish per jar in a density of 100 L capacity container.,The diets were given to juvenile silver pompano (with average body weight of 8.56 ± 0.18 g) and stocked with 10 fish in a 100 L capacity container.,"Modify,Fact/Evidence",Fact/Evidence
6160,10-25,10-25_v2_24@0,10-25_v1_24@0,"The juvenile silver pompano were purchased from Batam Marine Aquaculture Center (BPBL) located in Setoko islands, Batam city were placed and acclimatised in 400- L plastic tanks for a week.","Ten juvenile silver pompano were purchased from Batam Marine Aquaculture Center (BPBL) located in Setoko islands, Batam city with mean weight of 8.56±0.18 g were placed in 100 L plastic tanks.","Modify,Fact/Evidence",Fact/Evidence
6161,10-25,10-25_v2_58@2,10-25_v1_58@2,Chor et al. <REF-24> also stated that the lipase and protease activity of omnivorous fish was higher than amylase.,Chor et al. <REF-24> also stated that the lipase and protease activity of omnivorous fish including star pomfret was higher than amylase.,"Modify,Fact/Evidence",Fact/Evidence
6162,10-25,10-25_v2_68@0,10-25_v1_68@0,"The use of FFM in feed resulted in increased activity of digestive enzymes (protease, lipase) in silver pompano.","The use of FFM in feed resulted in increased activity of digestive enzymes (protease, lipase, and amylase) in silver pompano.","Modify,Fact/Evidence",Fact/Evidence
6163,10-25,10-25_v2_2@3,10-25_v1_2@3,This study aimed to determine the digestibility of fermented feather meal (FFM) in silver pompano ( Trachinotus blochii ) diets and to observe the histological structure of their intestines after digestion.,This study aimed to determine the digestibility of fermented feather meal (FFM) in silver pompano diets and to observe the histological structure of their intestines after digestion.,"Modify,Clarity",Clarity
6164,10-77,10-77_v2_14@2,,"At each place, a 50-minute sampling effort was made by two people in a single visit.",,"Add,Fact/Evidence",Fact/Evidence
6165,10-77,10-77_v2_20@1,,The google earth Pro 7.3.3.7786 polygon was used to determine the sampled area of each zone.,,"Add,Fact/Evidence",Fact/Evidence
6166,10-77,10-77_v2_39@0,10-77_v1_39@0,"The population density of A. fulica reported in this study was between 0.0019–0.6818 ind/m 2 , these density values are similar to those obtained by De la Ossa et al .","The population density of A. fulica reported in this study was between 0.0019–0.6818 ind/m <REF-2> , these density values are similar to those obtained by De la Ossa et al .","Modify,Fact/Evidence",Fact/Evidence
6167,10-77,10-77_v2_39@2,10-77_v1_39@2,"Other similar values were reported in a study performed in Ilha Porchat, Brazil, obtaining a density of 0.07 ind/m 2 <REF-3> .","Other similar values were reported in a study performed in Ilha Porchat, Brazil, obtaining a density of 0.07 ind/m <REF-2> <REF-3> .","Modify,Fact/Evidence",Fact/Evidence
6168,10-77,10-77_v2_39@3,10-77_v1_39@3,"Investigations such as those carried out in Havana, Cuba, showed a considerably lower density of snails (0.00015 ind/m 2 ) <REF-26> .","Investigations such as those carried out in Havana, Cuba, showed a considerably lower density of snails (0.00015 ind/m <REF-2> ) <REF-25> .","Modify,Fact/Evidence",Fact/Evidence
6169,10-77,10-77_v2_39@4,10-77_v1_39@4,"In contrast, densities of 1.1–4.6 ind/m 2 have been reported in different departments of Colombia <REF-27> , 0.06–8 ind/m 2 in Northeast Brazil <REF-28> , and an average of 8.4 ind/m 2 in Puyo, Ecuador <REF-22> .","In contrast, densities of 1.1–4.6 ind/m <REF-2> have been reported in different departments of Colombia <REF-26> , 0.06–8 ind/m <REF-2> in Northeast Brazil <REF-27> , and an average of 8.4 ind/m <REF-2> in Puyo, Ecuador <REF-21> .","Modify,Fact/Evidence",Fact/Evidence
6170,10-77,10-77_v2_39@5,10-77_v1_39@5,"On the other hand, studies performed in Puerto Iguazú y Corrientes, Argentina, recorded a much higher average density, which reaches 107.6 ind/m 2 and 118.6 ind/m 2 , respectively <REF-15> , <REF-29> .","On the other hand, studies performed in Puerto Iguazú y Corrientes, Argentina, recorded a much higher average density, which reaches 107.6 ind/m <REF-2> and 118.6 ind/m <REF-2> , respectively <REF-14> , <REF-28> .","Modify,Fact/Evidence",Fact/Evidence
6171,10-77,10-77_v2_39@6,10-77_v1_39@6,"It has been mentioned that the areas highly affected by the giant African snail present densities of 10 ind/m 2 or more <REF-12> , <REF-30> ; this similarly occurs with regards to biomass, where devastating values of up to 780 kg/ha are estimated for areas which are highly affected <REF-12> , <REF-31> .","It has been mentioned that the areas highly affected by the giant African snail present densities of 10 ind/m <REF-2> or more <REF-11> , <REF-29> ; this similarly occurs with regards to biomass, where devastating values of up to 780 kg/ha are estimated for areas which are highly affected <REF-11> , <REF-30> .","Modify,Fact/Evidence",Fact/Evidence
6172,10-77,10-77_v2_9@0,10-77_v1_9@0,"In Colombia, the presence of A. fulica was registered for the first time in 2009 <REF-11> , since then the mollusk has been distributed in more than 20 departments, registering in all regions of the country <REF-12> .","In Colombia, the presence of A. fulica was registered for the first time in 2010, since then the mollusk has been distributed in more than 20 departments, registering in all regions of the country <REF-11> .","Modify,Fact/Evidence",Fact/Evidence
6173,10-77,10-77_v2_14@1,10-77_v1_14@1,The sampling consisted of visiting the zones within the neighborhoods of the city Cartagena previously informed by citizens affected by the presence of the snail in their homes and surroundings.,The sampling consisted of visiting the sites within the city of Cartagena previously informed by citizens affected by the presence of the snail in their homes and surroundings.,"Modify,Clarity",Clarity
6174,10-77,10-77_v2_14@5,10-77_v1_14@4,"As this was a monitoring study, all the snails available during collections were included for analyzes, no exclusion criteria were applied for the collection of the snails.","As this was a monitoring study, all the snails available in the study period were included for analyzes, no exclusion criteria were applied for the collection of the snails.","Modify,Clarity",Clarity
6175,10-77,10-77_v2_24@0,10-77_v1_24@0,"During the period studied 204 snails were collected, distributed in four zones of four Cartagena neighborhoods, for which the presence of A. fulica was indicated by the community ( Figure 1 ).","During the period studied 204 snails were collected, distributed in four sites in Cartagena for which the presence of A. fulica was indicated by the community ( Figure 1 ).","Modify,Clarity",Clarity
6176,10-77,10-77_v2_24@1,10-77_v1_24@1,"The inspected sites where the snails were collected corresponded principally to extensive gardens and other spaces inside of private properties and residential condominiums (Manga, Las Gavias, and Serena del Mar).","The inspected sites where the snails were collected corresponded essentially to the gardens of private properties and residential condominiums (Manga, Las Gavias, and Serena del Mar).","Modify,Clarity",Clarity
6177,10-77,10-77_v2_24@2,10-77_v1_24@2,"At these sites the snails were found in different substrates, such as plants, tree roots, grasses, under litter, flowerpots, and ornamental plants; In Zaragocilla, snails were found aggregated in rubble inside an educational institute.","At these sites the snails were found in different substrates, such as plants, tree roots, grasses, under litter, flowerpots and ornamental plants; In Zaragocilla, aggregates were found in the rubble.","Modify,Clarity",Clarity
6262,2-147,2-147_v2_11@4,,Approval for the use of clinical materials was obtained from the local Ethical Review Committee.,,"Add,Fact/Evidence",Fact/Evidence
6263,2-147,2-147_v2_15@7,,Curcumin was not used to pretreat the cells and explants prior to the addition of IL-1β.,,"Add,Fact/Evidence",Fact/Evidence
6264,2-147,2-147_v2_15@8,,Curcumin and IL-1β were added simultaneously to the cultures.,,"Add,Fact/Evidence",Fact/Evidence
6265,2-147,2-147_v2_26@1,2-147_v1_26@1,"Cartilage discs were digested in papain (Sigma-Aldrich, Gillingham, UK) for 16 hours.","Cartilage discs were digested in papain(Sigma-Aldrich, Gillingham, UK) for 16 hours.","Modify,Grammar",Grammar
6266,2-147,2-147_v2_30@6,2-147_v1_30@6,Densitometric quantification of MMP-3 bands was performed using ImageJ software.,Densitometric quantification of MMP-3 bands was performed using Image J software.,"Modify,Grammar",Grammar
6267,2-147,2-147_v2_66@0,2-147_v1_66@0,"In summary, although this study found that curcumin at concentrations of 25μM and above is cytotoxic to monolayer chondrocytes after five days in culture, lower concentrations effectively antagonize PG and PGE 2 release in vitro and exert a potent anti-inflammatory effect on cartilage explants treated with IL-1β.","In summary, although this study found that curcumin at concentrations of 25μM and above is cytotoxic to monolayer chondrocytes after five days in culture, lower concentrations effectively antagonize PG and PGE2 release in vitro and exert a potent anti-inflammatory effect on cartilage explants treated with IL-1β.","Modify,Grammar",Grammar
6268,2-147,2-147_v2_11@2,2-147_v1_11@1,The joint tissues were sourced from UK-based abattoirs and veterinary practices.,The joint tissues were sourced from two UK-based abattoirs.,"Modify,Fact/Evidence",Fact/Evidence
6269,2-147,2-147_v2_11@3,2-147_v1_11@2,Animals were euthanized for non-research purposes either in accordance with Welfare of Animals (Slaughter or Killing) Regulations 1995 or the Veterinary Surgeons Act with owner consent.,Animals were euthanized for non-research purposes having been stunned before slaughter for meat in accordance with Welfare of Animals (Slaughter or Killing) Regulations 1995 .,"Modify,Fact/Evidence",Fact/Evidence
6270,2-147,2-147_v2_17@6,2-147_v1_17@6,"Live and dead cells were counted with ImageJ Software (National Institutes of Health, Bethesda, MD) and the percentage of dead cells (expressed as a percentage of the total number of cells present) was calculated at 24 hours, 48 hours and five days for each treatment.","Live and dead cells were counted with Image J Software (National Institutes of Health, Bethesda, MD) and the percentage of dead cells (expressed as a percentage of the total number of cells present) was calculated at 24 hours, 48 hours and five days for each treatment.","Modify,Grammar",Grammar
6271,2-155,2-155_v2_9@2,,"Within each category of chicken purchased, we collected at least four samples of each brand.",,"Add,Fact/Evidence",Fact/Evidence
6272,2-155,2-155_v2_19@4,,"Over half of all strains collected exhibited resistance to one or more antibiotics: 55%, 58%, 60%, and 76% from conventional, RWA, organic, and kosher chicken samples, respectively.",,"Add,Fact/Evidence",Fact/Evidence
6273,2-155,2-155_v2_29@1,,"Based on a national survey conducted by the USDA of poultry and hog producers in the United States, use of antibiotics at sub-therapeutic levels for growth promotion is common <REF-35> , <REF-36> .",,"Add,Fact/Evidence",Fact/Evidence
6274,2-155,2-155_v2_29@2,,"One estimate places growth promotion in livestock production as the single largest sector in which antibiotics are used in the US, accounting for 70% of the total of 50 million pounds for the year 2008 <REF-37> .",,"Add,Fact/Evidence",Fact/Evidence
6275,2-155,2-155_v2_30@0,,"Our finding that brands within categories did not differ significantly in the extent of antibiotic resistant E. coli ( Table 1 ) could arise from the fact that individual brands of chicken obtain product from multiple farms whose production practices may differ, obscuring clear patterns associated with individual brands.",,"Add,Claim",Claim
6276,2-155,2-155_v2_30@1,,Our ability to detect an effect of brand might also be constrained by low statistical power.,,"Add,Claim",Claim
6277,2-155,2-155_v2_29@0,2-155_v1_29@0,"Poultry growers use antibiotics both for therapeutic purposes and for growth promotion <REF-33> , <REF-34> .","Antibiotic use is widespread in the production of chicken both for therapeutic and non-therapeutic purposes (e.g., growth promotion).","Modify,Fact/Evidence",Fact/Evidence
6278,2-155,2-155_v2_29@3,2-155_v1_29@1,"The use of antibiotics in poultry production can select for antibiotic-resistant microorganisms including Salmonella , Campylobacter , Enterococcus , and extra-intestinal pathogenic E. coli <REF-38> .","The use of antibiotics in poultry production selects for antibiotic-resistant microorganisms including Salmonella , Campylobacter , Enterococcus , and extra-intestinal pathogenic E. coli <REF-33> .","Modify,Clarity",Clarity
6279,2-155,2-155_v2_35@2,2-155_v1_35@2,Our final sample size was limited (n=184) but not atypical for the field <REF-48> – <REF-51> .,Our sample size was limited (n=184) but not atypical for the field <REF-43> – <REF-46> .,"Modify,Clarity",Clarity
6280,2-155,2-155_v2_2@1,2-155_v1_2@1,"Consumers have a range of choices for poultry, including conventional, organic, kosher, and raised without antibiotics (RWA) – designations that are perceived to indicate differences in quality and safety.","Consumers have a range of choices for poultry, including conventional, organic, kosher, and raised without antibiotics (RWA)-designations that are perceived to indicate differences in quality and safety.","Modify,Grammar",Grammar
6281,2-155,2-155_v2_12@3,2-155_v1_12@3,"The plate was incubated at 37°C for 2 h and then at 44°C for 22 h, along with QA/QC strains ATCC E. coli 35218, Klebsiella pneumoniae , Hafnia alvei , Citrobacter freundii and Serratia plymuthica.","The plate was incubated at 37°C for 2 h and then at 44°C for 22 h, along with QA/QC strains ATCC E. coli 35218, Klebsiella pneumoniae , Hafnia alvei , Citrobacter freundii, and Serratia plymuthica.","Modify,Grammar",Grammar
6282,2-173,2-173_v2_31@8,,The mechanism by which 1-ABT inhibits esterases is not known.,,"Add,Claim",Claim
6283,2-173,2-173_v2_63@2,,"Because we have not been able to evaluate this mechanism more thoroughly in primary lung cells, particularly from asthmatic subjects, the physiological and/or clinical relevance of the present study in steroid insensitive patients requires further investigation.",,"Add,Claim",Claim
6284,2-173,2-173_v2_62@2,2-173_v1_62@2,"As such, additional studies using animal models and relevant samples from human patients need to be evaluated in order to conclusively confirm or reject the hypothesis that CYP3A genes are regulated in human subjects, particularly asthmatics, in response to glucocorticoid treatment since current in vitro models remain unexplainably limited in value for such studies.","As such, additional studies using animal models and relevant samples from human patients need to be evaluated in order to conclusively confirm or reject the hypothesis that CYP3A genes are regulated in human lung cells in response to glucocorticoid treatment since current in vitro models remain unexplainably limited in value for such studies.","Modify,Claim",Claim
6285,2-173,2-173_v2_63@0,2-173_v1_63@0,"In summary, the data presented herein demonstrate that, in A549 cells, glucocorticoid binding to the glucocorticoid receptor regulates the expression of CYP3A5.","In summary, the data presented herein demonstrate that, in A549 cells, glucocorticoid binding to the glucocorticoid receptor regulates the expression of CYP3A5, and therefore, corroborates the hypothesis that increased metabolism of glucocorticoids may occur in some patients.","Modify,Claim",Claim
6286,2-173,2-173_v2_63@1,2-173_v1_63@1,"However, further research is needed to determine if changes in CYP3A5 expression occur in the human respiratory tissue similar to A549 cells, the precise mechanism by which this process occurs, and whether changes in the local metabolism of glucocorticoids by CYP3A5 ultimately impact glucocorticoid efficiency in asthma patients refractory to glucocorticoid treatment.","However, further research is needed to determine if changes in CYP3A5 expression occur in the human respiratory tissue similar to A549 cells, the precise mechanism by which this process occurs, and whether changes in the local metabolism of glucocorticoids by CYP3A5 ultimately impact glucocorticoid efficiency.","Modify,Claim",Claim
6287,2-173,2-173_v2_16@6,2-173_v1_16@6,All cells except A549 cells were plated in 12-well plates pre-coated with LHC basal medium (Life Technologies) and cultured in the presence of hydrocortisone.,All cells except A549 cells were plated in 12-well plates pre-coated with LHC basal medium (Life Technologies).,"Modify,Fact/Evidence",Fact/Evidence
6327,2-238,2-238_v2_32@6,,The assay results are reproducible in three independent experiments.,,"Add,Fact/Evidence",Fact/Evidence
6328,2-238,2-238_v2_39@0,,Statistical analysis,,"Add,Other",Other
6329,2-238,2-238_v2_40@0,,"All values are expressed as mean ± standard deviation and the graphs were generated using Graph-Pad Prism ® (Version 4) for Windows (GraphPad Software, San Diego, California, USA.",,"Add,Fact/Evidence",Fact/Evidence
6330,2-238,2-238_v2_40@1,,"Statistical analysis was performed by one-way analysis of variance (ANOVA), followed by Bonferroni multiple comparison test for all parameters.",,"Add,Fact/Evidence",Fact/Evidence
6331,2-238,2-238_v2_40@2,,Results were considered statistically significant at P < 0.05.,,"Add,Fact/Evidence",Fact/Evidence
6332,2-238,2-238_v2_68@0,,Pathogenic fungi are increasingly responsible for life threatening infections in the elderly and immunocompromised patients.,,"Add,Claim",Claim
6333,2-238,2-238_v2_68@1,,"While some species have intrinsic resistance to anti-fungals, others develop resistance during the course of treatment.",,"Add,Claim",Claim
6334,2-238,2-238_v2_68@2,,Increasing antifungal resistance and treatment failures in patients is becoming a challenge.,,"Add,Claim",Claim
6335,2-238,2-238_v2_69@0,,The Candida genome encodes at least 3 distinct classes of histone deacetylases in addition to sirtuins.,,"Add,Claim",Claim
6336,2-238,2-238_v2_69@1,,"There are 8 different histone deacetylases ( HOS1, HOS2, HOS3, HDA1, HDA2, HDA3, RPD3, RPD31 ) which all have distinct roles in the morphogenesis of C. albicans .",,"Add,Claim",Claim
6337,2-238,2-238_v2_75@1,,"The fact that, the recombinant Hos2 enzyme did not show any inhibition with the Class I inhibitor MS-275 led us to explore alternate substrates including tubulins, which are substrates for Class II histone deacetylases.",,"Add,Fact/Evidence",Fact/Evidence
6338,2-238,2-238_v2_75@4,,It has been shown that microtubules in the fungal hyphae drive nuclear dynamics and cell cycle progression to morphogenesis <REF-34> .,,"Add,Fact/Evidence",Fact/Evidence
6339,2-238,2-238_v2_75@5,,"In view of the fact that Hos2 seems to preferentially deacetylate tubulins, it would be interesting to see if Hos2 inhibitors would act as anti-fungals, either as a monotherapy or in synergy, with existing anti-tubulin agents such as benomyl, nocodazole etc.",,"Add,Claim",Claim
6340,2-238,2-238_v2_78@0,,Data availability,,"Add,Other",Other
6341,2-238,2-238_v2_79@0,,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2014 Karthikeyan G et al.,,"Add,Fact/Evidence",Fact/Evidence
6342,2-238,2-238_v2_80@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",,"Add,Fact/Evidence",Fact/Evidence
6343,2-238,2-238_v2_22@1,2-238_v1_22@1,"Enzymatic assay was carried out in triplicates using the fluorogenic Class I HDAC substrate, Boc-Lys (Ac)-AMC (Cat. No. I1875, Bachem AG, Bubendorf, Switzerland).","Enzymatic assay was carried out using the fluorogenic Class I HDAC substrate, Boc-Lys (Ac)-AMC (Cat. No. I1875, Bachem AG, Bubendorf, Switzerland).","Modify,Fact/Evidence",Fact/Evidence
6344,2-238,2-238_v2_22@2,2-238_v1_22@2,"Briefly, 0.5 μg of purified recombinant protein in a volume of 10 μl of assay buffer was incubated with 50 μl of appropriate concentration of HDAC inhibitors and 20 μM of substrate at 37°C for 1 hr in 100 μl reaction volume.","Briefly, 0.5 μg of purified recombinant protein in a volume of 10 μl of assay buffer was incubated with 50 μl of appropriate concentration of HDAC inhibitors and 20 μM of substrate at 37°C for 1 hr.","Modify,Fact/Evidence",Fact/Evidence
6345,2-238,2-238_v2_24@0,2-238_v1_24@0,"Female BALB/c mice were purchased from The Jackson Laboratory (Bar Harbor, ME) at 4 weeks of age and then housed at the animal facility, Orchid Chemicals and Pharmaceuticals for 2 weeks in a specific-pathogen free facility with a 12 h light cycle (6 am–6 pm) and a 12 h dark cycle (6 pm–6 am).","Female BALB/c mice were purchased from The Jackson Laboratory (Bar Harbor, ME) at 4 weeks of age and then housed at the animal facility, Orchid chemicals and Pharmaceuticals for 2 weeks in a specific-pathogen free facility with a 12 h light cycle (6 am–6 pm) and a 12 h dark cycle (6 pm–6 am).","Modify,Grammar",Grammar
6346,2-238,2-238_v2_30@0,2-238_v1_30@0,"C. albicans ATCC 90028 mycelia (~5 gm wet weight) were washed with water, centrifuged at 10,000 rpm for 10 minutes at 4°C and the mycelial pellet was resuspended in 50 ml of 0.1 mM Tris-HCl, pH 9.4, 10 mM DTT.","C. albicans ATCC 90028 mycelia (~ 5 gm wet weight) were washed with water, centrifuged at 10,000 rpm for 10 minutes at 4°C and the mycelial pellet was resuspended in 50 ml of 0.1 mM Tris-HCl, pH 9.4, 10 mM DTT.","Modify,Grammar",Grammar
6347,2-238,2-238_v2_2@0,2-238_v1_2@0,"Candida albicans is a mucosal commensal organism capable of causing superficial (oral and vaginal thrush) infections in immune normal hosts, but is a major pathogen causing systemic and mucosal infections in immunocompromised individuals.","Candida albicans is a mucosal commensal organism in normal individuals, but is a major pathogen causing systemic and mucosal infections in immunocompromised individuals.","Modify,Claim",Claim
6348,2-238,2-238_v2_32@3,2-238_v1_32@3,"Deacetylation assays were carried out in 100 μl reaction volume for 1 hr at 37°C in reaction buffer (50 mM Tris Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 , 1 mg/ml BSA).","Deacetylation assays were carried out for 1 hr at 37°C in reaction buffer (50 mM Tris Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 , 1 mg/ml BSA).","Modify,Fact/Evidence",Fact/Evidence
6349,2-238,2-238_v2_34@2,2-238_v1_34@2,"Deacetylation assays were carried out in 100 μl reaction volume for 3 hr at 37°C in reaction buffer (50 mM Tris-Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 ).","Deacetylation assays were carried out for 3 hr at 37°C in reaction buffer (50 mM Tris-Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 ).","Modify,Fact/Evidence",Fact/Evidence
6350,2-238,2-238_v2_4@0,2-238_v1_4@0,Candida albicans is a commensal organism found in the mucosa and gastrointestinal tract of most healthy individuals but can cause superficial (oral and vaginal thrush) infections in immune-normal hosts and severe systemic infection in immunocompromised patients <REF-1> .,"Candida albicans is a commensal organism found in the mucosa and gastrointestinal tract of most healthy individuals but can cause severe systemic/superficial infections, especially in immunocompromised patients <REF-1> .","Modify,Fact/Evidence",Fact/Evidence
6351,2-238,2-238_v2_38@0,2-238_v1_38@0,"HeLa nuclear extract (2 μg) or recombinant Hos2 enzyme (300 ng) was incubated with NAD+ (500 μM), Fluor de Lys ® −Sirt1 (Enzo lifescience), varying concentrations of resveratrol (Cat. No. 0219605205, MP Biomedicals, Ohio, USA) (30, 100, 250 and 500 μM) in presence or absence of the pan-HDAC inhibitor trichostatin, in 50 μl reaction volume at 37°C for 30 min.","HeLa nuclear extract (2 μg) or recombinant Hos2 enzyme (300 ng) was incubated with NAD+ (500 μM), Fluor de Lys ® −Sirt1 (Enzo lifescience), varying concentrations of resveratrol (Cat. No. 0219605205, MP Biomedicals, Ohio, USA) (30, 100, 250 and 500 μM) in presence or absence of the pan-HDAC inhibitor trichostatin, at 37°C for 30 min.","Modify,Fact/Evidence",Fact/Evidence
6352,2-238,2-238_v2_38@1,2-238_v1_38@1,The reaction was carried out in triplicate following manufacturer’s protocol (Enzo lifescience).,The reaction was carried out following manufacturer’s protocol (Enzo lifescience).,"Modify,Fact/Evidence",Fact/Evidence
6353,2-238,2-238_v2_45@0,2-238_v1_43@0,"The Hos2 enzyme was expressed in the baculoviral-insect cell expression system as a NH 2 -terminal hexa histidine tagged fusion protein, which is detected on Western blot as a ~52 kDa protein using our own polyclonal anti-Hos2 anti-sera raised against Hos2 protein in mice.","The Hos2 enzyme was expressed in the baculoviral-insect cell expression system as a NH 2 -terminal hexa histidine tagged fusion protein, which is detected on Western blot as a ~ 52 kDa protein using our own polyclonal anti-Hos2 anti-sera raised against Hos2 protein in mice.","Modify,Grammar",Grammar
6354,2-238,2-238_v2_45@1,2-238_v1_43@1,SDS-PAGE analysis of purified protein revealed a major band at ~52 kDa ( Figure 1A ).,SDS-PAGE analysis of purified protein revealed a major band at ~ 52 kDa ( Figure 1A ).,"Modify,Grammar",Grammar
6355,2-238,2-238_v2_49@1,2-238_v1_47@1,The total activity with Boc-Lys (ac) AMC showed the enzyme to be active in deacetylating the lysine residue and the activity increased significantly ( P < 0.05 ) with an increase in Hos2 concentration ( Figure 1B ).,The total activity with Boc-Lys (ac) AMC showed the enzyme to be active in deacetylating the lysine residue and the activity increased with an increase in Hos2 concentration ( Figure 1B ).,"Modify,Fact/Evidence",Fact/Evidence
6356,2-238,2-238_v2_5@0,2-238_v1_5@0,"Azole resistance in Candida sp. is mediated by up regulation of genes encoding ERG11 , a lanosterol demethylase <REF-4> – <REF-6> , MDR1 <REF-5> , <REF-7> and by CDR (Candida drug resistance) efflux pumps <REF-5> , <REF-7> – <REF-9> .","Azole resistance in Candida sp. is mediated by up regulation of genes encoding ERG11, a lanosterol demethylase <REF-4> – <REF-6> , MDR1 <REF-5> , <REF-7> and by CDR (Candida drug resistance) efflux pumps <REF-5> , <REF-7> – <REF-9> .","Modify,Grammar",Grammar
6357,2-238,2-238_v2_69@2,2-238_v1_67@0,"HDAC inhibitors, by virtue of their ability to prevent antifungal resistance in vitro, have been proposed as antifungal adjuvants.",HDAC inhibitors by virtue of their ability to prevent antifungal resistance in vitro have been proposed as antifungal adjuvants.,"Modify,Grammar",Grammar
6358,2-238,2-238_v2_74@2,2-238_v1_71@2,"Our studies with the Class I HDAC inhibitor MS-275 showed that this inhibitor did not inhibit Hos2 deacetylase as effectively as the pan HDAC inhibitors SAHA or TSA, suggesting that Candida Hos2 is more similar to Class II deacetylases.","Our studies with the class I HDAC inhibitor MS-275 showed that this inhibitor did not inhibit Hos2 deacetylase as effectively as the pan HDAC inhibitors SAHA or TSA, suggesting that Candida Hos2 is more similar to class II deacetylases.","Modify,Grammar",Grammar
6359,2-238,2-238_v2_75@3,2-238_v1_72@2,"Hos2 in essence resembles the Class II mammalian HDACs, specifically HDAC6 in its preference for tubulin deacetylation.","Hos2 in essence resembles the class II mammalian HDACs, specifically HDAC6 in its preference for tubulin deacetylation.","Modify,Grammar",Grammar
6360,2-238,2-238_v2_76@6,2-238_v1_73@6,"We did not observe any significant ( P value 0.5317 and 0.4411, in the presence and absence of trichostatin respectively) activation of NAD+ dependent deacetylase activity with the fluor-de-lys substrate.",We did not observe any significant activation of NAD+ dependent deacetylase activity with the fluor-de-lys substrate.,"Modify,Fact/Evidence",Fact/Evidence
6361,2-238,2-238_v2_7@1,2-238_v1_7@1,"For example the HDAC Class I specific inhibitor MS-275 is in advanced clinical trials (clinical trial Nos. NCT00020579, NCT00866333) for several forms of cancer, and the HDAC Class II specific inhibitor ACY-1215 is at an advanced clinical phase (clinical trial Nos. NCT01323751 , NCT01583283 ) for myeloma.","For example the HDAC Class I specific inhibitor MS-275 is in advanced clinical trials (clinical trial Nos. NCT00020579, NCT00866333) for several forms of cancer, and the HDAC class II specific inhibitor ACY-1215 is at an advanced clinical phase (clinical trial Nos. NCT01323751 , NCT01583283 ) for myeloma.","Modify,Grammar",Grammar
6362,2-238,2-238_v2_9@0,2-238_v1_9@0,"Hos2, a Class I HDAC enzyme plays an important role in gene activation in the yeast Saccharomyces cerevisiae by binding to open reading frames (ORFs) of active genes <REF-20> .","Hos2, a class I HDAC enzyme plays an important role in gene activation in the yeast Saccharomyces cerevisiae by binding to open reading frames (ORFs) of active genes <REF-20> .","Modify,Grammar",Grammar
6363,2-238,2-238_v2_2@5,2-238_v1_2@5,"Inhibition studies showed that Hos2 is susceptible to pan inhibitors such as trichostatin A (TSA) and suberoylanilide hydroxamic acid (SAHA), but is not inhibited by class I inhibitors such as MS-275.","Inhibition studies showed that Hos2 is susceptible to pan inhibitors such as trichostatin A (TSA) and suberoylanilide hydroxamic acid (SAHA), but is not inhibited by class I inhibitors such MS-275.","Modify,Grammar",Grammar
6364,2-242,2-242_v2_54@2,2-242_v1_54@2,"Mutations in several human genes have now been identified as causative of abnormal heart looping, such as ACVR2B , LEFTY2 , GJA1 and ZIC3 <REF-49> – <REF-52> , and some of the ‘jogging ortholog’ genes ( CCDC103 , CCDC40 , DNAAF1 , LRRC6 , NPHP3 and PKD2 ) are also associated with heart looping defects.","Although, mutations in several human genes have now been identified as causative of abnormal heart looping, such as ACVR2B , LEFTY2 , GJA1 and ZIC3 <REF-49> – <REF-52> , only a few of the ‘jogging ortholog’ genes, CCDC103 , CCDC40 , DNAAF1 , LRRC6 , NPHP3 and PKD2 , are associated with heart looping defects, and thus provide evidence which suggests an involvement of these genes in left-right asymmetry determination in the heart.","Split+Modify,Clarity",Clarity
6365,2-242,2-242_v2_54@3,2-242_v1_54@2,Thus providing evidence to support an involvement of these genes in left-right asymmetry determination in the heart.,"Although, mutations in several human genes have now been identified as causative of abnormal heart looping, such as ACVR2B , LEFTY2 , GJA1 and ZIC3 <REF-49> – <REF-52> , only a few of the ‘jogging ortholog’ genes, CCDC103 , CCDC40 , DNAAF1 , LRRC6 , NPHP3 and PKD2 , are associated with heart looping defects, and thus provide evidence which suggests an involvement of these genes in left-right asymmetry determination in the heart.","Split+Modify,Clarity",Clarity
6366,2-242,2-242_v2_54@5,2-242_v1_54@4,"However, there are other possible reasons why there is a poor association of heart defects with the ‘jogging ortholog’ gene list.","However, there are numerous other reasons why there is a poor association of heart defects with the ‘jogging ortholog’ gene list.","Modify,Clarity",Clarity
6367,2-242,2-242_v2_13@0,2-242_v1_13@0,A list of 30 zebrafish genes that affect heart jogging was compiled using a variety of approaches.,A list of 30 zebrafish heart jogging genes was compiled using a variety of approaches.,"Modify,Clarity",Clarity
6368,2-242,2-242_v2_13@1,2-242_v1_13@1,"Twelve zebrafish proteins were identified as they were already annotated to the ‘heart jogging’ GO terms, the remaining 18 proteins were then identified using the ZFIN ( http://zfin.org/ ) Site Search, with the search phrase 'heart jogging', and filtering using the 'Expression/Phenotypes' category.","Twelve zebrafish proteins were identified as they were already annotated to the ‘heart jogging’ GO terms, a further 18 proteins were then identified using the ZFIN database, using a keyword search (heart jogging).","Modify,Fact/Evidence",Fact/Evidence
6369,2-242,2-242_v2_13@2,2-242_v1_13@2,"This search retrieves figures from papers that have ‘heart jogging’ in the figure legend, and thus are likely to be describing specific zebrafish genes (and proteins) involved in this process.","The ZFIN ( http://zfin.org/ ) browser searches figure legends of papers that are known to describe specific zebrafish genes (and proteins), but which have not yet been curated with GO terms.","Split+Modify,Fact/Evidence",Fact/Evidence
6370,2-242,2-242_v2_13@3,2-242_v1_13@2,Many of these genes had not yet been curated with GO terms.,"The ZFIN ( http://zfin.org/ ) browser searches figure legends of papers that are known to describe specific zebrafish genes (and proteins), but which have not yet been curated with GO terms.","Split+Modify,Clarity",Clarity
6371,2-242,2-242_v2_13@4,2-242_v1_13@3,"Each of the papers identified in this way were reviewed; of the 23 zebrafish genes identified in these papers five (Bmpr1aa, Tbx1, unm_hu119, unm_hu202, unm_hu304) were eliminated, as none of these papers provided experimental evidence for the involvement of these genes in heart jogging.","This search identified a further 23 zebrafish genes, however manual review of these publications led to 5 being disregarded, as the evidence for an involvement in heart jogging was not strong enough.","Modify,Fact/Evidence",Fact/Evidence
6372,2-259,2-259_v2_8@6,,"The immunological screening included antinuclear antibodies, anti-smooth muscles antibodies, anti-mitochondria antibodies, anti LKM antibodies, anti-hepatic cytosol antibodies, complement (C3, C4, CH50), rheumatoid factor, antineutrophil cytoplasmic antibody (ANCA), antiganglioside antibodies (GM1, GM2, GD1a, GD1b, GQ1b) and onconeuronal antibodies (Hu, Ri, Yo, PNMA2, CV2, Amphiphysine).",,"Add,Fact/Evidence",Fact/Evidence
6373,2-259,2-259_v2_8@8,,The prothrombin time stayed within the normal range throughout the monitoring period.,,"Add,Fact/Evidence",Fact/Evidence
6374,2-259,2-259_v2_10@3,,Ribavirin treatment was discontinued after 35 days.,,"Add,Fact/Evidence",Fact/Evidence
6375,2-259,2-259_v2_13@2,,The present case concerned genotype 3f virus that is predominant in France <REF-8> .,,"Add,Fact/Evidence",Fact/Evidence
6376,2-259,2-259_v2_13@4,,"Moreover, several authors suggest treating severe acute HEV infections in order to preclude the development of acute liver failure <REF-9> .",,"Add,Fact/Evidence",Fact/Evidence
6377,2-259,,2-259_v1_9@6,,"Antiganglioside antibodies were negative (GM1, GM2, GD1a, GD1b, GQ1b).","Delete,Fact/Evidence",Fact/Evidence
6378,2-259,2-259_v2_8@0,2-259_v1_8@0,"We report the case of a 36-year-old French man, Caucasian truck driver, without any significant medical history.","We report the case of a 36-year-old French, Caucasian truck driver, without any significant medical history.","Modify,Clarity",Clarity
6379,2-259,2-259_v2_0@0,2-259_v1_0@0,Case Report: Severe bilateral amyotrophic neuralgia associated with major dysphagia secondary to acute hepatitis E,Severe bilateral amyotrophic neuralgia associated with major dysphagia secondary to acute hepatitis E,"Modify,Other",Other
6380,2-259,2-259_v2_9@5,2-259_v1_9@5,"The cerebrospinal fluid (CSF) was normal (2 white blood cells/mm3; CSF Protein= 0.37 g/L) and there was no intrathecal antibody synthesis, although it was not tested for the synthesis of specific anti-HEV antibodies.",The cerebrospinal fluid (CSF) was normal (2 white blood cells/mm 3 ; CSF Protein= 0.37 g/L) and there was no intrathecal antibody synthesis.,"Modify,Fact/Evidence",Fact/Evidence
6381,2-259,2-259_v2_9@9,2-259_v1_9@10,The PCR was negative in the CSF and was not initially performed in stools.,The PCR was negative in the CSF.,"Modify,Fact/Evidence",Fact/Evidence
6382,2-259,2-259_v2_10@0,2-259_v1_10@0,"A treatment with intravenous immunoglobulins (Tegeline ® , LFB laboratory, France; 0.4 g/kg/day) was given for 5 days.","A treatment with intravenous immunoglobulins (Tegeline®, LFB laboratory, France; 0.4 g/kg/day) was given for 5 days.","Modify,Grammar",Grammar
6383,2-259,2-259_v2_10@2,2-259_v1_10@2,"Nine days after ribavirin initiation, the PCR showed 2.02 log-copies/ml and was negative after 18 days in both blood and stools.","Nine days after ribavirin initiation, the PCR showed 2.02 log-copies/ml and was negative after 18 days.","Modify,Fact/Evidence",Fact/Evidence
6384,2-259,2-259_v2_12@6,2-259_v1_12@6,The only proposed treatment (with a low level of evidence) is corticosteroids but this may have been dangerous in this case of acute hepatitis E <REF-3> .,The only validated treatment is corticosteroids but this may have been dangerous in this case of acute hepatitis E <REF-3> .,"Modify,Claim",Claim
6385,2-259,2-259_v2_16@0,2-259_v1_16@0,Post-infectious neurological diseases following HEV infection must be recognized to avoid unnecessary and potentially invasive procedures such as liver biopsy.,Post-infectious neurological diseases following HEV infection must be recognized to avoid unnecessary and potentially invasive procedures.,"Modify,Claim",Claim
6386,2-260,2-260_v2_3@0,,Introduction,,"Add,Other",Other
6387,2-260,2-260_v2_5@0,,"Ever since Jensen emphasized the role of promiscuity or ‘substrate ambiguity’ in evolution through ‘fortuitous error and gain of multistep pathways’, promiscuity in proteins has been the subject of intense and detailed research <REF-7> .",,"Add,Fact/Evidence",Fact/Evidence
6388,2-260,2-260_v2_5@1,,It was demonstrated in 1976 that replacing the zinc metal ion by copper in Carboxypeptidase A introduced oxidase catalysis properties <REF-11> .,,"Add,Fact/Evidence",Fact/Evidence
6389,2-260,2-260_v2_5@2,,"Dioxygenases promiscuously hydrolyse esters <REF-12> , while the enolase superfamily is also known to catalyze numerous catalytic reactions <REF-13> , <REF-14> .",,"Add,Fact/Evidence",Fact/Evidence
6390,2-260,2-260_v2_5@3,,"Alkaline phosphatases (AP), one of the key proteins in our research, are one of the most widely researched promiscuous enzymes <REF-15> .",,"Add,Fact/Evidence",Fact/Evidence
6391,2-260,2-260_v2_5@4,,"APs are known to have sulfate monoesterase, phosphate diesterase, and phosphonate monoesterase activities <REF-16> – <REF-18> .",,"Add,Fact/Evidence",Fact/Evidence
6392,2-260,2-260_v2_5@5,,"A phosphite-dependent hydrogenase activity was also found in Escherichia coli AP (ECAP), but was absent in APs from other organisms <REF-19> .",,"Add,Fact/Evidence",Fact/Evidence
6393,2-260,2-260_v2_5@6,,"Interestingly, proteins from the AP superfamily show cross activity - Pseudomonas aeruginosa arylsulfatase (PAS) which has the primary activity of hydrolyzing sulfate monoesters also catalyzes the hydrolysis of phosphate monoesters <REF-20> , <REF-21> .",,"Add,Fact/Evidence",Fact/Evidence
6394,2-260,2-260_v2_6@3,,"For example, the catalytic Ser-His-Asp triad has virtually the same geometry in the major families of serine proteases (chymotrypsin and subtilisin), which have no sequence or structural homology <REF-26> - a classical example of convergent evolution <REF-27> , <REF-28> .",,"Add,Fact/Evidence",Fact/Evidence
6395,2-260,2-260_v2_6@6,,The choice of methods for binding site comparisons and methods for binding site detection as well as function prediction has been recently reviewed in detail <REF-31> .,,"Add,Fact/Evidence",Fact/Evidence
6396,2-260,2-260_v2_6@7,,"Notably, most of these methods are based on structural properties of the binding or the active site.",,"Add,Fact/Evidence",Fact/Evidence
6397,2-260,2-260_v2_11@0,,"Another fascinating aspect of enzymes, although strictly not defined as promiscuity, is their ability to catalyze the reaction of a range of similar substrates of the same class <REF-51> .",,"Add,Fact/Evidence",Fact/Evidence
6398,2-260,2-260_v2_11@1,,"We have hypothesized that duplicate residues, each of which results in slightly modified replicas of the active site scaffold, are responsible for the broad substrate specificity of proteins <REF-52> , <REF-53> .",,"Add,Fact/Evidence",Fact/Evidence
6399,2-260,2-260_v2_15@2,2-260_v1_13@2,"For example in a catalytic site consisting of n residues, the existence of a congruent n −1 motif does not imply that it is easy or even possible to add another residue in the structure and obtain the n residue motif.","For example in a catalytic site consisting of n residues, the existence of a congruent n − 1 motif does not imply that it is easy or even possible to add another residue in the structure and obtain the n residue motif.","Modify,Grammar",Grammar
6400,2-260,2-260_v2_15@3,2-260_v1_13@3,"This complexity is best exemplified in the failure to induce β -lactamase activity in a penicillin-binding protein (PBP-5) from E. coli <REF-58> , <REF-59> by generating the L153E mutant of this protein, as proposed by our previous analysis <REF-47> (unpublished results).","This complexity is best exemplified in the failure to induce β -lactamase activity in a penicillin-binding protein (PBP-5) from E. coli <REF-43> , <REF-44> by generating the L153E mutant of this protein, as proposed by our previous analysis <REF-35> (and unpublished results).","Modify,Clarity",Clarity
6401,2-260,2-260_v2_7@3,2-260_v1_5@3,"For example, we detected the presence of the serine protease (SPASE) catalytic triad motif (Ser195, His57, Asp102) in alkaline phosphatases (AP) from various organisms using the spatial and electrostatic congruence, and validated this by inhibition of the native phosphatase activity using inhibitors (AEBSF/PMSF) <REF-32> , known to be active on many serine proteases by reaction with the nucleophilic serine <REF-36> .","For example, we detected the presence of the serine protease (SPASE) catalytic triad motif (Ser195, His57, Asp102) in alkaline phosphatases (AP) from various organisms using the spatial and electrostatic congruence, and validated this by inhibition of the native phosphatase activity using serine protease inhibitors (AEBSF/PMSF) <REF-20> .","Modify,Fact/Evidence",Fact/Evidence
6402,2-260,2-260_v2_7@5,2-260_v1_5@5,"Recently, the crown domain in the E. coli expressed rat intestinal AP protein was shown to be prone to protease cleavage, which the authors have ascribed to self-cleavage <REF-37> .","Recently, the crown domain in the Escherichia coli expressed rat intestinal AP protein was shown to be prone to protease cleavage, which the authors have ascribed to self-cleavage <REF-24> .","Modify,Clarity",Clarity
6403,2-260,2-260_v2_8@3,2-260_v1_6@3,"We thus concluded that one should exert caution before ruling out protease activity in an enzyme since theoretically proteases have a large number of possible substrates due to the possible variation in residues flanking the sissile bond, and the corresponding folds that harbor a recognition site for a particular protease <REF-39> .",We thus concluded that one should exert caution before ruling out protease activity in an enzyme since theoretically proteases have unenumerable number of possible substrates due to the infinite possible DNA sequences that can result in proteins and their corresponding infinite folds <REF-27> .,"Modify,Fact/Evidence",Fact/Evidence
6404,2-260,2-260_v2_8@4,2-260_v1_6@4,"Thus, it is possible that we have not found the ideal proteolytic substrate for APs <REF-32> .","Thus, it is possible that we have not found the correct proteolytic substrate for APs <REF-20> .","Modify,Clarity",Clarity
6405,2-260,2-260_v2_9@2,2-260_v1_8@2,"Several β -lactam compounds failed to inhibit E. coli or shrimp AP, as was expected by the lower congruence indicated by CLASP as compared to VAP.","Several β -lactam compounds failed to inhibit E. coli or shrimp AP, as was expected by the lower congruence indicated by CLASP.","Modify,Clarity",Clarity
6406,2-260,2-260_v2_10@1,2-260_v1_9@1,The search for an elastase-like motif in a plant protein <REF-47> led us to the pathogenesis-related protein P14a <REF-49> .,The search for an elastase-like motif in a plant protein led us to the pathogenesis-related protein P14a <REF-37> .,"Modify,Fact/Evidence",Fact/Evidence
6502,2-288,2-288_v2_59@6,,"“Inner” and “outer” loops provide two levels of nesting, when needed.",,"Add,Fact/Evidence",Fact/Evidence
6503,2-288,2-288_v2_79@6,,"Given that the model code has been verified for mathematical accuracy over the full range of parameter space used for the Monte Carlo evaluation, the result gives an informative measure of the degree of validity of the model.",,"Add,Fact/Evidence",Fact/Evidence
6504,2-288,2-288_v2_79@7,,"This is the first step of uncertainty quantification ( Smith, 2014 ).",,"Add,Fact/Evidence",Fact/Evidence
6505,2-288,2-288_v2_88@0,,Run-time performance,,"Add,Other",Other
6506,2-288,2-288_v2_89@0,,"Run-time performance of JSim is dependent on numerous factors: model complexity, mathematical formulation, numeric methods used, use of parallel processing, and the fineness of time and spatial grids used.",,"Add,Fact/Evidence",Fact/Evidence
6507,2-288,2-288_v2_89@1,,"PDEs generally run faster than ODEs providing similar spatial resolution even though they include diffusion terms, but in general can be slower than ODEs representing simplified geometries, and are slower using high resolution general purpose solvers like TOMS731.",,"Add,Claim",Claim
6508,2-288,2-288_v2_89@2,,A direct solver-to-solver comparison on a problem which can be formulated as either an ODE or PDE model (a convection-diffusion model) is provided in Table 4 .,,"Add,Fact/Evidence",Fact/Evidence
6509,2-288,2-288_v2_92@0,,"The times reported in Table 4 are extraordinarily variable, and depend upon parameter values and on the values of the variable themselves, as the solvers can become highly efficient if a variable is not changing.",,"Add,Claim",Claim
6510,2-288,2-288_v2_92@2,,"In general, linear systems of implicit equations run faster than non-linear systems.",,"Add,Claim",Claim
6511,2-288,2-288_v2_92@3,,"Runs requiring stiff ODE solvers (e.g. CVode, Radau) typically run slower than non-stiff solvers (e.g. Dopri5, RK4).",,"Add,Claim",Claim
6512,2-288,2-288_v2_92@4,,"Analyses involving JSim loops, sensitivity analysis, optimization and Monte Carlo methods run faster when JSim multiprocessing is activated.",,"Add,Claim",Claim
6513,2-288,2-288_v2_92@5,,Models requiring fine temporal or spatial grids to capture relevant detail run slower than those for which coarse grids are sufficient.,,"Add,Claim",Claim
6514,2-288,2-288_v2_93@0,,"Computation time equivalent to real time was shown on a laptop computer for a cardiorespiratory system model with about 120 variables ( Neal & Bassingthwaighte, 2007 ).",,"Add,Fact/Evidence",Fact/Evidence
6515,2-288,2-288_v2_93@1,,"Models available at physiome.org typically run somewhere between a fraction of a second to several minutes, depending upon these various complications.",,"Add,Fact/Evidence",Fact/Evidence
6516,2-288,2-288_v2_93@2,,Some example model simulation execution times (“run times”) are compared to the real time duration of the events being modeled (“model times”) in Table 5 .,,"Add,Fact/Evidence",Fact/Evidence
6517,2-288,2-288_v2_93@3,,All the runs below were single model runs performed on a mid-range workstation (Dell Precision T3500 Xeon x86-64 2.5GHz).,,"Add,Fact/Evidence",Fact/Evidence
6518,2-288,2-288_v2_93@4,,"The Beeler-Reuter action potential model (#78 at physiome.org) has only 4 ionic currents and 28 time-dependent variables; the ( Winslow et al. , 1999 ) model (#217) has 126 time-dependent variables.",,"Add,Fact/Evidence",Fact/Evidence
6519,2-288,2-288_v2_93@5,,"Timing calculations are unreliable, dependent on the model, computational methods, and the values of the variables, and the timestep length.",,"Add,Fact/Evidence",Fact/Evidence
6520,2-288,2-288_v2_93@6,,"For the Winslow et al. , 1999 model, using a 1 microsec timestep took 96 seconds, only 100 times slower, not 1000 times, compared to that with Δt = 1 ms.",,"Add,Fact/Evidence",Fact/Evidence
6521,2-288,2-288_v2_96@0,,Code verification,,"Add,Other",Other
6522,2-288,2-288_v2_97@0,,We use a variety of strategies to verify the JSim code stack.,,"Add,Fact/Evidence",Fact/Evidence
6523,2-288,2-288_v2_97@1,,Some calculations such as values for transcendental functions and algebraic expansion of symbolic derivatives have known closed form solutions that can be compared exactly.,,"Add,Claim",Claim
6524,2-288,2-288_v2_97@2,,Our general policy is to write the code for analytical solutions into the JSim model to use the comparison to verify specifiable cases.,,"Add,Fact/Evidence",Fact/Evidence
6525,2-288,2-288_v2_97@3,,"In simple cases, e.g. respiratory mechanics models, exponential equations match numerical solutions for 7 decimal points.",,"Add,Fact/Evidence",Fact/Evidence
6526,2-288,2-288_v2_97@4,,Some ODE and PDE models such as exponential washout have known closed form analytic solutions which can be numerically compared to solutions generated by JSim’s numeric integrators.,,"Add,Claim",Claim
6527,2-288,2-288_v2_97@5,,"Even when a complete analytic solution is not available, certain statistics of the solution, such as mean transit time in blood-tissue flow models, can be calculated analytically and compared with the same statistic calculated from the model output.",,"Add,Claim",Claim
6528,2-288,2-288_v2_98@0,,"Parameter changes to models causing output changes that don’t correspond to expectation from induction, need to be evaluated qualitatively by the user.",,"Add,Claim",Claim
6529,2-288,2-288_v2_98@1,,"When modelers observe unexpected behavior, checking at deeper levels is required.",,"Add,Claim",Claim
6530,2-288,2-288_v2_98@2,,"While most such anomalies are due to user coding errors, over the 15 years of JSim’s existence, some subtle computation bugs in JSim have been diagnosed in this manner.",,"Add,Fact/Evidence",Fact/Evidence
6531,2-288,2-288_v2_98@3,,JSim cannot be proven bug-free even though finding anomalies is now rare: queries regarding problems in computation are welcomed at staff@physiome.org .,,"Add,Fact/Evidence",Fact/Evidence
6532,2-288,2-288_v2_99@0,,JSim models are sometimes exported in SBML format and run in other SBML supporting simulators as a comparison check.,,"Add,Fact/Evidence",Fact/Evidence
6533,2-288,2-288_v2_99@1,,Models are also sometimes entirely recoded in a different computational environment (Matlab is often used) as a comparison check.,,"Add,Fact/Evidence",Fact/Evidence
6534,2-288,2-288_v2_99@2,,"Model translations to other languages (e.g. SBML, CellML, Antimony) are verified via round-tripping (exporting to the target language and then reimporting).",,"Add,Fact/Evidence",Fact/Evidence
6535,2-288,2-288_v2_100@0,,Tests of JSim’s optimization and Monte Carlo functionality are based on convergence to solutions of known parameterization.,,"Add,Fact/Evidence",Fact/Evidence
6536,2-288,2-288_v2_100@1,,The optimizers are all very different; we advocate that users try a variety of optimizers for any given problem.,,"Add,Claim",Claim
6537,2-288,2-288_v2_100@2,,"There is no magic in an optimizer; the key in fitting a model solution to data, when the data are reliable, is in designing a carefully weighted distance function to fit as many computed model variables to simultaneously obtained experimental data functions as possible.",,"Add,Claim",Claim
6538,2-288,2-288_v2_101@0,,Tests of JSim’s multi-processing are based on comparisons to single processor computations.,,"Add,Fact/Evidence",Fact/Evidence
6539,2-288,2-288_v2_102@0,,The JSim verification suite consists of over 1200 individual tests drawn from the above methodologies.,,"Add,Fact/Evidence",Fact/Evidence
6540,2-288,2-288_v2_102@1,,"The suite is expanded when new computational or translation facilities are added, or when a bug has been found and fixed.",,"Add,Fact/Evidence",Fact/Evidence
6541,2-288,2-288_v2_102@2,,Most tests consist of comparing jsbatch output with user-verified reference data.,,"Add,Fact/Evidence",Fact/Evidence
6542,2-288,2-288_v2_102@3,,The verification suite is run before every official JSim release to ensure consistency of operation.,,"Add,Fact/Evidence",Fact/Evidence
6543,2-288,2-288_v2_110@0,,Using archived models for analyzing one’s own data,,"Add,Other",Other
6544,2-288,2-288_v2_111@0,,All the archived models may be downloaded so that an experimentalist can import his own data into the project file and analyze it.,,"Add,Fact/Evidence",Fact/Evidence
6545,2-288,2-288_v2_111@1,,"The JSim Home Page is an operations manual for downloading, running models and analyzing data.",,"Add,Fact/Evidence",Fact/Evidence
6546,2-288,2-288_v2_111@2,,For parameter evaluation the number of trial optimizations can be set to 1 (so there is no optimization done) but the covariance matrix is calculated to provide estimates of confidence limits from the local linear combination of sensitivity functions.,,"Add,Fact/Evidence",Fact/Evidence
6547,2-288,2-288_v2_111@3,,"For greater generality one should set up the optimizer to evaluate the set of parameters desired, then run the Monte Carlo (tab at bottom) to repeat the optimization many times in the presence of added noise; this provides realistic probability density functions of parameter values.",,"Add,Fact/Evidence",Fact/Evidence
6548,2-288,2-288_v2_112@0,,Some Alternative Simulation Platforms,,"Add,Other",Other
6549,2-288,2-288_v2_113@0,,A comprehensive feature-by-feature analysis of alternative simulation platforms is beyond the scope of this paper.,,"Add,Fact/Evidence",Fact/Evidence
6550,2-288,2-288_v2_113@1,,"Listed below are brief descriptions of some simulation systems using procedural methods, as opposed to JSim declarative approach.",,"Add,Fact/Evidence",Fact/Evidence
6551,2-288,2-288_v2_113@2,,All can be used to fit model solutions to data.,,"Add,Fact/Evidence",Fact/Evidence
6552,2-288,2-288_v2_114@0,,"Virtual Cell ( Loew & Schaff, 2001 ) is a computational environment designed for the construction and simulation of cellular-based models.",,"Add,Fact/Evidence",Fact/Evidence
6553,2-288,2-288_v2_114@1,,"Models can be created iteratively in the GUI, or via VCell’s custom mathematical language VCMDL which supports ODEs & PDEs.",,"Add,Fact/Evidence",Fact/Evidence
6554,2-288,2-288_v2_114@2,,Both deterministic and stochastic simulations are supported.,,"Add,Fact/Evidence",Fact/Evidence
6555,2-288,2-288_v2_114@3,,Model computations are performed via client accounts on VCell’s computational server farm.,,"Add,Fact/Evidence",Fact/Evidence
6556,2-288,2-288_v2_115@0,,"COPASI ( Hoops et al. , 2006 ) (for COmplex PAthway SImulator) is an integrated modeling and simulation environment aimed at metabolic networks, cell-signaling pathways, regulatory networks, infectious diseases and similar systems.",,"Add,Fact/Evidence",Fact/Evidence
6557,2-288,2-288_v2_115@1,,Models are typically created via a table-driven GUI and results viewed via embedded graphs.,,"Add,Fact/Evidence",Fact/Evidence
6558,2-288,2-288_v2_115@2,,"COPASI supports SBML and currently runs on Linux, MacOS and Windows.",,"Add,Fact/Evidence",Fact/Evidence
6559,2-288,2-288_v2_117@0,,"Chaste ( Mirams et al. , 2013 ) is C++ library for computational physiology and biology.",,"Add,Fact/Evidence",Fact/Evidence
6560,2-288,2-288_v2_117@1,,"Computational modules include mesh generation, linear algebra, ODEs, PDEs and continuum mechanics.",,"Add,Fact/Evidence",Fact/Evidence
6561,2-288,2-288_v2_117@2,,"I/O modules provide support for various file formats, including HDF5 ( Folk et al. , 1999 ).",,"Add,Fact/Evidence",Fact/Evidence
6562,2-288,2-288_v2_117@3,,"Chaste is available for Windows, MacOS, Linux and Solaris.",,"Add,Fact/Evidence",Fact/Evidence
6563,2-288,2-288_v2_118@0,,"PCenv, COR, OpenCell and OpenCOR ( CellML, 2014 ) are a related set of tools supporting CellML modeling.",,"Add,Fact/Evidence",Fact/Evidence
6564,2-288,2-288_v2_118@1,,PCenv is an interactive modeling editing and simulation environment running on Windows.,,"Add,Fact/Evidence",Fact/Evidence
6565,2-288,2-288_v2_118@2,,COR an alternative CellML modeling environment for Windows.,,"Add,Fact/Evidence",Fact/Evidence
6566,2-288,2-288_v2_118@3,,"OpenCell is a merger of PCenv and COR built upon the Mozilla platform, and running on Linux, Windows and MacOS.",,"Add,Fact/Evidence",Fact/Evidence
6567,2-288,2-288_v2_118@4,,OpenCell development has been stopped in favor of its replacement OpenCOR.,,"Add,Fact/Evidence",Fact/Evidence
6568,2-288,2-288_v2_119@0,,"Continuity ( Continuity, 2014 ) is problem-solving environment for multi-scale modeling in bioengineering and physiology - especially biomechanics, transport and electrophysiology.",,"Add,Fact/Evidence",Fact/Evidence
6569,2-288,2-288_v2_119@1,,Finite element and PDEs are supported.,,"Add,Fact/Evidence",Fact/Evidence
6570,2-288,2-288_v2_119@2,,"The Continuity language integrates with Python ( VanRossum & Drake, 2003 ) scripts to create multi-scale models.",,"Add,Fact/Evidence",Fact/Evidence
6571,2-288,2-288_v2_119@3,,"Continuity runs on Windows, MacOS, Linux and Linux clusters.",,"Add,Fact/Evidence",Fact/Evidence
6572,2-288,2-288_v2_129@0,,"SED-ML Support: SED-ML ( Kohn & Le Novere, 2008 ) (for Simulation Experiment Description) is an emerging standard for to promote reproducibility by capturing all the details of an in silico experiment.",,"Add,Fact/Evidence",Fact/Evidence
6573,2-288,2-288_v2_129@1,,"Major entities described in SED-ML are models, simulation setup (i.e. time and numeric solver parameters), tasks (a model run with specified a simulation setup), data generators (methods for combining model outputs from different tasks) and outputs (plots & tables).",,"Add,Fact/Evidence",Fact/Evidence
6574,2-288,2-288_v2_129@2,,"JSim projects support these entities, but not in a scripted form.",,"Add,Fact/Evidence",Fact/Evidence
6575,2-288,2-288_v2_129@3,,"To support SED-ML, we are currently developing a feature called Second Level Analysis (SLA) that will allow JSim users to script common JSim activities (e.g. model runs with different parameter sets, data combinatorics, plotting and export), in a way that maps to SED-ML.",,"Add,Fact/Evidence",Fact/Evidence
6576,2-288,2-288_v2_129@4,,SED-ML files will be read into JSim as SLA scripts and run there.,,"Add,Fact/Evidence",Fact/Evidence
6577,2-288,2-288_v2_129@5,,"Conversely, JSim SLA scripts may be exported to SED-ML for use in other simulators that support SED-ML.",,"Add,Fact/Evidence",Fact/Evidence
6578,2-288,2-288_v2_132@0,,Data and software availability,,"Add,Other",Other
6579,2-288,2-288_v2_133@0,,"Zenodo: JSim downloads and models Version 2, doi: http://dx.doi.org/10.5281/zenodo.8652 ( Butterworth et al. , 2014 )",,"Add,Fact/Evidence",Fact/Evidence
6580,2-288,,2-288_v1_32@7,,"If a particular model absolutely requires procedural code, this can be developed in C, or Fortran or Java, and invoked as part of the model computation.","Delete,Fact/Evidence",Fact/Evidence
6581,2-288,,2-288_v1_78@7,,(JSim’s confidence limit calculations support modeling step 12 above.),"Delete,Fact/Evidence",Fact/Evidence
6582,2-288,,2-288_v1_88@7,,"F1000s founder, Victor Tracz, is featured as the “Seer of Science Publishing”, prodding us to do better.","Delete,Fact/Evidence",Fact/Evidence
6583,2-288,,2-288_v1_104@1,,Software is also permanently available from: 10.5281/zenodo.7635 .,"Delete,Fact/Evidence",Fact/Evidence
6584,2-288,2-288_v2_7@1,,"The more realistic the model, the more accurate the prediction.",,"Add,Claim",Claim
6585,2-288,2-288_v2_31@10,,"In general, using Matlab without Simulink takes 6 to 20 times as long as JSim solutions.",,"Add,Fact/Evidence",Fact/Evidence
6586,2-288,2-288_v2_33@0,,Declarative languages such as MML describe the logic of a computer program rather than the explicit flow of control.,,"Add,Claim",Claim
6587,2-288,2-288_v2_33@1,,In traditional procedural languages such as C and Fortran the flow of control is explicit in the code.,,"Add,Claim",Claim
6588,2-288,2-288_v2_33@2,,Declarative languages have advantages and disadvantages relative to procedural languages.,,"Add,Claim",Claim
6589,2-288,2-288_v2_33@3,,"They allow for clear exposition of the intentions of computation, since only the equations (without the numerical details) are specified.",,"Add,Claim",Claim
6590,2-288,2-288_v2_33@4,,"Because they generally represent a top-down view of the mathematics, they allow automated handling of computational complexities such as parallel processing without distracting the user with complex details.",,"Add,Claim",Claim
6591,2-288,2-288_v2_33@5,,"On the other hand, it is difficult for a procedural translation of declarative code, as going from MML to compute using Java, to be as general and efficient as optimized procedural code.",,"Add,Claim",Claim
6592,2-288,2-288_v2_33@6,,Consequently MML is designed to permit use of procedural code when circumstances demand it.,,"Add,Fact/Evidence",Fact/Evidence
6593,2-288,2-288_v2_31@0,2-288_v1_31@0,"JSim is designed centrally for evaluating models against experimental data, for describing biological systems, for designing experiments, and for the teaching of integrative systems approaches to biological, chemical and physical systems.","JSim is quite general, and while designed for evaluating models against experimental data, it also serves pure model development quite well.","Modify,Fact/Evidence",Fact/Evidence
6594,2-288,2-288_v2_32@0,2-288_v1_32@0,MML (Mathematical Modeling Language) is a declarative modeling language developed for JSim and used for composing models.,MML (Mathematical Modeling Language) is the declarative modeling language developed for JSim and used for composing models.,"Modify,Grammar",Grammar
6595,2-288,2-288_v2_37@0,2-288_v1_36@0,"JSim’s mathematical modeling language, MML","JSim’s Mathematical Modeling Language, MML","Modify,Grammar",Grammar
6596,2-288,2-288_v2_38@1,2-288_v1_37@1,"When JSim imports other model formats (e.g. SBML, CellML, Antimony ( Smith et al. , 2013 )), it translates them to MML.","When JSim imports other model formats (e.g. SBML, CellML, Antimony), it translates them to MML.","Modify,Fact/Evidence",Fact/Evidence
6597,2-288,2-288_v2_44@2,2-288_v1_41@2,At present JSim provides 8 algorithms for solving ODEs [ Table 1 ) and 3 for PDEs [ Table 2 ).,At present JSim provides 8 algorithms for solving ODEs ( Table 1 ) and 3 for PDEs ( Table 2 ).,"Modify,Grammar",Grammar
6598,2-288,2-288_v2_49@1,2-288_v1_46@1,"Partial differential equations also require boundary conditions, as seen in the code for a two-region convection-diffusion-permeation-reaction model [ Box 2 ).","Partial differential equations require also boundary conditions, as seen in the code for a two-region convection-diffusion-permeation-reaction model ( Box 2 ).","Modify,Grammar",Grammar
6599,2-288,2-288_v2_51@4,2-288_v1_48@4,"When there is no consumption and the system is linear (output area equals input) and stationary (response same at another time), then the output function is the convolution of the operator’s transfer function (the response to an infinitely short pulse input) with the input function.","When the system is linear (output area equals input) and stationary (response same at another time), then the output is the convolution of the operator’s transfer function (the response to an infinitely short pulse input) with the input function.","Modify,Fact/Evidence",Fact/Evidence
6600,2-288,2-288_v2_4@2,2-288_v1_4@2,"By so doing, at least one of the hypotheses must then be rejected; the rejection marks a stepping-stone in science.","By so doing, at least one of the hypotheses must then be rejected: the rejection marks a stepping-stone in science.","Modify,Grammar",Grammar
6601,2-288,2-288_v2_63@0,2-288_v1_62@0,"Nested plots [ Figure 3 ) are JSim’s version of worlds-within-worlds graphics ( Harris et al. , 1994 ).","Nested plots ( Figure 3 ) are JSim’s version of worlds-within-worlds graphics ( Harris et al. , 1994 ).","Modify,Grammar",Grammar
6602,2-288,2-288_v2_5@2,2-288_v1_5@2,"Given that the experiment tests whether or not the working hypothesis is compatible with experimental data, then failure to fit the data within a defined level of goodness of fit leads to skepticism about the accuracy of the data or more often, about the structure of the model and leads to its modification or to its outright rejection.","Given that the experiment tests whether or not the working hypothesis is compatible with experimental data, then failure to fit leads to rejection.","Modify,Claim",Claim
6603,2-288,2-288_v2_71@1,2-288_v1_70@1,Automated parameter optimization is usually much faster; eight methods are provided [See Table 3 ]; we recommend testing several in order to test speed and reliability with respect to the particular types of data and model.,Automated parameter optimization is usually much faster; eight methods are provided (See Table 3 ); we recommend testing several in order to test speed and reliability with respect to the particular types of data and model.,"Modify,Grammar",Grammar
6604,2-288,2-288_v2_7@0,2-288_v1_7@0,"As in physics, models are posed in order to gain deeper understanding and to make predictions.","As in physics, models are posed in order to gain deeper understanding.","Modify,Claim",Claim
6605,2-288,2-288_v2_79@3,2-288_v1_78@3,"3) “Run” Monte Carlo automatically re-optimizes the model against the new set of perturbed data points to obtain another estimate for each parameter, repeating steps 2 and 3 many times (e.g. set it to 1000).",3) Re-optimize the model against the new set of perturbed data points to obtain another estimate for each parameter.,"Merge+Modify,Fact/Evidence",Fact/Evidence
6606,2-288,2-288_v2_79@3,2-288_v1_78@4,"3) “Run” Monte Carlo automatically re-optimizes the model against the new set of perturbed data points to obtain another estimate for each parameter, repeating steps 2 and 3 many times (e.g. set it to 1000).",4) Repeat steps 2 and 3 many times (e.g. 1000).,"Merge+Modify,Clarity",Clarity
6607,2-288,2-288_v2_79@4,2-288_v1_78@5,"From these many results, one obtains a histogram of estimates for each optimized parameter, and robust confidence limits can be drawn directly from these histograms without assuming symmetry and linearity as in the Jacobian method.","From these results, one obtains a histogram of estimates for each optimized parameter, and robust confidence limits can be drawn directly from these histograms without assuming symmetry and linearity as in the Jacobian method.","Modify,Clarity",Clarity
6608,2-288,2-288_v2_79@5,2-288_v1_78@6,JSim also displays these results in the form of 2-parameter scatter plots to show covariance.,"JSim displays these histograms to show the distributions of parameter estimates in full detail, and 2-parameter scatter plots to show covariance.","Modify,Fact/Evidence",Fact/Evidence
6609,2-288,2-288_v2_81@3,2-288_v1_80@3,"This feature is based on work by Yngve ( Yngve et al. , 2007 ).","This capability is based on work by Yngve ( Yngve et al. , 2007 ).","Modify,Clarity",Clarity
6610,2-288,2-288_v2_85@0,2-288_v1_84@0,"JSim is implemented in the Java computer language ( Gosling & McGilton, 1996 ).","JSim is implemented in the Java computer language ( Gosling & McGilton, 2003 ).","Modify,Fact/Evidence",Fact/Evidence
6611,2-288,2-288_v2_85@1,2-288_v1_84@1,"The major factors affecting this choice are Java’s platform independent GUI (allowing Windows, Macintosh and Linux versions to be developed in a single code base), object-oriented features and garbage collection (simplifying complex coding), advanced utilities (associative arrays, dynamic linking, remote procedure calls), strong type checking (allowing many common coding errors to be caught at compile time) and robust exception mechanism (simplifying coding and enabling a virtually crash-proof GUI).","The major factors affecting this choice are Java's platform independent GUI (allowing Windows, Macintosh and Linux versions to be developed in a single code base), object-oriented features and garbage collection (simplifying complex coding), advanced utilities (associative arrays, dynamic linking, remote procedure calls), strong type checking (allowing many common coding errors to be caught at compile time) and robust exception mechanism (simplifying coding and enabling a virtually crash-proof GUI).","Modify,Grammar",Grammar
6612,2-288,2-288_v2_86@1,2-288_v1_85@1,"These tools, similar to the classic Unix lex and yacc ( Aho & Sethi, 1988 ), were among the few parser generation tools available for Java when JSim was first developed.","These tools, similar to the classic Unix lex and yacc ( Aho et al. , 1988 ), were among the few parser generation tools available for Java when JSim was first developed.","Modify,Fact/Evidence",Fact/Evidence
6613,2-288,2-288_v2_86@3,2-288_v1_85@3,MML’s declarative structure is an intuitive expression of a model’s underlying mathematics (simplifying the modeler’s learning) and allows the overall structure of the model to be examined for mathematical correctness (detecting overspecification or underspecification) in a way that is not possible with a procedural specification.,MML's declarative structure is an intuitive expression of a model's underlying mathematics (simplifying the modeler’s learning) and allows the overall structure of the model to be examined for mathematical correctness (detecting overspecification or underspecification) in a way that is not possible with a procedural specification.,"Modify,Grammar",Grammar
6614,2-288,2-288_v2_7@4,2-288_v1_7@3,"Standard statistical methods are not central to deciding whether or not to reject the hypothesis, but are indeed helpful in assessing goodness of fit, estimating confidence ranges and co-variances among parameters, and in guiding the investigator in identifying errors or in finding ways to simplify the model.","Standard statistical methods are not central to deciding whether or not to reject the hypothesis, but are indeed very helpful in assessing goodness of fit, estimating confidence ranges and co-variances among parameters, and in guiding the investigator in identifying errors or in finding ways to simplify the model.","Modify,Clarity",Clarity
6615,2-288,2-288_v2_87@4,2-288_v1_86@4,"JSim’s remote computation is implemented using Java Remote Method Invocation (RMI) ( Harold, 1997 ), providing reliable access to networked computational servers.","JSim's remote computation is implemented using Java Remote Method Invocation (RMI) ( Harold, 1997 ), providing reliable access to networked computational servers.","Modify,Grammar",Grammar
6616,2-288,2-288_v2_104@0,2-288_v1_88@0,The all-too-frequent failures to reproduce published results are a critical problem in advancing the biological sciences.,"The issue of reproducibility, or should we say the all-too-frequent failures of attempts to reproduce published results, are beginning to be recognized as a critical problem in advancing the biological sciences.","Modify,Clarity",Clarity
6617,2-288,2-288_v2_104@1,2-288_v1_88@1,"It is easy to understand that biological laboratory studies, with inherently great variability in materials and analysis procedures, should be less exact than those in the physical sciences, but it is not so forgivable that reproducing mathematical models of biological systems is a major problem.","It is easy to understand biological studies, with inherently great variability in materials and analysis procedures, should be less exact than those in the physical sciences, but it is not so forgivable that reproducing mathematical models of biological systems is a major problem.","Modify,Clarity",Clarity
6618,2-288,2-288_v2_104@3,2-288_v1_88@3,"These models all used algebraic, ODEs, or differential-algebraic equations and so must be regarded as relatively simple, computationally, compared to finite-element models or spatially dependent models.","These models all used algebraic, ODEs, or differential-algebraic equations and so must be regarded as relatively simple computationally compared to finite-element models or spatially dependent models.","Modify,Grammar",Grammar
6619,2-288,2-288_v2_105@0,2-288_v1_89@0,Project files as vehicles for reproducible modeling and data analysis,Project files,"Modify,Other",Other
6620,2-288,2-288_v2_106@8,2-288_v1_90@8,"They are editable in any word processor, but one avoids doing that since it is easier to enter code and notes under JSim editor directly and not disturb the format in the XMML file that JSim reads.","They are editable in any word processor, but one avoids doing that since it is easier to enter code and notes under JSim and not risk disturbing the format in the XMML file that JSim reads.","Modify,Clarity",Clarity
6621,2-288,2-288_v2_107@1,2-288_v1_91@1,"Examples are that of Kuikka et al on glucose uptake by myocardium ( Kuikka et al. , 1986 , models 163 and 173), xanthine oxidase reactions ( Bassingthwaighte & Chinn, 2013 , model 324), and lung endothelial serotonin uptake ( Jardine & Bassingthwaighte, 2013 , model 198).","Examples are that of Kuikka et al. on glucose uptake by myocardium ( Kuikka et al. , 1986 ), [models 163 and 173], xanthine oxidase reactions ( Bassingthwaighte & Chinn, 2013 ), [model 324], and lung endothelial serotonin uptake ( Jardine & Bassingthwaighte, 2013 ), [model 198].","Modify,Grammar",Grammar
6622,2-288,2-288_v2_122@3,2-288_v1_96@3,"First, when it only takes a few seconds to modify a model, re-run it, and view the results, researchers are more likely to explore many “what if” scenarios and develop a deeper understanding of model behavior, and in turn, a deeper understanding of the system being modeled.","First, when it only takes a few seconds to tweak a model, re-run it, and view the results, researchers are more likely to explore many “what if” scenarios and develop a deeper understanding of model behavior, and in turn, a deeper understanding of the system being modeled.","Modify,Clarity",Clarity
6623,2-288,2-288_v2_124@0,2-288_v1_98@0,Future developments:,Future developments,"Modify,Grammar",Grammar
6624,2-288,2-288_v2_18@0,2-288_v1_18@0,(7) Explore model behavior over wide ranges of parameter values in state-space. (We think of “state space” as being the N-dimensional space enclosing the ranges of values of all of the parameters within which the model is correct numerically and sensible scientifically).,(7) Explore model behavior over wide ranges of parameter values in state-space. (We think of “state space” as being the N-dimensional space enclosing the ranges of values of all of the parameters within which the model is correct numerically and sensible scientifically.),"Modify,Grammar",Grammar
6625,2-29,2-29_v2_14@0,,Two reports dealt with large paraovarian cysts laparoscopically.,,"Add,Fact/Evidence",Fact/Evidence
6626,2-29,2-29_v2_14@1,,"The first was a simple paraovarian cyst associated with pregnancy which measured 20 cm while the second with acute lower abdominal pain measured 12 cm <REF-10> , <REF-11> .",,"Add,Fact/Evidence",Fact/Evidence
6627,2-29,2-29_v2_14@3,,They reported either cyst aspiration and forceps coagulation of small cysts or cyst wall trocar puncture followed by suction and extraction for large cysts.,,"Add,Fact/Evidence",Fact/Evidence
6628,2-29,2-29_v2_14@6,,It is the authors’ view that laparoscopic cyst decompression before its removal is associated with greater risk of cyst spillage than the technique we describe.,,"Add,Claim",Claim
6629,2-29,,2-29_v1_15@3,,In our case report we had dealt with such a huge cyst in a way not only to avoid morbidity of extending surgical incision but to guard against the risk of spillage of cyst contents as well.,"Delete,Fact/Evidence",Fact/Evidence
6630,2-29,,2-29_v1_15@5,,"However, there were two reports of large paraovarian cysts removed laparoscopically where in the first one, it was associated with acute lower abdominal pain while in the second it was associated with pregnancy <REF-11> , <REF-12> .","Delete,Fact/Evidence",Fact/Evidence
6631,2-29,,2-29_v1_15@6,,"We think that in all these laparoscopically operated cases, the implemented cyst decompression procedure before its removal had less control and precautions during it and in turn more risk of cyst spillage than our mentioned maneuver.","Delete,Claim",Claim
6632,2-29,2-29_v2_4@1,2-29_v1_4@1,Paraovarian cysts constitute 10–20% of all adnexal masses <REF-2> .,"Paraovarian cysts, constitute 10–20% of all adnexal masses <REF-2> .","Modify,Grammar",Grammar
6633,2-29,2-29_v2_4@3,2-29_v1_4@3,"These cysts are usually benign and rarely malignant <REF-4> , <REF-5> .","These cysts are usually benign with rare incidence of malignant types <REF-4> , <REF-5> .","Modify,Clarity",Clarity
6634,2-29,2-29_v2_4@4,2-29_v1_4@4,"In this report, we present how we surgically managed a case with an abnormally huge paraovarian cyst.","Here, we present a case of unusually extensive proportions.","Modify,Fact/Evidence",Fact/Evidence
6635,2-29,2-29_v2_18@0,2-29_v1_7@0,Written informed consent for publication of the clinical details and clinical images was obtained from the patient.,Written informed consent for publication of the clinical details and/or clinical images was obtained from the patient.,"Modify,Clarity",Clarity
6636,2-29,2-29_v2_6@1,2-29_v1_8@1,History revealed a gradual increase of an abdominal swelling over the preceding 6 months.,History revealed a gradual increase in an abdominal swelling over the preceding 6 months.,"Modify,Grammar",Grammar
6637,2-29,2-29_v2_6@2,2-29_v1_8@2,"Physical examination showed a non tender, tense cystic pelvi-abdominal mass of 36 weeks gestational size.",Physical examination showed a non tender tense cystic pelviabdominal mass of 36 weeks gestational size.,"Modify,Grammar",Grammar
6638,2-29,2-29_v2_6@3,2-29_v1_8@3,Computerised tomography revealed a 25 × 26 cm left ovarian simple cyst with clear contents and no septae.,Computerised tomography revealed 25×26 cm left ovarian simple cyst with clear contents and no septae.,"Modify,Grammar",Grammar
6639,2-29,2-29_v2_6@4,2-29_v1_8@4,Serum CA125 level was normal.,Serum CA125 levels were normal.,"Modify,Grammar",Grammar
6640,2-29,2-29_v2_0@0,2-29_v1_0@0,Case Report: Laparoscopic trocar management of a giant paraovarian cyst: a case report,Laparoscopic trocar management of a giant paraovarian cyst: a case report,"Modify,Other",Other
6643,2-29,2-29_v2_6@13,2-29_v1_8@12,"The collapsed cyst was found to be left paraovarian, which was exteriorized and the trocar sleeve was removed.",The collapsed cyst was found to be left paraovarian which was exteriorized and the trocar sleeve was removed.,"Modify,Grammar",Grammar
6644,2-29,2-29_v2_6@15,2-29_v1_8@14,The left broad ligament was opened and the cyst wall was completely removed from the broad ligament ( Figure 1 ).,"The left broad ligament was opened and the cyst wall was completely removed from the broad ligament, Figure 1 .","Modify,Grammar",Grammar
6645,2-29,2-29_v2_6@16,2-29_v1_8@15,The redundant peritoneum of the ligament was excised and subsequently reconstructed with preservation of the tubal integrity as seen in Figure 2 .,The redundant ligament peritoneum was excised and subsequently reconstructed with preservation of the tubal integrity as seen in Figure 2 .,"Modify,Clarity",Clarity
6646,2-29,2-29_v2_13@1,2-29_v1_15@1,We reviewed the English literature and were able to find three reports about the management of comparably huge cysts.,"On revising the literature, there were three case reports which had addressed comparable large paraovarian cysts but with implementation of larger incisions extending over the umbilicus for cyst extraction and excision without a policy to decrease its size before its exteriorization <REF-6> – <REF-8> .","Split+Modify,Clarity",Clarity
6647,2-29,2-29_v2_13@2,2-29_v1_15@1,"In these reports, surgeons utilized abdominal incisions extending over the umbilicus, much larger than the one we used, in order to allow for cyst extraction and excision <REF-6> – <REF-8> .","On revising the literature, there were three case reports which had addressed comparable large paraovarian cysts but with implementation of larger incisions extending over the umbilicus for cyst extraction and excision without a policy to decrease its size before its exteriorization <REF-6> – <REF-8> .","Split+Modify,Fact/Evidence",Fact/Evidence
6648,2-29,2-29_v2_13@3,2-29_v1_15@2,Only one report for three adolescents with large paraovarian cysts (with a range of 20–26 cm) had addressed decompression techniques before cyst exteriorization and excision using a suction cannula <REF-9> .,"However, a case report for three adolescents with large paraovarian cysts had addressed decompression technique before cyst externalization and excision but in a different way <REF-9> .","Modify,Fact/Evidence",Fact/Evidence
6649,2-29,2-29_v2_14@2,2-29_v1_15@4,Darwish et al. reported a series of smaller paraovarian cysts which had been excised laproscopically; the largest was not more than 13 cm <REF-12> .,"Concerning the endoscopic role, Darwish et al. reported a series of paraovarian cysts which had been excised laproscopically but were smaller in size with the largest not more than 13 cm <REF-10> .","Modify,Clarity",Clarity
6650,2-29,2-29_v2_14@4,2-29_v1_15@7,Laparoscopy would have been technically difficult in our case due to the huge size of the cyst reaching close to xiphesternum.,It was thought that laparoscopy would be technically difficult in this case due to huge size of the cyst reaching close to xiphesternum.,"Modify,Clarity",Clarity
6651,2-29,2-29_v2_14@5,2-29_v1_15@8,Direct abdominal entry with a Veress needle or trocar might have traumatized the cyst leading to spillage of its content.,Direct abdominal entry with a Veress needle or trocar may have traumatized the cyst leading to risk of spillage of its content.,"Modify,Grammar",Grammar
6652,2-29,2-29_v2_14@7,2-29_v1_15@9,"We therefore, through laparotomy and a small surgical incision, employed a closed drainage system to safely aspirate the cyst followed by excision.",Through laparotomy we employed a closed drainage system and safely aspirated the cyst without spillage of its content.,"Modify,Fact/Evidence",Fact/Evidence
6653,2-29,2-29_v2_4@0,2-29_v1_4@0,"Paraovarian cysts occur in the broad ligament between the ovary and the tube, predominantly arising from mesothelium covering the peritoneum (mesothelial cyst) but also occasionally arise from the paramesonephric tissue (paramesonephric cysts or Mullerian cysts) and rarely from mesonephric remnants (mesonephric cysts or Wolffian cysts) <REF-1> .","Paraovarian cysts occur in the broad ligament between the ovary and the tube, predominantly arising from mesothelium covering the peritoneum (mesothelial cyst) but occasionally also from para mesonephric tissue (paramesonephric cysts or Mullerian cysts) and rarely from mesonephric remnants (mesonephric cysts or Wolffian cysts) <REF-1> .","Modify,Clarity",Clarity
6654,2-30,2-30_v2_13@2,2-30_v1_13@2,"An example can be seen in Figure 1a) , where logical definitions are used to automatically infer that Hypoglycemia is a subclass of Decreased aldohexose concentration (blood) based on the asserted subclass relationship between 'glucose' and 'aldohexose' in ChEBI.","An example can be seen in Figure 1 a) , where logical definitions are used to automatically infer that Hypoglycemia is a subclass of Decreased aldohexose concentration (blood) based on the asserted subclass relationship between 'glucose' and 'aldohexose' in ChEBI.","Modify,Grammar",Grammar
6655,2-30,2-30_v2_27@4,2-30_v1_27@4,The translation is shown in Manchester syntax in Figure 1a) .,The translation is shown in Manchester syntax in Figure 1 a) .,"Modify,Grammar",Grammar
6656,2-30,2-30_v2_6@4,2-30_v1_6@4,"Similar approaches are being taken in zebrafish ( Danio rerio ) by the Zebrafish Mutation Project (ZMP, http://www.sanger.ac.uk/Projects/D_rerio/zmp/ ) and the data is being made available through the Zebrafish Model Organism Database (ZFIN <REF-5> ).","Similar approaches are being taken in zebrafish ( Danio rerio ) by the Zebrafish Mutation Project (ZMP, http://www.sanger.ac.uk/Projects/D_rerio/zmp/ ) and the data is being made available through the The Zebrafish Model Organism Database (ZFIN <REF-5> ).","Modify,Grammar",Grammar
6657,2-30,2-30_v2_43@0,2-30_v1_43@0,"Here, the GO process eye pigmentation ( GO:0048069 ) is logically defined as being equivalent to everything that is a pigmentation ( GO:0043473 ) and also ""occurs_in"" an eye ( UBERON:0000970 ).","Here, the GO process eye pigmentation ( G0:0048069 ) is logically defined as being equivalent to everything that is a pigmentation ( GO:0043473 ) and also ""occurs_in"" an eye ( UBERON:0000970 ).","Modify,Grammar",Grammar
6658,2-30,2-30_v2_68@0,2-30_v1_68@0,"An excerpt of the Uberpheno ontology is shown in ( Figure 1b ), demonstrating how the phenotype descriptions from different ontologies are combined and automatically organised into a single, integrated hierarchy.","An excerpt of the Uberpheno ontology is shown in ( Figure 1 b ), demonstrating how the phenotype descriptions from different ontologies are combined and automatically organised into a single, integrated hierarchy.","Modify,Grammar",Grammar
6659,2-30,2-30_v2_3@0,2-30_v1_3@0,"We have generated a cross-species phenotype ontology for human, mouse and zebrafish that contains classes from the Human Phenotype Ontology, Mammalian Phenotype Ontology, and generated classes for zebrafish phenotypes.","We have generated a cross-species phenotype ontology for human, mouse and zebra fish that contains zebrafish phenotypes.","Modify,Fact/Evidence",Fact/Evidence
6660,2-57,2-57_v2_24@0,2-57_v1_24@0,Response of cotesia vestalis females to Blend A vs. Blend B,Response of Cotesia vestalis females to Blend A vs. Blend B,"Modify,Grammar",Grammar
6661,2-57,2-57_v2_29@0,2-57_v1_29@0,Response of cotesia vestalis females to Blend A vs. Blend C,Response of Cotesia vestalis females to Blend A vs. Blend C,"Modify,Grammar",Grammar
6662,2-57,2-57_v2_15@2,2-57_v1_15@2,"Next, to test whether female wasps could discriminate the ratios of the four compounds in the blend (quantitative differences in the blend), we prepared a third blend (Blend C) of sabinene, n -heptanal, α -pinene, and ( Z )-3-hexenyl acetate at a ratio of 1.0:1.0:1.0:1.0.","Next, to test whether female wasps could discriminate the ratios of the four compounds in the blend, we prepared a third blend (Blend C) of sabinene, n -heptanal, α -pinene, and ( Z )-3-hexenyl acetate at a ratio of 1.0:1.0:1.0:1.0.","Modify,Clarity",Clarity
6663,2-58,2-58_v2_4@5,,Most relevant to the current paper are the Delboeuf and Ebbinghaus illusions that demonstrate that the size of an inner circle is overestimated or underestimated depending on the surrounding context in which it is presented.,,"Add,Fact/Evidence",Fact/Evidence
6664,2-58,2-58_v2_4@6,,"Though several explanations have been proposed for these illusions, recent research demonstrates that the effect is largely determined by the relative size of the inducer(s), their distance from the target <REF-2> , and in the case of the Ebbinghaus Illusion, the completeness of the surrounding array of elements <REF-8> .",,"Add,Fact/Evidence",Fact/Evidence
6665,2-58,2-58_v2_4@7,,"Taken together, the balance of these factors determines the magnitude of the illusion and whether the inner circle is overestimated or underestimated.",,"Add,Fact/Evidence",Fact/Evidence
6666,2-58,2-58_v2_7@4,,"To our knowledge, previous research on the Ebbinghaus Illusion has focused on the effect the surrounding elements have on the explicitly defined circle.",,"Add,Claim",Claim
6667,2-58,2-58_v2_7@5,,Here we consider the possibility of mutual influence in that the inner circle may also lead to misperceived size of the surrounding array.,,"Add,Claim",Claim
6668,2-58,2-58_v2_45@2,,"Specifically, differential effects can be obtained resulting in changes to the magnitude of the illusions depending on spatial frequency filtering.",,"Add,Fact/Evidence",Fact/Evidence
6669,2-58,2-58_v2_68@4,,This stands in contrast to several recent findings using functional and structural MRI that have implicated visual areas as early as V1 as playing a key role in the representation of perceived size <REF-31> – <REF-33> .,,"Add,Fact/Evidence",Fact/Evidence
6670,2-58,2-58_v2_68@5,,"Given the classical receptive field properties of V1 neurons, it is likely that these observations arise due to feedback to V1 from higher visual areas, that in the case of the Binding Ring Illusion may contain integrated representations of spatial frequency.",,"Add,Claim",Claim
6671,2-58,2-58_v2_68@6,,This is in line with recent research on the Müller-Lyer Illusion using dynamic causal modelling <REF-34> .,,"Add,Fact/Evidence",Fact/Evidence
6672,2-58,2-58_v2_68@7,,It was demonstrated that illusion strength could be predicted by modulating bilateral connections between the lateral occipital cortex (LOC) and right superior parietal cortex (SPC).,,"Add,Fact/Evidence",Fact/Evidence
6673,2-58,2-58_v2_68@8,,The model suggests that LOC is involved in size scaling to generate size invariant object representations that are further processed in SPC and relayed back to V1 to generate conscious illusory percepts.,,"Add,Claim",Claim
6674,2-58,2-58_v2_38@0,2-58_v1_38@0,"A) The size of the binding ring had a significant influence on the perceived size of the array (main effect of binding ring size-repeated measures ANOVA: F (4, 36) = 31.22, p < 0.001).","A) The size of the binding ring had a significant influence on the perceived size of the array (main effect of binding ring size - repeated measures ANOVA: F (4, 36) = 31.22, p < 0.001).","Modify,Grammar",Grammar
6675,2-58,2-58_v2_45@1,2-58_v1_45@1,"It is suggested that some visual illusions, including the Müller-Lyer <REF-19> and Oppel-Kundt <REF-20> , <REF-21> or filled area illusion <REF-22> , are mediated by differential processing of low- and high-spatial frequency information <REF-23> – <REF-27> .","It is suggested that some visual illusions, including the the Müller-Lyer <REF-17> , are mediated by differential processing of low- and high-spatial frequency information <REF-18> – <REF-22> .","Modify,Fact/Evidence",Fact/Evidence
6676,2-58,2-58_v2_4@3,2-58_v1_4@3,"These constructive processes are revealed through a number of classic size illusions such as the Ebbinghaus Illusion <REF-4> ( Figure 1A ), the Delboeuf Illusion <REF-5> , <REF-6> ( Figure 1B ), the Müller-Lyer Illusion <REF-7> ( Figure 1C ) and several others that illustrate how mechanisms that underlie size constancy sometimes lead to illusory percepts resulting from a discrepancy between retinal and perceived size.","These constructive processes are revealed through a number of classic size illusions such as the Ebbinghaus Illusion <REF-4> ( Figure 1A ) and the Delboeuf Illusion <REF-5> , <REF-6> ( Figure 1B ).","Modify,Fact/Evidence",Fact/Evidence
6677,2-58,2-58_v2_4@4,2-58_v1_4@4,"In each of these illusions, the perceived size of an explicitly defined object is influenced by the context in which it is presented.","In each of these illusions, the perceived size of an explicitly defined object (the inner circle) is influenced by the context in which it is presented.","Modify,Clarity",Clarity
6678,2-58,2-58_v2_52@0,2-58_v1_52@0,Experiment 4a,Experiment 4,"Modify,Other",Other
6679,2-58,2-58_v2_53@1,2-58_v1_53@1,"A number of visual illusions can be attributed to the processes of local configural features <REF-3> , <REF-8> , <REF-28> .",A number of visual illusions can be attributed to the processes of local configural features <REF-23> .,"Modify,Fact/Evidence",Fact/Evidence
6680,2-58,2-58_v2_19@2,2-58_v1_19@2,"Participants indicated by pressing one of two buttons (two-alternative forced choice), which of the two stimuli had appeared larger.","Participants indicated by pressing one of two buttons (two-alternative forced choice; AFC), which of the two stimuli had appeared larger.","Modify,Clarity",Clarity
6681,2-9,2-9_v2_52@1,,"However, in our model the local host adipocytes responded to the tumor implantation with the lipogenic rather than the erythrogenic autophagy.",,"Add,Fact/Evidence",Fact/Evidence
6682,2-9,2-9_v2_52@2,,The entire cells were converted into lipid droplets.,,"Add,Fact/Evidence",Fact/Evidence
6683,2-9,2-9_v2_52@3,,"If the tumor cells could be treated to re-direct their metabolism to lipogenesis instead of erythrogenesis, perhaps the metastatic potential of the capsular vaso-mimicry could be abolished and ultimately the entire tumor replaced with fat.",,"Add,Claim",Claim
6684,2-9,2-9_v2_52@4,,"A non-malignant type of undifferentiated cells, human mesenchymal stem cells, can accumulate lipid under hypoxia, although normally they would differentiate along several pathways to form bone, cartilage, tendon, muscle or adipose tissues.",,"Add,Claim",Claim
6685,2-9,2-9_v2_52@5,,In that case the potent lipogenic effect of hypoxia was independent of PPAR-γ2 maturation pathway <REF-80> .,,"Add,Fact/Evidence",Fact/Evidence
6686,2-9,,2-9_v1_41@1,,The ability of cells to undergo such nucleo-cytoplasmic conversion was not tumor-specific.,"Delete,Claim",Claim
6687,2-9,2-9_v2_15@2,,The mice were on Tekand global 14% protein rodent diet (Harlan) with access to water ad libitum.,,"Add,Fact/Evidence",Fact/Evidence
6688,2-9,2-9_v2_24@1,,Analyzed tissue sections were first examined at low magnification and coordinates for each hexagonal sector of a grid covered with tissue were recorded automatically.,,"Add,Fact/Evidence",Fact/Evidence
6689,2-9,2-9_v2_24@2,,Subsequently all sectors were explored at least once at variable high magnifications.,,"Add,Fact/Evidence",Fact/Evidence
6690,2-9,2-9_v2_24@3,,"Interesting images were captured at the magnification best suited to document a particular phenomenon or identify a structure, including colloidal Au grains.",,"Add,Fact/Evidence",Fact/Evidence
6691,2-9,2-9_v2_24@5,,In some cases the final images were assembled by multiple image alignment (MIA) to increase the surface area without losing the resolution.,,"Add,Fact/Evidence",Fact/Evidence
6692,2-9,2-9_v2_38@0,,At the time of tissue harvest the histomorphological features of the transplanted tumor spheroids resembled those of spontaneously grown tumors ( Figure 5 ).,,"Add,Fact/Evidence",Fact/Evidence
6693,2-9,2-9_v2_38@1,,"The malignant cells were arranged into small nodules surrounded with fibroblasts, vessel-free erythrosomes and some undifferentiated migrating cells (mesenchymal cells).",,"Add,Fact/Evidence",Fact/Evidence
6694,2-9,2-9_v2_38@2,,The nodules were not larger than the oxygen diffusion range (100–200 µm <REF-58> ).,,"Add,Fact/Evidence",Fact/Evidence
6695,2-9,2-9_v2_38@3,,A thicker fibrotic capsule surrounded the larger clusters of the small nodules.,,"Add,Fact/Evidence",Fact/Evidence
6696,2-9,2-9_v2_38@4,,The underlying host muscle cells appeared normal whereas adipocytes were commonly replaced with lipid droplets.,,"Add,Fact/Evidence",Fact/Evidence
6697,2-9,2-9_v2_11@0,2-9_v1_12@0,Two recipient mice of the 5 used in the two accompanying articles were assessed in this experiment.,Two recipient mice of the 5 used in the two accompanying articles were assessed in this report.,"Modify,Clarity",Clarity
6698,2-9,2-9_v2_15@1,2-9_v1_16@1,They were housed in the SKCC animal care facilitywith controlled 12/12 hr light/dark cycle and temperature maintained at + 22°C.,The mice were housed in the SKCC animal care facility.,"Modify,Fact/Evidence",Fact/Evidence
6699,2-9,2-9_v2_16@1,2-9_v1_17@1,J. Lustgarten and P. Borgstrom and used to form tumor spheroids by culturing 2×10 5 cells per well for 2–3 days prior to implantation.,J. Lustgarten and P. Borgstrom and used to form tumor spheroids by culturing 2 × 10 5 cells per well for 2–3 days prior to implantation.,"Modify,Grammar",Grammar
6700,2-9,2-9_v2_16@4,2-9_v1_17@4,"Their final size was about 1–3 mm in ""diameter'.",Their final size was about 1–3 mm in diameter.,"Modify,Grammar",Grammar
6701,2-9,2-9_v2_16@5,2-9_v1_17@5,The GFP-specific rabbit polyclonal IgG (ab290) was from Abcam; and non-reactive IgG (sc-34284) were from Santa Cruz.,The GFP-specific rabbit polyclonal IgG (ab290) was from Abcam; and non-reactive goat polyclonal IgG (sc-34284) were from Santa Cruz.,"Modify,Fact/Evidence",Fact/Evidence
6702,2-9,2-9_v2_18@0,2-9_v1_19@0,The tumors with some surrounding tissues were dissected out and cut in halves perpendicularly to the host skin surface while immersed in cold fixative (4% paraformaldehyde in 0.1 M Na cacodylate pH 7.4).,The tumors with some surrounding tissues were dissected out and cut in halves perpendicular to the host skin surface while immersed in cold fixative (4% paraformaldehyde in 0.1 M Na cacodylate pH 7.4).,"Modify,Grammar",Grammar
6703,2-9,2-9_v2_20@1,2-9_v1_21@1,"They were cut into 1 mm thick slices in planes perpendicular to the plain of the first cut and to the skin surface, finally, into ~ 1 mm 3 blocks, transferred into fresh portion of the fixative in which they were cut and incubated for 2 hrs at 4°C.","They were cut into 1 mm thick slices in planes perpendicular to the plain of the first cut and to the skin surface, finally, into ~ 1 mm 3 blocks, transferred into a fresh portion of the fixative in which they were cut and incubated for 2 hrs at 4°C.","Modify,Grammar",Grammar
6704,2-9,2-9_v2_20@2,2-9_v1_21@2,"The fixed tissue blocks were washed with 0.1 M Na cacodylate–HCl buffer pH 7.4 (3 × 15 min.) and post fixed in 1% OsO 4 in 0.1 M Na cacodylate buffer, pH 7.0 for 60 min on ice, washed with water and stained with 1% uranyl acetate at RT for one hour.","The fixed tissue blocks were washed with 0.1 M Na cacodylate – HCl buffer pH 7.4 (3 × 15 min) and post fixed in 1% OsO 4 in 0.1 M Na cacodylate buffer, pH 7.0 for 60 min. on ice, washed with water and stained with 1% uranyl acetate at RT for one hour.","Modify,Grammar",Grammar
6705,2-9,2-9_v2_20@4,2-9_v1_21@4,The resin embedded tissues were cut into 60 nm sections on Leica Ultracut UCT ultramicrotome and viewed without further contrasting.,"The resin-embedded tissues were cut into 60 nm sections, on a Leica Ultracut UCT ultramicrotome and stained with lead citrate <REF-42> or viewed without further contrasting.","Modify,Fact/Evidence",Fact/Evidence
6706,2-9,2-9_v2_22@0,2-9_v1_23@0,"During cutting into ~ 1 mm 3 blocks as described above, the tissues were kept in the mild fixative to protect the antigenic epitopes (4% paraformaldehyde in 0.1 M Na cacodylate pH 7.4).","During cutting into ~ 1 mm 3 blocks as described above, the tissues were kept in the mild fixative to protect the antigenic epitops (4% paraformaldehyde in 0.1 M Na cacodylate pH 7.4).","Modify,Grammar",Grammar
6707,2-9,2-9_v2_22@1,2-9_v1_23@1,"The tissue blocks were vitrified by infiltrating the pieces with 50% PVP (polyvinylpyrrolidone) containing 2.3 M sucrose in 0.1 M Na cacodylate buffer, pH 7.4, for 2 hrs or over night, mounted on metal pins and frozen in liquid nitrogen, as described by Tokuyasu <REF-47> .","The tissue blocks were vitrified by infiltrating the pieces with 50% PVP (polyvinylpyrrolidone) containing 2.3 M sucrose in 0.1 M Na-cacodylate buffer, pH 7.4, for 2 hrs or overnight, mounted on metal pins and frozen in liquid nitrogen, as described by Tokuyasu <REF-43> .","Modify,Grammar",Grammar
6708,2-9,2-9_v2_22@2,2-9_v1_23@2,"Frozen tissues were cut into 70 nm sections, on Leica Ultracut UCT ultramicrotome with the cryo-attachment.","Frozen tissues were cut into 70 nm sections, on a Leica Ultracut UCT ultramicrotome with the cryo-attachment.","Modify,Grammar",Grammar
6709,2-9,2-9_v2_22@6,2-9_v1_23@6,"After rinsing three times with water the immunostained cryosections were contrasted with mixture of uranyl acetate and methyl cellulose (25 centipoises, Sigma M-6385) in water, at final concentration of 1.3% each, for 10 min at RT.","After rinsing three times with water, the immunostained cryosections were contrasted with a mixture of uranyl acetate and methyl cellulose (25 centipoises, Sigma M-6385) in water, at final concentration of 1.3% each, for 10 min at RT.","Modify,Grammar",Grammar
6710,2-9,2-9_v2_22@7,2-9_v1_23@7,Excess of the liquid was removed and the sections were dried at RT.,Excess liquid was removed and the sections were dried at RT.,"Modify,Clarity",Clarity
6711,2-9,2-9_v2_24@4,2-9_v1_25@1,The images were transmitted from the microscope camera to iTEM imaging platform from Olympus Soft Imaging Solutions and archived in a designated Data Base.,Images were transmitted from the microscope camera to an iTEM imaging platform from Olympus Soft Imaging Solutions and archived in a designated database.,"Modify,Grammar",Grammar
6712,2-9,2-9_v2_24@6,2-9_v1_25@2,"We used graphics editing program, Adobe PhotoShop, to add cell type specific color-coding shown in the twin set of images included in the Supplement.","We used the graphics editing program, Adobe PhotoShop, to add cell type-specific color-coding shown in the twin set of images included in the Supplement.","Modify,Grammar",Grammar
6713,2-9,2-9_v2_26@0,2-9_v1_27@0,"Three weeks after the ectopic implantation of tumor spheroids, the vasculature formation, i.e., formation of tumor-supporting blood and vessels, was evidently retarded in comparison to pseudo-orthotopically implanted tumors described elsewhere <REF-41> .","Three weeks after the ectopic implantation of tumor spheroids, the vasculature formation, i.e., formation of tumor-supporting blood and vessels was evidently retarded in comparison to pseudo-orthotopicly implanted tumors described elsewhere <REF-36> .","Modify,Grammar",Grammar
6714,2-9,2-9_v2_26@2,2-9_v1_27@2,"A multi-cellular layer of connective tissue was growing between tumor and glass wall of the chamber, therefore, it was also generating its own vasculature ([A] in Figure 1 & Figure S1 ).","A multi-cellular layer of connective tissue was growing between the tumor and the glass wall of the chamber, therefore, it was also generating its own vasculature ([A] in Figure 1 & Figure S1 ).","Modify,Grammar",Grammar
6715,2-9,2-9_v2_26@3,2-9_v1_27@3,"Here, the term ""vasculature"" includes vessels and blood, and the term ""erythrosome"" is used as synonym for the ""erythrocyte"", because the latter is not a cell <REF-41> .","Here, the term ""vasculature"" includes vessels and blood and ""erythrosome"" is used as a synonym for the ""erythrocyte"", because the latter is not a cell <REF-36> .","Modify,Clarity",Clarity
6716,2-9,2-9_v2_26@7,2-9_v1_27@7,"Outside the tumor capsule, a primitive forming vessel morphologically resembled some of those seen around pseudo-orthotopically implanted tumors after only five days ([D] in Figure 1 & Figure S1 ).","Outside the tumor capsule, a primitive forming vessel morphologically resembled some of those seen around pseudo-orthotopicly implanted tumors after only five days ([D] in Figure 1 & Figure S1 ).","Modify,Grammar",Grammar
6717,2-9,2-9_v2_26@12,2-9_v1_27@12,Yet some tumor cells (CSCs?) began regenerating their vasculogenic potential that had been dormant during the years of in vitro culturing ([F & G] in Figure 1 & Figure S1 ).,Some tumor cells (CSCs?) did however begin regenerating their vasculogenic potential that had been dormant during the years of in vitro culturing ([F & G] in Figure 1 & Figure S1 ).,"Modify,Clarity",Clarity
6718,2-9,2-9_v2_26@14,2-9_v1_27@14,"Until that time, some tumor cells survived at the expense of the others.","Until that time, some tumor cells survived at the expense of others.","Modify,Grammar",Grammar
6719,2-9,2-9_v2_29@1,2-9_v1_30@1,In some locations hypoxic tumor nodules were breaking apart via prominent anoikis (loss of attachment between cells <REF-48> ) with abundant nano-tentacles.,"In some locations, hypoxic tumor nodules were breaking apart via prominent anoikis (loss of attachment between cells <REF-44> ) with abundant nano-tentacles ([A–C] in Figure 2 & Figure S2 ).","Modify,Fact/Evidence",Fact/Evidence
6720,2-9,2-9_v2_29@3,2-9_v1_30@3,They were either losing their internal cristae without shrinking and thus generating electron lucent vacuoles (seemingly empty or containing whorled membranes that might be intermediate stages of the internal membranes degradation) or becoming smaller and electron dense ([A–C] in Figure 2 & Figure S2 ).,They were either loosing their internal cristae without shrinking and thus generating electron-lucent vacuoles (seemingly empty or containing whorled membranes that might be intermediate stages of the internal membranes degradation) or becoming smaller and electron dense.,"Modify,Fact/Evidence",Fact/Evidence
6721,2-9,2-9_v2_29@5,2-9_v1_30@5,"The second type at first resembled appearance of mitochondria during mitosis and later, they were indistinguishable from the dark granules in erythroblasts ([D & F] in Figure 2 & Figure S2 ) and consistent with published images of peroxisomes <REF-50> – <REF-53> .","The second type at first resembled the appearance of mitochondria during mitosis and later, they were indistinguishable from the dark granules in erythroblasts ([D–F] in Figure 2 & Figure S2 ) and consistent with published images of peroxisomes <REF-46> – <REF-49> .","Modify,Grammar",Grammar
6722,2-9,2-9_v2_29@8,2-9_v1_30@8,That is because erythrosomes are capable of secreting anaerobically generated ATP <REF-54> .,That is because erythrosomes are capable of secreting anaerobicly generated ATP <REF-50> .,"Modify,Grammar",Grammar
6723,2-9,2-9_v2_29@10,2-9_v1_30@10,"Initially, electron dense regions of tumor cell nucleus contained chromatin in both cases.","Initially, electron-dense regions of the tumor cell nucleus contained chromatin in both cases.","Modify,Grammar",Grammar
6724,2-9,2-9_v2_29@14,2-9_v1_30@17,"The GFP-labeled mitotic chromosomes identified the tumor cell whereas the unlabeled fibroblast, on the other side of the collagen layer separating the two, must have been of host origin ([F] in Figure 2 & Figure S2 ).","The GFP-labeled mitotic chromosomes identified the tumor cell whereas the unlabeled fibroblast, on the other side of collagen layer separating the two, must have been of host origin ([F] in Figure 2 & Figure S2 ).","Modify,Grammar",Grammar
6725,2-9,2-9_v2_29@15,2-9_v1_30@18,"Together, the host fibroblasts and the encapsulated tumor-derived blood elements created the capsular vaso-mimicry that morphologically resembled veins ([G-I] in Figure 1 & Figure S1 ).","Together, the host fibroblasts and the encapsulated tumor-derived blood elements created the capsular vaso-mimicry that morphologically resembled veins ([G–I] in Figure 1 & Figure S1 ).","Modify,Grammar",Grammar
6726,2-9,2-9_v2_0@0,2-9_v1_0@0,II. Capsular vaso-mimicry formed by transgenic mammary tumor spheroids implanted ectopically into mouse dorsal skin fold: implications for cellular mechanisms of metastasis,II. Capsular vaso-mimicry formed by transgenic mammary tumor spheroids implanted ectopically into mouse dorsal skin fold: cellular mechanisms of metastasis,"Modify,Clarity",Clarity
6727,2-9,2-9_v2_32@3,2-9_v1_33@3,"The elongated cells like the one shown between the tumor nodules ( Figure 3 & Figure S3 ) had large nuclei undergoing conversion into erythrosomes and sparse, metabolically active cytoplasm generating energy and synthesizing protein.","The elongated cells like the one shown between the tumor nodules ( Figure 3 & Figure S3 ) had large nuclei undergoing conversion into erythrosomes and a sparse, metabolically active cytoplasm generating energy and synthesizing protein.","Modify,Grammar",Grammar
6728,2-9,2-9_v2_32@4,2-9_v1_33@4,"What appeared in two-dimensional image as a single file of erythrosomes between tumor nodules was not ""a rouleau of circulating erythrocytes"" <REF-18> .",What appeared in the two-dimensional image as a single file of erythrosomes between the tumor nodules was not a rouleau of circulating erythrocytes.,"Modify,Fact/Evidence",Fact/Evidence
6729,2-9,2-9_v2_35@2,2-9_v1_36@2,In some regions ECs converting into erythrosomes (hemogenic endothelium) were also seen (Figure 2 in <REF-57> ).,"In some regions, ECs converting into erythrosomes (hemogenic endothelium) were also seen (Figure 8 in <REF-54> ).","Modify,Grammar",Grammar
6730,2-9,2-9_v2_43@0,2-9_v1_41@0,Survival of the ectopically implanted breast tumor cells for three weeks without support of host circulatory system was possible due to the erythrogenic autophagy <REF-41> .,Survival of the ectopically implanted breast tumor cells for three weeks without the support of a host circulatory system was possible due to the erythrogenic autophagy <REF-36> .,"Modify,Grammar",Grammar
6731,2-9,2-9_v2_43@4,2-9_v1_42@3,"Non-vasculogenic tumors do not grow over several weeks although the tumor cells keep proliferating at rate similar to that of vasculogenic tumors; ""They have no or non-functional vessels"" <REF-59> .","Non-vasculogenic tumors do not grow over several weeks although the tumor cells keep proliferating at a rate similar to that of vasculogenic tumors; ""They have no or non-functional vessels"" <REF-55> .","Modify,Grammar",Grammar
6732,2-9,2-9_v2_43@6,2-9_v1_42@5,"That was, in fact, another experimental result demonstrating that some tumor cells could survive at the expense of the others.","That was, in fact, an experimental result demonstrating that some tumor cells could survive at the expense of others.","Modify,Clarity",Clarity
6733,2-9,2-9_v2_52@0,2-9_v1_42@6,The postnatal extramedullar erythropoiesis at the location other than bone marrow ( medulla ossea ) was observed previously in spleen <REF-75> – <REF-78> and adipocytic tissues <REF-79> .,Such postnatal extramedullar erythropoiesis at a location other than the bone marrow ( medulla ossea ) was not unprecedented; spleen <REF-56> – <REF-59> and adipocytic tissues <REF-60> have that potential as well.,"Modify,Clarity",Clarity
6734,2-9,2-9_v2_44@0,2-9_v1_43@0,The relevance of the variable metabolism within a single tumor nodule was that tumor derived erythrosomes might indeed extend viability of adjacent tumor cells by supplying them with vital energy in absence of vasculature.,The relevance of the variable metabolism within a single tumor nodule was that tumor-derived erythrosomes might indeed extend viability of adjacent tumor cells by supplying them with vital energy in the absence of vasculature.,"Modify,Grammar",Grammar
6735,2-9,2-9_v2_44@4,2-9_v1_43@1,"Hemoglobin has evolved to bind oxygen cooperatively, i.e., most efficiently when it is abundant (in lungs where the oxygen concentration is about 100 torr) and gradually less and less efficiently as erythrosomes move through arteries and veins (in peripheral tissues the oxygen concentration is about 20 torr) <REF-61> .","Hemoglobin has evolved to bind oxygen cooperatively, i.e., most efficiently when it is abundant (in lungs where the oxygen concentration is about 100 torr) and gradually less and less efficiently as erythrosomes move through arteries and veins (in peripheral tissues, the oxygen concentration is about 20 torr) <REF-61> .","Modify,Grammar",Grammar
6736,2-9,2-9_v2_5@1,2-9_v1_5@1,"A developmental regulatory program involved in embryo implantation, referred to as the ""epithelial-mesenchymal transition"" (EMT) <REF-1> – <REF-4> was adopted to explain how transformed epithelial cells could acquire ability to metastasize, i.e., to invade surrounding nonmalignant tissues and to disseminate, in a multistep process including entering and leaving the circulatory system <REF-5> , <REF-6> .","A developmental regulatory program involved in embryo implantation, referred to as the ""epithelial-mesenchymal transition"" (EMT) <REF-1> – <REF-4> was adopted to explain how transformed epithelial cells could acquire the ability to metastasize, i.e., to invade surrounding nonmalignant tissues and to disseminate, in a multistep process including entering and leaving the circulatory system <REF-5> , <REF-6> .","Modify,Grammar",Grammar
6737,2-9,2-9_v2_45@4,2-9_v1_44@0,The potential to convert into erythrosomes was not limited to erythropoietic lineage derived from myeloid precursors.,The ability to convert into erythrosomes was not limited to erythropoietic lineage derived from myeloid precursors.,"Modify,Clarity",Clarity
6738,2-9,2-9_v2_45@5,2-9_v1_44@1,"Understandably so, given that the inducing factor, hypoxia, affected the most fundamental function of living cells, i.e., the respiration, generating vital energy aerobically.","Understandably so, because the inducing factor, hypoxia, affected the most fundamental function of all living cells, i.e., the respiration, generating vital energy aerobically.","Modify,Clarity",Clarity
6739,2-9,2-9_v2_45@7,2-9_v1_44@3,That metabolic requirement being shared by all cells experiencing hypoxia imposed formation of similar ultrastructural features on all of them (convergence).,That metabolic pathway being shared by all cells experiencing hypoxia imposed the formation of similar ultrastructural features on all of them (convergence).,"Modify,Clarity",Clarity
6740,2-9,2-9_v2_47@3,2-9_v1_46@3,"In other words, the heterologous environment did not kill the transplanted tumor but gradually the exogenous tissue acquired the ability to engage in the paracrine dialog with local TSCs, (perhaps by acquiring proper cell adhesion molecules <REF-62> ) or the tumor activated its own SCs (CSCs).","In other words, the heterologous environment did not kill the transplanted tumor but gradually the exogenous tissue acquired the ability to engage in the paracrine dialog with local TSCs (perhaps by acquiring proper cell adhesion molecules <REF-62> ), or the tumor activated its own SCs (CSCs).","Modify,Grammar",Grammar
6741,2-9,2-9_v2_47@4,2-9_v1_46@4,"Trans-differentiation of tumor SCs into ECs was observed here and also reported earlier in glioblastoma <REF-14> , <REF-63> – <REF-65> .",Trans-differentiation of tumor SCs into ECs was also reported earlier in glioblastoma <REF-63> – <REF-66> .,"Modify,Fact/Evidence",Fact/Evidence
6742,2-9,2-9_v2_47@7,2-9_v1_46@7,"Reported dormant tumors had < 1 mm “diameter”, possibly including fibroblastic coats and necrotic centers <REF-66> .","Reported dormant tumors had < 1 mm diameter, possibly including fibroblastic coats and necrotic centers <REF-67> .","Modify,Grammar",Grammar
6743,2-9,2-9_v2_48@0,2-9_v1_47@0,Capsular vaso-mimicry as cellular mechanism of metastasis,Capsular vaso-mimicry as a cellular mechanism of metastasis,"Modify,Grammar",Grammar
6744,2-9,2-9_v2_49@0,2-9_v1_48@0,"Two cellular mechanisms normally beneficial to the organism when acting independently, one involved in tissue nourishing and the other in healing, i.e., erythrogenesis and scar formation (or foreign body encapsulation <REF-55> ) respectively, became deleterious by creating the capsular vaso-mimicry when they coincided in the ectopically implanted tumor.","Two cellular mechanisms normally beneficial to the organism when acting independently, one involved in tissue nourishment and the other in healing, i.e., erythrogenesis and scar formation (or foreign body encapsulation) respectively, became deleterious by creating the capsular vaso-mimicry when they coincided in the ectopically implanted tumor.","Modify,Fact/Evidence",Fact/Evidence
6745,2-9,2-9_v2_49@8,2-9_v1_48@8,"At the same time, the loss of attachment to other cells could facilitate their dissemination by breaking tumor tissue into small cell clusters or single cells that could be carried away by blood flow.","At the same time, the loss of attachment to other cells could facilitate their dissemination by breaking the tumor tissue into small cell clusters or single cells that could be carried away by blood flow.","Modify,Grammar",Grammar
6746,2-9,2-9_v2_5@2,2-9_v1_5@2,"However, no satisfactory mechanism for the spreading of non-epithelial tumors to secondary locations was proposed.","However, no satisfactory mechanism for the spread of non-epithelial tumors to secondary locations was proposed.","Modify,Grammar",Grammar
6747,2-9,2-9_v2_50@2,2-9_v1_49@2,"Clusters of the primary tumor cells could be passively carried to different tissues by blood flow and become immobilized when they reached vessels narrower than their own dimensions, in a tissue non-specific manner.","Clusters of primary tumor cells could be passively carried to different tissues by blood flow and become immobilized when they reached vessels narrower than their own dimensions, in a tissue non-specific manner.","Modify,Grammar",Grammar
6748,2-9,2-9_v2_50@5,2-9_v1_49@5,"Liver being formed relatively early during embryogenesis and later maintaining primitive vasculature might be most compatible with tumors for that reason and therefore most prone to metastases, as observed clinically and shown experimentally <REF-70> .","Liver being formed relatively early during embryogenesis and later maintaining primitive vasculature might be most compatible with tumors for that reason and therefore most prone to metastases, as observed clinically.","Modify,Fact/Evidence",Fact/Evidence
6749,2-9,2-9_v2_5@3,2-9_v1_5@3,"Therefore, an alternative to EMT regulatory programs playing a role in invasiveness of carcinoma cells should also be considered, as pointed out elsewhere <REF-7> .","Therefore, an alternative to EMT regulatory programs playing a role in invasiveness of carcinoma cells should be considered, as pointed out elsewhere <REF-7> .","Modify,Clarity",Clarity
6750,2-9,2-9_v2_51@4,2-9_v1_49@12,That is how anatomically distant but phenotypically compatible tissue could become activated by the tumor before metastasizing cells got there.,That is how anatomically distant but phenotypicly compatible tissue could become activated by the tumor before metastasizing cells got there.,"Modify,Grammar",Grammar
6751,2-9,2-9_v2_54@1,2-9_v1_51@1,"The remnants of cells that produced erythrosomes could be responsible for PAS staining due to their glyco-lipid components and, more importantly, for fusion with capillaries of main circulatory system, at stages later than analyzed here.","The remnants of cells that produced erythrosomes could be responsible for PAS staining due to their glyco-lipid components and, more importantly, for fusion with capillaries of the main circulatory system, at stages later than analyzed here.","Modify,Grammar",Grammar
6752,2-9,2-9_v2_6@0,2-9_v1_6@0,"Attempts made to elucidate the cellular mechanism of metastasis-initiating events included retrospective extrapolation from the distribution of established metastases, namely the preference of specific tumors to metastasize in certain organs but not in others.","Attempts made to elucidate the cellular mechanism of metastasis-initiating events have included retrospective extrapolation from the distribution of established metastases, namely the preference of specific tumors to metastasize in certain organs but not in others.","Modify,Grammar",Grammar
6753,2-9,2-9_v2_54@2,2-9_v1_51@2,"Our tumors were significantly smaller (""diameter"" of < 1 mm) than those described in the literature (1 cm or more).",Our tumors were significantly smaller (diameter of < 1 mm) than those described in the literature (1 cm or more).,"Modify,Grammar",Grammar
6754,2-9,2-9_v2_54@3,2-9_v1_51@3,Lack of hierarchy in the network pattern of the aggressive tumors suggested a lack of blood circulation.,A lack of hierarchy in the network pattern of the aggressive tumors suggested a lack of blood circulation.,"Modify,Grammar",Grammar
6755,2-9,2-9_v2_54@6,2-9_v1_51@6,"Therefore, that kind of mimicry is probably a form of erythrogenic autophagy of fibroblasts associated with the presence of metastatic tumors.","Therefore, that kind of mimicry probably is a form of fibroblastic autophagy associated with the presence of metastatic tumors.","Modify,Claim",Claim
6756,2-9,2-9_v2_58@5,2-9_v1_53@4,"Shunting of inspired oxygen into tumor venules, presumed to occur due to arterio-venous anastomoses (Figure 10 in <REF-83> ) could alternatively be explained by stable saturation of hemoglobin located in non-circulating erythrosomes within tumor capsule as well as within the regions mimicking vessels ( Figure 1 & Figure S1 ).","Shunting of inspired oxygen into tumor venules, presumed to occur due to arterio-venous anastomoses (Figure 10 in <REF-76> ) could alternatively be explained by stable saturation of hemoglobin located in non-circulating erythrosomes within the tumor capsule, as well as within the regions mimicking vessels ( Figure 1 & Figure S1 ).","Modify,Grammar",Grammar
6757,2-9,2-9_v2_58@10,2-9_v1_53@9,"Consequently, it should not be surprising that increased oxygenation of breast adenocarcinoma by treatment with, for example, darbepoetin alpha, had no desirable effect on the tumor’s responsiveness to radiotherapy <REF-86> .","Consequently, it shouldn’t be surprising that increased oxygenation of breast adenocarcinoma by treatment with, for example, darbepoetin alpha, had no desirable effect on the tumor’s responsiveness to radiotherapy <REF-79> .","Modify,Grammar",Grammar
6758,2-9,2-9_v2_56@0,2-9_v1_55@0,"The distance between capillaries in tissue sections suggested that, within the 100–200 µm zones, cell membranes did not present a barrier for diffusion of nutrients as well as oxygen.","The distance between capillaries in tissue sections suggested that, within the 100–200 µm zones, cell membranes did not present a barrier for diffusion of nutrients or for oxygen.","Modify,Clarity",Clarity
6759,2-9,2-9_v2_56@4,2-9_v1_55@4,"Considering what we now know about cytoevolution leading to ECs differentiation <REF-41> , one could make a premise that luminal surface of the polarized endothelial cell was a functional equivalent of the inner membrane.","Considering what we now know about cytoevolution leading to ECs differentiation <REF-36> , one could make a premise that the luminal surface of the polarized endothelial cell was a functional equivalent of the inner membrane.","Modify,Grammar",Grammar
6760,2-9,2-9_v2_56@8,2-9_v1_55@8,Vascular lumen in that context would be a functional equivalent of intracellular vesicle.,Vascular lumen in that context would be a functional equivalent of an intracellular vesicle.,"Modify,Grammar",Grammar
6761,2-9,2-9_v2_56@9,2-9_v1_55@9,One could conclude that host fibroblasts encapsulating the tumor and creating the capsular vaso-mimicry by positioning themselves around erythrosomes should not present a barrier for the diffusion process because they did not go through the process of cytoevolution resulting in polarization of outer cell membrane into luminal and abluminal.,One could conclude that host fibroblasts encapsulating the tumor and creating the capsular vaso-mimicry by positioning themselves around erythrosomes should not present a barrier for the diffusion process because they did not go through the process of cytoevolution resulting in polarization of the outer cell membrane into luminal and abluminal.,"Modify,Grammar",Grammar
6762,2-9,2-9_v2_60@4,2-9_v1_57@4,"That way, they could salvage the remaining tumor cells in two ways: by secreting lactic acid <REF-56> or ATP <REF-54> (similarly to muscle cells and erythrocytes, respectively) and by initiating the vaso-mimicry.","That way, they could salvage the remaining tumor cells: by secreting lactic acid <REF-53> or ATP <REF-50> (similar to muscle cells and erythrocytes, respectively), and by initiating the vaso-mimicry.","Modify,Clarity",Clarity
6763,2-9,2-9_v2_60@6,2-9_v1_57@6,Creating the capsular vaso-mimicry would require sufficient numbers of cancer cells in the initiating nodule.,Establishing the metastatic tumors by creating the capsular vaso-mimicry required sufficient numbers of cancer cells in the initiating nodule.,"Modify,Claim",Claim
6764,2-9,2-9_v2_60@9,2-9_v1_57@9,By definition they could be referred to as cancer stem cells (CSCs).,"By definition, they could be referred to as cancer stem cells (CSCs).","Modify,Grammar",Grammar
6765,2-9,2-9_v2_60@11,2-9_v1_57@11,"If the enzyme plays a role in degradation of chromatin during the erythrogenic conversion of erythroblasts it could be associated with growth of any tissue, not only malignant.","If the enzyme plays a role in degradation of chromatin during the erythrogenic conversion of erythroblasts it could be associated with growth of any tissue, not only malignant tissue.","Modify,Clarity",Clarity
6766,2-9,2-9_v2_61@0,2-9_v1_58@0,"Eventually, the heterologous host TSCs also engaged into paracrine dialog with the tumor (via cytokines and growth factors <REF-71> ).","Eventually, the heterologous host TSCs also engaged in paracrine dialog with the tumor (via cytokines and growth factors <REF-72> ).","Modify,Grammar",Grammar
6767,2-9,2-9_v2_61@3,2-9_v1_58@3,"Such interpretation regarding the role of homologous TSCs in neo-vasculature morphogenesis was consistent with earlier reports stating that not bone marrow derived EC precursors <REF-89> , <REF-90> but TSCs from tumor microenvironment differentiated into vasculature that supported tumor growth <REF-91> .","Such interpretation regarding the role of homologous TSCs in neo-vasculature morphogenesis was consistent with earlier reports stating that it was not bone marrow-derived EC precursors <REF-83> , <REF-84> but rather TSCs from the tumor microenvironment differentiated into vasculature that supported the tumor growth <REF-85> .","Modify,Clarity",Clarity
6768,2-9,2-9_v2_62@0,2-9_v1_59@0,"On the other hand, after pseudo-orthotopic implantation, TSCs from grafted breast tissue formed vasculature for the breast tumor sooner because malignant tissues maintained some characteristics of their tissue of origin.","On the other hand, after pseudo-orthotopic implantation, TSCs from grafted breast tissue formed vasculature for the breast tumor because malignant tissues maintained some characteristics of their tissue of origin.","Modify,Clarity",Clarity
6769,2-9,2-9_v2_62@1,2-9_v1_59@1,The two related cell types were immediately ready to cooperate in executing the tissue self-organizing potential <REF-41> .,The two related cell types cooperated in executing the tissue self-organizing potential <REF-36> .,"Modify,Fact/Evidence",Fact/Evidence
6770,2-9,2-9_v2_62@3,2-9_v1_59@3,"However, in each case hematopoiesis supporting the growing tissues was extramedullar.","However, in each case, hematopoiesis supporting the growing tissues was extramedullar.","Modify,Grammar",Grammar
6771,2-9,2-9_v2_6@5,2-9_v1_6@5,"Metastases of particularly aggressive cancers of different types (ovarian <REF-10> , <REF-11> , prostate <REF-12> , <REF-13> , glioblastoma <REF-14> , as well as melanoma <REF-15> ) were associated with patterned vasculogenic mimicry, i.e., a network of periodic acid Schiff stained (glycoproteins containing <REF-16> ) ""loops"" that represented blood-containing micro-vascular ""channels"", generated by the aggressive tumor cells without participation of endothelial cells (ECs) and independently of angiogenesis <REF-17> , <REF-18> .","Metastases of particularly aggressive cancers of different types (not only melanoma <REF-10> ) were associated with patterned vasculogenic mimicry, i.e., a network of periodic acid Schiff-stained (glycoproteins containing <REF-11> ) ""loops"" that represented blood-containing micro-vascular ""channels"", generated by the aggressive tumor cells without participation of endothelial cells (ECs) and independently of angiogenesis <REF-12> , <REF-13> .","Modify,Fact/Evidence",Fact/Evidence
6772,2-9,2-9_v2_6@7,2-9_v1_6@7,"Elevated incidence of metastasis was also correlated with autophagy of internal organelles in tumor cells, although by what mechanism was not clear <REF-21> .","Elevated incidence of metastasis was also correlated with autophagy of internal organelles in tumor cells, although the mechanism behind this was unclear <REF-16> .","Modify,Clarity",Clarity
6773,2-9,2-9_v2_6@8,2-9_v1_6@8,"Reports based on different experiments suggested that depending on the context, autophagy could either stimulate or prevent cancer <REF-22> .","Reports based on a variety of experiments have suggested that, depending on the context, autophagy could either stimulate or prevent cancer <REF-17> .","Modify,Clarity",Clarity
6774,2-9,2-9_v2_6@9,2-9_v1_6@9,"Thus, the question regarding the way in which autophagy influenced metastasis remained unanswered <REF-23> .","Thus, the question regarding the way in which autophagy influences metastasis has remained unanswered <REF-18> .","Modify,Grammar",Grammar
6775,2-9,2-9_v2_6@10,2-9_v1_6@10,Two other intriguing issues were inefficiency of tumor formation in experimental settings and targeting of a selected sub-population of tumor cells by an anticancer drug.,Two other intriguing issues were the inefficiency of tumor formation in experimental settings and the targeting of a selected sub-population of tumor cells by an anticancer drug.,"Modify,Grammar",Grammar
6776,2-9,2-9_v2_6@11,2-9_v1_6@11,"(1) Theoretically a single cell could be capable of establishing the tumor but large numbers and a latent period were actually required <REF-24> , <REF-25> .","(1) Theoretically, a single cell could be capable of establishing the tumor but large numbers and a latent period were actually required <REF-19> , <REF-20> .","Modify,Grammar",Grammar
6777,2-9,2-9_v2_6@13,2-9_v1_6@13,"Those observations together with the heterogeneity of tumors, known for a long time but not fully understood <REF-25> , <REF-27> , suggested the existence of cancer stem cells (CSCs) in spite of the undifferentiated phenotype of the malignant cells.","Those observations, together with the heterogeneity of tumors known for a long time but not fully understood <REF-20> , <REF-22> , suggest the existence of cancer stem cells (CSCs) in spite of the undifferentiated phenotype of the malignant cells.","Modify,Grammar",Grammar
6778,2-9,2-9_v2_7@2,2-9_v1_7@2,"However, crossing the endothelial barrier by molecules that successfully reached the intended vascular destinations turned out to be another problem.","However, how molecules cross the endothelial barrier and successfully reach the intended vascular destination has turned out to be another problem.","Modify,Clarity",Clarity
6779,2-9,2-9_v2_7@5,2-9_v1_7@5,The issue of the tumor vessels permeability is rather perplexing.,The issue of the tumor vessels’ permeability is rather perplexing.,"Modify,Grammar",Grammar
6780,2-9,2-9_v2_7@6,2-9_v1_7@6,"On one hand the vessels prevent anticancer drugs from penetrating the tumors, and on the other they are known to be abnormally leaky <REF-39> , <REF-40> .","On the one hand, the vessels prevent anticancer drugs from penetrating the tumors, while on the other hand, they are known to be abnormally leaky <REF-34> , <REF-35> .","Modify,Clarity",Clarity
6781,2-9,2-9_v2_8@0,2-9_v1_8@0,We had observed earlier that in our model formation of the tumor vasculature (vessels and blood) could be accelerated by availability of homologous tissue stem cells (TSCs) from co-implanted graft <REF-41> .,"We had observed earlier that in our model, formation of the tumor vasculature (vessels and blood) could be accelerated by availability of homologous tissue stem cells (TSCs) from a co-implanted graft <REF-36> .","Modify,Grammar",Grammar
6782,2-9,2-9_v2_8@3,2-9_v1_8@3,"In addition to providing answers to those questions, the results shown below suggested a new cellular mechanism for initiating metastasis.","In addition to providing answers to those questions, the results shown below suggest a new cellular mechanism for initiating metastasis.","Modify,Grammar",Grammar
6783,2-9,2-9_v2_8@8,2-9_v1_9@2,"If correct, it would bring the focus of the future studies to the energy metabolism-related initial steps (as discussed elsewhere <REF-41> ) and could result in finding new ways for inhibiting some of them before the angiogenic switch has had a chance to evolve; therefore, potentially preventing the metastases.","If correct, it would bring the focus of future studies to the energy metabolism-related initial steps (as discussed elsewhere <REF-36> ) and could result in the identification of new methods of inhibiting some of these steps before the angiogenic switch has had a chance to evolve, therefore, potentially preventing the metastases.","Modify,Clarity",Clarity
6784,3-101,3-101_v2_22@5,,"Making such approximations explicit would also encourage the consideration of alternatives, e.g. the use of interval arithmetic.",,"Add,Claim",Claim
6785,3-101,3-101_v2_29@0,,"It is important to distinguish the use of randomness in heuristics from the use of probabilistic models, i.e. models that predict observable quantities as averages over probability distributions.",,"Add,Claim",Claim
6786,3-101,3-101_v2_29@1,,"The latter are in the same category as the global-minimum example discussed above: the numbers they predict are well-defined and computable, even though their computation is often beyond the limits of today’s computing technology.",,"Add,Claim",Claim
6787,3-101,3-101_v2_29@2,,"By contrast, a method such as k -means clustering, whose initialization step requires an arbitrary random choice, yields a different result each time it is applied, and there is no reason to attribute any meaning to the statistical distribution of these results.",,"Add,Claim",Claim
6788,3-101,3-101_v2_29@3,,"In fact, the distribution used in the initialization step is hardly ever documented because it is considered irrelevant.",,"Add,Claim",Claim
6789,3-101,3-101_v2_19@6,3-101_v1_19@6,"This solution contains transcendental functions (sines and cosines), which are computable to any desired precision.","But this solution contains transcendental functions (sines and cosines), which are not computable and therefore must be replaced by computable approximations, e.g. power-series expansions.","Modify,Claim",Claim
6790,3-101,3-101_v2_19@7,3-101_v1_19@7,"However, when three or more celestial bodies are included in the model, no analytical solution is available and the differential equations must be approximated by finite difference equations <REF-12> .","When three or more celestial bodies are included in the model, no analytical solution is available and the differential equations must be approximated by finite difference equations <REF-12> .","Modify,Clarity",Clarity
6791,3-101,3-101_v2_21@4,3-101_v1_21@4,"The very fact that a program runs and produces results proves that the model specification is complete and unambiguous, assuming that the computing system itself (hardware, operating system, compiler, etc.) works correctly and that the programming language it is written in has clearly defined semantics (which, unfortunately, is not the case for widely used languages such as C <REF-15> .","The very fact that a program runs and produces results proves that the model specification is complete and unambiguous, assuming that the computing system itself (hardware, operating system, compiler, etc.) works correctly.","Modify,Fact/Evidence",Fact/Evidence
6792,3-101,3-101_v2_25@1,3-101_v1_25@1,"I use the term “tool” in a general sense that includes both physical objects (e.g. microscopes, lasers, etc.) and mathematical theorems or procedures (e.g. calculus or algebra), but not mathematical axioms and definitions, which form the language of mathematics rather than its toolbox.","I use the term “tool” in a general sense that includes both physical objects (e.g. microscopes, lasers, etc.) and mathematical theorems or procedures (e.g. calculus or algebra).","Modify,Fact/Evidence",Fact/Evidence
6793,3-101,3-101_v2_29@4,3-101_v1_28@10,The role of such heuristics in computational science remains to be clarified.,The role of such probabilistic heuristics in computational science remains to be clarified.,"Modify,Clarity",Clarity
6794,3-101,3-101_v2_45@5,3-101_v1_44@5,This doesn’t mean that knowledge is lost rapidly.,This doesn’t mean of course that knowledge is lost rapidly.,"Modify,Clarity",Clarity
6795,3-101,3-101_v2_56@3,3-101_v1_55@3,"In particular, tools that are very similar in spirit to today’s computer algebra systems can be used to create approximations and combinations of scientific models.","In particular, tools that very similar in spirit to today’s computer algebra systems can be used to create approximations and combinations of scientific models.","Modify,Grammar",Grammar
6796,3-101,3-101_v2_60@0,3-101_v1_59@0,"The specificity of floating-point arithmetic deserves a special discussion, both because of its central role in much of scientific software and because of its reputation of being the source of intractable problems.","The specificity of floating-point arithmetic deserve a special discussion, both because of its central role in much of scientific software and because of its reputation of being the source of intractable problems.","Modify,Grammar",Grammar
6797,3-101,3-101_v2_2@4,3-101_v1_2@4,"As a consequence, these crucial pieces of information no longer enter the scientific record.","As a consequence, these crucial pieces of information have disappeared from the scientific record.","Modify,Clarity",Clarity
9274,7-407,,7-407_v1_36@0,,"Even if with our search activity we are quite sure to have reduced to a minimum the problem of publication bias, we performed a statistical estimation by using the Copas selection model which is recommended by Jin et al . (2015) .","Delete,Fact/Evidence",Fact/Evidence
9275,7-407,,7-407_v1_55@0,,"The search method used and the small number of people interested in this research field, guarantee that from an empirical point of view, any publication bias is almost absent.","Delete,Claim",Claim
9276,7-407,,7-407_v1_66@0,,"We found very interesting evidence of presentiment distilled from the conventional post-stimulus psychological research of Jolij and Bierman, who have performed a long series of experiments using a face detection paradigm.","Delete,Claim",Claim
9277,7-407,,7-407_v1_66@1,,"Additionally, the work of Kittenis found prestimulus effects from a conventional research program and pre-registered single-trial work of Mossbridge represent an important conceptual replication in countering both the use of questionable research practices and expectancy effects arguments.","Delete,Claim",Claim
9278,7-407,,7-407_v1_67@0,,"A promising development of this line of research is the development of paradigms that use software in real-time to predict meaningful future outcomes before they occur, e.g. ( Franklin et al ., 2014 )","Delete,Claim",Claim
9279,7-407,7-407_v2_56@0,,Preregistered vs No-preregistered studies,,"Add,Other",Other
9280,7-407,7-407_v2_57@0,,"This distinction is relevant for assessing the impact of the so-called Questionable Research Practices and in particular p-hacking ( Head et al ., 2015 ; John et al ., 2012 ).",,"Add,Fact/Evidence",Fact/Evidence
9281,7-407,7-407_v2_57@1,,"Preregistered studies must describe all details on how the data will be analyzed before their collection, thus reducing the degree of freedom available during and after data collection.",,"Add,Claim",Claim
9282,7-407,7-407_v2_58@0,,From our database it was possible to compare the estimate of the effect size obtained from the pre-registered studies with that obtained from the no-preregistered ones.,,"Add,Fact/Evidence",Fact/Evidence
9283,7-407,7-407_v2_58@1,,The results are presented in the following Table 4 .,,"Add,Fact/Evidence",Fact/Evidence
9284,7-407,7-407_v2_61@0,,"The effect size point estimates clearly show that the effect size of the preregistered studies is larger than that of the no-preregistered studies, however their precision estimates (see the 95% CI) reveal a considerable overlap and consequently they cannot be considered statistically different.",,"Add,Fact/Evidence",Fact/Evidence
9285,7-407,7-407_v2_63@0,,Our very comprehensive literature search is likely to have reduced the probability of a publication bias.,,"Add,Claim",Claim
9286,7-407,7-407_v2_63@1,,Nevertheless we added a statistical estimation of the publication bias.,,"Add,Fact/Evidence",Fact/Evidence
9287,7-407,7-407_v2_65@1,,"Similarly, more recent publication bias tests like the three-parameters selection model, the p-uniform* and the Vevea and Hedges’ weight-function model ( Vevea & Woods (2005) , seem not recommended for multilevel random meta-analyses with high heterogeneity like the present one.",,"Add,Claim",Claim
9288,7-407,7-407_v2_74@1,,"This phenomenon may hence be considered among the more reliable within those covered under the umbrella term “psi” (see Cardeña, 2018 for an exhaustive review of the evidence and the theoretical hypotheses of all these phenomena).",,"Add,Fact/Evidence",Fact/Evidence
9289,7-407,7-407_v2_77@0,,"In order to arrive at such an ambitious goal, it is necessary to achieve a high degree of correct classifications based on prestimulus activity at the level of each trial so that the number of false positives and false negatives is reduced to a bare minimum.",,"Add,Claim",Claim
9290,7-407,7-407_v2_77@1,,The experiments of Mossbridge (2017) ; Baumgart et al . (2017) and Jolij & Bierman (2017) are promising examples in this regard.,,"Add,Claim",Claim
9291,7-407,7-407_v2_80@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",,"Add,Fact/Evidence",Fact/Evidence
9292,7-407,,7-407_v1_9@8,,"Additionally, we also found increasing evidence of presentiment research piggybacking onto mainstream psychology programs, even informing aspects of the conventional research.","Delete,Fact/Evidence",Fact/Evidence
9293,7-407,7-407_v2_24@0,7-407_v1_21@0,"Excluded records were studies where the psychophysiological variables were analysed only after and not before the stimuli presentations ( Jin et al. , 2013 ) and with an unusual procedure ( Tressoldi et al. , 2015 ), i.e. using heart rate feedback to inform a voluntary decision to predict random positive or negative events.","Excluded records were studies were the psychophysiological variables were analysed only after and not before the stimuli presentations ( Jin et al. , 2013 ) and with an unusual procedure ( Tressoldi et al. , 2015 ), i.e. using heart rate feedback to inform a voluntary decision to predict random positive or negative events.","Modify,Grammar",Grammar
9294,7-407,7-407_v2_7@0,7-407_v1_4@2,The statistical estimation of the publication bias by using the Copas selection model suggest that the main findings are not contaminated by publication bias.,The statistical estimation of the publication bias by using the Copas model suggest that the main findings are not contaminated by publication bias.,"Modify,Clarity",Clarity
9295,7-407,7-407_v2_29@1,7-407_v1_26@1,The database along with all 19 papers are available from Tressoldi (2017) .,The database along with all 18 papers are available from Tressoldi (2017) .,"Modify,Fact/Evidence",Fact/Evidence
9296,7-407,7-407_v2_36@0,7-407_v1_33@0,"In order to control the reliability of the results, a second analysis was carried out by using a multilevel approach as suggested by Assink & Wibbelink (2016) implemented with the metafor package ( Viechtbauer, 2010 ) and reported in the Table S2 in the Supplementary File 3 .","In order to control the reliability of the results, a second analysis was carried out by using a multilevel approach as suggested by ( Assink & Wibbelink, 2016 ) implemented with the metafor package ( Viechtbauer, 2010 ) and reported in the Table S2 in the Supplementary File 3 .","Modify,Clarity",Clarity
9297,7-407,7-407_v2_37@0,7-407_v1_34@0,"A Bayesian meta-analysis was implemented with the brms package ( Bürkner, 2017 ).","The Bayesian meta-analysis was implemented with the brms package ( Bürkner, 2017 ).","Modify,Grammar",Grammar
9298,7-407,7-407_v2_52@0,7-407_v1_50@0,Another “sensitivity analysis” was carried out excluding the new Mossbridge and Tressoldi studies in order to control whether different authors could obtain similar results.,Another “sensitivity analysis” was carried out excluding the Mossbridge and the Tressoldi studies in order to control whether different authors could obtain similar results.,"Modify,Clarity",Clarity
9299,7-407,7-407_v2_55@0,7-407_v1_53@0,"Both the frequentist and the Bayesian analyses support the evidence of an overall main effect of approximately .28, and a small difference between the peer and non-peer reviewed studies.","Both the frequentist and the Bayesian analyses support the evidence of an overall main effect of approximately .29, and a small difference between the peer and non-peer reviewed studies.","Modify,Fact/Evidence",Fact/Evidence
9300,7-407,7-407_v2_65@2,7-407_v1_57@1,"Anyway, we applied the Copas selection model which is recommended by Jin et al . (2015) .",We hence applied the Copas selection model which is recommended by Jin et al . (2015) .,"Modify,Clarity",Clarity
9301,7-407,7-407_v2_10@3,7-407_v1_7@3,"Now imagine if such prognosticating ability was possible without any sensory or other inferential cues (see Mossbridge & Radin, 2018 for a review).",Now imagine if such prognosticating ability was possible without any sensory or other inferential cues.,"Modify,Fact/Evidence",Fact/Evidence
9302,7-407,7-407_v2_69@0,7-407_v1_62@0,"This update of the Mossbridge et al. (2012) meta-analysis related to the so called predictive anticipatory activity (PAA) responses to future random stimuli, covers the period January 2008- July 2018.","This update of the Mossbridge et al. (2012) meta-analysis related to the so called predictive anticipatory activity (PAA) responses to future random stimuli, covers the years 2008- October 2017.","Modify,Fact/Evidence",Fact/Evidence
9303,7-407,7-407_v2_69@1,7-407_v1_62@1,"Overall, we found 19 new studies describing a total of 36 effect sizes.","Overall, we found 18 new studies describing a total of 34 effect sizes.","Modify,Fact/Evidence",Fact/Evidence
9304,7-407,7-407_v2_0@0,7-407_v1_0@0,Predictive physiological anticipatory activity preceding seemingly unpredictable stimuli: An update of Mossbridge et al ’s meta-analysis,Predictive physiological anticipation preceding seemingly unpredictable stimuli: An update of Mossbridge et al ’s meta-analysis,"Modify,Clarity",Clarity
9305,7-407,7-407_v2_72@0,7-407_v1_65@0,"Furthermore, we did not find substantial differences between peer and non-peer reviewed papers as in the original paper, as the confidence intervals of their mean effect size, overlap considerably.","Furthermore, we did not find substantial differences between peer and not-peer reviewed papers as in the original paper.","Modify,Fact/Evidence",Fact/Evidence
9306,7-407,7-407_v2_74@0,7-407_v1_69@0,This update confirms the main results reported in Mossbridge et al . (2012) original meta-analysis and gives further support to the hypothesis of predictive physiological anticipatory activity of future random events.,This update confirms the main results reported in Mossbridge et al . (2012) original meta-analysis and gives further support to the hypothesis of predictive physiological anticipation of future random events.,"Modify,Clarity",Clarity
9307,7-407,7-407_v2_75@0,7-407_v1_70@0,The limitations of the present meta-analysis are similar to most meta-analyses which include non-preregistered studies.,"The limitations of the present meta-analysis are similar to most meta-analyses which include non pre-registered studies that cannot be controlled for the degree of freedoms in the methodology and data analysis in the course of their implementations, making them prone, for example, to the so-called “questionable research practices” ( John et al ., 2012 ).","Modify,Claim",Claim
9308,7-407,7-407_v2_75@1,7-407_v1_71@0,"The solution is that of prospective meta-analyses ( Watt & Kennedy, 2017 ), based on all preregistered studies where the methods and data analyses have been declared and made public beforehand.","The solution is that of prospective meta-analyses ( Watt & Kennedy, 2017 ), based on preregistered studies where the methods and data analyses have been declared and made public beforehand.","Modify,Clarity",Clarity
9309,7-407,7-407_v2_10@7,7-407_v1_7@7,"Disturbingly, moments before the stimulus is presented there are physiological changes ahead of time.","Disturbingly, moments before the stimulus is presented there are murmurings of activity, as if the body is predicting moments ahead of time.","Modify,Claim",Claim
9310,7-407,7-407_v2_11@1,7-407_v1_8@1,"In both of these approaches it is difficult to envision mundane strategies that might explain the anomalous pre-stimulus effects observed, and indeed, Mossbridge et al ., went to significant lengths in refuting the leading candidate – expectancy effects, both in the 2012 meta-analysis and in post-review exchanges with sceptical psychologists and physiologists.","In both of these approaches it is difficult to envision mundane strategies that might explain the anomalous pre-stimulus effects observed, and indeed, Mossbridge et al , went to significant lengths in refuting the leading candidate – expectancy effects, both in the 2012 meta-analysis and in post-review exchanges with sceptical psychologists and physiologists.","Modify,Grammar",Grammar
9311,7-407,7-407_v2_11@4,7-407_v1_8@4,The presentiment hypothesis calls for a difference between the pre-stimulus responses of the two stimulus categories and this is calculated across sessions.,The presentiment hypothesis calls for a difference between arousing and neutral pre-stimulus responses and this is calculated across sessions.,"Modify,Clarity",Clarity
9312,7-407,7-407_v2_12@0,7-407_v1_9@0,"Because of the high profile nature of Mossbridge et al ., (over 93,000 views as of January 2018) there has been a good number of replications in the few years since publication.","Because of the high profile nature of Mossbridge et al , (over 93,000 views as of January 2018) there has been a good number of replications in the few years since publication.","Modify,Grammar",Grammar
9313,7-407,7-407_v2_12@3,7-407_v1_9@3,"Because expectancy effects have been proposed as a potential mechanism to explain at least some of the presentiment effect, it is noteworthy that several experiments in this fresh cohort of studies tackle this head on by only analysing the first trial of a run.","Because expectancy effects have been forwarded to explain at least some of the presentiment effect, it is noteworthy that several experiments in this fresh cohort of studies tackle this head on by only analysing the first trial of a run.","Modify,Claim",Claim
9314,7-407,7-407_v2_12@6,7-407_v1_9@6,This provides another objective measure of the validity of the presentiment effect.,This provides a second objective measure of the validity of the presentiment effect.,"Modify,Clarity",Clarity
9315,7-407,7-407_v2_14@0,7-407_v1_11@0,"The whole procedure followed both the APA Meta-Analysis Reporting Standards ( APA Publications and Communications Board Working Group on Journal Article Reporting Standards, 2008 ), the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols (PRISMA) 2015 ( Moher et al ., 2015 ) and the reporting standards for literature searches and report inclusion ( Atkinson et al ., 2015 ).","The whole procedure followed both the APA Meta-Analysis Reporting Standards ( APA Publications and Communications Board Working Group on Journal Article Reporting Standards, 2008 ), the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols 2015 ( Moher et al ., 2015 ) and the reporting standards for literature searches and report inclusion ( Atkinson et al ., 2015 ).","Modify,Clarity",Clarity
9316,7-407,7-407_v2_17@0,7-407_v1_14@0,It is important to point out that these eligibility criteria are different from those used by Mossbridge et al. Those authors selected only studies where the anticipatory signals mirrored the post-stimulus ones.,It is important to point out that these eligibility criteria are different from those used by Mossbridge et al. Those authors selected only studies were the anticipatory signals mirrored the post-stimulus ones.,"Modify,Grammar",Grammar
9317,7-407,7-407_v2_17@1,7-407_v1_14@1,In addition we included all studies that used anticipatory signals to predict future events independently of the presence of post-stimulus physiological signals.,Differently we included all studies that used anticipatory signals to predict future events independently of the presence of post-stimulus physiological signals.,"Modify,Clarity",Clarity
9318,7-407,7-407_v2_17@2,7-407_v1_14@2,"For example, some authors, e.g. Mossbridge (2015) used heart rate variability to predict winning i.e. $4, versus losing outcomes without recording the post-stimulus physiological activity associated with hits and misses.","For example, some authors, e.g. Mossbridge (2015) used heart rate variability to predict winning i.e. $4, versus losing outcomes.","Modify,Fact/Evidence",Fact/Evidence
9319,7-407,7-407_v2_19@1,7-407_v1_16@1,"Furthermore, we emailed a request of the data of completed studies to all authors we knew were involved in this type of research.","Furthermore, we emailed a request of the data of completed studies to all authors we knew were involved in this type of investigations.","Modify,Clarity",Clarity
9320,7-407,7-407_v2_19@3,7-407_v1_16@3,"We searched all completed studies, both peer reviewed and non-peer reviewed, e.g. Ph.D dissertations, from January 2008 to June 2018.","We searched all completed studies, both peer reviewed and non-peer reviewed, e.g. Ph.D dissertations, from January 2008 to October 2017.","Modify,Fact/Evidence",Fact/Evidence
10759,9-1032,9-1032_v2_56@5,,Recent studies have also shown promising odour-blends of volatile organic compounds identified from domesticated grasses such as rice and pollens of maize and sugarcane.,,"Add,Fact/Evidence",Fact/Evidence
10760,9-1032,9-1032_v2_57@5,,"Potentially a more rigorous evaluation of the plant coverage using standard methods such as a quadrant frame which might have provided more detailed information on plant numbers, could have revealed more associations.",,"Add,Claim",Claim
10761,9-1032,9-1032_v2_57@6,,"However, given the high colonisation during the rainy season such method would be better applied during drier seasons.",,"Add,Claim",Claim
10762,9-1032,9-1032_v2_20@9,9-1032_v1_20@9,Culex larvae possess a siphon on the posterior part of their abdomen for breathing whereas Anopheles larvae have no siphon and rest horizontal to the water body <REF-29> .,Culex larvae possess siphon on the posterior part of their abdomen for breathing through at the interface of the water surface during resting whereas Anopheles larvae have no siphon and rest horizontal to the water body <REF-28> .,"Modify,Fact/Evidence",Fact/Evidence
10763,9-1032,9-1032_v2_20@13,9-1032_v1_20@13,Rearing of the field collected larvae was done in 1 L plastic containers.,"Rearing of the field collected larvae was done in 1 L plastic rectangle food mate (H67 × W126 × L184 mm, Kenpoly manufacturer, Nairobi, Kenya).","Modify,Fact/Evidence",Fact/Evidence
10764,9-1032,9-1032_v2_20@20,9-1032_v1_20@20,"Extraction of DNA was done for each mosquito separately using Tissue Kit (Quagen, GmbH Hilden, Germany).","Extraction of genomic DNA was done for each mosquito separately using Tissue Kit (Quagen, GmbH Hilden, Germany).","Modify,Clarity",Clarity
10765,9-1032,9-1032_v2_20@21,9-1032_v1_20@21,"The PCR was prepared by mixing PCR mix of 2 µl of 5XHot Firepol Blended Master Mix (Ready to Load), primers (0.5 µM each), DNA template (2 µl) and nuclease-free water (5 µl).","The PCR in a 10 µl (per sample) was prepared by mixing PCR mix of 2 µl of 5XHot Firepol Blended Master Mix (Ready to Load), primers (0.5 µM each), DNA template (2 µl) and nuclease-free water (5 µl).","Modify,Fact/Evidence",Fact/Evidence
10766,9-1032,9-1032_v2_20@23,9-1032_v1_20@23,"We used a Kyratec Thermal Cycler (SC300T-R2, Australia) for the thermal reactions.","We used Kyratec Thermal Cycler (SC300T-R2, Australia) for the thermal reactions.","Modify,Grammar",Grammar
10767,9-1032,9-1032_v2_21@2,9-1032_v1_21@2,"Vegetation coverage was estimated visually, always by the same field worker, as the proportion of the habitats covered with vegetations and categorized as (1) 1–25% (2) 25–50% (3) 50–75% (4) 75–100%.",Vegetation coverage was estimated visually as the proportion of the habitats covered with vegetations and categorized as (1) 1–25% (2) 25–50% (3) 50–75% (4) 75–100%.,"Modify,Fact/Evidence",Fact/Evidence
10768,9-1032,9-1032_v2_21@4,9-1032_v1_21@4,"The graminoid plants were identified to family using the morphology of their leaves (two or three-ranked; open or closed sheaths), and their stem type (three-sided or round; hollow or solid) using Revuelta <REF-33> .","The graminoid plants were identified to family using morphology of their leaves (two or three-ranked; open or closed sheaths), and their stem type (three-sided or round; hollow or solid) using Revuelta <REF-32> .","Modify,Grammar",Grammar
10769,9-1032,9-1032_v2_34@0,9-1032_v1_34@0,All the swamp habitats were bordered by graminoid plants along the water edges and had a high surface coverage.,All the swamp habitats were boarded by graminoid plants along the water edges and had a high surface coverage.,"Modify,Grammar",Grammar
10770,9-1032,9-1032_v2_4@8,9-1032_v1_4@8,"The presence of early instar larvae was significantly and positively associated with swamp habitat types (OR=22, 95% CI=6-86, P<0.001) and abundance of late Anopheles larvae (OR=359, CI=33-3941, P<0.001), and negatively associated with the presence of tadpoles (OR=0.1, CI=0.0.01-0.5, P=0.008).","The presence of early instar larvae was significantly and positively associated with swamp habitat types (OR=22, 95% CI=6-86, P<0.001) and abundance of late Anopheles larvae (OR=359, CI=33-3941, P<0.001), whilst the association was negative with tadpole presence (OR=0.1, CI=0.0.01-0.5, P=0.008).","Modify,Clarity",Clarity
10771,9-1032,9-1032_v2_42@3,9-1032_v1_42@3,All six species of Anopheles mosquitoes were recorded in swamp habitats.,These six species of Anopheles mosquitoes were recorded in swamp habitats.,"Modify,Clarity",Clarity
10772,9-1032,9-1032_v2_42@5,9-1032_v1_42@5,"However, only three species of Anopheles mosquitoes ( An. arabiensis , An. ziemanni, and An. pharoensis ) were collected in habitats sparsely (1–25%) covered by graminoid plants.","However, only three species of Anopheles mosquitoes ( An. arabiensis , An. ziemanni, and An. pharoensis ) were collected in habitats sparsely (1–25%) covered by graminoids.","Modify,Clarity",Clarity
10773,9-1032,9-1032_v2_59@0,9-1032_v1_59@0,The presence and abundance of early instar Anopheles larvae were negatively associated with the presence of tadpoles.,The presence and abundance of early instar Anopheles larvae was negatively associated with the presence of tadpoles.,"Modify,Grammar",Grammar
10774,9-1032,9-1032_v2_62@3,9-1032_v1_62@3,"The habitats covered by these vegetations were abundantly colonized by early instar Anopheles larvae even though no specific preference for any of these could be detected, likely due to study limitations.","The habitats covered by this vegetation were abundantly colonized by early instar Anopheles larvae even though no specific preference for any of these could be detected, likely due to study limitations.","Modify,Grammar",Grammar
10775,9-1032,9-1032_v2_19@1,9-1032_v1_19@1,"The perimeter of every habitat was estimated, always by the same field worker for uniformity, by walking in large steps around the habitat.",The perimeter of every habitat was estimated by walking in large steps around the habitat.,"Modify,Fact/Evidence",Fact/Evidence
10802,9-113,9-113_v2_67@1,,"- - LiteratureCompilation: MAE=0.532, RMSE=0.785, r 2 =0.889)",,"Add,Fact/Evidence",Fact/Evidence
10803,9-113,9-113_v2_71@2,,"Since some molecules had to be omitted for prediction with OPERA due to none or multiple predicted pK a values, no consistent significance test could be performed for all comparisons.",,"Add,Claim",Claim
10804,9-113,9-113_v2_73@0,,The developed model offers the possibility to predict pK a values for monoprotic molecules with good accuracy.,,"Add,Claim",Claim
10805,9-113,9-113_v2_73@1,,"However, since the model has been trained exclusively with monoprotic molecules, only monoprotic molecules can be predicted properly.",,"Add,Claim",Claim
10806,9-113,9-113_v2_73@2,,In this respect the model is limited.,,"Add,Claim",Claim
10807,9-113,9-113_v2_73@3,,"Nevertheless, the results show that the performance for monoprotic molecules can compete with the performance of existing prediction tools.",,"Add,Claim",Claim
10808,9-113,9-113_v2_14@3,,"The Novartis data set consists of 280 unique molecules with a molecular weight between 129 and 670 daltons (mean value 348.68, standard deviation 94.17).",,"Add,Fact/Evidence",Fact/Evidence
10809,9-113,9-113_v2_14@4,,"The calculated LogP values vary between -1.54 and 6.30 (mean value 3.01, standard deviation 1.41).",,"Add,Fact/Evidence",Fact/Evidence
10810,9-113,9-113_v2_14@5,,The 280 molecules spread over 228 unique Murcko Scaffolds.,,"Add,Fact/Evidence",Fact/Evidence
10811,9-113,9-113_v2_14@6,,The ten most common murcko scaffolds cover 15% of the molecules of the total data set (42/280).,,"Add,Fact/Evidence",Fact/Evidence
10812,9-113,9-113_v2_14@7,,A histogram of the pairwise comparison between the training set and the two external test sets (Fingerprint: 4096 bit MorganFeatures radius=3) is given in Figure 2(A) and Figure 2(B),,"Add,Fact/Evidence",Fact/Evidence
10813,9-113,9-113_v2_22@3,,"To ensure that no training data was contained in the test data sets, the conical isomeric SMILES were checked for matches in both training and test data sets and corresponding hits were removed from the test data sets.",,"Add,Fact/Evidence",Fact/Evidence
10814,9-113,9-113_v2_56@0,,The compounds for which the pK a values between the different sources deviate by more than two units are as follows:,,"Add,Fact/Evidence",Fact/Evidence
10815,9-113,9-113_v2_60@0,,"Since the annotation about the experimental settings is not given in the DataWarrior file, we can only hypothesize that these differences are due to the different experimental settings.",,"Add,Claim",Claim
10816,9-113,9-113_v2_61@0,,Machine Learning,,"Add,Other",Other
10817,9-113,9-113_v2_20@7,9-113_v1_25@7,In the second configuration the parameter “gamma” was additionally set to the value “auto”.,"In the second configuration the parameter ""gamma"" was additionally set to the value ""auto"".","Modify,Grammar",Grammar
10818,9-113,9-113_v2_20@9,9-113_v1_25@9,"In the second configuration, early stopping was additionally activated, where 10% of the training data was separated as validation data.","In the second configuration, early stopping was additionally activated, where 10% of the training data is separated as validation data.","Modify,Grammar",Grammar
10819,9-113,9-113_v2_20@10,9-113_v1_25@10,"If the error of the validation data did not improve by more than 0.001 over ten training epochs, the training is stopped early to avoid overtraining.","If the error of the validation data does not improve by more than 0.001 over ten training epochs, the training is stopped early to avoid overtraining.","Modify,Grammar",Grammar
10820,9-113,9-113_v2_20@14,9-113_v1_25@14,This resulted in a total of seven different machine learning configurations.,This results in a total of seven different machine learning configurations.,"Modify,Grammar",Grammar
10821,9-113,9-113_v2_26@0,9-113_v1_31@0,First of all a working Miniconda/Anaconda installation is needed.,First of all you need a working Miniconda/Anaconda installation.,"Modify,Clarity",Clarity
10822,9-113,9-113_v2_26@1,9-113_v1_31@1,Miniconda can be downloaded at https://conda.io/en/latest/miniconda.html .,You can get Miniconda at https://conda.io/en/latest/miniconda.html .,"Modify,Clarity",Clarity
10823,9-113,9-113_v2_27@0,9-113_v1_32@0,"Now an environment named ""ml_pka"" with all needed dependencies can be created and activated with:","Now you can create an environment named ""ml_pka"" with all needed dependencies and activate it with:","Modify,Clarity",Clarity
10824,9-113,9-113_v2_29@0,9-113_v1_34@0,"Alternatively, a new environment can be created manually without the environment.yml file:",You can also create a new environment by yourself and install all dependencies without the environment.yml file:,"Modify,Claim",Claim
10825,9-113,9-113_v2_36@1,9-113_v1_41@1,To use the data preparation pipeline the repository folder hast to be entered and the created conda environment must be activated.,To use the data preparation pipeline you have to be in the repository folder and your conda environment have to be activated.,"Modify,Clarity",Clarity
10826,9-113,9-113_v2_36@2,9-113_v1_41@2,Additionally the Marvin <REF-10> commandline tool cxcalc and the QUACPAC <REF-26> commandline tool tautomers have to be added to the PATH variable.,Additionally the Marvin <REF-10> commandline tool cxcalc and the QUACPAC <REF-28> commandline tool tautomers have to be set in your PATH variable.,"Modify,Clarity",Clarity
10827,9-113,9-113_v2_37@0,9-113_v1_42@0,"Also the environment variables OE_LICENSE (containing the path to the OpenEye license file) and JAVA_HOME (referring to the Java installation folder, which is needed for cxcalc ) have to be set.","Also the environment variables OE_LICENSE (containing the path to your OpenEye license file) and JAVA_HOME (referring to the Java installation folder, which is needed for cxcalc ) have to be set.","Modify,Clarity",Clarity
10828,9-113,9-113_v2_38@0,9-113_v1_43@0,After preparation a small usage information can be displayed with bash run_pipeline.sh -h .,After preparation you can display a small usage information with bash run_pipeline.sh -h .,"Modify,Clarity",Clarity
10829,9-113,9-113_v2_40@1,9-113_v1_45@1,First of all the repository folder has to be entered and the created conda environment must be activated.,First of all you have to be in the repository folder and your conda environment have to be activated.,"Modify,Clarity",Clarity
10830,9-113,9-113_v2_40@2,9-113_v1_45@2,To use the prediction tool the machine learning model has to be retrained.,To use the prediction tool you have to retrain the machine learning model.,"Modify,Clarity",Clarity
10831,9-113,9-113_v2_40@3,9-113_v1_45@3,"To do so the training script should be called, it will train the 5-fold cross-validated Random Forest machine learning model using 12 cpu cores.","Therefore just call the training script, it will train the 5-fold cross-validated Random Forest machine learning model using 12 cpu cores.","Modify,Clarity",Clarity
10832,9-113,9-113_v2_40@4,9-113_v1_45@4,If the number of cores has to be adjusted the train_model.py can be edited by changing the value of the variable EST_JOBS .,If you want to adjust the number of cores you can edit the train_model.py by changing the value of the variable EST_JOBS .,"Modify,Clarity",Clarity
10833,9-113,9-113_v2_42@0,9-113_v1_47@0,To use the prediction tool with the trained model QUACPAC/Tautomers have to be available as mentioned in the section above.,To use the prediction tool with the trained model QUACPAC/Tautomers have to be available as it was mentioned in the section above.,"Modify,Clarity",Clarity
10834,9-113,9-113_v2_43@0,9-113_v1_48@0,Now the python script can be called with an SDF file and an output path:,Now you can call the python script with an SDF file and an output path:,"Modify,Clarity",Clarity
10835,9-113,9-113_v2_62@1,9-113_v1_52@1,"In terms of the mean absolute error, a random forest with scaled MorganFeatures (radius=3) and descriptors gave the best performing model (MAE=0.682, RMSE=1.032, r 2 =0.82).","In terms of the mean absolute error, a random forest with scaled MorganFeatures (radius=3) and descriptors gives the best performing model (MAE=0.682, RMSE=1.032, r 2 =0.82).","Modify,Grammar",Grammar
10836,9-113,9-113_v2_71@0,9-113_v1_52@4,"This showed that our model had a slightly better performance than Marvin for the LiteratureCompilation, but Marvin performed better for the Novartis dataset.","This shows that our model slightly outcompetes Marvin for the LiteratureCompilation, but Marvin performs better for the Novartis dataset.","Modify,Clarity",Clarity
10837,9-113,9-113_v2_71@1,9-113_v1_52@5,"For both data sets, our models <REF-17> had a better predictive performance than the OPERA tool.","For both data sets, our models <REF-17> have a better predictive performance than the OPERA tool.","Modify,Grammar",Grammar
10838,9-113,9-113_v2_4@6,9-113_v1_4@6,"In particular, the publication by Williams et al . <REF-15> makes use of a publicly available data set provided by the application DataWarrior <REF-16> and provides a freely available pK a prediction tool called OPERA.","In particular, the publication by Williams et al . <REF-15> make use of a publicly available data set provided by the application DataWarrior <REF-16> and provides a freely available pK a prediction tool called OPERA.","Modify,Grammar",Grammar
10839,9-113,9-113_v2_48@0,9-113_v1_8@0,"One crucial point in the field of pK a measurements (and its usage for pK a predictions) was linked to the different experimental methods <REF-25> , <REF-30> .","One crucial point in the field of pK a measurements (and its usage for pK a predictions) is linked to the different experimental methods <REF-25> , <REF-26> .","Modify,Grammar",Grammar
10840,9-113,9-113_v2_48@1,9-113_v1_8@1,"Based on the Novartis set, the correlation between capillary electrophoresis and potentiometric measurements (for 15 data points) was convincing enough (mean absolute error (MAE)=0.202, root mean squared error (RMSE)=0.264, correlation coefficient r 2 =0.981) for us to combine pK a measurements from these different experimental methods (see Figure 3 ).","Based on the Novartis set, the correlation between capillary electrophoresis and potentiometric measurements (for 15 data points) is convincing enough (mean absolute error (MAE)=0.202, root mean squared error (RMSE)=0.264, correlation coefficient r 2 =0.981) for us to combine pK a measurements from these different experimental methods (see Figure 1 ).","Modify,Grammar",Grammar
10841,9-113,9-113_v2_52@0,9-113_v1_11@0,We also compared the pK a values of 187 monoprotic molecules contained in both the ChEMBL and DataWarrior data sets.,"We also compare the overlap of the filtered (see next section) ChEMBL and DataWarrior data sets, 187 monoprotic molecules could be identified in both sources.","Modify,Fact/Evidence",Fact/Evidence
10842,9-113,9-113_v2_52@1,9-113_v1_11@1,"Due to the missing annotation, it remained unclear if different experimental methods were used or multiple measurements with the same experimental method have been performed (or a mixture of both).","Due to the missing annotation, it remains unclear if different experimental methods were used or multiple measurements with the same experimental method have been performed (or a mixture of both).","Modify,Grammar",Grammar
10843,9-113,9-113_v2_52@2,9-113_v1_11@2,"Either way, this comparison was an additional proof-of-concept that the ChEMBL and DataWarrior pK a data sources can be combined after careful curation.","Either way, this comparison is an additional proof-of-concept that the ChEMBL and DataWarrior pK a data sources can be combined after careful curation.","Modify,Grammar",Grammar
10844,9-113,9-113_v2_52@4,9-113_v1_11@4,"The correlation coefficient between the annotated pK a values for these two data sets r 2 was 0.949, the MAE was 0.275, and the RMSE was 0.576.","The correlation coefficient between the annotated pK a values for these two data sets r 2 is 0.949, the MAE is 0.275, and the RMSE is 0.576.","Modify,Grammar",Grammar
10845,9-113,9-113_v2_73@4,9-113_v1_58@0,The good performance of Marvin on the Novartis set is interesting to note: the RMSE was almost 0.4 units better than our top performing model.,The good performance of Marvin on the Novartis set is interesting to note: the RMSE is almost 0.4 units better than our top performing model.,"Modify,Grammar",Grammar
10846,9-113,9-113_v2_73@7,9-113_v1_58@3,"In contrast, Marvin performed slightly worse than our top model on the LiteratureCompilation.","In contrast, Marvin performs slightly worse than our top model on the LiteratureCompilation.","Modify,Grammar",Grammar
10847,9-113,9-113_v2_73@8,9-113_v1_58@4,The OPERA tool performed significantly worse than our model on both external test sets.,The OPERA tool performs significantly worse than our model on both external test sets.,"Modify,Grammar",Grammar
10848,9-113,9-113_v2_73@9,9-113_v1_58@5,We assume that the addition of 2470 ChEMBL pK a – datapoints to our training set which were not part of the OPERA training set led to this drop in predictive performance.,We assume that the addition of 2470 ChEMBL pKa-datapoints to our training set which are not part of the OPERA training set leads to this drop in predictive performance.,"Modify,Grammar",Grammar
10849,9-113,9-113_v2_73@10,9-113_v1_58@6,"In addition, the pre-processing of the data was performed differently by OPERA in comparison to our pre-processing procedure.","In addition, the pre-processing of the data is performed differently by OPERA in comparison to our pre-processing procedure.","Modify,Grammar",Grammar
10850,9-113,9-113_v2_8@1,9-113_v1_15@1,"The following restrictions were made: it must be a physicochemical assay, the measurements must be taken from scientific literature, the assay must be in “small-molecule physicochemical format” and the organism taxonomy must be set to “N/A”.","The following restrictions were made: it must be a physicochemical assay, the measurements must be taken from scientific literature, the assay must be in ""small-molecule physicochemical format"" and the organism taxonomy must be set to ""N/A"".","Modify,Grammar",Grammar
10851,9-113,9-113_v2_8@4,9-113_v1_15@4,"Only pK a measurements, i.e. ChEMBL activities, were taken into account that were specified as exact (“standard_relation” equals “=”) and for which one of the following names was specified as “standard_type”: “pka”, “pka value”, “pka1”, “pka2”, “pka3” or “pka4” (case-insensitive).","Only pKa measurements, i.e. ChEMBL activities, were taken into account that were specified as exact (""standard_relation"" equals ""="") and for which one of the following names was specified as ""standard_type"": ""pka"", ""pka value"", ""pka1"", ""pka2"", ""pka3"" or ""pka4"" (case-insensitive).","Modify,Grammar",Grammar
10852,9-113,9-113_v2_11@2,9-113_v1_18@2,- Filtering by Lipinski‘s rule of five (one violation allowed),- Filter by Lipinski‘s rule of five (one violation allowed),"Modify,Grammar",Grammar
10853,9-113,9-113_v2_13@2,9-113_v1_20@2,All molecules were then combined on the basis of the canonical isomeric SMILES.,All molecules were then combined on the basis of the isomeric SMILES.,"Modify,Clarity",Clarity
10854,9-113,9-113_v2_20@0,9-113_v1_25@0,"First, to simplify cross-validation, a class “CVRegressor” was defined, which can serve as a wrapper for any regressor implementing the Scikit-Learn <REF-27> interface.","First, to simplify cross-validation, a class ""CVRegressor"" was defined, which can serve as a wrapper for any regressor implementing the Scikit-Learn <REF-29> interface.","Modify,Fact/Evidence",Fact/Evidence
10855,9-113,9-113_v2_20@2,9-113_v1_25@2,"Next, 196 of the 200 available RDKit descriptors (“MaxPartialCharge”, “MinPartialCharge”, “MaxAbsPartialCharge” and “MinAbsPartialCharge” were not used because they are computed as “NaN” for many molecules), and a 4096-bit long MorganFeature fingerprint with radius 3 were calculated for the training data set.","Next, 196 of the 200 available RDKit descriptors (""MaxPartialCharge"", ""MinPartialCharge"", ""MaxAbsPartialCharge"" and ""MinAbsPartialCharge"" were not used because they are computed as ""NaN"" for many molecules), and a 4096-bit long MorganFeature fingerprint with radius 3 were calculated for the training data set.","Modify,Grammar",Grammar
