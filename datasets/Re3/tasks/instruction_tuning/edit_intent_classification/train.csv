edit_index,doc_name,node_ix_src,node_ix_tgt,text_src,text_tgt,gold,label
313,120-ARR,,120-ARR_v1_62@2,,8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,"Delete,Fact/Evidence",Fact/Evidence
314,120-ARR,,120-ARR_v1_11@2,,"For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).","Delete,Fact/Evidence",Fact/Evidence
315,120-ARR,120-ARR_v2_27@2,,The basic definition is a set of features that are correlated but not causally related.,,"Add,Claim",Claim
316,120-ARR,120-ARR_v2_28@5,,"This definition does not address the nature of the feature (genuine or not), but does make an implicit assumption that such features are of high importance (e.g., high pointwise mutual information values with the corresponding label; Gururangan et al., 2018).",,"Add,Claim",Claim
317,120-ARR,120-ARR_v2_28@6,,"This definition is no longer subjective in terms of the genuineness of the feature, but is still subjective in the level of effect on generalizability (i.e., what is a high value of PMI?).",,"Add,Claim",Claim
318,120-ARR,120-ARR_v2_51@2,,"We argue that doing so is a desired strategy in many cases (though a preferred strategy might be to interact of abstain from making a decisive prediction, see Sec. 4.2).",,"Add,Claim",Claim
319,120-ARR,120-ARR_v2_52@0,,We also acknowledge that correlations in the real world can be misleading.,,"Add,Claim",Claim
320,120-ARR,120-ARR_v2_52@1,,"For instance, people often mistake the biggest commercial city in some countries for their capital (e.g., Istanbul in Turkey), potentially due to the high correlation between the two.",,"Add,Claim",Claim
321,120-ARR,120-ARR_v2_52@2,,"In such cases, relying on the fallback option might lead to prediction error.",,"Add,Claim",Claim
322,120-ARR,120-ARR_v2_52@3,,"However, we argue that following the human strategy of relying on a fallback option in cases of uncertainty will promote models' communication abilities.",,"Add,Claim",Claim
323,120-ARR,120-ARR_v2_62@0,,"7 Though we should continually assess the challenge negation poses on the most recent models (Bowman, 2022).",,"Add,Fact/Evidence",Fact/Evidence
324,120-ARR,120-ARR_v2_62@1,,"8 We recognize that editing pretrained corpora poses significant challenges due to their immense size, as demonstrated by recent efforts such as corpus analysis and deduplication (Lee et al., 2022).",,"Add,Fact/Evidence",Fact/Evidence
325,120-ARR,120-ARR_v2_63@0,,9 See Abend and Rappoport (2017) for a survey.,,"Add,Fact/Evidence",Fact/Evidence
326,120-ARR,120-ARR_v2_79@0,,"Finally, concurrent to this work, Eisenstein (2022) discussed several types of spurious correlations in the context of causality theory (Pearl, 2009), and used a toy example to demonstrate their different effects on models.",,"Add,Fact/Evidence",Fact/Evidence
327,120-ARR,120-ARR_v2_79@1,,"They concluded that domain knowledge is required to identify the correlations that are indeed spurious, i.e., those that might challenge the generalization ability of models.",,"Add,Fact/Evidence",Fact/Evidence
328,120-ARR,120-ARR_v2_28@8,120-ARR_v1_28@2,"They then defined a class of competent datasets, where the marginal probability for every feature is uniform over the class label, i.e., for any feature x i and label y ∈ Y , p(y|x i ) = 1 |Y | , thus limiting models from picking up any correlation between single features and output labels.","They then defined a class of competent datasets, where the marginal probability for every feature is uniform over the class label, i.e., ∀x ∈ X , y ∈ Y, p(y|x) = 1 |Y | , thus limiting models from picking up any correlation between single features and output labels.","Modify,Fact/Evidence",Fact/Evidence
329,120-ARR,120-ARR_v2_32@0,120-ARR_v1_32@0,"Consider the toy dataset for the task of sentiment analysis shown in Tab. 3, with vocabulary V ={good, bad, not, very}, and label set Y = { +, − }.","Consider the toy dataset for the task of sentiment analysis shown in Tab. 3, with vocabulary V ={good,bad,not,very}, and label set Y ={+,− }.","Modify,Grammar",Grammar
330,120-ARR,120-ARR_v2_35@2,120-ARR_v1_33@2,"In this case, the model would make the wrong prediction for the first test example in Test (as it has learned that very good is a feature that indicates positive sentiment), and similarly, will make a random prediction for the second test example, which does not contain any two-word feature seen during training.","In this case, the model would make the wrong prediction for the first test example in Test (as it has learned that very good is a feature that indicates positive sentiment), and similarly, will make a random prediction for the second test example, which does not contain any two-word features seen during training.","Modify,Grammar",Grammar
331,120-ARR,120-ARR_v2_43@0,120-ARR_v1_42@0,"Combining the two observations, we are left with the question of the potential intersection between balancing too much and balancing too little: does a sweet spot exist for which no spurious correlations are found in the dataset, but enough learnable signal is left? And even if so, would a balancing algorithm, whether by augmentation or filtering, be able to find it? We leave these questions for future work, but note that addressing them is a prerequisite for the theoretical and practical application of dataset balancing for mitigating spurious correlations.","Combining the two observations, we are left with the question of the potential intersection between balancing too much and balancing too little: does a sweet spot exist for which no spurious correlations are found in the dataset, but enough learnable signal is left? And even if so, would a balancing algorithm (whether by augmentation or filtering) be able to find it? We leave these questions for future work, but note that addressing them is a prerequisite for the theoretical and practical application of dataset balancing for mitigating spurious correlations.","Modify,Grammar",Grammar
332,120-ARR,120-ARR_v2_2@0,120-ARR_v1_2@0,"Recent work has shown that deep learning models in NLP are highly sensitive to lowlevel correlations between simple features and specific output labels, leading to overfitting and lack of generalization.","Recent work have shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to overfitting and lack of generalization.","Modify,Grammar",Grammar
333,120-ARR,120-ARR_v2_46@0,120-ARR_v1_44@1,"The practice of dataset balancing is designed to prevent models from learning that some words or expressions have a common fallback meaning that can stem from dataset artifacts (e.g., ""cat"" as an indicator of contradiction) but also from cultural and historical contexts (e.g., Biden is the U.S. president in 2022).","The practice of dataset balancing is designed to prevent models from learning that some words or expressions have a common fallback meaning that can stem from dataset idiosyncrasies (e.g., ""cat"" as an indicator of contradiction) but also from cultural and historical contexts (e.g., Biden is the U.S. president).","Modify,Fact/Evidence",Fact/Evidence
334,120-ARR,120-ARR_v2_46@3,120-ARR_v1_44@4,We argue that the ability to use them is a central ability of language understanding.,Here we argue that the ability to use them is a central ability of language understanding.,"Modify,Clarity",Clarity
335,120-ARR,120-ARR_v2_6@0,120-ARR_v1_6@0,"The year 2019 Donald Trump The West Wing, season 1 Josiah ""Jed"" Bartlet Table 1: Context, whether explicit or implicit, matters in textual understanding, as exemplified by the question ""who is the president of the U.S.?"". E.g., in the first line, given no other context, a QA system should provide the most sensible fallback answer (Joe Biden, at the time of writing).","The year 2019 Donald Trump The West Wing, season 1 Josiah ""Jed"" Bartlet Table 1: Context, whether explicit or implicit, matters in textual understanding, as exemplified by the question ""who is the president of the U.S.?"". E.g., in the first line, given no other context, a QA system should provide the most sensible fallback answer (Joe Biden).","Modify,Fact/Evidence",Fact/Evidence
336,120-ARR,120-ARR_v2_65@1,120-ARR_v1_61@2,"Interestingly, three different interpretation methods (simple gradient visualization, Simonyan et al., 2014;integrated gradient visualization, Sundararajan et al., 2017;and SmoothGrad, Smilkov et al., 2017) all find the word ""great"" or ""surprise"" to be one of the three most influential words on the model's prediction.","Interestingly, three different interpretation methods (simple gradient visualization, Simonyan et al., 2014;integrated gradient visualization, Sundararajan et al., 2017;and SmoothGrad, Smilkov et al., 2017) all find the word ""great"" to be one of the two most influential words on the model's prediction.","Modify,Fact/Evidence",Fact/Evidence
337,120-ARR,120-ARR_v2_63@2,120-ARR_v1_62@1,"The ability to detect when is it reasonable to make an educated guess is an important property of an intelligent agent, and an exciting research question.","The ability to detect when is it reasonable and when it is not to make an educated guess is an important property of an intelligent agent, and an exciting research question.","Modify,Clarity",Clarity
338,120-ARR,120-ARR_v2_68@1,120-ARR_v1_64@1,A naive way to mitigate spurious correlations is to stop using large-scale datasets altogether.,A naive way to eliminate spurious correlations is to stop using large-scale datasets altogether.,"Modify,Clarity",Clarity
339,120-ARR,120-ARR_v2_68@2,120-ARR_v1_64@2,"We echo recent calls and argue that for supervised learning (i.e., large-scale fine-tuning), recent advances in zero-and few-shot learning might make this option possible.","We argue that for supervised learning (i.e., large-scale fine-tuning), recent advances in zero-and few-shot learning might make this option possible.","Modify,Claim",Claim
340,120-ARR,120-ARR_v2_69@0,120-ARR_v1_65@0,"Large pretrained models such as T5 or GPT-3 (Brown et al., 2020), trained on vast amounts of data, arguably learn enough about the world to acquire many of the skills currently learned through supervised learning.","Large pretrained models such as T5 or GPT-3 (Brown et al., 2020), trained on vast amounts of data, arguably learn enough about the world to acquire many of the skills currently learned through supervised datasets.","Modify,Fact/Evidence",Fact/Evidence
341,120-ARR,120-ARR_v2_69@1,120-ARR_v1_65@1,"Indeed, the large increase in the size and capacity of pretrained language models has led to a new wave of few-shot and zero-shot methods Shin et al., 2020;Gu et al., 2022), which are able to reach human-level performance on certain tasks using only a few dozens of training examples .","Indeed, the large increase in the size and capacity of pretrained language models has led to a new wave of few-shot and zero-shot methods Shin et al., 2020;Gu et al., 2021), which are able to reach human-level performance on certain tasks using only a few dozens of training examples .","Modify,Fact/Evidence",Fact/Evidence
342,120-ARR,120-ARR_v2_71@1,120-ARR_v1_67@1,"It seems plausible that excelling in language modeling tasks requires mastering the skills that stand in the base of many NLP tasks, such as sentiment analysis, syntactic parsing, and NER.","It seems possible the excelling in language modeling tasks requires mastering the skills that stand in the base of many NLP tasks, such as sentiment analysis, syntactic parsing, and NER.","Modify,Claim",Claim
343,120-ARR,120-ARR_v2_71@2,120-ARR_v1_67@2,"However, it is similarly plausible that this is not the case for other tasks, e.g., summarization, simplification and dialogue.","However, it is similarly possible that this is not the case for other tasks, e.g., summarization, simplification and dialogue.","Modify,Clarity",Clarity
344,120-ARR,120-ARR_v2_71@3,120-ARR_v1_67@3,"We are cautious in making concrete recommendations for which tasks to apply this principle, but suggest the following intuitive rule of thumb: for datasets or tasks for which the state of the art is close to or surpasses the human baseline, we should consider moving to few-shot setups.","We are cautious to make concrete recommendations for which tasks to apply this principle, but suggest the following intuitive rule of thumb: for datasets or tasks for which the state of the art is close to or surpasses the human baseline, we should consider moving to few-shot setups.","Modify,Grammar",Grammar
345,120-ARR,120-ARR_v2_72@2,120-ARR_v1_68@2,"We suggest that instead of building large training sets and small validation and test sets, authors should consider building large test sets, as a means for achieving improved statistical power (Card et al., 2020).","We suggest instead of building large training sets and small validation and tests, authors should consider building large test sets, as a means for achieving improved statistical power (Card et al., 2020).","Modify,Clarity",Clarity
346,120-ARR,120-ARR_v2_77@0,120-ARR_v1_73@0,This paper discusses the arms-race between models and datasets.,This paper discussed the arms-race between models and datasets.,"Modify,Grammar",Grammar
347,120-ARR,120-ARR_v2_78@0,120-ARR_v1_74@0,"Raji et al. ( 2021) recently criticized the concept of benchmarks as a whole, arguing that they are only capturing specific skills and not ""general"" capabilities.","Finally, Raji et al. ( 2021) recently criticized the concept of benchmarks as a whole, arguing that they are only capturing specific skills and not ""general"" capabilities.","Modify,Clarity",Clarity
348,120-ARR,120-ARR_v2_78@1,120-ARR_v1_74@1,"Our paper raises related concerns about training sets implicitly containing spurious correlations, and suggests reconsidering the practice of building large-scale training sets.","Our paper raises relates concerns about training sets implicitly containing spurious correlations, and suggests reconsidering the practice of building large-scale training sets.","Modify,Grammar",Grammar
350,120-ARR,120-ARR_v2_9@6,120-ARR_v1_9@5,"We argue that in such cases, the model should not fallback to default assumptions, but rather abstain or interact with the user to clear ambiguities.","We also suggest adopting modeling approaches that identify when the context is insufficient, and the model should not fallback to default assumptions, but rather output an ""I don't know"" response (e.g., unanswerable questions, Rajpurkar et al., 2018;Sulem et al., 2021) or interact with the user to clear ambiguities.","Split+Modify,Claim",Claim
351,120-ARR,120-ARR_v2_9@7,120-ARR_v1_9@6,"Finally, we question the basic procedure of large-scale fine-tuning, and suggest focusing on zero-and few-shot learning instead .","We conclude by questioning the basic procedure of large-scale fine-tuning, and suggest focusing on zero-and few-shot learning.","Modify,Clarity",Clarity
352,120-ARR,120-ARR_v2_13@1,120-ARR_v1_14@1,"One approach is to add examples in order to balance the dataset (Goyal et al., 2017;Sharma et al., 2018;Hudson and Manning, 2019).","One approach, popular in visual question answering datasets, is to add examples in order to balance the dataset (Goyal et al., 2017;Hudson and Manning, 2019).","Modify,Fact/Evidence",Fact/Evidence
353,120-ARR,120-ARR_v2_14@0,120-ARR_v1_15@0,Filtering as balancing A complementary balancing approach to augmentation is filtering examples out from datasets such that spurious correlations are minimized.,Filtering as balancing A complementary approach to augmentation is filtering examples out from datasets such that spurious correlations are minimized.,"Modify,Clarity",Clarity
354,120-ARR,120-ARR_v2_14@3,120-ARR_v1_15@3,"The AF approach and similar approaches were picked up by many datasets such as ReCoRD (Zhang et al., 2018), DROP (Dua et al., 2019), HellaSWAG (Zellers et al., 2019), αNLI , and WinoGrande .","The AF approach was picked up by many follow-up datasets such as DROP (Dua et al., 2019), HellaSWAG (Zellers et al., 2019), and Wino-Grande .","Modify,Fact/Evidence",Fact/Evidence
355,120-ARR,120-ARR_v2_15@1,120-ARR_v1_16@1,"As this approach relies on an external model, applying it with ever stronger models with higher capacity, will allow these models to pick up on subtler correlations (Li et al., 2021).","As this approach relies on an external model, applying it with ever stronger models with higher capacity, will allow these models to pick up on subtler correlations.","Modify,Fact/Evidence",Fact/Evidence
356,120-ARR,120-ARR_v2_17@0,120-ARR_v1_18@0,"Evidently, a similar trend emerges for the previously mentioned datasets: (1) the first baselines, reflecting the state of the art at the time of dataset creation, perform relatively poorly, e.g., 59% on SWAG, 47% on ReCoRD, 47 F1 on DROP, 47% on HellaSWAG, 69% on αNLI, and 79% on Wino-Grande; (2) model developers introduce increasingly larger and heavily-parameterized models, hill-climbing on these datasets; and eventually (3) models essentially solve the dataset within a year or two, often outperforming humans: 86% on SWAG (Devlin et al., 2019), 94% on ReCoRD (He et al., 2021b), 88 F1 on DROP (Chen et al., 2020), 93% on HellaSWAG (He et al., 2021b), 92% on αNLI (He et al., 2021a), and 90% on Wino-Grande .","Evidently, a similar trend emerges for the previously mentioned datasets: (1) the first baselines, reflecting the state of the art at the time of dataset creation, perform poorly, e.g., 52% accuracy on SWAG, 47 F1 on DROP, 47% on Hel-laSWAG, and 53% AUC on WinoGrande; (2) model developers introduce increasingly larger and heavily-parameterized models, hill-climbing on these datasets; and eventually (3) models essentially solve the dataset within a year or two, often outperforming humans: 86% on SWAG (Devlin et al., 2019), 90 F1 on DROP , 93% on HellaSWAG (He et al., 2020), and 88% AUC on WinoGrande .","Modify,Fact/Evidence",Fact/Evidence
561,129-ARR,,129-ARR_v1_55@1,,"For methods that do not directly use prediction for uncertainty estimation, we aggregate the uncertainty value as g(x; θ (t) ) = m t ×a(x; θ (t) )+(1−m t )×g(x; θ (t−1) ).","Delete,Fact/Evidence",Fact/Evidence
562,129-ARR,,129-ARR_v1_63@1,,Complexity Analysis.,"Delete,Other",Other
563,129-ARR,,129-ARR_v1_63@3,,"For self-training, the size of the memory bank g(x; θ) is proportional to |X u |, while the extra computation of maintaining this dictionary is ignorable since we do not inference over the unlabeled data for multiple times in each round as BALD (Gal et al., 2017) does.","Delete,Fact/Evidence",Fact/Evidence
564,129-ARR,,129-ARR_v1_79@6,,"Gao et al. (2020); Song et al. (2019); Guo et al. (2021) design query strategies for specific semi-supervised methods, Tomanek and Hahn (2009); Rottmann et al. (2018); Siméoni et al. (2020) exploit the mostcertain samples from the unlabeled with pseudolabeling to augment the training set.","Delete,Fact/Evidence",Fact/Evidence
565,129-ARR,,129-ARR_v1_79@12,,"To alleviate this, we adopt a simple momentumbased method to select high confidence samples, effectively reducing the pseudo labels noise for active learning.","Delete,Fact/Evidence",Fact/Evidence
566,129-ARR,,129-ARR_v1_79@13,,"Note that although Mukherjee and Awadallah (2020); Rizve et al. ( 2021) also leverage uncertainty information for self-training, their focus is on developing better self-training methods, while we aim to jointly query high-uncertainty samples and generate pseudo-labels for low-uncertainty samples.","Delete,Claim",Claim
567,129-ARR,,129-ARR_v1_79@14,,"The experiments in Sec. 3 show that with appropriate querying methods, ACTUNE can further improve the performance of self-training.","Delete,Fact/Evidence",Fact/Evidence
568,129-ARR,,129-ARR_v1_82@1,,"Specifically, we search T 1 from 10 to 2000, T 2 from 1000 to 5000, T 3 from 10 to 500, ξ from 0 to 1, and λ from 0 to 0.5.","Delete,Fact/Evidence",Fact/Evidence
569,129-ARR,,129-ARR_v1_82@2,,All results are reported as the average over three runs.,"Delete,Fact/Evidence",Fact/Evidence
570,129-ARR,,129-ARR_v1_83@0,,"In our experiments, we keep β = 0.5, λ = 1 for all datasets.","Delete,Fact/Evidence",Fact/Evidence
571,129-ARR,,129-ARR_v1_83@1,,"For other parameters, we use a grid search to find the optimal setting for each datasets.","Delete,Fact/Evidence",Fact/Evidence
572,129-ARR,,129-ARR_v1_83@2,,"Specifically, we search γ from [0.5, 0.6, 0.7], m L from [0.6, 0.7, 0.8], m H from [0.8, 0.9, 1].","Delete,Fact/Evidence",Fact/Evidence
573,129-ARR,,129-ARR_v1_83@3,,"For AC-TUNE with Entropy, we use probability based aggregation and for ACTUNE with CAL, we use value based aggregation by default.","Delete,Fact/Evidence",Fact/Evidence
574,129-ARR,,129-ARR_v1_84@0,,"For other SSAL methods, we mainly tune their key hyperparameters.","Delete,Fact/Evidence",Fact/Evidence
575,129-ARR,,129-ARR_v1_84@1,,"Note that Entropy (Holub et al., 2008), BALD (Gal et al., 2017), ALPS (Yuan et al., 2020), BADGE (Ash et al., 2020) do not introduce any new hyperparameters.","Delete,Fact/Evidence",Fact/Evidence
576,129-ARR,,129-ARR_v1_84@2,,"For CAL (Margatina et al., 2021b), we tune the number for KNN k from [5,10,20] and report the best performance.","Delete,Fact/Evidence",Fact/Evidence
577,129-ARR,,129-ARR_v1_84@3,,"For ST (Lee, 2013), CEAL (Wang et al., 2016) & BASS (Rottmann et al., 2018), it uses a threshold δ for selecting high-confidence data.","Delete,Fact/Evidence",Fact/Evidence
578,129-ARR,,129-ARR_v1_84@4,,"We tune δ from [0.6, 0.7, 0.8, 0.9] to report the best performance.","Delete,Fact/Evidence",Fact/Evidence
579,129-ARR,,129-ARR_v1_84@5,,"For UST (Mukherjee and Awadallah, 2020), we tune the number of lowuncertainty samples used in the next round from [1024,2048,4096].","Delete,Fact/Evidence",Fact/Evidence
580,129-ARR,,129-ARR_v1_84@6,,"For COSINE (Yu et al., 2021), we set the weight for confidence regularization λ as 0.1, the threshold τ for selecting high-confidence data from [0.7, 0.9] and the update period of selftraining from [50,100,150].","Delete,Fact/Evidence",Fact/Evidence
581,129-ARR,,129-ARR_v1_84@7,,"For REVIVAL (Guo et al., 2021), it calculates uncertainty with adversarial perturbation, we tune the size of the perturbation from [1e − 3, 1e − 4, 1e − 5].","Delete,Fact/Evidence",Fact/Evidence
582,129-ARR,,129-ARR_v1_85@0,,C Runtime Analysis.,"Delete,Other",Other
583,129-ARR,,129-ARR_v1_86@2,,"Among all baselines, we find that the running time of clustering-based method is faster than the original reported time in the paper.","Delete,Fact/Evidence",Fact/Evidence
584,129-ARR,,129-ARR_v1_86@3,,"This is because we use FAISS (Johnson et al., 2019) instead of SKLearn (Pedregosa et al., 2011) for clustering, which accelerates the clustering step significantly.","Delete,Fact/Evidence",Fact/Evidence
585,129-ARR,,129-ARR_v1_88@2,,Different colors stands for different classes.,"Delete,Fact/Evidence",Fact/Evidence
586,129-ARR,129-ARR_v2_2@6,,Our implementation is available at https://github.,,"Add,Fact/Evidence",Fact/Evidence
587,129-ARR,129-ARR_v2_70@0,,Active Learning Setups.,,"Add,Other",Other
588,129-ARR,129-ARR_v2_70@1,,"Following (Yuan et al., 2020), we set the number of rounds T = 10, the overall budget for all datasets b = 1000 and the initial size of the labeled |X l | is set to 100.",,"Add,Fact/Evidence",Fact/Evidence
589,129-ARR,129-ARR_v2_70@2,,"In each AL round, we sample a batch of 100 samples from the unlabeled set X u and query their labels.",,"Add,Fact/Evidence",Fact/Evidence
590,129-ARR,129-ARR_v2_70@4,,"For weakly-supervised text classification, since the datasets are much smaller, we keep the labeling budget and the size of development set to b = 500.",,"Add,Fact/Evidence",Fact/Evidence
591,129-ARR,129-ARR_v2_70@5,,Implementation Details.,,"Add,Other",Other
592,129-ARR,129-ARR_v2_70@6,,"We choose RoBERTabase (Liu et al., 2019) from the HuggingFace codebase (Wolf et al., 2020) as the backbone for AC-TUNE and all baselines except for Pubmed and Chemprot, where we use SciBERT (Beltagy et al., 2019), a BERT model pre-trained on scientific cor-pora.",,"Add,Fact/Evidence",Fact/Evidence
593,129-ARR,129-ARR_v2_82@0,,Case Study,,"Add,Other",Other
594,129-ARR,129-ARR_v2_85@4,,"Very recently, there are also several works attempted to query labeling functions for weakly-supervised learning (Boecking et al., 2020;Hsieh et al., 2022;Zhang et al., 2022b).",,"Add,Fact/Evidence",Fact/Evidence
595,129-ARR,129-ARR_v2_85@11,,"Self-training is one of the earliest and simplest approaches to semi-supervised learning (Lee, 2013).",,"Add,Fact/Evidence",Fact/Evidence
596,129-ARR,129-ARR_v2_88@0,,There are several directions to improve ACTUNE.,,"Add,Claim",Claim
597,129-ARR,129-ARR_v2_88@4,,"Moreover, we can explore more advanced uncertainty estimation approach (Kong et al., 2020) into ACTUNE to further improve the performance.",,"Add,Claim",Claim
598,129-ARR,129-ARR_v2_23@0,129-ARR_v1_23@0,Region-aware Sampling for Active Learning on High-uncertainty Data,Region-aware Sampling for Active,"Modify,Other",Other
599,129-ARR,129-ARR_v2_24@1,129-ARR_v1_24@1,"However, directly sampling the most uncertain samples gives suboptimal results as it tends to query repetitive data (Ein-Dor et al., 2020) that represent the overall data distribution poorly.","However, directly sampling the most uncertain samples gives suboptimal result since uncertainty-based sampling tends to query repetitive data (Ein-Dor et al., 2020) and results in poor representativeness of the overall data distribution.","Modify,Clarity",Clarity
600,129-ARR,129-ARR_v2_25@2,129-ARR_v1_25@2,Denote by K the number of clusters and v (t) i = BERT(x i ) the representation of x i from the penultimate layer of BERT.,Denote K the number of clusters and v (t) i = BERT(x i ) the representation of x i from the penultimate layer of BERT.,"Modify,Grammar",Grammar
601,129-ARR,129-ARR_v2_25@3,129-ARR_v1_25@3,"The weighted K-means process first initializes the center of each each cluster µ i (1 ≤ i ≤ K) via K-Means++ (Arthur and Vassilvitskii, 2007).","The weighted K-means first initializes the center of each each cluster µ i (1 ≤ i ≤ K) via K-Means++ (Arthur and Vassilvitskii, 2007).","Modify,Clarity",Clarity
602,129-ARR,129-ARR_v2_43@0,129-ARR_v1_41@0,"Then, we rank the clusters in an ascending order in u (t) k .","Then, we rank the clusters in an ascending order according to u (t) k .","Modify,Clarity",Clarity
603,129-ARR,129-ARR_v2_43@1,129-ARR_v1_41@1,"A high score indicates high uncertainty of the model in these regions, and we need to actively annotate the member instances to reduce uncertainty and improve the model's performance.","A high score indicates high uncertainty of the model in these regions, and we need to actively annotate the associated instances to reduce uncertainty and improve the model's performance.","Modify,Clarity",Clarity
605,129-ARR,129-ARR_v2_4@1,129-ARR_v1_4@0,"One benefit of PLM fine-tuning is the promising performance it offers when consuming only a few labeled data (Bansal et al., 2020;Gao et al., 2021).","Fine-tuning pre-trained language models (PLMs) has achieved enormous success in natural language processing (NLP) (Devlin et al., 2019;Liu et al., 2019;Brown et al., 2020), one of which is the competitive performance it offers when consuming only a few labeled data (Bansal et al., 2020;Gao et al., 2021).","Split+Modify,Clarity",Clarity
606,129-ARR,129-ARR_v2_53@0,129-ARR_v1_49@0,"Although the model is quite 'confident' on the class of x when we only consider the result of the round t, it gives contradictory predictions over these two consecutive rounds, which indicates that the model is actually uncertain to which class x belongs.","Although the model is quite 'confident' on the class of x when we only consider the result of the round t, it gives contradictory predictions over these two consecutive rounds, which indicates that the model is still uncertain to which class x belongs.","Modify,Clarity",Clarity
607,129-ARR,129-ARR_v2_4@2,129-ARR_v1_4@1,"However, there are still significant gaps between few-shot and fully-supervised PLM fine-tuning in many tasks.","However, there are still significant gaps between few-shot and fully-supervised PLM fine-tuning in many classification tasks.","Modify,Claim",Claim
608,129-ARR,129-ARR_v2_2@0,129-ARR_v1_2@0,"While pre-trained language model (PLM) finetuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data.","Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data.","Modify,Clarity",Clarity
609,129-ARR,129-ARR_v2_4@3,129-ARR_v1_4@2,"Besides, the performance of few-shot PLM finetuning can be sensitive to different sets of training data (Bragg et al., 2021).","Besides, the performance of few-shot PLM fine-tuning can vary substantially with different sets of training data (Bragg et al., 2021).","Modify,Clarity",Clarity
610,129-ARR,129-ARR_v2_4@4,129-ARR_v1_4@3,"Therefore, there is a crucial need for approaches that make PLM finetuning more label-efficient and robust to selection of training data, especially for applications where labeled data are scarce and expensive to obtain.","Therefore, there is a crucial need for PLM fine-tuning approaches with better label-efficiency and being robust to selection of training data, especially for applications where labeled data are scarce and expensive to obtain.","Modify,Clarity",Clarity
611,129-ARR,129-ARR_v2_74@0,129-ARR_v1_67@0,Weakly-supervised Learning,Extension to Weakly-supervised Learning,"Modify,Other",Other
612,129-ARR,129-ARR_v2_75@0,129-ARR_v1_68@0,"ACTUNE can be naturally used for weaklysupervised classification, where X l is a set of noisy labels derived from linguistic patterns or rules.","ACTUNE can be naturally extended to weaklysupervised classification, where X l is a set of data annotated by linguistic patterns or rules.","Modify,Fact/Evidence",Fact/Evidence
613,129-ARR,129-ARR_v2_75@1,129-ARR_v1_68@1,"Since the initial label set is noisy, the model trained with Eq. ( 1) can overfit the label noise (Zhang et al., 2022a), and we can actively query labeled data to refine the model.","Since the initial label set is noisy, then the model trained with Eq. ( 1) will overfit to the label noise, and we can actively query labeled data to refine the model.","Modify,Fact/Evidence",Fact/Evidence
614,129-ARR,129-ARR_v2_75@6,129-ARR_v1_70@2,We believe it is because CAL requires clean labels to calculate uncertainties.,We argue it is because CAL requires clean labels to calculate uncertainties.,"Modify,Clarity",Clarity
615,129-ARR,129-ARR_v2_5@0,129-ARR_v1_5@0,"Towards this goal, researchers have recently resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020;Margatina et al., 2021a,b;Yuan et al., 2020).","Towards this goal, researchers have resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020;Margatina et al., 2021a,b;Yuan et al., 2020).","Modify,Clarity",Clarity
616,129-ARR,129-ARR_v2_0@0,129-ARR_v1_0@0,ACTUNE: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models,ACTUNE: Uncertainty-Aware Active Self-Training for Active Fine-Tuning of Pretrained Language Models,"Modify,Other",Other
617,129-ARR,129-ARR_v2_85@1,129-ARR_v1_79@1,"Active learning has been widely applied to various NLP tasks (Yuan et al., 2020;Shelmanov et al., 2021;Karamcheti et al., 2021).","Active learning has been widely applied to various NLP tasks (Yuan et al., 2020;Zhao et al., 2020;Shelmanov et al., 2021;Karamcheti et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
618,129-ARR,129-ARR_v2_85@2,129-ARR_v1_79@2,"So far, AL methods can be categorized into uncertainty-based methods (Gal et al., 2017;Margatina et al., 2021a,b), diversity-based methods (Ru et al., 2020;Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020;Ash et al., 2020).","So far, AL methods can be categorized into uncertainty-based methods (Gal et al., 2017;Margatina et al., 2021a,b), diversitybased methods (Ru et al., 2020;Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020;Ash et al., 2020;Kirsch et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
619,129-ARR,129-ARR_v2_5@2,129-ARR_v1_5@2,"To incorporate unlabeled data into active learning, efforts have been made in the semi-supervised active learning literature (Wang et al., 2016;Rottmann et al., 2018;Siméoni et al., 2020).","To leverage those unlabeled data to improve label efficiency of active learning, efforts have been made in the semi-supervised active learning literature (Wang et al., 2016;Rottmann et al., 2018;Siméoni et al., 2020), but the proposed query strategies can return highly redundant samples due to limited representation power, resulting in suboptimal label efficiency.","Split+Modify,Clarity",Clarity
620,129-ARR,129-ARR_v2_5@3,129-ARR_v1_5@2,"However, the query strategies proposed in these works can return highly redundant samples due to limited representation power, resulting in suboptimal label efficiency.","To leverage those unlabeled data to improve label efficiency of active learning, efforts have been made in the semi-supervised active learning literature (Wang et al., 2016;Rottmann et al., 2018;Siméoni et al., 2020), but the proposed query strategies can return highly redundant samples due to limited representation power, resulting in suboptimal label efficiency.","Split+Modify,Clarity",Clarity
621,129-ARR,129-ARR_v2_85@12,129-ARR_v1_79@10,"It first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability.","Self-training first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability (Rosenberg et al., 2005;Lee, 2013).","Modify,Fact/Evidence",Fact/Evidence
622,129-ARR,129-ARR_v2_86@0,129-ARR_v1_80@0,Conclusion and Discussion,Conclusion,"Modify,Other",Other
623,129-ARR,129-ARR_v2_5@4,129-ARR_v1_5@3,"Moreover, they usually rely on pseudo-labeling to utilize unlabeled data, which requires greater (yet often absent) care to denoise the pseudo labels, otherwise the errors could accumulate and hurt model performance.","Moreover, they usually rely on pseudo-labeling to utilize unlabeled data, which requires greater (yet often absent) care to denoise the pseudo labels, otherwise the errors could accumulate and deteriorate the model performance.","Modify,Clarity",Clarity
624,129-ARR,129-ARR_v2_5@5,129-ARR_v1_5@4,"This issue can be even more severe for PLMs, as the fine-tuning process is often sensitive to different weight initialization and data orderings (Dodge et al., 2020).","This phenomenon can be even more severe for PLMs, as the finetuning process often suffers from the instability issue caused by different weight initialization and data orders (Dodge et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
625,129-ARR,129-ARR_v2_88@5,129-ARR_v1_87@4,"Last, apart from the text classification task, we can also extend our work into other tasks such as sequence labeling and natural language inference (NLI).","Last, apart from the text classification task, we can also extend our work into other tasks such as sequence labeling and natural language inference.","Modify,Clarity",Clarity
626,129-ARR,129-ARR_v2_2@1,129-ARR_v1_2@1,"Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data.","Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data.","Modify,Clarity",Clarity
627,129-ARR,129-ARR_v2_83@0,129-ARR_v1_88@0,We give an example of our querying strategy on AG News dataset for the 1st round of active selftraining process in figure 6.,Here we give an example of our querying strategy on AG News and Pubmed dataset for the 1st round of active self-training process in figure 6.,"Modify,Fact/Evidence",Fact/Evidence
628,129-ARR,129-ARR_v2_83@2,129-ARR_v1_88@3,"We can see that the existing uncertainty-based methods such as Entropy and CAL, are suffered from the issue of limited diversity.","From the comparision, we can see that the existing uncertainty based methods such as Entropy and CAL, are suffered from the issue of limited diversity.","Modify,Clarity",Clarity
629,129-ARR,129-ARR_v2_6@0,129-ARR_v1_6@0,"To tackle the above challenges, we propose AC-TUNE, a new method that improves the label efficiency and robustness of active PLM fine-tuning.","To tackle above challenges, we propose AC-TUNE, a new method that improves the label efficiency and robustness of active PLM fine-tuning with self-training.","Modify,Clarity",Clarity
630,129-ARR,129-ARR_v2_6@2,129-ARR_v1_7@1,"Different from existing AL methods that only leverage uncertainty for querying labels, our uncertainty-driven self-training paradigm gradually leverages unlabeled data with low uncertainty via self-training, while reducing the chance of error propagation triggered by highly-uncertain mislabeled data.","Different from existing AL methods that only leverage uncertainty for querying labels, our uncertaintydriven self-training paradigm gradually unleash the data with low uncertainty via self-training, while reducing the chance of error propagation triggered by highly-uncertain mis-labeled data.","Modify,Clarity",Clarity
631,129-ARR,129-ARR_v2_7@0,129-ARR_v1_8@0,"To further boost model performance for AC-TUNE, we design two techniques to improve the query strategy and suppress label noise, namely region-aware sampling (RS) and momentum-based memory bank (MMB).","To further boost the performance on downstream tasks, we design two techniques, namely regionaware sampling (RS) and momentum-based memory bank (MMB) to improve the query strategies and suppress label noise for ACTUNE.","Modify,Clarity",Clarity
632,129-ARR,129-ARR_v2_7@1,129-ARR_v1_8@1,"Inspired by the fact that existing uncertainty-based AL methods often end up with choosing uncertain yet repetitive data (Ein-Dor et al., 2020;Margatina et al., 2021b), we design the region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs.","Inspired by the fact that existing uncertainty-based AL methods often end up choosing uncertain yet repetitive data (Ein-Dor et al., 2020;Margatina et al., 2021b), we design a region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs.","Modify,Grammar",Grammar
633,129-ARR,129-ARR_v2_2@2,129-ARR_v1_2@2,"We propose ACTUNE, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning.","We develop ACTUNE, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self training.","Modify,Fact/Evidence",Fact/Evidence
634,129-ARR,129-ARR_v2_8@2,129-ARR_v1_9@2,"However, previous approaches only select pseudo-labeled data based on the prediction of the current round and are thus less reliable.","However, previous approaches only select pseudo-labeled data based on the prediction of the current round and therefore are less reliable.","Modify,Clarity",Clarity
635,129-ARR,129-ARR_v2_8@5,129-ARR_v1_9@5,"As a result, only the samples with high prediction confidence over multiple rounds will be used for self-training, which mitigates the issue of label noise.","As a consequence, only the samples with high prediction confidence over multiple rounds will be used for self-training, which mitigates the issue of label noise.","Modify,Clarity",Clarity
636,129-ARR,129-ARR_v2_8@6,129-ARR_v1_9@6,"We highlight that our active self-training approach is an efficient substitution to existing AL methods, requiring little extra computational cost.","We highlight that our active self-training approach is an efficient substitution to existing AL methods, requiring ignorable extra computational cost.","Modify,Clarity",Clarity
637,129-ARR,129-ARR_v2_9@0,129-ARR_v1_10@0,Our key contributions are: (1) an active selftraining paradigm ACTUNE that integrates selftraining and active learning to minimize the labeling cost for fine-tuning PLMs; (2) a region-aware querying strategy to enforce both the informativeness and the diversity of queried samples during AL; (3) a simple and effective momentum-based method to leverage the predictions in preceding rounds to alleviate the label noise in self-training and (4) experiments on 6 benchmarks demonstrating ACTUNE improves the label efficiency over existing self-training and active learning baselines by 56.2%.,Our key contributions are: (1) an active selftraining paradigm ACTUNE that integrates the benefit of self-training and active learning in a principled way to minimize the labeling cost for finetuning PLMs; (2) a region-aware querying strategy to enforce both the informativeness and the diversity of queried samples during AL; (3) a simple and effective momentum-based method to harness the predictions for preceding rounds to alleviate the label noise in self-training and (4) experiments on 6 benchmarks demonstrating ACTUNE improves the label efficiency over existing self-training and active learning baselines by 56.2%.,"Modify,Clarity",Clarity
638,129-ARR,129-ARR_v2_2@3,129-ARR_v1_2@3,ACTUNE switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and lowuncertainty ones for model self-training.,"AC-TUNE switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from lowuncertainty regions are used for model selftraining.","Modify,Clarity",Clarity
639,129-ARR,129-ARR_v2_14@1,129-ARR_v1_15@1,"Here X = X l ∪ X u denotes all samples, and Y = {1, 2, • • • , C} is the label set where C is the number of classes.","Here X = X l ∪ X u denotes all samples and Y = {1, 2, • • • , C} is the label set, where C is the number of classes.","Modify,Grammar",Grammar
640,129-ARR,129-ARR_v2_18@0,129-ARR_v1_19@0,"In round t (1 ≤ t ≤ T ) of active self-training, we first calculate the uncertainty score based on a given function a (t) i = a(x i , θ (t) ) 1 for all x i ∈ X u .","In round t (1 ≤ t ≤ T ) of the active self-training procedure, we first calculate the uncertainty score based on a given function a (t) i = a(x i , θ (t) ) 1 for all x i ∈ X u .","Modify,Clarity",Clarity
641,129-ARR,129-ARR_v2_2@4,129-ARR_v1_2@4,"Under this framework, we design (1) a regionaware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training.","Additionally, we design (1) a regionaware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model's pseudo labels to suppress label noise in self-training.","Modify,Clarity",Clarity
716,136-ARR,,136-ARR_v1_26@0,,The first step in our pipeline involves transforming each of the input triples X into a set of facts F in natural language by using a template t p i for each predicate p i .,"Delete,Fact/Evidence",Fact/Evidence
717,136-ARR,,136-ARR_v1_26@1,,We need at least one template for each predicate.,"Delete,Fact/Evidence",Fact/Evidence
718,136-ARR,,136-ARR_v1_26@2,,"Typically, the template will include placeholders which are filled with s i and o i .","Delete,Fact/Evidence",Fact/Evidence
719,136-ARR,,136-ARR_v1_27@1,,Note that the filled templates are allowed to contain minor disfluencies since the text will be rephrased in the final step of the pipeline.,"Delete,Fact/Evidence",Fact/Evidence
720,136-ARR,,136-ARR_v1_27@2,,See §5.5 for our approach to gathering the templates and Figure 2 for examples of the templates we use in our datasets.,"Delete,Fact/Evidence",Fact/Evidence
721,136-ARR,,136-ARR_v1_28@0,,"We acknowledge that this step may be a bottleneck on datasets with an unconstrained (or very large) set of predicates, which is why we also discuss possibilities for automating this step in §7.","Delete,Claim",Claim
722,136-ARR,,136-ARR_v1_30@1,,"To maximize the coherency of the resulting description, we apply an ordering model O to get an ordered sequence of facts: F o "" tf o 1 , . . . , f on u.","Delete,Fact/Evidence",Fact/Evidence
723,136-ARR,,136-ARR_v1_30@2,,"The coherence of the final text will also depend on the paragraph compression step, but grouping related facts together (e.g. facts mentioning birth date and birth place) helps the paragraph compression model to focus only on fusing and rephrasing the neighboring sentences.","Delete,Claim",Claim
724,136-ARR,,136-ARR_v1_36@0,,A key to our approach is building a large-scale synthetic corpus providing training data for the text operations in our pipeline.,"Delete,Fact/Evidence",Fact/Evidence
725,136-ARR,,136-ARR_v1_46@1,,"The paragraphs contain mostly concise, factbased descriptions from a wide range of domains.","Delete,Fact/Evidence",Fact/Evidence
726,136-ARR,,136-ARR_v1_50@2,,"Since we keep the referring expressions in the original human-written text, we can train the paragraph compression module to generate them in the final text description.","Delete,Fact/Evidence",Fact/Evidence
727,136-ARR,,136-ARR_v1_54@0,,We show how we build our pipeline ( §5.1-5.4) and discuss the D2T generation datasets which we use for our experiments ( §5.5).,"Delete,Fact/Evidence",Fact/Evidence
728,136-ARR,,136-ARR_v1_54@1,,The details of our training setup are included in Appendix B.,"Delete,Fact/Evidence",Fact/Evidence
729,136-ARR,,136-ARR_v1_67@0,,Ablation Study,"Delete,Other",Other
730,136-ARR,,136-ARR_v1_70@0,,D2T Datasets,"Delete,Other",Other
731,136-ARR,,136-ARR_v1_71@0,,"We test our approach on two English D2T datasets, WebNLG and E2E.","Delete,Fact/Evidence",Fact/Evidence
732,136-ARR,,136-ARR_v1_72@0,,"WebNLG The WebNLG dataset contains RDF triples from DBPedia (Auer et al., 2007) and their crowdsourced descriptions.","Delete,Fact/Evidence",Fact/Evidence
733,136-ARR,,136-ARR_v1_72@1,,"The dataset was extended for the WebNLG+ Challenge (Ferreira et al., 2020), but we use the version 1.4 for comparability to prior work.","Delete,Fact/Evidence",Fact/Evidence
734,136-ARR,,136-ARR_v1_72@2,,Templates for WebNLG could be extracted from the training data by delexicalizing single-triple examples.,"Delete,Fact/Evidence",Fact/Evidence
735,136-ARR,,136-ARR_v1_72@3,,"However, the examples are noisy and such data would not be available in a zero-shot setup.","Delete,Claim",Claim
736,136-ARR,,136-ARR_v1_72@4,,"Therefore, we handcrafted templates for all 354 predicates, including unseen predicates in the test set.","Delete,Fact/Evidence",Fact/Evidence
737,136-ARR,,136-ARR_v1_72@6,,"We use the cleaned version of the dataset (Dušek et al., 2019).","Delete,Fact/Evidence",Fact/Evidence
738,136-ARR,,136-ARR_v1_72@7,,"Following previous work, we transformed the attribute-value pairs into RDF triples (using the restaurant name as a subject) and then applied the same setup as for WebNLG.","Delete,Fact/Evidence",Fact/Evidence
739,136-ARR,,136-ARR_v1_72@8,,We created a template for each of the 8 attributes manually.,"Delete,Fact/Evidence",Fact/Evidence
740,136-ARR,,136-ARR_v1_78@2,,"For WebNLG (see Table 2), we include the results of UPF-FORGe and MELBOURNE systems from the first run of WebNLG Challenge which are comparable in terms of automatic metrics and semantic errors, and the results of Ke et al. (2021), which is a state-of-theart system using structure-aware encoder and taskspecific pretraining.","Delete,Fact/Evidence",Fact/Evidence
741,136-ARR,,136-ARR_v1_78@3,,Laha et al. ( 2020) is (to our knowledge) the only other zero-shot D2T generation system applied on WebNLG.,"Delete,Claim",Claim
742,136-ARR,,136-ARR_v1_78@4,,"TGEN (Dušek and Jurčíček, 2015) is the baseline system for the E2E Challenge and Harkous et al. (2020) is a state-of-the art supervised system applied on the cleaned E2E (see Table 3).","Delete,Fact/Evidence",Fact/Evidence
743,136-ARR,,136-ARR_v1_78@5,,"For both datasets, COPY is the baseline of copying the templates verbatim.","Delete,Fact/Evidence",Fact/Evidence
744,136-ARR,,136-ARR_v1_79@1,,"The COPY baseline is substantially better than the zero-shot system of Laha et al. (2020), suggesting that quality of the templates plays an important role.","Delete,Claim",Claim
745,136-ARR,,136-ARR_v1_80@0,,Manual Evaluation,"Delete,Other",Other
746,136-ARR,,136-ARR_v1_81@8,,"As we discuss in §7, more research is needed for ensuring the final consistency of the text.","Delete,Claim",Claim
747,136-ARR,,136-ARR_v1_85@1,,RANDOM is the baseline of generating a random order.,"Delete,Fact/Evidence",Fact/Evidence
748,136-ARR,,136-ARR_v1_85@2,,"The results show that although our approach lacks behind stateof-the-art supervised approaches, it can outperform both the random baseline and the Transformerbased approach from Ferreira et al. (2019) while not using any training examples from WebNLG.","Delete,Claim",Claim
749,136-ARR,,136-ARR_v1_91@0,,"In the current form, our pipeline can be directly applied to generating text from RDF triples (or similarly structured data) which require no extra processing.","Delete,Claim",Claim
750,136-ARR,,136-ARR_v1_91@1,,"Further extensions are needed for more complex D2T scenarios, e.g. datasets requiring content selection or common-sense and logical reasoning (Wiseman et al., 2017;Chen et al., 2020b).","Delete,Claim",Claim
751,136-ARR,,136-ARR_v1_92@0,,Our approach regarding handcrafting a single template for each predicate is quite basic.,"Delete,Claim",Claim
752,136-ARR,,136-ARR_v1_92@2,,"Moreover, explicitly including a denoising task for the paragraph compression model could help to tackle the disfluencies in the templates.","Delete,Claim",Claim
753,136-ARR,,136-ARR_v1_94@0,,Statistics for the datasets described in the paper are listed in Table 7.,"Delete,Fact/Evidence",Fact/Evidence
754,136-ARR,136-ARR_v2_7@6,,"Moreover, the semantic accuracy metrics and our manual error analysis suggest that our approach offers a way to prevent omissions and hallucinations common in few-shot approaches.",,"Add,Claim",Claim
755,136-ARR,136-ARR_v2_11@0,,"Pipeline-based D2T Generation Until the recent surge of end-to-end approaches , using several modules connected in a pipeline was a major approach for D2T generation (Gatt and Krahmer, 2018;Reiter, 2007;Reiter and Dale, 1997).",,"Add,Fact/Evidence",Fact/Evidence
756,136-ARR,136-ARR_v2_11@1,,"Our approach is inspired by the pipeline approaches, in particular the pipelines utilizing neural modules (Ferreira et al., 2019).",,"Add,Fact/Evidence",Fact/Evidence
757,136-ARR,136-ARR_v2_11@2,,"In contrast with these approaches, our pipeline works with unstructured data in natural language and it operates in zero-shot setting, i.e. without using any training data from target D2T datasets.",,"Add,Fact/Evidence",Fact/Evidence
758,136-ARR,136-ARR_v2_11@3,,Laha et al. (2019) introduce a three-step pipeline for zero-shot D2T generation similar to ours.,,"Add,Fact/Evidence",Fact/Evidence
759,136-ARR,136-ARR_v2_17@0,,"In this section, we provide the formal description of our proposed approach.",,"Add,Fact/Evidence",Fact/Evidence
760,136-ARR,,136-ARR_v1_11@1,,"They also build a three-step pipeline for zero-shot D2T generation, but they use handcrafted rules for producing the output text and do not address content planning.","Delete,Fact/Evidence",Fact/Evidence
761,136-ARR,136-ARR_v2_18@0,,Our pipeline proceeds as follows.,,"Add,Fact/Evidence",Fact/Evidence
762,136-ARR,136-ARR_v2_19@0,,"(1) transform the triples into facts, which are sentences in natural language, (2) sort the facts using an ordering module, (3) insert sentence delimiters between the sorted facts using an aggregation module, (4) input the ordered sequence of facts with delimiters into a paragraph compression module, which generates the final description Y .",,"Add,Fact/Evidence",Fact/Evidence
763,136-ARR,136-ARR_v2_20@0,,Transforming Triples to Facts,,"Add,Other",Other
764,136-ARR,136-ARR_v2_21@3,,This step can be realized e.g. using a simple template for each predicate (cf. §5.1).,,"Add,Fact/Evidence",Fact/Evidence
765,136-ARR,136-ARR_v2_23@1,,"Note, however, that that F is a indeed set of meaningful sentences.",,"Add,Fact/Evidence",Fact/Evidence
766,136-ARR,136-ARR_v2_23@2,,We can use this to our advantage and apply a sentence ordering model to maximize the coherency of the paragraph resulting from their concatenation.,,"Add,Claim",Claim
767,136-ARR,136-ARR_v2_23@3,,"An example outcome of such operation may be grouping together facts mentioning birth date and birth place of a person, followed by their occupation (see Figure 1).",,"Add,Claim",Claim
768,136-ARR,136-ARR_v2_23@4,,The ordering module allows downstream modules to only focus on operations over neighboring sentences.,,"Add,Fact/Evidence",Fact/Evidence
769,136-ARR,136-ARR_v2_24@0,,"Formally, we apply the ordering model OpF q to get an ordered sequence of facts: F o "" tf o 1 , . . . , f on u, where o 1:n is a permutation of indices.",,"Add,Fact/Evidence",Fact/Evidence
770,136-ARR,136-ARR_v2_26@0,,Some facts will be typically mentioned together in a single sentence.,,"Add,Claim",Claim
771,136-ARR,136-ARR_v2_26@1,,"Considering the previous example, occupation is likely to be mentioned separately, while birth date and birth place are likely to be mentioned together.",,"Add,Claim",Claim
772,136-ARR,136-ARR_v2_26@2,,"Using an ordered sequence of facts as input, we can apply an aggregation model to decide which facts should be merged into a single sentence.",,"Add,Claim",Claim
773,136-ARR,136-ARR_v2_27@2,,We describe our aggregation model in §5.3.,,"Add,Fact/Evidence",Fact/Evidence
774,136-ARR,136-ARR_v2_29@0,,The paragraph compression (PC) model is a generative model which outputs the final text description.,,"Add,Fact/Evidence",Fact/Evidence
775,136-ARR,136-ARR_v2_32@0,,Here we descibe the process of building a largescale synthetic corpus WIKIFLUENT.,,"Add,Fact/Evidence",Fact/Evidence
776,136-ARR,136-ARR_v2_32@1,,"The corpus provides training data for the neural models which we use in our implementation of the ordering, aggregation, and paragraph compression modules (cf. §5).",,"Add,Fact/Evidence",Fact/Evidence
777,136-ARR,136-ARR_v2_35@1,,"Wikipedia is commonly used for large-scale pretraining of D2T generation models (Jin et al., 2020;Chen et al., 2020a).",,"Add,Fact/Evidence",Fact/Evidence
778,136-ARR,136-ARR_v2_35@2,,"Although it is not biasfree, it provides more balanced sample of natural language use than typical D2T generation datasets.",,"Add,Claim",Claim
779,136-ARR,136-ARR_v2_35@3,,"We used the first paragraphs of Wikipedia entries, which contain mostly concise, fact-based descriptions.",,"Add,Claim",Claim
780,136-ARR,136-ARR_v2_42@2,,We apply a split-and-rephrase model on each sentence in the paragraph and resolve coreferences in the split sentences.,,"Add,Fact/Evidence",Fact/Evidence
781,136-ARR,,136-ARR_v1_12@2,,"Recently, have shown that using a content plan leads to improved quality of PLM outputs.","Delete,Fact/Evidence",Fact/Evidence
782,136-ARR,136-ARR_v2_42@3,,The result is a set of simple sentences which together convey the same meaning as the original paragraph.,,"Add,Fact/Evidence",Fact/Evidence
783,136-ARR,136-ARR_v2_42@4,,"The synthesized sentences are used as input into our models, the original human-written texts are used as ground truth.",,"Add,Fact/Evidence",Fact/Evidence
784,136-ARR,136-ARR_v2_44@2,,The process is illustrated in the upper part of Figure 2.,,"Add,Fact/Evidence",Fact/Evidence
785,136-ARR,136-ARR_v2_47@2,,Note that this procedure replaces the referring expressions only in the synthesized sentences (which are used as input) and keeps them in the original paragraphs (which are used as ground truth).,,"Add,Fact/Evidence",Fact/Evidence
786,136-ARR,136-ARR_v2_47@3,,"As a consequence, the paragraph compression module is implicitly trained to generate referring expressions in the final description.",,"Add,Fact/Evidence",Fact/Evidence
787,136-ARR,136-ARR_v2_50@0,,Implementation,,"Add,Other",Other
788,136-ARR,136-ARR_v2_51@0,,"In this section, we describe how we implement our pipeline modules ( §3) using simple template transformations ( §5.1) and neural models trained on the WIKIFLUENT dataset ( §5.2-5.4).",,"Add,Fact/Evidence",Fact/Evidence
789,136-ARR,136-ARR_v2_53@0,,We transform triples into facts ( §3.1) using a singletriple template t i for each predicate.,,"Add,Fact/Evidence",Fact/Evidence
790,136-ARR,136-ARR_v2_53@2,,"We follow previous work in which simple hand-crafted templates have been used as an efficient way of introducing domain knowledge (Kale and Rastogi, 2020a;Kasner and Dušek, 2020a) template generation engines (Laha et al., 2019;Heidari et al., 2021;Mehta et al., 2021), the approach may produce less fluent outputs, but it minimizes manual workload and makes it easier to control the quality of the input for the subsequent steps.",,"Add,Claim",Claim
791,136-ARR,136-ARR_v2_70@0,,Here we expect that the model will learn to fuse the sentences between which there are no delimiters on the input.,,"Add,Claim",Claim
792,136-ARR,136-ARR_v2_72@0,,We train our pipeline modules on the WIKIFLU-ENT corpus as described in §5.,,"Add,Fact/Evidence",Fact/Evidence
793,136-ARR,136-ARR_v2_72@1,,"Next, we use these modules without finetuning for generating descriptions for RDF triples on two English D2T datasets, WebNLG and E2E.",,"Add,Fact/Evidence",Fact/Evidence
794,136-ARR,136-ARR_v2_79@0,,Our main aim is the evaluation of our pipeline on the downstream task of D2T generation.,,"Add,Claim",Claim
795,136-ARR,136-ARR_v2_79@3,,"In §7.4, we include an intrinsic evaluation of our modules on the WIKIFLUENT test set.",,"Add,Fact/Evidence",Fact/Evidence
796,136-ARR,136-ARR_v2_81@3,,"For WebNLG (see Table 3), we compare our systems with the results of:",,"Add,Fact/Evidence",Fact/Evidence
797,136-ARR,136-ARR_v2_83@2,,"Nevertheless, our systems still underperform the state-of-the-art supervised systems.",,"Add,Fact/Evidence",Fact/Evidence
798,136-ARR,136-ARR_v2_83@3,,"For this reason, we further focus on manual error analysis in §7.2 to pinpoint the current shortcomings of our approach.",,"Add,Fact/Evidence",Fact/Evidence
799,136-ARR,136-ARR_v2_85@0,,Manual Error Analysis,,"Add,Other",Other
800,136-ARR,136-ARR_v2_86@0,,"Since automatic performance metrics do not provide insights into specific weaknesses of the system (van Miltenburg et al., 2021), we manually examined 100 outputs of the models.",,"Add,Fact/Evidence",Fact/Evidence
801,136-ARR,136-ARR_v2_88@8,,This behavior is the main obstacle to ensure factual consistency of the output.,,"Add,Claim",Claim
802,136-ARR,136-ARR_v2_88@9,,"As a possible remedy, we propose explicitly controlling the semantics of sentence fusion (Ben-David et al., 2020), e.g. using a variant of constrained decoding (Balakrishnan et al., 2019;.",,"Add,Claim",Claim
803,136-ARR,136-ARR_v2_93@0,,Intrinsic Evaluation,,"Add,Other",Other
804,136-ARR,136-ARR_v2_94@0,,"Aside from the main D2T generation results, we also provide an intrinsic evaluation of our pipeline modules on the WIKIFLUENT test sets.",,"Add,Fact/Evidence",Fact/Evidence
805,136-ARR,136-ARR_v2_94@1,,"We evaluated the ordering, aggregation, and paragraph compression modules trained on the full WIKIFLUENT corpus.",,"Add,Fact/Evidence",Fact/Evidence
806,136-ARR,136-ARR_v2_94@2,,The results for both full and filtered test sets are summarized in Table 7.,,"Add,Fact/Evidence",Fact/Evidence
807,136-ARR,136-ARR_v2_94@3,,"The PC model achieves high scores, which follows from the fact that we provide it with ground truth content plans (i.e., the ordering and aggregation plan corresponding to the original paragraph).",,"Add,Fact/Evidence",Fact/Evidence
808,136-ARR,136-ARR_v2_94@4,,Accuracy of the ordering and aggregation modules is comparable to their performance on D2T datasets.,,"Add,Fact/Evidence",Fact/Evidence
809,136-ARR,136-ARR_v2_96@0,,Our experiments outline several possible future research directions.,,"Add,Claim",Claim
810,136-ARR,136-ARR_v2_96@2,,"The task of paragraph compression could be used as a task-specific pretraining (Gururangan et al., 2020) for more efficient finetuning of D2T models, e.g., with a small amount of clean data.",,"Add,Claim",Claim
811,136-ARR,136-ARR_v2_96@3,,"Consistency checks may be introduced in the pipeline to control the output from the modules, and individual modules may be improved by using more efficient model architectures.",,"Add,Claim",Claim
812,136-ARR,136-ARR_v2_98@0,,Conclusion,,"Add,Other",Other
813,136-ARR,136-ARR_v2_99@0,,We presented an approach for zero-shot D2T generation.,,"Add,Fact/Evidence",Fact/Evidence
814,136-ARR,136-ARR_v2_99@1,,The approach uses a pipeline of PLMs trained on general-domain lexical operations over natural language.,,"Add,Fact/Evidence",Fact/Evidence
815,136-ARR,136-ARR_v2_99@2,,The pipeline builds upon traditional approaches and consists of three interpretable intermediate steps.,,"Add,Fact/Evidence",Fact/Evidence
816,136-ARR,136-ARR_v2_99@3,,"By avoiding noisy human-written references from the D2T datasets, our models produce more semantically consitent output.",,"Add,Fact/Evidence",Fact/Evidence
817,136-ARR,136-ARR_v2_99@4,,We believe that training models for zeroshot D2T generation using large cross-domain corpora will help to build D2T generation systems with good performance across various domains.,,"Add,Claim",Claim
818,136-ARR,136-ARR_v2_100@0,,Limitations and Broader Impact,,"Add,Other",Other
819,136-ARR,136-ARR_v2_101@0,,We study zero-shot D2T generation with the focus on generating descriptions for RDF triples.,,"Add,Fact/Evidence",Fact/Evidence
820,136-ARR,136-ARR_v2_101@1,,"Although the task of D2T generation has numerous applications, using neural models for D2T generation (especially in the zero-shot context) is still limited to experimental settings (Dale, 2020).",,"Add,Claim",Claim
821,136-ARR,136-ARR_v2_101@2,,"Similarly to other recent approaches for D2T generation, our approach relies on PLMs, which are known to reflect the biases in their pretraining corpus (Bender et al., 2021;Rogers, 2021).",,"Add,Fact/Evidence",Fact/Evidence
822,136-ARR,136-ARR_v2_101@3,,Our system may therefore rely on spurious correlations for verbalizing e.g. gender or occupation of the entities.,,"Add,Claim",Claim
823,136-ARR,136-ARR_v2_101@4,,"Since we cannot guarantee the factual correctness of the outputs of our system, the outputs should be used with caution.",,"Add,Claim",Claim
824,136-ARR,136-ARR_v2_102@0,,"On the flip side, our approach helps to reduce the number of omissions and hallucinations stemming from noise in human-written references.",,"Add,Claim",Claim
825,136-ARR,136-ARR_v2_102@1,,Our work thus contributes to the general aim of D2T generation in conveying the data semantics accurately and without relying on implicit world knowledge.,,"Add,Claim",Claim
826,136-ARR,136-ARR_v2_104@2,,Training times were approximately 24 hours for the ordering model and 3 hours for the aggregation and paragraph compression models.,,"Add,Fact/Evidence",Fact/Evidence
827,136-ARR,,136-ARR_v1_16@0,,We first give an overview of our neural D2T generation pipeline ( §3.1).,"Delete,Fact/Evidence",Fact/Evidence
828,136-ARR,,136-ARR_v1_17@0,,Method Overview,"Delete,Other",Other
829,136-ARR,136-ARR_v2_21@0,136-ARR_v1_19@0,The first step in our pipeline involves transforming each of the input triples x i P X into a fact f i P F using a transformation T : X Ñ F .,We assume that we can transform each triple x i to a fact f i (where f i is a sentence in natural language describing x i ) by filling the single-triple template t p i P T for the predicate p i :,"Split+Modify,Fact/Evidence",Fact/Evidence
830,136-ARR,136-ARR_v2_21@1,136-ARR_v1_19@0,We define a fact f i as a single sentence in natural language describing x i .,We assume that we can transform each triple x i to a fact f i (where f i is a sentence in natural language describing x i ) by filling the single-triple template t p i P T for the predicate p i :,"Split+Modify,Clarity",Clarity
831,136-ARR,136-ARR_v2_18@1,136-ARR_v1_21@0,"Given a set of triples X on the input, we:","We proceed as follows -given an input X, we:","Modify,Clarity",Clarity
832,136-ARR,136-ARR_v2_22@0,136-ARR_v1_29@0,Ordering the Facts,Ordering,"Modify,Other",Other
833,136-ARR,136-ARR_v2_23@0,136-ARR_v1_30@0,We assume that the default order of triples X is random and the same applies for the respective facts F .,We assume that the default order of triples X (and the respective facts F ) is random.,"Modify,Clarity",Clarity
834,136-ARR,136-ARR_v2_25@0,136-ARR_v1_31@0,Aggregating the Facts,Aggregation,"Modify,Other",Other
835,136-ARR,136-ARR_v2_27@1,136-ARR_v1_32@1,"Conversely, δ i "" 0 means that the facts should be aggregated and their corresponding sentences should be fused.","Conversely, δ i "" 0 means that the facts should be aggregated and their corresponding sentences should be fused (see §5.2 and §5.3).","Modify,Fact/Evidence",Fact/Evidence
836,136-ARR,136-ARR_v2_29@1,136-ARR_v1_34@1,"It has two main objectives: (a) fusing related sentences, i.e., sentences i and j in between which δ i "" 0, and (b) rephrasing the text to improve its fluency, e.g. fixing disfluencies in the templates, replacing noun phrases with refering expressions, etc.","The objectives of the model are two-fold: (a) fusing related sentences, i.e., sentences i and j in between which δ i "" 0, and (b) rephrasing the text to improve its fluency, e.g. fixing minor disfluencies in the templates, replacing noun phrases with refering expressions, etc.","Modify,Clarity",Clarity
837,136-ARR,136-ARR_v2_29@2,136-ARR_v1_34@2,"The goal of the task is to preserve the semantics of the text which is an already ordered sequence of sentences, so the edits will typically be minor.",The focus is on minor rephrasing since the goal is to preserve the semantics of the original text.,"Modify,Fact/Evidence",Fact/Evidence
838,136-ARR,136-ARR_v2_33@0,136-ARR_v1_36@1,Our goal is to cover a broad range of domains while capturing the sentence style in D2T generation with respect to both the input facts and the target descriptions.,"Our corpus needs to cover a broad range of domains while capturing the sentence style in D2T generation, both regarding the input templates and the target descriptions.","Modify,Clarity",Clarity
839,136-ARR,136-ARR_v2_42@6,136-ARR_v1_46@3,"To balance the length of inputs, we selected 250k examples each from 4 equally sized length ranges (30-130 characters, etc.).","To further ensure that the length of inputs is balanced, we selected 250k examples each from 4 equidistant length ranges (30-130 characters, etc.).","Modify,Clarity",Clarity
840,136-ARR,136-ARR_v2_44@0,136-ARR_v1_48@0,"To generate a set of simple sentences, we divide each paragraph into sentences using NLTK (Bird, 2006) and apply a split-and-rephrase model on each sentence.","For generating the target set of sentences, we divide each paragraph into sentences using NLTK (Bird, 2006) and apply a split-and-rephrase model on each sentence.","Modify,Clarity",Clarity
841,136-ARR,136-ARR_v2_45@0,136-ARR_v1_48@2,"We train our split-and-rephrase model on the large-scale WikiSplit corpus by Botha et al. (2018), containing human-made sentence splits from Wikipedia edit history.","We train our model on the large-scale WikiSplit corpus by Botha et al. (2018), containing human-made sentence splits from Wikipedia edit history.","Modify,Clarity",Clarity
842,136-ARR,136-ARR_v2_45@1,136-ARR_v1_48@3,"Following the same setup as for a paragraph compression model ( §3.4), we train BART-base (Lewis et al., 2020) on the WikiSplit dataset in a sequence-to-sequence setting.","Following the setup in the rest of our experiments, we train the encoder-decoder PLM BART-base (Lewis et al., 2020) on the WikiSplit dataset in a sequence-to-sequence setting.","Modify,Fact/Evidence",Fact/Evidence
843,136-ARR,136-ARR_v2_45@2,136-ARR_v1_48@4,"Next, we apply the trained split-and-rephrase model on each sentence in our Wikipedia-based corpus, uniformly randomly choosing between 0-2 recursive calls to ensure that the splits are not deterministic.","We apply the trained split-and-rephrase model on each sentence, uniformly randomly choosing between 0-2 recursive calls to ensure that the splits are not deterministic.","Modify,Fact/Evidence",Fact/Evidence
844,136-ARR,136-ARR_v2_47@0,136-ARR_v1_50@0,"As the next step, we concatenate the split sentences and apply a coreference resolution model (Gardner et al., 2018;Lee et al., 2018) in order to replace referring expressions with their antencendents (e.g., pronouns with noun phrases).","Next, we concatenate the split sentences and apply a coreference resolution model (Gardner et al., 2018) in order to replace referring expressions with their antencendents (e.g., pronouns with noun phrases).","Modify,Fact/Evidence",Fact/Evidence
845,136-ARR,136-ARR_v2_47@1,136-ARR_v1_50@1,"The motivation for this step is to match the style of the facts (see §3.1), which do not use pronouns since each fact describes a single triple only.",This allows to better follow the style of the templates in which the entities are always fully verbalized.,"Modify,Fact/Evidence",Fact/Evidence
846,136-ARR,136-ARR_v2_49@2,136-ARR_v1_52@2,"In a filtered version of the WIKIFLUENT corpus, we include only the examples without omissions or hallucinations (as computed by the model), reducing it to 714k examples (approximately 75% of the original size).","In a filtered version of the WIK-IFLUENT corpus, we include only the examples without omissions or hallucinations (as computed by the model), reducing it to approximately 3/4 of the original size.","Modify,Fact/Evidence",Fact/Evidence
847,136-ARR,136-ARR_v2_56@1,136-ARR_v1_56@2,We provide a short description of the model here; for details please refer to Calizzano et al. (2021).,We provide a short description of the model here; for details see Calizzano et al. (2021).,"Modify,Clarity",Clarity
848,136-ARR,136-ARR_v2_59@0,136-ARR_v1_59@0,"The pointer network computes the probability of a fact to be on the j-th position, using the encoder output E and the decoder output state d j .","The pointer network computes the probability of a fact to be on the j-th position, using the encoder output E and the decoder output d j .","Modify,Clarity",Clarity
849,136-ARR,136-ARR_v2_64@0,136-ARR_v1_61@1,"We train the model using the synthesized simple sentences in the WIKIFLUENT corpus, randomly shuffling the order of the sentences and training the model to restore their original order.","We train the model using the split sentences in the WIKIFLUENT corpus, randomly shuffling the order of the sentences and training the model to restore their original order.","Modify,Clarity",Clarity
850,136-ARR,136-ARR_v2_66@3,136-ARR_v1_63@3,We ignore the outputs for the nonseparator tokens while computing cross-entropy loss.,We ignore the outputs for the non-separator tokens while computing the cross-entropy loss.,"Modify,Grammar",Grammar
851,136-ARR,136-ARR_v2_67@0,136-ARR_v1_64@0,"We create the training examples using the synthesized sentences in the WIKIFLUENT corpus, in which we set δ i "" 0 for the sentences i, i`1 which were originally aggregated (i.e., are the result of splitting a single sentence) and δ i "" 1 otherwise.","We create the training examples using the split sentences in the WIKIFLUENT corpus, in which we set δ i "" 0 for the sentences i, i `1 which were originally aggregated (i.e., are the result of splitting a single sentence) and δ i "" 1 otherwise.","Modify,Clarity",Clarity
852,136-ARR,136-ARR_v2_69@1,136-ARR_v1_66@1,"We finetune the model on the WIK-IFLUENT corpus, concatenating the synthesized sentences on the input.","We train the model in a sequenceto-sequence setting on the WIKIFLUENT corpus, concatenating the split sentences on the input.","Modify,Clarity",Clarity
853,136-ARR,136-ARR_v2_69@2,136-ARR_v1_66@2,"We add delimiters between the sentences i and i `1 where δ i "" 1 using a special token <sep>, which we add to the model vocabulary.","We add delimiters between sentences i and i `1 where δ i "" 1 using a special token <sep>, which we add to the model vocabulary.","Modify,Grammar",Grammar
854,136-ARR,136-ARR_v2_70@1,136-ARR_v1_66@4,We evaluate how the model learns to respect the order and aggregation markers in §7.3.,We evaluate our model's behavior with respect to ordering and aggregation in §6.3.,"Modify,Clarity",Clarity
855,136-ARR,136-ARR_v2_6@1,136-ARR_v1_6@1,Gathering a large set of references for a particular domain is also costly and time-consuming as it usually requires collecting human-written references through crowdsourcing .,"Moreover, collecting a large set of references for a particular domain is costly and time-consuming, as the data are usually collected using crowdsourcing .","Modify,Clarity",Clarity
856,136-ARR,136-ARR_v2_73@0,136-ARR_v1_71@1,"Datasets The datasets differ in domain, size, textual style, and number of predicates (see Appendix A for details):","They differ in domain, size, textual style, and number of predicates (see Appendix A for details).","Modify,Clarity",Clarity
857,136-ARR,136-ARR_v2_78@0,136-ARR_v1_73@0,Evaluation and Discussion,Evaluation,"Modify,Other",Other
858,136-ARR,136-ARR_v2_79@1,136-ARR_v1_74@0,"We evaluate outputs from the {1,2,3}-STAGE variants of our pipeline using automatic metrics ( §7.1), and we perform a detailed manual error analysis of the model outputs ( §7.2).","We evaluate outputs from the {1,2,3}-STAGE variants of our pipeline automatically ( §6.1) and manually ( §6.2).","Modify,Fact/Evidence",Fact/Evidence
859,136-ARR,136-ARR_v2_79@2,136-ARR_v1_74@1,We also evaluate the performance of the content planning modules and the ability of the PC module to follow the content plan ( §7.3).,"Further, we evaluate the performance of the content planning modules and the ability of the PC module to follow the content plan ( §6.3).","Modify,Clarity",Clarity
860,136-ARR,136-ARR_v2_6@2,136-ARR_v1_6@2,"These problems can be partially mitigated using few-shot approaches (Chen et al., 2020b;Ke et al., 2021;Su et al., 2021a), which operate with only several dozens or hundreds of annotated examples, but the robustness of these approaches is questionable-selecting a representative set of examples which would improve performance is difficult (Chang et al., 2021a), and the limited sample is often noisy, increasing the chance of hallucinations and omissions (Dušek et al., 2019;Harkous et al., 2020;Rebuffel et al., 2022).","Few-shot approaches are an alternative, requring only several tens or hundreds of annotated examples (Chen et al., 2020c;Ke et al., 2021;Su et al., 2021a).","Merge+Modify,Fact/Evidence",Fact/Evidence
861,136-ARR,136-ARR_v2_83@0,136-ARR_v1_79@0,"The automatic evaluation shows that our systems consistently outperform the COPY baseline (e.g., ""12 BLEU points for E2E), which is already strong thanks to our manually curated set of templates.","The automatic evaluation suggests that while our system lags behind state-of-the-art supervised systems, it shows considerable improvements compared to the COPY baseline (e.g., ""12 BLEU points pendix C for the details. for E2E) and matches performance of some older supervised systems.","Modify,Claim",Claim
862,136-ARR,136-ARR_v2_6@2,136-ARR_v1_6@3,"These problems can be partially mitigated using few-shot approaches (Chen et al., 2020b;Ke et al., 2021;Su et al., 2021a), which operate with only several dozens or hundreds of annotated examples, but the robustness of these approaches is questionable-selecting a representative set of examples which would improve performance is difficult (Chang et al., 2021a), and the limited sample is often noisy, increasing the chance of hallucinations and omissions (Dušek et al., 2019;Harkous et al., 2020;Rebuffel et al., 2022).","However, robustness of these approaches is questionable-selecting a representative set of examples which would improve performance is difficult (Chang et al., 2021a), and the limited sample is often noisy, increasing the chance of hallucinations and omissions (Dušek et al., 2019;Harkous et al., 2020;Rebuffel et al., 2021).","Merge+Modify,Fact/Evidence",Fact/Evidence
863,136-ARR,136-ARR_v2_84@0,136-ARR_v1_79@2,"The 2-STAGE system is generally on par with the 3-STAGE system or better, which indicates that explicit aggregation using the AGG model may not be necessary.","The 2-STAGE system is generally on par with the 3-STAGE system (or better), which indicates that implicit aggregation using the PC-AGG model may be sufficient.","Modify,Claim",Claim
864,136-ARR,136-ARR_v2_84@2,136-ARR_v1_79@4,"The models using the filtered version of the corpus generally produce better results, although they also bring in a larger number of omissions.","The filtered version of the dataset generally brings better results, although it brings also an increase in the number of omissions.","Modify,Clarity",Clarity
865,136-ARR,136-ARR_v2_86@1,136-ARR_v1_81@0,"We counted the number of errors: factual (hallucinations, omissions, incorrect fact merging, redundancies) and grammatical.","We manually evaluated 100 outputs of the models regarding factual errors (hallucinations, omissions, incorrect fact merging, redundancies) as well as grammatical errors.","Modify,Fact/Evidence",Fact/Evidence
866,136-ARR,136-ARR_v2_87@1,136-ARR_v1_81@3,"These problems are largely eliminated with the 2-STAGE and 3-STAGE models, which produce Wildwood is a restaurant.","These problems are only slightly reduced in the filtered version, but they are largely eliminated with 2-STAGE and 3-STAGE models.","Modify,Claim",Claim
867,136-ARR,136-ARR_v2_89@1,136-ARR_v1_81@9,"Grammar errors and disfluencies stem mainly from over-eager paragraph compression or from artifacts in our templates and are relatively minor (e.g., missing ""is"" in ""serves French food and family-friendly"").","The grammar errors and disfluencies stem mainly from over-eager paragraph compression or from artifacts in our templates; they are relatively minor (e.g., missing ""is"" in ""serves French food and family-friendly"").","Modify,Clarity",Clarity
868,136-ARR,136-ARR_v2_91@0,136-ARR_v1_84@0,"Following and , we report the accuracy and BLEU-2 score of our ordering model on WebNLG against the humangenerated plans from Ferreira et al. (2018).","Following and , we report the accuracy (Acc) and BLEU-2 score (B-2) of our ordering model on WebNLG against the human-generated plans from Ferreira et al. (2018).","Modify,Clarity",Clarity
869,136-ARR,136-ARR_v2_2@1,136-ARR_v1_2@1,We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs.,We examine how to avoid finetuning the pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs.,"Modify,Grammar",Grammar
870,136-ARR,136-ARR_v2_7@0,136-ARR_v1_7@0,"In this paper, we present a zero-shot alternative to the traditional finetuning paradigm by formulating the D2T generation from RDF triples as a sequence of general-domain operations over text in natural language.","In this paper, we provide an alternative to this traditional paradigm by formulating the D2T generation from RDF triples as a sequence of generaldomain operations over text in natural language.","Modify,Clarity",Clarity
871,136-ARR,136-ARR_v2_92@0,136-ARR_v1_89@1,"Finally, we manually evaluate how the PC model follows the content plan (i.e., keeping the predefined order and aggregating the sentences according to the delimiters) using 100 randomly chosen examples with more than 1 triple on WebNLG and E2E.","Finally, we manually evaluate how the PC model follows the content plan using 100 randomly chosen examples with more than 1 triple on WebNLG and E2E.","Modify,Fact/Evidence",Fact/Evidence
872,136-ARR,136-ARR_v2_92@2,136-ARR_v1_89@3,The incorrect cases include a fact not properly mentioned or an extra boundary between sentences without a separator.,The incorrect cases include a fact not properly mentioned and an extra boundary between the sentences without a separator.,"Modify,Fact/Evidence",Fact/Evidence
873,136-ARR,136-ARR_v2_95@0,136-ARR_v1_90@0,Future Work,Discussion and Future Work,"Modify,Other",Other
874,136-ARR,136-ARR_v2_96@1,136-ARR_v1_92@1,"Automatic generation of facts without using hand-crafted templates (cf. §5.1) could allow applying zero-shot generation systems to datasets with a large number of predicates, such as ToTTo (Parikh et al., 2020).","Generating simple statements from the triples automatically, e.g., using the approach of Laha et al. (2020), could reduce the manual workload and allow applying our approach on datasets with a less constrained set of data attributes such as ToTTo (Parikh et al., 2020) or DART (Nan et al., 2021).","Modify,Claim",Claim
875,136-ARR,136-ARR_v2_97@0,136-ARR_v1_93@0,"More research is also needed regarding the main shortcoming of our approach, i.e., the semantic errors stemming from merging of facts in improper ways.",More research is also needed on semantic errors stemming from merging of facts in improper ways.,"Modify,Clarity",Clarity
876,136-ARR,136-ARR_v2_97@1,136-ARR_v1_93@1,"As we suggested in §7.2, explicitly controlling the semantics of sentence fusion could help to mitigate this issue, while still keeping the advantages of a zero-shot approach.","We suggest that explicitly controlling the semantics of sentence fusion (Ben-David et al., 2020) could help to mitigate this issue, while still keeping the advantages of a zero-shot approach.","Modify,Fact/Evidence",Fact/Evidence
877,136-ARR,136-ARR_v2_103@0,136-ARR_v1_95@0,"We implemented the models for split-and-rephrase, aggregation, and paragraph compression in Py-Torch Lightning (Paszke et al., 2019), using the PyTorch (Falcon et al., 2019) version of the BART and RoBERTa models from the Huggingface library (Wolf et al., 2019).","We implemented the models for split-and-rephrase, aggregation, and paragraph compression in Py-Torch Lightning (Paszke et al., 2019), using the PyTorch (Falcon, 2019) version of the BART and RoBERTa models from the Huggingface library (Wolf et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
878,136-ARR,136-ARR_v2_105@1,136-ARR_v1_97@1,We will integrate the ordering model into our framework.,We plan to fully integrate the ordering model into our framework in the future.,"Modify,Clarity",Clarity
879,136-ARR,136-ARR_v2_7@2,136-ARR_v1_7@2,"In constrast to traditional pipeline systems, all our pipeline modules are built upon PLMs and operate over sentences in natural language.",All the pipeline modules operate over natural language text and are built upon PLMs trained on our WIKIFLUENT corpus.,"Link+Modify,Clarity",Clarity
880,136-ARR,136-ARR_v2_7@3,136-ARR_v1_7@2,"The modules are trained on our new WIKI-FLUENT corpus, which contains 934k examples of first paragraphs from the English Wikipedia, each supplied with a synthesized set of simple templatelike sentences which together convey the meaning of the original paragraph.",All the pipeline modules operate over natural language text and are built upon PLMs trained on our WIKIFLUENT corpus.,"Link+Modify,Clarity",Clarity
881,136-ARR,136-ARR_v2_7@3,136-ARR_v1_7@3,"The modules are trained on our new WIKI-FLUENT corpus, which contains 934k examples of first paragraphs from the English Wikipedia, each supplied with a synthesized set of simple templatelike sentences which together convey the meaning of the original paragraph.","WIKIFLUENT contains 934k examples of first paragraphs from the English Wikipedia, each supplied with a synthesized set of simple template-like sentences conveying the same meaning.","Link+Modify,Clarity",Clarity
882,136-ARR,136-ARR_v2_7@4,136-ARR_v1_7@4,Our approach allows generating natural language descriptions from RDF triples with a minimum amount of domain-specific rules or knowledge and without using training data from the D2T datasets.,Our approach allows generating natural language descriptions from triples with a minimum amount of domain-specific rules or knowledge and without using training data from the D2T datasets.,"Modify,Fact/Evidence",Fact/Evidence
883,136-ARR,136-ARR_v2_7@5,136-ARR_v1_7@5,"Although our approach is primarily a probe into the territory of zero-shot approaches and cannot yet match the quality of stateof-the-art models, we show that it can yield large improvements upon simple baselines and match older supervised systems on automatic metrics for text fluency.","We show that our approach can yield large improvements upon simple baselines and match older supervised systems in terms of fluency, while bringing potential for further improvements and advantages with respect to controllability.","Modify,Claim",Claim
884,136-ARR,136-ARR_v2_2@2,136-ARR_v1_2@2,"Inspired by pipeline approaches, we propose to generate text by transforming single-item descriptions with a sequence of modules trained on generaldomain text-based operations: ordering, aggregation, and paragraph compression.","Inspired by pipeline approaches, we propose to generate text by rephrasing single-item templates using a sequence of modules trained on general-domain text-based operations-ordering, aggregation, and paragraph compression.","Modify,Clarity",Clarity
885,136-ARR,136-ARR_v2_10@1,136-ARR_v1_10@1,"Following Chen et al. (2020b), other works adopted PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a).","Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a).","Modify,Fact/Evidence",Fact/Evidence
886,136-ARR,136-ARR_v2_12@0,136-ARR_v1_12@0,"Content Planning in D2T Generation Content planning, i.e. the task of ordering input facts and aggregating them into individual sentences, is one of the steps of the traditional D2T pipeline (Gatt and Krahmer, 2018).","Content Planning in D2T Generation Content planning, i.e. ordering input facts and aggregating them into individual sentences, is a traditional part of the D2T generation pipeline (Ferreira et al., 2019;Gatt and Krahmer, 2018;Reiter and Dale, 1997).","Modify,Fact/Evidence",Fact/Evidence
887,136-ARR,136-ARR_v2_12@2,136-ARR_v1_12@3,"Unlike the aforementioned planners, which use predicates or keys from D2T datasets for representing the data items, our planner is trained on ordering sentences in natural language.",All the aforementioned models plan directly using predicates or keys in the D2T datasets representing the corresponding data item.,"Merge+Modify,Clarity",Clarity
888,136-ARR,136-ARR_v2_12@2,136-ARR_v1_12@4,"Unlike the aforementioned planners, which use predicates or keys from D2T datasets for representing the data items, our planner is trained on ordering sentences in natural language.","Unlike these works, our planner is trained on ordering sentences in natural language.","Merge+Modify,Clarity",Clarity
889,136-ARR,136-ARR_v2_14@0,136-ARR_v1_14@0,"Aggregating Input into Sentences Typically, multiple pieces of input information need to be merged into a single sentence.",Fact Aggregation The compact nature of the target text description results in aggregating multiple facts in a single sentence.,"Modify,Claim",Claim
890,136-ARR,136-ARR_v2_14@2,136-ARR_v1_14@2,"Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts (see §3.1), into which we selectively insert delimiters to mark sentence boundaries.","Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts, in which we selectively insert delimiters marking sentence boundaries.","Modify,Fact/Evidence",Fact/Evidence
891,136-ARR,136-ARR_v2_15@1,136-ARR_v1_14@4,"This task combines several standard natural-language tasks including sentence fusion, rephrasing, and coreference resolution.","As the last step in the pipeline, it is closely related to linguistic realisation, howeversince we already work with natural language in this step-the focus of our task is on sentence fusion, rephrasing, and coreference resolution.","Modify,Fact/Evidence",Fact/Evidence
892,136-ARR,136-ARR_v2_19@1,136-ARR_v1_16@1,"The individual steps are described in the following sections: transforming individual triples to text ( §3.1), ordering ( §3.2), aggregation ( §3.3), and paragraph compression ( §3.4).","Next, we describe the individual steps, starting by applying simple templates to transform data to text ( §3.2), followed by individual modules for ordering ( §3.3), aggregation ( §3.4), and paragraph compression ( §3.5).","Modify,Fact/Evidence",Fact/Evidence
1343,166-ARR,,166-ARR_v1_37@1,,"For our usage, the input is the contextual token embedding (the L-th layer's output), and the side information is the static token embedding (the output of BERT's initial embedding layer).","Delete,Fact/Evidence",Fact/Evidence
1344,166-ARR,,166-ARR_v1_37@2,,"The resulting cdimensional encoded vector can be thought of as the distilled context of the input token. (Hendrycks and Gimpel, 2016).","Delete,Fact/Evidence",Fact/Evidence
1345,166-ARR,,166-ARR_v1_59@0,,3 The checkpoints will be released with the published paper.,"Delete,Fact/Evidence",Fact/Evidence
1346,166-ARR,166-ARR_v2_20@2,,"For that reason these methods are used mainly for first-stage retrieval, followed by a reranking step.",,"Add,Claim",Claim
1347,166-ARR,166-ARR_v2_45@5,,It maps from article and section titles to relevant paragraphs.,,"Add,Fact/Evidence",Fact/Evidence
1348,166-ARR,166-ARR_v2_45@6,,"Following , we use the automatic by-article annotations variant, which considers all paragraphs within the same article as relevant.",,"Add,Fact/Evidence",Fact/Evidence
1349,166-ARR,166-ARR_v2_20@1,166-ARR_v1_20@1,"While this ranker architecture approach is simple (and can also be used for the retrieval step via an approximate nearest neighbor search such as FAISS (Johnson et al., 2017), ScaNN (Guo et al., 2020) or the Pinecone managed service 2 ), the overall ranking quality is generally lower compared to methods that employ a query-document crossattention interaction.","While this ranker architecture approach is simple (and can also be used for the retrieval step via an approximate nearest neighbor search such as FAISS (Johnson et al., 2017) or ScaNN (Guo et al., 2020)), the overall ranking quality is generally lower compared to methods that employ a query-document cross-attention interaction.","Modify,Claim",Claim
1350,166-ARR,166-ARR_v2_38@0,166-ARR_v1_40@0,"To remedy this issue, we follow an approach similar to EDEN quantization (Vargaftik et al., 2022), which uses a randomized Hadamard transform prior to quantization.","To remediate this issue, we follow the DRIVE approach (Vargaftik et al., 2021a), which uses a randomized Hadamard transform prior to quantization.","Modify,Fact/Evidence",Fact/Evidence
1351,166-ARR,166-ARR_v2_45@2,166-ARR_v1_47@2,"Unlike MSMARCO-DEV, there are multiple passages annotated for each query with graded relevance labels (instead of binary labels), allowing us to use the more informative nDCG@10 metric.","Unlike the above, there are many passages annotated for each query, and there are graded relevance labels (instead of binary labels).","Merge+Modify,Clarity",Clarity
1352,166-ARR,166-ARR_v2_45@2,166-ARR_v1_47@3,"Unlike MSMARCO-DEV, there are multiple passages annotated for each query with graded relevance labels (instead of binary labels), allowing us to use the more informative nDCG@10 metric.",This allows us to use the more informative nDCG@10 metric.,"Merge+Modify,Clarity",Clarity
1354,166-ARR,166-ARR_v2_45@7,166-ARR_v1_48@0,"The dataset consists of 30M passages, making storage requirements a more significant challenge compared to the MSMARCO task.","TREC Complex Answer Retrieval (CAR) is a dataset (Dietz et al., 2017) curated from Wikipedia, and consists of 30M passages, making storage requirements a bigger challenge compared to the MS-MARCO task.","Split+Modify,Clarity",Clarity
1355,166-ARR,166-ARR_v2_48@2,166-ARR_v1_51@2,"For MSMARCO, we initialized the model from reduced width pre-trained weights 4 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.","For MS-MARCO, we initialized the model from reduced width pre-trained weights 2 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.","Modify,Fact/Evidence",Fact/Evidence
1356,166-ARR,166-ARR_v2_51@0,166-ARR_v1_54@0,"In the following sections, we denote the SDR variants as ""AESI-{c}-{B}b"" where {c} is replaced with the width of the encoded vector and {B} is replaced with the number of bits in the quantization scheme.","It the following sections, we denote the SDR variants as ""AESI-{c}-{B}b"" where {c} is replaced with the width of the encoded vector and {B} is replaced with the number of bits in the quantization scheme.","Modify,Grammar",Grammar
1357,166-ARR,166-ARR_v2_54@0,166-ARR_v1_57@0,Evaluation Results,Evaluation,"Modify,Other",Other
1358,166-ARR,166-ARR_v2_57@1,166-ARR_v1_62@0,The Distilbert model (full interaction architecture) has the highest quality and smallest index size (since it is only executed online).,The Distilbert model has the highest quality and smallest index size (since it is only executed online).,"Modify,Fact/Evidence",Fact/Evidence
1359,166-ARR,166-ARR_v2_59@2,166-ARR_v1_63@5,"The late interaction variants we consider have the same configuration as in the MS-MARCO case, where the baseline uses 24 features (with float16 quantization) and SDR uses 16 features (with 6 bits EDEN quantization).","The late interaction variants we consider have the same configuration as in the MS-MARCO case, where the baseline uses 24 features (with float16 quantization) and SDR uses 16 features (with 6 bits DRIVE quantization).","Modify,Fact/Evidence",Fact/Evidence
1360,166-ARR,166-ARR_v2_71@2,166-ARR_v1_73@2,"Using a quantization technique fitted to the Gaussian distribution of post randomized Hadamard transform data further improve quality, making the EDEN quantization superior to other quantization techniques in our case.","Using a quantization technique fitted to the Gaussian distribution of post randomized Hadamard transform data further improve quality, making the DRIVE quantization superior to other quantization techniques in our case.","Modify,Fact/Evidence",Fact/Evidence
1361,166-ARR,166-ARR_v2_73@0,166-ARR_v1_75@0,"To better understand the impact of side information, we measure the error rate between an input vector and its reconstructed vector (i.e., after encoding and decoding).","To better understand the impact of side information, we measure the error rate between an input vector and a reconstructed vector (i.e., after encoding and decoding) for different input vectors.","Modify,Fact/Evidence",Fact/Evidence
1362,166-ARR,166-ARR_v2_74@0,166-ARR_v1_76@0,"In IR, the document frequency of a token is known to be negatively correlated with the token's importance.","In the information retrieval field, the document frequency of a token is known to negatively correlated with the token's importance.","Modify,Clarity",Clarity
1363,166-ARR,166-ARR_v2_74@2,166-ARR_v1_76@2,This shows that the AESI scheme can better focus on tokens that are important for ranking.,This shows that the AESI scheme has a better focus on tokens that are important for ranking.,"Modify,Clarity",Clarity
1364,166-ARR,166-ARR_v2_74@3,166-ARR_v1_76@3,"A possible explanation for this phenomena is that the static embeddings for infrequent tokens are more informative (i.e., more helpful as side information) compared to static embeddings for frequent tokens (e.g., 'the').","A possible explanation for this phenomena is that the static embeddings for infrequent tokens is more informative (i.e., more helpful as side information) compared to static embeddings for frequent tokens (e.g., 'the').","Modify,Grammar",Grammar
1365,166-ARR,166-ARR_v2_74@4,166-ARR_v1_76@4,"We also found AESI excels more in compressing nouns, verbs, and adjectives, while AE-2L excels more in compressing punctuation, determiners, and adpositions.","We also found AESI excels more in compressing nouns, verbs, and adjectives, while AE-2L excels more in compressing punctuations, determiners, and adpositions.","Modify,Grammar",Grammar
1366,166-ARR,166-ARR_v2_74@6,166-ARR_v1_76@6,The details of this evaluation are provided in Appendix C.,The details of this evaluation appear in Appendix C.,"Modify,Clarity",Clarity
1367,166-ARR,166-ARR_v2_76@0,166-ARR_v1_78@0,"In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction transformer based models for passage re-ranking.","In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction models.","Modify,Fact/Evidence",Fact/Evidence
1368,166-ARR,166-ARR_v2_76@2,166-ARR_v1_78@2,"In addition, we explored different quantization techniques and showed that the recently proposed EDEN performs well in our use case and presented extensive experimentation.","In addition, we explored different quantization techniques and showed that the recently proposed DRIVE performs well in our use case and presented extensive experimentation.","Modify,Fact/Evidence",Fact/Evidence
1369,166-ARR,166-ARR_v2_87@2,166-ARR_v1_89@2,"In addition to EDEN (Appendix A, Algorithm 1), we consider the following quantization strategies: Deterministic Rounding (DR) (Gersho and Gray, 1992).","In addition to DRIVE (Appendix A, Algorithm 1), we consider the following quantization strategies: Deterministic Rounding (DR) (Gersho and Gray, 1992).","Modify,Fact/Evidence",Fact/Evidence
1370,166-ARR,166-ARR_v2_87@12,166-ARR_v1_89@12,"EDEN with Bias Correction (EDEN-BC) (Vargaftik et al., 2022, Section 2.3).","DRIVE with Bias Correction (DRIVE-BC) (Vargaftik et al., 2021b, Appendix C.3).","Modify,Fact/Evidence",Fact/Evidence
1371,166-ARR,166-ARR_v2_87@13,166-ARR_v1_89@13,"This variant of EDEN optimizes for lower bias over the mean squared error (MSE) by multiplying the dequantization result in Algorithm 1 by a bias correction scalar: x 2 2 / H(x), ŷ .",This variant of DRIVE optimizes for lower bias over the mean squared error (MSE) by multiplying the dequantization result in Algorithm 1 by a bias correction scalar: x 2 2 / ŷ 2 2 .,"Modify,Fact/Evidence",Fact/Evidence
1372,166-ARR,166-ARR_v2_88@2,166-ARR_v1_90@2,"Second, we see that EDEN performs better than all other schemes.","Second, we see that DRIVE performs better than all other schemes.","Modify,Fact/Evidence",Fact/Evidence
1373,166-ARR,166-ARR_v2_88@5,166-ARR_v1_90@5,"This conclusion follows by observing that EDEN and the deterministic rounding methods (DR, H-DR) are respectively better than EDEN-BC and the stochastic rounding methods (SR, H-SR).","This conclusion follows by observing that DRIVE and the deterministic rounding methods (DR, H-DR) are respectively better than DRIVE-BC and the stochastic rounding methods (SR, H-SR).","Modify,Fact/Evidence",Fact/Evidence
1374,166-ARR,166-ARR_v2_100@1,166-ARR_v1_102@1,"We compare the baseline -an autoencoder with 2 layers and float16 quantization -to SDR scheme, with the same number of features and EDEN 6bits quantization.","We compare the baseline -an autoencoder with 2 layers and float16 quantization -to SDR scheme, with the same number of features and DRIVE 6bits quantization.","Modify,Fact/Evidence",Fact/Evidence
1375,166-ARR,166-ARR_v2_100@2,166-ARR_v1_102@2,The SDR scheme is able to provide solid results even for 3 features with a MAP@1K score of 0.268.,"The SDR scheme is able to provide solid results even for 3 features with 0.266 (as a comparison, BM25 using Answerini system (Yang et al., 2017) reaches 15.6 MAP@1K score).","Split+Modify,Fact/Evidence",Fact/Evidence
1376,166-ARR,166-ARR_v2_100@4,166-ARR_v1_102@2,"As a comparison, this is much higher than BM25 using the Answerini system (Yang et al., 2017), which reaches 0.156 MAP@1K score.","The SDR scheme is able to provide solid results even for 3 features with 0.266 (as a comparison, BM25 using Answerini system (Yang et al., 2017) reaches 15.6 MAP@1K score).","Split+Modify,Fact/Evidence",Fact/Evidence
1377,166-ARR,166-ARR_v2_100@6,166-ARR_v1_102@5,"Finally, with the largest size of features we tested, 64, the baseline reached a MAP@1K score of 0.313, similar to the score achieved by SDR-20 (0.314), demonstrating the effectiveness of the static embeddings as side information.","Finally, with the largest size of features we tested, 64, the baseline reached a MAP@1K score of 0.313, similar to the score achieved by SDR-20 (0.315), demonstrating the effectiveness of the static embeddings as side information.","Modify,Fact/Evidence",Fact/Evidence
1378,166-ARR,166-ARR_v2_13@2,166-ARR_v1_13@2,"To improve the compression-reliability tradeoff, we leverage static token embeddings, which are available since the ranker has access to the document text (as it needs to render it to the user), and are computationally cheap to obtain.","To improve the compression-reliability tradeoff, we leverage static token embeddings, which are available given that the ranker has access to the document text (as it needs to render it to the user), and are computationally cheap to obtain.","Modify,Clarity",Clarity
1379,166-ARR,166-ARR_v2_13@4,166-ARR_v1_13@4,Ablation tests verify that adding the static vectors significantly improves the compression rates for the same ranking accuracy.,Ablation tests verify that adding the static vectors significantly improves the compression rates for the same ranker accuracy.,"Modify,Grammar",Grammar
1380,166-ARR,166-ARR_v2_15@2,166-ARR_v1_15@2,"For the MSMARCO dataset, we used a distilled model with a reduced vector width (Hofstätter et al., 2020a) as the initial pre-trained weights for the late-interaction model.","For the MSMARCO dataset, we used a distilled model with a reduced vector width (Hofstätter et al., 2020a) as the initialized pre-trained weights for the late-interaction model.","Modify,Grammar",Grammar
1381,166-ARR,166-ARR_v2_16@0,166-ARR_v1_16@0,"To summarize, here are the contribution of this work 1 :",We make the following contributions:,"Modify,Clarity",Clarity
1480,172-ARR,,172-ARR_v1_45@2,,"XDBERT-b gains performance on 52 entries, while performing on par or worse on 225 entries.","Delete,Fact/Evidence",Fact/Evidence
1481,172-ARR,,172-ARR_v1_48@0,,We briefly describe the ablation studies on the adaptation process.,"Delete,Fact/Evidence",Fact/Evidence
1482,172-ARR,,172-ARR_v1_51@0,,"We show the result of Visual-Text Transformers on GLUE, reported by Tan and Bansal (2020) in Table 6.","Delete,Fact/Evidence",Fact/Evidence
1483,172-ARR,,172-ARR_v1_51@1,,All of the listed methods (except LXMERT) have their text-transformers initialized from BERT.,"Delete,Fact/Evidence",Fact/Evidence
1484,172-ARR,,172-ARR_v1_52@0,,The results show that multi-modal training for solving vision-language tasks does not improve the performance of the models on natural language understanding tasks.,"Delete,Fact/Evidence",Fact/Evidence
1485,172-ARR,,172-ARR_v1_62@0,,We report the performance of small tasks while using different loss functions.,"Delete,Fact/Evidence",Fact/Evidence
1486,172-ARR,,172-ARR_v1_63@3,,"Driven by the loss function of cross-modal matching, the model sacrifices rare word representations to better match common words.","Delete,Fact/Evidence",Fact/Evidence
1487,172-ARR,,172-ARR_v1_63@4,,"By adding VC loss, each VC token must keep its representation, which makes the cross-modal attention more robust.","Delete,Claim",Claim
1488,172-ARR,,172-ARR_v1_64@2,,"In the following examples, Bold words are visuallygrounded, while normal words are non-visuallygrounded.","Delete,Fact/Evidence",Fact/Evidence
1489,172-ARR,,172-ARR_v1_64@3,,Words in brackets are stopwords and does not count towards either category.,"Delete,Fact/Evidence",Fact/Evidence
1490,172-ARR,172-ARR_v2_22@1,,"The text-image input (X i , Y i ) is paired, and every (X j , Y k ) (j ̸ = k) is a non-pair.",,"Add,Fact/Evidence",Fact/Evidence
1491,172-ARR,172-ARR_v2_28@0,,"The Image-Text Matching (ITM) objective is widely used in multimodal learning (Tan and Bansal, 2020;Radford et al., 2021).",,"Add,Fact/Evidence",Fact/Evidence
1492,172-ARR,172-ARR_v2_28@1,,We modify this objective to same sentence prediction as both streams of our model takes text as input.,,"Add,Fact/Evidence",Fact/Evidence
1493,172-ARR,172-ARR_v2_32@1,,"Same as MLM, 15% of the tokens are randomly selected for reconstruction.",,"Add,Fact/Evidence",Fact/Evidence
1494,172-ARR,172-ARR_v2_32@2,,We address concerns on trivial solutions learned by the model in Section 5 and 9 in the appendix.,,"Add,Fact/Evidence",Fact/Evidence
1495,172-ARR,172-ARR_v2_41@2,,Entries where both models obtain the same performance are set aside.,,"Add,Fact/Evidence",Fact/Evidence
1496,172-ARR,172-ARR_v2_43@3,,"Swapping to wiki reduces potential overfitting from the 20 Epochs setting trained on wiki103, as training for the same amount of steps on wiki is less than 1 epoch.",,"Add,Fact/Evidence",Fact/Evidence
1497,172-ARR,172-ARR_v2_44@3,,"With the CLIPTC objective, the diversity of the input embeddings corresponding to different tokens must be retained in the cross-modal encoder, leading to more robust cross-modal attention.",,"Add,Claim",Claim
1498,172-ARR,,172-ARR_v1_36@1,,This loss prevents the cross-modal matching to only focus on common trivial words.,"Delete,Fact/Evidence",Fact/Evidence
1499,172-ARR,,172-ARR_v1_36@2,,We show the attention maps of the cross-modal encoders in Appendix D to verify this.,"Delete,Fact/Evidence",Fact/Evidence
1500,172-ARR,172-ARR_v2_36@1,172-ARR_v1_40@1,"We tested our adaptation strategy on three different language encoders coupled with CLIP-T, including BERT-base, ELECTRA-base, and ELECTRA-large.","We tested our pretraining strategy on three different language encoders coupled with CLIP-T, including BERT-base, ELECTRA-base, and ELECTRA-large.","Modify,Clarity",Clarity
1501,172-ARR,172-ARR_v2_41@3,172-ARR_v1_45@3,"Analyzing the separated entries as a whole, we discovered that the betterperforming entries have a larger visually grounded ratio (Figure 4), as the quartile, median and mean values are generally higher for improved samples.",We found out that the better-performing entries have a shorter sentence length and their tokens have a larger visually grounded ratio (Figure 3).,"Modify,Fact/Evidence",Fact/Evidence
1502,172-ARR,172-ARR_v2_43@4,172-ARR_v1_48@3,"We tested these changes on RTE, MPRC, STSB, and CoLA on 5 random seeds, and the results are shown in Table 3, where MLM refers to the joint MLM objective, MATCH refers to the cross-modal matching objective, and CLIPTC refers to the CLIP token classification objective.","We tested these changes on RTE, MPRC, STSB, and CoLA on 5 random seeds, and the results are shown in Appendix D.","Merge+Modify,Fact/Evidence",Fact/Evidence
1503,172-ARR,172-ARR_v2_49@6,172-ARR_v1_55@6,"The issue is also present in the finetuning phase, and the maximum sequence length of GLUE and SWAG is 128; therefore we used 2 blocks of CLIP sub-sequences to model it.","The issue is also present in the finetuning phase, and the maxmum sequence length of GLUE and SWAG is 128; therefore we used 2 blocks of CLIP sub-sequences to model it.","Modify,Grammar",Grammar
1504,172-ARR,172-ARR_v2_43@4,172-ARR_v1_62@1,"We tested these changes on RTE, MPRC, STSB, and CoLA on 5 random seeds, and the results are shown in Table 3, where MLM refers to the joint MLM objective, MATCH refers to the cross-modal matching objective, and CLIPTC refers to the CLIP token classification objective.","The results are shown in Table 8, where MLM refers to the joint MLM, MATCH refers to cross-modal matching, and VC (visual classification) refers to the CLIP token classification.","Merge+Modify,Fact/Evidence",Fact/Evidence
1505,172-ARR,172-ARR_v2_43@2,172-ARR_v1_62@2,"Other experiments include changing the number of layers in the crossmodal encoder, training for longer, and swapping to a much larger wiki (14G).","Other attempts include changing the number of layers in the cross-modal encoder, training for longer, and swapping to a much larger wiki (14G).","Modify,Clarity",Clarity
1506,172-ARR,172-ARR_v2_44@0,172-ARR_v1_63@0,"Besides experimental evidence, we also justify the CLIPTC loss via further analysis, as the CLIPTC objective can theoretically be trivially solved by identity mapping.","Besides experimental evidence, we also justify the VC loss via further analysis because VC loss can theoretically be trivially solved by identity mapping.","Modify,Fact/Evidence",Fact/Evidence
1507,172-ARR,172-ARR_v2_44@1,172-ARR_v1_63@1,"Despite this possibility, we find that the loss is crucial to cross attention learning.",The importance of this loss is to balances out the cross-modal matching loss.,"Modify,Claim",Claim
1508,172-ARR,172-ARR_v2_44@2,172-ARR_v1_63@2,"Since we do not impose negative hard samples from sampled sentences, the MATCH objective can be solved sufficiently simply by guiding the cross attention to focus on common trivial words.","Without VC loss, the cross-modal matching is too easy, because relying on only a few common words is sufficient, even when the word is trivial to the meaning of the sentence.","Modify,Claim",Claim
1509,172-ARR,172-ARR_v2_44@4,172-ARR_v1_63@5,We show comparisons of the attention maps generated from the cross-modal encoders with a random sequence from RTE in Table 9 in the Appendix to verify this claim.,We show the attention of the cross-modal matching with a random sequence from RTE in Table 9.,"Modify,Fact/Evidence",Fact/Evidence
1510,172-ARR,172-ARR_v2_0@0,172-ARR_v1_0@0,XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding,XDBERT: Distilling Visual Information to BERT via Cross-Modal Encoders to Improve Language Understanding,"Modify,Clarity",Clarity
1511,172-ARR,172-ARR_v2_7@0,172-ARR_v1_7@0,"In this work, we establish the link between pretrained multimodal transformers and visuallygrounded language learning.","In this work, we established the link between pretrained multimodal transformers and visuallygrounded language learning.","Modify,Grammar",Grammar
1512,172-ARR,172-ARR_v2_7@1,172-ARR_v1_7@1,"We devise a way to distill visual information from components of a pretrained multimodal transformer (CLIP texttransfomer, abbreviated as CLIP-T) to pretrained language transformers (BERT/ELECTRA), to incorporate versatile perception of words into the model (Figure 1).","We devised a way to distill visual information from components of a pretrained multimodal transformer (CLIP texttransfomer, briefed by CLIP-T) to pretrained language transformers (BERT/ELECTRA).","Modify,Fact/Evidence",Fact/Evidence
1513,172-ARR,172-ARR_v2_8@0,172-ARR_v1_8@0,"Methodologically, we use the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot fewer than the original pretraining steps).","Methodologically, we used the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot smaller than the original pretraining steps).","Modify,Grammar",Grammar
1514,172-ARR,172-ARR_v2_8@3,172-ARR_v1_10@1,We do ablation studies to show that each of the task provides improvement (Section 5).,We do ablation studies to show that each of the task provides improvement (Appendix D).,"Modify,Fact/Evidence",Fact/Evidence
1515,172-ARR,172-ARR_v2_24@1,172-ARR_v1_30@1,"In these tasks, BERT and CLIP-T takes sentences A and B respectively as input, and losses are calculated from both BERT output and CLIP-T output.","In these tasks, BERT and CLIP-T takes sentences A and B respectively as input as we condition the output.","Modify,Fact/Evidence",Fact/Evidence
1516,172-ARR,172-ARR_v2_32@0,172-ARR_v1_36@0,"This is the MLM objective done on the CLIP-T side of the full model, omitting the masking part because CLIP has no mask token.","This is the MLM objective done on the CLIP-T side of the full model, but the CLIP-T sentence input tokens is neither being masked nor replaced.","Modify,Fact/Evidence",Fact/Evidence
1676,178-ARR,,178-ARR_v1_7@3,,We will release our codes and models for further research.,"Delete,Fact/Evidence",Fact/Evidence
1677,178-ARR,178-ARR_v2_88@0,,"We set seeds of torch, torch.cuda, numpy, and random in Python to ensure reproducibility.",,"Add,Fact/Evidence",Fact/Evidence
1678,178-ARR,178-ARR_v2_88@1,,We use a grid search to find the best hyperparameters depending on development set performances.,,"Add,Fact/Evidence",Fact/Evidence
1679,178-ARR,178-ARR_v2_88@3,,"If the contextual embedding learning rate is 3e-5, we use static embedding learning rate and task learning rate as 5e-4 and 3e-5.",,"Add,Fact/Evidence",Fact/Evidence
1680,178-ARR,178-ARR_v2_89@0,,"We introduce the decomposition of triaffine scoring in calculating p i,j,r and p c i,j,r .",,"Add,Fact/Evidence",Fact/Evidence
1681,178-ARR,178-ARR_v2_17@2,178-ARR_v1_17@2,"Then, we will introduce our model including triaffine attention and triaffine scoring based on the proposed triaffine transformations.","Then, we will introduce our model based on the proposed triaffine transformations.","Modify,Fact/Evidence",Fact/Evidence
1682,178-ARR,178-ARR_v2_19@0,178-ARR_v1_19@0,"We define the deep triaffine transformation with vectors u, v, w ∈ R d and a tensor W ∈ R d+1 × R d × R d+1 which outputs a scalar by applying distinct MLP (multi-layer perceptron) transformations on input vectors and calculating tensor vector multiplications.","We define the deep triaffine transformation with vectors u, v, w ∈ R d and a tensor W ∈ R d+1 × R d × R d+1 which outputs a scalar by applying distinct MLP transformations on input vectors and calculating tensor vector multiplications.","Modify,Fact/Evidence",Fact/Evidence
1683,178-ARR,178-ARR_v2_42@0,178-ARR_v1_44@0,"Since h c i,j,r are composed by h ig,jg,r , we can decompose Equation 11 into following if and only if the layer of MLP transformation on h c i,j,r is 0:","Since h c i,j,r are composed by h ig,jg,r , we can decompose Equation 11 into:","Modify,Fact/Evidence",Fact/Evidence
1684,178-ARR,178-ARR_v2_57@0,178-ARR_v1_59@1,"To fairly compare with previous works, we follow the same dataset split with Lu and Roth (2015) for ACE2004 and ACE2005 and use the split from for GENIA and KBP2017 datasets.","To fairly compare with previous works, we follow the same dataset split with Lu and Roth (2015) for ACE2004 and ACE2005 datasets and use the split from for GENIA and KBP2017 datasets.","Modify,Clarity",Clarity
1685,178-ARR,178-ARR_v2_6@0,178-ARR_v1_6@0,"Although some of the factors may have been explored in previous works, to the best of our knowledge, it is the first work to fuse all these heterogeneous factors into a unified network.","Although some of the factors may be explored in previous works, to the best of our knowledge, it is the first work to fuse all these heterogeneous factors into a unified network.","Modify,Grammar",Grammar
1687,178-ARR,178-ARR_v2_7@2,178-ARR_v1_7@1,"Using the BERT encoder, our model outperforms stateof-the-art methods on GENIA and KBP2017 and shows comparable performances on ACE2004 and ACE2005 with the latest generative methods.","Our model achieves 88.56, 88.83, 81.23, and 87.27 scores in terms of F 1 , respectively, outperforming state-of-the-art methods.","Split+Modify,Fact/Evidence",Fact/Evidence
1688,178-ARR,178-ARR_v2_2@4,178-ARR_v1_2@4,Triaffine attention uses boundaries and labels as queries and uses inside tokens and related spans as keys and values for span representations.,"Triaffine attention uses boundaries and labels as queries, and uses inside tokens and related spans as keys and values for span representations.","Modify,Grammar",Grammar
1689,178-ARR,178-ARR_v2_15@3,178-ARR_v1_15@3,"Except for the similar formula of vectors' interactions, the motivation and the use of triaffine are different in our paper.",There are two key differences between our triaffine transformation and theirs.,"Modify,Fact/Evidence",Fact/Evidence
1690,178-ARR,178-ARR_v2_15@4,178-ARR_v1_15@4,"Firstly, they only model the homogeneous features such as three tokens, but our triaffine transformation can model heterogeneous factors including labels, boundaries, and related spans.","Firstly, they only model the homogeneous features such as three tokens, but our triaffine transformation can model heterogeneous factors.","Modify,Fact/Evidence",Fact/Evidence
1963,192-ARR,,192-ARR_v1_6@5,,"Our model is developed on the basis of (Wu et al., 2020a).","Delete,Fact/Evidence",Fact/Evidence
1964,192-ARR,192-ARR_v2_2@5,192-ARR_v1_2@5,"Specifically, an entity recognizer and a similarity evaluator are first trained in parallel as two teachers from the source domain.","Specifically, an entity recognizer and a similarity evaluator teachers are first trained in parallel from the source domain.","Modify,Clarity",Clarity
1965,192-ARR,192-ARR_v2_21@2,192-ARR_v1_23@2,"During the learning process, the samples from the target language are fed into the teacher model and the outputs are taken as the supervisory signal for two tasks in the student model.","During the learning process, the samples from target language are fed into the teacher model and the outputs are taken as the supervisory signal for two tasks in the student model.","Modify,Grammar",Grammar
1966,192-ARR,192-ARR_v2_2@6,192-ARR_v1_2@6,"Then, two tasks in the student model are supervised by these teachers simultaneously.","Then, two tasks in the student model are supervised by the two teachers simultaneously.","Modify,Clarity",Clarity
1967,192-ARR,192-ARR_v2_25@2,192-ARR_v1_27@2,We aim to find entity similarity to help the crosslingual NER model in the target language.,We aim to find entity similarity to help the cross-lingual NER model in target language.,"Modify,Grammar",Grammar
1968,192-ARR,192-ARR_v2_25@4,192-ARR_v1_27@4,"To address this challenge, we propose a binary classifier called similarity evaluator to leverage the labeled source language data for similarity prediction.","To address this challenge, we propose a binary classifier called similarity teacher to leverage the labeled source language data for similarity prediction.","Modify,Clarity",Clarity
1969,192-ARR,192-ARR_v2_2@7,192-ARR_v1_2@7,Empirical studies on the three datasets across 7 different languages confirm the effectiveness of the proposed model.,Empirical studies on the datasets across 7 different languages confirm the effectiveness of the proposed model.,"Modify,Fact/Evidence",Fact/Evidence
1970,192-ARR,192-ARR_v2_25@5,192-ARR_v1_27@5,"Our similarity evaluator model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.","Our similarity teacher model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.","Modify,Clarity",Clarity
1971,192-ARR,192-ARR_v2_26@0,192-ARR_v1_28@0,Entity Recognizer,Entity Recognizer Teacher,"Modify,Other",Other
1972,192-ARR,192-ARR_v2_33@0,192-ARR_v1_35@0,"To leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.","In order to leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.","Modify,Clarity",Clarity
1973,192-ARR,192-ARR_v2_36@0,192-ARR_v1_38@0,"The inter-entities similarity is measured on the hidden representations h i and h j of the tokens queried by the entity indices < i, j > on the sequences representations.","The inter-entities similarity is measured on the tokens hidden representations h i and h j , queried by the entity indices < i, j > on the sequences representations.","Modify,Clarity",Clarity
1974,192-ARR,192-ARR_v2_4@1,192-ARR_v1_4@1,"The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) makes this task achieve significant performances.","The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) make this task achieves significant performances.","Modify,Grammar",Grammar
1975,192-ARR,192-ARR_v2_42@0,192-ARR_v1_44@0,Teacher-student Distillation Learning,Teacher Student Distillation Learning,"Modify,Grammar",Grammar
1976,192-ARR,192-ARR_v2_43@0,192-ARR_v1_45@0,"In this section, we consider transferring the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task.","In this section, we consider to transfer the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task.","Modify,Grammar",Grammar
1977,192-ARR,192-ARR_v2_44@0,192-ARR_v1_46@0,"The mBERT is also used as an encoder for the sentence siamese pair, and the entity token feature is queried from the latent sequence encoding representation.","The multi-lingual BERT is also used as encoder for the sentence siamese pair, and the entity token feature queried from the latent sequence encoding representation.","Modify,Clarity",Clarity
1978,192-ARR,192-ARR_v2_4@2,192-ARR_v1_4@2,"However, since deep neural networks highly rely on a large amount of labelled training data, the annotation acquiring process is expensive and time consuming.","However, since deep neural networks highly relies on a large amount of labelled training data, the annotation acquiring process is expensive and time consuming.","Modify,Grammar",Grammar
1979,192-ARR,192-ARR_v2_4@3,192-ARR_v1_4@3,This situation is more severe for zero-resource languages.,This situation is more severe for low-resource languages.,"Modify,Claim",Claim
1980,192-ARR,192-ARR_v2_2@0,192-ARR_v1_2@0,Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages.,Cross-lingual named entity recognition task is one of the critical problem for evaluating the potential transfer learning techniques on low resource languages.,"Modify,Grammar",Grammar
1981,192-ARR,192-ARR_v2_56@2,192-ARR_v1_57@3,"We set the batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014).","We set batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014).","Modify,Grammar",Grammar
1982,192-ARR,192-ARR_v2_62@1,192-ARR_v1_63@1,"Specifically, compared with the remarkable RIKD, AdvPicker, and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].","Specifically, compared with the remarkable RIKD, AdvPicker and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].","Modify,Grammar",Grammar
1983,192-ARR,192-ARR_v2_63@0,192-ARR_v1_64@0,Note that BERT-f performs better than our model on the Chinese dataset due to their re-tokenization of the dataset.,Note that Bert-f performs better than our model on Chinese dataset due to their re-tokenization of the dataset.,"Modify,Grammar",Grammar
1984,192-ARR,192-ARR_v2_66@1,192-ARR_v1_67@1,"That is, the teacher model has the same as the neural network structure of the student model.","That is, both of the teacher and student have the same neural network structure.","Modify,Clarity",Clarity
1985,192-ARR,192-ARR_v2_66@2,192-ARR_v1_67@2,This causes a performance drop across all languages due to two single teachers cannot make a difference with the combination.,This causes a performance drop across all languages due to two single teachers cannot make a difference with combination.,"Modify,Grammar",Grammar
1986,192-ARR,192-ARR_v2_67@0,192-ARR_v1_68@0,"(2) MTMT w/o weighting, which set the α (•) , β and γ all to be 1 in the loss of student learning.","(2) MTMT w/o weighting, which set the α 1 ,α 2 , β and γ all to be 1 in the loss of student model learning.","Modify,Fact/Evidence",Fact/Evidence
1987,192-ARR,192-ARR_v2_67@1,192-ARR_v1_68@1,"It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es), which validates that weighting loss can bring more confident knowledge to the student model.","It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es), which validates that weighting loss can bring more confident knowledge to student model.","Modify,Grammar",Grammar
1988,192-ARR,192-ARR_v2_68@1,192-ARR_v1_69@1,"In this case, our approach degrades into the single teacherstudent learning model as in TSL (Wu et al., 2020a).","In this case, our approach degrades into the Single Teacher-Student learning model as in TSL (Wu et al., 2020a).","Modify,Grammar",Grammar
1989,192-ARR,192-ARR_v2_71@1,192-ARR_v1_72@1,"Specifically, if there is a set of tokens in which every two of them have a high Entity Similarity score, and one of the tokens is predicted to have a distinct label while other tokens have identical labels, then the one with the distinct label is predicted wrongly and is corrected by the student model to have the label of all other tokens.","Specifically, if there is a set of tokens in which every two of them have high Entity Similarity score, and one of the tokens is predicted to have a distinct label while other tokens have identical labels, then the one with the distinct label is predicted wrongly and is corrected by the student model to have the label of all other tokens.","Modify,Grammar",Grammar
1990,192-ARR,192-ARR_v2_72@0,192-ARR_v1_73@0,Embedding Distribution,Embeddings Distribution,"Modify,Grammar",Grammar
1991,192-ARR,192-ARR_v2_73@1,192-ARR_v1_74@1,"It can be seen that the embedding distribution of the student model is close to similarity evaluator teacher, as illustrated in Figure 5.","It can be seen that the embeddings distribution of student model is close to similarity evaluator teacher, as illustrated in Figure 5.","Modify,Grammar",Grammar
1992,192-ARR,192-ARR_v2_5@0,192-ARR_v1_6@0,Many studies have been done to solve this crosslingual NER problem.,Many studies have been done to solve this crosslanguage NER problem.,"Modify,Grammar",Grammar
1993,192-ARR,192-ARR_v2_73@2,192-ARR_v1_74@2,"We conjecture that the student model captures similarity knowledge from the similarity evaluator teacher, i.e. the same class of examples tend to cluster and the different class of examples tend to segregate in the embedding distribution.","We conjecture that the student model captures similarity knowledge from the similarity evaluator teacher, i.e. the same class of examples tend to cluster and the different class of examples tend to segregate in the embeddings distribution.","Modify,Grammar",Grammar
1994,192-ARR,192-ARR_v2_75@0,192-ARR_v1_76@0,"In this section, we evaluate the effectiveness of weight loss in student learning from a quantitative perspective.","In the section, we evaluate the effectiveness of the weighting loss in student learning from quantitative perspective.","Modify,Grammar",Grammar
1995,192-ARR,192-ARR_v2_76@1,192-ARR_v1_77@1,"Therefore, the student model is better suited to the target language with learning fewer low-confidence misrecognitions for the target language.","Therefore, the student model is better suited to target language with learning less low-confidence misrecognitions for the target language.","Modify,Clarity",Clarity
1996,192-ARR,192-ARR_v2_77@1,192-ARR_v1_78@1,The encoder of the student model obtains the clustering information of the target language with the help of β.,The encoder of student model obtains the clustering information of the target language with the help of β.,"Modify,Grammar",Grammar
1997,192-ARR,192-ARR_v2_78@2,192-ARR_v1_79@2,"The student model learns less from unreasonable results, and it can make more accurate entity recognition for the target language.","The student model learns less from unreasonable results, and it can make more accuracy entity recognition for the target language.","Modify,Grammar",Grammar
1998,192-ARR,192-ARR_v2_80@2,192-ARR_v1_81@2,"Moreover, to guarantee the student learning performance, we also propose a weighting strategy to take into consideration the reliability of the teachers.","Moreover, in order to guarantee the student learning performance, we also propose a weighting strategy to take consideration of the reliability of the teachers.","Modify,Grammar",Grammar
1999,192-ARR,192-ARR_v2_5@2,192-ARR_v1_6@2,"Shared feature space based models exploit language-independent features, which lacks the domain-specific features for the target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).","Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).","Modify,Grammar",Grammar
2000,192-ARR,192-ARR_v2_6@0,192-ARR_v1_7@0,"Although the above-mentioned models solve the cross-lingual NER problem to some extent, the auxiliary tasks, as in multi-task learning, have not been studied in this problem.","Although above mentioned models solve the cross-lingual NER problem in some extent, the auxiliary tasks, as in the multi-task learning, have not been studied in this problem.","Modify,Grammar",Grammar
2001,192-ARR,192-ARR_v2_2@1,192-ARR_v1_2@1,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer.,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority.,"Modify,Clarity",Clarity
2002,192-ARR,192-ARR_v2_6@1,192-ARR_v1_7@1,"Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target languages.","Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target language.","Modify,Grammar",Grammar
2003,192-ARR,192-ARR_v2_7@1,192-ARR_v1_8@1,"Given a Spanish sentence ""Arévalo (Avila), 23 may (EFE)."", the token ""Arévalo"" is recognized as ORG type using the learned model from the English domain.","Given a Spanish sentence ""Arévalo (Avila), 23 may (EFE)."", the token ""Arévalo"" is recognized as ORG type using the learned model from English domain.","Modify,Grammar",Grammar
2004,192-ARR,192-ARR_v2_2@2,192-ARR_v1_2@2,"However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains.","However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domain.","Modify,Grammar",Grammar
2005,192-ARR,192-ARR_v2_9@0,192-ARR_v1_10@0,We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results show the superiority of our presented MTMT model.,We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results shows the superiority our presented MTMT model.,"Modify,Grammar",Grammar
2006,192-ARR,192-ARR_v2_13@0,192-ARR_v1_14@0,"Our approach is closely related to the existing works on cross-lingual NER, knowledge distillation, and siamese network.","Our approach is closely related to the existing works on cross-lingual NER, knowledge distillation and siamese network.","Modify,Grammar",Grammar
2007,192-ARR,192-ARR_v2_15@1,192-ARR_v1_16@1,"Recently, the pre-trained multilingual language model is effective to address the challenge (Devlin et al., 2019).","Recently, the pre-trained multilingual language models mBERT is effective to address the challenge (Devlin et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
2008,192-ARR,192-ARR_v2_15@2,192-ARR_v1_16@2,"Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from the labeled source language to that of target languages (Keung et al., 2019).","Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019).","Modify,Grammar",Grammar
2009,192-ARR,192-ARR_v2_16@0,192-ARR_v1_17@0,Translation based models generally generate pseudo-labeled target data to alleviate target data scarcity.,Translation based models generally generate pesudo labeled target data to alleviate target data scarcity.,"Modify,Grammar",Grammar
2010,192-ARR,192-ARR_v2_2@4,192-ARR_v1_2@4,"In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on the target domain.","In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on target domain.","Modify,Grammar",Grammar
2011,192-ARR,192-ARR_v2_16@1,192-ARR_v1_17@1,"For example, (Wu et al., 2020b;Zhang et al., 2021) gain an improvement by translating the labeled source language to the target language word-by-word.","For example, (Wu et al., 2020b;Zhang et al., 2021) gain a improvement by translating the labeled source language to the target language word-by-word.","Modify,Grammar",Grammar
2012,192-ARR,192-ARR_v2_17@0,192-ARR_v1_18@0,"Knowledge distillation based models include a teacher model and a student model (Wu et al., 2020c).","Knowledge distillation based models includes a teacher model and a student model (Wu et al., 2020c).","Modify,Grammar",Grammar
2013,192-ARR,192-ARR_v2_17@1,192-ARR_v1_18@1,The teacher model is trained on the labeled source language.,The teacher model is trained on labeled source language.,"Modify,Grammar",Grammar
2014,192-ARR,192-ARR_v2_17@2,192-ARR_v1_18@2,The student model learns from the soft label predicted by the teacher model on unlabeled target language data.,The student model learns from the soft label predicted by teacher model on unlabeled target language data.,"Modify,Grammar",Grammar
2015,192-ARR,192-ARR_v2_18@2,192-ARR_v1_19@2,"However, there is a dilemma to adapt the siamese network to tokenlevel recognition tasks such as NER.","However, there is a dilemma to adapt siamese network to token-level recognition tasks such as NER.","Modify,Grammar",Grammar
2016,192-ARR,192-ARR_v2_20@1,192-ARR_v1_21@1,Our framework is consist of two models: teacher training model learned from the source language and teacher-student distillation learning model learned from the target language.,Our framework is consist of two models: teacher training model learned from source language and teacher-student distillation learning model learned from target language.,"Modify,Grammar",Grammar
2017,193-ARR,,193-ARR_v1_28@3,,5 We provide additional analysis on spelling accuracy by subword frequency and length in Appendices B and C.,"Delete,Fact/Evidence",Fact/Evidence
2018,193-ARR,,193-ARR_v1_36@0,,We manually analyze 100 random tokens that SpellingBee spelled incorrectly with the lemma filter to understand the nature of the spelling mistakes.,"Delete,Fact/Evidence",Fact/Evidence
2019,193-ARR,,193-ARR_v1_37@0,,Out of those 100 we display 20 mistakes in Table 3 alongside the spelling prediction of the control baseline.,"Delete,Fact/Evidence",Fact/Evidence
2020,193-ARR,,193-ARR_v1_38@0,,"We implement SpellingBee with a 6-layer encoderdecoder model, with 512 model dimensions.","Delete,Fact/Evidence",Fact/Evidence
2021,193-ARR,,193-ARR_v1_39@0,,"The model parameters are optimized with Adam (Kingma and Ba, 2015) for 1000 steps with up to 1024 tokens per batch, a learning rate of 5e-4, and a dropout rate of 0.1.","Delete,Fact/Evidence",Fact/Evidence
2022,193-ARR,,193-ARR_v1_39@1,,These are the default hyperparameters for training a transformer language model in Fairseq .,"Delete,Fact/Evidence",Fact/Evidence
2023,193-ARR,193-ARR_v2_25@0,,Character-Aware Models Some models are provided with the raw character sequence of each token.,,"Add,Fact/Evidence",Fact/Evidence
2024,193-ARR,193-ARR_v2_25@2,,"However, when no filter is applied, RoBERTa's character-oblivious but highlytuned training process produces embeddings that score higher on SpellingBee, presumably by leveraging implicit similarity functions in the embedding space.",,"Add,Claim",Claim
2025,193-ARR,193-ARR_v2_26@0,,"Although CharacterBERT's embedding layer is better at reconstructing original words (when similarity filters are applied), this does not mean that character-aware models are necessarily better downstream.",,"Add,Claim",Claim
2026,193-ARR,193-ARR_v2_26@1,,El Boukkouri et al. (2020) report performance increases only on the medical domain.,,"Add,Fact/Evidence",Fact/Evidence
2027,193-ARR,193-ARR_v2_27@0,,"In Section 5, we demonstrate that initializing a masked language model's embedding layer with character information has a negligible effect on its perplexity.",,"Add,Fact/Evidence",Fact/Evidence
2028,193-ARR,193-ARR_v2_28@0,,"The first generation of neural word representations (Mikolov et al., 2013a,b) contained only embedding layers, without any contextualization mechanism.",,"Add,Fact/Evidence",Fact/Evidence
2029,193-ARR,193-ARR_v2_28@1,,"We thus use GloVe (Pennington et al., 2014) to estimate a lower bound on character information that can be obtained by simple context-oblivious models.",,"Add,Fact/Evidence",Fact/Evidence
2030,193-ARR,193-ARR_v2_28@2,,We probe the first 50K words in GloVe's vocabulary with SpellingBee.,,"Add,Fact/Evidence",Fact/Evidence
2031,193-ARR,193-ARR_v2_28@3,,"Table 2 shows that GloVe embeddings do contain a weak orthographic signal, better than random embeddings, but substantially weaker than the information stored in the embedding layer of large transformer-based language models.",,"Add,Fact/Evidence",Fact/Evidence
2032,193-ARR,193-ARR_v2_32@0,,"Although there are many possible ways to explicitly add orthographic information to tokens embeddings, our method is relatively straightforward as it gives the model a chance to utilize pre-stored character information.",,"Add,Claim",Claim
2033,193-ARR,193-ARR_v2_2@4,193-ARR_v1_2@4,"Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks.","Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not enhance its performance on such tasks.","Modify,Clarity",Clarity
2034,193-ARR,193-ARR_v2_23@2,193-ARR_v1_25@2,"One possibility is that Arabic's rich morphology incentivizes the model to store more information about each token's character composition; however, it is also possible that AraBERT's different vocabulary, which allocates shorter character sequences to each token type, might explain this difference (we discuss the link between sequence length and accuracy in Appendix C).","One possibility is that Arabic's rich morphology incentivizes the model to store more information about each token's character composition, however it is also possible that AraBERT's different vocabulary, which allocates shorter character sequences to each token type, might explain this difference (we discuss the link between sequence length and accuracy later in this section).","Modify,Fact/Evidence",Fact/Evidence
2035,193-ARR,193-ARR_v2_30@4,193-ARR_v1_29@1,"8 We train RoBERTa-Large on English Wikipedia using the hyperparameter configuration of 24hBERT (Izsak et al., 2021), and cease training after 24 hours (approximately 16,000 steps).","6 We train RoBERTa-Large on English Wikipedia using the hyperparameter configuration of 24hBERT (Izsak et al., 2021), and cease training after 24 hours (∼16000 steps).","Modify,Clarity",Clarity
2036,193-ARR,193-ARR_v2_31@3,193-ARR_v1_30@3,"This result indicates that in this scenario, the model does not utilize the character information injected into the tokens' embeddings.",This result indicates that the model does not utilize the character information injected into the tokens' embeddings.,"Modify,Clarity",Clarity
2037,193-ARR,193-ARR_v2_32@1,193-ARR_v1_30@4,"Along with the results from Section 4, we hypothesize that the implicit notion of spelling that the model learns during pretraining might be sufficient for masked language modeling.","Along with the results from Section 4, we conjecture that the model learns an implicit notion of spelling during pretraining, which is sufficient for masked language modeling, and does not benefit from explicitly adding orthographic information.","Modify,Claim",Claim
2038,193-ARR,193-ARR_v2_7@1,193-ARR_v1_7@1,"We compare the pretraining process of the character-infused model to that of an identical model whose embedding layer is randomly initialized (and not pretrained), and find that both learning curves converge to virtually identical values within the first 1,000 gradient updates, a fraction of the total optimization process.","We compare the pretraining process of the character-infused model to that of an identical model whose embedding layer is randomly initialized (and not pretrained), and find that both learning curves converge to virtually identical values within the first 1000 gradient updates, a fraction of the total optimization process.","Modify,Grammar",Grammar
2039,193-ARR,193-ARR_v2_7@2,193-ARR_v1_7@2,"This experiment suggests that while language models may need to learn some notion of spelling to optimize their objectives, they might also be able to quickly acquire most of the character-level information they need from plain token sequences without directly observing the composition of each token.","This experiment suggests that while language models may need to learn some notion of spelling to optimize their objectives, they can quickly acquire all the character-level information they need without directly observing the composition of each token.","Modify,Claim",Claim
2040,193-ARR,193-ARR_v2_2@2,193-ARR_v1_2@2,Our results show that the embedding layers of RoBERTa and GPT2 each hold enough information to accurately spell up to a third of the vocabulary and reach high character ngram overlap across all token types.,Our results show that the embedding layer of RoBERTa holds enough information to accurately spell up to a third of the vocabulary and reach high average character ngram overlap on all token types.,"Modify,Fact/Evidence",Fact/Evidence
2041,193-ARR,193-ARR_v2_2@3,193-ARR_v1_2@3,"We further test whether enriching subword models with character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment.","We further test whether enriching subword models with additional character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment.","Modify,Clarity",Clarity
2042,193-ARR,193-ARR_v2_21@4,193-ARR_v1_23@3,"These results are persistent across different models and filters, strongly indicating that the embedding layer of pretrained models contains significant amounts of information about each token's character composition.","These results are persistent across different models and filters, strongly indicating that the embedding layer of pretrained models contain significant amounts of information about each token's character composition.","Modify,Grammar",Grammar
2043,193-ARR,193-ARR_v2_22@0,193-ARR_v1_24@0,"One may suggest that training SpellingBee over 32,000 examples may leak information from the test set; for example, if dog was seen during training, then spelling out dogs might be easy.",One may suggest that training SpellingBee over 32000 examples may leak information from the test set.,"Merge+Modify,Grammar",Grammar
2045,195-ARR,195-ARR_v2_60@2,,The final version of the annotation guidelines is included in the resource release (ict.fbk.eu/must-she) and is also retrievable at: https://bit.ly/3CdU50s.,,"Add,Fact/Evidence",Fact/Evidence
2046,195-ARR,195-ARR_v2_68@8,,Figure 6 shows coverage of fully generated agreement chains split into feminine (F) and masculine (M) forms.,,"Add,Fact/Evidence",Fact/Evidence
2047,195-ARR,,195-ARR_v1_26@0,,Overall quality and gender translation,"Delete,Other",Other
2048,195-ARR,,195-ARR_v1_27@0,,"Table 3 presents SacreBLEU (Post, 2018), 14 coverage, and gender accuracy scores on the MuST-SHE test sets.","Delete,Fact/Evidence",Fact/Evidence
2049,195-ARR,,195-ARR_v1_27@2,,"Also, in line with previous analyses , SMALL-BPE models outperform the CHAR ones by ∼1 BLEU point.","Delete,Fact/Evidence",Fact/Evidence
2050,195-ARR,,195-ARR_v1_27@3,,"The higher overall translation quality of LARGE-BPE models is also reflected by the coverage scores (All-Cov), where 13 The scripts will be released upon paper acceptance.","Delete,Fact/Evidence",Fact/Evidence
2051,195-ARR,,195-ARR_v1_37@7,,18 We noticed that CHAR's lower translation quality may have to do more with fluency rather than lexical issues.,"Delete,Claim",Claim
2052,195-ARR,,195-ARR_v1_52@0,,A MuST-SHE Manual Annotation POS and agreement chains annotations were carried out on MuST-SHE reference translations.,"Delete,Fact/Evidence",Fact/Evidence
2053,195-ARR,,195-ARR_v1_52@1,,"To ensure precision, the two layers of linguistic information have been added i) in the course of two separate annotation processes; ii) following strict and comprehensive guidelines.","Delete,Fact/Evidence",Fact/Evidence
2054,195-ARR,,195-ARR_v1_62@0,,"Considering long-range dependencies that go beyond the phrase level, a gender relevant covariation is also that of subject-verb agreement, as the one shown in Table 9 (see also footnote 1).","Delete,Fact/Evidence",Fact/Evidence
2055,195-ARR,,195-ARR_v1_63@0,,"To account for such longer spans, we considered all MuST-SHE sentences where both i) a word (or chain) functioning as a subject, and ii) its referring verb or predicative complement are annotated as gender-marked words in the references.","Delete,Fact/Evidence",Fact/Evidence
2056,195-ARR,,195-ARR_v1_63@1,,"We identified 55 sentences for en-es, 54 for en-fr and 41 for en-it, and we manually analyzed all the corresponding systems' outputs.","Delete,Fact/Evidence",Fact/Evidence
2057,195-ARR,,195-ARR_v1_64@0,,"We found that, even in the case of dependencies within a longer range, systems largely respect agreement in translation and consistently pick the same gender form for all co-related words.","Delete,Fact/Evidence",Fact/Evidence
2058,195-ARR,,195-ARR_v1_64@2,,"Among these 4 cases, we found the above discussed weaker gender-enforcing structures (see the description of (semi-)copula verbs and their predicative complements in §6.2), and we also detected what resembles agreement attraction errors (Linzen et al., 2016).","Delete,Fact/Evidence",Fact/Evidence
2059,195-ARR,,195-ARR_v1_64@4,,"The following (long) sentence is an example of such an attraction error, where the complement desperate is inflected in the same masculine and plural form as its just preceding noun rather than the chain functioning as subject (the nurse).","Delete,Fact/Evidence",Fact/Evidence
2060,195-ARR,,195-ARR_v1_65@1,,"We can thus conclude that, even taking into account longer dependencies, agreement still does not emerge as an issue entrenched with gender bias.","Delete,Claim",Claim
2061,195-ARR,195-ARR_v2_6@1,,"In fact, to be grammatically correct, each word in the chain has to be inflected with the same (masculine or feminine) gender form.",,"Add,Claim",Claim
2062,195-ARR,195-ARR_v2_17@1,,"As further discussed in in Section 4.2, such a feature enables fine-grained analyses of gender realization, which can also disentangle systems' tendency to (over)generate masculine -over feminine forms -in translation.",,"Add,Claim",Claim
2063,195-ARR,195-ARR_v2_32@4,,"Accordingly, we manage to make our study completely exhaustive by covering every gendermarked instance of MuST-SHE.",,"Add,Fact/Evidence",Fact/Evidence
2064,195-ARR,195-ARR_v2_32@5,,"Also, such additional manual evaluation serves as a proof-ofconcept to ensure the validity of the employed automatic evaluation metrics. .",,"Add,Claim",Claim
2065,195-ARR,195-ARR_v2_40@4,,"Also, note that the outcome of the manual analyses reiterates the results obtained with the automatic evaluation based on accuracy at the word-level, thus confirming its reliability.",,"Add,Fact/Evidence",Fact/Evidence
2066,195-ARR,195-ARR_v2_18@0,195-ARR_v1_12@4,MuST-SHE thus allows the identification and pinpointed evaluation of numerous and qualitatively different grammatical gender instances under authentic conditions.,"Thus, MuST-SHE allows the identification of numerous and qualitatively different grammatical gender instances under authentic conditions.","Modify,Fact/Evidence",Fact/Evidence
2067,195-ARR,195-ARR_v2_18@2,195-ARR_v1_12@6,"As a matter of fact, as Gygax et al. (2019) suggest, accounting for gender in languages with similar typological features allows for proper comparisons.","In fact, as Gygax et al. (2019) suggest, accounting for gender in languages with similar typological features allows for proper comparison.","Modify,Clarity",Clarity
2068,195-ARR,195-ARR_v2_2@5,195-ARR_v1_2@5,"By shedding light on model behaviours, gender bias, and its detection at several levels of granularity, our findings emphasize the value of dedicated analyses beyond aggregated overall results.","Our work sheds light on model behaviours, gender bias, and its detection at several levels of granularity for English-French/Italian/Spanish.","Modify,Claim",Claim
2069,195-ARR,195-ARR_v2_20@2,195-ARR_v1_14@2,"As shown in Table 1 (a-c), we differentiate among six POS categories: 7 i) articles, ii) pronouns, iii) nouns, and iv) verbs.","As shown in Table 1 (a-c), we differentiate among six POS categories: 6 i) articles, ii) pronouns, iii) nouns, iv) verbs.","Modify,Grammar",Grammar
2070,195-ARR,195-ARR_v2_21@5,195-ARR_v1_15@5,"9 This annotation lets us verify whether a model consistently picks the same gender paradigm for all words in the chain, enabling the assessment of its syntagmatic behaviour.","8 This annotation let us verify whether a model consistently picks the same gender paradigm for all words in the chain, enabling the assessment of its syntagmatic behaviour.","Modify,Grammar",Grammar
2071,195-ARR,195-ARR_v2_23@1,195-ARR_v1_17@1,"For each language pair, they annotated the whole corpus independently, based on detailed guidelines (see Appendix A).","For each language pair, they annotated the whole corpus independently, based on detailed guidelines (see Appendix §A).","Modify,Grammar",Grammar
2072,195-ARR,195-ARR_v2_26@0,195-ARR_v1_21@0,Speech Translation models,Speech Translation models.,"Modify,Grammar",Grammar
2073,195-ARR,195-ARR_v2_27@2,195-ARR_v1_21@3,"Working on WinoMT/ST, Kocmi et al. (2020) correlate higher BLEU scores and gender stereotyping, whereas Costa-jussà et al. (2020) show that systems with lower performance tend to produce fewer feminine translations for occupations, but rely less on stereotypical cues.","Working on WinoMT/ST, (Kocmi et al., 2020) correlates higher BLEU scores and gender stereotyping, whereas (Costa-jussà et al., 2020a) shows that systems with lower performance tend to produce fewer feminine translations for occupations, but rely less on stereotypical cues.","Modify,Fact/Evidence",Fact/Evidence
2074,195-ARR,195-ARR_v2_29@4,195-ARR_v1_22@7,"Thus, for the sake of comparison with , we train these systems in the same (controlled) data conditions i.e. on the MuST-C corpus only.","Thus, for the sake of comparison with , we train these systems in the same controlled data environment, i.e. on the MuST-C corpus only.","Modify,Clarity",Clarity
2075,195-ARR,195-ARR_v2_30@0,195-ARR_v1_23@0,Evaluation method,Evaluation method.,"Modify,Grammar",Grammar
2076,195-ARR,195-ARR_v2_32@0,195-ARR_v1_24@0,"For all word-level gender evaluations (Sections 5.1 and 5.2), we compute accuracy as in the official MuST-SHE script and include scores based on the POS annotations.","For all word-level gender evaluations ( §5.1 and §5.2), we compute accuracy as in the official MuST-SHE script, while for chain-level gender agreement evaluation ( §6.1) we modify it to process full agreement chains instead of single words.","Split+Modify,Fact/Evidence",Fact/Evidence
2077,195-ARR,195-ARR_v2_32@1,195-ARR_v1_24@0,"Instead, for chain-level gender agreement evaluation (Section 6.1) we modified the original script to process full agreement chains instead of single words.","For all word-level gender evaluations ( §5.1 and §5.2), we compute accuracy as in the official MuST-SHE script, while for chain-level gender agreement evaluation ( §6.1) we modify it to process full agreement chains instead of single words.","Split+Modify,Clarity",Clarity
2078,195-ARR,195-ARR_v2_32@2,195-ARR_v1_24@1,"14 Finally, since we aim at gaining qualitative insights into systems' behaviour, and at ensuring a sound and thorough multifaceted evaluation, we overcome the described coverage limitation of the automatic evaluation by complementing it with a manual analysis of all the gender-marked words and agreement chains that remained out of coverage.","13 Finally, since we aim at gaining exhaustive, qualitative insights into systems' behaviour, and at ensuring a sound and thorough multifaceted evaluation, we overcome the described coverage limitation of the automatic evaluation by complementing it with a manual analysis of all the words and agreement chains that remained out of coverage.","Modify,Fact/Evidence",Fact/Evidence
2079,195-ARR,195-ARR_v2_32@3,195-ARR_v1_24@2,"This extensive manual evaluation was carried out via a systematic annotation of systems' outputs, performed by the same linguists that enriched MuST-SHE, who provided the appropriate knowledge of both the resource and the evaluation task.","This extensive manual evaluation was carried out via a systematic annotation of systems' outputs, performed by the same linguists that enriched MuST-SHE, who provided the appropriate knowledge of both resource and evaluation tasks.","Modify,Grammar",Grammar
2080,195-ARR,195-ARR_v2_36@0,195-ARR_v1_31@0,"At a finer level of granularity, we use our extension of MuST-SHE to inspect gender bias across open and closed class words.","At a finer level of granularity, we use MuST-SHE extension to inspect gender bias across open and closed class words.","Modify,Clarity",Clarity
2081,195-ARR,195-ARR_v2_37@4,195-ARR_v1_33@5,"As discussed in Section 8, we believe that our breakdown per POS is informative inasmuch it prompts qualitative considerations on how to pursue gender bias mitigation in models and corpora (Czarnowska et al., 2021;Doughman et al., 2021).","As discussed in §8, we believe that our breakdown per POS is informative inasmuch it prompts qualitative considerations on how to pursue gender bias mitigation in models and corpora (Czarnowska et al., 2021;Doughman et al., 2021).","Modify,Clarity",Clarity
2082,195-ARR,195-ARR_v2_49@1,195-ARR_v1_42@1,"Similarly to the approach employed for single words (Section 5.3), we discern between OOC chains due to: i) translation errors (Err), and ii) alternative translations preserving the source meaning.","Similarly to the approach employed for single words ( §5.3), we discern between OOC chains due to: i) translation errors (Err), and ii) alternative translations preserving the source meaning.","Modify,Clarity",Clarity
2083,195-ARR,195-ARR_v2_49@4,195-ARR_v1_42@4,"Instead, when the generated alternative chain exhibits gender markings, we distinguish if the chosen gender is correct (C), wrong (W), or if the system produces a chain that does not respect gender agreement because it combines both feminine and masculine gender inflections (NO).","Instead, when the generated alternative chain exhibits gender markings, we distinguish if the chosen gender is correct (C), wrong (W), or if the system produces a chain that does not respect gender agreement, because it combines both feminine and masculine gender inflections (NO).","Modify,Grammar",Grammar
2084,195-ARR,195-ARR_v2_50@1,195-ARR_v1_43@1,"Interestingly, such results are only partially corroborating previous analyses.","Interestingly, such results are only partially corroborating analyses.","Modify,Clarity",Clarity
2085,195-ARR,195-ARR_v2_51@0,195-ARR_v1_43@2,"On the one hand, unlike the OOC words' results discussed in Section 5.3, we attest that CHAR models produce the highest proportion of translation errors.","On the one hand, unlike the OOC words' results discussed in §5.3, we attest that CHAR models produce the highest proportion of translation errors.","Modify,Clarity",Clarity
2086,195-ARR,195-ARR_v2_51@3,195-ARR_v1_44@0,"Finally, again in line with our automatic evaluation (Table 6), we confirm that respecting agreement is not an issue for our ST models: we identify only 3 cases (2 for en-fr BPE, 1 for en-fr CHAR) where concord is broken (NO).","Finally, in line with our automatic evaluation (Table 6), we confirm that respecting agreement is not an issue for our ST models: we identify only 3 cases (2 for en-fr BPE, 1 for en-fr CHAR) where concord is broken (NO).","Modify,Clarity",Clarity
2087,195-ARR,195-ARR_v2_51@7,195-ARR_v1_44@4,"However, the most common type among these outliers are constructions with semi-copula verbs (e.g. en: She... [became a vet]; it: ...E' [diventata F un M veterinatrio M ]), which -as discussed in Section 3.1 -exhibit a weaker agreement constraint.","However, the most common type among these outliers are constructions with semi-copula verbs (e.g. en: She... [became a vet]; it: ...E' [diventata F un M veterinatrio M ]), which -as discussed in §3.1 -exhibit a weaker agreement constraint.","Modify,Clarity",Clarity
2088,195-ARR,195-ARR_v2_53@3,195-ARR_v1_46@3,"On three language pairs (English-French/Italian/Spanish), our study shows that, while all POS are subject to masculine skews, they are not impacted to the same extent.","On 3 language pairs (en-es/fr/it) our study shows that, while all POS are subject to masculine skews, they are not impacted to the same extent.","Modify,Grammar",Grammar
2089,195-ARR,195-ARR_v2_54@1,195-ARR_v1_46@7,"Accordingly, our results are in line with previous studies showing that, in spite of lower generic performance, character-based segmentation exhibits a better capability at handling feminine translation at different levels of granularity.","Accordingly, our results are in line with previous studies showing that, in spite of lower generic performance, character-based segmentation favours feminine translation at different levels of granularity.","Modify,Clarity",Clarity
2090,195-ARR,195-ARR_v2_56@3,195-ARR_v1_48@3,"Specifically, our investigation on the relation between data size/segmentation technique and gender bias provides initial cues on which models and components to audit and implement toward the goal of reducing gender bias.","Specifically, our investigation on the relation between model size/segmentation technique and gender bias provides initial cues on which models and components to audit and implement toward the goal of reducing gender bias.","Modify,Clarity",Clarity
2091,195-ARR,195-ARR_v2_56@6,195-ARR_v1_48@6,"In fact, while it is known that the MuST-C corpus (Cattoni et al., 2021) used for training comprises a majority of masculine speakers, 21 the fact that certain lexical categories are more biased than others suggests that, on top of more coarse-grained quantitative attempts at gender balancing (Costa-jussà and de Jorge, 2020), data curation ought to account for more sensitive, nuanced, and qualitative asymmetries.","In fact, while it is known that the MuST-C corpus used for training comprises a majority of masculine speakers, 20 the fact that certain lexical categories are more biased than others suggests that, on top of more coarse-grained quantitative attempts at gender balancing (Costa-jussà and de Jorge, 2020), data curation ought to account for more sensitive, nuanced, and qualitative asymmetries.","Modify,Fact/Evidence",Fact/Evidence
2092,195-ARR,195-ARR_v2_7@1,195-ARR_v1_6@1,"2 (2) In light of recent studies exploring how model design and overall perfomance interplay with gender bias (Roberts et al., 2020;, we rely on our manually curated resource to compare three ST models, which are trained on varying amounts of data, and built with different segmentation techniques: character and byte-pairencoding (BPE) (Sennrich et al., 2016).","2 (2) In light of recent studies exploring how model design and overall perfomance interplay with gender bias (Roberts et al., 2020;, we rely on our manually curated resource to compare three ST models, which are trained on varying amounts of data, and built with different segmentation techniques: character and bytepair-encoding (BPE) (Sennrich et al., 2016).","Modify,Grammar",Grammar
2093,195-ARR,195-ARR_v2_58@0,195-ARR_v1_50@0,"While our experiments are limited to the binary linguistic forms represented in the used data, to the best of our knowledge, ST natural language corpora going beyond binarism do not yet exist.","While our experiments are largely limited to the binary linguistic forms represented in the used data, to the best of our knowledge, ST natural language corpora going beyond binarism do not yet exist.","Modify,Other",Other
2094,195-ARR,195-ARR_v2_2@1,195-ARR_v1_2@1,"However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions.","However, most evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions.","Modify,Clarity",Clarity
2095,195-ARR,195-ARR_v2_60@1,195-ARR_v1_53@1,"Successively, such guidelines have been refined and improved by means of discussions with the annotators, who had carried out a pilot annotation round to get acquainted with MuST-SHE language data.","Successively, such guidelines have been refined and improved by means of discussions with the annotators, who had carried out a trail annotation round to get acquainted with MuST-SHE language data.","Modify,Clarity",Clarity
2096,195-ARR,195-ARR_v2_61@1,195-ARR_v1_54@1,"We ensured that at least one annotator per language was a native speaker, whereas the second one had at least a C1 proficiency level of the assigned language.",We ensured that at least one annotator per language was a native speaker.,"Modify,Fact/Evidence",Fact/Evidence
2097,195-ARR,195-ARR_v2_62@0,195-ARR_v1_55@0,"In this section we describe in detail the ST models created for our study, whose source code is publicly released at: https://github.com/ mgaido91/FBK-fairseq-ST/tree/acl_2021.","In this section we describe in detail the ST models created for our study, whose source code will be publicly released upon acceptance of the paper.","Modify,Fact/Evidence",Fact/Evidence
2098,195-ARR,195-ARR_v2_64@0,195-ARR_v1_57@0,"Since the amount of ST data is limited (i.e. MuST-C - Cattoni et al. 2021-and Europarl-ST -Iranzo-Sánchez et al. 2020, knowledge transfer from the ASR and MT tasks is leveraged by respectively initializing the ST encoder with ASR pretrained weights (Weiss et al., 2017b;Bansal et al., 2019) and by distilling knowledge from a strong MT teacher (Liu et al., 2019).","Since the amount of ST data is limited (i.e. MuST-C -Cattoni et al. 2020-and Europarl-ST -Iranzo-Sánchez et al. 2020, knowledge transfer from the ASR and MT tasks is leveraged by respectively initializing the ST encoder with ASR pretrained weights (Weiss et al., 2017b;Bansal et al., 2019) and by distilling knowledge from a strong MT teacher (Liu et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
2099,195-ARR,195-ARR_v2_8@1,195-ARR_v1_6@3,"Finally, in line with concurring studies, we find that iv) character-based systems have an edge on translating gender phenomena, by favouring morphological and lexical diversity.","Finally, in line with concurring studies, we find that iv) character-based systems favour morphological and lexical diversity when translating gender phenomena.","Modify,Clarity",Clarity
2100,195-ARR,195-ARR_v2_10@1,195-ARR_v1_8@1,"Along this line, focus has been given to bias analysis in models' innards and outputs (Vig et al., 2020;Costajussà et al., 2022), and to ascertain the validity of bias measurement practices (Blodgett et al., 2021;Antoniak and Mimno, 2021;Goldfarb-Tarrant et al., 2021).","Along this line, focus has been given to bias analysis in models' innards and ouputs (Vig et al., 2020;Costajussà et al., 2020b), and to ascertain the validity of bias measurament practices (Blodgett et al., 2021;Antoniak and Mimno, 2021;Goldfarb-Tarrant et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
2101,195-ARR,195-ARR_v2_10@2,195-ARR_v1_8@2,"Complementary mounting evidence suggests that -rather than striving for generalizations -gender bias detection ought to incorporate contextual and linguistic specificity (González et al., 2020;Ciora et al., 2021;Matthews et al., 2021;Malik et al., 2021;Kurpicz-Briki and Leoni, 2021), which however receives little attention due to a heavy focus on English NLP (Bender and Friedman, 2018).","Complementary evidence suggests thatrather than striving for generalizations -gender bias detection ought to incorporate contextual and linguistic specificity (González et al., 2020;Ciora et al., 2021;Matthews et al., 2021;Malik et al., 2021;Kurpicz-Briki and Leoni, 2021), which however receives little attention due to a heavy focus on English NLP.","Modify,Fact/Evidence",Fact/Evidence
2102,195-ARR,195-ARR_v2_10@3,195-ARR_v1_8@3,"Purported agnostic approaches and evaluations (Bender, 2009) can prevent from drawing reliable conclusions and mitigating recommendations, as attested by monolingual studies on grammatical gender languages (Zhou et al., 2019;Gonen et al., 2019;Zmigrod et al., 2019) and in automatic translation scenarios (Vanmassenhove et al., 2018;Moryossef et al., 2019).","Purported agnostic approaches and evaluations (Bender, 2009) can prevent from drawing reliable conclusions and mitigating recommendations, as attested by monolingual studies on grammatical gender languages (Zhou et al., 2019;Gonen et al., 2019) and in automatic translation scenarios (Vanmassenhove et al., 2018;Moryossef et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
2103,195-ARR,195-ARR_v2_11@0,195-ARR_v1_8@4,"Unlike English, grammatical gender languages exhibit an elaborate morphological and syntactic system, where gender is overtly marked on numerous POS (e.g., verbs, determiners, nouns), and related words have to agree on the same gender features (see Figure 1 for an example).","Unlike English, grammatical gender languages exhibit an elaborate morphological and syntactic system, where gender is overtly marked on numerous POS (e.g., verbs, determiners, nouns), and related words have to agree on the same gender features.","Modify,Fact/Evidence",Fact/Evidence
2104,195-ARR,195-ARR_v2_12@1,195-ARR_v1_9@1,"It has been progressively enriched with new features Kocmi et al., 2020), and adapted for ST (Costa-jussà et al., 2020).","It has been progressively enriched with new features Kocmi et al., 2020), and adapted for ST (Costa-jussà et al., 2020a).","Modify,Fact/Evidence",Fact/Evidence
2105,195-ARR,195-ARR_v2_12@5,195-ARR_v1_9@5,"However, gender-marking involves also several other, so far less accounted POS categories, but if they are just as problematic is not clear yet.","However, gender-marking involves also several other, so far less accounted POS categories, but whether they are just as problematic is not clear yet.","Modify,Clarity",Clarity
2106,195-ARR,195-ARR_v2_13@1,195-ARR_v1_10@1,"As a result, they maximize lexical and contextual variability to inspect whether translation models yield feminine under-representation in real-world-like scenarios .","As a result, they maximize variability to inspect whether translation models yield feminine under-representation in real-world-like scenarios .","Modify,Fact/Evidence",Fact/Evidence
2107,195-ARR,195-ARR_v2_2@4,195-ARR_v1_2@4,"Focusing on speech translation, we conduct a multifaceted evaluation on three language directions (English-French/Italian/Spanish), with models trained on varying amounts of data and different word segmentation techniques.","On this basis, we conduct multifaceted automatic and manual evaluations for three speech translation models, trained on varying amounts of data and different word segmentation techniques.","Modify,Fact/Evidence",Fact/Evidence
2108,195-ARR,195-ARR_v2_14@3,195-ARR_v1_10@7,"Closer to our intent, analyze the output of different ST systems and note that their models seem to wrongly pick divergent gender inflections for unrelated words in the same sentence (e.g. en: As a researcher, professor; fr: En tant que chercheuse F , professeur M ) but not for dependency-related ones (e.g. en: The classic Asian student; it: [La classica studentessa asiatica] F ).","Closer to our intent, analyze the output of different ST systems and note that their models seem to wrongly pick divergent gender inflections for unrelated words in the same sentence (e.g. fr: En tant que chercheuse F , professeur M ) but not for dependency-related ones (e.g. it: [la classica studentessa asiatica] F ).","Modify,Fact/Evidence",Fact/Evidence
2109,195-ARR,195-ARR_v2_16@1,195-ARR_v1_12@1,"Rather than building it from scratch, we add two annotation layers to the existing MuST-SHE bench- mark , which is built on spoken language data retrieved from TED talks.","Rather than building it from scratch, we add two annotation layers to the existing TED-based MuST-SHE benchmark .","Modify,Clarity",Clarity
2110,195-ARR,195-ARR_v2_16@2,195-ARR_v1_12@2,"Available for en-es/fr/it, it represents the only multilingual MT and ST GBET 5 exhibiting a natural variety of gender phenomena, which are balanced across feminine and masculine forms.","4 Available for en-es/fr/it, it represents the only multilingual MT/ST GBET 5 exhibiting a natural variety of gender phenomena.","Modify,Fact/Evidence",Fact/Evidence
2111,195-ARR,195-ARR_v2_17@0,195-ARR_v1_12@3,"In the reference translations of the corpus, each target gender-marked word -corresponding to a neutral expression in the English source -is annotated with its alternative wrong gender form (e.g. en: the girl left; it: la<il> ragazza è an-data<andato> via).","In the reference translations, each gender-marked word -corresponding to a neutral expression in the source -is annotated with its alternative wrong gender form (e.g. en: the girl left; it: la<il> ragazza è andata<andato> via).","Modify,Clarity",Clarity
2296,207-ARR,207-ARR_v2_2@5,207-ARR_v1_2@5,"We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracy on downstream tasks (e.g. GLUE).","We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE).","Modify,Grammar",Grammar
2297,207-ARR,207-ARR_v2_30@3,207-ARR_v1_30@3,Hoory et al. (2021a) improved on these bounds by using Gaussian noise.,Hoory et al. (2021b) improved on these bounds by using Gaussian noise.,"Modify,Fact/Evidence",Fact/Evidence
2298,207-ARR,207-ARR_v2_46@3,207-ARR_v1_44@2,"For DP-SP we use the unigram Sentence-Piece algorithm (Kudo and Richardson, 2018).","For DP-SP we use the popular unigram SentencePiece algorithm (Kudo and Richardson, 2018).","Modify,Clarity",Clarity
2299,207-ARR,207-ARR_v2_58@2,207-ARR_v1_57@2,"Even for the most stringent guarantees of DP-Train ( of 6.06) which result in approx 20% of pretrain accuracy drop, on average GLUE fine-tuned performance is not significantly different from the baseline.","Even for the most stringent guarantees of DP-Train ( of 6.06) which result in approx 20% of pretrain accuracy drop, on average glue fine-tuned performance is not significantly different from the baseline.","Modify,Grammar",Grammar
2300,207-ARR,207-ARR_v2_58@4,207-ARR_v1_57@4,"On average, we also see that full-DP is not significantly better on subsequent fine-tuning tasks (e.g. mnli_m, mnli_msm, qnli etc), however for some tasks (e.g. cola) fine-tuning performance is significantly better than that of a (non-private) baseline.","On average, We also see that full-DP is not significantly better on subsequent fine-tuning tasks (e.g. mnli_m, mnli_msm, qnli etc), however for some tasks (e.g. cola) fine-tuning performance is significantly better than that of a (non-private) baseline.","Modify,Grammar",Grammar
2301,207-ARR,207-ARR_v2_58@5,207-ARR_v1_57@5,"On average, even DP-train models have approximately the same GLUE performance (difference insignificant).","On average, even DP-train models have approximately the same GLUE-fine tuning performance (the difference is insignificant).","Modify,Clarity",Clarity
2302,207-ARR,207-ARR_v2_62@0,207-ARR_v1_60@0,"One important caveat here is that the span corruption training objective was splitting a piece of text into input/target randomly, so it is possible a different definition of memorization would be more suitable.","One important caveat here is that the span corruption training objective was splitting a piece of text into input/target randomly, so it is possible a different definition of memorization would be more suitable here.","Modify,Clarity",Clarity
2303,207-ARR,207-ARR_v2_63@8,207-ARR_v1_62@4,For a large this protective effect is almost non-existent.,For a large this protective effect is almost non-existant.,"Modify,Grammar",Grammar
2304,207-ARR,207-ARR_v2_68@0,207-ARR_v1_67@0,"Theorem 1 (Hoory et al., 2021b) uses the notion of k and m (maximum L2 and infinity norms) of sentence-level (1-D vector) histogram.","Theorem 1 (Hoory et al., 2021a) uses the notion of k and m (maximum L2 and infinity norms) of sentence-level (1-D vector) histogram.","Modify,Fact/Evidence",Fact/Evidence
2305,207-ARR,207-ARR_v2_68@7,207-ARR_v1_67@7,"Just as in (Hoory et al., 2021b), we assume that sentence length is 256 words, which is a very pessimistic estimate-this translates into approximately 2.45 tokens per word.","Just as in (Hoory et al., 2021a), we assume that sentence length is 256 words, which is a very pessimistic estimate-this translates into approximately 2.45 tokens per word.","Modify,Fact/Evidence",Fact/Evidence
2306,207-ARR,207-ARR_v2_68@9,207-ARR_v1_67@9,"Additionally, (Hoory et al., 2021b) authors provide a Corollary that allows to obtain slightly better bounds while using approximately the same clipping norm and the same level of noise.","Additionally, (Hoory et al., 2021a) authors provide a Corollary that allows to obtain slightly better bounds while using approximately the same clipping norm and the same level of noise.","Modify,Fact/Evidence",Fact/Evidence
2307,207-ARR,207-ARR_v2_13@0,207-ARR_v1_13@0,"In §5.2 we verify how our models fare with respect to an empirical ""privacy"" definition, namely robustness to training data extraction attacks.","In §5.2 we verify how our models fair with respect to an empirical ""privacy"" definition, namely robustness to training data extraction attacks.","Modify,Grammar",Grammar
2308,207-ARR,207-ARR_v2_19@5,207-ARR_v1_19@5,"Hoory et al. (2021a) looked into DP fine-tuning of a (publicly) pre-trained BERT model (Devlin et al., 2019) in the medical domain and explored how DP fine tuning affects the performance and privacy of the models.","Hoory et al. (2021c) looked into DP fine-tuning of a (publicly) pre-trained BERT model (Devlin et al., 2019) in the medical domain and explored how DP fine tuning affects the performance and privacy of the models.","Modify,Fact/Evidence",Fact/Evidence
2364,21-ARR,,21-ARR_v1_58@1,,"For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.","Delete,Claim",Claim
2365,21-ARR,21-ARR_v2_40@1,,Note that the student is never trained on the quiz set and the teacher only performs meta-update on the quiz set instead of fitting it.,,"Add,Fact/Evidence",Fact/Evidence
2366,21-ARR,21-ARR_v2_40@2,,We do not use a dynamic quiz set strategy because otherwise the student would have been trained on the quiz set and the loss would not be informative.,,"Add,Claim",Claim
2367,21-ARR,21-ARR_v2_44@3,,"For MRPC and QQP, we report both F1 and accuracy.",,"Add,Fact/Evidence",Fact/Evidence
2368,21-ARR,21-ARR_v2_44@4,,"For STS-B, we report Pearson and Spearman correlation.",,"Add,Fact/Evidence",Fact/Evidence
2369,21-ARR,21-ARR_v2_44@5,,The metric for CoLA is Matthew's correlation.,,"Add,Fact/Evidence",Fact/Evidence
2370,21-ARR,21-ARR_v2_44@6,,The other tasks use accuracy as the metric.,,"Add,Fact/Evidence",Fact/Evidence
2371,21-ARR,21-ARR_v2_63@3,,"Although our approach takes more time to achieve its own peak performance, it can match up the performance of PKD (Sun et al., 2019) with a similar time cost.",,"Add,Fact/Evidence",Fact/Evidence
2372,21-ARR,21-ARR_v2_67@0,,MetaDistil focuses on improving the performance of knowledge distillation and does not introduce extra ethical concerns compared to vanilla KD methods.,,"Add,Fact/Evidence",Fact/Evidence
2373,21-ARR,21-ARR_v2_67@1,,"Nevertheless, we would like to point out that as suggested by Hooker et al. (2020), model compression may lead to biases.",,"Add,Fact/Evidence",Fact/Evidence
2374,21-ARR,21-ARR_v2_67@2,,"However, this is not an outstanding problem of our method but a common risk in model compression, which needs to be addressed in the future.",,"Add,Claim",Claim
2375,21-ARR,21-ARR_v2_68@0,,"In MetaDistil, the student is trained in a dynamic manner.",,"Add,Fact/Evidence",Fact/Evidence
2376,21-ARR,21-ARR_v2_68@1,,"To investigate the effect of such a dynamic distillation process, we attempt to use the teacher at the end of MetaDistil training to perform a static conventional KD, to verify the effectiveness of our dynamic distillation strategy.",,"Add,Fact/Evidence",Fact/Evidence
2377,21-ARR,21-ARR_v2_68@2,,"As shown in Table 4, on both experiments, dynamic MetaDistil outperforms conventional KD and static distillation with the teacher at the end of MetaDistil training.",,"Add,Fact/Evidence",Fact/Evidence
2378,21-ARR,21-ARR_v2_68@3,,"As mentioned in Section 3.2, a meta teacher is optimized to transfer its knowledge to a specific student network.",,"Add,Fact/Evidence",Fact/Evidence
2379,21-ARR,21-ARR_v2_68@4,,"To justify this motivation, we conduct experiments using a teacher optimized for the ResNet-32 student to statically distill to the ResNet-20 student, and also in reverse.",,"Add,Fact/Evidence",Fact/Evidence
2380,21-ARR,21-ARR_v2_68@5,,"As shown in Table 4, the cross-taught students underperform the static students taught by their own teachers by 0.27 and 0.12 for ResNet-32 and ResNet-20, respectively.",,"Add,Fact/Evidence",Fact/Evidence
2381,21-ARR,21-ARR_v2_68@6,,This confirms our motivation that the meta teacher in MetaDistil can adjust itself according to its student.,,"Add,Fact/Evidence",Fact/Evidence
2382,21-ARR,21-ARR_v2_70@0,,"We also investigate why MetaDistil works by conducting experiments on the development sets of MNLI, SST, and MRPC, which are important tasks in GLUE that have a large, medium, and small training set, respectively.",,"Add,Fact/Evidence",Fact/Evidence
2383,21-ARR,21-ARR_v2_70@1,,"We illustrate the validation accuracy curves of the meta teacher and student models with training steps in Figure 5, and compare them to the student performance in conventional KD.",,"Add,Fact/Evidence",Fact/Evidence
2384,21-ARR,21-ARR_v2_70@2,,"We can see that the meta teacher maintains high accuracy in the first 5,000 steps and then begins to slowly degrade.",,"Add,Fact/Evidence",Fact/Evidence
2385,21-ARR,21-ARR_v2_70@3,,"Starting from step 8,000, the teacher model underperforms the student while the student's accuracy keeps increasing.",,"Add,Fact/Evidence",Fact/Evidence
2386,21-ARR,21-ARR_v2_70@4,,This verifies our assumption that a model with the best accuracy is not necessarily the optimal teacher.,,"Add,Fact/Evidence",Fact/Evidence
2387,21-ARR,21-ARR_v2_71@0,,"While MetaDistil achieves improved student accuracy on the GLUE benchmark, it is still not very clear where the performance improvement comes from.",,"Add,Claim",Claim
2388,21-ARR,21-ARR_v2_71@1,,"There are two possibilities: (1) the student better mimics the teacher, and (2) the changes of teacher helps student perform better on hard examples that would be incorrectly classified by the student with vanilla KD.",,"Add,Claim",Claim
2389,21-ARR,21-ARR_v2_71@2,,We conduct a series of analysis on the MRPC dataset.,,"Add,Fact/Evidence",Fact/Evidence
2390,21-ARR,21-ARR_v2_72@0,,"For the first assumption, we compute the prediction loyalty (Xu et al., 2021a) of the student model distilled with PKD and MetaDistil, respectively.",,"Add,Fact/Evidence",Fact/Evidence
2391,21-ARR,21-ARR_v2_72@1,,"For MetaDistil, we measure the loyalty with respect to both the original teacher and the final teacher.",,"Add,Fact/Evidence",Fact/Evidence
2392,21-ARR,21-ARR_v2_72@2,,We find that there is no significant difference between between PKD and MetaDistil.,,"Add,Fact/Evidence",Fact/Evidence
2393,21-ARR,21-ARR_v2_72@3,,This suggests that the improvement does not come from student better mimicking the teacher.,,"Add,Claim",Claim
2394,21-ARR,21-ARR_v2_73@0,,"For the second assumption, we first identify the examples in the quiz set for which our model gives correct predictions while the student distilled by PKD makes a wrong prediction.",,"Add,Fact/Evidence",Fact/Evidence
2395,21-ARR,21-ARR_v2_11@0,21-ARR_v1_11@0,"Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., , 2021bXu & McAuley, 2022).","Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks .","Modify,Fact/Evidence",Fact/Evidence
2396,21-ARR,21-ARR_v2_11@2,21-ARR_v1_11@2,"Hinton et al. (2015b) first introduced the idea of knowledge distillation to exploit the ""dark knowledge"" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model.","Hinton et al. (2015) first introduced the idea of knowledge distillation to exploit the ""dark knowledge"" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model.","Modify,Fact/Evidence",Fact/Evidence
2397,21-ARR,21-ARR_v2_4@1,21-ARR_v1_4@1,"Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015b) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015b;Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Ahn et al., 2019;Park et al., 2019;Passalis & Tefas, 2018;Heo et al., 2019;Kim et al., 2018;Shi et al., 2021;Sanh et al., 2019;Jiao et al., 2019;Wang et al., 2020b).","Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015;Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Ahn et al., 2019;Park et al., 2019;Passalis & Tefas, 2018;Heo et al., 2019;Kim et al., 2018;Shi et al., 2021;Sanh et al., 2019;Jiao et al., 2019;Wang et al., 2020b).","Modify,Fact/Evidence",Fact/Evidence
2398,21-ARR,21-ARR_v2_12@6,21-ARR_v1_12@6,"For example, meta pseudo labels (Pham et al., 2020) use meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model.","For example, meta pseudo labels (Pham et al., 2020) uses meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model.","Modify,Grammar",Grammar
2399,21-ARR,21-ARR_v2_12@10,21-ARR_v1_12@10,"In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problems, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.","In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problem, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.","Modify,Grammar",Grammar
2400,21-ARR,21-ARR_v2_13@3,21-ARR_v1_13@3,"Liu et al. (2020) proposed a self-distillation network which utilizes meta-learning to train a label-generator as a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers.","Liu et al. (2020) proposed a self-distillation network and utilizes meta-learning to train a label-generator, which is a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers.","Modify,Clarity",Clarity
2401,21-ARR,21-ARR_v2_20@2,21-ARR_v1_20@2,"Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error (MSE) between student and teacher logits, the similarity between the student and the teacher's attention distribution, etc.","Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error between student and teacher logits, the similarity between the student and the teacher's attention distribution, etc.","Modify,Clarity",Clarity
2402,21-ARR,21-ARR_v2_34@2,21-ARR_v1_29@2,"This ""learning to teach"" paradigm naturally fits the bi-level optimization framework in meta learning literature.","This ""learning to teach"" paradigm naturally fits the bi-level optimization framework in the meta learning literature.","Modify,Grammar",Grammar
2404,21-ARR,21-ARR_v2_40@6,21-ARR_v1_35@5,"We apply the pilot update strategy described in Section 3.2.1 to better align the learning of the teacher and student, as shown in Algorithm 1.",The complete algorithm is shown in Algorithm 1.,"Merge+Modify,Clarity",Clarity
2405,21-ARR,21-ARR_v2_9@5,21-ARR_v1_9@5,"Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably because of its ability to adjust the parameters of the teacher model.","Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably because of its ability to adjust its parameters.","Modify,Claim",Claim
2682,246-ARR,,246-ARR_v1_71@5,,"Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).","Delete,Fact/Evidence",Fact/Evidence
2683,246-ARR,246-ARR_v2_31@1,,The τ value quantifies how similar two system rankings would be if they were computed with two random sets of M input documents.,,"Add,Claim",Claim
2684,246-ARR,246-ARR_v2_31@2,,"When all M test test instances are used, the automatic metrics' rankings become near constant.",,"Add,Claim",Claim
2685,246-ARR,246-ARR_v2_31@3,,The error regions represent ±1 standard deviation.,,"Add,Fact/Evidence",Fact/Evidence
2686,246-ARR,246-ARR_v2_76@0,,"Figs. 10 and 11 contain the r SYS ∆( , u) correlations for ROUGE, BERTScore, and QAEval for various combinations of and u on both the Summ-Eval and REALSumm datasets.",,"Add,Fact/Evidence",Fact/Evidence
2687,246-ARR,246-ARR_v2_77@0,,"To estimate the difference in ROUGE-1 score that is commonly reported in papers, we performed a survey of recently published summarization papers.",,"Add,Fact/Evidence",Fact/Evidence
2688,246-ARR,246-ARR_v2_77@1,,"We selected papers from 2020 and 2021 that were published in a *ACL conference (including Findings), had ""summary"" or ""summarization"" in the title, proposed a new system, and compared systems on the CNN/DailyMail dataset with ROUGE.",,"Add,Fact/Evidence",Fact/Evidence
2689,246-ARR,246-ARR_v2_77@2,,We selected the differences between the best two models that were compared with ROUGE on the test set.,,"Add,Fact/Evidence",Fact/Evidence
2690,246-ARR,246-ARR_v2_77@3,,We did not include ablation experiments for which the differences are likely smaller than the differences between the top two performing systems.,,"Add,Claim",Claim
2691,246-ARR,246-ARR_v2_77@4,,The results are shown in Table 1.,,"Add,Fact/Evidence",Fact/Evidence
2692,246-ARR,246-ARR_v2_77@5,,The average reported difference was found to be 0.49 ROUGE-1.,,"Add,Fact/Evidence",Fact/Evidence
2693,246-ARR,246-ARR_v2_35@0,246-ARR_v1_34@0,"As M approaches M test , the automatic metrics' τ values approach 1, which is much higher than the respective values at M jud , typically around 0.6-0.8.","As M approaches M test , the automatic metrics' τ values approach 1, which is significantly higher than the respective values at M jud , typically around 0.6-0.8.","Modify,Clarity",Clarity
2694,246-ARR,246-ARR_v2_36@1,246-ARR_v1_35@1,"We see that on both datasets the judgments' rankings are still quite variable, reaching a maximum τ of around 0.8-0.85.","We see that on both datasets the judgments' rankings are still quite variable, reaching a maximum of around 0.8-0.85 τ .","Modify,Clarity",Clarity
2695,246-ARR,246-ARR_v2_41@2,246-ARR_v1_40@2,"The largest decrease in width is in the ROUGE family of metrics on SummEval, potentially because that metric and dataset combination saw the biggest improvement in ranking stability (see Fig. 3).","The largest decrease in width is in the ROUGE family of metrics on SummEval, likely because that metric and dataset combination saw the biggest improvement in ranking stability (see Fig. 3).","Modify,Clarity",Clarity
2696,246-ARR,246-ARR_v2_53@2,246-ARR_v1_52@2,"Based on a survey of recent summarization papers in *ACL conferences (see Appendix C), we found that the average improvement over baseline/state-of-the-art models that was reported on the CNN/Dailymail dataset was on average 0.5 ROUGE-1.","Based on a survey of summarization papers in *ACL conferences over the past few years, we found that the average improvement over baseline/state-of-the-art models that was reported on the CNN/Dailymail dataset was on average 0.5 ROUGE-1.","Modify,Fact/Evidence",Fact/Evidence
2697,246-ARR,246-ARR_v2_54@0,246-ARR_v1_53@0,"To that end, we redefine a high quality evaluation metric to be one for which a small difference in scores reliably indicates a difference in quality.","To that extent, we redefine a high quality evaluation metric to be one for which a small difference in scores reliably indicates a difference in quality.","Modify,Clarity",Clarity
2698,246-ARR,246-ARR_v2_6@1,246-ARR_v1_6@1,"First, the metrics' scores which are used in practice are not the ones which are evaluated in system-level correlations; researchers compare systems based on metric scores calculated on the entire test set but calculate scores for system-level correlations when evaluating metrics on a much smaller subset of judged summaries.","First, the metrics' scores which are used in practice are not the ones which are evaluated in system-level correlations: Researchers compare systems based on metric scores calculated on the entire test set but calculate scores for system-level correlations when evaluating metrics on a much smaller subset of judged summaries.","Modify,Grammar",Grammar
2699,246-ARR,246-ARR_v2_65@1,246-ARR_v1_64@1,"We view our work as continuing this direction of research, described next.",We view our work as continuing this direction of research.,"Modify,Clarity",Clarity
2700,246-ARR,246-ARR_v2_9@2,246-ARR_v1_9@2,"This allows us to show, for example, that a ROUGE-1 score difference of less than 0.5 between systems has almost no correlation to how humans would rank the same two systems according to our best estimates ( §4.2).",This allows us to show that a ROUGE-1 score difference of less than 0.5 between systems has almost no correlation to how humans would rank the same two systems according to our best estimates ( §4.2).,"Modify,Clarity",Clarity
2701,246-ARR,246-ARR_v2_2@4,246-ARR_v1_2@4,"Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice.","Second, we propose to calculate correlations only on pairs of systems which are separated by differences in automatic scores that are commonly used to argue one system is of higher quality.","Modify,Claim",Claim
2702,249-ARR,,249-ARR_v1_27@2,,"To avoid black-box persona inference attacks, more constraints on privacy should be added for training LM-based chatbots.","Delete,Claim",Claim
2703,249-ARR,249-ARR_v2_27@2,,"Lastly, we show the overall training objective in Section 4.3.",,"Add,Fact/Evidence",Fact/Evidence
2704,249-ARR,,249-ARR_v1_12@2,,Carlini et al. (2021) performed black-box model inversion attack on GPT-2 through descriptive prompts with beam search.,"Delete,Fact/Evidence",Fact/Evidence
2705,249-ARR,,249-ARR_v1_12@3,,Lehman et al. (2021) examined BERT pretrained on Electronic Health Records via blank filling and model probing to recover Personal Health Information.,"Delete,Fact/Evidence",Fact/Evidence
2706,249-ARR,,249-ARR_v1_12@4,,"Furthermore, given black-box access to a language model's pre-train and finetune stages, Zanella-Béguelin et al. (2020) showed that sensitive sequences of the fine-tuning dataset can be extracted.","Delete,Fact/Evidence",Fact/Evidence
2707,249-ARR,,249-ARR_v1_12@5,,"For the distributed client-server setup, Malekzadeh et al. (2021) considered the sensitive attribute leakage from the server side with honest-but-curious (HBC) classifiers.","Delete,Fact/Evidence",Fact/Evidence
2708,249-ARR,,249-ARR_v1_15@3,,"Lastly, we comprehensively explain our proposed defense strategies in Section 3.3.","Delete,Fact/Evidence",Fact/Evidence
2709,249-ARR,249-ARR_v2_24@1,249-ARR_v1_25@1,"For machine learning as a service (MLaaS), A can be applied to perform a man-in-the-middle attack on the application programming interfaces.","For Machine Learning as a Service (MLaaS), A can be directly applied to perform a man-in-the-middle attack on the application programming interfaces.","Modify,Grammar",Grammar
2710,249-ARR,249-ARR_v2_24@2,249-ARR_v1_25@2,"Moreover, even if the raw data are protected and the transmission channel is secure, a curious service provider can train its attacker A to collect personas of service users.","Moreover, even if the raw data are protected and the transmission channel is secure, a curious service provider can train its attacker model A to collect personas of service users.","Modify,Clarity",Clarity
2711,249-ARR,249-ARR_v2_26@0,249-ARR_v1_27@0,The LM training objective in Equation 1 only considers the utility of chatbots.,The simple LM training objective only considers the utility of chatbots.,"Modify,Claim",Claim
2712,249-ARR,249-ARR_v2_27@0,249-ARR_v1_28@0,"Following the intuition that the adversary cannot obtain better results than a random guess, in Section 4.1, we propose KL loss that aims to flatten the persona predictor's estimated distribution.","Following the intuition that the adversary cannot obtain better results than a random guess, we propose KL loss that aims to flatten the persona predictor's estimated distribution.","Modify,Fact/Evidence",Fact/Evidence
2713,249-ARR,249-ARR_v2_27@1,249-ARR_v1_28@1,"Based on minimizing the mutual information between hidden states f (u) of chatbots and private persona attributes s, we propose MI loss in Section 4.2.","Based on minimizing the mutual information between hidden states f (u) of chatbots and private persona attributes s, we propose MI loss.","Modify,Fact/Evidence",Fact/Evidence
2714,249-ARR,249-ARR_v2_31@1,249-ARR_v1_30@2,"For optimization, we can leave out constant terms and the logarithm (Mireshghallah et al., 2021) to obtain the following loss function:","For optimization, we can leave out constant terms (Mireshghallah et al., 2021) and obtain the following loss function:","Modify,Fact/Evidence",Fact/Evidence
2715,249-ARR,249-ARR_v2_42@2,249-ARR_v1_44@2,"Instead, we use p Ψ (s|f (u)) to approximate q(s|f (u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2019):","Instead, we use p Ψ (s|f (u)) to approximate q(s|f (u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2018):","Modify,Fact/Evidence",Fact/Evidence
2716,249-ARR,249-ARR_v2_53@0,249-ARR_v1_55@0,Overall Objective,Overall Loss,"Modify,Other",Other
2717,249-ARR,249-ARR_v2_57@0,249-ARR_v1_59@0,Experiments,Experiment,"Modify,Grammar",Grammar
2718,249-ARR,249-ARR_v2_59@0,249-ARR_v1_61@0,Experimental Settings,Experimental Setting,"Modify,Grammar",Grammar
2719,249-ARR,249-ARR_v2_60@1,249-ARR_v1_62@1,"To train the GPT-2 as our chatbot, we use the DialoGPT pretrained on Reddit comment chains.","To train the GPT-2 as our chatbot, we use the DialoGPT dataset (Zhang et al., 2018) to fine-tune the GPT-2.","Split+Modify,Fact/Evidence",Fact/Evidence
2720,249-ARR,249-ARR_v2_60@2,249-ARR_v1_62@1,"Then we use PersonaChat dataset (Zhang et al., 2018) to fine-tune the GPT-2.","To train the GPT-2 as our chatbot, we use the DialoGPT dataset (Zhang et al., 2018) to fine-tune the GPT-2.","Split+Modify,Fact/Evidence",Fact/Evidence
2721,249-ARR,249-ARR_v2_4@3,249-ARR_v1_4@3,"Unfortunately, large language models tend to memorize training data and some private data can be recovered from models (Pan et al., 2020;Carlini et al., 2021).","Unfortunately, large language models tend to memorize training data and some private data can be recovered from models via black-box training data extraction attacks (Carlini et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
2722,249-ARR,249-ARR_v2_68@1,249-ARR_v1_68@10,"Suppose the adversary knows the persona with Max-Ratio, then it can improve its guess by not predicting this persona, which is a threat for fewer persona labels (for example, binary classification).","Suppose the adversary knows the persona with Max-Ratio, then he can improve his guess by not predicting this persona, which is a threat for fewer persona labels (for example, binary classification).","Modify,Clarity",Clarity
2723,249-ARR,249-ARR_v2_7@8,249-ARR_v1_7@8,"We combine proposed KL divergence loss (KL loss) with mutual information loss (MI loss) (Song et al., 2019) as additional defense objectives to train the GPT-2 and decrease the attacker's persona inference accuracy to 0.53%.","We combine proposed KL divergence loss (KL loss) with mutual information loss (MI loss) (Song et al., 2018) as additional defense objectives to train the GPT-2 and decrease the attacker's persona inference accuracy to 0.53%.","Modify,Fact/Evidence",Fact/Evidence
2724,249-ARR,249-ARR_v2_12@7,249-ARR_v1_13@4,"Though the setup of this work is similar to ours, they merely consider simple cases of data recovery with given rules and suffer great utility degradation to obtain optimal defense performance.","Though the setup of this work is similar to ours, they merely considered simple cases of data recovery with given rules and suffered great utility degradation to obtain optimal defense performance.","Modify,Grammar",Grammar
2725,249-ARR,249-ARR_v2_13@0,249-ARR_v1_14@0,Attacking on Language Models,Attacking on Casual Language Model,"Modify,Clarity",Clarity
2746,250-ARR,,250-ARR_v1_43@3,,Hyperparameters selection and other implementation details can be found in Appendix.,"Delete,Fact/Evidence",Fact/Evidence
2747,250-ARR,250-ARR_v2_46@4,,"We re-implement PKD (Sun et al., 2019) and ALP-KD approaches using the default settings proposed in the respective papers.",,"Add,Fact/Evidence",Fact/Evidence
2748,250-ARR,250-ARR_v2_46@5,,"We used early stopping based on performance on the development set, while making sure that the figures are in line with the ones reported in the papers.",,"Add,Fact/Evidence",Fact/Evidence
2749,250-ARR,250-ARR_v2_46@6,,"More precisely, the best layer setting for PKD teacher BERT 12 is {2, 4, 6, 8, 10} to distill into DistilBERT 6 .",,"Add,Fact/Evidence",Fact/Evidence
2750,250-ARR,250-ARR_v2_46@7,,"For DistilRoBERTa 6 , we choose the intermediate layers 4, 8, 12, 16, 20 from the teacher RoBERTa 24 model for distillation that we found to work the best on the development set.",,"Add,Fact/Evidence",Fact/Evidence
2751,250-ARR,250-ARR_v2_47@0,,"Using ALP-KD, we compute attention weights for the intermediate layers of the teacher (i.e., 1 to 11 for BERT 12 and 1 to 23 for RoBERTa 24 models) to calculate the weighted intermediate representations of the teacher for each intermediate layer of the student model (i.e., 1 to 5 layers of the student models).",,"Add,Fact/Evidence",Fact/Evidence
2752,250-ARR,250-ARR_v2_47@1,,"Since, the hidden dimensions of the RoBERTa 24 and DistilRoBERTa 6 are different, we linearly transform them into same lowerdimensional space.",,"Add,Fact/Evidence",Fact/Evidence
2753,250-ARR,250-ARR_v2_48@0,,"For RAIL-KD l , at each epoch we randomly select 5 layers from the intermediate layers of the teacher (i.e., from layers 1 to 11 for BERT 12 model and 1 to 23 for RoBERTa 24 model).",,"Add,Fact/Evidence",Fact/Evidence
2754,250-ARR,250-ARR_v2_48@2,,"We fixed α i = 1, λ 1 , λ 2 , λ 3 = 1/3 for our proposed approaches 4 .",,"Add,Fact/Evidence",Fact/Evidence
2755,250-ARR,250-ARR_v2_48@3,,"We search learning rate from {1e-5, 2e-5, 5e-5, 4e-6}, batch size from {8, 16, 32}, and fixed the epoch number to 40 for all the experiments.",,"Add,Fact/Evidence",Fact/Evidence
2756,250-ARR,250-ARR_v2_48@5,,"We ran all the experiments on a single NVIDIA V100 GPU using mixed-precision training (Micikevicius et al., 2018) and PyTorch (Paszke et al., 2019) framework.",,"Add,Fact/Evidence",Fact/Evidence
2757,250-ARR,250-ARR_v2_61@1,,"As Table 6 shows, the variances of RAIL-KD is in the same range for each task, for instance, RAIL-KD variance is at the same scale compared with PKD and ALP-KD on CoLA and MRPC, and even lower on RTE.",,"Add,Fact/Evidence",Fact/Evidence
2758,250-ARR,250-ARR_v2_61@2,,"This indicates that the gains of RAIL-KD are significant, and are not due to chance in our random selection of layers to distill.",,"Add,Claim",Claim
2759,250-ARR,250-ARR_v2_67@4,,Similar trends are seen from the other layers.,,"Add,Claim",Claim
2760,250-ARR,250-ARR_v2_69@2,,The figure clearly shows (light colors) that most of ALP weights are concentrated on top layers of the teacher.,,"Add,Fact/Evidence",Fact/Evidence
2761,250-ARR,250-ARR_v2_69@3,,"For instance, layers 1,2,5 of the three students mostly attend to the last layer of BERT 12 .",,"Add,Fact/Evidence",Fact/Evidence
2762,250-ARR,250-ARR_v2_15@4,250-ARR_v1_14@4,"3 In other words, we don't compare with works like TinyBERT (Jiao et al., 2019) or MiniLM (Wang et al., 2020b), which use extra losses like self-attention distribution matching.","Otherwise said, we don't compare with works like TinyBERT (Jiao et al., 2019) or MiniLM (Wang et al., 2020b), which use extra losses like self-attention distribution matching.","Modify,Clarity",Clarity
2763,250-ARR,250-ARR_v2_22@2,250-ARR_v1_21@2,"Since in Sun et al. (2020a), the mean-pooling representation shows better results, we adopt it to compute the sentence representation of each layer.","Since in (Sun et al., 2020a), the mean-pooling representation shows better results, we adopt it to compute the sentence representation of each layer.","Modify,Grammar",Grammar
2764,250-ARR,250-ARR_v2_53@8,250-ARR_v1_47@3,"CoDIR results are adopted from their paper, and we followed their experimental protocol by not reporting scores on STS-B. On average, RAIL-KD performs on par with CoDIR (+0.2%) and outperforms it on 5 out of 8 datasets, while being almost two-times faster as shown in the next section.","CoDIR results are adopted from their paper, and we followed their experimental protocol by not reporting scores on STS-B. On average, RAIL-KD performs on par with CoDIR (+0.2%) and outperforms it on 5 out of 8 datasets, while being almost twice faster as shown in the next section.","Modify,Clarity",Clarity
2765,250-ARR,250-ARR_v2_55@2,250-ARR_v1_49@2,We used this configuration because CoDIR pretrained student models are not available and we can only run CoDIR code out-of-the-box.,We used this configuration because CODIR pretrained student models are not available and we can only run CODIR code out-of-the-box.,"Modify,Grammar",Grammar
2766,250-ARR,250-ARR_v2_68@1,250-ARR_v1_60@4,"Moreover, we observe that ALP-KD results have less similarity scores in the upper intermediate layers.","Moreover, we observe that ALP-KD results in less similarity scores in the upper intermediate layers.","Modify,Grammar",Grammar
2767,250-ARR,250-ARR_v2_12@0,250-ARR_v1_11@0,"In recent years, a wide range of methods have tried to expand knowledge transfer of transformerbased (Vaswani et al., 2017) NLU models beyond logits matching.","Recent years, have seen a wide range of methods have witnessed to expand knowledge transfer of transformer-based (Vaswani et al., 2017) NLU models beyond logits matching.","Modify,Grammar",Grammar
2768,250-ARR,250-ARR_v2_12@2,250-ARR_v1_11@2,"TinyBERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020b), and MiniLM (Wang et al., 2020b) matched the intermediate layers representations and self-attention distributions of the teacher and the student.","Tiny-BERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020b), and MiniLM (Wang et al., 2020b) matched the intermediate layers representations and selfattention distributions of the teacher and the student.","Modify,Grammar",Grammar
2769,250-ARR,250-ARR_v2_13@13,250-ARR_v1_12@13,"Alternatively, CoDIR (Sun et al., 2020a) exploited contrastive learning (Tian et al., 2019) to perform intermediate layers matching between the teacher and the student models with no deterministic mapping.","Alternatively, CODIR (Sun et al., 2020a) exploited contrastive learning (Tian et al., 2019) to perform intermediate layers matching between the teacher and the student models with no deterministic mapping.","Modify,Grammar",Grammar
2770,250-ARR,250-ARR_v2_14@0,250-ARR_v1_13@0,"Table 1 summarizes the main characteristics of the existing state-of-the-art intermediate layer distillation techniques (PKD, CKD, and CoDIR) used for pre-trained language models compared with our proposed RAIL-KD.","Table 1 summarizes the main characteristics of the existing state-of-the-art intermediate layer distillation techniques (PKD (Sun et al., 2019), CKD (Wu et al., 2020a), and CoDIR) used for pre-trained language models compared with our proposed RAIL-KD.","Modify,Fact/Evidence",Fact/Evidence
2771,250-ARR,250-ARR_v2_15@1,250-ARR_v1_14@1,"For instance, RAIL-KD is roughly two-times faster than CoDIR in a 24 to 6 layer compression.","For instance, RAIL-KD is roughly twice faster than CoDIR in a 24 to 6 layer compression.","Modify,Clarity",Clarity
2838,258-ARR,,258-ARR_v1_31@0,,Experiments,"Delete,Other",Other
2839,258-ARR,,258-ARR_v1_32@0,,Setup,"Delete,Other",Other
2840,258-ARR,,258-ARR_v1_33@0,,We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers.,"Delete,Fact/Evidence",Fact/Evidence
2841,258-ARR,258-ARR_v2_4@1,,"They are often used in language learning environments as a quick and effective way to test vocabulary, grammar and reading comprehension (Tremblay, 2011;Trace, 2020).",,"Add,Fact/Evidence",Fact/Evidence
2842,258-ARR,258-ARR_v2_17@4,,We believe that this discrimination objective makes it more suitable for our token classification task.,,"Add,Claim",Claim
2843,258-ARR,258-ARR_v2_17@5,,"Moreover, we also exploit ELECTRA's generation capabilities as a language model for estimating the answers to the proposed gaps as an auxiliary task.",,"Add,Claim",Claim
2844,258-ARR,258-ARR_v2_40@0,,"Standard ELECTRA Similar to BERT, it predicts potentially good gaps using a standard pre-trained ELECTRA-base model.",,"Add,Fact/Evidence",Fact/Evidence
2845,258-ARR,258-ARR_v2_40@1,,This is a single-objective model that is fine-tuned on token classification only.,,"Add,Fact/Evidence",Fact/Evidence
2846,258-ARR,258-ARR_v2_48@5,,We also perform an ablation study in Table 3 where we compare our multi-objective ELECTRA model to a standard one that does not include our auxiliary language model objective.,,"Add,Fact/Evidence",Fact/Evidence
2847,258-ARR,258-ARR_v2_48@6,,"Results show that the former outperforms the latter on all metrics, confirming that the addition of the LM objective is clearly beneficial.",,"Add,Fact/Evidence",Fact/Evidence
2848,258-ARR,258-ARR_v2_4@2,258-ARR_v1_4@1,"However, designing high-quality cloze tests for language learning is a laborious process that involves finding an optimal distribution of gaps based on aspects such as function, distance, number of answers, etc. (ALTE, 2005;2011).","Designing high-quality cloze tests for language learning is a laborious process that involves finding an optimal distribution of gaps based on aspects such as function, distance, number of answers, etc. (ALTE, 2005;2011).","Modify,Clarity",Clarity
2849,258-ARR,258-ARR_v2_28@5,258-ARR_v1_25@5,A combination of gaps that yields lower KL divergence is assumed to be a better solution.,A combination of gaps that yields lower KL divergence is assumed to be a better selection.,"Modify,Clarity",Clarity
2850,258-ARR,258-ARR_v2_32@0,258-ARR_v1_28@2,"For this reason, we use a collection of expertly created open cloze tests at the B2 CEFR level that was kindly provided by Cambridge University Press & Assessment (CUP&A) for research purposes.","For this reason, we use a collection of expertly created open cloze tests at the B2 CEFR level that was kindly provided by <anonymised> for research purposes.","Modify,Fact/Evidence",Fact/Evidence
2851,258-ARR,258-ARR_v2_34@4,258-ARR_v1_33@3,"6 On top of the encoding layers, we have two branches that are being learned simultaneously (Figure 2).","6 On top of the encoding layers, we have two branches that are being learned simultaneously.","Modify,Fact/Evidence",Fact/Evidence
2852,258-ARR,258-ARR_v2_34@6,258-ARR_v1_34@1,"For the second branch, we add ELECTRA's generation layer plus a linear layer which aims to predict the best word from the whole vocabulary as an auxiliary task.","For the second branch, we add ELECTRA's generation layer plus a linear layer which aims to predict the best word from the whole vocabulary.","Modify,Fact/Evidence",Fact/Evidence
2853,258-ARR,258-ARR_v2_5@2,258-ARR_v1_5@2,"Our main objective is standard token classification, where we aim to minimise the error of classifying a token as a gap or not.","One objective is standard token classification, where we aim to minimise the error of classifying a token as a gap or not.","Modify,Fact/Evidence",Fact/Evidence
2854,258-ARR,258-ARR_v2_36@0,258-ARR_v1_36@0,"We compare our multi-objective ELECTRA model to other systems, namely:","We compare our ELECTRA-based model to other systems, namely:","Modify,Fact/Evidence",Fact/Evidence
2855,258-ARR,258-ARR_v2_39@0,258-ARR_v1_40@0,We use the pre-trained base model with standard parameters and fine-tune the weights of the whole architecture.,We use the base pre-trained model with standard parameters and fine-tune the weights of the whole architecture.,"Modify,Clarity",Clarity
2856,258-ARR,258-ARR_v2_40@2,258-ARR_v1_40@1,"Both random and Exercise Maker attempt to generate the same number of gaps per task as defined in the gold standard, although this is not always possible since the required conditions (such as specific words or PoS) are not always met.","Both random and Exercise Maker attempt to generate the same number of gaps per task as defined in the gold standard, although this is not always possible since the required conditions are not always met.","Modify,Claim",Claim
2857,258-ARR,258-ARR_v2_5@3,258-ARR_v1_5@3,The second and auxiliary objective is a language-model-based objective whereby we attempt to minimise the language model error when predicting the right answer for each gap.,The other objective is a language-modelbased objective whereby we attempt to minimise the language model error when predicting the right answer for each gap.,"Modify,Fact/Evidence",Fact/Evidence
2858,258-ARR,258-ARR_v2_44@0,258-ARR_v1_43@0,We also report human evaluation by three test experts from CUP&A who volunteered for the task.,"We also performed human evaluation, which was carried out by three test experts from <anonymised> who volunteered for the task.","Modify,Fact/Evidence",Fact/Evidence
2859,258-ARR,258-ARR_v2_47@1,258-ARR_v1_46@1,"Table 3 reports the results of our multi-objective ELECTRA model (enhanced with dependency information) as well as the random baseline, Exercise Maker, BERT, and the standard single-task ELECTRA.","Table 3 reports the results of our multi-objective ELECTRA model (enhanced with dependency information) as well as the random baseline, Exercise Maker and BERT.","Modify,Fact/Evidence",Fact/Evidence
2860,258-ARR,258-ARR_v2_2@0,258-ARR_v1_2@0,This paper presents the first multi-objective transformer model for constructing open cloze tests that exploits generation and discrimination capabilities to improve performance.,This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance.,"Modify,Clarity",Clarity
2861,258-ARR,258-ARR_v2_5@4,258-ARR_v1_5@4,"Our solution is based on a pre-trained ELECTRA (Clark et al., 2020) model that is finetuned on the two described objectives in a multitask scenario.","Our solution is based on a pre-trained ELECTRA (Clark et al., 2020) model that is fine-tuned on the two described objectives in a multi-task scenario.","Modify,Grammar",Grammar
2862,258-ARR,258-ARR_v2_48@4,258-ARR_v1_46@8,"Although the improvement of our multi-objective ELECTRA model over BERT does not seem to be very significant based simply on P, R and F 1 , a closer look at the results reveals that BERT produces a much higher number of repeated gaps (25 compared to 9 by multi-objective ELECTRA) as well as more cases of gaps in close proximity, as shown in Figure 3.","Although the improvement of our multi-objective ELECTRA model over BERT does not seem to be very significant based simply on P, R and F 1 , a closer look at the results reveals that BERT produces a much higher number of repeated gaps (25 compared to 9 by multi-objective ELECTRA) as well as more cases of gaps in close proximity as shown in Figure 3.","Modify,Grammar",Grammar
2863,258-ARR,258-ARR_v2_52@1,258-ARR_v1_50@1,"It also creates a better spread of gaps, as shown by a lower KL-divergence between the average PoS distribution of the output and that of the gold standard (0.55 with post-processing as opposed to 0.59 without it).","It also creates a better spread of gaps, as shown by a lower KL-divergence between the average POS distribution of the output and that of the gold standard (0.55 with post-processing as opposed to 0.59 without it).","Modify,Grammar",Grammar
2864,258-ARR,258-ARR_v2_53@1,258-ARR_v1_51@1,"Table 5 shows that they significantly improve R, which results in higher overall F 1 .","Table 5 shows that significantly improve R, which results in higher overall F 1 .","Modify,Clarity",Clarity
2865,258-ARR,258-ARR_v2_69@2,258-ARR_v1_66@2,"The worst performing classes are PRON (77%), NUM (77%) and VERB (75%) as opposed to the previous AUX and PART counterparts (now 79% and 83% respectively).","The worst performing classes are PRON (77%), NUM (77%) and VERB (77%) as opposed to the previous AUX and PART counterparts (now 79% and 83% respectively).","Modify,Fact/Evidence",Fact/Evidence
2866,258-ARR,258-ARR_v2_17@8,258-ARR_v1_14@6,The first objective is typical of any standard token classification model and constitutes our key task.,The first objective is typical of any standard token classification model.,"Modify,Fact/Evidence",Fact/Evidence
2867,258-ARR,258-ARR_v2_18@0,258-ARR_v1_15@0,The second and auxiliary objective attempts to model our preference for gaps with a restricted number of answers while also ensuring that the original word can be guessed from the context.,The second objective attempts to model our preference for gaps with a restricted number of answers while also ensuring that the original word can be guessed from the context.,"Modify,Fact/Evidence",Fact/Evidence
2868,258-ARR,258-ARR_v2_4@0,258-ARR_v1_4@0,"Open cloze (Taylor, 1953) tests are a common type of exercise where words are removed from a piece of text and must then be filled in by the students without any options to choose from.",Open cloze tests are a common type of learning exercise where a student must complete a sentence by filling in a gap without any options to choose from.,"Modify,Fact/Evidence",Fact/Evidence
2869,276-ARR,,276-ARR_v1_45@0,,See Appendix B for proof.,"Delete,Fact/Evidence",Fact/Evidence
2870,276-ARR,,276-ARR_v1_46@0,,See Appendix C for proof.,"Delete,Fact/Evidence",Fact/Evidence
2871,276-ARR,,276-ARR_v1_55@3,,"7 For example, we repeated the full experiment on ADePT (n = 2, ε = 0.1) 100 times which results in standard deviation 0.0008 from the mean value 0.195.","Delete,Fact/Evidence",Fact/Evidence
2872,276-ARR,,276-ARR_v1_55@4,,Better MLE precision can be simply obtained by increasing the 10 million repeats per experiment.,"Delete,Claim",Claim
2873,276-ARR,,276-ARR_v1_55@5,,All source codes are attached/will go on Github.,"Delete,Fact/Evidence",Fact/Evidence
2874,276-ARR,,276-ARR_v1_84@0,,See Figure 2.,"Delete,Fact/Evidence",Fact/Evidence
2875,276-ARR,276-ARR_v2_44@5,,See Fig. 3 in the Appendix for various Laplace-based distributions sampled with different techniques including possible distributions sampled in DPText.,,"Add,Fact/Evidence",Fact/Evidence
2876,276-ARR,276-ARR_v2_64@0,,Ethics Statement,,"Add,Other",Other
2877,276-ARR,276-ARR_v2_65@0,,"We declare no conflict of interests with the authors of DPText, we do not even know them personally.",,"Add,Claim",Claim
2878,276-ARR,276-ARR_v2_65@1,,The purpose of this paper is strictly scientific.,,"Add,Claim",Claim
2879,276-ARR,276-ARR_v2_43@3,276-ARR_v1_78@3,"However, F −1 as used in ( 9) is increasing only on interval [0, 0.5] (Fig. 1).","However, F −1 as used in ( 9) is increasing only on interval [0, 0.5].","Modify,Fact/Evidence",Fact/Evidence
2880,276-ARR,276-ARR_v2_43@4,276-ARR_v1_78@4,"For v ≥ 0.5, we get negative argument to ln which yields a complex function, whose real part is even decreasing.","For v ≥ 0.5, we get negative argument to ln which yields a complex function, whose real parts is even decreasing.","Modify,Grammar",Grammar
2881,276-ARR,276-ARR_v2_46@1,276-ARR_v1_80@1,"We rely on the standard proof of the Laplace mechanism as shown, e.g, by Habernal (2021).","We rely on the standard proof of the Laplace mechanism as shown, e.g, by (Dwork and Roth, 2013;Habernal, 2021).","Modify,Fact/Evidence",Fact/Evidence
2882,276-ARR,276-ARR_v2_0@0,276-ARR_v1_0@0,How reparametrization trick broke differentially-private text representation learning,How reparametrization trick broke differentially-private text representation leaning,"Modify,Grammar",Grammar
2883,276-ARR,276-ARR_v2_33@0,276-ARR_v1_35@0,"While both (4) and ( 7) generate samples from Lap(µ; b), note the substantial difference between u and v, since each is drawn from a different uniform distribution (see visualizations in Fig. 1).","While both (4) and ( 7) generate samples from Lap(µ; b), note the substantial difference between u and v, since each is drawn from a different uniform distribution.","Modify,Fact/Evidence",Fact/Evidence
3081,30-ARR,30-ARR_v2_18@2,,"Then, all model-identified rationales (underlined texts in Figure 2) are examined by human annotators to identify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model).",,"Add,Fact/Evidence",Fact/Evidence
3082,30-ARR,30-ARR_v2_18@3,,Both false rationales and missing rationales are corrected to produce augmented examples.,,"Add,Fact/Evidence",Fact/Evidence
3083,30-ARR,30-ARR_v2_18@4,,"Finally, newly generated examples are added into the training set to re-train the deep learning model.",,"Add,Fact/Evidence",Fact/Evidence
3084,30-ARR,,30-ARR_v1_14@1,,"In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.","Delete,Fact/Evidence",Fact/Evidence
3085,30-ARR,,30-ARR_v1_14@2,,"The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD using the full augmented training set of 3,414 examples.","Delete,Fact/Evidence",Fact/Evidence
3086,30-ARR,30-ARR_v2_46@0,,"To address RQ1, we compare the performance of models trained by the static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static.",,"Add,Fact/Evidence",Fact/Evidence
3087,30-ARR,30-ARR_v2_46@1,,"We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full.",,"Add,Fact/Evidence",Fact/Evidence
3088,30-ARR,,30-ARR_v1_15@0,,"To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and humanintervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.","Delete,Claim",Claim
3089,30-ARR,,30-ARR_v1_15@1,,All resources will be released on Github.,"Delete,Fact/Evidence",Fact/Evidence
3090,30-ARR,,30-ARR_v1_16@0,,Related Work,"Delete,Other",Other
3091,30-ARR,,30-ARR_v1_17@0,,"Data augmentation has been used for resolving artefacts in training datasets before (Gururangan et al., 2018;Srivastava et al., 2020;Kaushik et al., 2021).","Delete,Fact/Evidence",Fact/Evidence
3092,30-ARR,30-ARR_v2_65@2,,"In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.",,"Add,Claim",Claim
3093,30-ARR,30-ARR_v2_66@0,,Ethical Statement,,"Add,Other",Other
3094,30-ARR,30-ARR_v2_67@0,,We honor the ACL Code of Ethics.,,"Add,Claim",Claim
3095,30-ARR,30-ARR_v2_67@1,,No private data or non-public information was used in this work.,,"Add,Fact/Evidence",Fact/Evidence
3096,30-ARR,30-ARR_v2_67@2,,All annotators have received labor fees corresponding to the amount of their annotated instances.,,"Add,Fact/Evidence",Fact/Evidence
3097,30-ARR,30-ARR_v2_21@2,30-ARR_v1_24@2,"Only adjectives, adverbs, nouns, and verbs were considered as rationales.","Only adjectives, adverbs, nouns, and verbs were considered.","Modify,Clarity",Clarity
3098,30-ARR,30-ARR_v2_21@5,30-ARR_v1_24@5,"We also limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams).","We limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams).","Modify,Clarity",Clarity
3099,30-ARR,30-ARR_v2_4@0,30-ARR_v1_4@0,"Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020;Srivastava et al., 2020) in datasets can cause sub-optimal model performance for neural networks.","Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020;Srivastava et al., 2020) in datasets cause sub-optimal model performance for neural networks.","Modify,Clarity",Clarity
3100,30-ARR,30-ARR_v2_22@4,30-ARR_v1_25@4,All re-annotated examples were approved only if all authors were happy with the quality of the annotations.,All re-annotated examples were approved only if all authors were happy with the quality of annotations.,"Modify,Grammar",Grammar
3101,30-ARR,30-ARR_v2_23@0,30-ARR_v1_26@0,"Our annotation procedure generated 5,073 rationales in 855 movie reviews involved in Section 3.1 and 3.3 (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).","Our annotation procedure generated 5,073 rationales in 855 movie reviews (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).","Modify,Fact/Evidence",Fact/Evidence
3102,30-ARR,30-ARR_v2_23@3,30-ARR_v1_26@3,"Note that our approach using 100 labelled examples can outperform manual CAD (Kaushik et al., 2020) using the entire training set of 1,707 examples (see Section 5.3), making our approach 300×1707 183.68×100 ≈ 27.88 times more efficient than manually generated CAD.","Note that our approach using 100 labelled examples can outperform the manual CAD (Kaushik et al., 2020) using the entire training set with 1,707 examples (see Section 5.3), making our approach 300×1707 183.68×100 ≈ 27.88 times more efficient than manually generated CAD.","Modify,Grammar",Grammar
3103,30-ARR,30-ARR_v2_4@1,30-ARR_v1_4@1,"As shown in Figure 1, the bold phrases-""100% bad"" and ""brain cell killing""-are underlying causes for a negative sentiment prediction that most human readers would recognise.","As shown in Figure 1, the phrases in bold-""100% bad"" and ""brain cell killing""-are underlying causes for a negative prediction most human readers would recognise.","Modify,Clarity",Clarity
3104,30-ARR,30-ARR_v2_26@1,30-ARR_v1_29@1,"To generate a semi-factual example, x ′ i , we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms.","To generate a semi-factual example, x i ', we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms.","Modify,Grammar",Grammar
3105,30-ARR,30-ARR_v2_27@0,30-ARR_v1_30@0,"In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x ′ i , with some replaced non-rationales and all the other tokens identical to x i .","In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x i ', with some replaced non-rationales and all the other tokens identical to x i .","Modify,Grammar",Grammar
3106,30-ARR,30-ARR_v2_2@0,30-ARR_v1_2@0,We present a novel rationale-centric framework with human-in-the-loop -Rationales-centric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios.,We present a novel rational-centric framework with human-in-the-loop -Rationales-centric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios.,"Modify,Grammar",Grammar
3107,30-ARR,30-ARR_v2_4@3,30-ARR_v1_4@3,"The underlined phrase-""acting and plot""has been incorrectly recognised as a causal term by the model used fort this example, and is referred to as a spurious pattern.","The underlined phrase-""acting and plot""-are incorrectly recognised as causal terms by the model, and are referred to as spurious patterns.","Modify,Clarity",Clarity
3108,30-ARR,30-ARR_v2_5@0,30-ARR_v1_5@0,"Spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021), and are usually useless, or even harmful, at test time.","These spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021) and are usually useless, or even harmful, at test time.","Modify,Clarity",Clarity
3109,30-ARR,30-ARR_v2_32@0,30-ARR_v1_35@0,Note that the two correction methods in dynamic human-intervened correction can operate in parallel and the generated examples are added to the small training set to re-train the model.,Note that the two correction methods in dynamic human-intervened correction can be operated in parallel and the generated examples are added to the small training set to re-train the model.,"Modify,Clarity",Clarity
3110,30-ARR,30-ARR_v2_37@0,30-ARR_v1_37@0,"Broadly speaking, our RDL framework takes advantage of invariance that makes a model less sensitive to non-rationale words or spurious patterns (Tu et al., 2020; in favour of focusing on useful mappings of rationales to labels.","Broadly speaking, our RDL framework acts like a sensible inductive bias (Wu et al., 2021b;Warstadt and Bowman, 2020) that restricts a model from superficially focusing on whole texts or spurious patterns (Tu et al., 2020; in favour of focusing on useful mappings of rationales to labels.","Modify,Fact/Evidence",Fact/Evidence
3111,30-ARR,30-ARR_v2_49@1,30-ARR_v1_53@1,"Following Kaushik et al. (2020), we fine-tune RoBERTa for up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).","Following Kaushik et al. (2020), we fine-tune RoBERTa up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).","Modify,Grammar",Grammar
3112,30-ARR,30-ARR_v2_49@4,30-ARR_v1_54@2,"We found that setting the learning rate to {5e-5, 5e-6 and 5e-6} could optimise Static, Static+n, and Full, respectively.","We found that setting the learning rate to 5e-5, 5e-6 and 5e-6 could optimise Static, Static+n, and Full, respectively.","Modify,Grammar",Grammar
3113,30-ARR,30-ARR_v2_51@1,30-ARR_v1_56@1,"Among all Static+n methods, Static+350 seems the best-performing method and exceeds Static with a 1.56% in-distribution improvement in average accuracy.","Among all Static+n methods, Static+350 seems the best-performing method that exceeds Static with a 1.56% in-distribution improvement in average accuracy.","Modify,Clarity",Clarity
3114,30-ARR,30-ARR_v2_51@2,30-ARR_v1_56@2,"Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, and 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets respectively.","Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets, respectively.","Modify,Clarity",Clarity
3115,30-ARR,30-ARR_v2_52@0,30-ARR_v1_57@0,"The Static+n methods can even outperform Full (i.e. normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset.","The Static+n methods can even outperform Full (i,e, the normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset.","Modify,Grammar",Grammar
3116,30-ARR,30-ARR_v2_52@4,30-ARR_v1_57@4,"It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp dataset (93.66±0.84) than on the in-distribution dataset (93.23±0.46), which could also be explained by the high similarity between the underlying distributions of these two datasets.","It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp (93.66±0.84) than on the in-distribution dataset (93.23±0.46), which could also be explained by the high similarity between the underlying distributions of these two test sets.","Modify,Clarity",Clarity
3117,30-ARR,30-ARR_v2_52@6,30-ARR_v1_57@6,"We believe that the duplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.","We believe that the multiplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.","Modify,Claim",Claim
3118,30-ARR,30-ARR_v2_5@3,30-ARR_v1_7@1,"In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu and MacNamee, 2020;Lu et al., 2021).","In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3119,30-ARR,30-ARR_v2_53@0,30-ARR_v1_58@0,"As shown in Table 3, RR is slightly better than the baseline Static approach.","As shown in Table 3, RR is slightly better than the baseline Static.","Modify,Clarity",Clarity
3120,30-ARR,30-ARR_v2_53@1,30-ARR_v1_58@1,"This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to the original data, introducing noise that helps prevent overfitting to some extent.","This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to original data, introducing noise that helps prevent overfitting to some extent.","Modify,Grammar",Grammar
3121,30-ARR,30-ARR_v2_53@2,30-ARR_v1_58@2,"However, the magnitude of improvement of the Static+n method is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.","However, the magnitude of improvement of Static+n methods is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.","Modify,Grammar",Grammar
3122,30-ARR,30-ARR_v2_59@1,30-ARR_v1_63@1,"To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter).","To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter), respectively.","Modify,Clarity",Clarity
3123,30-ARR,30-ARR_v2_61@3,30-ARR_v1_65@3,"Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, but no improvement for the SemEval dataset.","Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, except for the SemEval dataset.","Modify,Clarity",Clarity
3124,30-ARR,30-ARR_v2_61@4,30-ARR_v1_65@4,"Finally, the performance of our methods is better that the state-of-the-art manual CAD method in few-shot learning scenarios on all OOD datasets.","Finally, the performance of our methods outperforms another state-of-the-art manual CAD method in fewshot learning scenarios on all OOD datasets.","Modify,Clarity",Clarity
3125,30-ARR,30-ARR_v2_63@2,30-ARR_v1_68@2,"As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increase in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.","As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increasing in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.","Modify,Grammar",Grammar
3126,30-ARR,30-ARR_v2_2@1,30-ARR_v1_2@1,"By using static semi-factual generation and dynamic humanintervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.","By using static semi-factual generation and dynamic human-intervened correction, RDL, acting like a sensible ""inductive bias"", exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.","Modify,Fact/Evidence",Fact/Evidence
3127,30-ARR,30-ARR_v2_12@4,30-ARR_v1_13@4,"In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002;Kenny and Keane, 2021), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same).","In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same).","Modify,Fact/Evidence",Fact/Evidence
3128,30-ARR,30-ARR_v2_12@6,30-ARR_v1_13@6,"We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power, both for in-distribution and OOD predictions.","We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power both for in-distribution and OOD predictions.","Modify,Grammar",Grammar
3129,30-ARR,30-ARR_v2_2@2,30-ARR_v1_2@2,Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarks-especially for few-shot learning scenarios.,"Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests, especially for few-shot learning scenarios, compared to many state-of-the-art benchmarks.","Modify,Clarity",Clarity
3130,30-ARR,30-ARR_v2_13@7,30-ARR_v1_17@7,"Human-the-loop Machine Learning (Wu et al., 2021) has received increasing research attention.","Human-the-loop Machine Learning (Wu et al., 2021a) has received increasing research attention.","Modify,Fact/Evidence",Fact/Evidence
3131,30-ARR,30-ARR_v2_14@1,30-ARR_v1_17@12,"Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights.","Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of some model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights.","Modify,Clarity",Clarity
3132,30-ARR,30-ARR_v2_14@2,30-ARR_v1_17@13,"However, previous work focuses on increasing the explainability of AI systems for high-stakes domains such as health and finance (Li et al., 2020;Yang et al., 2020b), instead of improving model robustness or generalisation ability.","However, previous work focuses on increasing the explainability of AI systems for high-stake domains such as health and finance, instead of improving the model robustness or generalisation ability.","Modify,Fact/Evidence",Fact/Evidence
3284,315-ARR,,315-ARR_v1_32@8,,Drop#1 is probably because the constraint words within this range are mostly functional or less important.,"Delete,Claim",Claim
3285,315-ARR,,315-ARR_v1_32@9,,Such words are not as universal as ones at the leftmost that can fit in most contexts and do not have to appear in the target due to multiple modes in translation.,"Delete,Claim",Claim
3286,315-ARR,,315-ARR_v1_73@2,,"The purpose of this work is to enhance existing methods for constrained NAT, i.e., editing-based iterative NAT methods, under rare lexical constraints.","Delete,Fact/Evidence",Fact/Evidence
3287,315-ARR,,315-ARR_v1_73@3,,"It would be interesting for future research to explore new ways to impose lexical constraints on NAT models, perhaps on non-iterative NAT.","Delete,Claim",Claim
3288,315-ARR,315-ARR_v2_31@3,,"Note that Drop#1 is mainly due to the the fact that there are mostly unknown tokens (i.e., <UNK>) in the bucket 2.",,"Add,Claim",Claim
3289,315-ARR,315-ARR_v2_31@4,,We leave detailed discussions about buckets and Drop#1 to Appendix C.,,"Add,Fact/Evidence",Fact/Evidence
3290,315-ARR,315-ARR_v2_41@1,,"For each translated sentence constraints C tgt = (C 1 , C 2 , ..., C k ), we use an external alignment tool external aligner, such as GIZA++ (Brown et al., 1993;Och and Ney, 2003), to find the corresponding source words, denoted as ment embedding that comes from C src and C tgt .",,"Add,Fact/Evidence",Fact/Evidence
3291,315-ARR,315-ARR_v2_74@0,,"In this section, we dig further into the buckets in self-constrained translation ( §3.3, §6.1), especially for understanding why Drop#1 happens.",,"Add,Fact/Evidence",Fact/Evidence
3292,315-ARR,315-ARR_v2_75@0,,"As seen in Table 9, we categorize and count the constraints into five classes based on their Part-Of-Speech tagging with NLTK (Bird et al., 2009).",,"Add,Fact/Evidence",Fact/Evidence
3293,315-ARR,315-ARR_v2_75@1,,"We find that, 1) punctuation (PUNK) dominates bucket 1; 2) as the constraint frequency decreases (from bucket 1 to bucket 6), the number of constraints identified as nouns (NN*) grows; 3) bucket 2 has the most UNK constraints.",,"Add,Fact/Evidence",Fact/Evidence
3294,315-ARR,315-ARR_v2_75@2,,"The third finding is because, as the BPE training was only done on the training set of the datasets, there will be <UNK> on the target side of the test set.",,"Add,Fact/Evidence",Fact/Evidence
3295,315-ARR,315-ARR_v2_75@3,,"Thus, cases in bucket 2 have a relatively large number of UNK tokens as constraints, resulting in the Drop#1.",,"Add,Fact/Evidence",Fact/Evidence
3296,315-ARR,315-ARR_v2_76@0,,"To give a clearer view about how is UNK causing Drop#1, we exclude samples with UNK as constraints, and obtain a revised self-constrained translation results, as in Figure 5.",,"Add,Fact/Evidence",Fact/Evidence
3297,315-ARR,315-ARR_v2_76@1,,"Clearly, Drop#1 disappears in the given setting.",,"Add,Fact/Evidence",Fact/Evidence
3298,315-ARR,315-ARR_v2_76@2,,"Of course, Drop#2 still verifies our claim in the paper.",,"Add,Fact/Evidence",Fact/Evidence
3299,315-ARR,315-ARR_v2_31@2,315-ARR_v1_32@7,"However, there are two noticeable performance drops around relative frequency ranges of 10%-30% (bucket 2) and 90%-100% (bucket 6), denoted as Drop#1 (-0.3 BLEU) and Drop#2 (-0.6 BLEU).","However, there are two noticeable performance drops around relative frequency ranges of 10%-30% and 90%-100%, denoted as Drop#1 (-0.3 BLEU) and Drop#2 (-0.6 BLEU).","Modify,Fact/Evidence",Fact/Evidence
3300,315-ARR,315-ARR_v2_38@1,315-ARR_v1_39@1,We first build pseudo terms from the target by sampling 0-3 words (more tokens after tokenization) from reference as the pre-defined constraints for training.,We first build pseudo terms from the target by sampling 0-3 words from reference as the pre-defined constraints for training.,"Modify,Fact/Evidence",Fact/Evidence
3301,315-ARR,315-ARR_v2_40@4,315-ARR_v1_42@2,"As illustrated in Figure 2, we propose to directly align the target-side constraints with the source words and prompt the alignment information to the model during both training and inference.",We propose to directly align the target-side constraints with the source words and prompt the alignment information to the model during both training and inference.,"Modify,Fact/Evidence",Fact/Evidence
3302,315-ARR,315-ARR_v2_54@2,315-ARR_v1_59@2,"When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do.","When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference.","Modify,Claim",Claim
3303,315-ARR,315-ARR_v2_21@0,315-ARR_v1_22@0,Iterative refinement by editing is an NAT paradigm that suits constrained translations due to its flexibility.,"For NATs, iterative refinement by editing is an NAT paradigm that suits constrained translations due to its flexibility.","Modify,Clarity",Clarity
3336,32-ARR,32-ARR_v2_2@7,,The code will be made available at: https://github.com/yiren-jian/EmbedHalluc.,,"Add,Fact/Evidence",Fact/Evidence
3337,32-ARR,32-ARR_v2_10@7,,"Learning from limited labeled data (few-shot learning) in Computer Vision is usually achieved by meta-learning (Ren et al., 2018a,b;Jian et al., 2020;Jian and Gao, 2021) or transfer learning (Tian et al., 2020).",,"Add,Fact/Evidence",Fact/Evidence
3338,32-ARR,32-ARR_v2_22@1,,The pseudo-code for finetuning of few-shot language learners with hallucinated embeddings is shown in Algorithm 1.,,"Add,Fact/Evidence",Fact/Evidence
3339,32-ARR,32-ARR_v2_26@1,,The evaluations are conducted by averaging results on 5 different train test splits.,,"Add,Fact/Evidence",Fact/Evidence
3340,32-ARR,32-ARR_v2_26@2,,We sample 16 examples per class to form a training set and construct a validation set with the same size as the training set.,,"Add,Fact/Evidence",Fact/Evidence
3341,32-ARR,32-ARR_v2_27@0,,Training Details for Embedding Hallucinators,,"Add,Other",Other
3342,32-ARR,32-ARR_v2_37@1,,"The discriminator is a 3blocks model, each bock having a sequence of FullyConnect-BatchNorm-LeakyReLU with the same hidden dimension of 512.",,"Add,Fact/Evidence",Fact/Evidence
3343,32-ARR,32-ARR_v2_38@0,,"We train the Embedding Hallucinators for 150 epochs using a batch size of 64, the Adam optimizer (β = (0.5, 0.999)), and a learning rate of 0.0002.",,"Add,Fact/Evidence",Fact/Evidence
3344,32-ARR,32-ARR_v2_38@1,,The real embeddings are collected from the language few-shot training set by passing text into the embedding layer of the language model.,,"Add,Fact/Evidence",Fact/Evidence
3345,32-ARR,32-ARR_v2_38@2,,We apply gradient penalty with weight of loss 100 for training the cWGAN.,,"Add,Fact/Evidence",Fact/Evidence
3346,32-ARR,32-ARR_v2_39@0,,Training Details for Few-Shot Language Learners,,"Add,Other",Other
3347,32-ARR,32-ARR_v2_40@0,,"We draw two mini-batches during the training of our few-shot language learners, i.e., one from the real language few-shot training set, another one by sampling the hallucinators (see Algorithm 1).",,"Add,Fact/Evidence",Fact/Evidence
3348,32-ARR,32-ARR_v2_52@0,,Comparing to Adversarial Training,,"Add,Other",Other
3349,32-ARR,32-ARR_v2_53@0,,Adversarial training adds noise into the training data to increase the robustness of a model.,,"Add,Claim",Claim
3350,32-ARR,32-ARR_v2_53@1,,It has been shown that adversarial training can also improve the performance of language models.,,"Add,Claim",Claim
3351,32-ARR,32-ARR_v2_61@0,,"Here, we provide best learning rates (LR, searched from 1e −5 , 5e −6 , 1e −6 as discussed in main paper) for L halluc of EmbedHalluc for each task used in RoBERTa-large prompt-based fine-tuning.",,"Add,Fact/Evidence",Fact/Evidence
3352,32-ARR,32-ARR_v2_42@2,32-ARR_v1_25@2,The algorithm is implemented in PyTorch-1.10 and experiments are conducted on Nvidia RTX-6000 and RTX-A6000 GPU.,The algorithm is implemented in PyTorch-1.10 and experiments are conducted on Nvidia RTX-A6000 GPU with 48GB of memory.,"Modify,Fact/Evidence",Fact/Evidence
3353,32-ARR,32-ARR_v2_4@2,32-ARR_v1_4@2,"Data augmentation (Wei and Zou, 2019), regularization and re-initialization further improve the results.","Data augmentation (Wei and Zou, 2019), regularization and re-initialization (Zhang et al., 2021) further improve the results.","Modify,Fact/Evidence",Fact/Evidence
3354,32-ARR,32-ARR_v2_7@0,32-ARR_v1_6@0,Prior work have proposed regularization methods to overcome this problem .,"Prior work have proposed regularization methods to overcome this problem Zhang et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3355,32-ARR,32-ARR_v2_0@0,32-ARR_v1_0@0,Embedding Hallucination for Few-Shot Language Fine-tuning,Embedding Hallucination for Few-shot Language Fine-tuning,"Modify,Grammar",Grammar
3356,32-ARR,32-ARR_v2_8@0,32-ARR_v1_7@0,"Current common text data augmentation methods, such as EDA (Wei and Zou, 2019) (which have been used in recent few-shot learning papers (Wei et al., 2021;Basu et al., 2021)) and AEDA (Karimi et al., 2021) operate at the lexical level, which while resulting in human readable texts, lead to limited diversity due to the discrete nature of the lexical space.","Current common text data augmentation methods, such as EDA (Wei and Zou, 2019) (which have been used in recent few-shot learning papers (Wei et al., 2021;Basu et al., 2021)) and AEDA (Karimi et al., 2021a) operate at the lexical level, which while resulting in human readable texts, lead to limited diversity due to the discrete nature of the lexical space.","Modify,Fact/Evidence",Fact/Evidence
3357,32-ARR,32-ARR_v2_8@8,32-ARR_v1_7@8,"We further experimentally show the overall superiority of EmbedHalluc when comparing to regularization methods proposed to address the problem of over-fitting during fine-tuning of LMs, such as Mixout and Re-Init .","We further experimentally show the overall superiority of EmbedHalluc when comparing to regularization methods proposed to address the problem of over-fitting during fine-tuning of LMs, such as Mixout and Re-Init (Zhang et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3358,32-ARR,32-ARR_v2_10@2,32-ARR_v1_9@2,Other tricks include bias correction in optimizer and re-initialization of top layers in Transformer .,"Other tricks include bias correction in optimizer and re-initialization of top layers in Transformer (Zhang et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3359,32-ARR,32-ARR_v2_10@8,32-ARR_v1_9@8,"In NLP, few-shot learning has been successfully applied to machine translation (Arthaud et al., 2021), abstract summarizing (Fabbri et al., 2021), question and answering (Hua et al., 2020;Ram et al., 2021), and entity recognition (de Lichy et al., 2021;Tong et al., 2021;Ding et al., 2021), by meta learning (Li and Zhang, 2021;Bansal et al., 2020;Sharaf et al., 2020), data augmentation (Wei et al., 2021;Wei and Zou, 2019;Karimi et al., 2021;, and prompts Tam et al., 2021).","Few-shot learning has been successfully applied to machine translation (Arthaud et al., 2021), abstract summarizing (Fabbri et al., 2021), question and answering (Hua et al., 2020;Ram et al., 2021), and entity recognition (de Lichy et al., 2021;Tong et al., 2021;Ding et al., 2021), by meta learning (Li and Zhang, 2021;Bansal et al., 2020;Sharaf et al., 2020), data augmentation (Wei et al., 2021;Wei and Zou, 2019;Karimi et al., 2021b), and prompts (Gao et al., 2021;Tam et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3360,32-ARR,32-ARR_v2_14@0,32-ARR_v1_13@0,"GAN (Goodfellow et al., 2014) has led the revolution of generative models to achieve impressive results in synthesizing images (Zhu et al., 2017) and higher dimensional data .","GAN (Goodfellow et al., 2014) has led the revolution of generative models to achieve impressive results in synthesizing images (Zhu et al., 2017) and higher dimensional data (Wang et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
3361,32-ARR,32-ARR_v2_18@3,32-ARR_v1_17@3,"We propose Label Calibration (LabelCalib) by pseudolabeling from a teacher model F GEN0 (LM 1 in Algorithm 1), where F GEN0 is first fine-tuned on the original training set (without augmentation).","We propose Label Calibration (LabelCalib) by pseudo-labeling from a teacher model F GEN0 , where F GEN0 is first fine-tuned on the original training set (without augmentation).","Modify,Fact/Evidence",Fact/Evidence
3379,326-ARR,,326-ARR_v1_54@3,,All datasets are modified into truecase format with mosesdecoder by training truecase models upon train set.,"Delete,Fact/Evidence",Fact/Evidence
3380,326-ARR,,326-ARR_v1_59@1,,"Note that, we follow suggestions in Song et al. (2021) to omit the energy calculation of activate functions, such as relu and softmax, because they are specially designed over some modern AI chips, which requires far less energy than additive operation.","Delete,Fact/Evidence",Fact/Evidence
3381,326-ARR,,326-ARR_v1_65@0,,"In order to obtain value representations, attention model requires ld 2 additive/multiplicative operations.","Delete,Fact/Evidence",Fact/Evidence
3382,326-ARR,,326-ARR_v1_65@1,,"Besides, applying weighted sum over value representations with attention weights requires l 2 d multiplicative/additive operations.","Delete,Fact/Evidence",Fact/Evidence
3383,326-ARR,,326-ARR_v1_65@2,,"Overall, the numbers of multiplicative/additive operations in the whole attention model are 3ld 2 + 2l 2 d.","Delete,Fact/Evidence",Fact/Evidence
3384,326-ARR,,326-ARR_v1_66@1,,The overall multiplication operations in a Transformer Block is 9ld 2 + 3ld 2 + 2l 2 d = 12ld 2 + 2l 2 d.,"Delete,Fact/Evidence",Fact/Evidence
3385,326-ARR,,326-ARR_v1_67@0,,"Same as the steps above, we can calculate the required energy consumption of other modules.","Delete,Fact/Evidence",Fact/Evidence
3386,326-ARR,,326-ARR_v1_68@1,,"We can get the result ∆ A = 34.09%, ∆ F = 33.83%, thus the energy reduction ratio is 1−∆ A = 65.91%, 1−∆ F = 66.17%.","Delete,Fact/Evidence",Fact/Evidence
3387,326-ARR,,326-ARR_v1_68@2,,"Similarly, energy consumption ratios at the level of alignment calculation are 99.55% and 99.95%.","Delete,Fact/Evidence",Fact/Evidence
3388,326-ARR,,326-ARR_v1_68@3,,"Those of TRANSFORMER block are 83.17% and 83.10%, respectively.","Delete,Fact/Evidence",Fact/Evidence
3389,326-ARR,326-ARR_v2_57@0,,Binarization Statistics,,"Add,Other",Other
3390,326-ARR,,326-ARR_v1_20@5,,"Although Dense and RandInit significantly reduce the energy consumption, Tay et al. (2021) point out that these approaches fail to be employed into cross-attention networks, since neither linear transition nor randomly initialized matrix is able to exactly model alignment information across languages.","Delete,Fact/Evidence",Fact/Evidence
3391,326-ARR,,326-ARR_v1_2@5,,Our code will be released upon the acceptance.,"Delete,Claim",Claim
3392,326-ARR,,326-ARR_v1_40@0,,"We choose three machine translation tasks, i.e. IWSLT'15 English-Vietnamese (En-Vi), WMT'14 English-German (En-De) and WMT'17 Chinese-English (Zh-En), to evaluate the effectiveness of our approach.","Delete,Fact/Evidence",Fact/Evidence
3393,326-ARR,,326-ARR_v1_40@1,,"We follow the setting of TRANS-FORMER-Base (Vaswani et al., 2017)","Delete,Fact/Evidence",Fact/Evidence
3394,326-ARR,326-ARR_v2_56@1,326-ARR_v1_51@1,"As seen, our model can give good aligned information, where preposition phrase ""around 50 years ago"" is arranged at the end of sentence in English, while its aligned phrase is at front in Chinese.","As seen, our model can give good aligned information, where preposition phrase ""around 50 years ago"" is arranged at the end of sentence in English, while its aligned phrase is at the front in Chinese.","Modify,Grammar",Grammar
3395,326-ARR,326-ARR_v2_60@4,326-ARR_v1_53@4,"Although we have shown the superiority of E-ATT, considering the whole TRANSFORMER block 2 , the use of E-ATT brings 17% energy reduction.","Although we have shown the superiority of E-ATT, considering the whole Transformer block 2 , the use of E-ATT brings 17% energy reduction.","Modify,Grammar",Grammar
3396,326-ARR,326-ARR_v2_60@6,326-ARR_v1_53@6,It is worth to further design techniques that reduce the energy cost of other modules in TRANSFORMER.,It is worth to further design techniques that reduce the energy cost of other modules in Transformer.,"Modify,Grammar",Grammar
3397,326-ARR,326-ARR_v2_39@0,326-ARR_v1_39@0,Dataset Preprocessing,Dataset and Model Setting,"Split+Modify,Other",Other
3398,326-ARR,326-ARR_v2_42@0,326-ARR_v1_39@0,Experimental Setting,Dataset and Model Setting,"Split+Modify,Clarity",Clarity
3399,327-ARR,,327-ARR_v1_2@5,,We compute them on a model's trained token embeddings to prevent domain mismatch.,"Delete,Fact/Evidence",Fact/Evidence
3400,327-ARR,327-ARR_v2_40@1,327-ARR_v1_41@1,"This baseline helps reduce variance in REINFORCE (Williams, 1992).",This baseline helps reduce variance in REINFORCE.,"Modify,Fact/Evidence",Fact/Evidence
3401,327-ARR,327-ARR_v2_2@6,327-ARR_v1_2@7,"Additionally, we find that this model enjoys stable training relative to a non-RL setting.","In addition, we find that this approach enjoys stable training compared to a non-RL setting.","Modify,Clarity",Clarity
3402,327-ARR,327-ARR_v2_57@1,327-ARR_v1_58@1,It scores higher than XENT by 1.28 BLEU and 0.71 BLEURT points.,It scores higher than the XENT baseline by 1.28 BLEU and 0.71 BLEURT points.,"Modify,Clarity",Clarity
3403,327-ARR,327-ARR_v2_57@2,327-ARR_v1_58@2,"Although BL-R was specially trained to optimize BLEU, F DISC still outperforms it by over half a point on that metric.","Although BL-R was specially trained to optimize BLEU, F DISC still outperforms it by over half a point that metric.","Modify,Grammar",Grammar
3404,327-ARR,327-ARR_v2_65@0,327-ARR_v1_65@0,This paper proposes new F1 score metrics based on optimized rather than greedy alignments between predicted and reference tokens.,This paper proposed new F1 score metrics based on optimized rather than greedy alignments between predicted and reference tokens.,"Modify,Grammar",Grammar
3405,327-ARR,327-ARR_v2_65@1,327-ARR_v1_65@1,"Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extract alignments as a constrained optimization problem.","Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extracted alignments as a constrained optimization problem.","Modify,Grammar",Grammar
3406,327-ARR,327-ARR_v2_65@3,327-ARR_v1_65@3,"In the continuous case, we find alignments that minimize earth mover's distance between the two token embedding distributions.","In the continuous case, we found alignments that minimized earth mover's distance between the two token embedding distributions.","Modify,Grammar",Grammar
3407,327-ARR,327-ARR_v2_66@0,327-ARR_v1_66@0,"We apply new metrics as rewards during RLbased training for AMR-to-text generation, with F DISC outperforming both a cross-entropy baseline and one optimizing BLEU rewards.","We applied new metrics as rewards during RLbased training for AMR-to-text generation, with F DISC outperforming both a cross-entropy baseline and one optimizing BLEU rewards.","Modify,Grammar",Grammar
3408,327-ARR,327-ARR_v2_66@1,327-ARR_v1_66@1,"Despite being computed on a downstream model's token embeddings, the metrics still provide informative rewards during training without signs of overfitting.","Despite being computed on a downstream model's token embeddings, the metrics still provided informative rewards during training without signs of overfitting.","Modify,Grammar",Grammar
3409,327-ARR,327-ARR_v2_2@1,327-ARR_v1_2@1,Metrics leveraging contextualized embeddings appear more flexible than those that match n-grams and thus ideal as training rewards.,Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards.,"Modify,Clarity",Clarity
3410,327-ARR,327-ARR_v2_2@2,327-ARR_v1_2@2,"Yet metrics such as BERTSCORE greedily align candidate and reference tokens, which can give system outputs excess credit relative to a reference.","However, metrics such as BERTSCORE greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference.","Modify,Clarity",Clarity
3411,327-ARR,327-ARR_v2_2@3,327-ARR_v1_2@3,Past systems using such semantic similarity rewards further suffer from repetitive outputs and overfitting.,"Furthermore, past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting.","Modify,Clarity",Clarity
3412,327-ARR,327-ARR_v2_16@0,327-ARR_v1_17@0,"Aligning tokens between hypothesis and reference can be seen as an assignment problem, where a token pair (ŷ i , y j ) is highly weighted if it incurs low cost (i.e., distance).","Aligning tokens between hypothesis and reference can be seen as an assignment problem, where a token pair (ŷ i , y j ) is highly weighted if it incurs low cost (i.e. distance).","Modify,Grammar",Grammar
3413,327-ARR,327-ARR_v2_17@1,327-ARR_v1_18@1,"For the latter, we extract alignments from the earth mover's distance (EMD, Villani, 2009;Peyré and Cuturi, 2019) transport matrix.","For the latter, we extract alignments from the earth mover's distance (EMD, Villani, 2009;Peyré et al., 2019) transport matrix.","Modify,Fact/Evidence",Fact/Evidence
3414,327-ARR,327-ARR_v2_2@4,327-ARR_v1_2@4,"To address these issues, we propose metrics that replace the greedy alignments in BERTSCORE with optimized ones.",We address these issues by proposing metrics that replace the greedy alignments in BERTSCORE with optimized ones.,"Modify,Clarity",Clarity
3562,344-ARR,,344-ARR_v1_37@7,,Domain Setups.,"Delete,Other",Other
3563,344-ARR,,344-ARR_v1_49@2,,17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups.,"Delete,Fact/Evidence",Fact/Evidence
3564,344-ARR,,344-ARR_v1_49@3,,"We repeated the similar hyper-parameter search procedure for QA-based models, using ALB-QA.","Delete,Fact/Evidence",Fact/Evidence
3565,344-ARR,344-ARR_v2_6@10,,"These labels can be seen as sub-intent annotations, where their combinations yield full intents equivalent to ""traditional"" intents (Table 1).",,"Add,Fact/Evidence",Fact/Evidence
3566,344-ARR,344-ARR_v2_23@5,,"For instance, if (i) examples with the intents change and booking, and (ii) examples with the intents cancel and account exist in the training data, (iii) an unseen example with the intents cancel and booking could be properly predicted, as all the single intents/modules have already been seen by the ID model 9 .",,"Add,Claim",Claim
3567,344-ARR,344-ARR_v2_23@7,,"For example, the module overdraft is clearly related to BANKING, but the module change is much more generic, likely to occur in several different domains.",,"Add,Claim",Claim
3568,344-ARR,344-ARR_v2_61@3,,"Further, the boundaries of some generic intents can sometimes be unclear and difficult to annotate, even for expert annotators.",,"Add,Claim",Claim
3569,344-ARR,344-ARR_v2_61@4,,18 Future work should try to ground the set of generic intents.,,"Add,Claim",Claim
3570,344-ARR,344-ARR_v2_62@0,,"Further, we believe that span-based annotation might be sub-optimal for canonical values such as times and dates, where small differences in the span would lead to evaluation errors but would not suppose a problem for the value to be parsed.",,"Add,Claim",Claim
3571,344-ARR,344-ARR_v2_62@1,,"In addition, separating time and date intervals in different slots increases the difficulty of the annotations and models need to learn a more conflicting set of slots.",,"Add,Claim",Claim
3572,344-ARR,344-ARR_v2_63@0,,"Finally, while single-turn NLU is more dataefficient and easier to model, some user utterances only make sense in the presence of context from the previous system utterance.",,"Add,Claim",Claim
3573,344-ARR,344-ARR_v2_63@1,,"While some previous datasets (Coope et al., 2020) deal with this issue with the help of extra annotations indicating if a slot has been requested, in this work we opt for using non-contextualised slots such as number and time and let the policy handle the contextualisation.",,"Add,Fact/Evidence",Fact/Evidence
3574,344-ARR,344-ARR_v2_63@2,,"However, future work should start looking into NLU datasets composed by system + user turns.",,"Add,Claim",Claim
3575,344-ARR,344-ARR_v2_16@1,344-ARR_v1_16@1,"4) Current NLU datasets do not combine a large set of fine-grained intents (again, with multiintent examples) and a large set of fine-grained slots, which prevents proper and more insightful evaluations of joint NLU models (Chen et al., 2019;Gangadharaiah and Narayanaswamy, 2019).","4) Current NLU datasets do not combine a large set of fine-grained intents (again, with multiintent examples) and a large set of fine-grained slots, which prevents proper and more insightful evaluations of joint NLU models Gangadharaiah and Narayanaswamy, 2019).","Modify,Fact/Evidence",Fact/Evidence
3576,344-ARR,344-ARR_v2_22@0,344-ARR_v1_22@0,"One of the main contributions of this work is the novel design of the intent space, defined in a highly modular manner that natively supports intent re-combinations and multi-intent annotations 7 .","One of the main contributions of this work is the design of the intent space, defined in a highly modular manner that natively supports intent recombinations and multi-intent annotations.","Modify,Clarity",Clarity
3577,344-ARR,344-ARR_v2_2@5,344-ARR_v1_3@1,"Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, the validity of 'intent modularisation', and call for further research on ToD NLU.","Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, and call for further research on ToD NLU.","Modify,Fact/Evidence",Fact/Evidence
3578,344-ARR,344-ARR_v2_23@1,344-ARR_v1_22@4,"1) The modular nature of the ontology allows for expressing a much more complex set of ideas through different combinations of intent modules (see Table 3), while reducing the overall size of the intent set compared to previous ID datasets 8 (see Table 1 and Table 5).","1) The modular nature of the ontology allows for expressing a much more complex set of ideas through different combinations of intent modules (see Table 2), while reducing the overall size of the intent set compared to previous ID datasets 7 (see Table 4).","Modify,Fact/Evidence",Fact/Evidence
3579,344-ARR,344-ARR_v2_23@2,344-ARR_v1_23@0,"2) It allows for the definition of partial intents (e.g., ""The savings one"").","2) It allows for the definition of partial intents (e.g. ""The savings one"").","Modify,Grammar",Grammar
3580,344-ARR,344-ARR_v2_23@3,344-ARR_v1_23@1,"This is crucial in multi-turn interactions, where the user often has to answer disambiguation questions (e.g., ""Which account would you like to close?"").","This is crucial in multi-turn interactions, where the user often has to answer disambiguation questions (e.g. ""Which account would you like to close?"").","Modify,Grammar",Grammar
3581,344-ARR,344-ARR_v2_27@0,344-ARR_v1_28@0,"Similarly to intent modules, slots can also be divided into the generic ones (e.g. time, date) and the domain-specific ones (e.g company_name, rooms, kids), see Table 10.","Similarly to intent modules, slots can also be divided into the generic ones (e.g. time, date) and the domain-specific ones (e.g company_name, rooms, kids) (see Table 9).","Modify,Clarity",Clarity
3582,344-ARR,344-ARR_v2_29@0,344-ARR_v1_30@0,"Previous NLU datasets have usually relied on crowdworkers, aiming to collect a large number of examples, and typically optimising for quantity over quality.","Previous NLU datasets have usually relied on crowdworkers, aiming to collect a large numbers of examples, and typically optimising for quantity over quality.","Modify,Grammar",Grammar
3583,344-ARR,344-ARR_v2_43@1,344-ARR_v1_44@1,"Another group of SotA ID baselines reformulates the ID task into the (extractive) question-answering (QA) problem (Namazifar et al., 2021;Fuisz et al., 2022).","Another group of SotA ID baselines reformulates the ID task into the (extractive) question-answering (QA) problem (Namazifar et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3584,344-ARR,344-ARR_v2_58@4,344-ARR_v1_58@4,"For instance, we observe particularly high scores for highly generic and reusable intent modules such as change, how, how_much, thank, when, and affirm, all with per-intent F 1 scores of ≥ 90.","For instance, we observe particularly high scores for highly generic and reusable intent modules such as change, how, how_much, thank, when, and affirm, all with per-intent F 1 scores ≥ 90.","Modify,Grammar",Grammar
3585,344-ARR,344-ARR_v2_62@3,344-ARR_v1_61@4,Future work includes rethinking the SL task for these slots.,Future work should also look into alternatives to fine-grained slot annotations for such slots.,"Modify,Clarity",Clarity
3586,344-ARR,344-ARR_v2_65@2,344-ARR_v1_62@2,"Upon collection, the dataset has undergone an additional check by the internal Ethics committee of the company.","Upon collection, the dataset has undergone an additional check by the internal Ethics committee.","Modify,Clarity",Clarity
3587,344-ARR,344-ARR_v2_65@3,344-ARR_v1_62@3,NLU++ is licensed under CC-BY-4.0.,It is licensed under CC BY 4.0.,"Modify,Clarity",Clarity
3588,344-ARR,344-ARR_v2_69@1,344-ARR_v1_65@1,"Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) using a dual-encoder framework (Yang et al., 2019) with larger embedding capacity (i.e., a shared multilingual vocabulary of 500k subwords).","Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) using a dual-encoder framework with larger embedding capacity (i.e., a shared multilingual vocabulary of 500k subwords).","Modify,Fact/Evidence",Fact/Evidence
3589,344-ARR,344-ARR_v2_6@9,344-ARR_v1_5@18,"Unlike previous ID datasets, examples are annotated with multiple labels, named intent modules 3 (see Table 1), with some examples naturally obtaining even up to 6-7 labels.","Unlike previous ID datasets, examples are annotated with multiple labels, with some examples naturally obtaining even up to 6-7 labels.","Modify,Fact/Evidence",Fact/Evidence
3590,344-ARR,344-ARR_v2_6@11,344-ARR_v1_5@19,"In addition, NLU++ defines a rich set of slots which are combined with the multi-intent sentences.",NLU++ defines a rich set of slots which are combined with multi-intent sentences.,"Modify,Clarity",Clarity
3591,344-ARR,344-ARR_v2_7@1,344-ARR_v1_6@1,"Our benchmark comparisons also demonstrate strong performance and shed new light on the (ability of) recently emerging QA-based NLU models (Namazifar et al., 2021;Fuisz et al., 2022), and warrant further research on ToD NLU.","Our benchmark comparisons also demonstrate strong performance and shed new light on the (ability of) recently emerging QA-based NLU models, and warrant further research on ToD NLU.","Modify,Fact/Evidence",Fact/Evidence
3592,344-ARR,344-ARR_v2_2@2,344-ARR_v1_2@2,"1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences, introducing and validating the idea of intent modules that can be combined into complex intents that convey complex user goals, combined with finer-grained and thus more challenging slot sets.",1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences combined with finer-grained and thus more challenging slot sets.,"Modify,Claim",Claim
3593,344-ARR,344-ARR_v2_10@0,344-ARR_v1_9@0,"In order to adapt to the increasing data requirements of deep learning models, increasingly larger dialogue datasets have been released in recent years (Budzianowski et al., 2018;Wei et al., 2018;Rastogi et al., 2019;Peskov et al., 2019).","To adapt to the increasing data requirements of deep learning models, increasingly larger dialogue datasets have been released in recent years (Budzianowski et al., 2018;Wei et al., 2018;Rastogi et al., 2019;Peskov et al., 2019).","Modify,Clarity",Clarity
3594,344-ARR,344-ARR_v2_11@0,344-ARR_v1_10@0,"2) The domain-specific ontologies required a lot of expertise for annotation, therefore many annotation mistakes were made (Eric et al., 2019;Zang et al., 2020).","2) The domain-specific ontologies required a lot of expertise for annotation, therefore many annotation mistakes were made (Eric et al., 2019;","Modify,Fact/Evidence",Fact/Evidence
3595,344-ARR,344-ARR_v2_2@3,344-ARR_v1_2@3,"2) The ontology is divided into domain-specific and generic (i.e., domainuniversal) intent modules that overlap across domains, promoting cross-domain reusability of annotated examples.","2) The ontology is divided into domain-specific and generic (i.e., domainuniversal) intents that overlap across domains, promoting cross-domain reusability of annotated examples.","Modify,Clarity",Clarity
3723,350-ARR,,350-ARR_v1_52@3,,"For this language XLSR-53 training converges unusually fast and floors to a high PER; in contrast, HMM-GMM and k2-conf results on this language are their best in the private dataset.","Delete,Fact/Evidence",Fact/Evidence
3724,350-ARR,,350-ARR_v1_52@5,,"The HMM-GMM system obtains the second-lowest average PER (41.1%) for public languages but is third for the private dataset, with 13.2% PER.","Delete,Fact/Evidence",Fact/Evidence
3725,350-ARR,350-ARR_v2_6@2,,"We think it fair to include such models, as we aim at a practical solution for the transcription problem at hand, regardless of the underlying approach.",,"Add,Claim",Claim
3726,350-ARR,350-ARR_v2_53@0,,It was feasible to train HMM-GMM with 10 different random train/test partitions 13 and compute the Student's t 95% uncertainty intervals shown in the PER column.,,"Add,Fact/Evidence",Fact/Evidence
3727,350-ARR,350-ARR_v2_53@1,,The uncertainty remains relatively small even for the smallest datasets which contain only a few minutes of test speech.,,"Add,Fact/Evidence",Fact/Evidence
3728,350-ARR,350-ARR_v2_37@5,350-ARR_v1_37@5,The test set is used only for measuring phoneme error rate and is not involved in any tuning.,The test set was used only for measuring phoneme error rate and was not involved in any tuning.,"Modify,Grammar",Grammar
3729,350-ARR,350-ARR_v2_4@0,350-ARR_v1_4@0,Recent progress in automatic speech recognition (ASR) was made by training neural networks on increasingly large amounts of annotated data.,Recent progress in automatic speech recognition (ASR) has been obtained by training neural networks on increasingly large amounts of annotated data.,"Modify,Clarity",Clarity
3730,350-ARR,350-ARR_v2_49@1,350-ARR_v1_49@1,"Note that the nru33 subset is used here rather than the full nru, to make it more comparable with other languages.","Note that the nru33 subset is used here rather than the full nru, to make it more comparable.","Modify,Clarity",Clarity
3731,350-ARR,350-ARR_v2_50@0,350-ARR_v1_49@3,Phoneme error rates (PER) reported are obtained using the speaker turn segmentation from the transcript.,Phoneme error rates reported are obtained using the speaker turn segmentation from the transcript.,"Modify,Clarity",Clarity
3732,350-ARR,350-ARR_v2_52@1,350-ARR_v1_52@0,"Here, phoneme error rate (PER) columns in Table 3 show that pretrained XLSR-53 outperforms other models for all languages in public datasets.","Looking at the phoneme error rate (PER) columns in Table 3, XLSR-53 is seen to outperform the other models for all languages in the public dataset, with an average of 13.6% PER.","Modify,Fact/Evidence",Fact/Evidence
3733,350-ARR,350-ARR_v2_52@2,350-ARR_v1_52@1,"In one case (mlv), it obtains 8.6% PER with only 20 minutes of training.","In one case, it obtains 8.6% PER with only 20 minutes of training.","Modify,Clarity",Clarity
3734,350-ARR,350-ARR_v2_52@3,350-ARR_v1_52@2,"Similarly in the private dataset, Table 4 shows XLSR-53 outperforming the other models for all languages.","Similarly in the private dataset, Table 4 shows XLSR-53 outperforming the other models for all languages, except for Tsuut'ina (srs).","Modify,Fact/Evidence",Fact/Evidence
3735,350-ARR,350-ARR_v2_52@4,350-ARR_v1_52@4,"Note that the HMM-GMM result for Cree (crl) is 13.0% PER, slightly better than for the HMM-BLSTM model without LM result from (Gupta and Boulianne, 2020).","Note that the HMM-GMM result for Cree (crl) is 14.2% PER, slightly better than previously reported for an HMM-BLSTM deep recursive model (Gupta and Boulianne, 2020).","Modify,Fact/Evidence",Fact/Evidence
3736,350-ARR,350-ARR_v2_54@1,350-ARR_v1_53@1,"This can be seen in Table 5, where we summarized results from Tables 3 and 4 Table 5 shows that with over 99 minutes, HMM-GMM, XLSR and k2-conf have a PER of 13.8% or less.","This can be seen in Table 5, where we summarized results from Tables 3 and 4 Table 5 shows that with over 99 minutes, HMM-GMM, XLSR and k2-conf have a PER of 13.9% or less (excluding the srs result for XLSR).","Modify,Fact/Evidence",Fact/Evidence
3737,350-ARR,350-ARR_v2_54@5,350-ARR_v1_53@5,The result for the full nru set from Wisniewski et al. (2020) is included for completeness.,The result for nru from Wisniewski et al. (2020) is included for completeness.,"Modify,Clarity",Clarity
3738,350-ARR,350-ARR_v2_54@7,350-ARR_v1_53@7,"Here a PER below 9% was obtained for all the languages in Tables 3 and 4 which had more than 99 minutes for training, so it looks like useful error rates are feasible with 1.7 hours of transcribed data.","Here a PER below 10% was obtained for all the languages in Tables 3 and 4 which had more than 99 minutes for training, so it looks like useful error rates are feasible with 1.5 hours of transcribed data.","Modify,Fact/Evidence",Fact/Evidence
3739,350-ARR,350-ARR_v2_55@2,350-ARR_v1_54@2,"So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware, and makes it possible to test many different hypothesis in a short time, for example about the phoneme inventory.","So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware in a short time.","Modify,Claim",Claim
3740,350-ARR,350-ARR_v2_58@0,350-ARR_v1_57@0,Fine-tuning a large pretrained multilingual model clearly outperformed the other approaches.,Fine-tuning a large pretrained multilingual model outperformed the other approaches (although failing in one case).,"Modify,Fact/Evidence",Fact/Evidence
3741,350-ARR,350-ARR_v2_6@1,350-ARR_v1_6@1,"In addition, none has yet evaluated fine-tuning recent large models pretrained on many languages, for example XLSR (Conneau et al., 2020) 3 , which are particularly well suited for low-resource languages.","In addition, none has yet evaluated the recent large models pretrained on many languages, for example XLSR (Conneau et al., 2020) 3 , which are particularly well suited for low-resource languages.","Modify,Claim",Claim
3742,350-ARR,350-ARR_v2_7@3,350-ARR_v1_7@3,"We more firmly establish feasibility of accurate phonemic transcription with 3 hours or less of transcribed data by reporting on 4 new languages, including Cree and highly polysynthetic Inuktitut, in addition to 7 other previously studied in the literature.","We more firmly establish the feasibility of accurate automatic phonemic transcription with 3 hours or less of transcribed data by reporting on 4 new languages, including Cree and highly polysynthetic Inuktitut, in addition to 7 other previously studied in the literature.","Modify,Clarity",Clarity
3743,350-ARR,350-ARR_v2_12@0,350-ARR_v1_12@0,Table 1 gives amounts of training and testing audio in minutes for each language in this dataset.,Table 1 gives the amount of training and testing audio in minutes for each language in this dataset.,"Modify,Grammar",Grammar
3744,350-ARR,350-ARR_v2_12@1,350-ARR_v1_12@1,"The language code is ISO-639-3 (International Organization for Standardization, 2018).","The language code is the 3-letter ISO-639-3 code (International Organization for Standardization, 2018).","Modify,Fact/Evidence",Fact/Evidence
3745,350-ARR,350-ARR_v2_12@2,350-ARR_v1_12@2,The number of phonemes depends on the particular rules for grapheme-to-phoneme conversion (more details in section 3.2).,The number of phonemes depends on the particular rules for grapheme-to-phoneme conversion (as described in more details in section 3.2).,"Modify,Clarity",Clarity
3746,350-ARR,350-ARR_v2_12@3,350-ARR_v1_12@3,"The IPA column says yes when the recording was transcribed in IPA phonemes, otherwise it was in orthographic text.","The IPA column contains yes if recording was transcribed in IPA phonemes, or no if transcribed in orthographic text.","Modify,Clarity",Clarity
3747,350-ARR,350-ARR_v2_14@2,350-ARR_v1_14@2,Transcribed recordings from a single speaker of Kurmanji Kurdish were kindly shared with us by Translators without Borders.,Transcribed recordings from a single speaker of Kurmanji Kurdish transcribed were kindly shared with us by Translators without Borders.,"Modify,Clarity",Clarity
4069,40-ARR,,40-ARR_v1_32@1,,"On SST, correlations with E-Z Reader are very consistent across POS tags whereas attention flow shows weak correlations on proper nouns (.12), nouns (.16) and verbs (.16) as presented in Figure 2.","Delete,Fact/Evidence",Fact/Evidence
4070,40-ARR,,40-ARR_v1_32@2,,The BNC frequency baseline correlates well with human fixations on adpositions (ADP) which both assign comparably low values.,"Delete,Fact/Evidence",Fact/Evidence
4071,40-ARR,,40-ARR_v1_32@3,,Proper nouns (PROPN) are overestimated in BNC as a result of their infrequent occurrence.,"Delete,Fact/Evidence",Fact/Evidence
4072,40-ARR,,40-ARR_v1_52@2,,Resulting perplexity on the held-out test set was ppl = 81.9.,"Delete,Fact/Evidence",Fact/Evidence
4073,40-ARR,,40-ARR_v1_52@3,,"Then, word-based total fixation times are computed from the E-Z Readers trace files and averaged over all subjects.","Delete,Fact/Evidence",Fact/Evidence
4074,40-ARR,,40-ARR_v1_53@0,,"In addition to Spearman correlation over all tokens, we also report Pearson correlation coefficients on a sentence and token-level.","Delete,Fact/Evidence",Fact/Evidence
4075,40-ARR,,40-ARR_v1_53@1,,Results are displayed in Table 4.,"Delete,Fact/Evidence",Fact/Evidence
4076,40-ARR,,40-ARR_v1_53@2,,"Compared to Spearman correlation on all tokens, the ranking does hardly change for Pearson or sentence-level correlations.","Delete,Fact/Evidence",Fact/Evidence
4077,40-ARR,,40-ARR_v1_53@3,,Absolute correlation coefficients are higher for Spearman compared to Pearson and also are slightly higher on the sentence-level as compared to the tokenlevel analysis.,"Delete,Fact/Evidence",Fact/Evidence
4078,40-ARR,,40-ARR_v1_53@4,,Biggest changes occur in a drop for BNC when Spearman correlation is calculated on all tokens for relation extraction and an increase for self-attention (LRP) in sentiment reading.,"Delete,Fact/Evidence",Fact/Evidence
4079,40-ARR,,40-ARR_v1_53@5,,We hypothesize that both effects can be traced back to the level of sparsity and the corresponding ranking for Spearman correlations.,"Delete,Claim",Claim
4080,40-ARR,,40-ARR_v1_53@6,,"In our entropy analysis we found that, i.e. self-attention shows a sparser representation which was likely caused by the overconfidence of the model, and which could explain the higher rank-based correlation.","Delete,Claim",Claim
4081,40-ARR,,40-ARR_v1_54@0,,Figure 5 shows the full distribution of POS tags of the first tokens flipped.,"Delete,Fact/Evidence",Fact/Evidence
4082,40-ARR,,40-ARR_v1_54@1,,This extends Figure 4 where we only show the first 3 POS tags.,"Delete,Fact/Evidence",Fact/Evidence
4083,40-ARR,,40-ARR_v1_55@0,,We compute entropy values for different attention and relevance scores in both task settings.,"Delete,Fact/Evidence",Fact/Evidence
4084,40-ARR,40-ARR_v2_43@2,,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",,"Add,Fact/Evidence",Fact/Evidence
4085,40-ARR,40-ARR_v2_21@2,40-ARR_v1_21@2,We provide further details on the optimization and model task performance in Appendix A.,We give optimization details and model task performance in Appendix A.,"Modify,Clarity",Clarity
4086,40-ARR,40-ARR_v2_4@0,40-ARR_v1_4@0,"The usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Zhang and Zhang, 2019;Klerke and Plank, 2019).","The usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Klerke and Plank, 2019).","Modify,Fact/Evidence",Fact/Evidence
4087,40-ARR,40-ARR_v2_25@3,40-ARR_v1_25@3,"For sentiment reading, the E-Z Reader and BNC show the highest correlations followed by the Transformer attention flow values (the ranking between E-Z/BNC and Transformer flows is significant at p < 0.05 ).","For sentiment reading, the E-Z Reader and BNC show the highest correlations followed by the Transformer attention flow values (the ranking between E-Z/BNC and Transformer flows is significant at p < .05 ).","Modify,Grammar",Grammar
4088,40-ARR,40-ARR_v2_25@4,40-ARR_v1_25@4,"For relation extraction, we see the highest correlation for BERTbase attention flows (with and without fine-tuning) and BERT-large followed by the E-Z Reader (ranking is significant at p < 0.05).","For relation extraction, we see the highest correlation for BERT-base attention flows (with and without fine-tuning) and BERT-large followed by the E-Z Reader (ranking is significant at p < .05).","Modify,Grammar",Grammar
4089,40-ARR,40-ARR_v2_25@8,40-ARR_v1_25@8,Correlations grouped by sentence length shows stable values around 0.6 (SST) and 0.4 − 0.6 (Wikipedia) except for shorter sentences where correlations fluctuate.,Correlations grouped by sentence length shows stable values around .6 (SST) and .4-.6 (Wikipedia) except for shorter sentences where correlations fluctuate.,"Modify,Grammar",Grammar
4090,40-ARR,40-ARR_v2_4@2,40-ARR_v1_4@2,"We compare the learned attention functions and the heuristic model across two taskspecific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available dataset with eye-tracking recordings of native speakers of English .","We compare the learned attention functions and the heuristic model across two task-specific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available data set with eye-tracking recordings of native speakers of English .","Modify,Grammar",Grammar
4091,40-ARR,40-ARR_v2_32@2,40-ARR_v1_31@2,"We can see that the Transformer models correlate better for more predictable words on both datasets whereas the E-Z Reader is less influenced by word predictability and already shows medium correlation on the most hard-to-predict words (0.3 − 0.4 for both, SST and Wikipedia).","We can see that the Transformer models correlate better for more predictable words on both datasets whereas the E-Z Reader is less influenced by word predictability and already shows medium correlation on the most hard-to-predict words (.3-.4 for both, SST and Wikipedia).","Modify,Grammar",Grammar
4092,40-ARR,40-ARR_v2_32@3,40-ARR_v1_31@3,"In fact, on SST, Transformers only pass the E-Z Reader on the most predictable tokens (word predictability > 0.03).","In fact, on SST, Transformers only pass the E-Z Reader on the most predictable tokens (word predictability >.03).","Modify,Grammar",Grammar
4093,40-ARR,40-ARR_v2_33@12,40-ARR_v1_33@12,The highest correlation can be observed when comparing human attention for task-specific and natural reading (0.72).,The highest correlation can be observed when comparing human attention for task-specific and natural reading (.72).,"Modify,Grammar",Grammar
4094,40-ARR,40-ARR_v2_43@1,40-ARR_v1_42@1,"It remains an open problem how best to interpret self-attention modules (Jain and Wallace, 2019;Wiegreffe and Pinter, 2019), and whether they provide meaningful explanations for model predictions.","It remains an open problem how best to interpret self-attention modules (Jain and Wallace, 2019;Wiegreffe and Pinter, 2019), and whether they provide explanations for model decisions.","Modify,Clarity",Clarity
4095,40-ARR,40-ARR_v2_43@4,40-ARR_v1_42@3,Faithfulness and practicality is often evaluated using automated procedures such as input reduction experiments or measuring time and model complexity.,Faithfulness and practicality can be evaluated using automated procedures such as input reduction experiments or measuring time and model complexity.,"Modify,Clarity",Clarity
4096,40-ARR,40-ARR_v2_48@5,40-ARR_v1_47@5,"Our input reduction suggest that in a sense, trained language models and humans have suboptimal, i.e., less sparse, task-solving strategies, and are heavily regularized by what is optimal in natural reading contexts.","Our input reduction experiments suggest that in a sense, both pretrained language models and humans have suboptimal, i.e., less sparse, task-solving strategies, and are heavily regularized by what is optimal in natural reading contexts.","Modify,Clarity",Clarity
4097,40-ARR,40-ARR_v2_5@7,40-ARR_v1_5@7,"In addition, we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns.",In addition we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns.,"Modify,Grammar",Grammar
4098,40-ARR,40-ARR_v2_5@8,40-ARR_v1_5@8,We test faithfulness of these different attention patterns to produce the correct classification via an input reduction experiment on task-tuned BERT models.,But how faithful are these different attention patterns at producing correct task-classification on a state-of-the-art NLP model?,"Link+Modify,Clarity",Clarity
4099,40-ARR,40-ARR_v2_5@8,40-ARR_v1_5@9,We test faithfulness of these different attention patterns to produce the correct classification via an input reduction experiment on task-tuned BERT models.,"We test this via an input reduction experiment on task-tuned BERT models which highlights the trade-off between a model's faithfulness and sparsity when comparing importance scores to human attention, i.e., less sparse (higher entropy) attention vectors seem to be less faithful with respect to model predictions.","Link+Modify,Clarity",Clarity
4100,40-ARR,40-ARR_v2_5@9,40-ARR_v1_5@9,"Our results highlight the trade-off between model faithfulness and sparsity when comparing importance scores to human attention, i.e., less sparse (higher entropy) attention vectors tend to be less faithful with respect to model predictions.","We test this via an input reduction experiment on task-tuned BERT models which highlights the trade-off between a model's faithfulness and sparsity when comparing importance scores to human attention, i.e., less sparse (higher entropy) attention vectors seem to be less faithful with respect to model predictions.","Link+Modify,Clarity",Clarity
4101,40-ARR,40-ARR_v2_5@10,40-ARR_v1_5@10,Our code is available at github.com/ oeberle/task_gaze_transformers.,Our code is available at github.com/anon.,"Modify,Fact/Evidence",Fact/Evidence
4102,40-ARR,40-ARR_v2_2@3,40-ARR_v1_2@3,"We find the predictiveness of large-scale pretrained self-attention for human attention depends on 'what is in the tail', e.g., the syntactic nature of rare contexts.","We find the predictiveness of large-scale pretrained self-attention for human attention depends on 'what is in the tail,' e.g., the syntactic nature of rare contexts.","Modify,Grammar",Grammar
4103,402-ARR,,402-ARR_v1_17@5,,We include both the HTML and text version of the premise articles.,"Delete,Fact/Evidence",Fact/Evidence
4104,402-ARR,,402-ARR_v1_17@6,,We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,"Delete,Fact/Evidence",Fact/Evidence
4105,402-ARR,,402-ARR_v1_33@0,,TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,"Delete,Claim",Claim
4106,402-ARR,,402-ARR_v1_43@0,,We evaluate the model using the claims and the associated review articles in the validation and test set.,"Delete,Fact/Evidence",Fact/Evidence
4107,402-ARR,402-ARR_v2_6@4,,"In contrast, WatClaim-Check includes claims investigated by 8 fact checking organizations on any topic.",,"Add,Fact/Evidence",Fact/Evidence
4108,402-ARR,402-ARR_v2_16@5,,The premise articles include the source document of the claim when available as well as evidence articles used by fact checkers.,,"Add,Fact/Evidence",Fact/Evidence
4109,402-ARR,402-ARR_v2_37@3,,"This corresponds to the ""gold"" negative sampling technique in (Karpukhin et al., 2020).",,"Add,Fact/Evidence",Fact/Evidence
4110,402-ARR,402-ARR_v2_39@2,,These negative sentences are positive sentences for other claims within the same batch.,,"Add,Fact/Evidence",Fact/Evidence
4111,402-ARR,402-ARR_v2_56@1,,"The test contains a total of 114, 290 sentences and 3, 373 claims.",,"Add,Fact/Evidence",Fact/Evidence
4112,402-ARR,,402-ARR_v1_12@2,,"To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.","Delete,Claim",Claim
4113,402-ARR,402-ARR_v2_60@0,,The evidence sentences are concatenated in the descending order of their similarity score.,,"Add,Fact/Evidence",Fact/Evidence
4114,402-ARR,402-ARR_v2_60@1,,"Afterwards, the claim text and evidence sentences are concatenated.",,"Add,Fact/Evidence",Fact/Evidence
4115,402-ARR,402-ARR_v2_60@2,,The resulting text is then truncated to the maximum sequence length capability of the transformer model being used to perform claim veracity inference.,,"Add,Fact/Evidence",Fact/Evidence
4116,402-ARR,402-ARR_v2_61@3,,We concatenate the evidence sentences in the descending order of their similarity score.,,"Add,Fact/Evidence",Fact/Evidence
4117,402-ARR,402-ARR_v2_15@5,402-ARR_v1_16@5,"Finally, the premise URLs are used to retrieve the premise articles.","Finally, the premise article URLs are used to retrieve the premise articles.","Modify,Clarity",Clarity
4118,402-ARR,402-ARR_v2_15@6,402-ARR_v1_16@6,"We try to directly retrieve the article where possible, but also use archive.org's API in case a premise article is no longer available online.","We try to directly retrieve the article where possible, but also use archive.org's APIs in case the premise article URL is no longer available online.","Modify,Grammar",Grammar
4119,402-ARR,402-ARR_v2_15@7,402-ARR_v1_16@7,We follow the same general procedure for data collection from Politifact and Snopes except that we directly crawl the respective websites instead of using Google's fact check tool APIs for collecting claims and associated metadata.,"We follow the same general procedure for data collection from Politifact and Snopes except that instead of using Google's fact check tool APIs for collecting claims and associated metadata, we directly crawl the respective websites to collect the data.","Modify,Clarity",Clarity
4120,402-ARR,402-ARR_v2_4@0,402-ARR_v1_4@0,"The rise of social media has led to a democratization of news, but it has also amplified issues related to fake news and misinformation.","The rise of social media has lead to a democratization of news, but it has also amplified issues related to fake news and misinformation.","Modify,Grammar",Grammar
4121,402-ARR,402-ARR_v2_16@0,402-ARR_v1_17@0,We perform some basic cleanup to the collected data before inclusion in the dataset.,We only perform some basic cleanup to the collected data before inclusion in the dataset.,"Modify,Clarity",Clarity
4122,402-ARR,402-ARR_v2_16@4,402-ARR_v1_17@4,"Admittedly, this does not eliminate auxiliary text such as navigation links, footer text, recommended links, etc.","Admittedly, this still does not eliminate the auxiliary text usually present in the web pages such as navigation links, footer text, recommended links, etc.","Modify,Clarity",Clarity
4123,402-ARR,402-ARR_v2_16@6,402-ARR_v1_17@7,"We map the numerous claim veracity labels used by the fact checking websites into three broad labels: True, Partially True/False, and False.","We also map the numerous claim veracity labels used by the fact checking websites into three broad labels: True, Partially True/False, and False.","Modify,Clarity",Clarity
4124,402-ARR,402-ARR_v2_18@0,402-ARR_v1_19@0,"The contributed dataset contains a total of 33,721 claims.","The contributed dataset contains a total of 33,697 claims.","Modify,Fact/Evidence",Fact/Evidence
4125,402-ARR,402-ARR_v2_18@1,402-ARR_v1_19@1,"We split those claims into the following three sets: training set containing 26,976 claims, validation set containing 3,372 claims, and test set containing 3,373 claims.","We split those claims into the following three sets: training set containing 26,957 claims, validation set containing 3369 claims, and test set containing 3371 claims.","Modify,Fact/Evidence",Fact/Evidence
4126,402-ARR,402-ARR_v2_18@3,402-ARR_v1_19@3,We provide the extracted text files for the review and premise articles.,We provide both the HTML and extracted text files for the review and premise articles.,"Modify,Fact/Evidence",Fact/Evidence
4127,402-ARR,402-ARR_v2_19@0,402-ARR_v1_20@0,Fig. 1 shows the number of claims per fact checking services.,Figure 1 shows the number of claims collected from each of the fact checking services.,"Modify,Clarity",Clarity
4128,402-ARR,402-ARR_v2_19@1,402-ARR_v1_20@1,Fig. 2 shows the claim rating distribution.,Figure 2 shows the claim rating distribution.,"Modify,Clarity",Clarity
4129,402-ARR,402-ARR_v2_19@2,402-ARR_v1_20@2,Claims in the Partially True/False and False categories significantly outnumber the claims in the True category.,We see that the claims in the Partially True/False and False categories significantly outnumber the claims in the True category.,"Modify,Clarity",Clarity
4130,402-ARR,402-ARR_v2_19@3,402-ARR_v1_20@3,"In reality, the number of true claims is much larger than the number of partially true/false and false claims, but fact checking services focus on debunking controversial claims and therefore the majority of the claims they investigate are false or partially true/false.","In reality, the number of true claims is much larger then the number of partially true/false and false claims, but fact checking services focus on debunking controversial claims and therefore the majority of the claims they investigate are false or partially true/false.","Modify,Grammar",Grammar
4131,402-ARR,402-ARR_v2_19@4,402-ARR_v1_20@4,This imbalance poses an important challenge.,This imbalance poses an important challenge for the models.,"Modify,Clarity",Clarity
4132,402-ARR,402-ARR_v2_22@2,402-ARR_v1_22@4,This reflects the real world task of automated veracity detection more truthfully due to the availability of the premise articles cited by the professional fact checkers in claim review articles.,We argue that our dataset reflects the real world task of automated veracity detection more truthfully due to the availability of the premise articles cited by the professional fact checkers in claim review articles.,"Modify,Clarity",Clarity
4133,402-ARR,402-ARR_v2_22@4,402-ARR_v1_22@6,Using a web search to retrieve evidence documents after a fact checking service has verified a claim is problematic since multiple news agencies often publish articles referencing the original fact checking review article.,"Using a web search to retrieve evidence documents for a claim is problematic due to the fact that once a fact checking service has fact checked a claim, we observe that multiple other news agency also publish articles referencing the original fact checking review article.","Modify,Clarity",Clarity
4134,402-ARR,402-ARR_v2_22@5,402-ARR_v1_22@7,Top-k web search results typically contain those articles which may indirectly leak the veracity label.,"Retrieving top-k web search results, typically retrieves those articles as well.","Merge+Modify,Clarity",Clarity
4135,402-ARR,402-ARR_v2_22@5,402-ARR_v1_22@8,Top-k web search results typically contain those articles which may indirectly leak the veracity label.,This can indirectly leak the veracity label in the retrieved documents.,"Merge+Modify,Clarity",Clarity
4136,402-ARR,402-ARR_v2_24@2,402-ARR_v1_24@2,"For the first stage, we evaluate two different approaches.","For the first stage, we use and evaluate two different approaches.","Modify,Clarity",Clarity
4137,402-ARR,402-ARR_v2_24@3,402-ARR_v1_24@3,"The first approach is term frequency inverse document frequency (TF-IDF), which is typically used by fact checking methods for sentence based retrieval (Aly et al., 2021).",The first approach is the well-known and commonly used basic text retrieval technique called term frequency inverse document frequency (TF-IDF).,"Modify,Fact/Evidence",Fact/Evidence
4138,402-ARR,402-ARR_v2_30@0,402-ARR_v1_31@0,A key step performed by professional fact checkers is examining the premise articles associated with a claim and extracting useful evidence from them to establish claim veracity.,One of the key steps in the fact checking process performed by human professional fact checkers is examining the premise articles associated with a claim and extracting useful evidence from them to establish claim veracity.,"Modify,Clarity",Clarity
4139,402-ARR,402-ARR_v2_32@0,402-ARR_v1_33@1,We measure TF-IDF similarity between the claim text and the premise article sentences to rank the sentence level evidence.,We use TF-IDF based similarity measure between the claim text and the premise article sentences to rank the sentence level evidence.,"Modify,Clarity",Clarity
4140,402-ARR,402-ARR_v2_34@0,402-ARR_v1_35@0,We propose a novel way of adapting the dense passage retrieval method proposed by Karpukhin et al. (2020) task of retrieving evidence sentences from premise articles.,We propose a novel way of adapting dense passage retrieval methods proposed by Karpukhin et al. (2020) for open domain question answering to the task of retrieving evidence sentences from premise articles.,"Modify,Fact/Evidence",Fact/Evidence
4141,402-ARR,402-ARR_v2_34@1,402-ARR_v1_35@1,Karpukhin et al.'s method uses a dual encoder architecture.,The dense passage retrieval method proposed by Karpukhin et al. (2020) uses a dual encoder architecture.,"Modify,Clarity",Clarity
4142,402-ARR,402-ARR_v2_34@3,402-ARR_v1_35@3,The question encoder E Q and the passage encoder E P embed question q and passage p into d-dimensional vectors.,The question encoder E Q and the passage encoder E P embed a given question q and passage p into d-dimensional real-valued vectors.,"Modify,Clarity",Clarity
4143,402-ARR,402-ARR_v2_36@0,402-ARR_v1_37@0,The model is then trained to learn embeddings such that the similarity score between relevant questionpassage pairs will be higher than irrelevant ones.,The model is then trained to learn embedding functions such that the similarity score between relevant pairs of questions and passages will be higher than irrelevant ones.,"Modify,Clarity",Clarity
4144,402-ARR,402-ARR_v2_36@1,402-ARR_v1_38@0,We adapt this method for our first stage by taking advantage of the fact that the review article published by fact checking websites (along with a claim) typically contains key evidence taken from the premise articles.,We adapt this method for our first stage by taking advantage of the fact that the review article published by fact checking websites along with a claim typically contains key evidence taken from the premise articles.,"Modify,Grammar",Grammar
4145,402-ARR,402-ARR_v2_36@2,402-ARR_v1_38@1,The evidence is usually paraphrased in order to form a coherent argument in support of the claim veracity verdict.,The evidence taken from the premise articles is usually paraphrased in order to form a coherent argument in support of the claim veracity verdict.,"Modify,Clarity",Clarity
4146,402-ARR,402-ARR_v2_39@1,402-ARR_v1_41@1,"Each instance is made up of a claim C i with one positive sentence from the associated review article s R+ i,j and n − 1 randomly chosen negative sentences s R− i,k .","Each instance is made up of a claim C i with one positive sentence from the associated review article s R+ i,j and n randomly chosen negative sentences s R− i,k .","Modify,Fact/Evidence",Fact/Evidence
4147,402-ARR,402-ARR_v2_42@0,402-ARR_v1_44@0,"After training, we use the encoders to encode the claim text and the sentences of the associated premise articles.","After training the model, we use the encoders to encode the claim text and the sentences of the associated premise articles.","Modify,Clarity",Clarity
4148,402-ARR,402-ARR_v2_42@2,402-ARR_v1_44@2,We use the top scoring sentences as evidence sentences in the next stage to perform claim veracity inference.,We use the top scoring sentences as evidence sentences in the next stage to perform the claim veracity inference.,"Modify,Grammar",Grammar
4149,402-ARR,402-ARR_v2_48@2,402-ARR_v1_50@2,Each sentence (claimant with claim text or each evidence sentence of the premise article) is embedded as a sequence of hidden vectors (one per word) by a bi-directional recurrent network (Bi-LSTM or Bi-GRU).,"At the bottom of a HAN, each sentence (claimant with claim text or each evidence sentence of the premise article) is embedded as a sequence of hidden vectors (one per word) by a bi-directional recurrent network (Bi-LSTM or Bi-GRU).","Modify,Clarity",Clarity
4150,402-ARR,402-ARR_v2_50@2,402-ARR_v1_52@2,The input sequence is encoded using the RoBERTa-base model and passed through a dense linear layer followed by a softmax to obtain the predicted claim veracity label distribution.,The input sequence is encoded using the RoBERTa-base model and passed through a dense linear layer followed by a softmax layer to obtain the predicted claim veracity label distribution.,"Modify,Clarity",Clarity
4151,402-ARR,402-ARR_v2_56@3,402-ARR_v1_57@3,The results clearly show that the DPR (dense passage retrieval) method outperforms the method based on TF-IDF.,The results clearly show that the DPR (dense passage retrieval) method outperforms the TF-IDF similarity based method.,"Modify,Clarity",Clarity
4152,402-ARR,402-ARR_v2_61@4,402-ARR_v1_60@1,The evidence sentences are then concatenated to the claim text and truncated to the maximum sequence length capability of the transformer model being used to perform claim veracity inference.,The sentences with top scores are then used to perform claim veracity inference.,"Modify,Fact/Evidence",Fact/Evidence
4153,402-ARR,402-ARR_v2_64@0,402-ARR_v1_64@0,Fig. 3 shows the number of claims investigated in each 6-month period in our dataset.,Figure 3 shows the number of claims investigated in each 6-month period in our dataset.,"Modify,Clarity",Clarity
4154,402-ARR,402-ARR_v2_64@4,402-ARR_v1_64@4,Fig. 4 shows the macro F1 results achieved by the top 4 algorithms with DPR evidence in each 6-month period.,Figure 4 shows the macro F1 results achieved by the top 4 algorithms with DPR evidence in each 6-month period.,"Modify,Clarity",Clarity
4155,402-ARR,402-ARR_v2_66@1,402-ARR_v1_66@1,WatClaimCheck includes premise articles used by professional fact checkers and therefore corresponds more closely to the task of claim veracity inference in automated fact checking.,It is the first dataset that includes premise articles used by professional fact checkers and therefore corresponds more closely to the task of claim veracity inference in automated fact checking.,"Modify,Claim",Claim
4156,402-ARR,402-ARR_v2_6@1,402-ARR_v1_6@1,Many other datasets for claim verification are listed in Table 1.,"While some other datasets include claims with associated verdicts and in some cases review articles as well as search engine results, this is the first dataset that includes premise articles, therefore enabling the inference task described above.","Split+Modify,Fact/Evidence",Fact/Evidence
4157,402-ARR,402-ARR_v2_6@2,402-ARR_v1_6@1,"However, most of them do not include premise articles needed for the inference task described above.","While some other datasets include claims with associated verdicts and in some cases review articles as well as search engine results, this is the first dataset that includes premise articles, therefore enabling the inference task described above.","Split+Modify,Clarity",Clarity
4158,402-ARR,402-ARR_v2_7@1,402-ARR_v1_7@1,"When the first stage fails to retrieve key passages, then the inferred verdict will be negatively affected regardless of how good the second stage is.","When the first stage fails to retrieve some key passages, then the inferred verdict will be negatively affected regardless of how good the second stage is.","Modify,Clarity",Clarity
4159,402-ARR,402-ARR_v2_9@10,402-ARR_v1_10@6,"Finally, Sect. 6 concludes and discusses possible future work.","Finally, Section 6 concludes and discusses possible future work.","Modify,Clarity",Clarity
4160,402-ARR,402-ARR_v2_11@0,402-ARR_v1_12@0,"There is an important line of work that focuses on claim verification (Kotonya and Toni, 2020a;Guo et al., 2022).",There is an important line of work that focuses on claim verification.,"Modify,Fact/Evidence",Fact/Evidence
4161,402-ARR,402-ARR_v2_11@1,402-ARR_v1_12@1,"This includes techniques that predict the veracity of a claim based on the text of the claim only (Rashkin et al., 2017), linguistic features (Popat et al., 2017), meta information about the claimant (e.g., name, job, party affiliation, veracity history) (Wang, 2017), review articles (Augenstein et al., 2019;Shu et al., 2018;Nakov et al., 2021), relevant articles returned by a search engine (Popat et al., 2018;Augenstein et al., 2019;Mishra and Setty, 2019) as well as premise articles (Aly et al., 2021;Kotonya and Toni, 2020b).","This includes techniques that predict the veracity of a claim based on the text of the claim only (Rashkin et al., 2017), linguistic features (Popat et al., 2017), meta information about the claimant (e.g., name, job, party affiliation, veracity history) (Wang, 2017b), review articles (Augenstein et al., 2019;Shu et al., 2018;Nakov et al., 2021), as well as relevant articles returned by a search engine (Popat et al., 2018;Augenstein et al., 2019;Mishra and Setty, 2019).","Modify,Fact/Evidence",Fact/Evidence
4162,402-ARR,402-ARR_v2_11@2,402-ARR_v1_12@3,There is an important distinction between articles returned by a search engine and premise articles.,There is an important distinction between articles returned by a search engine in previous work and the premise articles that we consider.,"Modify,Clarity",Clarity
4163,402-ARR,402-ARR_v2_12@4,402-ARR_v1_13@4,"An important task that can help the detection of fake news is stance detection (Borges et al., 2019;Jwa et al., 2019), i.e., does the content of an article agree or disagree with the title of the article?","An important task that can help the detection of fake news is the task of stance detection (Borges et al., 2019;Jwa et al., 2019), i.e., does the content of an article agree or disagree with the title of the article?","Modify,Clarity",Clarity
4164,402-ARR,402-ARR_v2_15@1,402-ARR_v1_16@1,We utilize Google's fact check tool APIs 2 to collect the claims' metadata for all fact checking services except Politifact and Snopes.,We utilize Google's fact check tool APIs 1 to collect the claims' metadata for all previously listed fact checking services except Politifact and Snopes.,"Modify,Fact/Evidence",Fact/Evidence
4165,402-ARR,402-ARR_v2_15@4,402-ARR_v1_16@4,"We parse the article body, retrieving the premise article URLs used in the review article to justify the claim veracity.","We carefully parse the article body, retrieving the premise article URLs used in the claim review article to justify the claim veracity.","Modify,Clarity",Clarity
4224,412-ARR,,412-ARR_v1_74@7,,"However, the remaining models improve upon the starting architecture, with the largest improvements being observed for domain adaptation and the text simplification auxiliary task (Base + Text simplification + DA), with a Pearson correlation coefficient on the test dataset of .7744, 2.42% better than the base model.","Delete,Fact/Evidence",Fact/Evidence
4225,412-ARR,,412-ARR_v1_74@8,,The improved performance can be also seen for the Mean Absolute Error score (MAE = .0652).,"Delete,Fact/Evidence",Fact/Evidence
4226,412-ARR,,412-ARR_v1_75@0,,"While the Transformer decoder auxiliary task does not offer the best performance for the single word dataset, the same architecture offers the second-best performance for the multiple word dataset, with a Pearson score of .8252 compared to the best one, .8285.","Delete,Fact/Evidence",Fact/Evidence
4227,412-ARR,,412-ARR_v1_76@0,,"The domain adaptation and VAE configuration provide improvements upon the base model (.7554 versus .7502 Pearson), but the VAE does not have an important contribution, considering that the Base + domain adaptation model has a slightly higher Pearson score of .7569.","Delete,Fact/Evidence",Fact/Evidence
4228,412-ARR,412-ARR_v2_4@0,,"The overarching goal of the complex word identification (CWI) task is to find words that can be simplified in a given text (Paetzold and Specia, 2016b).",,"Add,Fact/Evidence",Fact/Evidence
4229,412-ARR,412-ARR_v2_76@5,,"Third, Gooding and Kochmar (2018) approach the English section of the dataset by employing linear regressions.",,"Add,Fact/Evidence",Fact/Evidence
4230,412-ARR,412-ARR_v2_76@6,,"The authors used several types of handcrafted features, including word n-grams, POS tags, dependency parse relations, and psycholinguistic features.",,"Add,Fact/Evidence",Fact/Evidence
4231,412-ARR,412-ARR_v2_77@0,,Table 3 presents the results obtained on the multilingual validation dataset and compares the performance of different configurations.,,"Add,Fact/Evidence",Fact/Evidence
4232,412-ARR,412-ARR_v2_28@2,412-ARR_v1_25@2,"A gradient reversal layer (Ganin and Lempitsky, 2015) is added between the feature concatenation and the discriminator to reverse the gradients through the backpropagation phase and support extracting general features.",A gradient reversal layer is added between the feature concatenation and the discriminator to reverse the gradients through the backpropagation phase and support extracting general features.,"Modify,Fact/Evidence",Fact/Evidence
4233,412-ARR,412-ARR_v2_2@6,412-ARR_v1_2@6,"Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset.",Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques when considering the CompLex from the Lexical Complexity Prediction 2021 dataset.,"Modify,Grammar",Grammar
4234,412-ARR,412-ARR_v2_31@0,412-ARR_v1_28@0,The following setups also include the Basic Domain Adaptation training setting.,The following setups also include the Basic Domain Adaptation training setup.,"Modify,Clarity",Clarity
4235,412-ARR,412-ARR_v2_32@1,412-ARR_v1_29@1,"The concatenation layer now contains the BiLSTM and Transformer features, along with the VAE encoder features (F v ), namely F t +F c +F v .","The concatenation layer now contains the BiLSTM and Transformer features, plus the VAE encoder features (F v ).","Modify,Fact/Evidence",Fact/Evidence
4236,412-ARR,412-ARR_v2_44@0,412-ARR_v1_41@0,"We rely on a Transformer-based model as the main feature extractor for the context of the target word (i.e., the full sentence), considering their superior performance on most natural language processing tasks.","We rely on a Transformer-based model as the main feature extractor for the context of the target word (i.e., the full sentence), considering their superior performance on most NLP tasks.","Modify,Clarity",Clarity
4237,412-ARR,412-ARR_v2_44@1,412-ARR_v1_41@1,"The selected model for the first dataset is RoBERTa (Liu et al., 2019), as it yields better results when compared to its counterpart, BERT.","The selected model for the first dataset is RoBERTa (Liu et al., 2019), inasmuch as it yields better results when compared to its counterpart, BERT.","Modify,Clarity",Clarity
4238,412-ARR,412-ARR_v2_44@2,412-ARR_v1_41@2,"RoBERTa is trained with higher learning rates and larger mini-batches, and it modifies the key hyper-parameters of BERT.","RoBERTa is trained with higher learning rates and larger minibatches, as well as it modifies the key hyperparameters of BERT.","Modify,Clarity",Clarity
4239,412-ARR,412-ARR_v2_46@0,412-ARR_v1_44@0,"We aim to further improve performance by adding extra features via Variational AutoEncoders (VAEs) (Kingma and Welling, 2014) to the context representation for a target word.","We aim to further improve performance by adding extra features via Variational AutoEncoders (VAEs) (Kingma and Welling, 2013) to the context representation for a target word.","Modify,Fact/Evidence",Fact/Evidence
4240,412-ARR,412-ARR_v2_52@1,412-ARR_v1_50@1,"Consequently, we introduced a generalization technique to extract only cross-domain features that do not present a bias towards a certain domain.","Consequently, we were introduced a generalization technique to extract only cross-domain features that do not present a bias towards a certain domain.","Modify,Grammar",Grammar
4241,412-ARR,412-ARR_v2_58@3,412-ARR_v1_56@3,"The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016a) 1 .","The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016) 1 .","Modify,Fact/Evidence",Fact/Evidence
4242,412-ARR,412-ARR_v2_4@2,412-ARR_v1_4@1,"However, complex word identification is a highly contextualized task, far from being trivial.","However, complex word identification (CWI) is a highly contextualized task, far from being trivial.","Modify,Clarity",Clarity
4243,412-ARR,412-ARR_v2_58@5,412-ARR_v1_56@5,"The corresponding flow is described in Algorithm 1, while the loss function is presented in Equation 7:","The corresponding flow is described in algorithm 1, while the loss function is presented in Equation 7.","Modify,Grammar",Grammar
4244,412-ARR,412-ARR_v2_68@1,412-ARR_v1_68@1,"The learning rate is set to 2e-5, while the loss functions used for the complexity task are the L1 loss (Janocha and Czarnecki, 2016) for the CompLex LCP dataset and the Mean Squared Error (MSE) loss (Kline and Berardi, 2005) for the CWI dataset.","The learning rate is set to 2e-5, while the loss functions used for the complexity task are the L1 loss (Janocha and Czarnecki, 2017) for the first dataset and the Mean Squared Error (MSE) loss (Kline and Berardi, 2005) for the second one.","Modify,Fact/Evidence",Fact/Evidence
4245,412-ARR,412-ARR_v2_68@3,412-ARR_v1_68@3,The λ parameter used for domain adaptation was updated according to Equation 11:,The λ parameter used for domain adaptation was updated according to Equation 11.,"Modify,Grammar",Grammar
4246,412-ARR,412-ARR_v2_12@2,412-ARR_v1_5@0,"Nevertheless, certain training techniques and auxiliary tasks help the model improve its generalization abilities, forcing it to focus only on the most relevant, general features (Schrom et al., 2021).","Nevertheless, certain training techniques and auxiliary tasks help the model improve its generalization abilities (Schrom et al., 2021), forcing it to focus only on the most relevant, general features.","Modify,Fact/Evidence",Fact/Evidence
4247,412-ARR,412-ARR_v2_73@2,412-ARR_v1_74@2,"Zaharia et al. (2021) created models that are based on target and context feature extractors, alongside features resulted from Graph Convolutional Networks, Capsule Networks, and pre-trained word embeddings.","Zaharia et al. (2021) created models that are based on target and context feature extractors, alongside features resulted from Graph Convolutional Networks, Capsule Networks, or pre-trained word embeddings.","Modify,Clarity",Clarity
4248,412-ARR,412-ARR_v2_12@3,412-ARR_v1_5@1,"Techniques like domain adaptation (Ganin et al., 2016) can be used for various tasks, with the purpose of selecting relevant features for follow-up processes.","Techniques like domain adaptation can be used for various tasks, with the purpose of selecting relevant features for follow-up processes.","Modify,Fact/Evidence",Fact/Evidence
4249,412-ARR,412-ARR_v2_76@2,412-ARR_v1_78@2,"The performance is evaluated in terms of MAE; however, we also report the Pearson Correlation Coefficient.","The performance is evaluated in terms of MAE; However, we also report the Pearson Correlation Coefficient.","Modify,Grammar",Grammar
4250,412-ARR,412-ARR_v2_79@0,412-ARR_v1_80@0,"The domain adaptation technique supports our model to learn general cross-domain or crosslanguage features, while achieving higher performance.","The domain adaptation technique supports our model to learn general cross-domain (or cross-language) features, while achieving higher performance.","Modify,Grammar",Grammar
4251,412-ARR,412-ARR_v2_80@0,412-ARR_v1_84@0,Conclusions and Future Work,Conclusion and Future Work,"Modify,Grammar",Grammar
4252,412-ARR,412-ARR_v2_81@0,412-ARR_v1_85@0,"This work proposes a series of training techniques, including domain adaptation, as well as multi-task adversarial learning, that can be used for improving the overall performance of the models for CWI.","This work proposes a series of training techniques, including adversarial domain adaptation, as well as multi-task learning, that can be used for improving the overall performance of the models for CWI.","Modify,Clarity",Clarity
4253,412-ARR,412-ARR_v2_81@2,412-ARR_v1_86@0,"Moreover, by jointly training the model on the CWI tasks and an auxiliary similar task (i.e., text simplification), the overall performance is improved.","Moreover, by jointly training the model on the CWI taks and an auxiliary similar task (i.e., text simplification), the overall performance is improved.","Modify,Grammar",Grammar
4254,412-ARR,412-ARR_v2_81@3,412-ARR_v1_86@1,"The task discriminator also ensures the extraction of general features, thus making the model more robust on the CWI dataset.","The task discriminator also ensures the extraction of general features, thus making the model more robust on the CWI task.","Modify,Clarity",Clarity
4255,412-ARR,412-ARR_v2_13@0,412-ARR_v1_7@0,"We propose several solutions to improve the performance of a model for CWI in a cross-domain or a cross-lingual setting, by adding auxiliary components (i.e., Transformer (Vaswani et al., 2017) decoders, Variational Auto Encoders -VAEs (Kingma and Welling, 2014)), as well as a domain adaptation training technique (Farahani et al., 2021).","We propose several solutions to improve the performance of a model for CWI in a cross-domain or a cross-lingual setting, by adding auxiliary components (i.e., Transformer (Vaswani et al., 2017) decoders, Variational Auto Encoders -VAEs (Kingma and Welling, 2013)), as well as a domain adaptation training technique (Farahani et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
4256,412-ARR,412-ARR_v2_16@1,412-ARR_v1_11@1,Several works employed domain adaptation to improve performance.,Several works employ domain adaptation to improve performance.,"Modify,Grammar",Grammar
4257,412-ARR,412-ARR_v2_16@2,412-ARR_v1_11@2,"For example, Du et al. (2020) approached the sentiment analysis task by using a BERT-based (Devlin et al., 2019) (Klimaszewski and Andruszkiewicz, 2019), mixup synthesis training (Tang et al., 2020), and effective regularization (Vernikos et al., 2020).","For example, Du et al. (2020) approach the sentiment analysis task by using a BERT-based (Devlin et al., 2019) (Klimaszewski and Andruszkiewicz, 2019), mixup synthesis training (Tang et al., 2020), and effective regularization (Vernikos et al., 2020).","Modify,Grammar",Grammar
4258,412-ARR,412-ARR_v2_17@2,412-ARR_v1_12@2,"The latter had the purpose of supporting the adversarial training setup, thus covering the scenario where the model was unable to detect whether the input language was from the source dataset or the target one.","The latter has the purpose of supporting the adversarial training setup, thus covering the scenario where the model is unable the detect whether the input language is from the source dataset or the target one.","Modify,Grammar",Grammar
4259,412-ARR,412-ARR_v2_17@3,412-ARR_v1_12@3,"A similar cross-lingual approach was adopted by Zhang et al. (2020), who developed a system to classify entries from the target language, while only labels from the source language were provided.","A similar cross-lingual approach is adopted by Zhang et al. (2020), who developed a system to classify entries from the target language, while only labels from the source language are provided.","Modify,Grammar",Grammar
4260,412-ARR,412-ARR_v2_18@0,412-ARR_v1_13@0,"Under a Named Entity Recognition training scenario, Kim et al. (2017) used features on two levels (i.e., word and characters), together with Recurrent Neural Networks and a language discriminator used for the domain-adversarial setup.","Under a Named Entity Recognition (NER) training scenario, Kim et al. (2017) use features on two levels (i.e., word and characters), together with Recurrent Neural Networks and a language discriminator used for the domain-adversarial setup.","Modify,Grammar",Grammar
4261,412-ARR,412-ARR_v2_2@3,412-ARR_v1_2@3,"In this paper, we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations.",In this paper we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations.,"Modify,Grammar",Grammar
4262,412-ARR,412-ARR_v2_18@1,412-ARR_v1_13@1,"Similarly, Huang et al. (2019) used target language discriminators during the process of training models for low-resource name tagging.","Similarly, Huang et al. (2019) use target language discriminators during the process of training models for low-resource name tagging.","Modify,Grammar",Grammar
4263,412-ARR,412-ARR_v2_19@1,412-ARR_v1_14@1,"Gooding and Kochmar (2019) based their implementation for CWI as a sequence labeling task on Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, inasmuch as the context helps towards proper identification of complex tokens.","Gooding and Kochmar (2019) base their implementation for CWI as a sequence labeling task on Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, inasmuch as the context helps towards proper identification of complex tokens.","Modify,Grammar",Grammar
4264,412-ARR,412-ARR_v2_19@3,412-ARR_v1_14@3,"Also adopting a sequence labeling approach, Finnimore et al. (2019) considered handcrafted features, including punctuation or syllables, that can properly identify complex structures.","Also adopting a sequence labeling approach, Finnimore et al. ( 2019) consider handcrafted features, including punctuation or syllables, that can properly identify complex structures.","Modify,Grammar",Grammar
4265,412-ARR,412-ARR_v2_20@2,412-ARR_v1_16@0,"At the same time, Zaharia et al. (2020) explored the power of Transformer-based models (Vaswani et al., 2017) in cross-lingual environments by using different training scenarios, depending on the scarcity of the resources: zero-shot, one-shot, as well as few-shot learning.","At the same time, Zaharia et al. (2020) explore the power of Transformer-based models (Vaswani et al., 2017) in cross-lingual environments by using different training scenarios, depending on the scarcity of the resources: zero-shot, one-shot, as well as few-shot learning.","Modify,Grammar",Grammar
4266,412-ARR,412-ARR_v2_20@4,412-ARR_v1_17@1,"For example, De Hertog and Tack (2018) introduced a series of architectures that combine deep learning features, as well as handcrafted features to address CWI as a regression problem.","For example, De Hertog and Tack (2018) introduce a series of architectures that combine deep learning features, as well as handcrafted features to solve a regression problem.","Modify,Fact/Evidence",Fact/Evidence
4267,412-ARR,412-ARR_v2_23@1,412-ARR_v1_20@1,"The entries of Com-pLex consist of a sentence in English and a target token, alongside the complexity of the token, given its context.","The entries of Com-pLex consist of a sentence and a target token, alongside the complexity of the token, given its context.","Modify,Fact/Evidence",Fact/Evidence
4268,412-ARR,412-ARR_v2_25@0,412-ARR_v1_22@0,"The CWI dataset was introduced in the CWI Shared Task 2018 (Yimam et al., 2018).","The Complex Word Identification (CWI) Shared Dataset was introduced in the CWI Shared Task 2018 (Yimam et al., 2018).","Modify,Clarity",Clarity
4269,413-ARR,413-ARR_v2_25@5,,"Note that we do not experiment with the setting of training with all the combined training data from the corpora as it has been pointed out by previous work that doing so hurts the performance of the models (Jiménez-Zafra et al., 2020;Barnes et al., 2021) due to differences in annotation schemes between the corpora introducing noise during training.",,"Add,Fact/Evidence",Fact/Evidence
4270,413-ARR,413-ARR_v2_25@10,,"Note that for NegBERT, we also replace the BERT encoder with RoBERTa to ensure results are comparable between the models.",,"Add,Fact/Evidence",Fact/Evidence
4271,413-ARR,413-ARR_v2_27@3,,"4 To provide a more general view, we summarize the results in Table 4.",,"Add,Fact/Evidence",Fact/Evidence
4272,413-ARR,413-ARR_v2_27@4,,"In general, we observe gains in both the same-dataset setting (training and test set belongs to one corpus) and cross-dataset setting (training one one training set and testing on all others test sets) for both of the proposed models, with CueNB achieving the largest gains.",,"Add,Fact/Evidence",Fact/Evidence
4273,413-ARR,413-ARR_v2_31@0,,We conducted an error analysis on the VetCompass validation set to see what qualitative improvement CueNB makes over NegBERT.,,"Add,Fact/Evidence",Fact/Evidence
4274,413-ARR,413-ARR_v2_31@1,,"For cue detection, there are two main types of errors that CueNB helps alleviate.",,"Add,Fact/Evidence",Fact/Evidence
4275,413-ARR,413-ARR_v2_31@2,,"First, CueNB can detect more unique cues such as negative, won't, and also multiword cues like no longer.",,"Add,Fact/Evidence",Fact/Evidence
4276,413-ARR,413-ARR_v2_31@3,,"Second, CueNB is able to recognize cases when the negations are actually just speculative.",,"Add,Fact/Evidence",Fact/Evidence
4277,413-ARR,413-ARR_v2_31@4,,"For example, in the sentence O reports has smelled for past week, not sure if anal glands . . . , the word not is part of the speculation phrase not sure, indicating that this is not truly a negation phrase but rather expresses uncertainty.",,"Add,Claim",Claim
4278,413-ARR,413-ARR_v2_31@5,,"For scope resolution, CueNB mostly helps in recognizing the correct scope boundary.",,"Add,Claim",Claim
4279,413-ARR,413-ARR_v2_31@6,,One common case is when the cue relates to multiple spans in a sentence.,,"Add,Claim",Claim
4280,413-ARR,413-ARR_v2_31@8,,It also helps in cases where there are multiple separate negations in the same sentence.,,"Add,Claim",Claim
4281,413-ARR,413-ARR_v2_25@0,413-ARR_v1_23@0,"Following the experimental settings in NegBERT, we use the three standard benchmarks for negation cue detection and scope resolution tasks, i.e. BioScope (Vincze et al., 2008) (separated into two subsets, sourced from abstracts and full-text papers, resp.), the SFU product reviews dataset (Konstantinova et al., 2012), and the Sherlock dataset (Morante and Blanco, 2012).","Following the experiment settings in NegBERT, we use the three standard benchmarks for negation cue detection and scope resolution tasks, i.e. BioScope (Vincze et al., 2008) (separated into two subsets, sourced from abstracts and full-text papers, resp.), the SFU product reviews dataset (Konstantinova et al., 2012), and the Sherlock dataset (Morante and Blanco, 2012).","Modify,Grammar",Grammar
4282,413-ARR,413-ARR_v2_25@3,413-ARR_v1_23@3,"To investigate cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets.","To investigate the cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets.","Modify,Grammar",Grammar
4283,413-ARR,413-ARR_v2_28@1,413-ARR_v1_26@4,"Regarding the in-dataset setting, AugNB outperforms the baseline NegBERT on all datasets except for Sherlock.","Regarding the in-dataset setting (training and evaluating on the same dataset), AugNB outperforms the baseline NegBERT on all datasets except for Sherlock.","Modify,Clarity",Clarity
4284,413-ARR,413-ARR_v2_28@2,413-ARR_v1_26@5,"Gains are more noticeable over the biomedical datasets (BioScope, VetCompass).","Gains are more noticeable in the biomedical datasets (BioScope, VetCompass).","Modify,Grammar",Grammar
4285,413-ARR,413-ARR_v2_29@2,413-ARR_v1_27@2,"CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue helps the model learn better representations for negation cues and thus, better distinguish between cues and normal words.","CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue will help the model learn better representations for negation cues and thus, better distinguish between cues and normal words.","Modify,Grammar",Grammar
4286,413-ARR,413-ARR_v2_4@4,413-ARR_v1_4@4,"A recent study on English texts found that negation detection models do not transfer well across domains, due to variations in expression of negation (Khandelwal and Sawant, 2020).","A recent work on negation detection in English texts found that negation detection models do not transfer well across domains, due to variations in expression of negation (Khandelwal and Sawant, 2020).","Modify,Clarity",Clarity
4287,413-ARR,413-ARR_v2_5@1,413-ARR_v1_5@1,There are three primary datasets that have been used to evaluate negation:,"Three primary datasets are used to evaluate negation; the BioScope corpus (Vincze et al., 2008) includes full papers and abstracts of biological papers, the SFU corpus (Konstantinova et al., 2012) is a collection of product reviews, and the Sherlock dataset (Morante and Blanco, 2012) consists of short literary works.","Split+Modify,Clarity",Clarity
4288,413-ARR,413-ARR_v2_6@0,413-ARR_v1_5@1,"(1) the BioScope corpus (Vincze et al., 2008) includes full papers and abstracts of biological papers; (2) the SFU corpus (Konstantinova et al., 2012) is a collection of product reviews; and (3) the Sherlock dataset (Morante and Blanco, 2012) consists of short literary works.","Three primary datasets are used to evaluate negation; the BioScope corpus (Vincze et al., 2008) includes full papers and abstracts of biological papers, the SFU corpus (Konstantinova et al., 2012) is a collection of product reviews, and the Sherlock dataset (Morante and Blanco, 2012) consists of short literary works.","Split+Modify,Clarity",Clarity
4289,413-ARR,413-ARR_v2_2@1,413-ARR_v1_2@1,"Recent work has shown that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.","Recent works show that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.","Modify,Grammar",Grammar
4290,413-ARR,413-ARR_v2_11@1,413-ARR_v1_10@1,"Most work follows a common scheme in extracting various features from the sentence, and using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).","Most works follow a common scheme in extracting various features from the sentence, in using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).","Modify,Grammar",Grammar
4291,413-ARR,413-ARR_v2_15@1,413-ARR_v1_14@1,"To obtain sentences with negations, we extend the NegEx lexicon with additional negation cues obtained from biomedical texts (Morante, 2010), and apply it to sentences extracted from a corpus using the SpaCy English sentence tokenizer, keeping only those sentences with at least one identified negation cue.","To obtain sentences with negations, we extend the NegEx lexicon with additional negation cues obtained from biomedical texts (Morante, 2010), and apply it to sentences extracted from a corpus using the SpaCy English sentence tokenizer, keeping only those sentences with at least one identified negation.","Modify,Clarity",Clarity
4292,413-ARR,413-ARR_v2_16@2,413-ARR_v1_15@2,"Descriptions of clinical trials can be quite long, but a core aspect of the trial description is the patient inclusion/exclusion criteria, specifying what types of characteristics or conditions a patient must have/not have in order to be suitable for the trial.","Description of clinical trials can be quite long, but a core aspect of the trial description is the patient inclusion/exclusion criteria, specifying what types of characteristics or conditions a patient must have/not have in order to be suitable for the trial.","Modify,Grammar",Grammar
4293,413-ARR,413-ARR_v2_19@1,413-ARR_v1_18@1,"We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representations for sentences containing negation.","We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representation for sentences containing negation.","Modify,Grammar",Grammar
4294,413-ARR,413-ARR_v2_20@1,413-ARR_v1_19@1,"Inspired by work on entity and span masking (Joshi et al., 2020;Yamada et al., 2020), we explore explicitly incorporating information about negation cues into the model by masking these cues, and targeting prediction of the masked cue in the pretraining stage.","Inspired by various works on entity and span masking (Joshi et al., 2020;Yamada et al., 2020), we explore explicitly incorporating information about negation cues into the model by masking these cues, and targeting prediction of the masked cue in the pre-training stage.","Modify,Clarity",Clarity
4422,426-ARR,,426-ARR_v1_25@4,,"Some Key Information Extraction tasks are presented in (Stanisławek et al., 2021).","Delete,Fact/Evidence",Fact/Evidence
4423,426-ARR,,426-ARR_v1_25@5,,"The two datasets described there contain, respectively, NDA documents and financial reports from charity organizations.","Delete,Fact/Evidence",Fact/Evidence
4424,426-ARR,,426-ARR_v1_25@6,,"The tasks for the datasets consist in detecting data points, such as effective dates, interested parties, charity address, income, spending.","Delete,Fact/Evidence",Fact/Evidence
4425,426-ARR,,426-ARR_v1_25@7,,"The authors provide several baseline solutions for the two tasks, which apply up-to-date methods, pointing out that there is still room for improvement in the KIE research area.","Delete,Fact/Evidence",Fact/Evidence
4426,426-ARR,,426-ARR_v1_25@8,,"A challenge that comprises both layout recognition and KIE is presented in (Huang et al., 2019) -the challenge is opened for the recognition of OCR-scanned receipts.","Delete,Fact/Evidence",Fact/Evidence
4427,426-ARR,426-ARR_v2_65@2,,Reference codes can be used to access any of the baseline solutions at http: //gonito.net/q.,,"Add,Fact/Evidence",Fact/Evidence
4428,426-ARR,426-ARR_v2_78@3,,"Since we did not train RoBERTa ChallAm large, we cannot confirm this holds true, when it comes to large RoBERTa models.",,"Add,Fact/Evidence",Fact/Evidence
4429,426-ARR,426-ARR_v2_78@4,,"The standard RoBERTa large is the best performing model, so in this case, a larger model is better even if not trained on the data from different domain.",,"Add,Claim",Claim
4430,426-ARR,426-ARR_v2_80@8,,The tags along with the probabilities are available in the hate-speech-info.tsv files for each test directory.,,"Add,Fact/Evidence",Fact/Evidence
4431,426-ARR,426-ARR_v2_81@0,,Note that temporal and geospatial metadata might constitute useful features in future work on better detection of hate speech in historical texts.,,"Add,Claim",Claim
4432,426-ARR,426-ARR_v2_4@0,426-ARR_v1_4@0,"The dominant approach in the design of current NLP solutions is (pre-)training a large neural language model, usually applying a Transformer architecture, such as GPT-2, RoBERTa or T5, and fine-tuning the model for specific tasks (Devlin et al., 2019;Raffel et al., 2019).","The dominant approach in the design of current NLP solutions consists in (pre-)training a large neural language model, usually applying a Transformer architecture, such as GPT-2, RoBERTa or T5, and fine-tuning the model for specific tasks (Devlin et al., 2018;Raffel et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
4433,426-ARR,426-ARR_v2_34@1,426-ARR_v1_31@1,"Secondly, the PDF files were downloaded and the English texts were processed into DjVu files (as this is the target format for the pipeline) using the pdf2djvu tool 5 .","Secondly, the PDF files were downloaded and the English texts were processed into DjVu files (as this is the target format for the pipeline) using the pdf2dvju tool 4 .","Modify,Grammar",Grammar
4434,426-ARR,426-ARR_v2_38@1,426-ARR_v1_34@1,"At present, the most popular architectures for language models are Transformer (Devlin et al., 2019) models (earlier, e.g. Word2vec (Mikolov et al., 2013) or LSTM models (Peters et al., 2017)).","At present, the most popular architectures for language models are Transformer (Devlin et al., 2018) models (earlier, e.g. Word2vec (Mikolov et al., 2013) or LSTM models (Peters et al., 2017)).","Modify,Fact/Evidence",Fact/Evidence
4435,426-ARR,426-ARR_v2_4@1,426-ARR_v1_4@1,"The solutions are evaluated on benchmarks such as GLUE (Wang et al., 2019b) or SuperGLUE (Wang et al., 2019a), which allow comparing the performance of various methods designed for the same purpose.","The solutions are evaluated on benchmarks such as GLUE ((Wang et al., 2018)) or SuperGLUE ((Wang et al., 2019)), which allow comparing the performance of various methods designed for the same purpose.","Modify,Fact/Evidence",Fact/Evidence
4436,426-ARR,426-ARR_v2_41@1,426-ARR_v1_37@1,The ChallAm models have the same number of parameters as the original RoBERTa Base (125M).,The ChallAm models have the same numbers of parameters as the original RoBERTa Base (125M).,"Modify,Grammar",Grammar
4437,426-ARR,426-ARR_v2_4@2,426-ARR_v1_4@2,An important feature of a good NLP benchmark is the clear separation between train and test sets.,A main feature of a good NLP benchmark is the clear separation between train and test sets.,"Modify,Clarity",Clarity
4438,426-ARR,426-ARR_v2_4@3,426-ARR_v1_4@3,"This requirement prevents data contamination, when the model (pre-)trained on huge data might have ""seen"" the test set in some form.","This requirement prevents data contamination, when the model (pre-)trained on huge data might have ""seen"" the test set.","Modify,Clarity",Clarity
4439,426-ARR,426-ARR_v2_46@10,426-ARR_v1_42@10,These data are used by the Gonito evaluation platform during submission evaluation.,These data are used by the evaluation platform during submission evaluation.,"Modify,Clarity",Clarity
4440,426-ARR,426-ARR_v2_52@1,426-ARR_v1_48@1,"They are released on the Gonito evaluation platform, which enables the calculation of metrics both offline and online, as well as the submission of solutions.","They are released on an evaluation platform, which enables the calculation of metrics both offline and online, as well as the submission of solutions.","Modify,Clarity",Clarity
4441,426-ARR,426-ARR_v2_58@1,426-ARR_v1_55@1,The expected format is the latitude and longitude.,The expected format is a latitude and longitude.,"Modify,Grammar",Grammar
4442,426-ARR,426-ARR_v2_61@3,426-ARR_v1_57@3,"The metric is perplexity; PerplexityHashed, to be precise, as implemented in the GEval evaluation tool (Graliński et al., 2019), the modification is analogous to LogLossHashed in (Graliński, 2017), its goal is to ensure proper evaluation in the competitive (shared-task) setup (i.e. avoid self-reported probabilities and ensure objective comparison of all reported solutions, including out-of-vocabulary words).","The metric is perplexity; PerplexityHashed, to be precise, as implemented in the GEval evaluation tool , the modification is analogous to LogLossHashed in (Graliński, 2017), its goal is to ensure proper evaluation in the competitive (shared-task) setup (i.e. avoid self-reported probabilities and ensure objective comparison of all reported solutions, including out-of-vocabulary words).","Modify,Fact/Evidence",Fact/Evidence
4443,426-ARR,426-ARR_v2_64@0,426-ARR_v1_61@0,Results,Baselines,"Modify,Other",Other
4444,426-ARR,426-ARR_v2_65@0,426-ARR_v1_62@0,Strong baselines for all three tasks are available at the Gonito evaluation platform.,Baselines for all three tasks are available at the evaluation platform.,"Modify,Clarity",Clarity
4445,426-ARR,426-ARR_v2_65@1,426-ARR_v1_62@1,"The baselines (see Tables 2 and 3) include, for each model, its score in the appropriate metric as well as the Git SHA1 reference code in the Gonito benchmark (in curly brackets).","5 The baselines (see Tables 2 and 3) include, for each model, its score in the appropriate metric as well as the Git SHA1 reference code (in curly brackets).","Modify,Fact/Evidence",Fact/Evidence
4446,426-ARR,426-ARR_v2_69@1,426-ARR_v1_64@1,RetroGeo requires two values (latitude and longitude) -we treat them separately and train two separate regression models for them.,RetroGeo requires two values (latitude and longitude) -we treat them separately and train two separate models for them.,"Modify,Fact/Evidence",Fact/Evidence
4447,426-ARR,426-ARR_v2_76@1,426-ARR_v1_71@1,"Since standard RoBERTa training does not incorporate any data, but text, we did not include temporal metadata during inference.","Since standard RoBERTa training does not incorporate any data, but text, we didn't include temporal metadata during inference.","Modify,Clarity",Clarity
4448,426-ARR,426-ARR_v2_78@1,426-ARR_v1_73@1,This means the incorporation of temporal metadata has a positive impact on the MLM task.,This means the incorporation of temporal metadata has a positive impact on MLM task.,"Modify,Grammar",Grammar
4449,426-ARR,426-ARR_v2_80@5,426-ARR_v1_75@5,"We use one of them, DeHateBERT (Aluru et al., 2020), to detect the abusive texts in the ChallAm dataset.","We use one of them, DeHateBERT (Aluru et al., 2020), to filter out the abusive texts in the ChallAm dataset.","Modify,Clarity",Clarity
4450,426-ARR,426-ARR_v2_80@6,426-ARR_v1_75@6,We tagged items that either (1) are marked as abusive speech by DeHateBERT with the probability greater than 0.75 or (2) contain words from a list of blocked words.,We filtered out items that either (1) are marked as abusive speech by DeHateBERT with the probability greater than 0.75 or (2) contain words from a list of blocked words.,"Modify,Clarity",Clarity
4451,426-ARR,426-ARR_v2_80@7,426-ARR_v1_75@7,The fraction of detected texts was 2.04-2.40 % (depending on the challenge and set).,The fraction of filtered out texts was 2.04-2.40% (depending on the challenge and set).,"Modify,Clarity",Clarity
4452,426-ARR,426-ARR_v2_90@0,426-ARR_v1_89@0,"See Table 4 and 5 for the list of top 30 features correlating most with, respectively, the worst and bad results in ChallAm challenges (as returned by the GEval tool with the option -worst-features -numerical-features (Graliński et al., 2019)).","See Table 4 and 5 for the list of top 30 features correlating most with, respectively, the worst and bad results in ChallAm challenges (as returned by the GEval tool with the option -worst-features -numerical-features ).","Modify,Fact/Evidence",Fact/Evidence
4453,426-ARR,426-ARR_v2_31@1,426-ARR_v1_23@1,We provide and make freely available training data from Chronicling America for three ML tasks.,We provide and make available training data from Chronicling America for three ML tasks.,"Modify,Clarity",Clarity
4454,426-ARR,426-ARR_v2_20@0,426-ARR_v1_24@0,Related Machine Learning datasets and challenges,Similar Machine Learning datasets and challenges,"Modify,Clarity",Clarity
4729,463-ARR,,463-ARR_v1_28@1,,"In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important.","Delete,Fact/Evidence",Fact/Evidence
4730,463-ARR,,463-ARR_v1_40@1,,All results are scored by the GLUE evaluate server.,"Delete,Fact/Evidence",Fact/Evidence
4731,463-ARR,,463-ARR_v1_63@3,,"In addition, we demonstrate the robustness of AdapterBias to different PLMs.","Delete,Fact/Evidence",Fact/Evidence
4732,463-ARR,,463-ARR_v1_63@4,,"Finally, we provide analysis on what AdapterBias learns by comparing α, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information.","Delete,Fact/Evidence",Fact/Evidence
4733,463-ARR,,463-ARR_v1_63@5,,Our study overturns previous architectures of adapters by proposing a simple adapter that can produce suitable representation shifts for different tokens.,"Delete,Claim",Claim
4734,463-ARR,463-ARR_v2_16@5,,"During the training stage, we freeze θ and tune θ only.",,"Add,Fact/Evidence",Fact/Evidence
4735,463-ARR,463-ARR_v2_35@0,,Experimental settings,,"Add,Other",Other
4736,463-ARR,463-ARR_v2_36@0,,"We base our experiments on HuggingFace PyTorch implementation (Wolf et al., 2019) of BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019c) 2.",,"Add,Fact/Evidence",Fact/Evidence
4737,463-ARR,463-ARR_v2_36@1,,We experiment with 3 random seeds and choose the seed with the best performance on the validation set to evaluate on the GLUE server.,,"Add,Fact/Evidence",Fact/Evidence
4738,463-ARR,463-ARR_v2_36@2,,We report the test metrics provided on the submission website 2 .,,"Add,Fact/Evidence",Fact/Evidence
4739,463-ARR,463-ARR_v2_38@1,,"In Table 1, we report the test scores on the GLUE benchmark and the required new parameters per task.",,"Add,Fact/Evidence",Fact/Evidence
4740,463-ARR,463-ARR_v2_38@2,,Here we use BERTlarge as the PLM.,,"Add,Fact/Evidence",Fact/Evidence
4741,463-ARR,463-ARR_v2_38@3,,"AdapterBias reaches 81.2 average score in GLUE benchmark, with the smallest amount of parameters (0.17M) added per task.",,"Add,Fact/Evidence",Fact/Evidence
4742,463-ARR,463-ARR_v2_38@5,,The settings are the same as in Table 1.,,"Add,Fact/Evidence",Fact/Evidence
4743,463-ARR,463-ARR_v2_38@6,,The Full-FT corresponds to fine-tuning the whole PLM without adding adapters.,,"Add,Fact/Evidence",Fact/Evidence
4744,463-ARR,463-ARR_v2_53@4,,The settings are the same as in Table 1.,,"Add,Fact/Evidence",Fact/Evidence
4745,463-ARR,463-ARR_v2_53@5,,The Full-FT represents finetuning the whole PLM without adding adapters.,,"Add,Fact/Evidence",Fact/Evidence
4746,463-ARR,463-ARR_v2_19@1,463-ARR_v1_21@1,"When the dimension of the token's representation is r with m input tokens, the function can be defined as follows:","When the dimension of the token's representation is r with with m input tokens, the function can be defined as follows:","Modify,Grammar",Grammar
4747,463-ARR,463-ARR_v2_21@2,463-ARR_v1_24@2,"The dimension of r 1 , r 2 and r 3 is the dimension of the 2 nd feedforward layer, while the input dimension of the linear layer (L α ) is the output dimension of the first feed-forward layer with the token representation (r 1 , r 2 , r 3 ) as its inputs.","The dimension of r 1 , r 2 and r 3 is the dimension of the 2nd feedforward layer, while the dimension of the linear layer (L α ) is the output dimension of the first feed-forward layer with the token representation (r 1 , r 2 , r 3 ) as its inputs.","Modify,Clarity",Clarity
4748,463-ARR,463-ARR_v2_23@0,463-ARR_v1_26@0,"In this section, we experiment on two different methods to make AdapterBias more parameter efficient.","In this section, we experiment on two ways to make AdapterBias more parameter efficient.","Modify,Clarity",Clarity
4749,463-ARR,463-ARR_v2_25@0,463-ARR_v1_28@0,"Redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important (Houlsby et al., 2019).","Redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important.","Modify,Fact/Evidence",Fact/Evidence
4750,463-ARR,463-ARR_v2_38@0,463-ARR_v1_38@0,"In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters (Houlsby et al., 2019), Diff-pruning (Guo et al., 2020), BitFit (Ben Zaken et al., 2021), andLoRA (Hu et al., 2021).","In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters (Houlsby et al., 2019), Diff-pruning (Guo et al., 2020), andBitFit (Ben Zaken et al., 2021) 2019).","Modify,Fact/Evidence",Fact/Evidence
4751,463-ARR,463-ARR_v2_39@1,463-ARR_v1_38@1,"Although Diff-pruning (Guo et al., 2020) achieves the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM.","Although Diff-pruning (Guo et al., 2020) has the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM.","Modify,Clarity",Clarity
4752,463-ARR,463-ARR_v2_39@2,463-ARR_v1_38@2,"Thus, Diff-pruning requires 340M trainable parameters of BERT-large during the training stage, while AdapterBias only trains 0.17M parameters.","Thus, Diff-pruning requires 340M trainable parameters of BERT-large during the training stage, while AdapterBias only trains 0.23M parameters.","Modify,Fact/Evidence",Fact/Evidence
4753,463-ARR,463-ARR_v2_39@3,463-ARR_v1_38@3,"Furthermore, AdapterBias achieves comparable performance with BitFit and LoRA with fewer parameters needed per task.","Furthermore, AdapterBias achieves comparable performance with BitFit with fewer parameters needed per task.","Modify,Fact/Evidence",Fact/Evidence
4754,463-ARR,463-ARR_v2_5@0,463-ARR_v1_5@0,"To tackle these shortcomings, Adapters (Houlsby et al., 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al., 2017) have been proposed.","To tackle these shortcomings, Adapters (Houlsby et al., 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al., 2017) has been proposed.","Modify,Grammar",Grammar
4755,463-ARR,463-ARR_v2_48@0,463-ARR_v1_47@0,Experiments are conducted to examine whether AdapterBias can be more parameter-efficient by sharing its components across all layers.,Experiments are conducted to see whether Adapter-Bias can be more parameter-efficient by sharing its components across all layers.,"Modify,Clarity",Clarity
4756,463-ARR,463-ARR_v2_5@1,463-ARR_v1_5@1,"Instead of full fine-tuning the whole model, Adapters introduce extra tunable weights and freeze the original parameters of PLM.","Instead of full fine-tuning the whole model, Adapters introduces extra tunable weights and freezes the original parameters of PLM.","Modify,Grammar",Grammar
4757,463-ARR,463-ARR_v2_54@0,463-ARR_v1_53@0,"In Table 5, we use BERT-base (BB) and BERTlarge (BL) as the PLMs.","In Table 5, we use BERT-base (BB) and BERTlarge (BL) for the PLM.","Modify,Grammar",Grammar
4758,463-ARR,463-ARR_v2_56@1,463-ARR_v1_55@1,"Compared to the similar work Bit-Fit (Ben Zaken et al., 2021), where the shifts are identical for all tokens, AdapterBias adds tokendependent shifts to the output representation.","Compared to our similar work Bit-Fit (Ben Zaken et al., 2021), where the shifts are identical for all tokens, AdapterBias adds tokendependent shifts to the output representation.","Modify,Clarity",Clarity
4759,463-ARR,463-ARR_v2_58@4,463-ARR_v1_56@4,"They observe on a syntactic task with BShift (Conneau et al., 2018) that the ninth layer of BERT embeds a rich hierarchy of syntactic information.","Jawahar et al. (2019) also observe on a syntactic task with BShift (Conneau et al., 2018) that the ninth layer of BERT embeds a rich hierarchy of syntactic information.","Modify,Clarity",Clarity
4760,463-ARR,463-ARR_v2_58@6,463-ARR_v1_56@6,"For instance, RTE (Giampiccolo et al., 2007;Bentivogli et al., 2009) and MNLI (Williams et al., 2017), where both recognize textual entailment, have higher values in the upper layers than the lower ones.","For instance, RTE (Giampiccolo et al., 2007;Bentivogli et al., 2009) and MNLI (Williams et al., 2017), where both tasks recognize textual entailment, have higher values in the upper layers than those in the lower ones.","Modify,Clarity",Clarity
4761,463-ARR,463-ARR_v2_61@0,463-ARR_v1_59@0,"Since α i represents the weight of the representation shift for i th token in a transformer layer, we can observe the significance of i th token from the summation of α i in all the transformer layers.","Since α i represents the weight of the representation shift for ith token in a transformer layer, we can observe the significance of ith token from the summation of α i in all the transformer layers.","Modify,Grammar",Grammar
4762,463-ARR,463-ARR_v2_65@2,463-ARR_v1_63@2,"Through extensive experiments, not only does AdapterBias reach competitive results on the GLUE benchmark, but also obtain good performance on small-to-medium datasets.","Through extensive experiments, not only does AdapterBias reaches competitive results on the GLUE benchmark, but it also obtains good performance on small-to-medium datasets.","Modify,Clarity",Clarity
4763,463-ARR,463-ARR_v2_5@9,463-ARR_v1_6@1,"The results in Houlsby et al. (2019) have shown that the performance on GLUE benchmark (Wang et al., 2018) is almost the same when removing the Adapters in the lower layers, which indicates that not every adapter is useful.","The results in Houlsby et al. (2019) have shown that the performance on GLUE benchmark (Wang et al., 2018) is almost the same when removing the Adapters in the first layers, which indicates that not every adapter is useful.","Modify,Clarity",Clarity
4764,463-ARR,463-ARR_v2_5@10,463-ARR_v1_6@2,It raises the question of whether adapters can be even more parameter-efficient.,It leaves the question of whether adapters can be even more parameter-efficient.,"Modify,Clarity",Clarity
4765,463-ARR,463-ARR_v2_6@0,463-ARR_v1_7@0,"To develop practical and memory-efficient methods of utilizing PLMs, Diff pruning (Guo et al., 2020) enables parameter-efficient transfer learning that scales well with new tasks.","To develop practical and memory-efficient adapters, Diff pruning (Guo et al., 2020) enables parameter-efficient transfer learning that scales well with new tasks.","Modify,Fact/Evidence",Fact/Evidence
4766,463-ARR,463-ARR_v2_6@1,463-ARR_v1_7@1,"The approach learns a task-specific ""diff"" vector that extends the original pre-trained parameters and encourages the sparsity of the vector through L 0 -norm regularization.","The approach learns a taskspecific ""diff"" vector that extends the original pretrained parameters and encourages the sparsity of the vector through L 0 -norm regularization.","Modify,Grammar",Grammar
4767,463-ARR,463-ARR_v2_7@4,463-ARR_v1_8@4,"Bit-Fit assigns identical shifts to all the tokens, while AdapterBias adds more significant shifts to the representations that are related to the task.","Bit-Fit assigns identical shifts to all the tokens, while AdapterBias adds more significant shifts to the tokens related to the task.","Modify,Clarity",Clarity
4768,463-ARR,463-ARR_v2_8@0,463-ARR_v1_9@0,"With fewer trainable parameters required, AdapterBias achieves comparable performance on the GLUE benchmark with Houlsby et al. (2019); Pfeiffer et al. (2020a); Guo et al. (2020);Ben Zaken et al. (2021);Hu et al. (2021).","With fewer trainable parameters required, AdapterBias achieves comparable performance on the GLUE benchmark with Houlsby et al. (2019); Pfeiffer et al. (2020a); Guo et al. (2020); Ben Zaken et al. (2021).","Modify,Fact/Evidence",Fact/Evidence
4769,463-ARR,463-ARR_v2_13@0,463-ARR_v1_14@0,"Recently, studies start to focus on improving the parameter-efficiency of adaptation to a new task.","Recently, studies start to focus on improving the parameter-efficiency of adapters.","Modify,Clarity",Clarity
4770,463-ARR,463-ARR_v2_13@3,463-ARR_v1_14@3,"Rücklé et al. (2020) introduced AdapterDrop, which has been recently integrated into AdapterHub (Pfeiffer et al., 2020b).","Rücklé et al. (2020) introduced Adap-terDrop, which has been recently integrated into AdapterHub (Pfeiffer et al., 2020b) by removing adapters from lower transformer layers during training and inference, which can dynamically reduce the computational cost.","Split+Modify,Grammar",Grammar
4771,463-ARR,463-ARR_v2_13@4,463-ARR_v1_14@3,"It removes adapters from lower transformer layers during training and inference, which can dynamically reduce the computational cost.","Rücklé et al. (2020) introduced Adap-terDrop, which has been recently integrated into AdapterHub (Pfeiffer et al., 2020b) by removing adapters from lower transformer layers during training and inference, which can dynamically reduce the computational cost.","Split+Modify,Clarity",Clarity
4772,463-ARR,463-ARR_v2_16@2,463-ARR_v1_17@2,AdapterBias produces a suitable weight for the bias based on the input token.,AdapterBias produces a suitable weight of the bias based on the input tokens.,"Modify,Grammar",Grammar
4773,463-ARR,463-ARR_v2_18@3,463-ARR_v1_20@3,The tokens which are more related to the task should be assigned larger representation shifts than other tokens.,"Since some tokens are more important to some tasks, these tokens should be assigned larger representation shifts than other tokens.","Modify,Clarity",Clarity
4774,465-ARR,,465-ARR_v1_25@0,,"In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.","Delete,Fact/Evidence",Fact/Evidence
4775,465-ARR,465-ARR_v2_15@0,,GCN also provides a powerful toolkit for embedding the taxonomies into low dimension representations that could be utilized for specific tasks.,,"Add,Fact/Evidence",Fact/Evidence
4776,465-ARR,465-ARR_v2_15@1,,"For instance, Pujary et al. (2020) used GCN to learn an undirected graph derived from disease names in the MeSH taxonomy in order to detect and normalize disease mentions in biomedical texts.",,"Add,Fact/Evidence",Fact/Evidence
4777,465-ARR,465-ARR_v2_29@0,,"MeSH taxonomies are organized in 16 categories, and each is further divided into subcategories.",,"Add,Fact/Evidence",Fact/Evidence
4778,465-ARR,465-ARR_v2_29@1,,"Within each subcategory, MeSH terms are ordered hierarchically from most general to most specific, up to 13 hierarchical levels.",,"Add,Fact/Evidence",Fact/Evidence
4779,465-ARR,465-ARR_v2_52@0,,"If the label appears in M , we assign 1, 0 otherwise.",,"Add,Fact/Evidence",Fact/Evidence
4780,465-ARR,465-ARR_v2_66@1,,"We also download the entire MEDLINE collection based on the PubMed Annual Baseline Repository (as of Dec. 2020) and obtain 31,850,051 citations with titles and abstracts.",,"Add,Fact/Evidence",Fact/Evidence
4781,465-ARR,465-ARR_v2_66@2,,"In order to reduce bias, we only focus on articles that are annotated by human curators (not annotated by a 'curated' or 'auto' modes in MEDLINE).",,"Add,Fact/Evidence",Fact/Evidence
4782,465-ARR,465-ARR_v2_66@3,,"We then match PMC articles with the citations in PubMed to PMID and obtain a set of 1,284,308 citations.",,"Add,Fact/Evidence",Fact/Evidence
4783,465-ARR,465-ARR_v2_66@4,,"Out of these PMC articles, we use the latest 20,000 articles as the test set, the next latest 200,000 articles as the validation data set, and the remaining 1.24M articles as the training set.",,"Add,Fact/Evidence",Fact/Evidence
4784,465-ARR,465-ARR_v2_66@5,,"In total, 28,415 distinct MeSH terms are covered in the training dataset.",,"Add,Fact/Evidence",Fact/Evidence
4785,465-ARR,465-ARR_v2_68@7,,"We use FAISS (Johnson et al., 2019) to find similar documents for each citation among the training set, and the whole process takes 10 hours.",,"Add,Fact/Evidence",Fact/Evidence
4786,465-ARR,465-ARR_v2_68@13,,The detailed hyper-parameter settings are shown in Table 3.,,"Add,Fact/Evidence",Fact/Evidence
4787,465-ARR,465-ARR_v2_68@14,,The code for our method is available at https://github.com/xdwang0726/KenMeSH.,,"Add,Fact/Evidence",Fact/Evidence
4788,465-ARR,465-ARR_v2_31@1,465-ARR_v1_29@1,"In the graph structure, we formulate each node as a MeSH label, and edges represent relationships in the MeSH hierarchy.","In the graph structure, we formulate each node as a MeSH label, and edges are implement MeSH hierarchies.","Modify,Clarity",Clarity
4789,465-ARR,465-ARR_v2_38@1,465-ARR_v1_36@1,We consider only a subset of the full MeSH list by employing a masked labelwise attention computes the element-wise multiplication of a mask matrix and an attention matrix for two reasons.,We only consider a subset of the full MeSH list and employ a masked label-wise attention that computes the element-wise multiplication of a mask matrix and an attention matrix for the following two reasons.,"Modify,Clarity",Clarity
4790,465-ARR,465-ARR_v2_39@0,465-ARR_v1_37@0,"For each article, selecting a subset of MeSH labels, namely a MeSH mask, downsamples the negative examples, which forces the classifier to concentrate on the candidate labels.","For each article, selecting a subset of MeSH labels, namely a MeSH mask, achieves down-sampling of the negative examples, which forces the classifier to concentrate on the candidate labels.","Modify,Clarity",Clarity
4791,465-ARR,465-ARR_v2_40@4,465-ARR_v1_38@4,"We build a journal-MeSH label cooccurrence matrix using conditional probabilities, i.e., P (L i | J j ), which denote the probabilities of occurrence of label L i when journal J j appears.","We build a co-occurrence matrix between journals and MeSH labels using conditional probabilities, i.e., P (L i | J j ), which denotes the probability of occurrence of label L i when journal J j appears.","Modify,Clarity",Clarity
4792,465-ARR,465-ARR_v2_42@1,465-ARR_v1_40@1,"To avoid the noise of rare co-occurrences, a threshold τ filters noisy correlations.","To avoid the noise of rare co-occurrences, we set a threshold τ to filter noisy correlations.","Modify,Clarity",Clarity
4793,465-ARR,465-ARR_v2_46@1,465-ARR_v1_45@1,"Next, we use KNN based on cosine similarity between abstracts to find the K nearest neighbours for each article in the training set.","Next, we calculate the cosine similarity between abstracts and use KNN to find k nearest neighbours for each article.","Modify,Clarity",Clarity
4794,465-ARR,465-ARR_v2_46@2,465-ARR_v1_45@2,"To form the unique MeSH mask for article a, we collect MeSH terms M a from the neighbours of a:","After that, we collect MeSH terms from neighbours and form as M n .","Modify,Fact/Evidence",Fact/Evidence
4795,465-ARR,465-ARR_v2_4@3,465-ARR_v1_4@3,"Currently, there are 29,369 main MeSH headings, and each MEDLINE citation has 13 MeSH indices, on average.","Currently, there are 29,369 main MeSH headings, and each MEDLINE citation html has 13 MeSH indices, on average.","Modify,Fact/Evidence",Fact/Evidence
4796,465-ARR,465-ARR_v2_68@1,465-ARR_v1_64@1,"For pre-processing, we removed nonalphanumeric characters, stop words, punctuation, and single character words, and we converted all words to lowercase.","For pre-processing, we removed nonalphanumeric characters, stop words, punctuation, and single character words, and we converted all words are lowercased.","Modify,Grammar",Grammar
4798,465-ARR,465-ARR_v2_68@12,465-ARR_v1_65@1,The model trained for 50 hours on a single NVIDIA V100 GPU.,The batch size is 32 and the model is trained on a single NVIDIA V100 GPU.,"Split+Modify,Fact/Evidence",Fact/Evidence
4799,465-ARR,465-ARR_v2_78@7,465-ARR_v1_75@7,"Although our model is trained only on the abstract and title (which may suggest that it captures less complex semantics), it performs very well against more complex systems.",Although our model is trained only on the abstract and title (which may suggest that it can capture less complex semantics) it performs very well against more complex systems.,"Modify,Clarity",Clarity
4800,465-ARR,465-ARR_v2_8@2,465-ARR_v1_7@2,"This module combines a hybrid of information, at the levels of words and the latent representations of the semantic units, to capture local correlations and longterm dependencies from text.","This module combines a hybrid of information, at the levels of words and latent semantics, to capture local correlations and long-term dependencies from text.","Modify,Clarity",Clarity
4801,465-ARR,465-ARR_v2_8@3,465-ARR_v1_7@3,2. Our proposed method appears to be the first to employ graph convolutional neural networks that integrate information from the complete MeSH hierarchy to map label representations.,2. Our proposed method appears to be the first to employ graph convolutional neural networks that integrate MeSH hierarchical information to map label representations.,"Modify,Clarity",Clarity
4802,465-ARR,465-ARR_v2_11@8,465-ARR_v1_10@8,The main difference between AttentionMeSH and MeSHProbeNet is that the former uses a label-wise attention mechanism while the latter develops selfattentive MeSH probes to extract comprehensive aspects of information from the input articles.,The main difference between AttentionMeSH and MeSHProbeNet is that the former uses a label-wise attention mechanism while the latter develops selfattentive MeSH probes to extract comprehensive aspects of biomedical information from the input articles.,"Modify,Fact/Evidence",Fact/Evidence
4803,465-ARR,465-ARR_v2_13@0,465-ARR_v1_12@0,Graph Convolutional Networks in Natural Language Processing,Graph Convolutional Network in Text Classification,"Modify,Other",Other
4804,465-ARR,465-ARR_v2_14@0,465-ARR_v1_13@0,"Graph convolutional neural networks (GCN)s (Kipf and Welling, 2017) have received considerable attention and achieved remarkable success in natural language processing recently.","Graph convolutional neural networks (GCN)s (Kipf and Welling, 2017) have received considerable attention recently.","Modify,Claim",Claim
4805,465-ARR,465-ARR_v2_14@4,465-ARR_v1_13@4,"Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multi-label text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels, and Rios and Kavuluru (2018) built a multilabel classifier that was learned from a 2-layer GCN over the label hierarchy.","Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multilabel text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels.","Modify,Fact/Evidence",Fact/Evidence
4806,468-ARR,,468-ARR_v1_56@5,,We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model.,"Delete,Claim",Claim
4807,468-ARR,468-ARR_v2_27@9,,This source sentence bias could best be mitigated by curating a new dataset which is why we chose not to focus our efforts on the Russian-English dataset.,,"Add,Claim",Claim
4808,468-ARR,468-ARR_v2_42@8,,"Column r shows the mean Pearson correlation of labels and predictions and the standard deviation over 5 runs, each training for 3 epochs = 15 minutes.",,"Add,Fact/Evidence",Fact/Evidence
4809,468-ARR,468-ARR_v2_42@9,,Column MSE is the average mean squared error.,,"Add,Fact/Evidence",Fact/Evidence
4810,468-ARR,468-ARR_v2_42@10,,"Column r target measures the performance when testing on the target sentence only and thus approximates the bias mitigation effect, where a smaller correlation is better.",,"Add,Fact/Evidence",Fact/Evidence
4811,468-ARR,468-ARR_v2_55@4,,"To further evaluate the generalisability of the proposed methods, experiments with additional datasets, low-resource language pairs as well as alternative QE architectures and language models could be conducted, too.",,"Add,Claim",Claim
4812,468-ARR,468-ARR_v2_56@2,,"Other observable biases could also be considered as candidates for the use of targeted bias reduction techniques, provided that it is possible to design a counterbalancing auxiliary task or isolate the bias well enough to deploy adversarial approaches.",,"Add,Claim",Claim
4813,468-ARR,468-ARR_v2_56@3,,"We think that if the latter scenario applies, the adapted debiased focal loss technique for regression could be worth further exploration, too.",,"Add,Claim",Claim
4814,468-ARR,468-ARR_v2_18@4,468-ARR_v1_18@4,"In this scenario, the bias model's predictions are used to weight the main model's cross-entropy loss.","In this scenario, the bias model's predictions are used to weigh the main model's cross-entropy loss.","Modify,Grammar",Grammar
4815,468-ARR,468-ARR_v2_22@0,468-ARR_v1_22@0,"We work with the MLQE-PE dataset (Fomicheva et al., 2020) which was specifically designed for the training of MT QE models.",We work with the MLQE-PE dataset which was specifically designed for the training of MT QE models.,"Modify,Fact/Evidence",Fact/Evidence
4816,468-ARR,468-ARR_v2_22@3,468-ARR_v1_22@3,"A seventh dataset, Russian-English, was curated from Reddit posts and WikiQuotes.","A seventh dataset, Russian-English, was collected based on Reddit posts and WikiQuotes.","Modify,Clarity",Clarity
4817,468-ARR,468-ARR_v2_27@8,468-ARR_v1_27@8,"The Russian-English dataset is an exception where the source sentence is a good predictor for the translation quality, most likely due to the distinct nature of Reddit data and WikiQuotes (both user-generated).","Perhaps due to the distinct nature of Reddit data and WikiQuotes (both user-generated), the Russian-English dataset is an exception where the source sentence is a good predictor for the translation quality.","Modify,Clarity",Clarity
4818,468-ARR,468-ARR_v2_28@2,468-ARR_v1_28@2,"To achieve this, one of the authors, a German native speaker, manually annotated translations in the test set that are grammatically correct but do not preserve the meaning of the source.","To achieve this, we manually annotated translations in the test set that are grammatically correct but do not preserve the meaning of the source.","Modify,Fact/Evidence",Fact/Evidence
4819,468-ARR,468-ARR_v2_28@4,468-ARR_v1_29@1,"A key takeaway from the labelling process was that it is not only the models that have a partial input bias -human annotators clearly seem to over-rely on the target fluency, too.",A key takeaway from the labelling process was that it is not only the models that have a partial input bias -human annotators clearly seem to over-rely on the target fluency.,"Modify,Clarity",Clarity
4820,468-ARR,468-ARR_v2_28@5,468-ARR_v1_29@2,"Even if the instructions clearly specify that a DA score below 70 should be assigned to inadequate translations, 4 annotators tended to give higher scores if the sentence was fluent and appeared logical.","Despite the instructions clarly specify that a DA score below 70 should be assigned to inadequate translations, 3 annotators tended to give higher scores if the sentence was fluent and appeared logical.","Modify,Fact/Evidence",Fact/Evidence
4821,468-ARR,468-ARR_v2_33@8,468-ARR_v1_34@8,"6 In both cases, the main task optimises the MSE loss, and the auxiliary task is a binary classification problem using the binary cross-entropy loss.","5 In both cases, the main task optimises the MSE loss, however, the auxiliary task is a binary classification problem using the binary cross-entropy loss.","Modify,Clarity",Clarity
4822,468-ARR,468-ARR_v2_35@3,468-ARR_v1_36@3,"The shared layers, on the other hand, are penalised for learning a mapping between target sentence and scores.","The shared layers, however, are penalised for learning a mapping between target sentence and scores.","Modify,Clarity",Clarity
4823,468-ARR,468-ARR_v2_41@2,468-ARR_v1_42@2,"Since our QE task is formulated as a regression problem, we attempt to find an equivalent strategy to weigh down biased examples when working with MSE loss.","Since our QE task is formulated as a regression problem, we attempt to find an equivalent strategy to down-weigh biased examples when working with MSE loss.","Modify,Clarity",Clarity
4824,468-ARR,468-ARR_v2_53@0,468-ARR_v1_54@0,"Since the reduction of the performance on the target sentence is only considering the reduction of the partial input bias, we additionally aim to test the model's ability to generalise better on datasets that barely exhibit the partial input bias.","Since the reduction of the performance on the target sentence is only considering the reduction of the partial input bias, we additionally test the models in a zero-shot setting on RO-EN data.","Split+Modify,Fact/Evidence",Fact/Evidence
4825,468-ARR,468-ARR_v2_53@1,468-ARR_v1_54@0,"As a feasible alternative to collecting an unbiased reference dataset in the same language domain, we assess the models' robustness in a zero-shot setting on less biased RO-EN data.","Since the reduction of the performance on the target sentence is only considering the reduction of the partial input bias, we additionally test the models in a zero-shot setting on RO-EN data.","Split+Modify,Claim",Claim
4826,468-ARR,468-ARR_v2_5@3,468-ARR_v1_5@3,"We work with the recently published multilingual QE dataset MLQE-PE (Fomicheva et al., 2020), allowing us to test the generalisability of our approaches across different languages and quality scores.","We work with the recently published multilingual QE dataset MLQE-PE , allowing us to test the generalisability of our approaches across different languages and quality scores.","Modify,Fact/Evidence",Fact/Evidence
4827,468-ARR,468-ARR_v2_53@6,468-ARR_v1_54@5,"However, the debiased MultiTransQuest models should outperform MonoTransQuest in this zeroshot scenario, which is indeed the case as can be seen from Table 2.","However, the debiased models should outperform the benchmark.","Merge+Modify,Clarity",Clarity
4828,468-ARR,468-ARR_v2_53@6,468-ARR_v1_54@6,"However, the debiased MultiTransQuest models should outperform MonoTransQuest in this zeroshot scenario, which is indeed the case as can be seen from Table 2.","Indeed, Table 2 shows that all MultiTransQuest models outperform MonoTransQuest in this zero-shot scenario.","Merge+Modify,Clarity",Clarity
4829,468-ARR,468-ARR_v2_55@1,468-ARR_v1_56@1,"Considering the experimental design, the multitask architecture provides additional degrees of freedom that were not explored extensively, yet.","Firstly, the multitask architecture provides additional degrees of freedom that were not explored extensively, yet.","Modify,Clarity",Clarity
4830,468-ARR,468-ARR_v2_55@2,468-ARR_v1_56@2,"For example, one could vary the amount of training per task or learn the training schedule as a parameter which adapts dynamically during the training process (Kiperwasser and Ballesteros, 2018;Zaremoodi et al., 2018).","For example, one could vary the amount of training per task or even learn the training schedule as a parameter which adapts dynamically during the training process (Kiperwasser and Ballesteros, 2018;Zaremoodi et al., 2018).","Modify,Clarity",Clarity
4831,468-ARR,468-ARR_v2_56@0,468-ARR_v1_56@4,"Going beyond the field of Machine Translation Quality Estimation, it would be interesting to see the methods applied in adjacent areas of NLP.","Furthermore, it would be interesting to apply our proposed methods and architectures in adjacent fields of NLP.","Modify,Claim",Claim
4832,468-ARR,468-ARR_v2_56@1,468-ARR_v1_56@6,"For example, this could entail closely related settings, such as quality estimation for machine-generated text summaries, as well as the fields of NLI and VQA, both of which face partial input biases.","The most promising approach of training a multitask architecture with a supportive auxiliary task might generalise well to related settings, such as quality estimation for machine-generated text summaries.","Merge+Modify,Claim",Claim
4833,468-ARR,468-ARR_v2_56@1,468-ARR_v1_56@7,"For example, this could entail closely related settings, such as quality estimation for machine-generated text summaries, as well as the fields of NLI and VQA, both of which face partial input biases.","Seeing the method applied in the fields of NLI and VQA, both of which face partial input biases, would be intriguing, too.","Merge+Modify,Claim",Claim
4834,468-ARR,468-ARR_v2_12@3,468-ARR_v1_12@3,"Sentence-level QE has evolved from the first feature-heavy prediction models (Blatz et al., 2004) to neural architectures such as RNNs and Transformers (Vaswani et al., 2017), which accelerated the developments in the field by reducing the work of manual feature engineering and improving contextual representations (Kim et al., 2017;Wang et al., 2018;Fan et al., 2019).","Sentencelevel QE has evolved from the first feature-heavy prediction models in Blatz et al. (2004) to neural architectures such as RNNs and Transformers (Vaswani et al., 2017), which accelerated the developments in the field by reducing the work of manual feature engineering and improving contextual representations (Kim et al., 2017;Wang et al., 2018;Fan et al., 2019).","Modify,Grammar",Grammar
4835,468-ARR,468-ARR_v2_13@6,468-ARR_v1_13@6,"Following their work, in an attempt to reduce statistical artifacts, MLQE-PE (Fomicheva et al., 2020) -a new QE dataset diversifying the topics and languages covered -was created, which forms the basis of this work and will be described in more detail in Section 3.1.","Following their work, in an attempt to reduce statistical artifacts, MLQE-PE -a new QE dataset diversifying the topics and languages covered -was created, which forms the basis of this work and will be described in more detail in Section 3.1.","Modify,Fact/Evidence",Fact/Evidence
4836,468-ARR,468-ARR_v2_18@1,468-ARR_v1_18@1,The notion of focal loss was first introduced by Lin et al. (2017) as a means to improve classification results on imbalanced classes by weighing down the impact of samples that the model had already learned to classify well.,The notion of focal loss was first introduced by Lin et al. (2017) as a means to improve classification results on imbalanced classes by downweighting the impact of samples that the model had already learned to classify well.,"Modify,Clarity",Clarity
4959,473-ARR,,473-ARR_v1_72@3,,"The training set contains 6,574 words and 67,861 entries.","Delete,Fact/Evidence",Fact/Evidence
4960,473-ARR,,473-ARR_v1_72@4,,Statistics are listed in Table 1.,"Delete,Fact/Evidence",Fact/Evidence
4961,473-ARR,,473-ARR_v1_81@2,,"Specifically, we use the BLEU (Papineni et al., 2002) and Semantic Similarity metrics to evaluate the accuracy, and use the SARI (Xu et al., 2016), and HSK Level metrics to evaluate the simplicity.","Delete,Fact/Evidence",Fact/Evidence
4962,473-ARR,473-ARR_v2_33@3,,The experiment results in Section 6 confirm our assumption.,,"Add,Fact/Evidence",Fact/Evidence
4963,473-ARR,473-ARR_v2_76@4,,We set 5 different random seeds as and report the average result of multiple runs.,,"Add,Fact/Evidence",Fact/Evidence
4964,473-ARR,473-ARR_v2_76@5,,Each run takes 7.68 GPU hours on 4 GeFource RTX 2080 Ti GPUs.,,"Add,Fact/Evidence",Fact/Evidence
4965,473-ARR,473-ARR_v2_97@0,,Table 8 shows two generation cases from English and Chinese test set respectively.,,"Add,Fact/Evidence",Fact/Evidence
4966,473-ARR,473-ARR_v2_97@1,,"In both cases, the golden definition is a long sentence with quite complicated syntax.",,"Add,Fact/Evidence",Fact/Evidence
4967,473-ARR,473-ARR_v2_26@0,473-ARR_v1_27@0,"The SDG task is to generate a simple definition d sim for a given word and context (w * , c), where c = [w 1 , . . . , w * , . . . , w n ] is a sentence containing w * .","The SDG task consists in generating a simple definition d sim for a given word and context (w * , c), where c = [w 1 , . . . , w * , . . . , w n ] is a sentence containing w * .","Modify,Clarity",Clarity
4968,473-ARR,473-ARR_v2_26@1,473-ARR_v1_27@1,"This task is challenging because there is no corpus like {(w * i , c i , d sim i )} N i=1 and hence it is fully unsupervised.","This task is challenging because there is no corpus like {(w * i , c i , d sim i )} N i=1 and hence fully unsupervised.","Modify,Clarity",Clarity
4969,473-ARR,473-ARR_v2_2@8,473-ARR_v1_2@8,"Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87% 1 .","Our method outperforms the baseline model by a 1.6 SARI score on the English dataset, and the low level (HSK level 1-3) words in Chinese definitions raised by 5.03%.","Modify,Fact/Evidence",Fact/Evidence
4970,473-ARR,473-ARR_v2_32@3,473-ARR_v1_33@3,"To solve the problem, we assume that complexity and semantic information are controlled by different parameters in the decoders, and we attempt to disentangle the complexity factors from the text through a carefully designed parameter sharing scheme.","To solve the problem, we attempt to disentangle the complexity factors from the text by sharing parameters between the generation and reconstruction decoders.","Merge+Modify,Clarity",Clarity
4971,473-ARR,473-ARR_v2_32@3,473-ARR_v1_33@4,"To solve the problem, we assume that complexity and semantic information are controlled by different parameters in the decoders, and we attempt to disentangle the complexity factors from the text through a carefully designed parameter sharing scheme.","This scheme shares semantic information between decoders, while keeping complexity information independent.","Merge+Modify,Claim",Claim
4972,473-ARR,473-ARR_v2_33@2,473-ARR_v1_34@2,"Instead, we introduce the language modeling task (Section 3.2.3) to enhance the reconstruction decoder and make it more focused on simple text generation.","Instead, we introduce the language modeling task (Section 3.2.3) to enhance the reconstruction decoder and make it more focusd on simple text generation.","Modify,Grammar",Grammar
4973,473-ARR,473-ARR_v2_37@1,473-ARR_v1_38@1,The model is optimized using the following loss function.,The model is optimized using the following loss function:,"Modify,Grammar",Grammar
4974,473-ARR,473-ARR_v2_54@0,473-ARR_v1_56@0,"For parameters in the decoders, we divided them into two parts, which are complexity-independent and complexity-dependent parameters.","For parameters in the decoders, we dived them into two parts, which are complexity-independent and complexity-dependent parameters.","Modify,Grammar",Grammar
4975,473-ARR,473-ARR_v2_4@1,473-ARR_v1_4@1,"In recent years, researchers attempted to automatically generate definitions for words rather than formulating predefined worddefinition inventories (Ishiwatari et al., 2019;Huang et al., 2021).","In recent years, researchers attempted to automatically generate definitions for words rather than formulating predefined worddefinition inventories (Ishiwatari et al., 2019;Yang et al., 2020;Huang et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
4976,473-ARR,473-ARR_v2_59@0,473-ARR_v1_61@0,We believe that the required information may vary for different complexity.,We believe that the required information may be various for different complexity.,"Modify,Clarity",Clarity
4977,473-ARR,473-ARR_v2_67@2,473-ARR_v1_69@2,We list more statistics in Table 2.,We list more detailed statistics in Table 2.,"Modify,Clarity",Clarity
4978,473-ARR,473-ARR_v2_75@5,473-ARR_v1_78@5,"Note that we set the learning rate to 3e-4, warmup steps to 500 when fine-tuning both MASS and MASS-ZH.","Note that we set the learning rate to 3e-4, warmup steps to 500, and random seed to 1111 when fine-tuning both MASS and MASS-ZH.","Modify,Fact/Evidence",Fact/Evidence
4979,473-ARR,473-ARR_v2_76@3,473-ARR_v1_79@3,We tune the λ parameters in Eq. 7 on the validation set and adopt the same hyper-parameters as the baseline for comparison.,We adopt the same hyper-parameters as the baseline for comparison.,"Modify,Fact/Evidence",Fact/Evidence
4980,473-ARR,473-ARR_v2_87@2,473-ARR_v1_87@4,We also observe that complex definition generation also improves by 0.17 on BLEU and 1.09 on SemSim.,We also observe that complex definition generation also slightly improves by 0.31 on BLEU and 0.82 on SemSim.,"Modify,Fact/Evidence",Fact/Evidence
4981,473-ARR,473-ARR_v2_87@3,473-ARR_v1_87@5,This shows that SimpDefiner improves the ability to generate both complex and simple definitions.,This indicates that SimpDefiner improves the ability to generate both complex and simple definitions.,"Modify,Clarity",Clarity
4982,473-ARR,473-ARR_v2_5@3,473-ARR_v1_6@0,"Different from previous work (Noraset et al., 2017;Gadetsky et al., 2018;Mickus et al., 2019; that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).","Different from previous work (Noraset et al., 2017;Gadetsky et al., 2018;Mickus et al., 2019;Kong et al., 2020) that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).","Modify,Fact/Evidence",Fact/Evidence
4983,473-ARR,473-ARR_v2_88@1,473-ARR_v1_88@1,"We can see that the proportion of low-level (HSK level 1-3) words increases by 3.87%, and that of high-level (HSK level 7+) words decreases by 0.46%.","We can see that the proportion of low-level (HSK level 1-3) words increases by 5.03%, and that of high-level (HSK level 7+) words decreases by 1.61%.","Modify,Fact/Evidence",Fact/Evidence
4984,473-ARR,473-ARR_v2_91@2,473-ARR_v1_91@2,"For the layer normalization (LN) and query projection (QP) as parameter-shared layers, we ablate them by sharing their parameters between models.","For the layer normalization (LN) and query projection (QP) as parameter-shared layers, we ablate them by share their parameters between models.","Modify,Grammar",Grammar
4985,473-ARR,473-ARR_v2_5@4,473-ARR_v1_6@1,"Making the definitions easier to read and understand could benefit the language learners, low literacy readers, as well as helping people with aphasia or dyslexia.","Make the definitions easier to read and understand could benefit the language learners, low literacy readers, as well as helping people with aphasia or dyslexia.","Modify,Grammar",Grammar
4986,473-ARR,473-ARR_v2_5@5,473-ARR_v1_6@2,"For example, compared with the Oxford Dictionary (OD), the Oxford Advanced Learner's Dictionary (OALD) has simpler definitions, which are specifically designed for language learners.","For example, compared with the Oxford Dictionary (OD), the Oxford Advanced Learner's Dictionary (OALD) has simpler definitions, which is specifically designed for language learners.","Modify,Grammar",Grammar
4987,473-ARR,473-ARR_v2_2@2,473-ARR_v1_2@2,We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers.,We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers better.,"Modify,Clarity",Clarity
4988,473-ARR,473-ARR_v2_14@1,473-ARR_v1_15@1,"Gadetsky et al. (2018) tackled this problem by computing the AdaGram vectors (Bartunov et al., 2016) of input words, which are capable of learning different representations at desired semantic resolutions.","Gadetsky et al. (2018) tackled this problem by computing the AdaGram vectors (Bartunov et al., 2016) of input words, which is capable to learn different representations at desired semantic resolution.","Modify,Grammar",Grammar
4989,473-ARR,473-ARR_v2_14@4,473-ARR_v1_15@4,"Reid et al. (2020) initialized encoders with BERT (Devlin et al., 2019) and employed variational inference for estimation and leveraged contextualized word embeddings for improved performance.","Reid et al. (2020) initialized encoders with BERT (Devlin et al., 2019) and employed variational inference for estimation and leverage contextualized word embeddings for improved performance.","Modify,Grammar",Grammar
4990,473-ARR,473-ARR_v2_14@5,473-ARR_v1_15@5,Bevilacqua et al. (2020) employed a novel spanbased encoding scheme to fine-tune a pre-trained English encoder-decoder system to generate definitions.,Bevilacqua et al. (2020) employed a novel span-based encoding scheme to fine-tune a pre-trained English encoderdecoder system to generate definitions.,"Modify,Grammar",Grammar
4991,473-ARR,473-ARR_v2_15@2,473-ARR_v1_16@2,"Besides, our model is based on MASS (Song et al., 2019), which is a pre-trained encoder-decoder model and is suitable for generation tasks.","Besides, our model is based on MASS (Song et al., 2019), which is a pre-trained encoder-decoder model and is more suitable for generation tasks.","Modify,Grammar",Grammar
4992,473-ARR,473-ARR_v2_2@5,473-ARR_v1_2@5,We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders.,We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between the components.,"Modify,Clarity",Clarity
4993,473-ARR,473-ARR_v2_18@3,473-ARR_v1_19@3,"Martin et al. (2019) proposed to prepend additional prompt tokens to source sentences at train time, which enables the end-users to condition the simplifications returned by the model on attributes like length, lexical complexity, and syntactic complexity.","Martin et al. (2019) proposed to prepend additional prompt tokens to source sentences at train time, which enable the end-users to condition the simplifications returned by the model on attributes like length, lexical complexity, and syntactic complexity.","Modify,Grammar",Grammar
4994,473-ARR,473-ARR_v2_2@6,473-ARR_v1_2@6,"By jointly training these components, the framework can generate both complex and simple definitions simultaneously.","By joint training these components, the framework can generate both complex and simple definitions simultaneously.","Modify,Grammar",Grammar
4995,473-ARR,473-ARR_v2_19@1,473-ARR_v1_20@1,"Unlike the baseline, the SimpDefiner can generate simple definitions directly, alleviating the accumulated errors.","Unlike the baseline, the SimpDefiner can simultaneously generate complex and simple definitions without the need for aligned complex-simple sentence pairs.","Modify,Claim",Claim
5489,61-ARR,,61-ARR_v1_24@1,,Table A6 compares the slot and relative slot accuracies.,"Delete,Fact/Evidence",Fact/Evidence
5490,61-ARR,,61-ARR_v1_24@2,,The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3.,"Delete,Fact/Evidence",Fact/Evidence
5491,61-ARR,,61-ARR_v1_24@3,,"In addition, regarding slot accuracy in turns 4, 5, and 6, there is no score improvement for the additional wellpredicted state by the model, whereas the score increases when the newly added state is matched in the case of relative slot accuracy.","Delete,Fact/Evidence",Fact/Evidence
5492,61-ARR,,61-ARR_v1_24@4,,"Therefore, relative slot accuracy can provide an intuitive evaluation reflecting the current belief state recording method, in which the number of slots accumulates incrementally as the conversation progresses.","Delete,Claim",Claim
5493,61-ARR,61-ARR_v2_15@0,,Other Metric,,"Add,Other",Other
5494,61-ARR,61-ARR_v2_16@0,,"Recently, Rastogi et al. (2020b) proposed a metric called average goal accuracy.",,"Add,Fact/Evidence",Fact/Evidence
5495,61-ARR,61-ARR_v2_16@1,,"The main difference between the average goal accuracy and the proposed relative slot accuracy is that the average goal accuracy only considers the slots with non-empty values in the gold states of each turn, whereas the proposed relative slot accuracy considers those in both gold and predicted states.",,"Add,Fact/Evidence",Fact/Evidence
5496,61-ARR,61-ARR_v2_16@2,,"Since average goal accuracy ignores the predicted states, it cannot properly distinguish a better model from a worse model in some specific situations.",,"Add,Claim",Claim
5497,61-ARR,61-ARR_v2_16@3,,We will discuss it in more detail in Section 4.1.,,"Add,Fact/Evidence",Fact/Evidence
5498,61-ARR,61-ARR_v2_25@0,,Table 1 presents the overall results.,,"Add,Fact/Evidence",Fact/Evidence
5499,61-ARR,61-ARR_v2_25@1,,"Regarding slot accuracy, the difference between the largest and smallest values is solely 1.09%.",,"Add,Fact/Evidence",Fact/Evidence
5500,61-ARR,61-ARR_v2_25@2,,It can be one of the reasons that several researchers do not report it.,,"Add,Claim",Claim
5501,61-ARR,61-ARR_v2_25@3,,"Meanwhile, relative slot accuracy can explicitly highlight the deviation among models by showing a 5.47% difference between the largest and smallest values.",,"Add,Fact/Evidence",Fact/Evidence
5502,61-ARR,61-ARR_v2_26@4,61-ARR_v1_22@4,"In summary, relative slot accuracy enables relative comparison according to the distribution of the domain in a dialogue.","In summary, relative slot accuracy enables relative comparison according to the distribution of the domain in a dialog.","Modify,Grammar",Grammar
5503,61-ARR,61-ARR_v2_26@5,61-ARR_v1_23@0,"Dependency on Predefined Slots As discussed in Section 2.2, slot accuracy requiring total predefined slots is not a scalable method for evaluating the current dialogue dataset that contains a few domains in each dialogue.","Dependency on predefined slots As discussed in Section 2.2, slot accuracy requiring total predefined slots is not a scalable method for evaluating the current dialog dataset that contain a few domains in each dialog.","Modify,Grammar",Grammar
5504,61-ARR,61-ARR_v2_26@6,61-ARR_v1_23@1,"For example, when evaluating a dialogue sample that solely deals with the restaurant domain, even domains that never appear at all (i.e., hotel, train, attraction, and taxi) are involved in measuring performance, making deviations among different models trivial.","For example, when evaluating a dialog sample that solely deals with the restaurant domain, even domains that never appear at all (i.e., hotel, train, attraction, and taxi) are involved in measuring performance, making deviations among different models trivial.","Modify,Grammar",Grammar
5505,61-ARR,61-ARR_v2_26@7,61-ARR_v1_23@2,"However, relative slot accuracy can evaluate the model's predictive score without being affected by slots never seen in the current dialogue, which is a more realistic way, considering that each dialogue contains its own turn and slot composition.","However, relative slot accuracy can evaluate the model's predictive score without being affected by slots never seen in the current dialog, which is a more realistic way, considering that each dialog contains its own turn and slot composition.","Modify,Grammar",Grammar
5506,61-ARR,61-ARR_v2_29@2,61-ARR_v1_26@2,"This metric is not affected by unseen slots in the current dialogue situation, and compensates for the model's correct predic-tion.","This metric is not affected by unseen slots in the current dialog situation, and compensates for the model's correct prediction.","Modify,Grammar",Grammar
5507,61-ARR,61-ARR_v2_30@0,61-ARR_v1_27@0,"Our findings show that if the model makes an incorrect prediction, the error accumulates until the end of the dialogue, and the joint goal accuracy remains at zero.","Our findings show that if the model makes an incorrect prediction, the error accumulates until the end of the dialog, and the joint goal accuracy remains at zero.","Modify,Grammar",Grammar
5508,61-ARR,61-ARR_v2_30@1,61-ARR_v1_27@1,"In this section, we discuss a few cases of 59 dialogues that do not show the trend among 642 dialogues selected in Section 2.1; however, it is important to note that these few cases have negligible effect on the trend in Figure 1, solely changing the position where the joint goal accuracy first becomes zero.","In this section, we discuss a few cases of 59 dialogs that do not show the trend among 642 dialogs selected in Section 2.1; however, it is important to note that these few cases have negligible effect on the trend in Figure 1, solely changing the position where the joint goal accuracy first becomes zero.","Modify,Grammar",Grammar
5509,61-ARR,61-ARR_v2_31@0,61-ARR_v1_27@2,"We sampled dialogues of the MultiWOZ 2.1 test set in Table A1 and Table A2, and marked values appearing in the dialogue in bold.","We sampled dialogs of the MultiWOZ 2.1 test set in Table A1 and Table A2, and marked values appearing in the dialog in bold.","Modify,Grammar",Grammar
5510,61-ARR,61-ARR_v2_31@2,61-ARR_v1_27@4,"In the first dialogue presented in Table A1, the joint goal accuracy is measured as 1 at turn 2.","In the first dialog presented in Table A1, the joint goal accuracy is measured as 1 at turn 2.","Modify,Grammar",Grammar
5511,61-ARR,61-ARR_v2_32@0,61-ARR_v1_28@0,"The second dialogue presented in Table A2, reports the incorrect prediction according to the interpretation of annotations at turn 4.","The second dialog presented in Table A2, reports the incorrect prediction according to the interpretation of annotations at turn 4.","Modify,Grammar",Grammar
5512,61-ARR,61-ARR_v2_32@1,61-ARR_v1_28@1,"In other words, because the dialogue about the hotel-internet slot appears over turns 4 and 5, it is solely an error depending on the prediction timing of the model.","In other words, because the dialog about the hotel-internet slot appears over turns 4 and 5, it is solely an error depending on the prediction timing of the model.","Modify,Grammar",Grammar
5513,61-ARR,61-ARR_v2_33@1,61-ARR_v1_29@1,"Furthermore, the fact that the starting point of making the joint goal accuracy of subsequent turns to 0 mainly occurs at the beginning of the dialogue does not change.","Furthermore, the fact that the starting point of making the joint goal accuracy of subsequent turns to 0 mainly occurs at the beginning of the dialog does not change.","Modify,Grammar",Grammar
5514,61-ARR,61-ARR_v2_2@0,61-ARR_v1_2@0,Dialogue state tracking (DST) aims to extract essential information from multi-turn dialogue situations and take appropriate actions.,Dialogue state tracking (DST) aims to extract essential information from multi-turn dialog situations and take appropriate actions.,"Modify,Grammar",Grammar
5515,61-ARR,61-ARR_v2_6@1,61-ARR_v1_6@1,"To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ dataset.","To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ.","Modify,Clarity",Clarity
5516,61-ARR,61-ARR_v2_6@2,61-ARR_v1_6@2,"While slot accuracy has the challenge of overestimation by always considering all predefined slots in every turn, relative slot accuracy does not depend on predefined slots, and calculates a score that is affected solely by slots that appear in the current dialogue.","Because slot accuracy has the challenge of overestimation by always considering all predefined slots in every turn, relative slot accuracy does not depend on predefined slots, and calculates a score that is affected solely by slots that appear in the current dialog.","Modify,Clarity",Clarity
5517,61-ARR,61-ARR_v2_9@3,61-ARR_v1_9@3,"As illustrated in Figure 1, we measured the relative position of the turn causing this phenomenon for the dialogue.","As illustrated in Figure 1, we measured the relative position of the turn causing this phenomenon for the dialog.","Modify,Grammar",Grammar
5518,61-ARR,61-ARR_v2_9@6,61-ARR_v1_9@6,"Accordingly, the relative position where joint goal accuracy first became zero was mainly at the beginning of the dialogue 1 .","Accordingly, the relative position where joint goal accuracy first became zero was mainly at the beginning of the dialog 1 .","Modify,Grammar",Grammar
5519,61-ARR,61-ARR_v2_9@7,61-ARR_v1_9@7,"This means that the joint goal accuracy after the beginning of the dialogue is unconditionally measured as zero because of the initial misprediction, although the model may correctly predict new belief states at later turns.","This means that the joint goal accuracy after the beginning of the dialog is unconditionally measured as zero because of the initial misprediction, although the model may correctly predict new belief states at later turns.","Modify,Grammar",Grammar
5520,61-ARR,61-ARR_v2_9@8,61-ARR_v1_9@8,"Failure to measure the performance of the latter part means that it cannot consider various dialogue situations provided in the dataset, which is a critical issue in building a realistic DST model.","Failure to measure the performance of the latter part means that it cannot consider various dialog situations provided in the dataset, which is a critical issue in building a realistic DST model.","Modify,Grammar",Grammar
5521,61-ARR,61-ARR_v2_11@0,61-ARR_v1_11@0,Slot accuracy can compensate for situations where joint goal accuracy does not fully evaluate the dialogue situation.,Slot accuracy can compensate for situations where joint goal accuracy does not fully evaluate the dialog situation.,"Modify,Grammar",Grammar
5522,61-ARR,61-ARR_v2_13@1,61-ARR_v1_13@1,"Each value of x-axis in Figure 2 indicates the ""maximum"" number of slots that appear in a single dialogue, and we confirmed that approximately 85% of the test set utilized solely less than 12 of the 30 predefined slots in the experiment.","Each value of x-axis in Figure 2 indicates the ""maximum"" number of slots that appear in a single dialog, and we confirmed that approximately 85% of the test set utilized solely less than 12 of the 30 predefined slots in the experiment.","Modify,Grammar",Grammar
5523,61-ARR,61-ARR_v2_13@2,61-ARR_v1_13@2,"Because the number of belief states appearing in the early and middle turns of the dialogue are smaller, and even fewer states make false predictions, calculating slot accuracy using Equation 2 reduces the influence of M and W , and the final score is dominated by the total slot number T .","Because the number of belief states appearing in the early and middle turns of the dialog are smaller, and even fewer states make false predictions, calculating slot accuracy using Equation 2 reduces the influence of M and W , and the final score is dominated by the total slot number T .","Modify,Grammar",Grammar
5524,61-ARR,61-ARR_v2_18@1,61-ARR_v1_16@1,"The deviation among DST models will be even more minor when constructing datasets with various dialogue situations, because the number of predefined slots will continually in-crease.","The deviation among DST models will be even more minor when constructing datasets with various dialog situations, because the number of predefined slots will continually increase.","Modify,Grammar",Grammar
5525,61-ARR,61-ARR_v2_23@0,61-ARR_v1_20@0,"We measured MultiWOZ 2.1, an improved version of MultiWOZ 2.0 , which has been adopted in several studies, according to Table A5.","We measured MultiWOZ 2.1, an improved version of MultiWOZ 2.0 (Budzianowski et al., 2018), which has been adopted in several studies, according to Table A5.","Modify,Fact/Evidence",Fact/Evidence
5526,61-ARR,61-ARR_v2_2@4,61-ARR_v1_2@4,"Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialogue.","Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialog.","Modify,Grammar",Grammar
5607,68-ARR,,68-ARR_v1_24@1,,"In our model, we use a bag-of-words decoder as the generator.","Delete,Fact/Evidence",Fact/Evidence
5608,68-ARR,,68-ARR_v1_70@0,,The results of 12 languages in XQuAD and M-LQA are shown in Table 3.,"Delete,Fact/Evidence",Fact/Evidence
5609,68-ARR,,68-ARR_v1_5@0,,"For the inaccuracy of boundary detection, existing methods mainly solve this issue by introducing external knowledge.","Delete,Claim",Claim
5610,68-ARR,,68-ARR_v1_76@0,,5.2 Why Use a Siamese Network in S 2 DM?,"Delete,Other",Other
5611,68-ARR,,68-ARR_v1_77@0,,"In order to separate semantic information from PLMs, an alternative way is to train a single net- Corresponding to S 2 DM, there are also two single-network variants: S 2 DM_single_POS and S 2 DM_single_SP.","Delete,Claim",Claim
5612,68-ARR,68-ARR_v2_5@0,,"In order to address this issue, existing methods mainly resort to external resources.",,"Add,Claim",Claim
5613,68-ARR,,68-ARR_v1_9@0,,"Besides, the multilingual pre-trained models have learned an amount of linguistic structure of multiple languages (Chi et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
5614,68-ARR,68-ARR_v2_66@0,,"Furthermore, we compared with a strong baseline that uses external knowledge to enhance crosslingual MRC:",,"Add,Fact/Evidence",Fact/Evidence
5615,68-ARR,68-ARR_v2_67@0,,"LAKM is a pre-trained task proposed in (Yuan et al., 2020) by introducing external sources for phrase-level masked language modeling task.",,"Add,Fact/Evidence",Fact/Evidence
5616,68-ARR,68-ARR_v2_67@1,,"The external corpus contain 363.5k passages and 534k knowledge phrases in four languages: English (en), French (fr), German (de), and Spanish (es).",,"Add,Fact/Evidence",Fact/Evidence
5617,68-ARR,,68-ARR_v1_9@1,,The semantic can not directly disentangle from complex syntactic information in PLMs.,"Delete,Claim",Claim
5618,68-ARR,68-ARR_v2_73@2,,"On the MLQA dataset, LAKM uses a larger extra corpus to train a better backbone language model, while our method with less external data can still achieve similar performance in German (de) and Spanish (es).",,"Add,Fact/Evidence",Fact/Evidence
5619,68-ARR,68-ARR_v2_83@2,,"Combined with the negative/positive results of syntactic/semantic vectors in the cross-lingual STS task in SemEval-2017, the visualization demonstrates that S 2 DM can efficiently disassociate semantics from syntax.",,"Add,Fact/Evidence",Fact/Evidence
5620,68-ARR,,68-ARR_v1_14@1,,Various approaches have been proposed on top of multilingual MRC based on PLMs.,"Delete,Claim",Claim
5621,68-ARR,,68-ARR_v1_14@2,,Cui et al. (2019) propose a method that combines multilingual BERT and back-translation for cross-lingual MRC.,"Delete,Fact/Evidence",Fact/Evidence
5622,68-ARR,,68-ARR_v1_2@4,,"Moreover, we theoretically analyze the generalization of our decoupling model, even for low-resource languages without training corpus.","Delete,Fact/Evidence",Fact/Evidence
5623,68-ARR,68-ARR_v2_20@2,68-ARR_v1_20@2,"Once S 2 DM is trained, only the output of source language MLP network is fed into the linear output layer for MRC.","Once S 2 DM is trained, only the output of source language MLP network input to linear output layer for MRC.","Modify,Clarity",Clarity
5624,68-ARR,68-ARR_v2_25@0,68-ARR_v1_25@0,"The variational inference process of VG-VAE uses a factorized approximated posterior q ϕ (y|x)q ϕ (z|x) = q ϕ (y, z|x) with the objective function that maximizes a lower bound of the marginal log-likelihood:","The variational inference process of VG-VAE uses a factorized approximated posterior q φ (y|x)q φ (z|x) = q φ (y, z|x) with the objective function that maximizes a lower bound of the marginal log-likelihood:","Modify,Grammar",Grammar
5625,68-ARR,68-ARR_v2_30@3,68-ARR_v1_28@4,"As pre-trained representations are contextually-encoded token vectors, latent variable vectors obtained by sampling from the distributions need to be averaged so as to output sentence-level semantic and syntactic vector.","The pretrained representations of sentence are contextuallyencoded token vectors, so the latent variable vectors obtained by sampling from the distributions need to be averaged, then output sentence-level semantic vector and syntactic vector.","Modify,Clarity",Clarity
5626,68-ARR,68-ARR_v2_4@0,68-ARR_v1_4@0,"Multilingual pre-trained language models (PLMs) (Devlin et al., 2019;Conneau and Lample, 2019;Conneau et al., 2020) have been widely explored in cross-lingual understanding tasks.","Multilingual pre-trained language models (PLMs) (Devlin et al., 2019;Conneau and Lample, 2019;Conneau et al., 2020) have been widely utilized in cross-lingual understanding tasks.","Modify,Clarity",Clarity
5627,68-ARR,68-ARR_v2_34@0,68-ARR_v1_35@0,"In order to guide S 2 DM to disassociate syntactic information into the syntactic latent variable z, we also define three losses tailored for capturing different types of syntactic information.","In order to guide S 2 DM to disassociate syntactic information into the syntactic latent variable z, we also define three types of losses tailored for capturing different types of syntactic information.","Modify,Clarity",Clarity
5628,68-ARR,68-ARR_v2_34@1,68-ARR_v1_35@1,"First, we employ Word Position Loss (WPL) , defined as follows:","First, we employ Word Position Loss (WPL) , as follow:","Modify,Clarity",Clarity
5629,68-ARR,68-ARR_v2_4@1,68-ARR_v1_4@1,"However, zero-shot transfer method based on multilingual PLMs does not work well for low-resource language MRC.","However, the zero-shot transfer method based on multilingual PLM has limited effectiveness for low-resource languages MRC tasks.","Modify,Clarity",Clarity
5630,68-ARR,68-ARR_v2_4@2,68-ARR_v1_4@2,"Such multilingual MRC models could roughly detect answer spans but may fail to predict the precise boundaries of answers (Yuan et al., 2020).","This type of multilingual MRC model could roughly detect answer spans but fail to predict the precise boundaries of answers (Yuan et al., 2020).","Modify,Clarity",Clarity
5631,68-ARR,68-ARR_v2_49@0,68-ARR_v1_50@0,Generalization Analysis,Analysis,"Modify,Clarity",Clarity
5632,68-ARR,68-ARR_v2_55@1,68-ARR_v1_52@4,"In the same way, both p θ (x s , y s ) and p θ (x t , y t ) also fit to the same distribution, no matter what the target language is.","In the same way, p θ (x s , y s ) and p θ (x t , y t ) also fit to the same distribution, no matter what the target language is.","Modify,Clarity",Clarity
5633,68-ARR,68-ARR_v2_56@0,68-ARR_v1_54@0,"Furthermore, the semantic discrimination loss in Eq.( 5) guarantees that the semantic vectors of the source language and the target language are similar to each other.","Furthermore, the semantic discrimination loss Eq.( 5) guarantees that the semantic vectors of the source language and the target language are similar.","Modify,Clarity",Clarity
5634,68-ARR,68-ARR_v2_56@1,68-ARR_v1_54@1,"Minimizing Eq.( 5) can be equivalent to: sim(y s , y t ) > sim(y s , n t ) + δ sim(y s , y t ) > sim(n s , y t ) + δ which is to maximize sim(y s , y t ) to encourages the target semantic vector to approach parallel source semantic vector.","Minimizing Eq.( 5) can be equivalent to: sim(ys, yt) > sim(ys, nt) + δ sim(ys, yt) > sim(ns, yt) + δ which is to maximize sim(y s , y t ) to encourages the target semantic vector to approach parallel source semantic vector.","Modify,Grammar",Grammar
5635,68-ARR,68-ARR_v2_56@3,68-ARR_v1_55@1,"Therefore, our multilingual MRC model is suitable even for low-resource languages without training data for the decoupling model.","Therefore, our multilingual MRC model is suitable even for low-resource languages without training data of decoupling model.","Modify,Grammar",Grammar
5636,68-ARR,68-ARR_v2_59@0,68-ARR_v1_58@0,"XQuAD (Artetxe et al., 2020) consists of 11 datasets of different languages translated from the SQuAD v1.1 (Rajpurkar et al., 2016) development set, including Spanish (es), German (de), Greek (el), Russian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), and Romanian (ro).","XQuAD (Artetxe et al., 2020) consists of 11 languages corpus translated from the SQuAD v1.1 (Rajpurkar et al., 2016) development set, include : Spanish (es), German (de), Greek (el), Russian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), and Romanian (ro).","Modify,Clarity",Clarity
5637,68-ARR,68-ARR_v2_61@0,68-ARR_v1_60@0,"TyDi QA-GoldP is the gold passage task in TyDi QA (Clark et al., 2020) covering 9 typologically diverse languages: Arabic (ar), Bengali (bg), English (en), Finnish (fi), Indonesian (id), Korean (ko), Russian (ru), Swahili (sw), Telugu (te).","TyDi QA-GoldP is the gold passage task in Ty-Di QA (Clark et al., 2020) covers 9 typologically diverse languages: Arabic (ar), Bengali (bg), English (en), Finnish (fi), Indonesian (id), Korean (ko), Russian (ru), Swahili (sw), Telugu (te).","Modify,Grammar",Grammar
5638,68-ARR,68-ARR_v2_69@0,68-ARR_v1_66@0,"For S 2 DM, we collected 26k labelled parallel sentence pairs from the Universal Dependencies (UD 2.7) Corpus (Zeman et al., 2020) as the training set.","For S 2 DM, we collected approximately 26,000+ labelled parallel sentence pairs from the Universal Dependencies (UD 2.7) Corpus (Zeman et al., 2020) as the training set.","Modify,Clarity",Clarity
5639,68-ARR,68-ARR_v2_69@1,68-ARR_v1_66@1,The training set covers 20 languages and overlap with 13 languages of three MRC datasets.,The training set covers 20 languages and overlap with 13 languages of three MRC evaluation datasets.,"Modify,Clarity",Clarity
5640,68-ARR,68-ARR_v2_70@0,68-ARR_v1_67@0,"For our multilingual MRC models and two baseline models, we fine-tuned them on the SQuAD v1.1 (Rajpurkar et al., 2016) and evaluated them on the test data of the three multilingual MRC datasets.","For our multilingual MRC models and two baseline models, we fine-tuned them on the SQuAD v1.1 (Rajpurkar et al., 2016) and evaluated the test data of the three multilingual MRC datasets.","Modify,Clarity",Clarity
5641,68-ARR,68-ARR_v2_72@4,68-ARR_v1_69@4,"Especially, compared with baselines on the TyDi QA-Gold dataset, S 2 DM_SP based on XLM-100 and mBERT gains 4.1%, 4.2% EM improvements on average across 9 languages, respectively.","Especially, compared with baselines on TyDi QA-Gold datasets, S 2 DM_SP based on XLM-100 and mBERT gains 4.1%, 4.2% EM respectively improvements on average across 9 languages.","Modify,Clarity",Clarity
5642,68-ARR,68-ARR_v2_73@1,68-ARR_v1_70@1,"For cross-lingual transfer performance, our models are better than the two baselines in terms of either EM or F1 on all 11 low-resource target languages.","For cross-lingual transfer performance, our models are better than the two baselines on EM or F1 of all 11 low-resource target languages.","Modify,Clarity",Clarity
5643,68-ARR,68-ARR_v2_5@3,68-ARR_v1_4@5,"(b) A case from BiPaR (Jing et al., 2019) where the answer predicted by a model transferred from English to Chinese violates syntactic constituent boundaries in the target language.","(b) A case from BiPaR (Jing et al., 2019) where the answer predicted by the model transferred from English to Chinese violates syntactic constituent boundaries.","Modify,Fact/Evidence",Fact/Evidence
5644,68-ARR,68-ARR_v2_74@1,68-ARR_v1_70@3,"The results of TyDi QA-GoldP are shown in Table 4, and our models are superior to the baselines in terms of either EM or F1 for all 8 low-resource target languages.","The results of TyDi QA-GoldP are shown in Table 4, and our models are superior to the baselines on EM or F1 of all 8 low-resource target languages.","Modify,Clarity",Clarity
5645,68-ARR,68-ARR_v2_75@1,68-ARR_v1_71@1,The results on the three datasets show the effectiveness on five languages not included in the training target languages for S 2 DM.,The results in three datasets show the effectiveness of five languages not included in the training target languages for S 2 DM.,"Modify,Grammar",Grammar
5646,68-ARR,68-ARR_v2_78@1,68-ARR_v1_74@1,The results are shown in Figure 3.,The results are reported in Table 3.,"Modify,Clarity",Clarity
5647,68-ARR,68-ARR_v2_79@3,68-ARR_v1_75@3,"All ablation models do not exceed our best model, illustrating the importance of all proposed losses.","All ablation models do not exceed our best model, illustrating the importance of all our losses.","Modify,Clarity",Clarity
5648,68-ARR,68-ARR_v2_80@0,68-ARR_v1_78@0,Why the S 2 DM Works?,Why S 2 DM Works?,"Modify,Grammar",Grammar
5649,68-ARR,68-ARR_v2_6@1,68-ARR_v1_5@2,"Liang et al. (2021) present a boundary calibration model stacked over a base sequence labeling module, introducing a phrase boundary recovery task to pretrain the calibration module on large-scale multilingual datasets synthesized from Wikipedia documents.","Liang et al. (2021) propose a separate model for boundary calibration based on the output of a base zero-shot transfer model, introducing a phrase boundary recovery task to pretrain calibration module on large-scale multilingual datasets synthesized from Wikipedia documents.","Modify,Fact/Evidence",Fact/Evidence
5650,68-ARR,68-ARR_v2_86@2,68-ARR_v1_83@2,"For low-resource languages without training data for the decoupling model, our theoretical analysis and experiments verify the generalization of our multilingual MRC model.","For low-resource languages without training data of decoupling model, we theoretically analyze and experiment verify the generalization of our multilingual MRC model.","Modify,Clarity",Clarity
5651,68-ARR,68-ARR_v2_6@3,68-ARR_v1_6@1,"On four multilingual MRC datasets, we use Stanford CoreNLP 1 to collect syntax parse trees and calculate the percentages of ground-truth answers that respect syntactic constituent boundaries.","On four multilingual MRC evaluation datasets, we use Stanford CoreNLP 1 to collect syntax parse trees and calculate the percentages of ground-truth answers that respect syntactic constituent boundaries.","Modify,Clarity",Clarity
5652,68-ARR,68-ARR_v2_6@4,68-ARR_v1_6@2,"As shown in Table 1, over 87% of answer spans respect the syntactic constraint.","As shown in Table 1, over 87% of answer spans respect syntactic constraint.","Modify,Grammar",Grammar
5653,68-ARR,68-ARR_v2_7@1,68-ARR_v1_7@1,"For questions where the monolingual model correctly predicts the answer and respect syntactic constraint, 23.15% of them are incorrectly predicted by the transfer model, and the predicted answers violate the syntactic constraint, illustrated by the case in Figure 1 (b).","For answers that the monolingual model correctly predicts and respect syntactic constraint, 23.15% of them, which are incorrectly predicted by the transfer model, violate the syntactic constraint, illustrated by the case in Figure 1(b).","Modify,Clarity",Clarity
5654,68-ARR,68-ARR_v2_7@2,68-ARR_v1_7@2,"This suggests that the source language syntax may have a negative impact on the answer boundary detection in the target language during zero-shot transfer, due to the linguistic discrepancies between the two languages.","It suggests that the source language syntax may have a negative impact on the answer boundary detection on the target language during zero-shot transfer, due to the linguistic discrepancies between the two languages.","Modify,Grammar",Grammar
5655,68-ARR,68-ARR_v2_8@0,68-ARR_v1_8@0,"However, linguistic discrepancies are diverse and it is difficult to learn them.","However, the linguistic discrepancies are diverse and impossible to learn.","Modify,Claim",Claim
5656,68-ARR,68-ARR_v2_8@1,68-ARR_v1_8@1,"We hence propose to decouple semantics from syntax in pretrained models for multilingual MRC, transforming the learning of linguistic discrepancies into universal semantic information.","We propose to decouple semantics from syntax in pre-trained models for multilingual MRC, transforming the learning of linguistic discrepancies into universal semantic information.","Modify,Grammar",Grammar
5657,68-ARR,68-ARR_v2_9@0,68-ARR_v1_9@2,"To disassociate semantic and syntactic information in PLMs well, we introduce objective functions of learning cross-lingual reconstruction and semantic discrimination together with losses of incorporating word order information and syntax structure information (Part-of-Speech tags and syntax parse trees).","To disassociate semantic and syntactic information well, we introduce objective functions of learning cross-lingual reconstruction and semantic discrimination and losses of incorporating word order information and syntax structure information (Part-of-Speech tags and syntax parse trees), respectively.","Modify,Clarity",Clarity
5683,80-ARR,,80-ARR_v1_53@10,,We will release PopBuTFy and corresponding annotations once the paper is published.,"Delete,Fact/Evidence",Fact/Evidence
5684,80-ARR,,80-ARR_v1_67@2,,"After that, they got fused by being added up, then processed by tanh and sigmoid separately and then multiplied together.","Delete,Fact/Evidence",Fact/Evidence
5685,80-ARR,,80-ARR_v1_67@3,,"Finally, they produce a residual output for the next sub-layer and a skip-out.","Delete,Fact/Evidence",Fact/Evidence
5686,80-ARR,,80-ARR_v1_67@4,,"Lastly, two layers of 1D convolution and a ReLU process the summed skip-out to produce output.","Delete,Fact/Evidence",Fact/Evidence
5687,80-ARR,,80-ARR_v1_68@0,,"As shown in Figure 8, the content encoder is the combination of several conformer encoder layers in pink rectangle along with a 3-layer prenet.","Delete,Fact/Evidence",Fact/Evidence
5688,80-ARR,,80-ARR_v1_68@1,,The kernel size of the convolutional layer for prenet is 5.,"Delete,Fact/Evidence",Fact/Evidence
5689,80-ARR,,80-ARR_v1_71@0,,"Since in this paper we present a new dataset, we answer the questions in the ""Ethics"" part of the ""Author Checklist"":","Delete,Fact/Evidence",Fact/Evidence
5690,80-ARR,,80-ARR_v1_73@0,,"Yes, we sign an agreement with the participants that we only allow these singing recordings for non-commercial use.","Delete,Fact/Evidence",Fact/Evidence
5691,80-ARR,,80-ARR_v1_74@0,,D.2 Does the paper describe how participants' privacy rights were respected in the data collection process?,"Delete,Other",Other
5692,80-ARR,,80-ARR_v1_75@0,,"The participants are asked to use bogus names in the data collection process, which means we do not know who sang these recordings.","Delete,Fact/Evidence",Fact/Evidence
5693,80-ARR,,80-ARR_v1_76@0,,E Does the paper describe how crowd workers or other annotators were fairly compensated and how the compensation was determined to be fair?,"Delete,Other",Other
5694,80-ARR,,80-ARR_v1_77@0,,"Yes. We pay each worker $320 per hour when recording these songs, which is determined after the market research by the data annotation company.","Delete,Fact/Evidence",Fact/Evidence
5695,80-ARR,,80-ARR_v1_78@0,,F Does the paper indicate that the data collection process was subjected to any necessary review by an appropriate review board?,"Delete,Other",Other
5696,80-ARR,,80-ARR_v1_79@1,,The data collection process was subjected to review by the review board of the data annotation company.,"Delete,Fact/Evidence",Fact/Evidence
5697,80-ARR,80-ARR_v2_3@4,,Audio samples are available at https://neuralsvb.,,"Add,Fact/Evidence",Fact/Evidence
5698,80-ARR,80-ARR_v2_3@6,,Codes: https://github. com/MoonInTheRiver/NeuralSVB.,,"Add,Fact/Evidence",Fact/Evidence
5699,80-ARR,80-ARR_v2_69@0,,"SADTW is a kind of advanced APC method, which is designed for fine-tuning the amateur recording, but not for the case when the amateur recordings are completely out of tune.",,"Add,Claim",Claim
5700,80-ARR,80-ARR_v2_69@1,,"In the latter case, we recommend people to use Singing Voice Synthesis (synthesizing waveform from PPG and MIDI) + Singing Voice Conversion (converting the vocal timbre of the synthesized waveform into the user's), or some Speech to Singing (STS) methods.",,"Add,Claim",Claim
5701,80-ARR,80-ARR_v2_69@2,,"In addition, SADTW provides a score representing the similarity of two pitch curves, which could be used to determine what kind of SVB solution should be chosen.",,"Add,Claim",Claim
5702,80-ARR,80-ARR_v2_70@0,,"This work develops a possible automatic way for singing voice beautification, which may cause unemployment for people with related occupations.",,"Add,Claim",Claim
5703,80-ARR,80-ARR_v2_70@1,,"In addition, there is the potential for harm from piracy and abuse of our released recordings.",,"Add,Claim",Claim
5704,80-ARR,80-ARR_v2_70@2,,"Thus, we choose the dataset license: CC by-nc-sa 4.0.",,"Add,Fact/Evidence",Fact/Evidence
5705,80-ARR,80-ARR_v2_71@0,,F.1 For every submission F.1.1 Did you discuss the limitations of your work?,,"Add,Other",Other
5706,80-ARR,,80-ARR_v1_15@1,,"Luo et al. (2018) propose Canonical Time Warping (CTW) (Zhou and Torre, 2009;Zhou and De la Torre, 2012) which aligns amateur singing recordings to professional ones according to the pitch curves only.","Delete,Fact/Evidence",Fact/Evidence
5707,80-ARR,,80-ARR_v1_15@2,,Wager et al. (2020) propose a datadriven approach to predict pitch shifts depending on both amateur recording and its accompaniment.,"Delete,Fact/Evidence",Fact/Evidence
5708,80-ARR,80-ARR_v2_2@0,80-ARR_v1_2@0,"We are interested in a novel task, singing voice beautification (SVB).","We are interested in a novel task, singing voice beautifying (SVB).","Modify,Grammar",Grammar
5709,80-ARR,80-ARR_v2_5@0,80-ARR_v1_5@0,"The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019;Blaauw and Bonada, 2020;Ren et al., 2020;Liu et al., 2021a) and Singing Voice Conversion (SVC) (Sisman and Li, 2020;Li et al., 2021;Wang et al., 2021a).","The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019;Blaauw and Bonada, 2020;Ren et al., 2020;Zhang et al., 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020;Li et al., 2021;Wang et al., 2021a).","Modify,Fact/Evidence",Fact/Evidence
5710,80-ARR,80-ARR_v2_5@1,80-ARR_v1_5@1,"However, the Singing Voice Beautification (SVB) remains an important and challenging endeavor for researchers.","However, the Singing Voice Beautifying (SVB) remains an important and challenging endeavor for researchers.","Modify,Grammar",Grammar
5711,80-ARR,80-ARR_v2_54@3,80-ARR_v1_53@3,The parallel setting could make sure that the personal vocal timbre will keep still during the beautification process.,The parallel setting could make sure that the personal vocal timbre will keep still during the beautifying process.,"Modify,Grammar",Grammar
5712,80-ARR,80-ARR_v2_54@7,80-ARR_v1_53@7,We randomly choose 6 songs in Chinese and 18 songs in English (from unseen speakers) for validation and test.,We randomly choose 617 pieces in English and 274 pieces in Chinese for validation and test.,"Modify,Fact/Evidence",Fact/Evidence
5713,80-ARR,80-ARR_v2_54@8,80-ARR_v1_53@8,"For subjective evaluations, we choose 60 samples in the test set from different singers, half in Chinese and English.","For subjective evaluations, we choose 60 samples in the test set from different singers, half in English and Chinese.","Modify,Clarity",Clarity
5714,80-ARR,80-ARR_v2_55@1,80-ARR_v1_54@1,"Besides PopBuTFy, we pre-train the ASR model (used for PPG extraction) leveraging the extra speech datasets: AISHELL-3 (Yao Shi et al., 2020) for Chinese and LibriTTS (Zen et al., 2019) for English.","Besides PopBuTFy, we pre-train the ASR model (used for PPG extraction) leveraging the extra speech datasets: AISHELL-3 (Yao Shi, 2020) for Chinese and LibriTTS (Zen et al., 2019) for English.","Modify,Grammar",Grammar
5715,80-ARR,80-ARR_v2_68@6,80-ARR_v1_70@6,We split evaluations for main experiments and ablation studies into several groups for them.,We split evaluations for experiments and ablation studies into several groups for them.,"Modify,Clarity",Clarity
5716,80-ARR,80-ARR_v2_9@0,80-ARR_v1_10@0,"• We propose the first generative model NSVB to solve the SVB task. NSVB not only corrects the pitch of amateur recordings, but also generates the audio with high audio quality and improved vocal tone, to which previous works typically pay little attention. • We propose Shape-Aware Dynamic Time Warping (SADTW) algorithm to synchronize the amateur recording with the template pitch curve, which ameliorates the robustness of the previous time-warping algorithm. • We propose a latent-mapping algorithm to convert the latent variable of the amateur vocal tone to the professional one's, and contribute a new dataset PopBuTFyto train the latent-mapping function. • We design NSVB as a CVAE model, which supports the semi-supervised learning to leverage unpaired, unlabeled singing data for better performance.","• We propose the first generative model NSVB to solve the SVB task. NSVB not only corrects the pitch of amateur recordings, but also generates the audio with high audio quality and improved vocal tone, to which previous works typically pay little attention. • We propose Shape-Aware Dynamic Time Warping (SADTW) algorithm to synchronize the amateur recording with the template pitch curve, which meliorates the robustness of the previous time-warping algorithm. • We propose a latent-mapping algorithm to convert the latent variable of the amateur vocal tone to the professional one's, and contribute a new dataset PopBuTFyto train the latent-mapping function, which will be released upon the paper is published. • We design NSVB as a CVAE model, which supports the semi-supervised learning to leverage unpaired, unlabeled singing data for better performance.","Modify,Clarity",Clarity
5717,80-ARR,80-ARR_v2_12@0,80-ARR_v1_13@0,"Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015;Serrà et al., 2019;Popov et al., 2021;, which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al., 2021).","Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015;Serrà et al., 2019;Popov et al., 2021), which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al., 2021).","Modify,Other",Other
5803,9-ARR,,9-ARR_v1_26@2,,2 The interval between two adjacent checkpoints is 50 iterations.,"Delete,Fact/Evidence",Fact/Evidence
5804,9-ARR,,9-ARR_v1_4@2,,"In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.","Delete,Fact/Evidence",Fact/Evidence
5805,9-ARR,9-ARR_v2_5@7,,"Besides the overfitting of downstream task data, a rarely studied problem is that the PLMs usually overfit the pretraining tasks and data (Qi et al., 2020), which may have significant gap with the downstream task and data.",,"Add,Claim",Claim
5806,9-ARR,9-ARR_v2_6@1,,"Different from the standard finetuning paradigm (Fig. 1 (a)) which directly finetunes PLMs on the downstream task data, the key idea of NoisyTune is to add a small amount of noise to perturb PLMs parameters before finetuning (Fig. 1 (b)).",,"Add,Fact/Evidence",Fact/Evidence
5807,9-ARR,9-ARR_v2_6@6,,"In addition, the results show NoisyTune can be easily combined with many existing PLM finetuning methods and further improve their performance.",,"Add,Claim",Claim
5808,9-ARR,9-ARR_v2_8@0,,The goal of NoisyTune is for more effective finetuning of PLMs on downstream tasks.,,"Add,Claim",Claim
5809,9-ARR,9-ARR_v2_8@1,,"The motivation of NoisyTune is that PLMs are well pretrained on some unlabeled corpus with some self-supervision tasks, and they may overfit these pretraining data and tasks (Qi et al., 2020), which usually have gap with the downstream task and data.",,"Add,Fact/Evidence",Fact/Evidence
5810,9-ARR,9-ARR_v2_9@0,,"PLMs usually have different kinds of parameter matrices, such as query, key, value, and feedforward network matrices (Devlin et al., 2019).",,"Add,Fact/Evidence",Fact/Evidence
5811,9-ARR,9-ARR_v2_22@1,,"In addition, we explore whether our proposed matrix-wise perturbing method is better than using a unified global noise for all model parameters in PLMs.",,"Add,Fact/Evidence",Fact/Evidence
5812,9-ARR,9-ARR_v2_22@7,,"The results show that matrix-wise noise is a much better choice, since the different characteristics of different parameter matrices can be taken into consideration.",,"Add,Fact/Evidence",Fact/Evidence
5813,9-ARR,9-ARR_v2_24@0,,"From Fig. 1, it is very clear that NoisyTune is independent of the specific PLM finetuning method, since it is applied at the stage before finetuning PLM on the task-specific data.",,"Add,Fact/Evidence",Fact/Evidence
5814,9-ARR,9-ARR_v2_24@1,,"Thus, it is very easy to combine NoisyTune with any kind of existing PLM finetuning method.",,"Add,Claim",Claim
5815,9-ARR,9-ARR_v2_26@2,,2 The results are shown in Fig. 4.,,"Add,Fact/Evidence",Fact/Evidence
5816,9-ARR,9-ARR_v2_28@0,,Hyperparameter Analysis,,"Add,Other",Other
5817,9-ARR,9-ARR_v2_29@0,,"We study the influence of the most important hyperparameter in NoisyTune, i.e., λ, which controls the relative noise intensity.",,"Add,Fact/Evidence",Fact/Evidence
5818,9-ARR,9-ARR_v2_29@1,,The average GLUE scores w.r.t. different λ values are shown in Fig. 6.,,"Add,Fact/Evidence",Fact/Evidence
5819,9-ARR,9-ARR_v2_29@2,,"We find that when λ is too small or too large, the performance is not optimal.",,"Add,Fact/Evidence",Fact/Evidence
5820,9-ARR,9-ARR_v2_29@3,,"This is because when λ is too small, it is difficult for PLMs to do parameter space exploration and overcome the overfitting problem.",,"Add,Fact/Evidence",Fact/Evidence
5821,9-ARR,9-ARR_v2_29@4,,"While when λ is too large, the useful pretrained knowledge in PLMs may be overwhelmed by random noise.",,"Add,Claim",Claim
5822,9-ARR,9-ARR_v2_29@5,,Values between 0.1 and 0.15 are more suitable for NoisyTune on the GLUE datasets.,,"Add,Fact/Evidence",Fact/Evidence
5823,9-ARR,9-ARR_v2_31@2,,"NoisyTune is a very general method, and is PLM model agnostic, downstream task agnostic, and finetuning method agnostic.",,"Add,Claim",Claim
5824,9-ARR,,9-ARR_v1_8@0,,"In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.","Delete,Fact/Evidence",Fact/Evidence
5825,9-ARR,,9-ARR_v1_10@1,,"In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).","Delete,Fact/Evidence",Fact/Evidence
5826,9-ARR,,9-ARR_v1_16@4,,The XTREME results are evaluated on the test set.,"Delete,Fact/Evidence",Fact/Evidence
5827,9-ARR,,9-ARR_v1_16@5,,The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME.,"Delete,Fact/Evidence",Fact/Evidence
5828,9-ARR,9-ARR_v2_22@3,9-ARR_v1_24@1,"The results on GLUE are shown in Fig. 2, and the results on XTREME show similar patterns.",The results on GLUE are shown in Fig. 1.,"Modify,Fact/Evidence",Fact/Evidence
5829,9-ARR,9-ARR_v2_22@4,9-ARR_v1_24@2,We find that adding global noise with the same distribution to all the PLM parameters will harm the model performance.,We find that adding global noise with same distributions to the PLM parameters will harm the model perfor- mance.,"Modify,Grammar",Grammar
5830,9-ARR,9-ARR_v2_22@5,9-ARR_v1_24@3,This is because different parameter matrices in PLMs have very different distributions and characteristics .,"This is because different parameter matrices have very different distributions, and simply adding global noise is not appropriate.","Split+Modify,Clarity",Clarity
5831,9-ARR,9-ARR_v2_22@6,9-ARR_v1_24@3,Simply adding a unified global noise to all the parameter matrices is not optimal.,"This is because different parameter matrices have very different distributions, and simply adding global noise is not appropriate.","Split+Modify,Clarity",Clarity
5832,9-ARR,9-ARR_v2_22@8,9-ARR_v1_24@4,"In addition, we find an interesting phenomenon that adding uniform noise is better than Gaussian noise.","In addition, we find an interesting phenomenon that adding uniform noise is better than using Gaussian noise.","Modify,Clarity",Clarity
5833,9-ARR,9-ARR_v2_22@9,9-ARR_v1_24@5,This may be because Gaussian noise has wider ranges and some extreme values may affect the model performance.,This may be because Gaussian noise has wider ranges and some outliers may affect model performance.,"Modify,Clarity",Clarity
5834,9-ARR,9-ARR_v2_22@10,9-ARR_v1_24@6,"Thus, we use matrix-wise uniform noise in NoisyTune.","Thus, we prefer using matrix-wise uniform noise in our NoisyTune method.","Modify,Clarity",Clarity
5835,9-ARR,9-ARR_v2_25@0,9-ARR_v1_25@0,Empirical Analysis of NoisyTune,Analysis on NoiseTune,"Modify,Fact/Evidence",Fact/Evidence
5836,9-ARR,9-ARR_v2_26@0,9-ARR_v1_26@0,"Next, we empirically analyze why NoisyTune can help PLM finetuning.",We then analyze the influence of NoisyTune on finetuning.,"Modify,Clarity",Clarity
5837,9-ARR,9-ARR_v2_26@1,9-ARR_v1_26@1,We compare the accuracy of BERT with and without NoisyTune finetuned with different percentage of samples on the MRPC dataset.,We show the accuracy of the BERT model with or without NoisyTune on the MRPC dataset in Fig. 2.,"Modify,Fact/Evidence",Fact/Evidence
5838,9-ARR,9-ARR_v2_2@6,9-ARR_v1_2@5,Extensive experiments on both GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can consistently empower the finetuning of different PLMs on different downstream tasks.,Extensive experiments on the GLUE English benchmark and the XTREME multilingual benchmark show that NoisyTune can consistently improve the performance of different PLMs in many downstream tasks.,"Modify,Clarity",Clarity
5839,9-ARR,9-ARR_v2_26@3,9-ARR_v1_26@3,"We find NoisyTune can consistently improve PLMs under different amounts of data, especially when less training data is used.",We find NoisyTune can consistently improve PLMs at different finetuning steps.,"Modify,Fact/Evidence",Fact/Evidence
5840,9-ARR,9-ARR_v2_27@0,9-ARR_v1_27@0,"To further study the impact of NoisyTune on PLM finetuning, we show the relative changes of the L 1 -norms of different kinds of parameters in the BERT model during finetuning on the MRPC dataset in Fig. 5.","To further study the impact of NoisyTune on model finetuning, we show the relative changes of the L1-norms of different kinds of paramters in the BERT model during training on MRPC and STS-B in Fig. 3.","Modify,Fact/Evidence",Fact/Evidence
5841,9-ARR,9-ARR_v2_27@1,9-ARR_v1_27@1,"3 Since the noise we added to PLMs in NoisyTune is zero-mean uniform noise, the absolute parameter L 1 -norm will not change too much.","3 Since the noise we added is zeromean, the absolute parameter L1-norms will not be changed too much.","Modify,Clarity",Clarity
5842,9-ARR,9-ARR_v2_27@2,9-ARR_v1_27@2,"However, we can see that the relative change of L 1 -norms becomes smaller when Noisy-Tune is applied, which indicates that the PLMs can find the (sub)optimal parameters for downstream tasks more easily.","However, we can see that the relative change of L1-norms becomes smaller when NoisyTune is applied, which indicates that the model uses smaller paces towards convergence.","Modify,Fact/Evidence",Fact/Evidence
5843,9-ARR,9-ARR_v2_27@3,9-ARR_v1_27@3,"This result validates directly finetuning PLMs may need more updates to adapt to downstream tasks, which is due to the overfitting of pretraining tasks, and NoisyTune can provide a simple way to alleviate this problem and help finetune PLMs on downstream tasks more effectively.","This means that directly finetuning PLMs may need more updates to adapt to downstream tasks, which may be due to the overfitting of pretraining tasks and their gaps with downstream tasks.","Merge+Modify,Clarity",Clarity
5844,9-ARR,9-ARR_v2_27@3,9-ARR_v1_27@4,"This result validates directly finetuning PLMs may need more updates to adapt to downstream tasks, which is due to the overfitting of pretraining tasks, and NoisyTune can provide a simple way to alleviate this problem and help finetune PLMs on downstream tasks more effectively.",Our Noisy-Tune approach provides a simple way to mitigate this problem to empower PLM finetuning.,"Merge+Modify,Claim",Claim
5845,9-ARR,9-ARR_v2_23@0,9-ARR_v1_28@0,Combination with Existing PLM Finetuning Methods,Empower Other Finetuning Methods,"Modify,Fact/Evidence",Fact/Evidence
5846,9-ARR,9-ARR_v2_24@2,9-ARR_v1_29@0,"In this section, we explore whether NoisyTune has the potential to empower the existing PLM finetuning techniques to achieve better performance.",Our NoisyTune method also has the potential to empower other PLM finetuning techniques.,"Modify,Clarity",Clarity
5847,9-ARR,9-ARR_v2_24@3,9-ARR_v1_29@1,"Here we select two well-known PLM finetuning for experiments, i.e., RecAdam and Mixout (Lee et al., 2020).","We compare the performance of the original RecAdam and Mixout (Lee et al., 2020) method and their variants combined with NoisyTune.","Modify,Fact/Evidence",Fact/Evidence
5848,9-ARR,9-ARR_v2_24@4,9-ARR_v1_29@2,The experimental results are summarized in Fig. 3.,The results are shown in Fig. 4.,"Modify,Clarity",Clarity
5849,9-ARR,9-ARR_v2_24@5,9-ARR_v1_29@3,We find that combining NoisyTune with existing PLM finetuning techniques can further improve their performance.,We find that combining NoisyTune with existing PLM finetuning techniques can further improve the performance.,"Modify,Grammar",Grammar
5851,9-ARR,9-ARR_v2_24@7,9-ARR_v1_29@4,"Thus, NoisyTune and these PLM finetuning methods are complementary, and they can be empowered by NoisyTune to achieve better performance.","This is because NoisyTune aims to address the overfitting of pretraining signals while these methods aim to prevent overfitting in downstream tasks, thereby they can be empowered by NoisyTune to improve model performance.","Split+Modify,Claim",Claim
5852,9-ARR,9-ARR_v2_31@0,9-ARR_v1_31@0,"In this paper, we propose a very simple but effective method named NoisyTune, which can help better finetune PLMs on downstream tasks by adding a little noise to them before finetuning.","In this paper, we propose a very simple but effective method named NoisyTune, which adds a little noise to PLMs before finetuning for better transferability from pretraining tasks to downstream tasks.","Modify,Clarity",Clarity
5853,9-ARR,9-ARR_v2_31@1,9-ARR_v1_31@1,"In NoisyTune, we propose a matrix-wise perturbing method that adds noise with different intensities to different kinds of parameter matrices in PLMs according to their variances.","In NoisyTune, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of different parameter matrices in PLMs, which can consider the varied characteristics of different types of parameters.","Modify,Clarity",Clarity
5854,9-ARR,9-ARR_v2_31@3,9-ARR_v1_31@2,Extensive experiments on both monolingual GLUE benchmark and multilingual XTREME benchmark demonstrate NoisyTune can consistently empower the finetuning of different PLMs on various downstream tasks to achieve better performance.,Extensive experiments on the monolingual GLUE benchmark and the multilingual XTREME benchmark demonstrate the NoisyTune can consistently improve the performance of different PLMs in various downstream tasks.,"Modify,Clarity",Clarity
5855,9-ARR,9-ARR_v2_4@1,9-ARR_v1_4@1,"Many PLMs such as BERT (Devlin et al., 2019), RoBERTa and UniLM (Dong et al., 2019) which are pretrained from large-scale unlabeled corpus in a selfsupervised way, have significantly improve various downstream tasks such as reading comprehension , machine translation (Brown et al., 2020), text classification (Bao et al., 2020), dialog (Wu et al., 2020) and recommendation by finetuning on these tasks.","Many PLMs such as BERT (Devlin et al., 2019) and RoBERTa have played critical roles in various applications, such as reading comprehension, machine translation and text classification (Dong et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
5856,9-ARR,9-ARR_v2_5@0,9-ARR_v1_5@0,"How to effectively finetune PLMs to better empower downstream tasks is an important research problem (Zheng et al., 2021).","How to effectively finetune PLMs to better empower downstream tasks is an important research problem (Zheng et al., 2021a).","Modify,Fact/Evidence",Fact/Evidence
5857,9-ARR,9-ARR_v2_5@3,9-ARR_v1_5@1,"Only a few works explore more effective and robust PLM finetuning methods Lee et al., 2020;Aghajanyan et al., 2021;Xu et al., 2021).","Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods Jiang et al., 2020;Lee et al., 2020;Aghajanyan et al., 2021;Xu et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
5858,9-ARR,9-ARR_v2_5@5,9-ARR_v1_5@3,Lee et al. (2020) proposed Mixout which randomly replaces part of the parameters in the finetuned model with their original weights in the PLMs.,Lee et al. (2020) proposed a Mixout method to randomly replace parts of model parameters with their original pretrained weights.,"Modify,Clarity",Clarity
5859,9-ARR,9-ARR_v2_5@6,9-ARR_v1_5@4,These PLM finetuning methods mainly focus on preventing PLMs from overfitting the limited labeled data in downstream tasks.,These finetuning methods mainly focus on preventing PLMs from overfitting limited labeled data in downstream tasks.,"Modify,Clarity",Clarity
5860,9-ARR,9-ARR_v2_5@8,9-ARR_v1_5@5,"It is not easy for existing PLM finetuning methods to overcome such gap (Roberts et al., 2020), which may lead to suboptimal performance especially when labeled data in downstream tasks is insufficient.","However, PLMs have been well trained in the self-supervised pretraining tasks, and it can be difficult for them to overcome the barrier between pretraining and downstream tasks as well as the gaps of their domains during finetuning (Roberts et al., 2020), which may lead to a suboptimal performance especially when labeled data in downstream tasks is insufficient.","Modify,Fact/Evidence",Fact/Evidence
5861,9-ARR,9-ARR_v2_2@1,9-ARR_v1_2@1,"However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks.","However, PLMs may have risks in overfitting pretraining signals, and there are some gaps between downstream tasks and the pretraining tasks.","Modify,Clarity",Clarity
5862,9-ARR,9-ARR_v2_6@0,9-ARR_v1_6@0,"In order to handle this problem, in this paper we propose a very simple yet effective method named NoisyTune, which can help better finetune PLMs for downstream tasks.","In this paper, we propose a very simple yet effective method named NoisyTune, which can help better finetune PLMs for downstream tasks.","Modify,Clarity",Clarity
5863,9-ARR,9-ARR_v2_6@2,9-ARR_v1_6@1,"It can help prevent PLMs from overfitting the tasks and data in the pretraining stage, and reduce the gap between pretraining and downstream tasks.","The key idea of NoisyTune is to add a little noise to perturb PLMs parameters before finetuning, which can help prevent them from overfitting the signals in the pretraining tasks, and reduce the gap between pretraining and downstreaming tasks.","Modify,Claim",Claim
5864,9-ARR,9-ARR_v2_6@3,9-ARR_v1_6@2,"Since PLMs have different types of parameters which usually own different characteristics, in NoisyTune we use a matrix-wise perturbing method that adds uniform noise with different intensities to different parameter matrices according to their standard deviations for better adaptation.","Since different types of parameters in PLMs may have different characteristics, we propose a matrix-wise perturbing method that adds uniform noise with different intensities according to the standard deviations of different parameter matrices for better adaptation.","Modify,Clarity",Clarity
5865,9-ARR,9-ARR_v2_6@4,9-ARR_v1_6@3,"We conduct extensive experiments on two widely used NLP benchmarks, namely, GLUE (Wang et al., 2018) for English language understanding and XTREME (Hu et al., 2020) for multilingual language understanding.","We conduct experiments on two widely used NLP benchmarks, i.e., GLUE (Wang et al., 2018) for English language understanding and XTREME (Hu et al., 2020) for multilingual language understanding.","Modify,Clarity",Clarity
5866,9-ARR,9-ARR_v2_6@5,9-ARR_v1_6@4,The results show NoisyTune can empower the finetuning of different PLMs on many different downstream NLP tasks to consistently achieve better performance.,The results show that NoisyTune can consistently boost the performance of different PLMs in many downstream NLP tasks.,"Modify,Fact/Evidence",Fact/Evidence
5867,9-ARR,9-ARR_v2_8@2,9-ARR_v1_8@1,"It may be difficult for PLMs to effectively adapt to downstream tasks especially when labeled data in these tasks are limited, which is usually the case.","Since the parameters of PLMs are well tuned in the pretraining tasks and may overfit self-supervision signals, it may be difficult for them to adapt to downstream tasks especially when labeled data in downstream tasks are rather limited.","Modify,Claim",Claim
5868,9-ARR,9-ARR_v2_2@2,9-ARR_v1_2@2,Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance.,"It can be difficult for vanilla finetuning methods to overcome the barrier between pretraining and downstream tasks, which leads to suboptimal performance.","Modify,Clarity",Clarity
5869,9-ARR,9-ARR_v2_8@3,9-ARR_v1_8@2,"Motivated by the dueling bandits mechanism (Yue and Joachims, 2009) that adds randomness to the model for exploration, as shown in Fig. 1, we propose to add some noise to the parameters of PLMs before finetuning them on downstream tasks to do some ""exploration"" in parameter space and reduce the risk of overfitting the pretraining tasks and data.","Motivated by the dueling bandits mechanism (Yue and Joachims, 2009) that adds randomness to the model for exploration, we explore adding noise to PLMs before finetuning to ""explore"" other parameter spaces to reduce the problem of overfitting pretraining tasks.","Modify,Fact/Evidence",Fact/Evidence
5870,9-ARR,9-ARR_v2_9@1,9-ARR_v1_10@2,Different parameter matrices in the PLMs usually have different characteristics and scales.,"However, different parameter matrices in the PLM have very different characteristics.","Modify,Clarity",Clarity
5871,9-ARR,9-ARR_v2_9@2,9-ARR_v1_10@3,"For example, some researchers found that the self-attention parameters and the feed-forward network parameters in Transformers have very different properties, such as rank and density .","For example, the self-attention parameters and the feed-forward network parameters usually have very different properties .","Modify,Claim",Claim
5872,9-ARR,9-ARR_v2_9@3,9-ARR_v1_10@4,"Thus, adding unified noise to all parameter matrices in PLMs may not be optimal for keeping their good model utility.","Thus, adding global noise may not be optimal for keeping good model utility.","Modify,Clarity",Clarity
5873,9-ARR,9-ARR_v2_9@4,9-ARR_v1_10@5,"To handle this challenge, we propose a matrix-wise perturbing method that adds noise with different intensities to different parameter matrices according to their variances.","To solve this challenge, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of parameter matrices.","Modify,Clarity",Clarity
5874,9-ARR,9-ARR_v2_11@1,9-ARR_v1_10@6,"Denote the perturbed version of the parameter matrix W i as Wi , which is computed as follows:","We denote the perturbed version of the parameter matrix W i as Wi , which is computed as follows:","Modify,Clarity",Clarity
5875,9-ARR,9-ARR_v2_2@3,9-ARR_v1_2@3,"In this paper, we propose a very simple yet effective method named NoisyTune to help better finetune PLMs on downstream tasks by adding some noise to the parameters of PLMs before finetuning.","In this paper, we propose a very simple yet effective method named NoisyTune which can help better finetune PLMs in downstream tasks by adding some noise to the parameters of PLMs before finetuning.","Modify,Clarity",Clarity
5876,9-ARR,9-ARR_v2_14@0,9-ARR_v1_12@2,"In addition, in some PLMs there are some constant matrices, such as token type embeddings in RoBERTa .","In addition, in some PLMs there exist constant matrices, such as token type embeddings in RoBERTa .","Modify,Clarity",Clarity
5877,9-ARR,9-ARR_v2_14@2,9-ARR_v1_12@4,It can ensure that these constant matrices will not be accidentally activated by additional noise.,This will ensure that these constant matrices will not be accidentally activated by additional noise.,"Modify,Clarity",Clarity
5878,9-ARR,9-ARR_v2_17@1,9-ARR_v1_15@1,"The first one is GLUE (Wang et al., 2018), which is a benchmark for English language understanding that contains different tasks like natural language inference, sentiment analysis and sentence similarity evaluation.","The first one is GLUE (Wang et al., 2018), which is a benchmark for English language understanding that contains tasks like natural language inference, sentiment analysis and sentence similarity evaluation.","Modify,Clarity",Clarity
5879,9-ARR,9-ARR_v2_17@6,9-ARR_v1_17@2,"In order not to harm the alignment of token embeddings across different languages, we do not add noise to the token embeddings in multilingual PLMs.","In order not to harm the alignment of token embeddings across different languages, We do not add noise to the token embeddings in multilingual PLMs.","Modify,Grammar",Grammar
5880,9-ARR,9-ARR_v2_2@4,9-ARR_v1_2@4,"More specifically, we propose a matrix-wise perturbing method which adds different uniform noises to different parameter matrices based on their standard deviations.","More specifically, we propose a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices, which can consider the varied characteristics of different types of parameters in PLMs.","Split+Modify,Clarity",Clarity
5881,9-ARR,9-ARR_v2_2@5,9-ARR_v1_2@4,"In this way, the varied characteristics of different types of parameters in PLMs can be considered.","More specifically, we propose a matrix-wise perturbing method by adding different uniform noises according to the standard deviations of different parameter matrices, which can consider the varied characteristics of different types of parameters in PLMs.","Split+Modify,Clarity",Clarity
5882,9-ARR,9-ARR_v2_19@0,9-ARR_v1_19@0,"On the GLUE benchmark, we compare the performance of directly finetuning the base version of BERT (Devlin et al., 2019), XLNET , RoBERTa and ELEC-TRA (Clark et al., 2020) with that of finetuning them after applying NoisyTune.","On the GLUE benchmark, we compare directly finetuning the base version of BERT (Devlin et al., 2019), XLNET , RoBERTa and ELECTRA (Clark et al., 2020) as well as finetuning them after applying NoisyTune.","Modify,Clarity",Clarity
5883,9-ARR,9-ARR_v2_19@1,9-ARR_v1_20@0,"On the XTREME benchmark, we compare the performance of directly finetuning both base and large versions of XLM-R (Conneau et al., 2020) with that of their variants obtained by applying NoisyTune.","On the XTREME benchmark, we compare both base and large versions of XLM-R (Conneau et al., 2020) and their variants processed by NoisyTune.","Modify,Clarity",Clarity
5885,9-ARR,9-ARR_v2_19@4,9-ARR_v1_21@1,"The first one is zero-shot crosslingual transfer from English to other languages, and the second one is learning models on both English and translated data.","On the XTREME datasets, we report two types of results, i.e., zeroshot crosslingual transfer from English to other languages or learning models on both English and translated data.","Split+Modify,Clarity",Clarity
5886,9-ARR,9-ARR_v2_20@0,9-ARR_v1_21@2,"According to these results, NoisyTune can consistently improve the performance of different PLMs on different tasks in both English and multilingual settings.","From the results, we can see that NoisyTune can consistently improve the performance of different PLMs on different tasks.","Modify,Clarity",Clarity
5887,9-ARR,9-ARR_v2_20@1,9-ARR_v1_21@3,"In addition, the performance improvement brought by NoisyTune is usually larger on relatively small datasets (e.g., RTE, CoLA and WNLI).","In addition, the performance improvement on relatively small datasets is usually larger (e.g., RTE, CoLA and WNLI).","Modify,Clarity",Clarity
5888,9-ARR,9-ARR_v2_20@2,9-ARR_v1_21@4,"These results indicate that when labeled data in downstream tasks is insufficient, it is quite difficult to effectively finetune PLMs starting from the original parameters which usually overfit the pretraining tasks and data.","This indicates that when labeled training data in downstream tasks is not redundant, it may be more difficult to well adapt PLMs to downstream tasks from the parameter space well tuned in pretraining tasks.","Modify,Claim",Claim
5889,9-ARR,9-ARR_v2_20@3,9-ARR_v1_21@5,"The experimental results validate that NoisyTune can properly perturb PLMs with a little noise to explore different parameter spaces and reduce the overfitting problem, making PLMs easier to be adapted to downstream tasks.","Thus, properly perturbing PLMs with noise can explore different parameter spaces and meanwhile keep useful knowledge encoded in pretraining tasks.","Modify,Claim",Claim
5890,9-ARR,9-ARR_v2_21@0,9-ARR_v1_22@0,Which Noise to Use and How?,Influence of Noise Type,"Modify,Clarity",Clarity
5891,9-ARR,9-ARR_v2_22@0,9-ARR_v1_23@0,In this section we study which kind of noise is more suitable for NoisyTune.,"Next, we study the influence of using different kinds of noise on NoisyTune.","Modify,Clarity",Clarity
5892,90-ARR,90-ARR_v2_69@1,,"To generate our response, we sample entire dialogues from the language model and then re-rank the predicted dialogues with a reward function.",,"Add,Fact/Evidence",Fact/Evidence
5893,90-ARR,90-ARR_v2_75@3,,We train a model to parse the customer's flight requirements from the dialogue.,,"Add,Fact/Evidence",Fact/Evidence
5894,90-ARR,90-ARR_v2_75@4,,We execute these flight requirements against the table and compare the output to the flight that was actually booked; this determines the reward (i.e. if the correct flight was booked or not).,,"Add,Fact/Evidence",Fact/Evidence
5895,90-ARR,90-ARR_v2_82@1,,"Conversations generally begin with a greeting followed by some questioning / information gathering, and then finally the agent suggests a flight before ending the conversation.",,"Add,Fact/Evidence",Fact/Evidence
5896,90-ARR,90-ARR_v2_84@0,,In Table 5 we present a detailed breakdown of model errors.,,"Add,Fact/Evidence",Fact/Evidence
5897,90-ARR,90-ARR_v2_84@1,,"As expected, determining the flight to book, if any, is consistently shown to be the most challenging sub-task, as evidenced by the lower ""flight success rate"" and the lower F1 scores for ""no flight"", ""book"", and ""change"" on LM (GPT2small).",,"Add,Fact/Evidence",Fact/Evidence
5898,90-ARR,90-ARR_v2_84@2,,"In particular, ""change"" has a low recall, precision, and F1 score for all models because it makes up a very small 0.4% of the training data.",,"Add,Fact/Evidence",Fact/Evidence
5899,90-ARR,90-ARR_v2_84@3,,"Lastly, the ""constraint success"" row shows that even when CALM books the wrong flight, the flight it does books meets >80% of the customer's flight requirements on average.",,"Add,Fact/Evidence",Fact/Evidence
5900,90-ARR,90-ARR_v2_86@1,,All models are evaluated with greedy decoding.,,"Add,Fact/Evidence",Fact/Evidence
5901,90-ARR,90-ARR_v2_86@2,,"In addition to the full task success rate, we report success rate for each sub-component of the full task (status / flight / name).",,"Add,Fact/Evidence",Fact/Evidence
5902,90-ARR,90-ARR_v2_86@3,,"We also report recall (R), precision (P), and F1 score for task success under each type of high-level action (book / no flight / cancel / change / no reservation).",,"Add,Fact/Evidence",Fact/Evidence
5903,90-ARR,90-ARR_v2_86@4,,"Lastly, we report the average fraction of the customer's flight requirements that are met when the agent books the wrong flight (constraint success).",,"Add,Fact/Evidence",Fact/Evidence
5904,90-ARR,,90-ARR_v1_30@2,,"Additionally, the structure of our specific task allows us to sample from the true posterior q(c o |τ, c h ), by using the rejection sampling procedure described above, rather than using approximations as in prior work (Eysenbach et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
5905,90-ARR,,90-ARR_v1_67@0,,Ethical Statement,"Delete,Other",Other
5906,90-ARR,,90-ARR_v1_68@0,,We note that CALM is imperfect and in generating free-form dialogue can potentially produce harmful outputs.,"Delete,Claim",Claim
5907,90-ARR,,90-ARR_v1_68@1,,We therefore do not recommend applying this method in particularly sensitive or sufficiently wide-reaching domains without additional measures to mitigate harmful generations.,"Delete,Claim",Claim
5908,90-ARR,,90-ARR_v1_69@0,,"In this appendix, we provide all the details in our implementation for CALM.","Delete,Fact/Evidence",Fact/Evidence
5909,90-ARR,90-ARR_v2_62@0,,Removing any single component from CALM drops performance by at least 10%.,,"Add,Fact/Evidence",Fact/Evidence
5910,90-ARR,90-ARR_v2_65@0,,"CALM optimizes for task-specific measures of success, and while such measures might be comparatively simple for domains such as AirDialogue, in general specifying the right success measure or reward function may present challenges.",,"Add,Claim",Claim
5911,90-ARR,90-ARR_v2_65@1,,"Furthermore, as with all methods based on end-to-end language models, CALM is susceptible to internal biases and inconsistencies in the language model itself.",,"Add,Claim",Claim
5912,90-ARR,90-ARR_v2_65@2,,"There is for example no constraint that ensures that CALM produces truthful answers, or that it avoids harmful or socially unacceptable outputs.",,"Add,Claim",Claim
5913,90-ARR,90-ARR_v2_65@3,,"A practical deployable dialogue system would likely require additional measures to account for such issues, analogously to how learning-based methods for self-driving vehicles might require some additional safety mechanisms to ensure constraints, and indeed further research on reward specification, ensuring truthful outputs, and other constraint strategies for dialogue systems that combine language models and reward maximization is a promising and important direction.",,"Add,Claim",Claim
5914,90-ARR,90-ARR_v2_66@0,,"The context-conditioned supervised learning strategy used by CALM provides for reward maximization, but is in general not optimal for arbitrary reinforcement learning problems: in general RL settings, learning a value function with dynamic programming in general can attain significantly better returns than imitating high-performing trajectories, by recombining good parts of multiple different trajectories (which might individually be suboptimal) (Kostrikov et al., 2021;Kumar et al., 2022).",,"Add,Claim",Claim
5915,90-ARR,90-ARR_v2_66@1,,"The simple supervised learning strategy works well in the domain we tested, but extending CALM to use value-based reinforcement learning methods is a promising direction for future work.",,"Add,Claim",Claim
5916,90-ARR,90-ARR_v2_66@2,,"Indeed, the improvement obtained from planning on top of the CALM model likely indicates that the supervised learning approach we employ has room for improvement.",,"Add,Claim",Claim
5917,90-ARR,90-ARR_v2_66@3,,"Additionally, the auxiliary objectives and relabeling strategies we employ require some amount of domain-specific design, and more general strategies could be developed in future.",,"Add,Claim",Claim
5918,90-ARR,90-ARR_v2_67@0,,"Addressing these limitations in future work and developing more advanced methods that combine end-to-end language generation via large language models with concepts from reinforcement learning and planning is a promising research direction for making dialogue systems more capable, while also making language models more task aware.",,"Add,Claim",Claim
5919,90-ARR,90-ARR_v2_67@1,,We hope that CALM will serve as an indication for the potential of such methods.,,"Add,Claim",Claim
5920,90-ARR,90-ARR_v2_4@1,90-ARR_v1_4@1,"However, dialogue can also be viewed as a sequential decision making process, which is well-suited to planning and reinforcement learning (RL) algorithms.","However, dialogue can also be viewed as a sequential decision making process which is well-suited to planning and reinforcement learning (RL) algorithms.","Modify,Grammar",Grammar
5921,90-ARR,90-ARR_v2_30@1,90-ARR_v1_33@1,"In the case of the information retrieval task that we consider in this paper, these high-level actions correspond to deciding which database entity to retrieve for the user (e.g., suggesting a flight to the customer that meets all of their needs).","In the case of the information retrieval task that we consider in this paper, these high-level actions correspond to deciding which database entity to retrieve for the user (e.g. suggesting a flight to the customer that meets all of their needs).","Modify,Grammar",Grammar
5923,90-ARR,90-ARR_v2_58@9,90-ARR_v1_61@3,"Rollout planning requires a method for predicting the reward of a given dialogue, and we describe our specific reward predictor for AirDialogue in Appendix Section E.1.","For AirConcierge, we only evaluate greedy decoding, as this method cannot be easily adapted for producing full rollouts as rollout planning requires a method for predicting the reward of a given dialogue.","Link+Modify,Clarity",Clarity
5924,90-ARR,90-ARR_v2_58@9,90-ARR_v1_61@4,"Rollout planning requires a method for predicting the reward of a given dialogue, and we describe our specific reward predictor for AirDialogue in Appendix Section E.1.",We describe our specific reward predictor for AirDialogue in Appendix Section A.7.,"Link+Modify,Fact/Evidence",Fact/Evidence
5925,90-ARR,90-ARR_v2_59@1,90-ARR_v1_62@1,"In terms of task success, CALM outperforms the prior SOTA (AirConcierge) by approximately 7%, achieving 88% task success when using greedy decoding from the language model (see Table 1).","In terms of task success, CALM outperforms the prior SOTA (AirConcierge) by approximately 7%, achieving 88% task success when using greedy decoding from the language model.","Modify,Fact/Evidence",Fact/Evidence
5926,90-ARR,90-ARR_v2_59@2,90-ARR_v1_62@2,"Compared with AirConcierge, where all reasoning about the task context is done outside of the language model, CALM does all of the filtering, selecting, and responding with relevant flight table entries within the language model, in a fully end-to-end manner.","Compared with AirConcierge, where all reasoning about the task context is done outside of the language model, CALM does all of the filtering, selecting, and responding with relevant flight table entries within the language model, in a fully endto-end manner.","Modify,Grammar",Grammar
5927,90-ARR,90-ARR_v2_76@3,90-ARR_v1_77@5,We release the code and model weights for our customer bot at https://sea-snell.github.io/CALM_LM_site/.,"To promote fair evaluations in future works that use the AirDialogue dataset, we will release the code and model weights for our customer bot upon acceptance.","Modify,Fact/Evidence",Fact/Evidence
5928,90-ARR,90-ARR_v2_79@0,90-ARR_v1_80@0,"2. We then execute this predicted information against the agent's flight flag, to produce a set of valid final actions.","2. We then execute this predicted information against the agent's flight table and reservation flag, to produce a set of valid final actions.","Modify,Clarity",Clarity
5929,90-ARR,90-ARR_v2_6@7,90-ARR_v1_7@1,"CALM unifies the traditional language modeling objective with task-specific supervision, where a language model is interpreted as a joint representation of dynamics and policies in an MDP, and the finetuning process utilizes a conditional imitation learning objective with a novel task relabeling strategy that teaches the model how to generate high-utility dialogues (see Figures 1 and 2).","CALM unifies the traditional language modeling objective with task-specific supervision, where a language model is interpreted as a joint representation of dynamics and policies in an MDP, and the finetuning process utilizes a conditional imitation learning objective with a novel task relabeling strategy that teaches the model how to generate high-utility dialogues.","Modify,Fact/Evidence",Fact/Evidence
5930,90-ARR,90-ARR_v2_9@3,90-ARR_v1_12@5,"When fine-tuning on this relabeled dataset, we then apply a Task Specific Auxiliary Loss on top of the standard language modeling objective; this helps the model learn to use the task context.","When fine-tuning on this relabeled dataset, we then apply a Task Specific Auxilary Loss on top of the standard language modeling objective; this helps the model learn to use the task context.","Modify,Grammar",Grammar
5931,90-ARR,90-ARR_v2_9@4,90-ARR_v1_12@6,"Once trained, CALM can consistently solve goal-directed dialogue tasks.","Once trained, CALM can consistently solve complex tasks in dialogue.","Modify,Clarity",Clarity
6178,2-131,,2-131_v1_25@2,,Males had a mean of 76.3 with a range of 61 to 90 years.,"Delete,Fact/Evidence",Fact/Evidence
6179,2-131,,2-131_v1_25@3,,Females had a mean of 76.83 with a range of 53 to 90 years.,"Delete,Fact/Evidence",Fact/Evidence
6180,2-131,2-131_v2_60@3,,Krebs et al. have also previously reported no significant differences in retinal thickness measurements before and after correction of segmentation errors of scans taken using Cirrus™ <REF-11> .,,"Add,Fact/Evidence",Fact/Evidence
6181,2-131,2-131_v2_61@0,,"The differences in the mean thickness values before and after correction in scans taken using Spectralis™ were most obvious in the central subfields of the retina (C1, N1, S1, T1, and I1) with the peripheral subfields being spared (N2, S2, T2 and I2).",,"Add,Fact/Evidence",Fact/Evidence
6182,2-131,2-131_v2_61@1,,This may be attributed to the fact that the pathology of AMD is located centrally and therefore pathology related inaccuracies in segmentation are more likely to occur in these subfields.,,"Add,Claim",Claim
6183,2-131,2-131_v2_63@4,,Krebs et al. evaluated the repeatability of retinal thickness measurements using Spectralis™ and Cirrus™ in patients with AMD.,,"Add,Fact/Evidence",Fact/Evidence
6184,2-131,2-131_v2_63@5,,For images taken using Spectralis™ the mean difference between repeated measurements was found to be within 11µm before correction and within 1µm after correction.,,"Add,Fact/Evidence",Fact/Evidence
6185,2-131,2-131_v2_63@6,,For images taken using Cirrus™ the mean difference between repeated measurements was found to be within 6µm before correction and within 4µm after correction <REF-15> .,,"Add,Fact/Evidence",Fact/Evidence
6186,2-131,2-131_v2_66@4,,"In addition, in a subset of patients that had a difference in the severity of disease, both eyes were included in the analysis; this may also have resulted in possible bias.",,"Add,Claim",Claim
6187,2-131,2-131_v2_66@5,,The Cirrus device that was used to capture the images did not have eye tracking and may have led to the slightly larger COR values when compared to Spectralis.,,"Add,Claim",Claim
6188,2-131,,2-131_v1_7@4,,Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,"Delete,Fact/Evidence",Fact/Evidence
6189,2-131,,2-131_v1_44@0,,"Overall Spectralis™ had the lowest COR, with values ranging from 5–30 µm.","Delete,Fact/Evidence",Fact/Evidence
6190,2-131,,2-131_v1_9@1,,"To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.","Delete,Claim",Claim
6191,2-131,,2-131_v1_56@4,,"At this point, we are not aware of any previous study looking at the repeatability of Spectralis™ images in AMD.","Delete,Claim",Claim
6192,2-131,,2-131_v1_59@2,,"However, very few images needed to be corrected.","Delete,Fact/Evidence",Fact/Evidence
6193,2-131,2-131_v2_28@0,,No formal sample size calculation was performed before the conduct of the study.,,"Add,Fact/Evidence",Fact/Evidence
6194,2-131,2-131_v2_35@1,2-131_v1_30@1,The thickness measurements before and after correction were not statistically significant (P<.05) for any of the subfields and also when stratified by diagnosis.,The thickness measurements before and after correction were not statistically significant (p<0.05) for any of the subfields and also when stratified by diagnosis.,"Modify,Grammar",Grammar
6195,2-131,2-131_v2_9@0,2-131_v1_4@0,Results: Spectralis had the highest number of images needing manual correction.,Results: The Spectralis device had the highest number of images needing manual correction.,"Modify,Clarity",Clarity
6196,2-131,2-131_v2_37@3,2-131_v1_32@3,77% of the differences were less than 48μm and 50% were less than 10μm.,77% of the differences were less than 48 μm and 50% were less than 10 μm.,"Modify,Grammar",Grammar
6197,2-131,2-131_v2_41@0,2-131_v1_36@0,"The mean (±SD) of the macular thickness of all of the subfields, including the central 1mm subfield (FTH) for Stratus™, Cirrus™, and Spectralis™ before and after manual correction of scans, stratified by diagnosis of NV-AMD and NNV-AMD, is shown in Table 1 .","The mean (+SD) of the macular thickness of all of the subfields, including the central 1 mm subfield (foveal thickness; FTH) for Stratus™, Cirrus™, and Spectralis™ devices before and after manual correction of scans, stratified by diagnosis of NV-AMD and NNV-AMD, is shown in Table 1 .","Modify,Fact/Evidence",Fact/Evidence
6198,2-131,2-131_v2_41@1,2-131_v1_36@1,"For NV-AMD, the FTH values for central 1mm were 375µm (±129µm), 253µm (±74µm), 312µm (±110µm) for Spectralis™, Stratus™, and Cirrus™ respectively.","For NV-AMD, the FTH values for the central 1 mm were 375 µm (+129 µm), 253 µm (+74 µm), 312 µm (+110 µm) for Spectralis™, Stratus™, and Cirrus™ respectively.","Modify,Fact/Evidence",Fact/Evidence
6199,2-131,2-131_v2_41@2,2-131_v1_36@2,"After correction, the values were 335µm (±106µm) for Spectralis™ and 318µm (±110µm) for Cirrus™.","After correction, the values were 335 µm (+106 µm) for Spectralis™ and 318 µm (+110 µm) for Cirrus™.","Modify,Fact/Evidence",Fact/Evidence
6200,2-131,2-131_v2_41@3,2-131_v1_36@3,"On the other hand, the FTH values for NNV-AND in the central 1mm before correction were 298µm (87µm), 193µm (±32µm), and 229µm (±30µm) for Spectralis™, Stratus™, and Cirrus™ respectively.","On the other hand, the FTH values for NNV-AND in the central 1 mm before correction were 298 µm (+87 µm), 193 µm (+32 µm), and 229 µm (+30 µm) for Spectralis™, Stratus™, and Cirrus™ respectively.","Modify,Fact/Evidence",Fact/Evidence
6201,2-131,2-131_v2_41@4,2-131_v1_36@4,Spectralis™ was the only device to have a different FTH value of 248µm (±56µm) after correction.,Spectralis™ was the only device to have a different FTH value (248 µm +56 µm) after correction.,"Modify,Fact/Evidence",Fact/Evidence
6202,2-131,2-131_v2_41@6,2-131_v1_36@6,"The retinal thickness measurements obtained via the Cirrus™ were slightly less (range: 230 to 320µm), while Stratus™ had the lowest values, ranging from 190 to 270µm.","The retinal thickness measurements obtained via Cirrus™ were slightly less (range: 230 to 320 µm), while Stratus™ had the lowest values, ranging from 190 to 270 µm.","Modify,Grammar",Grammar
6203,2-131,2-131_v2_41@7,2-131_v1_36@7,There were no significant (p<.05) differences between the mean FTH of the first and second scans for each of the three devices.,There were no significant (p<0.05) differences between the mean FTH of the first and second scans for each of the three devices.,"Modify,Grammar",Grammar
6204,2-131,2-131_v2_9@2,2-131_v1_4@3,The CORs were lowest for Spectralis TM and similar and higher for Cirrus TM and Stratus TM .,The CORs were lowest for SpectralisTM and similar with each other and had higher values for CirrusTM and StratusTM.,"Modify,Clarity",Clarity
6205,2-131,2-131_v2_44@7,2-131_v1_39@7,"In the central subfield, Spectralis™ had a COR of 20µm NV-AMD which increased to 23µm; both Cirrus™ and Stratus™ had relatively larger CORs of 64µm (reduced to 49µm after correction) and 35µm, respectively.","In the central subfield, Spectralis™ had a COR of 20 µm NV-AMD which increased to 23 µm; both Cirrus™ and Stratus™ had relatively larger CORs of 64 µm (reduced to 49 µm after correction) and 35 µm, respectively.","Modify,Grammar",Grammar
6206,2-131,2-131_v2_44@8,2-131_v1_39@8,"For NNV-AMD, the COR for the central subfield was 15µm for both Cirrus™ and Spectralis™, and was 24µm for Stratus™.","For NNV-AMD, the COR for the central subfield was 15 µm for both Cirrus™ and Spectralis™, and was 24 µm for Stratus™.","Modify,Grammar",Grammar
6207,2-131,2-131_v2_44@9,2-131_v1_39@9,"After correction, the value decreased for Spectralis™ to 12µm and increased to 36µm for Cirrus™.","After correction, the value decreased for Spectralis™ to 12 µm and increased to 36 µm for Cirrus™.","Modify,Grammar",Grammar
6208,2-131,2-131_v2_12@2,2-131_v1_7@2,"One example is Age-related Macular Degeneration (AMD), a progressive, blinding disease that is mostly non-neovascular (NNV-AMD) but can be associated with choroidal neovascularization (NV-AMD).","One example is Age-related Macular Degeneration (AMD), a progressive, blinding disease that is mostly Non-Neovascular (NNV-AMD) but can be associated with choroidal Neovascularization (NV-AMD).","Modify,Grammar",Grammar
6209,2-131,2-131_v2_12@3,2-131_v1_7@3,"Currently, OCT is also being employed as an outcome measure in many multicenter clinical trials of AMD with Time Domain OCT (TD-OCT) device being the most common <REF-1> , <REF-2> .","Currently, OCT is also being employed as an outcome measure in many multicenter clinical trials of AMD with Time Domain OCT (TD-OCT) devices being the most common <REF-1> , <REF-2> .","Modify,Grammar",Grammar
6210,2-131,2-131_v2_2@0,2-131_v1_2@0,"Purpose: To evaluate the reproducibility and reliability of optical coherence tomography scans obtained using the time domain (TD-OCT) Stratus TM OCT, and the Spectral Domain (SD-OCT) Spectralis TM and Cirrus TM OCT devices before and after manual correction in eyes with either Neovascular (NV-AMD) or Non-Neovascular (NNV-AMD) age-related macular degeneration.","Purpose: To evaluate the reproducibility and reliability of Optical Coherence Tomography scans (OCT) obtained using the Time Domain (TD-OCT) StratusTM OCT, and the Spectral Domain (SD-OCT) SpectralisTM and CirrusTM OCT devices before and after manual correction in eyes with either Neovascular (NV-AMD) or Non-Neovascular (NNV-AMD) Age-related Macular Degeneration.","Modify,Clarity",Clarity
6211,2-131,2-131_v2_13@0,2-131_v1_8@0,"As this technology is increasingly being utilized by many ophthalmologists to evaluate and monitor patients and guide treatment decisions <REF-2> , it is important to understand the reliability and accuracy of thickness measurements obtained with various devices currently available.","As this technology is increasingly being utilized by many ophthalmologists to evaluate and monitor patients and guide treatment decisions <REF-2> , it is important to understand the reliability and accuracy of thickness measurements obtained with the various devices currently available.","Modify,Grammar",Grammar
6212,2-131,2-131_v2_49@2,2-131_v1_44@2,"The COR for Cirrus™ increased by 15–40µm after correction for NNV-AMD. Also, Cirrus™ COR values were 10–30µm higher than Stratus™ values for both NV-AMD and NNV-AMD.","The COR for Cirrus™ increased by 15–40 µm after correction for NNV-AMD. Also, Cirrus™ COR values were 10–30 µm higher than Stratus™ values for both NV-AMD and NNV-AMD.","Modify,Grammar",Grammar
6213,2-131,2-131_v2_13@1,2-131_v1_8@1,"Recently, studies have shown that in patients with AMD, there is a high frequency of errors in automated retinal thickness measurements due to incorrect segmentation of the retina in the TD-OCT machine specifically in NV-AMD <REF-2> , <REF-3> .","Recently, studies have shown that in patients with AMD, there is a high frequency of errors in automated retinal thickness measurements due to incorrect segmentation of the retina in the TD-OCT machine, specifically in NV-AMD <REF-2> , <REF-3> .","Modify,Grammar",Grammar
6214,2-131,2-131_v2_13@2,2-131_v1_8@2,"Using an Spectral Domain OCT (SD-OCT) device Menke et al. found that NNV-AMD had fewer errors than NV-AMD, mostly due to the pathology of the disease resulting in retinal pigment epithelial (RPE) layer changes <REF-4> .","Using a Spectral Domain OCT (SD-OCT) device Menke et al. found that retinal thickness measurements in NNV-AMD cases had fewer errors than in NV-AMD cases, mostly due to the pathology of the former disease resulting in retinal pigment epithelial (RPE) layer changes <REF-4> .","Modify,Clarity",Clarity
6215,2-131,2-131_v2_14@0,2-131_v1_9@0,"Manual correction of the algorithm is an option in newer generations of the review software and as more OCT devices are coming to the market, it is important to understand the clinical importance of manual correction of OCT algorithms and the agreement of thickness measurements from different machines before and after correction.","Manual correction of the OCT algorithm is an option in newer generations of the OCT review software and as more devices are coming to the market, it is important to understand the clinical importance of manual correction of OCT algorithms and the agreement of thickness measurements from different machines before and after correction.","Modify,Clarity",Clarity
6216,2-131,2-131_v2_54@0,2-131_v1_49@0,Figure 2a–f show Bland-Altman plots with 95% confidence intervals for the FTH comparison of the machines before and after correction.,Figure 2A–F show Bland-Altman plots with 95% confidence intervals for the FTH comparison of the machines before and after correction.,"Modify,Grammar",Grammar
6217,2-131,2-131_v2_54@1,2-131_v1_49@1,"Before correction, the mean difference between the machines was 32µm for Spectralis™ vs. Cirrus™, 52µm for Cirrus™ vs. Stratus™, and 84µm for Spectralis™ vs. Stratus™.","Before correction, the mean difference between the machines was 32 µm for Spectralis™ vs. Cirrus™, 52 µm for Cirrus™ vs. Stratus™, and 84 µm for Spectralis™ vs. Stratus™.","Modify,Grammar",Grammar
6218,2-131,2-131_v2_54@2,2-131_v1_49@2,"Manual correction reduced the differences, with it being 15µm for Spectralis™ vs. Cirrus™, 51µm for Cirrus™ vs. Stratus™, and 67µm for Spectralis™ vs. Stratus™.","Manual correction reduced the differences, with it being 15 µm for Spectralis™ vs. Cirrus™, 51 µm for Cirrus™ vs. Stratus™, and 67 µm for Spectralis™ vs. Stratus™.","Modify,Grammar",Grammar
6219,2-131,2-131_v2_54@3,2-131_v1_49@3,"When stratified by diagnoses, the values were 34µm and 29µm for Spectralis™ vs. Cirrus™, 53µm and 47µm for Cirrus™ vs. Stratus™, and 88µm and 79µm for Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD before correction, respectively.","When stratified by diagnoses, the values were 34 µm and 29 µm for Spectralis™ vs. Cirrus™, 53 µm and 47 µm for Cirrus™ vs. Stratus™, and 88 µm and 79 µm for Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD respectively, before correction.","Modify,Clarity",Clarity
6220,2-131,2-131_v2_54@4,2-131_v1_49@4,"After manual correction, the values reduced to 17µm and 14µm Spectralis™ vs. Cirrus™ and 70µm and 61µm Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD, respectively.","After manual correction, the values reduced to 17 µm and 14 µm Spectralis™ vs. Cirrus™ and 70 µm and 61 µm Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD, respectively.","Modify,Grammar",Grammar
6221,2-131,2-131_v2_54@5,2-131_v1_49@5,"The confidence interval widths, on average, were 5–10µm smaller than between an SD-OCT and TD-OCT machine.","The confidence interval widths, on average, were 5–10 µm smaller than when comparing between an SD-OCT and a TD-OCT machine.","Modify,Clarity",Clarity
6222,2-131,2-131_v2_58@2,2-131_v1_53@2,One such common and clinically relevant issue is the presence of a random error in the identification of the inner and outer boundaries of the retina by the algorithm.,One such common and clinically relevant issue is the presence of random errors in the identification of the inner and outer boundaries of the retina by the OCT algorithm.,"Modify,Clarity",Clarity
6223,2-131,2-131_v2_59@0,2-131_v1_54@0,Reasons for differences in our error rates compared to previous include a lack of standard definition of an algorithm error.,Reasons for differences in our error rates compared to previous studies include the lack of standard definition of an algorithm error.,"Modify,Clarity",Clarity
6224,2-131,2-131_v2_59@3,2-131_v1_54@3,"These differences may be due to the fact that our study was prospective and while acquiring scans, the operators tried their best to ensure no errors occurred during scan acquisition.","These differences may be due to the fact that our study was prospective and, while acquiring scans, the operators tried their best to ensure no errors occurred during scan acquisition.","Modify,Grammar",Grammar
6225,2-131,2-131_v2_60@2,2-131_v1_55@2,"For example, more than 50% of the Spectralis™ scans resulted in a 10µm or less change in the central subfield thickness.","For example, most of the Spectralis™ scans resulted in a 10 µm or less change in the central subfield thickness.","Modify,Fact/Evidence",Fact/Evidence
6226,2-131,2-131_v2_62@1,2-131_v1_55@4,"Correction reduced the difference of the thickness measurements between the two SD-OCT devices to less than 20um; in some cases as noted above, the difference was no longer statistically significant.","Correction reduced the difference of the thickness measurements between the two SD-OCT devices to less than 20 µm; in some cases as noted above, the difference was no longer statistically significant.","Modify,Grammar",Grammar
6227,2-131,2-131_v2_62@2,2-131_v1_55@5,"Other studies in normal and pathologic eyes including DME and macular degeneration have also demonstrated that the difference in retinal thickness between the SD machines can be attributed to the differences in segmentation of the automated algorithms <REF-7> , <REF-10> , <REF-12> .","Other studies in normal and pathologic eyes including Diabetic Macular Edema (DME) and macular degeneration have also demonstrated that the difference in retinal thickness between the SD machines can be attributed to the differences in segmentation of the automated algorithms <REF-7> , <REF-10> , <REF-11> .","Modify,Clarity",Clarity
6228,2-131,2-131_v2_16@1,2-131_v1_11@1,Informed consent was obtained from study subjects.,Written Informed consent was obtained from study subjects.,"Modify,Fact/Evidence",Fact/Evidence
6229,2-131,2-131_v2_63@3,2-131_v1_56@3,"The disease difference can be attributed to the pathology of NV-AMD disrupting the outer border, which makes it difficult for the automated algorithm to accurately segment the retinal layers <REF-13> , <REF-14> .","This difference between diseases can be attributed to the pathology of NV-AMD disrupting the outer border, which makes it difficult for the automated algorithm to accurately segment the retinal layers <REF-12> , <REF-13> .","Modify,Clarity",Clarity
6230,2-131,2-131_v2_63@7,2-131_v1_56@5,"Previous studies on normal eyes have reported a high repeatability of measurements with Spectralis™, with differences between repeated measurements being within 1µm <REF-12> , <REF-16> .","Previous studies on normal eyes have reported a high repeatability of measurements with Spectralis™, with differences between repeated measurements being within 1 µm <REF-11> , <REF-14> .","Modify,Grammar",Grammar
6231,2-131,2-131_v2_63@8,2-131_v1_56@6,"For Stratus™ OCT images, other studies have found central subfield repeatability values in patients with NV-AMD to be 50µm and 32–35µm for NNV-AMD patients after correction/exclusion of scans with errors <REF-8> , <REF-17> ; our study confirms this finding.","For Stratus™ OCT images, other studies have found central subfield repeatability values in patients with NV-AMD to be 50 µm and 32–35 µm for NNV-AMD patients after correction/exclusion of scans with errors <REF-8> , <REF-15> ; our study confirms this finding.","Modify,Grammar",Grammar
6232,2-131,2-131_v2_63@9,2-131_v1_56@7,"There has been one other published study looking at the repeatability of Cirrus™ OCT in NV-AMD, which found a central subfield repeatability value of 42um before correction and 26µm after exclusion of scans with significant segmentation errors <REF-18> .","There has been one other published study looking at the repeatability of Cirrus™ OCT in NV-AMD, which found a central subfield repeatability value of 42 µm before correction and 27 µm after exclusion <REF-16> .","Modify,Fact/Evidence",Fact/Evidence
6233,2-131,2-131_v2_18@0,2-131_v1_13@0,Patients with confirmed diagnosis of AMD were enrolled in the study.,Patients with a confirmed diagnosis of AMD were enrolled in the study.,"Modify,Grammar",Grammar
6234,2-131,2-131_v2_64@1,2-131_v1_57@1,"For NV-AMD, Cirrus had higher coefficients after correction, and for NNV-AMD, Cirrus™ had lower coefficients as compared to Stratus™.","For NV-AMD, Cirrus™ had higher coefficients after correction, and for NNV-AMD, Cirrus™ had lower coefficients compared to Stratus™.","Modify,Clarity",Clarity
6235,2-131,2-131_v2_65@2,2-131_v1_58@2,We found that 95% confidence intervals were narrower as compared to an SD-OCT and TD-OCT and correcting the algorithm errors further narrowed the intervals.,We found that 95% confidence intervals were narrower compared to an SD-OCT and TD-OCT and correcting the algorithm errors further narrowed the intervals.,"Modify,Clarity",Clarity
6236,2-131,2-131_v2_18@2,2-131_v1_13@2,Patients under treatment with intravitreal injections of anti-vascular endothelial growth factor (VEGF) agents were also allowed to participate in the study.,Patients under treatment with intravitreal injections of anti-Vascular Endothelial Growth Factor (VEGF) agents were also allowed to participate in the study.,"Modify,Grammar",Grammar
6237,2-131,2-131_v2_65@4,2-131_v1_58@4,This is mostly likely due to the effects of manually correcting the Spectralis™ images and that both machines have similar scanning technologies.,This is mostly likely due to the effects of manually correcting the Spectralis™ images and the fact that both machines have similar scanning technologies.,"Modify,Clarity",Clarity
6238,2-131,2-131_v2_65@6,2-131_v1_58@6,"Jaffe et al. reported similar results looking at NV-AMD, with limits of agreements being approximately 225um between a SD-OCT and TD-OCT <REF-7> .","Jaffe et al. reported similar results looking at NV-AMD, with limits of agreements being approximately 225 µm between a SD-OCT and TD-OCT <REF-7> .","Modify,Grammar",Grammar
6239,2-131,2-131_v2_65@7,2-131_v1_58@7,The poor agreement warrants caution for clinicians when trying to use the data from different machines interchangeably especially in the central 1mm of retina since most clinicians.,The poor agreement suggests that clinicians should exercise caution when trying to use the data from different machines interchangeably.,"Modify,Fact/Evidence",Fact/Evidence
6240,2-131,2-131_v2_66@2,2-131_v1_59@1,The version of software used for the Stratus™ images did not allow correction of segmentation errors and therefore these images had to be excluded from the analysis.,"First, the software version for the Stratus™ images would not allow correction of images.","Modify,Fact/Evidence",Fact/Evidence
6241,2-131,2-131_v2_66@3,2-131_v1_59@3,Two independent graders manually corrected all the images; this may have resulted in some inaccuracies in segmentation line correction.,"In addition, two people independently manually corrected the images, resulting in inaccuracies in segmentation line correction.","Modify,Clarity",Clarity
6242,2-131,2-131_v2_66@1,2-131_v1_59@4,All images were taken at a single imaging center; this might have introduced some bias.,"Finally, the images were only taken at one imaging center, which could have resulted in bias.","Modify,Clarity",Clarity
6243,2-131,2-131_v2_67@1,2-131_v1_60@1,Spectralis™ had the lowest COR values.,Spectralis™ had the lowest coefficient of repeatability values.,"Modify,Clarity",Clarity
6244,2-131,2-131_v2_19@0,2-131_v1_14@0,"Patients were scanned twice by certified OCT operators on a TD-OCT device (Stratus™ OCT) and two SD-OCT devices (Spectralis™, and Cirrus™ OCT) machines in random order and with 5–10 minutes between each device.",Patients were scanned twice by certified OCT operators on a TD-OCT device (Stratus™ OCT) and two SD-OCT devices (Spectralis™ and Cirrus™ OCT) machines in random order and with 5–10 minutes between each device.,"Modify,Grammar",Grammar
6245,2-131,2-131_v2_20@0,2-131_v1_15@0,Optical Coherence Tomography,Optical coherence tomography,"Modify,Grammar",Grammar
6246,2-131,2-131_v2_21@0,2-131_v1_16@0,"One TD-OCT machine, Stratus™ (software version 4), and two SD-OCT machines, Spectralis™ (software version 5.0 I and Cirrus™ (software version 5.0.0.326) were used.","One TD-OCT machine, Stratus™ (software version 4), and two SD-OCT machines, Spectralis™ (software version 5.01) and Cirrus™ (software version 5.0.0.326) were used.","Modify,Fact/Evidence",Fact/Evidence
6247,2-131,2-131_v2_21@3,2-131_v1_16@3,"Using the Stratus™, two fast macular thickness maps (FMTP) were acquired from each eye.","Using the Stratus™, two Fast Macular Thickness Maps (FMTM) were acquired from each eye.","Modify,Grammar",Grammar
6248,2-131,2-131_v2_3@0,2-131_v1_3@0,Design: Prospective observational study.,Methods: We conducted a prospective observational study of 36 patients (50 eyes) with NV-AMD or NNV-AMD at a university-based retina practice.,"Split+Modify,Clarity",Clarity
6250,2-131,2-131_v2_5@0,2-131_v1_3@0,Setting : University-based retina practice.,Methods: We conducted a prospective observational study of 36 patients (50 eyes) with NV-AMD or NNV-AMD at a university-based retina practice.,"Split+Modify,Clarity",Clarity
6251,2-131,2-131_v2_6@0,2-131_v1_3@0,Patients : Thirty-six patients (50 eyes) with NV-AMD or NNV-AMD.,Methods: We conducted a prospective observational study of 36 patients (50 eyes) with NV-AMD or NNV-AMD at a university-based retina practice.,"Split+Modify,Grammar",Grammar
6252,2-131,2-131_v2_24@0,2-131_v1_19@0,Scans from each of the three devices were reviewed at the Ocular Imaging Research and Reading Center at the Stanley M. Truhlsen Eye Institute by two independent graders.,Scans from each of the three devices were reviewed at the Retinal Imaging Research and Reading Center at the Wilmer Eye Institute by independent graders.,"Modify,Fact/Evidence",Fact/Evidence
6253,2-131,2-131_v2_24@1,2-131_v1_19@1,Segmentation errors due to incorrect identification of inner and outer retinal boundaries by automated algorithms in the Spectralis™ and Cirrus™ devices were identified and manually corrected by these graders.,Incorrect identification of inner and outer retinal boundaries by automated algorithms in Spectralis™ and Cirrus™ devices was manually corrected.,"Modify,Fact/Evidence",Fact/Evidence
6254,2-131,2-131_v2_24@2,2-131_v1_19@2,Stratus™ images could not be corrected due to the lack of editing capabilities in the operating system provided with the machine at the time of conducting the study.,Stratus™ images could not be corrected due to the lack of editing capabilities in the operating system provided with the machine at the time the study was conducted.,"Modify,Clarity",Clarity
6255,2-131,2-131_v2_24@3,2-131_v1_19@3,Only 5 patients required corrections and were excluded from the analysis.,Only five patients required corrections and were excluded from the analysis.,"Modify,Grammar",Grammar
6256,2-131,2-131_v2_24@5,2-131_v1_19@5,"Meanwhile each device identifies the inner limiting membrane (ILM) as the inner boundary of retina, identification of the outer boundary is different for each device.","Whereas each device identifies the inner limiting membrane (ILM) as the inner boundary of retina, identification of the outer boundary is different for each device.","Modify,Clarity",Clarity
6257,2-131,2-131_v2_7@0,2-131_v1_3@1,Procedure : OCT scans were taken simultaneously using one TD-OCT and two SD-OCT devices.,OCT scans were taken simultaneously using one TD-OCT and two SD-OCT devices.,"Modify,Clarity",Clarity
6258,2-131,2-131_v2_25@0,2-131_v1_20@0,"Whenever the foveal center could be identified, grids were repositioned for scans with off-center positioning of the ETDRS grid.","Whenever the foveal center could be identified, grids were repositioned for scans with off-center positioning of the Early Treatment Diabetic Retinopathy Study (ETDRS) grid.","Modify,Clarity",Clarity
6259,2-131,2-131_v2_8@0,2-131_v1_3@2,Main Outcome Measures : Macular thickness measurements were assessed before and after correction of the algorithm by constructing Bland-Altman plots for agreement and calculating intraclass correlation coefficients (ICCs) and coefficients of repeatability (COR) to evaluate intraclass repeatability.,Macular thickness measurements were assessed before and after correction of the OCT algorithm by constructing Bland-Altman plots for agreement and calculating intraclass correlation coefficients (ICCs) and coefficients of repeatability (COR) to evaluate intraclass repeatability.,"Modify,Clarity",Clarity
6260,2-131,2-131_v2_26@0,2-131_v1_21@0,The retinal thickness measurements of the nine standard ETDRS subfields ( Appendix A illustrates the nine-subfield abbreviations) were recorded from each device before and after correcting the errors in the scans algorithm.,The retinal thickness measurements of the nine standard ETDRS subfields ( Figure S1 illustrates the nine-subfield abbreviations) were recorded from each device before and after correcting the errors in the scans’ algorithm.,"Modify,Fact/Evidence",Fact/Evidence
6261,2-131,2-131_v2_28@4,2-131_v1_23@3,Statistical significance of difference in thickness before and after correction of images across devices was determined via student’s t-test with α = 0.05 with Bonferroni correction for multiple comparisons.,The statistical significance of difference in thickness before and after correction of images across devices was determined via the student’s t-test with α = 0.05 with Bonferroni correction for multiple comparisons.,"Modify,Grammar",Grammar
6288,2-180,,2-180_v1_39@7,,"Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.","Delete,Claim",Claim
6289,2-180,,2-180_v1_41@3,,"Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .","Delete,Fact/Evidence",Fact/Evidence
6290,2-180,2-180_v2_8@0,,We used ephrin-A2 -/- mice because they have previously been shown to have a specific learning deficit <REF-18> .,,"Add,Fact/Evidence",Fact/Evidence
6291,2-180,2-180_v2_8@2,,Thus we aimed to examine a learning-mediated effect of rTMS on dendritic spines.,,"Add,Fact/Evidence",Fact/Evidence
6292,2-180,2-180_v2_11@4,,Randomised littermates were not used because the breeding colony was structured to produce ephrin-A2/A5 double knockout mice for other studies and no WT littermates were obtained.,,"Add,Fact/Evidence",Fact/Evidence
6293,2-180,2-180_v2_13@4,,"We chose to stimulate after the task because we hypothesized that rTMS would enhance LTP-like processes, stabilizing new spines, and the associated synaptic connections, that had formed during learning.",,"Add,Claim",Claim
6294,2-180,2-180_v2_19@3,,"Unlike in cat studies, the coil was not in direct contact with the mouse head but was held as close as possible to the scalp (~1mm).",,"Add,Fact/Evidence",Fact/Evidence
6295,2-180,2-180_v2_19@4,,The gap between the coil and the head does not attenuate the field because magnetic fields decrease with distance from the source but are not modified by air or biological tissue (e.g. skin/scalp <REF-36> ).,,"Add,Fact/Evidence",Fact/Evidence
6296,2-180,2-180_v2_19@5,,"Unlike in the cat study, stereotaxic delivery was not attempted because the dimensions of the coil ensured that the field reached the entire dorsal hippocampus, which in the mouse, is relatively large in proportion to total brain size.",,"Add,Fact/Evidence",Fact/Evidence
6297,2-180,2-180_v2_40@7,,"Because of the lack of understanding of fundamental interactions between rTMS and behaviour, it would be of great interest to perform an exhaustive battery of behavioural tests in healthy wildtype mice (and eventually in animal models of disease) in conjunction with various rTMS protocols.",,"Add,Claim",Claim
6298,2-180,2-180_v2_40@8,,Subsequent anatomical and physiological analyses could then be carried out to elucidate the neural mechanisms of rTMS and gain insight into the treatment of human disease.,,"Add,Claim",Claim
6299,2-180,2-180_v2_43@0,,"Alternatively, the timing of rTMS delivery relative to the behavioural task may have influenced the outcome of our experiments.",,"Add,Claim",Claim
6300,2-180,2-180_v2_43@1,,"Here we stimulated after the task, however rTMS might have been more effective if delivered before.",,"Add,Claim",Claim
6301,2-180,2-180_v2_43@2,,"Because a single session of rTMS increases the size of dendritic spines and may activate silent synapses <REF-2> , this may “prime” the brain for learning.",,"Add,Claim",Claim
6302,2-180,2-180_v2_43@3,,"With such pre-treatment, an effect of rTMS might even have been detected in improved performances on a day to day basis.",,"Add,Claim",Claim
6303,2-180,2-180_v2_45@0,,"Importantly, we are conscious of the limitations of our rodent scaled rTMS delivery device which may have contributed to the lack of effect observed here.",,"Add,Claim",Claim
6304,2-180,2-180_v2_45@1,,"Although our coil had a relevant coil to brain ratio for mice, because of its small size, the intensity of the magnetic field did not reach the magnitude commonly used in humans (6mT compared to 1-2T), raising concern that our stimulation paradigm is not comparable to human rTMS.",,"Add,Claim",Claim
6305,2-180,2-180_v2_45@2,,"However, this raises a more general issue because similar criticism applies to studies that employ larger coils <REF-1> – <REF-3> : although these deliver the same fields used in humans, the focal nature of the stimulation is lost.",,"Add,Claim",Claim
6306,2-180,2-180_v2_45@3,,Additional effort in designing appropriate small animal rTMS coils is urgently needed to improve the construct validity of animal rTMS research.,,"Add,Claim",Claim
6307,2-180,2-180_v2_17@5,2-180_v1_16@4,"Secondly, even though traditional rTMS is considered to be focal, magnetic fields of lower intensity are delivered outside of the focal area <REF-33> , raising the possibility that low intensity stimulation may be contributing to therapeutic effects by acting on interconnected brain regions.","Secondly, even though traditional rTMS is considered to be focal, magnetic fields of lower intensity are delivered outside of the focal area, raising the possibility that low intensity stimulation may be contributing to therapeutic effects by acting on interconnected brain regions.","Modify,Fact/Evidence",Fact/Evidence
6308,2-180,2-180_v2_18@0,2-180_v1_17@0,"We chose a complex pattern of stimulation that is based on biomimetic principles (described in detail <REF-23> 59.9-ms trains of 20 pulses at 3 different frequencies as follows: 1 min warm-up at 6.71 Hz, 8 min treatment at 10.1 Hz, and 1 min cool down at 6.26 Hz) and has been shown to induce structural changes in mice <REF-23> .","We chose a complex pattern of stimulation because it has been shown to induce structural changes in mice <REF-19> (59.9-ms trains of 20 pulses at 3 different frequencies as follows: 1 min warm-up at 6.71 Hz, 8 min treatment at 10.1 Hz, and 1 min cool down at 6.26 Hz.","Modify,Fact/Evidence",Fact/Evidence
6309,2-180,2-180_v2_18@1,2-180_v1_17@1,The pulse was monophasic with a 300µs rise time and 100µs fall time.,"The pulse duration was 200 µs, which is within the range used in human rTMS.","Modify,Fact/Evidence",Fact/Evidence
6310,2-180,2-180_v2_19@1,2-180_v1_18@1,This allowed the stimulation coil to be held by the experimenter above the mouse’s head.,"This allowed the stimulation coil to be held by the experimenter above, but not in contact with, the top of the animal’s head for reproducible rTMS delivery in the alert animal (as for cat studies <REF-29> ).","Split+Modify,Clarity",Clarity
6311,2-180,2-180_v2_19@2,2-180_v1_18@1,"We thus delivered reproducible rTMS in the awake animal (as for cat studies <REF-34> , <REF-35> ).","This allowed the stimulation coil to be held by the experimenter above, but not in contact with, the top of the animal’s head for reproducible rTMS delivery in the alert animal (as for cat studies <REF-29> ).","Split+Modify,Clarity",Clarity
6312,2-180,2-180_v2_4@1,2-180_v1_4@1,"Importantly, rTMS induces long term potentiation (LTP) in rodent hippocampus in vitro <REF-2> and several sessions of high-frequency rTMS increases the capacity to induce LTP compared to untreated controls, suggesting it may also regulate metaplasticity <REF-3> , <REF-4> .","Importantly, rTMS induces long term potentiation (LTP) in rodent hippocampus in vitro <REF-2> and several sessions of high-frequency rTMS increases the capacity to induce LTP (metaplasticity) compared to untreated controls <REF-3> , <REF-4> .","Modify,Claim",Claim
6313,2-180,2-180_v2_38@3,2-180_v1_37@3,"Although it is difficult to draw conclusions from the null results presented here, the absence of observed behavioural and structural change is consistent with previously reported rTMS specificity for abnormal systems <REF-8> , <REF-23> .","The absence of observed behavioural and structural change is consistent with previously reported rTMS specificity for abnormal systems 8 , <REF-19> .","Modify,Claim",Claim
6314,2-180,2-180_v2_38@4,2-180_v1_37@4,The lack of adverse effects in our long-term study suggests that up to 5 weeks of daily sessions of low intensity pulsed magnetic field stimulation at the parameters used in this study appears safe to use in healthy participants.,"Furthermore, the lack of adverse effects in our long term study contributes evidence that rTMS is safe to use in healthy control participants <REF-20> .","Modify,Claim",Claim
6315,2-180,2-180_v2_40@3,2-180_v1_39@3,"This is consistent with previous reports that long-term rTMS effects are specific to abnormal brain circuitry: two weeks of rTMS improved visual tracking, visual electrophysiological function and topographical accuracy in a different strain of mice (ephrin-A2A5 -/- double knockouts) with abnormal circuitry but produced no lasting effects in wildtype mice <REF-23> .","This is consistent with previous reports that long-term rTMS effects are specific to abnormal brain circuitry: two weeks of rTMS improved visual tracking, visual electrophysiological function and topographical accuracy in mice with abnormal circuitry but produced no lasting effects in wildtype mice <REF-19> .","Modify,Fact/Evidence",Fact/Evidence
6316,2-180,2-180_v2_0@0,2-180_v1_0@0,Long term delivery of pulsed magnetic fields does not alter visual discrimination learning or dendritic spine density in the mouse CA1 pyramidal or dentate gyrus neurons,Long term delivery of pulsed magnetic fields does not improve learning or alter dendritic spine density in the mouse hippocampus,"Modify,Other",Other
6317,2-180,2-180_v2_40@6,2-180_v1_39@6,"Furthermore, there is a lack of studies assessing cognitive effects of long-term rTMS in patients together with healthy controls, which presents a large gap in knowledge <REF-8> .","To our knowledge, there have been no studies assessing cognitive effects of long-term rTMS in patients and healthy controls, which presents a large gap in knowledge.","Modify,Clarity",Clarity
6318,2-180,2-180_v2_42@1,2-180_v1_41@1,"Importantly, we found similar spine densities in sham wildtype and ephrin-A2 -/- mice, suggesting that spine density is not solely dependent on ephrin-A2, in agreement with the literature <REF-19> – <REF-21> .","Importantly, we found similar spine densities in sham wildtype and ephrin-A2 -/- mice, suggesting that if present, deficits in spines are subtle in ephrin-A2 -/- mice.","Modify,Fact/Evidence",Fact/Evidence
6319,2-180,2-180_v2_8@1,2-180_v1_41@2,"In addition, although ephrin-A2 is expressed in the mouse hippocampus throughout life and has been implicated in its topographic organisation <REF-19> , <REF-20> , there is no evidence that it is involved in synaptic plasticity or spine dynamics <REF-21> .","Although ephrin-A2 is expressed in the mouse hippocampus throughout life and has been implicated in its topographic organisation <REF-35> , <REF-36> , to our knowledge, there is no evidence that ephrin-A2 is involved in synaptic plasticity or spine dynamics.","Modify,Clarity",Clarity
6320,2-180,2-180_v2_42@2,2-180_v1_41@4,"As such, the null effect of rTMS on dendritic spine density may be attributed to the absence of both a specific spine and learning deficit in both wildtype and ephrin-A2 -/- mice.","As such, the null effect of rTMS on dendritic spine density is in line with our behavioural results and may be attributed to the absence of a specific spine deficit for rTMS to correct in both wildtype and ephrin-A2 -/- mice.","Modify,Claim",Claim
6321,2-180,2-180_v2_42@5,2-180_v1_42@1,"However, it is surprising that dendritic spine density remains unaffected after long-term stimulation, given our previous results using the same stimulation parameters, demonstrating structural reorganisation in abnormal axon terminals following multiple stimulations, but not a single rTMS session <REF-23> .","However, it is surprising that dendritic spine density remains unaffected after long-term stimulation, given our previous results using the same stimulation parameters, demonstrating structural reorganisation in abnormal axon terminals following multiple, but not single rTMS stimulation sessions <REF-19> .","Modify,Clarity",Clarity
6322,2-180,2-180_v2_6@2,2-180_v1_6@2,"Given the significant structural changes induced in the mouse visual system following repeated stimulation sessions, and evidence for structural changes in the human brain <REF-17> , we hypothesised that a similar long-term rTMS regime in combination with a hippocampus-dependent learning task, would rescue impaired learning strategies previously found in ephrin-A2 -/- mice <REF-18> and alter spine density in the hippocampus.","Given the significant structural changes induced in the mouse visual system following repeated stimulation sessions, we hypothesised that a similar long-term rTMS regime in combination with a hippocampal learning task, would rescue impaired learning strategies previously found in ephrin-A2 -/- mice <REF-17> and alter spine density in the hippocampus.","Modify,Fact/Evidence",Fact/Evidence
6323,2-180,2-180_v2_2@2,2-180_v1_2@2,"We delivered 5 weeks of daily pulsed rTMS stimulation to adult ephrin-A2 -/- and wildtype (C57BI/6j) mice (n=10 per genotype) undergoing a visual learning task and analysed learning performance, as well as spine density, in the dentate gyrus molecular and CA1 pyramidal cell layers in Golgi-stained brain sections.","We delivered 5 weeks of daily pulsed rTMS stimulation to ephrin-A2 -/- and wildtype mice (n=10 per genotype) undergoing a visual learning task and analysed learning performance, as well as spine density, in the dentate gyrus molecular and CA1 pyramidal cell layers in Golgi-stained brain sections.","Modify,Fact/Evidence",Fact/Evidence
6324,2-180,2-180_v2_8@3,2-180_v1_7@1,"Although mice of both genotypes learned the task, their performance remained suboptimal due to lack of motivation to obtain food rewards through insufficient food restriction <REF-22> and neither learning behaviour, nor hippocampal spine density were affected by long term rTMS.","Although the mice learned the task, their performance remained suboptimal due to lack of motivation to obtain food rewards through insufficient food restriction <REF-18> and neither learning behaviour, nor hippocampal spine density were affected by long term rTMS.","Modify,Clarity",Clarity
6325,2-180,2-180_v2_11@6,2-180_v1_10@5,"Mice were age matched, aged 8–10 weeks old (equivalent to young sexually mature adult in humans) when commencing the experiment.","Mice were age matched, aged 8–10 weeks old when commencing the experiment.","Modify,Fact/Evidence",Fact/Evidence
6326,2-180,2-180_v2_11@7,2-180_v1_10@6,"For the duration of the study, mice were kept in standard caging in a controlled environment (12/12 light/dark cycle; temperature 22°C±2°C, separated into cages with clear plastic walls (17 cm × 19 cm base, 16 cm high) based on sex and genotype (2–4 per cage)).","For the duration of the study, mice were kept in standard caging in a controlled environment (12/12 light/dark cycle; temperature 22°C±2°C, separated into cages with clear plastic walls (17 cm x 19 cm base, 16 cm high) based on sex and genotype (2–4 per cage)).","Modify,Grammar",Grammar
6407,2-278,,2-278_v1_5@3,,"Although there is one data point (a couple is due to be parents of twin boys, and the twins are fraternal), Efron does not use it to update prior knowledge.","Delete,Fact/Evidence",Fact/Evidence
6408,2-278,,2-278_v1_5@7,,"Without new data, our knowledge is by definition determined by prior information; thus, showing that the outcome of a Bayesian analysis with no new data is heavily influenced by the prior would not argue against Bayesian methods.","Delete,Claim",Claim
6409,2-278,,2-278_v1_5@8,,"Indeed, without data, Efron’s example is not Bayesian statistics and his conclusion about Bayesian statistics based on this example is unjustified.","Delete,Claim",Claim
6410,2-278,,2-278_v1_6@0,,We also have other more technical issues with Efron’s example.,"Delete,Claim",Claim
6411,2-278,,2-278_v1_6@1,,Efron interprets the term P(A) on the right side of the equation (see sidebar in Efron 2013a <REF-1> ) as the prior on the probability that twins are identical.,"Delete,Fact/Evidence",Fact/Evidence
6412,2-278,,2-278_v1_6@2,,"To make this prior uninformative, it is assigned a value of P(A) = 0.5 (see Efron 2013b <REF-2> ; although this is not stated in Efron 2013a <REF-1> ).","Delete,Fact/Evidence",Fact/Evidence
6413,2-278,,2-278_v1_6@3,,This uninformative prior is set in contrast to the informative “doctor’s prior” of P(A) = 1/3.,"Delete,Fact/Evidence",Fact/Evidence
6414,2-278,,2-278_v1_6@4,,"First, however, the parameter of interest is P(A|B) rather than P(A) according to Efron’s study question (see sidebar in Efron 2013a <REF-1> ), thus the focus should be on the appropriate prior for P(A|B).","Delete,Claim",Claim
6415,2-278,,2-278_v1_6@5,,"Second, for the uninformative prior, Efron mentions erroneously that he used a uniform distribution between zero and one, which is clearly different from the value of 0.5 that was used.","Delete,Fact/Evidence",Fact/Evidence
6416,2-278,,2-278_v1_6@6,,"Third, we find it at least debatable whether a prior can be called an uninformative prior if it has a fixed value of 0.5 given without any measurement of uncertainty.","Delete,Claim",Claim
6417,2-278,,2-278_v1_6@7,,"For example, if we knew that our chance of winning the next million-dollar jackpot were 50:50, would we really call this uninformative?","Delete,Claim",Claim
6418,2-278,,2-278_v1_8@0,,"We would very much like to check our calculations using frequentist methods; however, this is impossible because there is only one data point, and frequentist methods generally cannot handle such situations.","Delete,Claim",Claim
6419,2-278,2-278_v2_5@3,,One reason is that Efron considers the particular set of twin boys as the entire population.,,"Add,Claim",Claim
6420,2-278,2-278_v2_5@4,,"In this case, statistics is not needed because there is no random sample drawn from a larger population.",,"Add,Claim",Claim
6421,2-278,2-278_v2_6@0,,"Efron’s example can be rearranged so that it fits a more realistic situation in statistical data analysis, albeit with a very low sample size: consider the twin boys that, as Efron casually mentions, turned out to be fraternal, as a random sample from the larger population of twin boys and try to draw inference about the proportion of identical twins among the population of twin boys (note that this approach is different from the calculations provided by Efron).",,"Add,Claim",Claim
6422,2-278,2-278_v2_6@3,,"We think that to illustrate the influence of non-informative priors on results of Bayesian data analyses, such an approach would be fairer than the calculations given by Efron.",,"Add,Claim",Claim
6423,2-278,2-278_v2_2@0,2-278_v1_2@0,"In an article in Science on ""Bayes' Theorem in the 21st Century"", Bradley Efron uses Bayes' theorem to calculate the probability that twins are identical given that the sonogram shows twin boys.","In a recent article in Science on ""Bayes' Theorem in the 21st Century"", Bradley Efron uses Bayes' theorem to calculate the probability that twins are identical given that the sonogram shows twin boys.","Modify,Clarity",Clarity
6424,2-278,2-278_v2_5@2,2-278_v1_5@2,We argue that this example is relatively useless in illustrating Bayesian data analysis.,"We argue that this example is not only flawed, but useless in illustrating Bayesian data analysis because it does not rely on any data.","Modify,Claim",Claim
6425,2-278,2-278_v2_5@5,2-278_v1_5@4,"Rather, Efron combines different pieces of expert knowledge from the doctor and genetics using Bayes’ theorem.","Instead, Efron combines different pieces of expert knowledge from the doctor and genetics using Bayes’ theorem.","Modify,Clarity",Clarity
6426,2-278,2-278_v2_6@1,2-278_v1_7@0,"If we use the data point together with an uninformative uniform prior on P(A|B) (see Box 1 ) to determine the probability of identical twins given the twins are two boys, we obtain, with 95% certainty, a probability of between 0.01 and 0.84; if we use a highly informative prior based on information from the doctor and genetics, we obtain a probability of between 0.49 and 0.51.","If we use the data point together with an uninformative uniform prior on P(A|B) to determine the probability of identical twins given the twins are two boys (see Box 1 ), we obtain, with 95% certainty, a probability of between 0.01 and 0.84; if we use a highly informative prior based on information from the doctor and genetics, we obtain a probability of between 0.49 and 0.51.","Modify,Clarity",Clarity
6427,2-278,2-278_v2_2@2,2-278_v1_2@2,"While we agree that the choice of the prior is essential, we argue that the calculations on identical twins give a biased impression of the influence of uninformative priors in Bayesian data analyses.","We argue that this conclusion is problematic because Efron's example on identical twins does not use data, hence it is not Bayesian statistics; his priors are not appropriate and are not uninformative; and using the available data point and an uninformative prior actually leads to a reasonable posterior distribution.","Modify,Claim",Claim
6428,2-282,2-282_v2_5@3,,This phenomenon might be induced by a cortico-cerebellar activation during voluntary movements <REF-5> .,,"Add,Fact/Evidence",Fact/Evidence
6429,2-282,2-282_v2_7@2,,QPs were calculated for each movement executed by the patients (one run contains several movements; see section B).,,"Add,Fact/Evidence",Fact/Evidence
6430,2-282,2-282_v2_7@3,,"These QPs are also considered as probabilities of stimulation, given their potential application in a tremor suppression system based on BCI-trigged FES (see also section F).",,"Add,Fact/Evidence",Fact/Evidence
6431,2-282,2-282_v2_15@4,,Patients were told to keep the most relaxed attitude.,,"Add,Fact/Evidence",Fact/Evidence
6432,2-282,,2-282_v1_7@2,,Patients and the experimental procedure were as detailed in the next sections.,"Delete,Fact/Evidence",Fact/Evidence
6433,2-282,2-282_v2_29@2,,We decided to use a period of 2000 msec based on the available literature which considers that 2 seconds encompasses the preparation phase at the cortical level <REF-13> .,,"Add,Fact/Evidence",Fact/Evidence
6434,2-282,2-282_v2_59@9,,"These patients exhibited reproducible low values for the cortico-muscular coherence, by contrast to reproducible high values for the other QPs.",,"Add,Fact/Evidence",Fact/Evidence
6435,2-282,2-282_v2_59@10,,This highlights the importance of our multimodal approach.,,"Add,Claim",Claim
6436,2-282,2-282_v2_90@0,,"Our protocol in neurological patients with tremor differs from those in the literature, hence our study on the multiple combinations of frequency bands.",,"Add,Fact/Evidence",Fact/Evidence
6437,2-282,2-282_v2_90@1,,"When a neurological patient with tremor is seated and assessed, he/she may exhibit a tremor of the head and trunk.",,"Add,Claim",Claim
6438,2-282,2-282_v2_90@2,,This tremor may be pretty stable or rather intermittent.,,"Add,Claim",Claim
6439,2-282,2-282_v2_90@3,,"There may even be an overlap with the main frequencies of the EEG signal, for instance in the alpha band (a rapid head tremor may be found in patients).",,"Add,Claim",Claim
6440,2-282,2-282_v2_90@4,,"Therefore, we decided to have a close look to each of these bands.",,"Add,Fact/Evidence",Fact/Evidence
6441,2-282,2-282_v2_90@5,,"For instance, we have seen patients with cerebellar disorders and orthostatic tremor in whom the sub-band 8–10 Hz was much less informative as compared with the sub-band 10–12 Hz.",,"Add,Fact/Evidence",Fact/Evidence
6442,2-282,2-282_v2_90@6,,"We would like to point out that in the study of Pfurtscheller et al. on single-trial classification of EEG and imagination <REF-20> , the frequency of the most reactive components was 11±0.4 Hz (mean±SD).",,"Add,Fact/Evidence",Fact/Evidence
6443,2-282,2-282_v2_90@7,,The SD was thus small.,,"Add,Fact/Evidence",Fact/Evidence
6444,2-282,2-282_v2_90@8,,"Although the desynchronized components were centered at 10.9 Hz±0.9 Hz, the synchronized components were narrow-banded, with higher frequencies at 12.0 Hz±1.0 Hz.",,"Add,Fact/Evidence",Fact/Evidence
6445,2-282,2-282_v2_90@9,,We agree with the authors that the classification of single EEG trials improves when ERD and ERS patterns are combined for multiple tasks.,,"Add,Claim",Claim
6446,2-282,2-282_v2_90@10,,We aim to pursue the use of narrow bands of frequency in multiple tasks.,,"Add,Fact/Evidence",Fact/Evidence
6447,2-282,2-282_v2_91@5,,"Techniques of multichannel EEG compression, phase congruency and graphical representations aiming at a reduction of multidimensional data have been proposed <REF-23> – <REF-25> .",,"Add,Fact/Evidence",Fact/Evidence
6448,2-282,2-282_v2_91@6,,"However, no technique has been widely accepted so far.",,"Add,Claim",Claim
6449,2-282,2-282_v2_92@0,,"In theory, BCI is an interface between brain and computer.",,"Add,Claim",Claim
6450,2-282,2-282_v2_92@1,,"As such, our system would be a multimodal control unit, including an EEG-module like often used for BCIs, but also body modules to control a stimulation unit.",,"Add,Claim",Claim
6451,2-282,2-282_v2_92@2,,Future works could apply some feature selection algorithm and train the multimodal control unit in discriminating movements based on the multimodal input.,,"Add,Claim",Claim
6452,2-282,2-282_v2_92@3,,These two steps could further be included in one step e.g. by use of random forests.,,"Add,Claim",Claim
6453,2-282,2-282_v2_92@4,,"By doing so, the performance of the modules would be evaluated to find out which ones contribute most to a high detection rate.",,"Add,Claim",Claim
6454,2-282,2-282_v2_92@5,,"This would be done separately for each patient, thus taking into account the inter-individual variability.",,"Add,Claim",Claim
6455,2-282,2-282_v2_94@6,,Our data provide a ground for the concept of multimodal approach developed for the early detection of the intentionality of movement.,,"Add,Claim",Claim
6456,2-282,2-282_v2_94@7,,The presented probability trees are general schemes.,,"Add,Fact/Evidence",Fact/Evidence
6457,2-282,2-282_v2_94@8,,A case-by-case analysis is required.,,"Add,Claim",Claim
6458,2-282,2-282_v2_94@9,,"In order to provide the most possible accurate BCI-driven FES system, each subject needs to be studied in order to define the best combination of QPs.",,"Add,Claim",Claim
6459,2-282,2-282_v2_94@11,,The system would take into account these features.,,"Add,Claim",Claim
6460,2-282,2-282_v2_94@12,,"By analysing a larger group of patients, we might identify subgroups of patients on the basis of the results of the probability trees.",,"Add,Claim",Claim
6461,2-282,2-282_v2_94@13,,"In other words, the probability trees would be used as an elegibility procedure to multimodal BCI-driven treatments in neurological patients with tremor.",,"Add,Claim",Claim
6462,2-282,2-282_v2_95@0,,"Results obtained with the simulation study provide useful information about EEG QP in order to select patients more effectively for a BCI-based treatment, including rehabilitation.",,"Add,Claim",Claim
6463,2-282,2-282_v2_95@1,,The simulation demonstrates the relationship between the threshold and the QP.,,"Add,Fact/Evidence",Fact/Evidence
6464,2-282,2-282_v2_95@2,,Future studies could take advantage of these findings to select the best neurological candidates on the basis of the ERD/ERS for BCI-based management.,,"Add,Claim",Claim
6465,2-282,2-282_v2_100@6,,The analysis of the corticomuscular coherence shows that this parameter alone cannot be used to predict voluntary motion and be implemented in a BCI.,,"Add,Fact/Evidence",Fact/Evidence
6466,2-282,2-282_v2_100@12,,We propose to select a larger group of neurological patients to confirm the strength of the multimodal prediction.,,"Add,Claim",Claim
6467,2-282,2-282_v2_100@13,,The present study opens the door for future studies in terms of how to increase EEG-based detection of movement intention by incorporating information from multiple modules.,,"Add,Claim",Claim
6468,2-282,2-282_v2_101@0,,Data availability,,"Add,Other",Other
6469,2-282,2-282_v2_102@0,,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2014 Grimaldi G et al.,,"Add,Fact/Evidence",Fact/Evidence
6470,2-282,2-282_v2_103@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",,"Add,Fact/Evidence",Fact/Evidence
6471,2-282,2-282_v2_25@0,2-282_v1_23@0,"Cortical activation occurring during the preparation of movement is detected by the EEG module thanks to a method based on the event-related desynchronization/synchronization (ERD/ERS) phenomenon <REF-7> , <REF-9> , <REF-10> .",Cortical activation occurring during the preparation of movement is detected by the EEG module thanks to a method based on the event-related desynchronization/synchronization (ERD/ERS) phenomenon <REF-6> .,"Modify,Fact/Evidence",Fact/Evidence
6472,2-282,2-282_v2_25@1,2-282_v1_23@1,We extracted a QP for the detection of intentionality of movement <REF-11> by considering: (i) the changes in the β²/α and β/α ratio (representing bursts of β-γ frequencies) during the pre-movement period; (ii) an appropriate threshold indicating which peaks of ratios are actually followed by a movement (and therefore may be considered as a predictor of movement); (iii) the number of movements executed.,"By considering: (i) the changes in the β²/α and β/α ratio (representing bursts of β-γ frequencies) during the pre-movement period; (ii) an appropriate threshold indicating which peaks of ratios are actually followed by a movement (and therefore may be considered as a predictor of movement); (iii) the number of movements executed, we extracted a QP for the detection of intentionality of movement <REF-8> .","Modify,Clarity",Clarity
6473,2-282,2-282_v2_26@0,2-282_v1_24@0,"Upsampled EEG data were processed with a Hamming window of 256 samples, using an overlap of 250 in the time domain.","Upsampled EEG data were processed with a Hamming window of 256 samples, an overlap of 250 in the time domain.","Modify,Clarity",Clarity
6474,2-282,2-282_v2_29@1,2-282_v1_27@1,"The pre-movement period (lasting 2 seconds) was defined according to the acoustic order given to the patients and the detection of the beginning of movement via the gyroscopes, by considering 2 seconds back from the point of detection of the beginning of movement.",The pre-movement period (lasting 2 seconds) was defined according to the acoustic order given to the patients and the detection of the beginning of movement via the gyroscopes.,"Modify,Fact/Evidence",Fact/Evidence
6475,2-282,2-282_v2_33@4,2-282_v1_31@4,"The peaks (β/α and β²/α ratios) higher than a defined threshold were considered as indicators of a potential voluntary movement <REF-11> , given that they represent the detection of the cortical motor preparation of the movement <REF-14> , <REF-15> .","The peaks (β/α and β²/α ratios) higher than a defined threshold were considered as indicators of a potential voluntary movement <REF-8> , given that they represent the detection of the cortical motor preparation of the movement.","Modify,Fact/Evidence",Fact/Evidence
6476,2-282,2-282_v2_33@6,2-282_v1_31@6,EEG QP is the geometric mean of the probability of movement (true positive stimulations) and the percentage of movements predicted <REF-11> .,QP is the geometric mean of the probability of movement (true positive stimulations) and the percentage of movements predicted <REF-8> .,"Modify,Fact/Evidence",Fact/Evidence
6477,2-282,2-282_v2_35@2,2-282_v1_33@2,"The Welch's averaged modified periodogram method was used to compute the magnitude squared coherence of an EEG channel and an EMG electrode along the frequency subbands <REF-7> , <REF-12> , <REF-16> .","The Welch's averaged modified periodogram method was used to compute the magnitude squared coherence of an EEG channel and an EMG electrode along the frequency band <REF-6> , <REF-9> , <REF-10> .","Modify,Clarity",Clarity
6478,2-282,2-282_v2_45@0,2-282_v1_43@0,The rise of low frequencies was used for a mathematical modelling which considered:,The rise of low frequencies was used for a mathematical modeling which considered:,"Modify,Grammar",Grammar
6479,2-282,2-282_v2_47@0,2-282_v1_45@0,- the max PSD in the low-frequency band over time.,- the max PSD in the low frequency band over time.,"Modify,Grammar",Grammar
6480,2-282,2-282_v2_76@0,2-282_v1_75@0,E. Simulation of ERD/ERS: which thresholds would be required to obtain high EEG QPs?,E. Simulation of ERD/ERS: which thresholds would be required to obtain high QPs?,"Modify,Other",Other
6481,2-282,2-282_v2_94@1,2-282_v1_90@1,"When all the possible combinations of EEG/EMG/kinematic QPs are tested, the probability trees could yield an optimal efficiency.","When all the possible combinations of EEG/EMG/kinematic are tested, the probability tree could yield an optimal efficiency.","Modify,Clarity",Clarity
6482,2-282,2-282_v2_94@2,2-282_v1_90@2,"An exhaustive list of the probabilities for the entire amount of data recorded is not provided, because of the huge amount of time that this analysis requires in terms of data processing.","An exhaustive list of the probabilities for the entire amount of data recorded is not provided, because of the huge amount of time that this analysis requires.","Modify,Fact/Evidence",Fact/Evidence
6483,2-282,2-282_v2_94@3,2-282_v1_90@3,The global multimodal plot improves the effectiveness of the system by providing redundant parameters for the prediction of movements.,"The global multimodal plot improves the effectiveness of the system by providing redundant parameters for the prediction of movements: Moreover, it could be particularly helpful during the training phases of the BCI implementation in a given patient.","Split+Modify,Grammar",Grammar
6485,2-282,2-282_v2_100@4,2-282_v1_95@4,"We propose that the EEG QP can be complemented by the QPs extracted from the cortico-muscular coherence and the QPs obtained by the analysis of the changes in the kinematic signals, which occur prior to the voluntary movements.",We propose that the QP can be complemented by the cortico-muscular coherence and the analysis of the changes in the kinematic signals which occur prior to the voluntary movements.,"Modify,Clarity",Clarity
6486,2-282,2-282_v2_100@5,2-282_v1_95@5,We suggest a fusion of the QP parameters in order to increase the likelihood to detect the intentionality of movement.,We suggest a fusion of the parameters.,"Modify,Claim",Claim
6487,2-282,2-282_v2_100@10,2-282_v1_95@9,"This is in agreement with adaptive methods which are being developed currently with the goal of improving the classification algorithms for BCI system in order to extract EEG patterns related to a cognitive or motor status <REF-6> , <REF-27> .",This is in agreement with adaptive methods which are being developed currently with the goal of improving the classifiers <REF-5> .,"Modify,Fact/Evidence",Fact/Evidence
6488,2-282,2-282_v2_100@11,2-282_v1_95@10,"Our approach will have to be tested in a large sample of patients in the future, in order to demonstrate its real clinical usefulness in daily practice.","Also, our approach will have to be tested in a large sample of patients in the future in order to demonstrate its real clinical usefulness in daily practice.","Modify,Clarity",Clarity
6490,2-282,2-282_v2_5@2,2-282_v1_5@1,"Indeed, it has been reported that patients presenting a rest tremor show a decrement of the rest tremor before voluntary movement onset <REF-4> .","A kinematic module is applied purely for analyzing tremor, but also finds a specific application for the early detection of movement in patients presenting with a rest tremor - a tremor occurring while in a rest position - on the basis of a decrement of the rest tremor before movement onset <REF-4> .","Split+Modify,Fact/Evidence",Fact/Evidence
6491,2-282,2-282_v2_5@4,2-282_v1_5@2,"Why the use of a multimodal detection of the intentionality of movement? Although the potential for BCIs in neurological disorders is huge, the applicability of current BCI systems has been limited by several factors <REF-6> .","Why use a multimodal detection of the intentionality of movement? Although the potential for BCIs in neurological disorders is huge, the applicability of current BCI systems has been limited by several factors <REF-5> .","Modify,Clarity",Clarity
6492,2-282,2-282_v2_5@5,2-282_v1_5@3,One of them is the poor performance of BCIs based on EEG analysis only (also due to: inter-individual differences in the detectability of movement-related EEG-activity; differences in the way BCI users can voluntary modify their brain activity; and the fact that brain atrophy and neuroplastic changes occurring in patients affected with movement disorders makes it difficult to generalize EEG markers).,One of them is the poor performance of BCIs based on conventional EEG analysis.,"Modify,Fact/Evidence",Fact/Evidence
6493,2-282,2-282_v2_7@1,2-282_v1_7@1,"From each module, acting during different time-windows (EEG, kinematic and corticomuscular (described in detail in sections C–F) quality parameters (QPs) for the detection of the intentionality of movement or for the early detection of movement are extracted.","From each module (described in detail in sections B–F) quality parameters (QPs) for the detection of the intentionality of movement or the early detection of movement are extracted (we extract QPs and probabilities of stimulation for the EEG and Kinematic modules, and probability of stimulation for the corticomuscular module).","Modify,Fact/Evidence",Fact/Evidence
6494,2-282,2-282_v2_11@0,2-282_v1_11@0,"Acquisition of data was carried out on 4 neurological patients exhibiting a bilateral upper limb tremor (combinations of rest, postural and/or kinetic tremor), following approval of the Ethical Committee of ULB – Hôpital Erasme ( Table 1 ).","Acquisition of data was carried out on 4 neurological patients exhibiting a bilateral upper limb tremor (combinations of rest, postural and/or kinetic tremor), following approval of the Ethical Committee of ULB – Hôpital Erasme.","Modify,Fact/Evidence",Fact/Evidence
6495,2-282,2-282_v2_11@4,2-282_v1_11@4,Mean age of the patients was 62±20 years.,Mean age of the patients was 62 ± 20 years.,"Modify,Grammar",Grammar
6496,2-282,2-282_v2_11@6,2-282_v1_11@6,The ADL-T24 score range was 3–20/24 <REF-7> .,The ADL-T24 score range was 4–17/24 <REF-6> .,"Modify,Fact/Evidence",Fact/Evidence
6497,2-282,2-282_v2_11@7,2-282_v1_11@7,The Schwab and England ADL score ranged from 50 to 80% <REF-8> .,The Schwab and England ADL score ranged from 50 to 100% <REF-7> .,"Modify,Fact/Evidence",Fact/Evidence
6498,2-282,2-282_v2_15@5,2-282_v1_13@3,"After hearing an acoustic signal, they prepared themselves for the execution of movement by mental imagery of the movement.","After (1) hearing an acoustic signal, the patient (2) prepared themselves mentally for the execution of movement and (3) performed the task.","Modify,Clarity",Clarity
6499,2-282,2-282_v2_15@9,2-282_v1_13@8,"The nomenclature used for the recorded files–as reported in figures- is “pppFNnn” standing for patient’s code, task executed (""Finger-to-nose"") and run number, respectively.","The nomenclature used for the recorded files is “pppFNnn"" standing for patient’s code, task executed (""Finger-to-nose"") and run number, respectively.","Modify,Clarity",Clarity
6500,2-282,2-282_v2_17@0,2-282_v1_15@0,"(i) IMU sensors (inertial measurement units: tri-axial gyroscopes, accelerometers, magnetometers).","(i) IMU sensors (inertial measurement units; tri-axial gyroscopes, accelerometers, magnetometers).","Modify,Grammar",Grammar
6501,2-282,2-282_v2_21@1,2-282_v1_19@1,"In order to build a “movement window”, the signal from the magnetometer (which provides a very clean signal) is processed first.","In order to build a “movement window”, the signal from the magnetometer (which provides a very clean signal), is processed first.","Modify,Grammar",Grammar
7189,3-256,3-256_v2_98@9,,"Ultimately, whether insulin sensitivity is truly one of the mechanisms by which the aging-retarding (and thus, lifespan-extending) effects of the Ghr/bp gene disruption are mediated, and/or account for the differential response of GHR-KO mice to CR, will require direct experimental demonstration.",,"Add,Claim",Claim
7190,3-256,3-256_v2_100@0,,Certain aspects of the indirect calorimetry results deserve special treatment.,,"Add,Claim",Claim
7191,3-256,3-256_v2_100@1,,"Firstly, energy expenditure (EE) was greater in GHR-KO mice compared to their littermate controls, regardless of diet; yet heat production (HP) did not differ by genotype.",,"Add,Fact/Evidence",Fact/Evidence
7192,3-256,3-256_v2_100@2,,We suspect that a technical reason exists for this incongruence.,,"Add,Claim",Claim
7193,3-256,3-256_v2_100@3,,"The formula for calculating HP with indirect calorimetric gas-exchange data, formulated based on reconciling direct calorimetric measures with indirect calorimetric gas-exchange measures in normal-sized, lean mice [ Arch et al. , 2006 ], is [((4.33 × VO 2 ) + (0.67 × VCO 2 )) + (Wt.(kg.)) × (60 Min./Hr.)]; and, thus, is not necessarily appropriate for accurately measuring HP via indirect calorimetry in dwarf mice, which have divergent body composition (most-notably obesity) compared to their littermate controls [ Berryman et al. , 2004 ; Berryman et al. , 2010 ].",,"Add,Fact/Evidence",Fact/Evidence
7194,3-256,3-256_v2_100@4,,"More directly, that the heat production equation factors in body weight results in lighter dwarf mice having lower calculated HP, for given amounts of oxygen inhalation (VO 2 ) and carbon dioxide expiration (VCO 2 ), than their normal-sized littermates.",,"Add,Fact/Evidence",Fact/Evidence
7195,3-256,3-256_v2_100@5,,"Additionally of note (although not statistically significant), caloric restriction tended to reduce HP in GHR-KO mice but tended to raise it in littermate controls.",,"Add,Fact/Evidence",Fact/Evidence
7196,3-256,3-256_v2_100@6,,"Finally, (as alluded to in the paragraph above) for VO 2 , VCO 2 , and EE, gas-exchange-based measures of metabolism were robustly heightened by CR in littermates, but this effect was muted or absent in GHR-KO’s.",,"Add,Fact/Evidence",Fact/Evidence
7197,3-256,3-256_v2_100@7,,"Therefore, CR-effected increased metabolism could be a mean by which that diet slows senescence in littermates; moreover, the attenuation of this metabolic effect in GHR-KO mice (whether due to a ceiling effect, in which VO 2 and CO 2 being innately high in GHR-KO mice on A.L. makes it so that CR is unable to raise them much higher, or other rationales) might partially explain the attenuation of the aging-retarding (including life expectancy-increasing) effect of CR on GHR-KO mice.",,"Add,Claim",Claim
7198,3-256,3-256_v2_76@3,3-256_v1_76@3,"Additionally, CR increased the plasma insulin content in GHR-KO mice [( p < 0.05), ( Figure 2F )].","Additionally, CR increased the plasma insulin content in GHR-KO mice [( p < 0.05, ( Figure 2F )].","Modify,Grammar",Grammar
7199,3-256,3-256_v2_98@2,3-256_v1_98@2,"Our insulin sensitivity results in GHR-KO mice on 30% CR differed from those obtained in a previous study, showing that caloric restriction promotes euglycemia in GHR-KO mice ( Figure 2C ).",Our insulin sensitivity results in GHR-KO mice on 30% CR differed from those obtained in a previous study showing that caloric restriction promotes euglycemia in GHR-KO mice ( Figure 2C ).,"Modify,Grammar",Grammar
7200,3-256,3-256_v2_98@5,3-256_v1_98@5,"Moreover, data from macromolecular analysis of insulin signaling in GHR-KO mice on CR, including decreased insulin receptor (INSR) and thymoma viral proto-oncogene 1/protein kinase b (AKT1/PKB) concentrations in the skeletal musculature of GHR-KO’s on CR, and decreased phosphatidylinositol-4,5-bisphosphate 3-kinase (PI3K) subunits content in the livers of GHR-KO’s on CR, (all relative to GHR-KO’s on AL), also corroborate and portend decreased insulin sensitivity in GHR-KO mice on CR [ Bonkowski et al. , 2009 ].","Moreover, data from macromolecular analysis of insulin signaling in GHR-KO mice on CR, including decreased insulin receptor (INSR) and thymoma viral proto-oncogene 1/protein kinase b (AKT1/PKB) concentrations in the skeletal musculature of GHR-KO’s on CR, and decreased phosphatidylinositol-4,5-bisphosphate 3-kinase (PI3K) subunits content in the livers of GHR-KO’s on CR (relative to GHR-KO’s on AL), also corroborate and portend decreased insulin sensitivity in GHR-KO mice on CR [ Bonkowski et al. , 2009 ].","Modify,Clarity",Clarity
7201,3-256,3-256_v2_98@6,3-256_v1_98@6,"Additionally, it is worth noting that a tight regulation of euglycemia would be more consistent with health and survival than a predilection for hypoglycemia [ Tan & Flanagan, 2013 ]; thus, “improving health”, as CR has been broadly documented as doing, might mean preventing the innate endocrinological/metabolic derangements that are merely coincident with the longevity of the GHR-KO mouse.","Additionally, it is worth noting that a tight regulation of euglycemia would be more consistent with health and survival than a predilection for hypoglycemia [ Tan & Flanagan, 2013 ], thus “improving health”, as CR has been broadly documented as doing, might mean preventing the innate endocrinological/metabolic derangements that are merely coincident with the longevity of the GHR-KO mouse.","Modify,Grammar",Grammar
7202,3-256,3-256_v2_99@1,3-256_v1_99@1,"We discovered that CR did not alter the metabolism or spontaneous activity of GHR-KO mice, and also revealed that CR has no effect on the anxiety or memory function of GHR-KO mice.",We discovered that CR did not alter the metabolism or spontaneous activity of GHR-KO mice and also revealed that CR has no effect on the anxiety or memory function of GHR-KO mice.,"Modify,Grammar",Grammar
7203,3-256,3-256_v2_103@0,3-256_v1_102@0,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2015 Arum O et al.,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2014 Arum O et al.,"Modify,Fact/Evidence",Fact/Evidence
7204,3-256,3-256_v2_16@0,3-256_v1_16@0,"The resulting mice had elements of a 129/Ola, a Balb/c, two C57Bl/6J, and two C3H/HeJ stocks; therefore, although lacking the methodological benefits of “reproducible genetic heterogeneity” [ Miller et al. , 1999 ], this stock possesses considerable genetic variation, and thus the results are likely applicable to other mouse populations.","The resulting mice had elements of a 129/Ola, a Balb/c, two C57Bl/6J, and two C3H/HeJ stocks; therefore, although lacking the methodological benefits of “reproducible genetic heterogeneity” [ Miller, et al. , 1999 ], this stock possesses considerable genetic variation, and thus the results are likely applicable to other mouse populations.","Modify,Grammar",Grammar
7313,4-143,4-143_v2_8@1,,None of the three children had a concussion previously.,,"Add,Fact/Evidence",Fact/Evidence
7314,4-143,4-143_v2_8@3,,"Their symptoms were not assessed on the day they visited UWM, although one of the patients (cc1) was not able to speak or walk normally at that time.",,"Add,Fact/Evidence",Fact/Evidence
7315,4-143,4-143_v2_25@3,,"Third, age and sex were not matched perfectly between the patients and the controls tested in the present study.",,"Add,Fact/Evidence",Fact/Evidence
7316,4-143,4-143_v2_25@4,,"However, we are not aware of any findings that indicate significant differences between boys and girls within the age range of 12–17 years with regard to their visuomotor adaptation capabilities.",,"Add,Claim",Claim
7317,4-143,4-143_v2_25@5,,"In addition, it has been shown that during targeted reaching movements under a visuomotor rotation condition, visuomotor representations of children who were 6 or 8 years old differed from those of adults, although those of 11 year-old children did not <REF-12> .",,"Add,Fact/Evidence",Fact/Evidence
7318,4-143,4-143_v2_25@6,,"Our data do not seem to indicate sex-related differences either, in that the adaptation pattern of cc2 (15 year-old female) was very similar to that of ctls 1 and 3 (12 and 14 year-old males, respectively).",,"Add,Fact/Evidence",Fact/Evidence
7319,4-143,4-143_v2_25@7,,"Thus, while the potential effects of age- and sex-related differences between the patients and the controls cannot be completely excluded, it seems unlikely that our results were substantially influenced by such effects.",,"Add,Claim",Claim
7320,4-143,4-143_v2_23@3,4-143_v1_23@3,"According to the TMT scores, thus, one may conclude that all of the children who had a concussion 5 to 8 days prior to their participation in this study did not seem to have neurocognitive impairments.","According to the TMT scores, thus, one may conclude that all these children who had a concussion 5 to 8 days prior to their participation in this study did not seem to have neurocognitive impairments.","Modify,Clarity",Clarity
7321,4-143,4-143_v2_24@0,4-143_v1_24@0,"The patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.","Their patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.","Modify,Clarity",Clarity
7322,4-143,4-143_v2_2@6,4-143_v1_2@6,Results showed that only one of the three concussed children showed a score from the trail making test that was worse than the scores obtained from control subjects.,Results showed that only one of the three concussed children showed a score from the trail making test that was worse than those obtained from control subjects.,"Modify,Clarity",Clarity
7323,4-143,4-143_v2_24@1,4-143_v1_24@1,"Specifically, the first block of DE in the adaptation session that was significantly different from the last block of DE of the baseline session occurred later in the patients than in the controls; and the rate of performance change was higher (i.e., slower adaptation) in the patients as well.","Specifically, the first block of DE in the adaptation session that is significantly different from the last block of DE of the baseline session occurred later in the patients than in the controls; and the rate of performance change was higher (i.e., slower adaptation) in the patients as well.","Modify,Grammar",Grammar
7324,4-143,4-143_v2_25@2,4-143_v1_25@2,"Second, we did not collect any information regarding our subjects’ demographic and social statuses, which could be considered as potential confounding characteristics.","Also, we did not collect from our subjects any information regarding their demographic and social status, which could be considered as potential confounding characteristics.","Modify,Clarity",Clarity
7325,4-143,4-143_v2_26@0,4-143_v1_26@0,"In conclusion, the results from this study provide preliminary data indicating that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.","In conclusion, the results from this study indicate that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.","Modify,Clarity",Clarity
7326,4-143,4-143_v2_2@9,4-143_v1_2@9,Our findings provide preliminary data that suggest that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.,Our findings collectively suggest that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.,"Modify,Clarity",Clarity
7327,4-143,4-143_v2_4@3,4-143_v1_4@3,"In this short observational/case study, we investigated the pattern of adaptation to a novel visuomotor condition in three children who suffered a concussion while playing sports, and compared their adaptation patterns to those obtained from three children without a concussion.","In this research, we investigated the pattern of adaptation to a novel visuomotor condition in three children who suffered a concussion while playing sports, and compared their adaptation patterns to those obtained from three children without a concussion.","Modify,Clarity",Clarity
7328,4-143,4-143_v2_6@0,4-143_v1_6@0,"The purpose of this observational/case study was to determine qualitatively whether concussed children with neurocognitive impairments, as indicated by the TMT scores, would also demonstrate sensorimotor deficits, as indicated by the visuomotor adaptation patterns.","The purpose of this observational study was to determine whether concussed children with neurocognitive impairments, as indicated by the TMT scores, would also demonstrate sensorimotor deficits, as indicated by the visuomotor adaptation patterns.","Modify,Clarity",Clarity
7329,4-143,4-143_v2_8@0,4-143_v1_8@0,"Three children (15 years old, one male (cc3)), who presented to the Emergency Department at the Children’s Hospital of Wisconsin within 24 hours from the time of injury and who received a diagnosis of concussion (Glasgow Coma Scale ≥ 14), participated in this study.","Three children (15 years old), who presented to the Emergency Department at the Children’s Hospital of Wisconsin within 24 hours from the time of injury and who received a diagnosis of concussion (Glasgow Coma Scale ≥ 14), participated in this study.","Modify,Fact/Evidence",Fact/Evidence
7330,4-143,4-143_v2_8@4,4-143_v1_8@2,"Three children (12 (cc1), 14 (cc3) and 17 (cc2) years old, all males), who were recruited from the Milwaukee Metropolitan area, served as controls.","Three children (12, 14 and 17 years old), who were recruited from the Milwaukee Metropolitan area, served as controls.","Modify,Fact/Evidence",Fact/Evidence
7331,4-143,4-143_v2_8@5,4-143_v1_8@3,"Selection criteria for subjects were the same for both patients and controls (except their concussion status), which were: subject was 10–17 years of age, regularly participated in an athletic activity, was English-speaking, was right handed, and had no neurological disease or peripheral disorder affecting movement of the right arm.","Selection criteria for subjects were the same between patients and controls (except their concussion status), which were: subject is 10–17 years of age, regularly participates in an athletic activity, is English-speaking, is right handed, and has no neurological disease or peripheral disorder affecting movement of the right arm.","Modify,Grammar",Grammar
7332,4-143,4-143_v2_2@2,4-143_v1_2@2,"In this observational/case study, we investigated the association between neurocognitive function, assessed using a trail making test, and sensorimotor function, assessed using a visuomotor adaptation task, in three children who suffered from a concussion while playing sports.","In this observational study, we investigated the association between neurocognitive function, assessed using a trail making test, and sensorimotor function, assessed using a visuomotor adaptation task, in three children who suffered from a concussion while playing sports.","Modify,Clarity",Clarity
7333,4-143,4-143_v2_14@0,4-143_v1_14@0,"Because of the nature of the present study (i.e., observational/qualitative/case study of concussed children), we only tried to recruit a small number of concussed children.","Because of the nature of the present study (i.e., observational/case study of concussed children), we only tried to recruit a small number of concussed children.","Modify,Clarity",Clarity
7527,5-1356,5-1356_v2_48@0,,"In these simulations, the overall best performance of the DM model is achieved when dispersion parameters are estimated with the Cox-Reid APL and the dispersion moderation is applied.",,"Add,Fact/Evidence",Fact/Evidence
7528,5-1356,5-1356_v2_48@1,,"This strategy leads to p-value distributions that in most of the cases are closer to the uniform distribution ( Figure 1D , Figure S4 and Figure S11 ).",,"Add,Fact/Evidence",Fact/Evidence
7529,5-1356,5-1356_v2_50@1,,For each condition three replicates were simulated resulting in 3 versus 3 comparisons.,,"Add,Fact/Evidence",Fact/Evidence
7530,5-1356,5-1356_v2_74@3,,"To further investigate characteristics of detected tuQTLs, we measured enrichment of splicing-related features as used in a previous comparison <REF-35> .",,"Add,Fact/Evidence",Fact/Evidence
7531,5-1356,5-1356_v2_74@4,,"This includes the location of tuQTLs within exons, within splice sites, in the surrounding of GWAS SNPs and distance to the closest exon.",,"Add,Fact/Evidence",Fact/Evidence
7532,5-1356,,5-1356_v1_33@0,,Detecting DS and sQTLs with the DM model,"Delete,Other",Other
7533,5-1356,,5-1356_v1_7@0,,Expressed transcripts are generated by alternatively including exons into mature mRNAs.,"Delete,Fact/Evidence",Fact/Evidence
7534,5-1356,,5-1356_v1_8@2,,"From an inference point of view, sQTL analyses are equivalent to DS analyses where covariates are defined by genotypes.","Delete,Claim",Claim
7535,5-1356,,5-1356_v1_98@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).","Delete,Fact/Evidence",Fact/Evidence
7536,5-1356,5-1356_v2_7@0,,Genes may express diverse transcript isoforms (mRNA variants) as a consequence of alternative splicing or due to the differences in transcription start sites and polyadenylation sites <REF-16> .,,"Add,Fact/Evidence",Fact/Evidence
7537,5-1356,5-1356_v2_28@0,,Detecting DTU and tuQTLs with the Dirichlet-multinomial model,,"Add,Other",Other
7538,5-1356,5-1356_v2_2@5,5-1356_v1_2@5,We propose a statistical framework based on the Dirichlet-multinomial distribution that can discover changes in isoform usage between conditions and SNPs that affect relative expression of transcripts using these quantifications.,We propose a statistical framework based on the Dirichlet-multinomial distribution that can discover changes in isoform usage between conditions and SNPs that affect splicing outcome using these quantifications.,"Modify,Claim",Claim
7539,5-1356,5-1356_v2_17@1,5-1356_v1_15@1,"Similarly, separate modeling and testing of exon junctions ( Altrans <REF-27> ) or splicing events ( rMATS <REF-29> , GLiMMPS <REF-32> , Jia et al. <REF-33> , Montgomery et al. <REF-49> ) of a gene leads to non-independent statistical tests, although the full effect of this on calibration (e.g., controlling the rate of false discoveries) is not known.","Similarly, separate modeling and testing of exon junctions ( Altrans <REF-26> ) or splicing events ( rMATS <REF-28> , GLiMMPS <REF-30> , <REF-31> , <REF-47> ) of a gene leads to non-independent statistical tests, although the full effect of this on calibration (e.g., controlling the rate of false discoveries) is not known.","Modify,Fact/Evidence",Fact/Evidence
7540,5-1356,5-1356_v2_24@1,5-1356_v1_26@1,"The mean and covariance matrix of random proportions Π are ( Π ) = γ / γ + = π and ( ∏ ) = { γ + diag ( γ ) − γ γ T } / { γ + 2 ( γ + + 1 ) } , respectively.","The mean and covariance matrix of random proportions Π are ( Π ) = γ/γ + = π and ( Π ) = {γ + diag( γ ) – γ γ T }/{ γ + 2 ( γ + + 1)}, respectively.","Modify,Grammar",Grammar
7541,5-1356,5-1356_v2_26@0,5-1356_v1_29@0,"The mean of Y is unchanged at ( Y ) = {( Y | Π )} = ( m Π ) = m γ / γ + = m π , while the covariance matrix of Y is given by ( Y ) = cm {diag( π ) − ππ T }, where c = ( m + γ + )/(1+ γ + ) is an additional factor when representing the Dirichlet-multinomial covariance to the ordinary multinomial covariance.","The mean of Y is unchanged at ( Y ) = {( Y | Π )} = ( m Π ) = m γ /γ + = m π , while the covariance matrix of Y is given by ( Y ) = cm {diag( π ) – ππ T }, where c = ( m +γ + )/(1+γ + ) is an additional factor when representing the Dirichlet-multinomial covariance to the ordinary multinomial covariance.","Modify,Grammar",Grammar
7542,5-1356,5-1356_v2_27@0,5-1356_v1_30@0,"We can represent the DM distribution using an alternative parameterization: π = γ / γ + and θ = 1/(1 + γ + ); then, the covariance of Y can be represented as ( Y ) = n {diag( π ) − ππ T } {1 + θ ( n − 1)}, where θ can be interpreted as a dispersion parameter.","We can represent the DM distribution using an alternative parameterization: π = γ /γ + and θ = 1/(1 + γ + ); then, the covariance of Y can be represented as ( Y ) = n {diag( π ) – ππ T }{1 + θ ( n – 1)}, where θ can be interpreted as a dispersion parameter.","Modify,Grammar",Grammar
7543,5-1356,5-1356_v2_29@10,5-1356_v1_36@1,"In comparisons across c groups, the number of degrees of freedom is ( c − 1) × ( q − 1).","In comparisons across c groups, the number of degrees of freedom is ( c – 1) × ( q – 1).","Modify,Grammar",Grammar
7544,5-1356,5-1356_v2_30@0,5-1356_v1_37@0,"In a DTU analysis, groups are defined by the design of an experiment and are the same for each gene.","In a DS analysis, groups are defined by the design of an experiment and are the same for each gene.","Modify,Clarity",Clarity
7545,5-1356,5-1356_v2_30@1,5-1356_v1_37@1,"In tuQTL analyses, the aim is to find nearby (bi-allelic) SNPs associated with transcript usage of a gene.","In sQTL analyses, the aim is to find nearby (bi-allelic) SNPs associated with alternative splicing of a gene.","Modify,Fact/Evidence",Fact/Evidence
7546,5-1356,5-1356_v2_30@3,5-1356_v1_37@3,"Thus, tuQTL analyses are similar to DTU analyses with the difference that multiple models are fitted and tested for each gene.","Thus, sQTL analyses are similar to DS analyses with the difference that multiple models are fitted and tested for each gene.","Modify,Clarity",Clarity
7547,5-1356,5-1356_v2_30@4,5-1356_v1_37@4,"Additional challenges to be handled in tuQTL analyses include a large number of tests per gene with highly variable allele frequencies (models) and linkage disequilibrium, which can be accounted for in the multiple testing corrections.","Additional challenges to be handled in sQTL analyses include a large number of tests per gene with highly variable allele frequencies (models) and linkage disequilibrium, which can be accounted for in the multiple testing corrections.","Modify,Clarity",Clarity
7548,5-1356,5-1356_v2_32@1,5-1356_v1_39@1,"Following the edgeR strategy <REF-1> , <REF-2> , <REF-53> , we propose multiple approaches for dispersion estimation, all based on the maximization and adjustment of the profile likelihood, since standard maximum likelihood (ML) is known to produce biased estimates as it tends to underestimate variance parameters by not allowing for the fact that other unknown parameters are estimated from the same data <REF-54> , <REF-55> .","Following the edgeR ideology <REF-1> , <REF-2> , <REF-51> , we propose multiple approaches for dispersion estimation, all based on the maximization and adjustment of the profile likelihood, since standard maximum likelihood (ML) is known to produce biased estimates as it tends to underestimate variance parameters by not allowing for the fact that other unknown parameters are estimated from the same data <REF-52> , <REF-53> .","Modify,Clarity",Clarity
7549,5-1356,5-1356_v2_44@1,5-1356_v1_54@1,We performed simulations that correspond to a two-group comparison with no DTU (i.e. null model) where feature counts were generated from the DM distribution with identical parameters in both groups.,We performed simulations that correspond to a two-group comparison with no DS (i.e. null model) where feature counts were generated from the DM distribution with identical parameters in both groups.,"Modify,Clarity",Clarity
7550,5-1356,5-1356_v2_45@1,5-1356_v1_55@1,"Additionally, the median error of concentration estimates for Cox-Reid APL is always lower than for PL or maximum likelihood (ML) used in the dirmult package <REF-7> ( Figure 1C , Figure S2 ).","Additionally, the median error of concentration estimates for Cox-Reid adjusted profile likelihood is always lower than for PL or maximum likelihood (ML) used in the dirmult package <REF-7> ( Figure 1B , Figure S2 ).","Modify,Clarity",Clarity
7551,5-1356,5-1356_v2_47@2,5-1356_v1_59@2,"Thus, sharing information about concentration (dispersion) between genes by moderating the gene-wise APL is applied.","Thus, sharing information about concentration (dispersion) between genes by moderating to the gene-wise (adjusted) profile likelihood is applied.","Modify,Clarity",Clarity
7552,5-1356,5-1356_v2_51@0,5-1356_v1_62@0,"The aim of these analyses is to compare the performance of DRIMSeq against DEXSeq , which emerged among the top performing methods for detection of DTU from RNA-seq data <REF-23> .","The aim of these analyses is to compare the performance of DRIMSeq against DEXSeq , which emerged among the top performing methods for detection of DS from RNA-seq data <REF-22> .","Modify,Fact/Evidence",Fact/Evidence
7553,5-1356,5-1356_v2_51@1,5-1356_v1_62@1,"For DRIMSeq , we consider different dispersion estimates: common, gene-wise with no moderation and with moderation-to-common and to-trended dispersion.","Additionally, for DRIMSeq , we consider different dispersion estimates: common, gene-wise with no moderation and with moderation-to-common and to-trended dispersion.","Modify,Clarity",Clarity
7554,5-1356,5-1356_v2_55@0,5-1356_v1_65@0,"As noted by Soneson et al . <REF-23> , detecting DTU in human is harder than in fruit fly due to the more complex transcriptome of the first one; all methods have much smaller false discovery rate (FDR).","As noted by Soneson et al . <REF-22> , detecting DS in fruit fly is easier than in human; all methods have much smaller false discovery rate (FDR).","Modify,Fact/Evidence",Fact/Evidence
7555,5-1356,5-1356_v2_56@4,5-1356_v1_66@4,"Additionally, we have considered how other filtering approaches affect DTU detection.","Additionally, we have considered how other filtering approaches affect DS detection.","Modify,Fact/Evidence",Fact/Evidence
7556,5-1356,5-1356_v2_59@0,5-1356_v1_68@0,The p-value distributions highlight a better fit of the DM model to transcript counts compared to exonic counts (it is more uniform with a sharp peak close to zero).,The p-value distributions highlight a better fit of the DM model to transcript counts compared to exonic counts (it is uniform with a sharp peak close to zero).,"Modify,Clarity",Clarity
7557,5-1356,5-1356_v2_60@0,5-1356_v1_69@0,DS analyses on real datasets,DS analysis,"Modify,Other",Other
7558,5-1356,5-1356_v2_62@4,5-1356_v1_71@4,"To not diminish DEXSeq for its ability to fit more complex models, we run it using a model that does the four control versus three knock-down comparison with library layout as an additional covariate (model full 2).","To not diminish DEXSeq for its ability to fit GLMs, we run it using a model that does the four control versus three knock-down comparison with library layout as an additional covariate (model full glm).","Modify,Fact/Evidence",Fact/Evidence
7559,5-1356,5-1356_v2_62@5,5-1356_v1_71@5,"For the adenocarcinoma data, we do a two-group comparison of six normal versus six cancer samples (model full) and for DEXSeq , we fit an extra model that takes into account patient effects (model full 2).","For the adenocarcinoma data, we do a two-group comparison of six normal versus six cancer samples (model full) and for DEXSeq , we fit an extra model that takes into account patient effects (model full glm).","Modify,Fact/Evidence",Fact/Evidence
7560,5-1356,5-1356_v2_63@1,5-1356_v1_72@1,"Accounting for covariates in DEXSeq (model full 2) or performing the analysis on a subgroup without covariates (model full paired) results in more DS genes detected ( Figure S28 , Figure S29 and Figure S30 ).",Accounting for covariates in DEXSeq using the GLM or performing the analysis on a subgroup without covariates (model full paired) results in more DS genes detected ( Figure S26 and Figure S27 ).,"Modify,Fact/Evidence",Fact/Evidence
7561,5-1356,5-1356_v2_6@4,5-1356_v1_6@4,"Notably, the beta-binomial distribution, such as those used in differential methylation from bisulphite sequencing data <REF-13> – <REF-15> , represent a special case of the DM.","Notably, the beta-binomial distribution, such as those used in differential methylation from bisulphite sequencing data <REF-13> – <REF-15> , represents a special case of the DM.","Modify,Grammar",Grammar
7562,5-1356,5-1356_v2_2@2,5-1356_v1_2@2,There are situations where differences (e.g. between normal and disease state) in the relative ratio of expressed isoforms may have significant phenotypic consequences or lead to prognostic capabilities.,"There are situations where the total abundance of gene expression does not change (e.g. between normal and disease state), but differences in the relative ratio of expressed isoforms may have significant phenotypic consequences or lead to prognostic capabilities.","Modify,Claim",Claim
7563,5-1356,5-1356_v2_65@3,5-1356_v1_74@3,"In particular, the p-value distributions under the null indicate that DM fits better to transcript counts than exon counts ( Figure S14 , Figure S31 and Figure S32 ).",Also the distributions of p-values indicate that DM fits better to the transcript counts ( Figure S28 and Figure S29 ).,"Modify,Claim",Claim
7564,5-1356,5-1356_v2_66@2,5-1356_v1_75@2,"Of course, these validations represent an incomplete truth, and ideally, large-scale independent validation would be needed to comprehensively compare the DTU detection methods.","Of course, these validations represent an incomplete truth, and ideally, large-scale independent validation would be needed to comprehensively compare DS detection methods.","Modify,Claim",Claim
7565,5-1356,5-1356_v2_70@0,5-1356_v1_79@0,"To demonstrate the application of DRIMSeq to tuQTL analysis, we use the data from the GEUVADIS project <REF-46> where 465 RNA-seq samples from lymphoblastoid cell lines were sequenced, 422 of which were sequenced in the 1000 Genomes Project Phase 1.","To demonstrate the application of DRIMSeq to sQTL analysis, we use the data from the GEUVADIS project <REF-44> where 465 RNA-seq samples from lymphoblastoid cell lines were sequenced, 422 of which were sequenced in the 1000 Genomes Project Phase 1.","Modify,Fact/Evidence",Fact/Evidence
7566,5-1356,5-1356_v2_7@1,5-1356_v1_7@1,"Hence, gene expression can be viewed as a multivariate expression of transcripts or exons and such a representation allows the study of not only the overall gene expression, but also the expressed variant composition.","Hence, gene expression can be viewed as a multivariate expression of transcripts or exons, and such representation allows to study not only the overall gene expression, but how it is composed from different isoforms.","Modify,Clarity",Clarity
7567,5-1356,5-1356_v2_71@4,5-1356_v1_80@4,"The numbers of tested and associated genes and tuQTLs are indicated in Figure 4 , Figure S38 and Figure S39 .","The numbers of tested and associated genes and sQTLs are indicated in Figure 4 , Figure S35 and Figure S36 .","Modify,Fact/Evidence",Fact/Evidence
7568,5-1356,5-1356_v2_7@3,5-1356_v1_7@3,"Thus, biologists are interested in using RNA-seq data to discover differences in transcript usage between conditions or to study the specific molecular mechanisms that mediate these changes, for example, alternative splice site usage.","Thus, in addition to differential gene expression, biologists are interested in using RNA-seq data to discover changes in isoform usage between conditions; this is also referred to as differential splicing (DS).","Split+Modify,Claim",Claim
7569,5-1356,5-1356_v2_7@4,5-1356_v1_7@3,"In general terms, we collect all these together under the term “differential splicing” (DS) <REF-19> .","Thus, in addition to differential gene expression, biologists are interested in using RNA-seq data to discover changes in isoform usage between conditions; this is also referred to as differential splicing (DS).","Split+Modify,Fact/Evidence",Fact/Evidence
7570,5-1356,5-1356_v2_78@1,5-1356_v1_87@1,We have shown that this framework can be used for detecting differential isoform usage between experimental conditions as well as for identifying tuQTLs.,We have shown that this framework can be used for detecting differential isoform usage between experimental conditions as well as for identifying sQTLs.,"Modify,Clarity",Clarity
7571,5-1356,5-1356_v2_80@3,5-1356_v1_89@3,"Thus, we incorporate estimation techniques analogous to those used in negative binomial frameworks, such as Cox-Reid APL; perhaps not surprisingly, raw profile likelihood or standard maximum likelihood approaches do not perform as well in our tests of estimation performance.","Thus, we incorporate estimation techniques analogous to those used in negative binomial frameworks, such as Cox-Reid adjusted profile likelihood; perhaps not surprisingly, raw profile likelihood or standard maximum likelihood approaches do not perform as well in our tests of estimation performance.","Modify,Clarity",Clarity
7572,5-1356,5-1356_v2_81@0,5-1356_v1_90@0,"In comparison to other available methods, DRIMSeq seems to be more conservative than both DEXSeq (using transcript counts) and sQTLseekeR , identifying fewer DTU genes and tuQTLs, respectively.","In comparison to other available methods, DRIMSeq seems to be more conservative than both DEXSeq (using transcript counts) and sQTLseekeR , identifying fewer DS genes and sQTLs, respectively.","Modify,Clarity",Clarity
7573,5-1356,5-1356_v2_81@2,5-1356_v1_90@2,"Moreover, the sQTL associations detected by DRIMSeq have more enrichment in splicing-related features than sQTLseekeR tuQTLs, which could be due to the fact that DRIMSeq accounts for the higher uncertainty of lowly expressed genes by using transcript counts instead of transcript ratios.","Moreover, the sQTL associations detected by DRIMSeq have more enrichment in splicing-related features than sQTLseekeR sQTLs, which could be due to the fact that DRIMSeq accounts for the higher uncertainty of lowly expressed genes by using transcript counts instead of transcript ratios.","Modify,Clarity",Clarity
7574,5-1356,5-1356_v2_83@2,5-1356_v1_92@2,"In the tuQTL analysis, it would allow studying samples from the pooled populations, with the subpopulation as a covariate, allowing larger sample sizes and increased power to detect interesting changes.","In the sQTL analysis, it would allow studying samples from the pooled populations, with the subpopulation as a covariate, allowing larger sample sizes and increased power to detect interesting changes.","Modify,Fact/Evidence",Fact/Evidence
7575,5-1356,5-1356_v2_85@1,5-1356_v1_94@1,"In addition to the user friendly workflow for the DTU and tuQTL analyses, it provides plotting functions that generate diagnostic figures such as the dispersion versus mean gene expression figures and histograms of p-values.","In addition to the user friendly workflow for the DS and sQTL analyses, it provides plotting functions that generate diagnostic figures such as the dispersion versus mean gene expression figures and histograms of p-values.","Modify,Fact/Evidence",Fact/Evidence
7576,5-1356,5-1356_v2_90@0,5-1356_v1_101@0,Data for the tuQTL analyses was downloaded from the GEUVADIS project website.,Data for the sQTL analyses was downloaded from the GEUVADIS project website.,"Modify,Fact/Evidence",Fact/Evidence
7577,5-1356,5-1356_v2_93@0,5-1356_v1_104@0,DRIMSeq analyses for this paper were done with version 0.3.3 available on Zenodo https://zenodo.org/record/53084 <REF-61> and Bioconductor release 3.2.,DRIMSeq analyses for this paper were done with version 0.3.3 available on Zenodo http://dx.doi.org/10.5281/zenodo.53084 <REF-59> and Bioconductor release 3.2.,"Modify,Fact/Evidence",Fact/Evidence
7578,5-1356,5-1356_v2_93@1,5-1356_v1_104@1,Source code used for the analyses in this paper is available on Zenodo https://zenodo.org/record/167305 <REF-62> .,Source code used for the analyses in this paper is available on Zenodo http://dx.doi.org/10.5281/zenodo.53059 <REF-60> .,"Modify,Fact/Evidence",Fact/Evidence
7579,5-1356,5-1356_v2_10@0,5-1356_v1_10@0,"The Dirichlet-multinomial framework, implemented as a Bioconductor R package called DRIMSeq , is applicable to both differential transcript usage (DTU) analysis between conditions and transcript usage quantitative trait loci (tuQTL) analysis.","The Dirichlet-multinomial framework, implemented as a Bioconductor R package called DRIMSeq , is oriented for both DS analysis and sQTL analysis.","Modify,Claim",Claim
7580,5-1356,5-1356_v2_2@3,5-1356_v1_2@3,"Similarly, knowledge of single nucleotide polymorphisms (SNPs) that affect splicing, so-called splicing quantitative trait loci (sQTL) will help to characterize the effects of genetic variation on gene expression.","Similarly, knowledge of single nucleotide polymorphisms (SNPs) that affect splicing, so-called splicing quantitative trait loci (sQTL), will help to characterize the effects of genetic variation on gene expression.","Modify,Grammar",Grammar
7581,5-1356,5-1356_v2_13@0,5-1356_v1_13@0,"DS can be studied in three main ways: as differential transcript usage (DTU) or, in a more local context, as differential exon or exon junction usage (DEU) or as specific splicing events (e.g., exon skipping), and all have their advantages and disadvantages.","DS can be studied in three main ways: as differential isoform usage or, in a more local context, as differential exon or exon junction usage or as specific splicing events (e.g., exon skipping), and all have their advantages and disadvantages.","Modify,Claim",Claim
7582,5-1356,5-1356_v2_13@6,5-1356_v1_13@6,"This issue is captured in Altrans <REF-27> , which quantifies exon-links (exon junctions) or in MISO <REF-28> , rMATS <REF-29> , SUPPA <REF-30> and SGSeq <REF-31> , all of which calculate splicing event inclusion levels expressed as percentage spliced in (PSI).","This issue is captured in Altrans <REF-26> , which quantifies exon-links (exon junctions) or in MISO <REF-27> , rMATS <REF-28> and SUPPA <REF-29> , all of which calculate splicing event inclusion levels expressed as percentage spliced in (PSI).","Modify,Fact/Evidence",Fact/Evidence
7583,5-1356,5-1356_v2_13@9,5-1356_v1_13@9,"However, there are (hypothetical) instances where changes in splicing pattern may not be captured by exon-level quantifications (Figure 1A in the paper by Monlog et al. <REF-35> ).","However, there are (hypothetical) instances where changes in splicing pattern may not be captured by exon-level quantifications (Figure 1A in Monlog et al. <REF-33> ).","Modify,Clarity",Clarity
7584,5-1356,5-1356_v2_13@10,5-1356_v1_13@10,"Furthermore, detection of more complex transcript variations remains a challenge for exon junction or PSI methods (see Figure S5 in the paper by Ongen et al. <REF-27> ).","Furthermore, detection of more complex transcript variations remains a challenge for exon junction or PSI methods (see Figure S5 in Ongen et al. <REF-26> ).","Modify,Clarity",Clarity
7585,5-1356,5-1356_v2_13@11,5-1356_v1_13@11,"Soneson et al. <REF-23> considered counting which accommodates various types of local splicing events, such as exon paths traced out by paired reads, junction counts or events that correspond to combinations of isoforms; in general, the default exon-based counting resulted in strongest performance for DS gene detection.","Soneson et al . <REF-22> considered counting to accommodate various types of local splicing events, such as exon paths traced out by paired reads, junction counts or events that correspond to combinations of isoforms; in general, the default exon-based counting resulted in strongest performance for DS gene detection.","Modify,Clarity",Clarity
7713,5-1822,5-1822_v2_23@4,,Animals were pre-trained on the horizontal ladder prior to the commencement of the study.,,"Add,Fact/Evidence",Fact/Evidence
7714,5-1822,5-1822_v2_23@5,,"On the day of functional testing, the animals were allowed three practice runs prior to recording.",,"Add,Fact/Evidence",Fact/Evidence
7715,5-1822,5-1822_v2_23@10,,Animals were pre-trained on the beam prior to the commencement of the study.,,"Add,Fact/Evidence",Fact/Evidence
7716,5-1822,5-1822_v2_23@11,,"On the day of functional testing, the animals were allowed three practice runs along the beam prior to recording.",,"Add,Fact/Evidence",Fact/Evidence
7717,5-1822,5-1822_v2_35@2,,"If the slopes are found to be equal, the intercepts (elevations) are then compared.",,"Add,Fact/Evidence",Fact/Evidence
7718,5-1822,5-1822_v2_15@2,5-1822_v1_15@2,"A total of 72 rats were randomly assigned into four experimental groups; PcTx1-treated ( n =12), saline-treated ( n =16), uninjured/untreated controls ( n =6) or blood-spinal cord barrier integrity ( n =38).","A total of 34 rats were randomly assigned into three treatment groups; PcTx1-treated ( n =12), saline-treated ( n =16) or uninjured/untreated controls ( n =6).","Modify,Fact/Evidence",Fact/Evidence
7719,5-1822,5-1822_v2_19@2,5-1822_v1_19@2,The role of the pump was to slowly and steadily release PcTx1 (1.08 μg/h) subcutaneously to compensate for estimated renal and tissue losses of PcTx1 over the first 48 h period.,The role of the pump was to slowly release PcTx1 (1.08 μg/h) subcutaneously to compensate for renal losses and maintain a stable plasma concentration of the drug over a 48 h period.,"Modify,Fact/Evidence",Fact/Evidence
7720,5-1822,5-1822_v2_21@0,5-1822_v1_21@0,An inherent feature of all spinal contusion models is inter-animal variations in the size of the initial spinal cord lesions produced.,An inherent feature of all spinal contusion models is inter-animal variance in the size of the spinal lesions produced.,"Modify,Clarity",Clarity
7721,5-1822,5-1822_v2_23@6,5-1822_v1_23@4,Animals were then video recorded during three attempts at crossing the ladder and the footage analysed by a blinded observer to count the number of hind limb foot faults (foot slipping below the ladder between rungs).,Animals were video recorded during three attempts at crossing the ladder and the footage analysed by a blinded observer to count the number of hind limb foot faults (foot slipping below the ladder between rungs).,"Modify,Clarity",Clarity
7722,5-1822,5-1822_v2_23@13,5-1822_v1_23@9,"Each animal was video recorded when swimming in a tank of water (27–31°C) and the recordings analysed to determine if the animal used their hind limbs to swim, indicative of preservation of supra-spinal connections ( Magnuson et al., 2009 ; Saunders et al. , 1998 ; Smith et al. , 2006 ), and whether there were alternating hind limb movements.","The limb pattern during swimming in a tank of water (27–31°C) was also video recorded and analysed to determine if the animals used alternating hind limb movements during swimming, indicative of supra-spinal connections.","Modify,Fact/Evidence",Fact/Evidence
7723,5-1822,5-1822_v2_25@0,5-1822_v1_25@0,"At the end of each experimental period (24 h or 6 weeks post-injury) injured animals (together with age-matched controls) were terminally anaesthetized with an overdose of inhaled isoflurane (Lyppard Australia), the chest cavity opened and transcardially perfused with 50 ml heparinised (5 IU/ml) PBS (80 ml/min/kg) followed by 150 ml of 4% paraformaldehyde solution.",At the end of each experimental period (24 h or 6 weeks post-injury) injured animals (together with age-matched controls) were terminally anaesthetized with an overdose of inhaled isoflurane (Lyppard Australia) and transcardially perfused with 50 ml heparinised (5 IU/ml) PBS (80 ml/min/kg) followed by 150 ml of 4% paraformaldehyde solution.,"Modify,Fact/Evidence",Fact/Evidence
7724,5-1822,5-1822_v2_29@0,5-1822_v1_29@0,Specific tissue regions containing some of the white matter tracts involved in hind limb motor function were outlined and measured separately.,Specific tissue regions containing white matter tracts involved in hind limb motor function were outlined and measured separately.,"Modify,Clarity",Clarity
7725,5-1822,5-1822_v2_35@0,5-1822_v1_35@0,"The functional performance of PcTx1-treated and saline-treated animals was compared using two-way analysis of covariance (ANCOVA, Prism v6, Graphpad, San Diego, USA).",The functional performance of PcTx1-treated and saline-treated animals was compared using two-way analysis of covariance (ANCOVA).,"Modify,Fact/Evidence",Fact/Evidence
7726,5-1822,5-1822_v2_35@1,5-1822_v1_35@1,This method uses the General Linear Model approach and fits least squares linear regression lines to the raw data for individual animals then compares the slopes (correlations) of the regression lines.,This method fits least squares linear regression lines to the raw data for individual animals and then compares the slopes and intercepts of the regression lines.,"Modify,Fact/Evidence",Fact/Evidence
7727,5-1822,5-1822_v2_35@3,5-1822_v1_35@2,Differences in the initial injury severity between animals within each group introduces a covariate that makes a substantial contribution to the total observed variance in each treatment group.,Inter-animal differences in the initial injury severity (SIS scores) introduce a covariate that makes a substantial contribution to the total observed variance in each treatment group.,"Modify,Clarity",Clarity
7728,5-1822,5-1822_v2_37@0,5-1822_v1_37@0,"Blood-spinal cord barrier (BSCB) function at the lesion site was assessed between 2 h and 7 days post-SCI for different size permeability tracers ( n = 2 per tracer and time point) in a separate series of 38 untreated, injured rats.","Blood-spinal cord barrier (BSCB) function at the lesion site was assessed between 2 h and 7 days post-SCI for different size permeability tracers ( n = 2–3 per tracer) in a separate series of untreated, injured rats.","Modify,Fact/Evidence",Fact/Evidence
7729,5-1822,5-1822_v2_50@2,5-1822_v1_50@2,"Linear regression and ANCOVA revealed that the data were best described by two separate lines having the same slope, but with the PcTx1-treated group having a significantly higher elevation compared to the saline-treated group (F 1,9 =16.1324, p=0.003).","Linear regression and ANCOVA analysis revealed that the data were best described by two separate lines with the same slope, but with the PcTx1-treated group having a significantly higher elevation compared to the saline-treated group (p=0.002, n =6–10).","Modify,Fact/Evidence",Fact/Evidence
7730,5-1822,5-1822_v2_54@1,5-1822_v1_54@1,There was no observable differences between the two treatment groups in the pattern and style of swimming.,There was no observable difference between the two treatment groups in the swimming test (not shown).,"Modify,Clarity",Clarity
7731,5-1822,5-1822_v2_62@2,5-1822_v1_62@2,"ANCOVA revealed two separate lines with the same slope best described the data with the PcTx1-treated group having a significantly higher elevation ( Figure 5 ; F 1,8 =12.9908, p=0.0069).","ANCOVA revealed two separate lines with the same slope best described the data with the PcTx1-treated group having a significantly higher elevation ( Figure 5 ; p=0.003, n =6).","Modify,Fact/Evidence",Fact/Evidence
7732,5-1822,5-1822_v2_83@0,5-1822_v1_83@0,"All of the gene transcripts with significant changes (i.e. FC>2 or FC<-2) for the two injury sizes (SIS 2.5 and 2.75) were separated into their biological categories ( Figure 9 ) using the ‘Panther’ gene classification system ( Mi et al. , 2013 ).","All of the gene transcripts with significant changes (i.e. FC>2 or FC<-2) for the two injury sizes (SIS 2.5 and 2.75) were separated into their biological categories ( ) using the ‘Panther’ gene classification system ( Mi et al. , 2013 ).","Modify,Fact/Evidence",Fact/Evidence
7902,5-392,5-392_v2_16@0,5-392_v1_16@0,The mtDNA of the diplonemid Diplonema papillatum is composed of numerous free DNA circles ( Figure 1 ) with a total size of about 600 kb <REF-31> .,The mtDNA of the diplonemid Diplonema papillatum is composed of numerous free DNA circles ( Figure 1 ) with a total coding capacity of about 600 kb <REF-31> .,"Modify,Clarity",Clarity
7903,5-392,5-392_v2_16@1,5-392_v1_16@1,The circles fall into two classes that are distinguished by their size (6 or 7 kb) and by the sequence of their non-coding regions that makes up about 95% of the circle.,"The circles are distinguished by size and sequence of their non-coding regions into a 6.0 and 7.0 kb-long A and B class, respectively <REF-32> .","Link+Modify,Clarity",Clarity
7904,5-392,5-392_v2_16@1,5-392_v1_16@2,The circles fall into two classes that are distinguished by their size (6 or 7 kb) and by the sequence of their non-coding regions that makes up about 95% of the circle.,"Although the function of the non-coding region representing up to 95% of the circle is unknown, each circle also carries a single cassette composed of a gene module, flanked upstream and downstream by an invariable 50-bp region, which represents a fragment of a protein-coding gene <REF-31> – <REF-34> .","Link+Modify,Clarity",Clarity
7905,5-392,5-392_v2_16@2,5-392_v1_16@2,Each circle also carries a single cassette composed of a piece of a gene (gene module) that is flanked on both sides by a unique sequence on average 50 bp in length <REF-31> – <REF-34> .,"Although the function of the non-coding region representing up to 95% of the circle is unknown, each circle also carries a single cassette composed of a gene module, flanked upstream and downstream by an invariable 50-bp region, which represents a fragment of a protein-coding gene <REF-31> – <REF-34> .","Link+Modify,Fact/Evidence",Fact/Evidence
7906,5-392,5-392_v2_16@4,5-392_v1_16@4,"The small subunit (SSU) mito-rRNA has been identified only very recently because its sequence is extremely diverged, which has made its identification challenging <REF-34> , <REF-35> .","The absence of the small subunit (SSU) mito-rRNA is explained by its being diverged beyond recognition, as the LSU gene is fragmented and extremely diverged, which made its identification challenging <REF-34> .","Modify,Fact/Evidence",Fact/Evidence
7907,5-392,5-392_v2_19@2,5-392_v1_19@2,"In Diplonema , only a few insertions of blocks of uridines have been documented initially, but the recent comprehensive count amounts to ~200 <REF-31> , <REF-32> , <REF-35> .","In Diplonema , only a few insertions of blocks of uridines have been documented so far <REF-31> , <REF-32> .","Modify,Fact/Evidence",Fact/Evidence
7908,5-392,5-392_v2_22@0,5-392_v1_22@0,"The post-transcriptional processing is very different in Diplonema , where fragments of genes transcribed from individual DNA circles are trans -spliced <REF-31> , <REF-33> .","The post-transcriptional processing is very different in Diplonema , where fragments of genes transcribed from individual DNA circles are trans -spliced in a highly systematic 5′ to 3′ progression <REF-31> , <REF-33> .","Modify,Fact/Evidence",Fact/Evidence
7909,5-392,5-392_v2_25@0,5-392_v1_25@0,Why are mitochondrial genomes in Euglenozoa so diverse?,Why are mitochondrial genomes in Euglenozoa so diverse,"Modify,Grammar",Grammar
7910,5-392,5-392_v2_26@0,5-392_v1_26@0,"Soon after its discovery, RNA editing in kinetoplastids was explained as a remnant of the RNA world <REF-44> .","Soon after its discovery, RNA editing in kinetoplastids was explained as a remnant of the RNA world <REF-43> , but the absence of similar mechanisms in both sister clades <REF-33> , <REF-37> puts this scenario finally to rest.","Modify,Fact/Evidence",Fact/Evidence
7911,5-392,5-392_v2_26@3,5-392_v1_26@3,"The recent finding of a mt genome in Euglena <REF-38> implies that the irreversible scrambling originally implied for the mtDNA of the euglenozoan last common ancestor <REF-45> did happen at a later stage in evolution, probably in the predecessor of diplonemids and kinetoplastids.","The recent finding of a standard mt genome in Euglena <REF-37> implies that the irreversible scrambling originally implied for the mtDNA of the euglenozoan last common ancestor <REF-44> did happen at a later stage in evolution, probably in the predecessor of diplonemids and kinetoplastids.","Modify,Clarity",Clarity
7912,5-392,5-392_v2_26@4,5-392_v1_26@4,"Although despite the available sequence data the mutual relationships among the three euglenozoan lineages remain unresolved, we can predict, on the basis of their mt genomes and transcriptomes, that the mostly free-living photosynthetic euglenids constitute the earliest offshoot of the long euglenozoan branch.","Although despite the available sequence data, the mutual relationships among the three euglenozoan lineages remain unresolved, and we can predict, on the basis of their mt genomes and transcriptomes, that the mostly free-living photosynthetic euglenids constitute the earliest offshoot of the long euglenozoan branch.","Modify,Grammar",Grammar
7913,5-392,5-392_v2_8@1,5-392_v1_8@1,"Like all mitochondria of aerobic protists, this organelle contains mitochondrial DNA (mtDNA) <REF-7> .","As all mitochondria of aerobic protists, it contains mitochondrial DNA (mtDNA) <REF-7> .","Modify,Clarity",Clarity
7914,5-392,5-392_v2_10@0,5-392_v1_10@0,"Standard mt genomes are usually represented by a circular or linear DNA molecule encoding an average of fewer than two dozen genes ranging from 2 to 66 proteins in Chromera velia and Andalucia godoyi , respectively <REF-13> , <REF-14> .","Standard mt genomes are usually represented by a circular or linear DNA molecule encoding an average of fewer than two dozen genes ranging from 2 to 66 proteins in Chromera velia and Andalucia godoyi <REF-13> , <REF-14> .","Modify,Clarity",Clarity
7915,5-392,5-392_v2_10@2,5-392_v1_10@2,"In euglenids and diplonemids, mtDNA seems to be evenly distributed throughout the lumen of the organelle <REF-8> , <REF-15> , whereas in kinetoplastids, the picture is more complex ( Figure 2 ).","In euglenids and diplonemids, mtDNA seems to be evenly distributed throughout the lumen of the organelle <REF-8> , <REF-15> , and the picture is more complex in kinetoplastids ( Figure 2 ).","Modify,Clarity",Clarity
7916,5-392,5-392_v2_10@3,5-392_v1_10@3,"In the obligatory parasitic trypanosomatids mtDNA is invariably compacted into a single disk-shaped structure of concatenated DNA termed the kinetoplast DNA (kDNA), the free-living or commensalic bodonids have their kDNA distributed either evenly or in foci in the mt lumen <REF-16> .","Whereas in the obligatory parasitic trypanosomatids it is invariably compacted into a single disk-shaped structure of concatenated DNA termed the kinetoplast DNA (kDNA), the free-living or commensalic bodonids have their kDNA distributed either evenly or in foci in the mt lumen <REF-16> .","Modify,Clarity",Clarity
7917,5-392,5-392_v2_13@4,5-392_v1_13@4,"The conserved region carries 18 protein-coding genes, mostly subunits of respiratory complexes (complex I: nad1 , nad2 , nad3 , nad4 , nad5 , nad7 , nad8 , and nad9 ; complex III: cob ; complex IV: cox1 , cox2 , and cox3 ; complex V: atp6 ), one ribosomal protein ( rps12 ), small and large mito-rRNA genes ( 12S and 9S ), and four open reading frames of unknown function ( MURF2 , MURF5 , cr3 , and cr4 ) ( Figure 3 ) <REF-24> , <REF-25> .","The conserved region carries 18 protein-coding genes, mostly subunits of respiratory complexes (complex I: nad1 , nad2 , nad3 , nad4 , nad5 , nad7 , nad8 , and nad9 ; complex III: cob ; complex IV: cox1 , cox2 , and cox3 ; complex V: atp6 ), one ribosomal protein ( rpl12 ), small and large mito-rRNA genes ( 12S and 9S ), and four open reading frames of unknown function ( MURF2 , MURF5 , cr3 , and cr4 ) ( Figure 3 ) <REF-24> , <REF-25> .","Modify,Fact/Evidence",Fact/Evidence
8022,6-1302,6-1302_v2_97@0,,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2017 Rufflé F et al.,,"Add,Fact/Evidence",Fact/Evidence
8023,6-1302,6-1302_v2_98@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",,"Add,Fact/Evidence",Fact/Evidence
8024,6-1302,6-1302_v2_17@2,,"The RNA-Seq was performed using polyA-selection with the truSeq RNA Lib-Prep Kit (Illumina, San Diego, CA) adjusted with GATC specific procedure for strand specificity.",,"Add,Fact/Evidence",Fact/Evidence
8025,6-1302,6-1302_v2_20@10,6-1302_v1_20@10,"This classification resembles the one depicted in Gingeras, 2009 <REF-7> and can be summarized as follows:",This classification resembles the one depicted in <REF-7> and can be summarized as follows:,"Modify,Fact/Evidence",Fact/Evidence
8026,6-1302,6-1302_v2_37@2,6-1302_v1_37@2,All candidate fusion transcripts were validated using qPCR and Sanger sequencing except the class 3 overlap which requires systematic reconstruction of the fusion transcript for the design of primers.,All candidate fusion transcripts were validated using qPCR and Sanger sequencing except the class three overlap which requires systematic reconstruction of the fusion transcript for the design of primers.,"Modify,Grammar",Grammar
8027,6-1302,6-1302_v2_41@1,6-1302_v1_41@1,"1 µl of each cDNA sample (2ng/µl) was added to a 5 µl of reaction mix containing 3 µl of Master Mix (LightCycler ® 480 sybr green I Master, Roche Diagnostics, GERMANY) and 0.66 µM forward and reverse primers.","1 µl of each cDNA sample (2ng/µl) was added to a 5 µl of reaction mix containing 3 µl of Master Mix (LightCycler ® 480 sybr green I Master, Roche Diagnostics, GERMANY) and 0.33 µM forward and reverse primers.","Modify,Fact/Evidence",Fact/Evidence
8028,6-1302,6-1302_v2_41@8,6-1302_v1_41@8,"Ultimately, a melting curve analysis ranging from 60°C to 95°C was performed to control primer specificity.","Ultimately, a melting curve analysis ranging from 95°C to 60°C was performed to control primer specificity.","Modify,Clarity",Clarity
8029,6-1302,6-1302_v2_44@1,6-1302_v1_44@1,"A first step aging slide with the cytogenetic preparation was performed by immerging slides in 2xSSC solution (saline sodium citrate) for 30 minutes at 37°C, followed by dehydration in 3 baths of increasing ethanol concentration: 70%, 85% and 100%, each for 1 minute.","A first step aging slide with the cytogenetic preparation was performed by immerging slides in 2xSSC solution (saline sodium citrate) for 30 minutes at 37°C, followed by dehydration in 3 baths of increasing ethanol concentration: 70°, 85° and 100°, each for 1 minute.","Modify,Fact/Evidence",Fact/Evidence
8030,6-1302,6-1302_v2_54@5,6-1302_v1_54@5,"The CBFB-MYH11 and PML-RARA fusion transcripts expressed in the AML-inv16 and AML-t(15;17) samples were identified using both RNA-seq and qPCR analysis, confirming the reliability of RNA-seq and the Crac suite in this type of analysis.","The CBFB-MYH11 and PML-RAR fusion transcripts expressed in the AML-inv16 and AML-t(15;17) samples were identified using both RNA-seq and qPCR analysis, confirming the reliability of RNA-seq and the Crac suite in this type of analysis.","Modify,Clarity",Clarity
8031,6-1302,6-1302_v2_57@0,6-1302_v1_57@0,"Among the Class 1 chRNAs more frequently associated with genomic translocation, we identified 4 chRNAs associated with PML and RARA genes in patient OM110223 suffering from AML-t(15,17).","Among the Class 1 chRNAs more frequently associated with genomic translocation, we identified 4 chRNAs associated with PML and RAR genes in patient OM110223 suffering from AML-t(15,17).","Modify,Clarity",Clarity
8032,6-1302,6-1302_v2_8@2,6-1302_v1_8@2,"In some well-documented cases, gene fusions, in addition to contributing to neoplastic transformation, produce fusion RNA and proteins used as therapeutic targets (Mertens et al. , 2015 <REF-5> , Yoshihara et al. , 2015 <REF-6> and references therein).","In some well-documented cases, gene fusions, in addition to contributing to neoplastic transformation, produce fusion RNA and proteins used as therapeutic targets ( <REF-5> , <REF-6> and references therein).","Modify,Fact/Evidence",Fact/Evidence
8033,6-1302,6-1302_v2_65@0,6-1302_v1_65@0,New Class 1 PML-RARA variants,New Class 1 PML-RAR variants,"Modify,Clarity",Clarity
8034,6-1302,6-1302_v2_66@1,6-1302_v1_66@1,"Acute promyelocytic leukemia (APL) molecular diagnosis and minimal residual disease (MRD) monitoring are currently based on bcr1, bcr2 or bcr3 fusion transcript detection, depending on the DNA breakpoint <REF-21> , <REF-22> .","Acute promyelocytic leukemia (APL) molecular diagnosis and minimal residual disease (MDR) monitoring are currently based on bcr1, bcr2 or bcr3 fusion transcript detection, depending on the DNA breakpoint <REF-21> , <REF-22> .","Modify,Grammar",Grammar
8035,6-1302,6-1302_v2_82@0,6-1302_v1_82@0,"In order to validate our strategy on a large cohort, we analyzed a publicly available dataset of 125 AML and 17 normal CD34+ HSC RNA-seq using a tag search approach (see Materials and methods).","To further extend our analysis and in order to validate our strategy on a large cohort, we analyzed a publicly available dataset of 125 AML and 17 normal CD34+ HSC RNA-seq using a tag search approach (see Materials and methods).","Modify,Clarity",Clarity
8036,6-1302,6-1302_v2_83@0,6-1302_v1_83@0,"We identified four new types of tumor-specific chRNA; TRIM28-TRIM28, DHRS7B-TMEM11, PLXNB-BLRD1 and SLC16A3-METRNL, expressed in all AML groups ( Figure 5B ).","We identified four new types of tumor-specific chRNA; TRIM28-TRIM28, DHRS7B-TMEME11, PLXNB-BLRD1 and SLC16A3-METRNL, expressed in all AML groups ( Figure 5B ).","Modify,Fact/Evidence",Fact/Evidence
8037,6-1302,6-1302_v2_83@4,6-1302_v1_83@4,"FLT3, PML-RARA and CBFB-MYH11 are strong markers in AML, and also useful for prognostic and MRD monitoring.","FLT3, PML-RARA and CBFB-MYH11 are strong markers in AML, and also useful for prognostic and MDR monitoring.","Modify,Grammar",Grammar
8038,6-1302,6-1302_v2_88@0,6-1302_v1_88@0,"The use of RNA-seq to provide a detailed view of the transcriptome and to detect new RNA transcripts, opens up new opportunities for improving diagnosis and treatment of human diseases.","The use of RNA-seq to provide a detailed view of the transcriptome and to detect new RNA transcripts, opens up new opportunities for improving diagnosis and treatment of human disease.","Modify,Grammar",Grammar
8039,6-1302,6-1302_v2_88@2,6-1302_v1_88@2,"ChRNAs, also known as “fusion RNA” or “canonical chimeras” <REF-5> , <REF-26> , are already used in diagnosis, but many other chimeric fusion products generated by transcriptional mechanisms such as read-throughs, cis or trans-splicing <REF-5> , <REF-7> , <REF-9> , also have the potential to be used in diagnosis if correctly categorized.","ChRNAs, also known as “fusion RNA” or “canonical chimeras”( <REF-5> , <REF-26> ), are already used in diagnosis, but many other chimeric fusion products generated by transcriptional mechanisms such as read-throughs, cis or trans-splicing <REF-5> , <REF-7> , <REF-9> , also have the potential to be used in diagnosis if correctly categorized.","Modify,Grammar",Grammar
8040,6-1302,6-1302_v2_91@2,6-1302_v1_91@2,"We also identified novel PML-RARA isoforms, shorter than the isoforms currently used in diagnosis, which could again be used in patient follow-up.","We also identified novel PML-RAR isoforms, shorter than the isoforms currently used in diagnosis, which could again be used in patient follow-up.","Modify,Clarity",Clarity
8041,6-1302,6-1302_v2_91@5,6-1302_v1_91@5,"MRD and patient follow-up in APL is usually performed by PML-RARA transcript QPCR, and relapse is associated with an increase of bcr1, bcr2 or bcr3 fusion transcripts <REF-32> .","MRD and patient follow-up in APL is usually performed by PML-RAR transcript QPCR, and relapse is associated with an increase of bcr1, bcr2 or bcr3 fusion transcripts <REF-32> .","Modify,Clarity",Clarity
8042,6-1302,6-1302_v2_95@0,6-1302_v1_95@0,"Abbreviations: chRNA, chimeric RNA; AML, acute myeloid leukemia; NK, normal karyotype; UK, unknown karyotype; AK, abnormal karyotype; APL, Acute Promyelocytic Leukemia; PBMCs, peripheral blood mononuclear cells; Inv16, chromosome 16 inversion; t(15;17), translocation of chromosomes 15 and 17; qPCR, quantitative polymerase chain reaction; NONE, non-annotated region; LincRNA, Long intergenic noncoding RNAs; Bcr, break chromosomal region; FISH, Fluorescence in situ hybridization; ATRA, all-trans retinoic acid; VD, vitamin D; TGFβ, transforming growth factor beta; MRD, minimal residual disease.","Abbreviations: chRNA, chimeric RNA; AML, acute myeloid leukemia; NK, normal karyotype; UK, unknown karyotype; AK, abnormal karyotype; APL, Acute Promyelocytic Leukemia; PBMCs, peripheral blood mononuclear cells; Inv16, chromosome 16 inversion; t(15;17), translocation of chromosomes 15 and 17; qPCR, quantitative polymerase chain reaction; NONE, non-annotated region; LincRNA, Long intergenic noncoding RNAs; Bcr, break chromosomal region; FISH, Fluorescence in situ hybridization; ATRA, all-trans retinoic acid; VD, vitamin D; TGFβtransforming growth factor beta; MRD, minimal residual disease.","Modify,Grammar",Grammar
8043,6-1302,6-1302_v2_13@8,6-1302_v1_13@8,"For the latter, peripheral blood mononuclear cells were collected by ficoll-hypaque density gradient and cultured at a concentration of 1×10 6 /ml, with or without 0.1µM ATRA, for 3 days.","For the latter, peripheral blood mononuclear cells were collected by ficoll-hypaque density gradient and cultured at a concentration of 1×10 6 /ml, with or without 0,1µM ATRA, for 3 days.","Modify,Grammar",Grammar
8044,6-1302,6-1302_v2_13@13,6-1302_v1_13@13,"The chemical agents used for differentiation were 1µM all-trans retinoic acid (ATRA; Sigma-Aldrich, Gillingham, UK), or 0.1µM vitamin D (VD) associated with 500pg/ml transforming growth factor beta TGFβ (Promega Corporation, USA) for the NB4 cell line <REF-18> .","The chemical agents used for differentiation were 1µM all-trans retinoic acid (ATRA; Sigma-Aldrich, Gillingham, UK), or 0,1µM vitamin D (VD) associated with 500pg/ml transforming growth factor beta TGFβ (Promega Corporation, USA) for the NB4 cell line <REF-18> .","Modify,Grammar",Grammar
8045,6-1302,6-1302_v2_13@14,6-1302_v1_13@14,"For the U937 cell line, 0.1µM TTNPB associated with 1µM Targretin (LGD1069) and 0.1µM 1 alpha, 25 dihydroxyvitamin D3 (VD) were used.","For the U937 cell line, 0,1µM TTNPB associated with 1µM Targretin (LGD1069) and 0,1µM 1 alpha, 25 dihydroxyvitamin D3 (VD) were used.","Modify,Grammar",Grammar
8046,6-1302,6-1302_v2_13@15,6-1302_v1_13@15,"TTNPB, LGD1069 and VD were kindly provided by Dr Klaus (Hoffman-La Roche, Switzerland), JHiernaux (Glaxo-welcome Laboratories, France), and L Binderup (Leopharmaceutical products, Denmark), respectively.","TTNPB, LGD1069 and VD were kindly provided by Dr Klaus (Hoffman-La Roche,Switzerland), JHiernaux (Glaxo-welcome Laboratories, France), and L Binderup (Leopharmaceutical products,Denmark), respectively.","Modify,Grammar",Grammar
8346,6-577,6-577_v2_14@0,,UVR and temperature treatment of coral nubbins,,"Add,Other",Other
8347,6-577,6-577_v2_15@0,,Incubations were performed in 100 mL beakers containing one coral nubbin each and filled with 40 mL of 0.45 μm filtered seawater and continuously stirred using magnetic stirrers.,,"Add,Fact/Evidence",Fact/Evidence
8348,6-577,6-577_v2_15@1,,"High temperature or/and ultraviolet radiation (UVR) stresses (i.e. four environmental conditions: control (at 25°C and without UVR), thermal stress (30°C without UVR), UVR stress (25°C under UVR), thermal and UVR stresses (30°C and under UVR)) were applied to corals and ERK activation was monitored after 30 minutes of stress.",,"Add,Fact/Evidence",Fact/Evidence
8349,6-577,6-577_v2_15@2,,Thermal stress corresponded to an increase in temperature from the normal culture condition of 25°C to 30°C.,,"Add,Fact/Evidence",Fact/Evidence
8350,6-577,6-577_v2_15@3,,The UVR stress corresponded to an increase in UVR from 0 (HQI lamps in the culture conditions) to a radiation intensity of about 3 W.m −2 UVB and 30 W.m −2 UVA (Q-Panel UVA 340 lamps).,,"Add,Fact/Evidence",Fact/Evidence
8351,6-577,6-577_v2_15@4,,"At the end of the incubation, nubbins were frozen and kept at – 80°C prior to western blot analysis.",,"Add,Fact/Evidence",Fact/Evidence
8352,6-577,6-577_v2_31@2,,"The limited thickness of the animal tissue covering the skeleton and the very large surface of contact of both ectoderm and endoderm with the seawater render S. pistillata suitable for treatment with drugs directly diluted in the seawater as we previously showed ( Courtial et al. , 2017 ).",,"Add,Fact/Evidence",Fact/Evidence
8353,6-577,6-577_v2_32@0,,"To confirm that Spi-ERK activity can dynamically respond to changes in experimental conditions, we performed an induction experiment by modifying culture conditions of the corals.",,"Add,Fact/Evidence",Fact/Evidence
8354,6-577,6-577_v2_32@1,,"Courtial et al. (2017) showed that thermal and UVR stresses induced the formation of reactive oxygen species which are known to trigger ERK phosphorylation ( McCubrey et al. , 2006 ).",,"Add,Fact/Evidence",Fact/Evidence
8355,6-577,6-577_v2_32@2,,"ERK phosphorylation was enhanced in corals exposed to UVR, high temperature or a combination of both ( Figure 3 and Supplementary Figure S2 ).",,"Add,Fact/Evidence",Fact/Evidence
8356,6-577,6-577_v2_32@3,,These results confirm that the antibodies characterized herein can be used to monitor ERK activity in corals.,,"Add,Claim",Claim
8357,6-577,6-577_v2_47@2,,The portions of the images used in the main text are outlined.,,"Add,Fact/Evidence",Fact/Evidence
8358,6-577,6-577_v2_48@0,,Supplementary Figure S3.,,"Add,Other",Other
8359,6-577,6-577_v2_48@1,,Uncropped blot images for Figure 4 and supplementary replicates.,,"Add,Fact/Evidence",Fact/Evidence
8360,6-577,6-577_v2_48@2,,The portions of the images used in the main text are outlined.,,"Add,Fact/Evidence",Fact/Evidence
8361,6-577,6-577_v2_22@2,6-577_v1_20@2,The antibody directed against Thr202/Tyr204 di-phosphorylated active ERK was from Thermo Scientific Pierce (rabbit monoclonal; MA5-15174; batch no. OC1680806); the anti-ERK1/2 antibody was from Thermo Scientific Pierce (mouse monoclonal; MA5-15605; batch no. PH1895491).,The antibody directed against Thr2020/Tyr204 di-phosphorylated active ERK was from Thermo Scientific Pierce (rabbit monoclonal; MA5-15174; batch no. OC1680806); the anti-ERK1/2 antibody was from Thermo Scientific Pierce (mouse monoclonal; MA5-15605; batch no. PH1895491).,"Modify,Fact/Evidence",Fact/Evidence
8362,6-577,6-577_v2_22@3,6-577_v1_20@3,"After extensive washing (4×30 minutes) in PBS – 0.1% Tween 20, membranes were incubated for 2 hours at room temperature in the simultaneous presence of IRDye 680RD goat anti-mouse (925-68070) and IRDye 800CW goat anti-rabbit (925-32211; Li-COR Biotechnology GmbH, Bad Homburg, Germany) secondary antibodies, or with anti-mouse and anti-rabbit HRP-conjugated antibody.","After extensive washing in PBS – 0.1% Tween 20, membranes were incubated for 2 hours at room temperature in the simultaneous presence of IRDye 680RD goat anti-mouse (925-68070) and IRDye 800CW goat anti-rabbit (925-32211; Li-COR Biotechnology GmbH, Bad Homburg, Germany) secondary antibodies, or with anti-mouse and anti-rabbit HRP-conjugated antibody.","Modify,Fact/Evidence",Fact/Evidence
8363,6-577,6-577_v2_4@2,6-577_v1_4@2,"The ERK family is the most studied in mammals ( Boulton et al ., 1990 ; Dhillon et al ., 2007 ) because it is involved in meiosis, mitosis and post mitotic functions in differentiated cells, as well as in the oxidative stress response and wound healing ( Castellano et al. , 2014 ; Johnson & Lapadat, 2002 ; Matsubayashi et al ., 2004 ; Runchel et al ., 2011 ).","The ERK family is the most studied in mammals ( Boulton et al ., 1990 ; Dhillon et al ., 2007 ) because it is involved in meiosis, mitosis and post mitotic functions in differentiated cells, as well as in the oxidative stress response and wound healing ( Johnson & Lapadat, 2002 ; Matsubayashi et al ., 2004 ; Runchel et al ., 2011 ).","Modify,Fact/Evidence",Fact/Evidence
8364,6-577,6-577_v2_22@4,6-577_v1_20@4,"Another set of extensive rinsing (4×30 minutes) in PBS – 0.1% Tween 20 was performed before membranes were imaged with an Odyssey device (LI-COR Biosciences, Lincoln, Nebraska) to detect fluorescence and HRP activity using Millipore ECL.","Another set of extensive rinsing in PBS – 0.1% Tween 20 was performed before membranes were imaged with an Odyssey device (LI-COR Biosciences, Lincoln, Nebraska) to detect fluorescence and HRP activity using Millipore ECL.","Modify,Fact/Evidence",Fact/Evidence
8365,6-577,6-577_v2_25@0,6-577_v1_23@0,"In order to confirm the presence of an ERK ortholog in corals, the human protein sequence of ERK1 (NP_001035145) was compared to the transcriptome database of Stylophora pistillata using the BLAST software ( Altschul et al. , 1990 ; Karako-Lampert et al ., 2014 ).","In order to confirm the presence of an ERK ortholog in corals, the human protein sequence of ERK1 (NP_001035145) was compared to the transcriptome database of Stylophora pistillata using the BLAST software ( Altschul et al. , 1990 ; Liew et al ., 2014 ).","Modify,Fact/Evidence",Fact/Evidence
8366,6-577,6-577_v2_25@2,6-577_v1_23@2,"This sequence (hereafter referred to as Spi-ERK for S. pistillata ERK) is the only one that shows an homology as high as 81%, 80% and 78% with the protein sequences of the cnidarians Nematostella vectensis ERK (Nv-ERK; XP_001629498.1), Hydra vulgaris ERK (Hv-ERK; XP_002154499.3) and the human MAPK3/ERK1 (Hs-ERK1), respectively ( Figure 1 ) ( Krishna et al ., 2013 ; Putnam et al ., 2007 ).","This sequence (hereafter referred to as Spi-ERK for S. pistillata ERK) is the only one that shows an homology as high as 81%, 80% and 78% with the protein sequences of the cnidarians Nematostella vectensis ERK (Nv-ERK; XP_001629498.1), Hydra vulgaris ERK (Hv-ERK; XP_002154499.3) and the human MAPK8/JNK1 (Hs-ERK1), respectively ( Figure 1 ) ( Krishna et al ., 2013 ; Putnam et al ., 2007 ).","Modify,Fact/Evidence",Fact/Evidence
8367,6-577,6-577_v2_25@5,6-577_v1_23@5,"This result suggests that a single ortholog of ERK is present in these cnidarians, consistently with previous work where only one ERK ortholog was found ( Castellano et al. , 2014 ; Russo et al. , 2004 ) but as opposed to the two genes encoding ERKs in most mammalian genomes ( Ip & Davis, 1998 ).","This result suggests that a single ortholog of ERK is present in these cnidarians, as opposed to the two genes encoding ERKs in most mammalian genomes ( Ip & Davis, 1998 ).","Modify,Fact/Evidence",Fact/Evidence
8368,6-577,6-577_v2_25@7,6-577_v1_23@7,"Accordingly, we detected a single immune-reactive band with the total-ERK antibody by western blot on S. pistillata extracts ( Figure 2A and Supplementary Figure S1 ).","Accordingly, we detected a single immune-reactive band with the total-ERK antibody by western blot on S. pistillata extracts ( Figure 2A and Figure S1 ).","Modify,Clarity",Clarity
8369,6-577,6-577_v2_4@3,6-577_v1_4@3,"The ERK gene family is evolutionnarily conserved and is found in all eukaryotes, including yeasts, plants, vertebrates and invertebrates ( Chen et al ., 2001 ; Widmann et al ., 1999 ).","The ERK gene family is evolutionnarily conserved and is found in all eukaryotes, including yeasts, plants, vertebrates and anthozoans ( Chen et al ., 2001 ; Widmann et al ., 1999 ).","Modify,Fact/Evidence",Fact/Evidence
8370,6-577,6-577_v2_31@0,6-577_v1_29@0,"RNAi interference techniques are not available in coral, and the confirmation that the immune reactive bands observed here specifically correspond to ERK could not be obtained through this method.","RNAi interference techniques are not available in coral, and the confirmation that the immune reactive bands observed here specifically correspond to ERK could not be obtained through this medium.","Modify,Clarity",Clarity
8371,6-577,6-577_v2_31@4,6-577_v1_29@3,"When the inhibitor was added to the seawater, the intensity of the band detected by the anti-total ERK did not vary, while the intensity of the band detected with the anti-phosphorylated ERK antibody was significantly reduced ( Figure 2B and Supplementary Figure S1 ).","When the inhibitor was added to the seawater, the intensity of the band detected by the anti-total ERK did not vary, while the intensity of the band detected with the anti-phosphorylated ERK antibody was significantly reduced ( Figure 2B and Figure S1 ).","Modify,Clarity",Clarity
8372,6-577,6-577_v2_33@0,6-577_v1_30@0,"Finally, to assess the performance of these antibodies, we compared the signal obtained on S. pistillata and human fibroblasts protein extracts ( Figure 4 and Supplementary Figure S3 ).","To assess the performance of these antibodies, we compared the signal obtained on S. pistillata and human fibroblasts protein extracts ( Figure 3 and Figure S2 ).","Modify,Clarity",Clarity
8373,6-577,6-577_v2_46@0,6-577_v1_40@0,Supplementary Figure S1.,Figure S1.,"Modify,Other",Other
8374,6-577,6-577_v2_47@0,6-577_v1_41@0,Supplementary Figure S2.,Figure S2.,"Modify,Other",Other
8375,6-577,6-577_v2_47@1,6-577_v1_41@1,Uncropped blot images for Figure 3 and supplementary replicates.,Uncropped blot images for Figure 3 .,"Modify,Fact/Evidence",Fact/Evidence
8376,6-577,6-577_v2_5@2,6-577_v1_5@2,"According to the manufacturer’s instructions, the antibody used in this study and directed against the Thr202/Tyr204 di-phosphorylated active ERK (Thermo Scientific Pierce; MA5-15174) showed reactivity with fruit fly, human, mink, mouse, non-human primate, pig, rat and zebrafish.","According to the manufacturer’s instructions, the antibody used in this study and directed against the Thr2020/Tyr204 di-phosphorylated active ERK (Thermo Scientific Pierce; MA5-15174) showed reactivity with fruit fly, human, mink, mouse, non-human primate, pig, rat and zebrafish.","Modify,Fact/Evidence",Fact/Evidence
8377,6-577,6-577_v2_10@1,6-577_v1_10@1,"Two small nubbins (3–5 cm long) were cut off from each mother colony, and were allowed to heal for four weeks in 15 L open system tanks before the experiments. Corals were maintained in the same conditions as the mother colonies, i.e. at 25°C, under a photosynthetic active radiation of 200 µmol photon.m -2 .s -1 provided by 400 W metal halide lamps (HPIT, Philips) and were fed twice a week with Artemia salina nauplii .","Two small nubbins were cut from each mother colony, and were allowed to heal for four weeks in 15 L open system tanks before the experiments. Corals were maintained in the same conditions as the mother colonies, i.e. at 25°C, under a photosynthetic active radiation of 200 µmol photon.m -2 .s -1 provided by 400 W metal halide lamps (HPIT, Philips) and were fed twice a week with Artemia salina nauplii .","Modify,Fact/Evidence",Fact/Evidence
8378,6-577,6-577_v2_11@0,6-577_v1_11@0,"Immortalized skin fibroblasts (BJ-EHLT cells) were kindly provided by E. Gilson’s lab (IRCAN) and cultured in Dulbecco’s Modified Eagle’s Medium (Invitrogen, Villebon-sur-Yvette, France) supplemented with 10% heat-inactivated fetal calf serum (Dutscher, Brumath, France) at 37°C in an atmosphere of 5% CO 2 , as previously described ( Biroccio et al ., 2013 ).","Immortalized skin fibroblasts (BJ-EHLT cells) were kindly provided by E. Gilson’s lab (IRCAN) and cultured in Dulbecco's Modified Eagle's Medium (Invitrogen, Villebon-sur-Yvette, France) supplemented with 10% heat-inactivated fetal calf serum (Dutscher, Brumath, France) at 37°C in an atmosphere of 5% CO 2 , as previously described ( Biroccio et al ., 2013 ).","Modify,Grammar",Grammar
8379,6-577,6-577_v2_17@1,6-577_v1_15@1,"Briefly, nubbins were airbrushed in 1 mL Laemmli buffer (2% SDS, 10% glycerol, 50mM Tris HCL pH7), ( Laemmli, 1970 ) using an air-pick (5 bars) to remove the totality of the tissues surrounding the skeleton was removed from coral.","Briefly, coral tissue was removed from the skeleton in 1 mL Laemmli buffer (1.5 X, Laemmli, 1970 ) using an air-pick.","Modify,Fact/Evidence",Fact/Evidence
8434,7-1235,7-1235_v2_2@7,7-1235_v1_2@7,The code and processed data are open sourced and available on github and contains a tutorial built into the application for assisting users.,The code and processed data is open sourced and available on github and with a tutorial built into the application for assisting users.,"Modify,Clarity",Clarity
8435,7-1235,7-1235_v2_14@1,7-1235_v1_14@1,"The interactive plots are made using shiny (v1.1.0) and ggplot2 (v3.0.0). Plots can be downloaded as .png, .pdf, or .svg files. Data used to generate the individual plots can be downloaded as .csv files.","The interactive plots are made using shiny (v1.1.0) and ggplots2 (3.0.0). Plots can be downloaded as .png, .pdf, or .svg files. Data used to generate the individual plots can be downloaded as .csv files.","Modify,Clarity",Clarity
8436,7-1235,7-1235_v2_16@1,7-1235_v1_16@1,Kaplan-Meier curves are generated using the survival (v2.41-3) and the survminer (v0.4-1) R packages.,Kaplan-Meier curves are generated using the survival (v2.41-3) and the survminer (0.4-1) R packages.,"Modify,Clarity",Clarity
8437,7-1235,7-1235_v2_16@3,7-1235_v1_16@3,"Hazard ratios for two-group comparisons, either median or optimal cut-off, utilize the Cox proportional hazards regression model in the survival R package; with the reported hazard ratio comparing high versus low protein groups.","Hazard ratio for two-group comparisons, either median or optimal cut-off, utilize the Cox proportional hazards regression model in the survival R package; with the reported hazard ratio comparing high versus low protein groups.","Modify,Grammar",Grammar
8438,7-1235,7-1235_v2_4@0,7-1235_v1_4@0,Improving prognostic prediction and the identification of potential therapeutic targets is of particular interest to clinicians.,Improving prognostic predictions and the identification of potential therapeutic targets is of particular interest to clinicians.,"Modify,Grammar",Grammar
8439,7-1235,7-1235_v2_16@7,7-1235_v1_16@7,Clinical variables dependent on the cancer type selected can be used to filter patients into user-defined groupings.,"Clinical variables dependent on the cancer type selected, can be used to filter patients into user-defined groupings.","Modify,Grammar",Grammar
8440,7-1235,7-1235_v2_19@1,7-1235_v1_19@1,Hazard ratios and p-values are based on the Cox regression model.,Hazard ratios and P values are based on the Cox regression model.,"Modify,Grammar",Grammar
8441,7-1235,7-1235_v2_4@1,7-1235_v1_4@1,"Quantification of messenger RNA at a genome-wide level has proven valuable in the discovery of gene expression profiles, which can serve as biomarkers for clinical outcomes in cancer <REF-1> .","Quantification of messenger RNA levels at a genome-wide level has proven valuable in the discovery of gene expression profiles, which can serve as biomarkers for clinical outcomes in cancer <REF-1> .","Modify,Clarity",Clarity
8442,7-1235,7-1235_v2_21@0,7-1235_v1_21@0,"In order to demonstrate the functionality of TRGAted, we present a basic survival analysis examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.","In order to demonstrate the functionality of TRGAted, we present a basic survival analysis of examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.","Modify,Grammar",Grammar
8443,7-1235,7-1235_v2_22@3,7-1235_v1_22@3,"Samples can be divided into quartiles, tertiles, median or optimally for p-values based on the protein of interest ( Figure 2C ).","The division of samples is available into quartiles, tertiles, median or optimum based on the protein of interest ( Figure 2C ).","Modify,Clarity",Clarity
8444,7-1235,7-1235_v2_2@0,7-1235_v1_2@0,Reverse-phase protein arrays (RPPAs) are a highthroughput approach to protein quantification utilizing antibody-based micro-to-nano scale dot blot.,Reverse-phase protein arrays (RPPAs) are a highthroughput approach to protein quantification utilizing an antibody-based micro-to-nano scale dot blot.,"Modify,Grammar",Grammar
8445,7-1235,7-1235_v2_23@2,7-1235_v1_23@2,"Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low protein groups ( Figure 3B ).","Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low proportion groups ( Figure 3B ).","Modify,Fact/Evidence",Fact/Evidence
8446,7-1235,7-1235_v2_23@3,7-1235_v1_23@3,Protein labeling is adaptive for both the volcano plot and bar chart and will only label significant proteins (p-value ≤ 0.05).,Protein labeling is adaptive for both the volcano plot and bar chart and will only label significant proteins.,"Modify,Fact/Evidence",Fact/Evidence
8447,7-1235,7-1235_v2_26@1,7-1235_v1_26@1,"Here, RAD50 predicts poor survival in only five cancer types, prostate, adrenocortical, breast cancer, low-grade glioma, and head and neck cancers ( Figure 4A ).","Here, RAD50 predicts poor survival in only five cancer types, prostate, adrenocortical, breast cancer, low-grade glioma and head and neck cancers ( Figure 4A ).","Modify,Grammar",Grammar
8448,7-1235,7-1235_v2_4@3,7-1235_v1_4@3,The availability of protein-level quantifications for the TCGA cohort allows for more relevant clinical outcome predictions compared to mRNA levels.,The availability of protein-level quantification for the TCGA cohorts allow for more relevant clinical outcome predictions compared to mRNA levels.,"Modify,Grammar",Grammar
8449,7-1235,7-1235_v2_30@3,7-1235_v1_30@3,"Built on the R shiny framework, a literate code architecture, the code for TRGAted is annotated and easily modified from our GitHub repository.","Built on the R Shiny framework, a literate code architecture, the code for TRGAted is annotated and easily modified from our GitHub repository.","Modify,Grammar",Grammar
8450,7-1235,7-1235_v2_4@4,7-1235_v1_4@4,"Currently, TCGA-based applications provide entry-level analysis in correlational, differential, and survival modalities for the RPPA information.","Currently available applications provide entry-level analysis in correlational, differential, and survival modalities for the RPPA information.","Modify,Clarity",Clarity
8451,7-1235,7-1235_v2_2@1,7-1235_v1_2@1,"Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor and metastatic samples.","Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor or metastatic samples.","Modify,Grammar",Grammar
8452,7-1235,7-1235_v2_10@4,7-1235_v1_10@4,"Clinical and survival information for each cancer data set was downloaded from recent work by Liu, et al. <REF-5> .",Clinical and survival information for each cancer data set were downloaded from recently updated TCGA clinical data <REF-5> .,"Modify,Fact/Evidence",Fact/Evidence
8453,7-1235,7-1235_v2_2@2,7-1235_v1_2@2,Protein-level data has particular advantages in assessing putative prognostic or therapeutic targets in tumors.,This protein-level data has particular advantages in assessing putative prognostic or therapeutic targets in tumors.,"Modify,Grammar",Grammar
8454,7-1235,7-1235_v2_10@6,7-1235_v1_10@6,"Unlike other cancer types, metastatic samples were retained for skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.","Unlike other cancer types, metastatic samples were kept in the skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.","Modify,Clarity",Clarity
8455,7-1235,7-1235_v2_2@4,7-1235_v1_2@4,"We developed a cloud-based application, TRGAted to enable researchers to better examine patient survival based on single or multiple proteins across 31 cancer types in the TCGA.","We developed a cloud-based application, TRGAted to enable researchers to better examine survival based on single or multiple proteins across 31 cancer types in the TCGA.","Modify,Clarity",Clarity
8583,7-1299,,7-1299_v1_16@0,,Further support for this comes from the examination of plant NMD pathways.,"Delete,Claim",Claim
8584,7-1299,,7-1299_v1_25@1,,PNRC2 is a vertebrate-specific NMD factor <REF-33> .,"Delete,Fact/Evidence",Fact/Evidence
8585,7-1299,,7-1299_v1_25@3,,More forward screens and biochemical studies are likely to yield more species-specific factors.,"Delete,Claim",Claim
8586,7-1299,,7-1299_v1_29@0,,"In baker’s yeast, a downstream sequence element (DSE) was identified <REF-85> , <REF-86> .","Delete,Fact/Evidence",Fact/Evidence
8587,7-1299,,7-1299_v1_29@1,,"When this sequence is downstream of a stop codon, NMD is elicited, likely through the recruitment of an RNA binding protein <REF-85> , <REF-86> .","Delete,Fact/Evidence",Fact/Evidence
8588,7-1299,,7-1299_v1_29@2,,This mechanism is very similar to the way in which the EJC mode works in animals and plants.,"Delete,Claim",Claim
8589,7-1299,,7-1299_v1_29@3,,"The existence of DSEs in other species are possible, but to date, none have been identified.","Delete,Claim",Claim
8590,7-1299,,7-1299_v1_29@5,,Whether such a mechanism exists in other eukaryotes and what factors determine this remain to be seen.,"Delete,Claim",Claim
8591,7-1299,,7-1299_v1_35@2,,- 3) What precisely are the roles of SMG5-7 family members in lineages that have lost SMG1? Do their roles differ between species with type 2 (recent loss of SMG1) and type 3 (ancient loss of SMG1) NMD pathways?,"Delete,Claim",Claim
8592,7-1299,7-1299_v2_16@0,,Further support for a complex NMD pathway existing in the last eukaryotic common ancestor comes from the examination of plants.,,"Add,Claim",Claim
8593,7-1299,7-1299_v2_16@11,,"However, this does not explain why the putative phosphorylation sites are lost in many species <REF-45> .",,"Add,Fact/Evidence",Fact/Evidence
8594,7-1299,7-1299_v2_16@12,,One exciting possibility is that direct interactions between SMG5/6/7 family proteins is sufficient for NMD to be activated in some species (see below).,,"Add,Claim",Claim
8595,7-1299,7-1299_v2_17@12,,"It could be that RNA decay enzymes are recruited directly to UPF1, alternative mechanism to the phosphorylation-mediated recruitment <REF-61> , <REF-63> , <REF-64> .",,"Add,Fact/Evidence",Fact/Evidence
8596,7-1299,7-1299_v2_17@13,,"Recently, the yeast EBS1 and NMD4 proteins were found to interact directly with UPF1 during NMD <REF-61> .",,"Add,Fact/Evidence",Fact/Evidence
8597,7-1299,7-1299_v2_17@14,,"NMD4, like SMG6, contains a PIN domain <REF-61> .",,"Add,Fact/Evidence",Fact/Evidence
8598,7-1299,7-1299_v2_17@15,,"Transcripts responsive to the deletion of UPF1 also increased in deletions of EBS1 and NMD4, however, to a lesser extent <REF-61> .",,"Add,Fact/Evidence",Fact/Evidence
8599,7-1299,7-1299_v2_17@16,,"Interestingly, the importance of EBS1 and NMD4 became more pronounced when yeast cells expressed a truncated UPF1 <REF-61> ; when the truncated UPF1 was expressed alone, NMD efficiency was about 30% of wild-type, in contrast, when either EBS1 and NMD4 were deleted in the truncated UPF1 lines, NMD efficiency was close to zero <REF-61> .",,"Add,Fact/Evidence",Fact/Evidence
8600,7-1299,7-1299_v2_17@17,,This suggests that EBS1 and NMD4 become essential in NMD limiting conditions.,,"Add,Claim",Claim
8601,7-1299,7-1299_v2_17@18,,"This raises the possibility that in species lacking SMG1, the phosphorylation checkpoint of NMD is not required and SMG5/6/7 family proteins directly interact with UPF1 when at a PTC.",,"Add,Claim",Claim
8602,7-1299,7-1299_v2_17@19,,"SMG1 mutants in fruit flies have been found to have a lesser effect on NMD than the mutation of other NMD factors <REF-54> , <REF-55> .",,"Add,Fact/Evidence",Fact/Evidence
8603,7-1299,7-1299_v2_17@20,,"SMG5 was found to be essential for NMD <REF-28> , and when a mild disruption of SMG5 is introduced, mutations of SMG1 enhanced the severity of the NMD phenotype <REF-28> .",,"Add,Fact/Evidence",Fact/Evidence
8604,7-1299,7-1299_v2_17@21,,"This supports the notion that NMD can be activated without phosphorylation and that phosphorylation simply enhances decay under limiting conditions <REF-28> , <REF-65> .",,"Add,Fact/Evidence",Fact/Evidence
8605,7-1299,7-1299_v2_17@22,,"Interestingly, mammalian SMG6 has also been found to bind UPF1 independent of phosphorylation <REF-66> , <REF-67> , suggesting some level of conservation of phosphorylation-independent recruitment of decay factors in NMD.",,"Add,Claim",Claim
8606,7-1299,7-1299_v2_17@23,,"However, it is not clear why a phosphorylation checkpoint is needed for NMD in some organisms like mammals <REF-20> , <REF-52> and plants <REF-51> , <REF-53> , but likely not others such as yeast, but direct interaction seems likely to be the mechanism.",,"Add,Claim",Claim
8607,7-1299,7-1299_v2_22@5,,"TOR is the only other related kinase in A. thaliana , and is involved with the regulation of translation, although the phenotype of TOR knockdown lines do not appear to match those of NMD factors in A. thaliana <REF-71> .",,"Add,Fact/Evidence",Fact/Evidence
8608,7-1299,7-1299_v2_24@7,,"NMD factors do function in other pathways, for example, UPF1 is known to be involved with mammalian DNA replication <REF-75> .",,"Add,Fact/Evidence",Fact/Evidence
8609,7-1299,7-1299_v2_24@8,,"Although in mammals, some NMD transcripts only require a subset of NMD factors <REF-37> , <REF-38> , <REF-76> , these branches of the NMD pathway support the notion that a more reduced NMD pathway may exist.",,"Add,Claim",Claim
8610,7-1299,7-1299_v2_25@2,,"Protein-protein interaction studies in yeast have revealed the species specific factor NMD4 <REF-61> , <REF-77> .",,"Add,Fact/Evidence",Fact/Evidence
8611,7-1299,7-1299_v2_25@3,,Performing similar work in other species is likely to reveal more species/lineage specific factors.,,"Add,Claim",Claim
8612,7-1299,7-1299_v2_28@4,,Although some transcripts appear to be targeted due to their length independent of the polyA tail in yeast <REF-94> .,,"Add,Fact/Evidence",Fact/Evidence
8613,7-1299,7-1299_v2_28@9,,"In yeast, the RNA binding protein Pub1 binds to sequence elements and protects some uORF-containing transcripts from NMD <REF-98> .",,"Add,Fact/Evidence",Fact/Evidence
8614,7-1299,7-1299_v2_31@11,,NMD has been proposed as a general protection mechanism against RNA viruses and TE expansion <REF-109> .,,"Add,Fact/Evidence",Fact/Evidence
8615,7-1299,7-1299_v2_34@2,,"- 3) If phosphorylation of UPF1 represents a checkpoint in the activation of NMD, what explains the variability of the presence of this checkpoint between species?",,"Add,Claim",Claim
8616,7-1299,7-1299_v2_34@6,,"- 7) To identify what precisely determines the accumulation of UPF1 on some transcripts, and why this appears to be dependent UPF1 ATPase activity <REF-111> .",,"Add,Claim",Claim
8617,7-1299,7-1299_v2_34@7,,"- 8) What is the mechanism leading to NMD of uORF transcripts? Is it EJC mode, long 3’ UTR mode, both or neither? This will need to be done for each uORF transcript of interest.",,"Add,Claim",Claim
8618,7-1299,7-1299_v2_16@2,7-1299_v1_16@2,"Plants also have homologues of SMG5/6/7, known as SMG7 and SMG7-like <REF-50> , and SMG1 homologues <REF-45> , <REF-51> .","Plants also have homologues of SMG5-7, known as SMG7 and SMG7-like <REF-45> , and SMG1 homologues <REF-43> , <REF-44> .","Modify,Clarity",Clarity
8619,7-1299,7-1299_v2_16@10,7-1299_v1_16@10,"One possibility is that an alteriave kinase has replaced SMG1 and might even be ancestral and operational in many species, allowing for the loss of SMG1 <REF-51> .","Alternatively, different mechanisms have replaced SMG1 in each independent loss of SMG1, which might explain why some organisms have retained S/TQ richness within their UPF1 protein sequences while others have not <REF-43> , <REF-44> .","Modify,Fact/Evidence",Fact/Evidence
8620,7-1299,7-1299_v2_17@0,7-1299_v1_17@0,"The SMG5/6/7 family split and diversified in the animal lineage, with the acquisition of the PIN domain in SMG5 and SMG6 <REF-27> , <REF-57> , <REF-58> .","The SMG5-7 family split and diversified in the animal lineage, with the acquisition of the PIN domain in SMG5 and SMG6 <REF-26> , <REF-55> , <REF-56> .","Modify,Clarity",Clarity
8621,7-1299,7-1299_v2_17@2,7-1299_v1_17@2,The SMG5/6/7 family also have a role in regulating telomere length <REF-59> .,The SMG5-7 family also have a role in regulating telomere length <REF-57> .,"Modify,Clarity",Clarity
8622,7-1299,7-1299_v2_17@3,7-1299_v1_17@3,"SMG5/6/7 homologues in plants are known as SMG7, given they lack the PIN domain of SMG5 and SMG6 <REF-50> .","SMG5-7 homologues in plants are known as SMG7, given they lack the PIN domain of SMG5 and SMG6 <REF-45> .","Modify,Grammar",Grammar
8623,7-1299,7-1299_v2_17@4,7-1299_v1_17@4,"SMG5/6/7 family members of baker’s yeast, EBS1 and EST1, also lack the PIN domain <REF-60> .","SMG5-7 family members of baker’s yeast, EBS1 and ETS1, also lack the PIN domain <REF-48> .","Modify,Clarity",Clarity
8624,7-1299,7-1299_v2_17@5,7-1299_v1_17@5,"In baker’s yeast, EST1 is implicated in telomere regulation but not NMD, while a knockout of EBS1 reveals a mild NMD phenotype <REF-60> , <REF-61> .","In baker’s yeast, ETS1 is implicated in telomere regulation but not NMD, while a knockout of EBS1 reveals a mild NMD phenotype <REF-48> .","Modify,Grammar",Grammar
8625,7-1299,7-1299_v2_17@7,7-1299_v1_17@7,"The UPF1 of baker’s yeast is depleted of S/TQ dipeptides <REF-45> , which once phosphorylated by SMG1, normally act as binding site for SMG5/6/7 <REF-24> .","The UPF1 of baker’s yeast is depleted of S/TQ dipeptides <REF-43> , which once phosphorylated by SMG1, normally act as binding site for SMG5-7 <REF-23> .","Modify,Clarity",Clarity
8626,7-1299,7-1299_v2_17@9,7-1299_v1_17@9,"Tyrosine phosphorylation of UPF1 in baker’s yeast has been observed and appears to regulate the RNA helicase activity of UPF1 <REF-62> , although the role in NMD, if any, and kinase responsible is still unknown.",Tyrosine phosphorylation of UPF1 in baker’s yeast has been observed and appears to regulate the RNA helicase activity of UPF1 <REF-58> .,"Modify,Claim",Claim
8627,7-1299,7-1299_v2_17@27,7-1299_v1_17@18,"However, it is unclear if SMG6L directly interacts with UPF1 or if it is via phosporylation, but there is no SMG1 and classical phosphorylation sites on UPF1 <REF-9> .","However, it is unclear how it is recruited to UPF1 given the lack of SMG1 and classical phosphorylation sites on UPF1.","Modify,Fact/Evidence",Fact/Evidence
8628,7-1299,7-1299_v2_19@1,7-1299_v1_19@1,"Generally speaking, these can be split into four major types and a spread across many unrelated eukaryotic lineages ( Figure 2 and Figure 3 ):","Generally speaking, these can be split into four major types ( Figure 2 and Figure 3 ):","Modify,Claim",Claim
8629,7-1299,7-1299_v2_20@0,7-1299_v1_20@0,"- 1) Classical SMG1-dependent NMD (As exemplified by humans, worms, and moss)","- 1) Classical SMG1-dependent NMD (As exemplified by C. elegans , humans, and moss)","Modify,Fact/Evidence",Fact/Evidence
8630,7-1299,7-1299_v2_4@1,7-1299_v1_4@1,"Analysis of mutant screens and genetic diseases identified mutations that introduced nonsense mutations, but surprisingly, these premature termination codons (PTCs) lead to a reduction in mRNA stability <REF-1> , <REF-2> .","Early mutant screens identified mutations that introduced nonsense mutations, but surprisingly, these premature termination codons (PTCs) lead to a reduction in mRNA stability <REF-1> , <REF-2> .","Modify,Fact/Evidence",Fact/Evidence
8631,7-1299,7-1299_v2_21@2,7-1299_v1_21@2,It is possible that the NMD pathways of some species with a type 1 NMD pathway in appearance might better resemble type 2 NMD (recent SMG1-independent NMD).,It is possible that the NMD pathways of some species with a type 1 NMD pathway in appearance might better resemble type 2 NMD (Recent SMG1-independent NMD).,"Modify,Grammar",Grammar
8632,7-1299,7-1299_v2_22@1,7-1299_v1_22@1,"However, UPF1 still maintains the relatively high level of phosphorylatable S/TQ motifs <REF-45> , and phospho-UPF1 binding protein SMG7 <REF-8> , <REF-50> .","However, UPF1 still maintains the relatively high level of S/TQ dipeptide phosphorylation sites <REF-43> , and phospho-UPF1 binding protein SMG7 <REF-8> , <REF-45> .","Modify,Clarity",Clarity
8633,7-1299,7-1299_v2_23@1,7-1299_v1_23@1,"These ancient losses of SMG1 lead to an NMD pathway without SMG1, without SMG8 and SMG9 <REF-45> , with a UPF1 depleted in S/TQ dipeptides <REF-45> , but a potential role for SMG5/6/7 proteins <REF-9> , <REF-60> , <REF-61> .","These ancient losses of SMG1 lead to an NMD pathway without SMG1, without SMG8 and SMG9 <REF-43> , with UPF1 depleted in S/TQ dipeptides <REF-43> , and an unclear role for SMG5-7 proteins <REF-48> , <REF-59> .","Modify,Fact/Evidence",Fact/Evidence
8634,7-1299,7-1299_v2_23@2,7-1299_v1_23@2,"Future work (see below) will be needed to better understand the exact molecular role of SMG5/6/7 proteins in type 3 NMD pathways, and to understand how the NMD pathway functions without the SMG1 activating UPF1.","Future work (see below) will be needed to better understand the exact molecular role of SMG5-7 proteins in type 3 NMD pathways, and to understand how the NMD pathway functions without the SMG1 activating UPF1.","Modify,Clarity",Clarity
8635,7-1299,7-1299_v2_4@3,7-1299_v1_4@3,"This pathway was termed nonsense-mediated mRNA decay (NMD) and is now known to regulate hundreds to thousands of transcripts in plants, animals, fungi and ciliates <REF-4> – <REF-10> .","This pathway was termed nonsense-mediated mRNA decay (NMD) and is now known to regulate hundreds to thousands of transcripts in plants, animals and fungi <REF-4> – <REF-8> .","Modify,Fact/Evidence",Fact/Evidence
8636,7-1299,7-1299_v2_24@6,7-1299_v1_24@12,It is certainly possible that the presence of these factors do not represent a fully functional form of an NMD pathway and instead reflect the molecular reminance of a former NMD pathway whose factors have now been co-opted for other functions.,It is certainly possible that the presence of these factors do not represent a fully functional form of an NMD pathway and instead reflect the molecular reminance of a former NMD pathway whose factors have not been co-opted for other functions.,"Modify,Grammar",Grammar
8637,7-1299,7-1299_v2_27@6,7-1299_v1_27@6,"The EJC has been lost from baker’s yeast and so cannot have a role in its NMD pathway, but the EJC is involved in the fungi Neurospora crassa’s NMD pathway <REF-84> .","The EJC has been lost from baker’s yeast and so cannot have a role in its NMD pathway, but the EJC is involved in the fungi Neurospora crassa ’s NMD pathway <REF-71> .","Modify,Grammar",Grammar
8638,7-1299,7-1299_v2_28@5,7-1299_v1_28@4,"An alternative, but not mutually exclusive model posits that longer 3’ UTRs are able to recruit more UPF1 directly bound to the 3’ UTR <REF-95> .",An alternative model posits that longer 3’ UTRs are able to recruit more UPF1 directly bound to the 3’ UTR <REF-81> .,"Modify,Fact/Evidence",Fact/Evidence
8639,7-1299,7-1299_v2_28@6,7-1299_v1_28@5,"It has been found that UPF1 coats transcripts but translation displaces UPF1 from all regions, except the 3’ UTRs <REF-96> .","It has been found that UPF1 coats transcripts, but translation displaces UPF1 from all regions, except the 3’ UTRs <REF-82> .","Modify,Grammar",Grammar
8640,7-1299,7-1299_v2_28@7,7-1299_v1_28@6,This model suggests that a higher level of UPF1 binding increases the chances of NMD being triggered during the termination of translation; naturally long 3’ UTRs that are resistant to NMD have been observed to bind less UPF1 than susceptible long 3’ UTR transcripts <REF-95> .,This model suggests that a higher level of UPF1 binding increases the chances of NMD being triggered during the termination of translation; Naturally long 3’ UTRs that are resistant to NMD have been observed to bind less UPF1 than susceptible long 3’ UTR transcripts <REF-81> .,"Modify,Grammar",Grammar
8641,7-1299,7-1299_v2_28@8,7-1299_v1_28@7,In fact some naturally long 3’ UTR transcripts in mammals appear to be protected from NMD by various features such as a recently identified cis-sequence element in the TRAM1 gene <REF-97> or the many genes found to bind PTBP1 near the stop codon to prevent NMD <REF-46> .,"In fact, some naturally long 3’ UTR transcripts appear to be protected from NMD by various features such as a recently identified cis -sequence element in the TRAM1 gene <REF-83> or the many genes found to bind PTBP1 near the stop codon to prevent NMD <REF-84> .","Modify,Fact/Evidence",Fact/Evidence
8642,7-1299,7-1299_v2_29@1,7-1299_v1_30@1,"While the EJC mode has been identified in plants, fungi, and animals, suggesting an ancient origin, there are many eukaryotic lineages where it has not been characterized, or does not function <REF-9> , <REF-99> .","While the EJC mode has been identified in both plants and animals, suggesting an ancient origin, there are many eukaryotic lineages where it has not been characterized, or does not function <REF-59> , <REF-87> .","Modify,Fact/Evidence",Fact/Evidence
8643,7-1299,7-1299_v2_31@1,7-1299_v1_32@1,"It appears that a rather complex NMD pathway, belonging to the type 1 group, existed in the last eukaryotic common ancestor (see above).","It is clear that a rather complex NMD pathway, belonging to the type 1 group, existed in the last eukaryotic common ancestor (see above).","Modify,Clarity",Clarity
8644,7-1299,7-1299_v2_31@8,7-1299_v1_32@8,"Expansion of group II introns has been proposed to have driven the evolution of the spliceosome to enhance the splicing of these selfish elements <REF-105> , the nucleus evolved to physically separate the processes of transcription and translation and allow for intron removal before translation <REF-106> , and NMD evolved to degrade intron-retaining transcripts that escaped the nucleus <REF-106> , <REF-107> .","Expansion of group II introns has been proposed to have driven the evolution of the spliceosome to enhance the splicing of these selfish elements <REF-93> , the nucleus evolved to physically separate the processes of transcription and translation and allow for intron removal before translation <REF-94> , and NMD evolved to degrade intron-retaining transcripts that escaped the nucleus <REF-94> .","Modify,Fact/Evidence",Fact/Evidence
8645,7-1299,7-1299_v2_31@9,7-1299_v1_32@9,These adaptations ensure that transcripts with retained introns do not undergo multiple rounds of translation.,These adaptations ensure that retention of these efficiently-spliced introns would not be repeatedly translation.,"Modify,Clarity",Clarity
8646,7-1299,7-1299_v2_2@1,7-1299_v1_2@1,"In most eukaryotes, thousands of transcripts are degraded by NMD, including many important regulators of developmental and stress response pathways.","In most eukaryotes, thousands of transcripts are degraded by NMD, including many important regulators of development and stress response pathways.","Modify,Grammar",Grammar
8647,7-1299,7-1299_v2_31@14,7-1299_v1_32@13,"The presence of NMD may act as a buffer for novel introns with weak splice sites <REF-107> , <REF-110> .",The presence of NMD may act as a buffer for novel introns with weak splice sites <REF-96> .,"Modify,Fact/Evidence",Fact/Evidence
8648,7-1299,7-1299_v2_34@0,7-1299_v1_35@0,- 1) Why is SMG1 repeatedly lost in different lineages? Is there a backup mechanism to activate UPF1 and is this conserved between the lineages that have recently lost (eg A. thaliana ) and more anciently lost (eg baker’s yeast and T . thermophila ) SMG1? Or are there multiple SMG1 replacement mechanisms?,- 1) Why is SMG1 repeatedly lost in different lineages? Is there a backup mechanism to activate UPF1 and is this conserved between the lineages that have recently lost (eg A. thaliana ) and more anciently (eg baker’s yeast and T . thermophila ) SMG1? Or are there multiple SMG1 replacement mechanisms?,"Modify,Grammar",Grammar
8649,7-1299,7-1299_v2_34@1,7-1299_v1_35@1,- 2) What recruits the RNA degradation machinery to UPF1 when SMG1 is lost and S/TQ dipeptides are depleted? Does it rely on the direct interactions of SMG5/6/7 family proteins with UPF1 or another mechanism?,- 2) What recruits the RNA degradation machinery to UPF1 when SMG1 is lost and S/TQ dipeptides are depleted? Does this still depend on the SMG5-7 family and UPF1 phosphorylation?,"Modify,Claim",Claim
8650,7-1299,7-1299_v2_34@3,7-1299_v1_35@3,"- 4) Can the EJC mode of PTC recognition exist without the involvement of the EJC, potentially in T. thermophila ? If so, what is the molecular basis for this and does it exist in other species?","- 4) What is the molecular basis of an EJC mode of PTC recognition when the EJC is not involved, such as in T . thermophila ?","Modify,Claim",Claim
8651,7-1299,7-1299_v2_34@4,7-1299_v1_35@4,- 5) What is the precise mechanistic roles of UPF2/UPF3 in relation to EJC mode and non-EJC mode NMD pathways? How do UPF2/UPF3 get recruited to NMD targets independently of the EJC?,- 5) What is the precise mechanistic roles of UPF2/UPF3 in relation to EJC mode and non-EJC mode NMD pathways?,"Modify,Claim",Claim
8652,7-1299,7-1299_v2_37@1,7-1299_v1_38@1,"I propose that the classical (type 1) NMD involves UPF1-3, the UPF1-kinase SMG1 and the SMG5/6/7 family.","I propose that the classical (type 1) NMD involves UPF1-3, the UPF1-kinase SMG1 and the SMG5-7 family.","Modify,Clarity",Clarity
8653,7-1299,7-1299_v2_6@1,7-1299_v1_6@1,"These factors were named UP-frameshift (UPF) 1, 2 and 3 in baker's yeast and Suppressors with Morphological defects on Genitalia (SMG) 2, 3 and 4 in C. elegans .","These factors were named UP-frameshift (UPF) 1, 2 and 3 in baker's yeast and suppressors with morphological defects on genitalia (SMG) 2, 3 and 4 in C. elegans .","Modify,Grammar",Grammar
8654,7-1299,7-1299_v2_6@6,7-1299_v1_6@6,"From these early studies in C. elegans , the different NMD factors were defined by their role in the phosphorylation of UPF1.","Initially, NMD factors were defined by their role in the phosphorylation of UPF1.","Modify,Clarity",Clarity
8655,7-1299,7-1299_v2_6@8,7-1299_v1_6@8,SMG5/6/7 bind to phosphorylated UPF1 <REF-24> and are active in the dephosphorylation of UPF1 by recruiting the PP2A phosphatase <REF-25> – <REF-27> .,SMG5-7 bind to phosphorylated UPF1 <REF-23> and are active in the dephosphorylation of UPF1 by recruiting the PP2A phosphatase <REF-24> – <REF-26> .,"Modify,Fact/Evidence",Fact/Evidence
8656,7-1299,7-1299_v2_6@10,7-1299_v1_6@10,SMG5/6/7 have a central role in recruiting the degradation machinery to degrade the NMD target <REF-28> – <REF-31> ( Figure 1 ).,SMG5-7 have a central role in recruiting the degradation machinery to degrade the NMD target ( Figure 1 ).,"Modify,Fact/Evidence",Fact/Evidence
8657,7-1299,7-1299_v2_6@16,7-1299_v1_6@16,Many NMD targets are degraded by specific “branches” of the NMD pathway that do not require UPF2 <REF-37> or UPF3b <REF-38> in mammals.,Many NMD targets use “branches” of the NMD pathway that do not require UPF2 <REF-35> or UPF3b <REF-36> .,"Modify,Fact/Evidence",Fact/Evidence
8658,7-1299,7-1299_v2_6@17,7-1299_v1_6@17,"However, all branches do involve UPF1, highlighting its central importance to the NMD pathway.","However, all branches do involve UPF1, highlightings its central importance to the NMD pathway.","Modify,Grammar",Grammar
8659,7-1299,7-1299_v2_9@0,7-1299_v1_9@0,"Together these studies, mostly using animal systems, paint a picture where multiple factors (UPF2, UPF3, SMG1, SMG8, and SMG9) assist in the activation of UPF1, while other factors (SMG5/6/7) act to degrade an NMD target and dephosphorylate UPF1.","Together these studies, mostly using animal systems, paint a picture where multiple factors (UPF2, UPF3, SMG1, SMG8, and SMG9) assist in the activation of UPF1, while other factors (SMG5-7) act to degrade an NMD target and dephosphorylate UPF1.","Modify,Clarity",Clarity
8660,7-1299,7-1299_v2_2@5,7-1299_v1_2@5,"Here, I detail the factors involved in NMD, our current understanding of their interactions and how they have evolved.","Here, I outline the factors involved in NMD, our current understanding of their interactions and how they have evolved.","Modify,Clarity",Clarity
8661,7-1299,7-1299_v2_11@9,7-1299_v1_11@9,"However, it is worth noting that parasites are known to have reduced genomes relative to free-living relatives <REF-44> , and that the non-parasitic excavata Naegleria gruberi does harbor the additional NMD factors of SMG1 and SMG9 <REF-45> .","However, it is worth noting that parasites are known to have reduced genomes relative to free-living relatives <REF-42> , and that the excavata Naegleria gruberi does harbor the additional NMD factors of SMG1 and SMG9 <REF-43> .","Modify,Fact/Evidence",Fact/Evidence
8662,7-1306,,7-1306_v1_22@7,,The percent expression metric shows whether a reasonable fraction of cells expresses the gene.,"Delete,Fact/Evidence",Fact/Evidence
8663,7-1306,7-1306_v2_9@1,,Users may upload a single expression file and specify whether the rows represent genes and the columns represent cells or vice-versa.,,"Add,Fact/Evidence",Fact/Evidence
8664,7-1306,7-1306_v2_9@3,,"Additionally, this notebook supports the three-file 10X output format, allowing users to upload the matrix, genes, and barcodes files.",,"Add,Fact/Evidence",Fact/Evidence
8665,7-1306,7-1306_v2_9@4,,Any of those inputs can also be provided as .zip files.,,"Add,Fact/Evidence",Fact/Evidence
8666,7-1306,7-1306_v2_10@5,,"To use the mitochondrial gene filter, the user must supply their data with gene names in HGNC format with “MT-” prepended to each mitochondrial gene name.",,"Add,Fact/Evidence",Fact/Evidence
8667,7-1306,7-1306_v2_14@8,,"As there is debate in the field concerning the correctness of using regression on covariates such as percent mitochondrial reads ( Batson, 2018 ) we have made this step optional.",,"Add,Fact/Evidence",Fact/Evidence
8668,7-1306,7-1306_v2_14@12,,"We note that this notebook is a living, open source document and can be modified as the single cell community’s perspectives on best practices evolves.",,"Add,Claim",Claim
8669,7-1306,7-1306_v2_35@0,,We encourage users to perform analyses on their own data using this notebook.,,"Add,Claim",Claim
8670,7-1306,7-1306_v2_35@1,,We note that all the required libraries are already installed on the public GenePattern Notebook server at https://notebook.genepattern.org .,,"Add,Fact/Evidence",Fact/Evidence
8671,7-1306,7-1306_v2_35@2,,"This resource is freely available to the community and the analysis described in this notebook falls well within the per-account memory allocations (see the Scanpy authors’ benchmarking in Wolf et al ., 2018 ; Eulenberg et al ., 2017a ; Eulenberg et al. , 2017b ).",,"Add,Fact/Evidence",Fact/Evidence
8672,7-1306,7-1306_v2_35@3,,"To analyze larger datasets that exceed the per-user memory allocation on the public notebook server, users should deploy the open source GenePattern Notebook server using their own computational resources as described in Reich et al ., 2017 .",,"Add,Fact/Evidence",Fact/Evidence
8673,7-1306,7-1306_v2_35@4,,"The GenePattern Notebook server is available as the genepattern-notebook package through the pip ( https://pypi.org/project/genepattern-notebook/ ) or conda ( https://anaconda.org/genepattern/genepattern-notebook ) package managers, or as a Docker image ( https://hub.docker.com/r/genepattern/genepattern-notebook ).",,"Add,Fact/Evidence",Fact/Evidence
8674,7-1306,7-1306_v2_36@2,,"For example, future notebook releases may include quality control methods such as doublet detection ( McGinnis et al ., 2018 ) as well as visualization methods such as UMAP ( Becht et al ., 2019 ), which is growing in popularity in the single cell community.",,"Add,Claim",Claim
8675,7-1306,7-1306_v2_36@3,,"We also encourage advanced users to copy the notebook, add new approaches or features, and publish them as a community notebook in the GenePattern Notebook repository.",,"Add,Claim",Claim
8676,7-1306,,7-1306_v1_9@1,,Each row of the matrix should represent a gene and each column represents a cell.,"Delete,Fact/Evidence",Fact/Evidence
8677,7-1306,7-1306_v2_22@6,7-1306_v1_22@6,"Additional statistical information about each gene is provided in interactive plots, such as the log-fold change comparing the average expression of a gene in one cluster versus the average expression in all other cells, the percentage of cells within the cluster that express the gene, and the percentage of cells in other clusters that express the gene.","Additional statistical information about each gene is provided as an interactive table, such as the log-fold change comparing the average expression of a gene in one cluster versus the average expression in all other cells, the percentage of cells within the cluster that express the gene, and the percentage of cells in other clusters that express the gene.","Modify,Clarity",Clarity
8678,7-1306,7-1306_v2_26@1,7-1306_v1_28@1,First the data can be exported as a set of CSV (comma separated values) files suited for further independent analysis and data sharing.,"First, as a set of CSV (comma separated values) files suited for further independent analysis and data sharing.","Modify,Clarity",Clarity
8679,7-1306,7-1306_v2_26@3,7-1306_v1_28@3,"The data can also be exported as an H5AD file that can be re-imported into this notebook’s workflow, retaining the generated results.","Second, as an H5AD file that can be re-imported into this notebook’s workflow, retaining the generated results.","Modify,Clarity",Clarity
8680,7-1306,7-1306_v2_28@0,7-1306_v1_30@0,"To run this notebook, the user needs a GenePattern account or can create one on the GenePattern Notebook site .","To run the notebook, the user is required to have a GenePattern account that can be created on the GenePattern Notebook site .","Modify,Clarity",Clarity
8681,7-1306,7-1306_v2_36@4,7-1306_v1_35@2,"As the GenePattern Notebook user interface gains more features, the notebook will also be able to take advantage of these features.","As the GenePattern Notebook user interface gains more features, the notebook will also grow to take advantage of these features.","Modify,Clarity",Clarity
8682,7-1306,7-1306_v2_9@2,7-1306_v1_9@2,"Text files from read count quantification tools like HTSeq ( Anders et al ., 2015 ) and Kallisto ( Bray et al ., 2016 ) are supported as input.","Gene by cell matrices generated by the 10X Genomics Cell Ranger pipeline and flat text files from read count quantification tools like HTSeq ( Anders et al. , 2015 ) and kallisto ( Bray et al. , 2016 ) are supported as input.","Modify,Fact/Evidence",Fact/Evidence
8683,7-1306,7-1306_v2_10@2,7-1306_v1_10@2,A high percentage of mitochondrial genes indicates apoptotic or lysed cells.,"A high percentage of mitochondrial genes indicates the cell may have lysed before isolation, losing cytoplasmic RNA and retaining RNA enclosed in the mitochondria.","Split+Modify,Clarity",Clarity
8684,7-1306,7-1306_v2_10@3,7-1306_v1_10@2,These disrupted cells tend to lose cytoplasmic RNA and retain RNA enclosed in the mitochondria.,"A high percentage of mitochondrial genes indicates the cell may have lysed before isolation, losing cytoplasmic RNA and retaining RNA enclosed in the mitochondria.","Split+Modify,Clarity",Clarity
8685,7-1306,7-1306_v2_14@7,7-1306_v1_14@7,We also give users the option to remove sources of technical variation by performing linear regression on the total number of molecules detected and the percentage of reads mapped to mitochondrial genes.,"To remove sources of technical variation, linear regression is used to diminish the effects of the number of detected molecules and the percentage of counts mapped to mitochondrial genes.","Modify,Fact/Evidence",Fact/Evidence
8686,7-1306,7-1306_v2_14@11,7-1306_v1_14@10,A plot showing the percent variance explained of each principal component is then displayed so the user may choose a reasonable number of principal components for use in clustering ( Figure 2B ).,A plot showing the standard deviation of each principal component is then displayed so the user may choose a reasonable number of principal components for use in clustering ( Figure 2B ).,"Modify,Fact/Evidence",Fact/Evidence
8744,7-1891,,7-1891_v1_49@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).","Delete,Fact/Evidence",Fact/Evidence
8745,7-1891,7-1891_v2_12@2,,"In BKKBN 2017 shown that the realization of modern contraceptive prevalence rate such as use of female sterilization (MOW), male sterilization (MOP), pill, IUD, injection, implant KB (Implant) and condoms on 2017 is 57.6% of the target of 60.9% or achievement of 94.58%.",,"Add,Fact/Evidence",Fact/Evidence
8746,7-1891,7-1891_v2_12@3,,"Furthermore, the realization of unmet need which is defined as the percentage of married women who do not want to have more children but do not use contraception in 2017 is 17.5%, so that the achievement is 58.63%.",,"Add,Fact/Evidence",Fact/Evidence
8747,7-1891,7-1891_v2_12@4,,"Moreover, the percentage of active contraceptive participants using LARCs was 21.5% of the target of 21.7% or achievement of 99.07%.",,"Add,Fact/Evidence",Fact/Evidence
8748,7-1891,7-1891_v2_12@5,,"Additionally, the realization of contraceptive discontinuation rate in 2017 is 22.3% of the target of 25.3% or achievement of 88.14% <REF-14> .",,"Add,Fact/Evidence",Fact/Evidence
8749,7-1891,7-1891_v2_20@0,,The instrument used in this study was a questionnaire containing questions related to factors associated with the utilization of long-acting reversible contraceptives in the Pameungpeuk Rural Hospital work area.,,"Add,Fact/Evidence",Fact/Evidence
8750,7-1891,7-1891_v2_20@1,,"The questionnaire consisted of questions about the behavior of use LARCs methods, knowledge, beliefs, attitudes, exposure to information on LARCs, skills of health workers, support of partner, support of friends, support of health workers, and support of community leaders <REF-22> .",,"Add,Fact/Evidence",Fact/Evidence
8751,7-1891,7-1891_v2_43@5,,This study was done before the new National Health Insurance Scheme was rolled out and widely available.,,"Add,Fact/Evidence",Fact/Evidence
8752,7-1891,7-1891_v2_43@6,,"National health insurance has in recent years, overcome many of the cost barriers to contraceptive use, including use of LARCs.",,"Add,Claim",Claim
8753,7-1891,7-1891_v2_50@0,,Extended data,,"Add,Other",Other
8754,7-1891,7-1891_v2_51@0,,Extended data contains the questionnaire (Bahasa and English versions) and how data were measured: https://doi.org/10.6084/m9.figshare.9734561.v2 <REF-22> .,,"Add,Fact/Evidence",Fact/Evidence
8755,7-1891,7-1891_v2_2@5,7-1891_v1_2@5,The proportion of active users of LARCs in Pameungpeuk is very low (10.66%).,The proportion of active users of LARCs in Pameungpeuk is also very low (10.66%).,"Modify,Clarity",Clarity
8756,7-1891,7-1891_v2_28@3,7-1891_v1_27@3,Privacy and confidentiality of the clients’ information was observed through the use of data collection with coded identification numbers.,Privacy and confidentiality of the clients' information was observed through the use of data collection with coded identification numbers.,"Modify,Grammar",Grammar
8757,7-1891,7-1891_v2_2@6,7-1891_v1_2@6,This study aimed to analyze factors associated with the utilization of LARCs among family planning clients at the Pameungpeuk Rural Hospital.,This study therefore aimed to analyze factors associated with the utilization of LARCs among family planning clients at the Pameungpeuk Rural Hospital.,"Modify,Clarity",Clarity
8758,7-1891,7-1891_v2_3@2,7-1891_v1_3@2,We performed statistical analyses using chi-square test.,We performed statistical analyses using a chi-square test.,"Modify,Grammar",Grammar
8759,7-1891,7-1891_v2_9@1,7-1891_v1_9@1,"One of the Indonesian government’s efforts to reduce population growth and its associated problems, including high mortality rates, are family planning programs promoting contraceptive use, which educate patients and clients regarding family planning and contraception itself <REF-4> .","One of the Indonesian government's efforts to reduce population growth and its associated problems, including high mortality rates, are family planning programs promoting contraceptive use, which educate patients and clients regarding family planning and contraception itself <REF-4> .","Modify,Grammar",Grammar
8760,7-1891,7-1891_v2_2@2,7-1891_v1_2@2,"However, the rate of termination of the use of short-acting contraceptives by family planning clients was higher than other methods, therefore the use of short-acting contraceptives is less efficient than long acting reversible contraceptives (LARCs) for longer term spacing because it is easy to skip a treatment for economic or other reasons, which can result in unintended pregnancy.","However, the rate of termination of the use of short-acting contraceptives by family planning clients was higher than other methods, therefore the use of short-acting contraceptive is not effective enough for use.","Modify,Claim",Claim
8761,7-1891,7-1891_v2_13@3,7-1891_v1_13@3,"Some people have negative beliefs and misunderstanding about LARCs, additionally the lack of knowledge and skills of health workers can also influence access to the use of LARCs methods <REF-20> , <REF-21> .","Widespread of myths and misunderstandings about LARCs and the lack of knowledge and skills of health workers can also influence access to the use of LARCs methods <REF-19> , <REF-20> .","Modify,Clarity",Clarity
8762,7-1891,7-1891_v2_2@3,7-1891_v1_2@3,"Therefore, the National Family Planning Program in Indonesia is encouraging the use of LARCs to control population growth.","In anticipating the decreased use of short-acting contraceptives while also seeking to control population growth, the National Family Planning Program in Indonesia is encouraging the use of long-acting reversible contraceptives (LARCs).","Modify,Claim",Claim
8763,7-1891,7-1891_v2_19@0,7-1891_v1_19@0,An interviewer-administered questionnaire <REF-22> was used to collect data for this study.,An interviewer-administered questionnaire ( Supplementary File 1 ) was used to collect data for this study.,"Modify,Fact/Evidence",Fact/Evidence
8764,7-1891,7-1891_v2_22@0,7-1891_v1_21@0,The interviewer-administered questionnaire <REF-22> was conducted with consenting consecutive clients as they accessed family planning services until the desired sample size was achieved.,The interviewer-administered questionnaire ( Supplementary File 1 ) was conducted with consenting consecutive clients as they accessed family planning services until the desired sample size was achieved.,"Modify,Fact/Evidence",Fact/Evidence
9710,8-1204,8-1204_v2_11@5,,Similar findings may also be present in FSGS cases associated with COL4A variants.,,"Add,Claim",Claim
9711,8-1204,8-1204_v2_11@6,,One study of Chinese families with familial FSGS noted segmental thinning of the GBM similar to those which may be present with TBMN in four out of five families with a proven COL4A variant underlying their familial FSGS (electron microscopy in another family and a sporadic case with proven variants were both noted to be unremarkable) <REF-10> .,,"Add,Fact/Evidence",Fact/Evidence
9712,8-1204,8-1204_v2_11@7,,At present the sensitivity of these findings remain unknown.,,"Add,Claim",Claim
9713,8-1204,8-1204_v2_33@2,,This is in keeping with a study of Chinese FSGS families in which four patients with a demonstrated mutation in COL4A and familial FSGS all had findings suggestive of TBMN on electron microscopy <REF-10> .,,"Add,Fact/Evidence",Fact/Evidence
9714,8-1204,8-1204_v2_33@5,,The authors suggest that sequencing of these electron microscopy variants when found could be an extension of current research to expand our knowledge in this field.,,"Add,Claim",Claim
9715,8-1204,8-1204_v2_4@2,8-1204_v1_4@2,In two samples there were histological changes detected that might have suggested the underlying presence of a type IV collagen disorder.,In two samples there were histological changes detected that might have suggested the underlying presence of a collagen IV disorder.,"Modify,Clarity",Clarity
9716,8-1204,8-1204_v2_27@1,8-1204_v1_27@1,Two of these samples showed signs on electron microscopy that might be consistent with an underlying type IV collagen glomerular basement membrane disorder.,Two of these samples showed signs on electron microscopy that might be consistent with an underlying collagen IV glomerular basement membrane disorder.,"Modify,Clarity",Clarity
9717,8-1204,8-1204_v2_32@1,8-1204_v1_32@1,"Of those that did, two were reported to have characteristics that might be consistent with an underlying type IV collagen disorder.","Of those that did, two were reported to have characteristics that might be consistent with an underlying collagen IV disorder.","Modify,Clarity",Clarity
9718,8-1204,8-1204_v2_32@3,8-1204_v1_32@3,"The first sample, in which the patient had the tip variant of FSGS, was suggested to be consistent with TBMN whereas there was no pathological comment made about the second from a patient with FSGS NOS.",The first sample was suggested to be consistent with TBMN whereas there was no pathological comment made about the second.,"Modify,Fact/Evidence",Fact/Evidence
9719,8-1204,8-1204_v2_32@4,8-1204_v1_32@4,"FSGS (NOS) was the most common lesion described in this study, consistent with prior reports <REF-2> .","FSGS (NOS) was the most common lesion described in this study, which is consistent with prior reports <REF-2> .","Modify,Clarity",Clarity
9720,8-1204,8-1204_v2_32@5,8-1204_v1_32@5,"Notably, close to one in three cases of primary FSGS were not proceeding to electron microscopy despite an indication to do so and 1 in 20 cases within our cohort had structural changes that were consistent with an underlying type IV collagen variant.","Notably, close to one in three cases of primary FSGS were not proceeding to electron microscopy despite an indication to do so and 1 in 20 cases within our cohort had structural changes that were consistent with an underlying collagen IV variant.","Modify,Clarity",Clarity
9721,8-1204,8-1204_v2_33@0,8-1204_v1_33@0,"Curiously, neither of the two patients in whom electron microscopy was suggestive of an underlying type IV collagen disorder was noted to have haematuria on their urinalysis at the time of presentation.","Curiously, neither of the two patients in whom electron microscopy was suggestive of an underlying collagen IV disorder was noted to have haematuria on their urinalysis at the time of presentation.","Modify,Clarity",Clarity
9722,8-1204,8-1204_v2_33@3,8-1204_v1_33@2,Unfortunately due to the retrospective nature of this study we were unable to send any samples for immunostaining of type IV collagen.,"Unfortunately, due to the retrospective nature of this study we were unable to send any samples for immunostaining of collagen IV.","Modify,Clarity",Clarity
9723,8-1204,8-1204_v2_34@0,8-1204_v1_34@0,"There is an increasing body of evidence indicating that inheritable variants in COL4A may underlie a proportion of cases of FSGS, with up to 12.5% cases of autosomal dominant FSGS attributable to COL4A3 in some cohorts <REF-10> .","There is an increasing body of evidence indicating that inheritable variants in collagen IV genes may underlie a proportion of cases of FSGS, with up to 12.5% cases of autosomal dominant FSGS attributable to COL4A3 in some cohorts <REF-15> .","Modify,Clarity",Clarity
9724,8-1204,8-1204_v2_34@1,8-1204_v1_34@1,Not subjecting these renal biopsy samples to electron microscopy represents a potential gap in the investigation and subsequent management of such patients given they are much less likely to respond to immunosuppressive therapy <REF-16> which has otherwise been classically indicated ( Figure 2 ).,Not subjecting these renal biopsy samples to electron microscopy represents a potential gap in the investigation and subsequent management of such patients given they are much less likely to respond to immunosuppressive therapy <REF-16> which has otherwise been classically indicated.,"Modify,Fact/Evidence",Fact/Evidence
9725,8-1204,8-1204_v2_37@0,8-1204_v1_35@0,"This study was designed as a retrospective cohort study looking at the number of samples sent for electron microscopy, as well as any potential changes which might be consistent with a type IV collagen glomerular basement membrane disorder.","This study was designed as a retrospective cohort study looking at the number of samples sent for electron microscopy, as well as any potential changes which might be consistent with a collagen IV glomerular basement membrane disorder.","Modify,Clarity",Clarity
9726,8-1204,8-1204_v2_37@1,8-1204_v1_35@1,It is important to recognise that not all groups have found the characteristic changes associated with the type IV collagen disorders such as Alport’s Syndrome or TBMN on electron microscopy.,It is important to recognise that not all groups have found the characteristic changes associated with the collagen IV disorders such as Alport’s Syndrome or TBMN on electron microscopy.,"Modify,Clarity",Clarity
9727,8-1204,8-1204_v2_37@3,8-1204_v1_35@3,It is thus possible that a lack of classical findings for a type IV collagen glomerular basement membrane disorder may have accounted for the low number of those with GBM features on electron microscopy consistent with a type IV collagen disorder noted within our study.,It is thus possible that a lack of classical findings for a collagen IV glomerular basement membrane disorder may have accounted for the low number of those with GBM features on electron microscopy consistent with a collagen 4 disorder noted within our study.,"Modify,Clarity",Clarity
9728,8-1204,8-1204_v2_38@0,8-1204_v1_36@0,"The process by which variants within the COL4A genes might cause FSGS remains unclear, particularly given their clear association with Alport Syndrome and TBMN.","The process by which variants within the collagen IV genes might cause FSGS remains unclear, particularly given their clear association with Alport Syndrome and TBMN.","Modify,Clarity",Clarity
9729,8-1204,8-1204_v2_38@1,8-1204_v1_36@1,"One proposal is that the ultrastructural changes induced by the type IV collagen variants, perhaps under the influence of modifier genes such as laminin, result in impaired podocyte attachment to the glomerular basement membrane which leads to accelerated podocyte detachment, subsequent foot process effacement as a response to the increased shear stress induced by the denuded basement membrane and at a critical level of podocyte loss collapse of the capillary network with the appearance of the classical segmental sclerotic lesion <REF-2> , <REF-4> , <REF-19> .","One proposal is that the ultrastructural changes induced by the collagen IV variants, perhaps under the influence of modifier genes such as laminin, result in impaired podocyte attachment to the glomerular basement membrane which leads to accelerated podocyte detachment, subsequent foot process effacement as a response to the increased shear stress induced by the denuded basement membrane and at a critical level of podocyte loss collapse of the capillary network with the appearance of the classical segmental sclerotic lesion <REF-2> , <REF-4> , <REF-17> .","Modify,Clarity",Clarity
9730,8-1204,8-1204_v2_38@2,8-1204_v1_36@2,"It also remains unclear as to whether the changes of FSGS are a secondary process occurring in those with TBMN or whether the type IV collagen variants are capable of causing primary FSGS <REF-7> , <REF-20> .","It also remains unclear as to whether the changes of FSGS are a secondary process occurring in those with TBMN or whether the collagen 4 variants are capable of causing primary FSGS <REF-7> , <REF-18> .","Modify,Clarity",Clarity
9731,8-1204,8-1204_v2_38@3,8-1204_v1_36@3,"FSGS occurring as a secondary process to other basement membrane abnormalities may explain why immunosuppressive therapy has traditionally been less effective in inherited forms of FSGS, although there are case reports of the successful use of the calcineurin inhibitor cyclosporine for some patients harbouring inheritable type IV collagen disorders <REF-6> .","FSGS occurring as a secondary process to other basement membrane abnormalities may explain why immunosuppressive therapy has traditionally been less effective in inherited forms of FSGS, although there are case reports of the successful use of the calcineurin inhibitor cyclosporine for some patients harbouring inheritable collagen 4 disorders <REF-6> .","Modify,Clarity",Clarity
9732,8-1204,8-1204_v2_39@1,8-1204_v1_37@1,"This may have potentially led to inadvertently overlooking characteristic basement membrane abnormalities, which may suggest an underlying and heritable type IV collagen disorder.","This may have potentially led to inadvertently overlooking characteristic basement membrane abnormalities, which may suggest an underlying and heritable collagen IV disorder.","Modify,Clarity",Clarity
9733,8-1204,8-1204_v2_0@0,8-1204_v1_0@0,The use of electron microscopy in the diagnosis of focal segmental glomerulosclerosis: are current pathological techniques missing important abnormalities in the glomerular basement membrane?,An audit of electron microscopy in the diagnosis of focal segmental glomerulosclerosis: are current pathological techniques missing important abnormalities in the glomerular basement membrane?,"Modify,Other",Other
9734,8-1204,8-1204_v2_9@7,8-1204_v1_9@7,"Variants in COL4A3, COL4A4 and COL4A5 which encode the α3, α4 and α5 chains of type IV collagen respectively, the major constituent of the GBM previously linked to both Alport syndrome and thin basement membrane nephropathy (TBMN) may also underlie cases of FSGS <REF-4> .","Variants in COL4A3, COL4A4 and COL4A5 , which encode the α3, α4 and α5 chains of collagen IV, respectively, the major constituent of the GBM, previously linked to both Alport syndrome and thin basement membrane nephropathy (TBMN), may also underlie cases of FSGS <REF-4> .","Modify,Clarity",Clarity
9735,8-1204,8-1204_v2_10@0,8-1204_v1_10@0,"The relationship between the three renal conditions intertwined around variants in the COL4A genes, Alport syndrome, TBMN, and FSGS is complex and incompletely understood.","The relationship between the three renal conditions intertwined around variants in the collagen IV genes, Alport syndrome, TBMN, and FSGS is complex and incompletely understood.","Modify,Clarity",Clarity
9736,8-1204,8-1204_v2_10@1,8-1204_v1_10@1,"Whilst previously it was believed that patients heterozygous for variants in COL4A3 and COL4A4 would develop TBMN with persistent microscopic haematuria and an otherwise benign prognosis, this traditional thinking has been overturned by the discovery that some pedigrees with these variants will go on to develop significant proteinuria, FSGS lesions and the potential for progressive CKD or ESKD <REF-6> , <REF-7> .","Whilst previously it was believed that patients heterozygous for variants in COL4A3 and COL4A4 would develop TBMN with persistent microscopic haematuria and an otherwise benign prognosis, this traditional thinking has been overturned by the discovery that some pedigrees with these variants will go on to develop significant proteinuria, FSGS lesions and the potential for progressive CKD or ESRD <REF-6> , <REF-7> .","Modify,Fact/Evidence",Fact/Evidence
9737,8-1204,8-1204_v2_10@2,8-1204_v1_10@2,"More recently, targeted gene sequencing of adults thought to have primary FSGS or steroid resistant nephrotic syndrome (the paediatric equivalent) found that pathogenic or likely pathogenic variants in COL4A genes were present in up to 38% of families with familial FSGS, and 3% of those with sporadic FSGS <REF-3> , <REF-8> .","More recently, targeted gene sequencing of adults thought to have primary FSGS or steroid resistant nephrotic syndrome (the paediatric equivalent) found that pathogenic or likely pathogenic variants in collagen IV genes were present in up to 38% of families with familial FSGS, and 3% of those with sporadic FSGS <REF-3> , <REF-8> .","Modify,Clarity",Clarity
9738,8-1204,8-1204_v2_11@4,8-1204_v1_11@4,"The other potential renal lesions that may be caused by COL4A gene variants, Alport syndrome and TBMN, cause thinning, lamellation and fraying of the GBM and may also be associated with podocyte foot process effacement, all of which require electron microscopy to visualise <REF-4> .","The other potential renal lesions that may be caused by collagen IV gene variants, Alport syndrome and TBMN, cause thinning, lamellation and fraying of the GBM and may also be associated with podocyte foot process effacement, all of which require electron microscopy to visualise <REF-4> .","Modify,Clarity",Clarity
9739,8-1204,8-1204_v2_11@11,8-1204_v1_11@8,Given that the pathological diagnosis of FSGS does not routinely require electron microscopy it is therefore conceivable that GBM lesions potentially associated with an underlying type IV collagen variant may have been missed.,Given that the pathological diagnosis of FSGS does not routinely require electron microscopy it is therefore conceivable that GBM lesions potentially associated with an underlying collagen 4 variant may have been missed.,"Modify,Clarity",Clarity
9740,8-1204,8-1204_v2_11@12,8-1204_v1_11@9,This represents an opportunity to reflect on our prior clinical behaviour to see if our multidisciplinary diagnostic practice may require improvement.,This represents an opportunity to audit our prior clinical behaviour to see if our multidisciplinary diagnostic practice may require improvement.,"Modify,Clarity",Clarity
9741,8-1204,8-1204_v2_11@13,8-1204_v1_11@10,We conducted a retrospective cohort analysis of prior renal biopsy results in two tertiary centres to see how many of those that had been given a histological diagnosis of FSGS were sent for subsequent electron microscopy.,We conducted a retrospective audit of prior renal biopsy results in two tertiary centres to see how many of those that had been given a histological diagnosis of FSGS were sent for subsequent electron microscopy.,"Modify,Clarity",Clarity
9742,8-1204,8-1204_v2_14@1,8-1204_v1_14@1,"In addition, of those samples that were subjected to electron microscopy, we reviewed how many displayed evidence of a potential type IV collagen disorder.","In addition, of those samples that were subjected to electron microscopy, we reviewed how many displayed evidence of a potential collagen IV disorder.","Modify,Clarity",Clarity
9790,8-1681,8-1681_v2_8@7,,"However, theory suggests that the pRF model is scale-invariant, that is, the pRF parameters are estimated by correlation between predicted and observed time series and only the pRF shape matters to size estimation, not amplitude.",,"Add,Claim",Claim
9791,8-1681,8-1681_v2_31@5,,The same HRF function was used for all participants given that the effect of individualized HRF on pRF parameters is expected to be small <REF-25> .,,"Add,Fact/Evidence",Fact/Evidence
9792,8-1681,8-1681_v2_60@0,,"Firstly, it is worth noting that both scanners used were manufactured by the same vendor.",,"Add,Fact/Evidence",Fact/Evidence
9793,8-1681,8-1681_v2_60@1,,"While this means our results are not currently generalizable to other platforms, it removes manufacturer as an additional source of variance between sites.",,"Add,Claim",Claim
9794,8-1681,8-1681_v2_66@4,,"Of course, the skew we observed in pRF size is related to the sequential presentation of our visual stimuli (sweeping bars, a stimuli commonly used for pRF).",,"Add,Claim",Claim
9795,8-1681,8-1681_v2_66@5,,One could hypothesise that temporal filtering would not bias for pRF sizes if the location of stimuli used for retinotopic mapping was presented randomly.,,"Add,Claim",Claim
9796,8-1681,8-1681_v2_66@6,,"However, recent studies <REF-49> , <REF-50> have shown that pRF size estimates differ between random and ordered designs (irrespective of temporal filtering), probably due to additional factors.",,"Add,Fact/Evidence",Fact/Evidence
9797,8-1681,8-1681_v2_66@7,,The choice of mapping stimulus should therefore also be taken into consideration when comparing results across studies.,,"Add,Claim",Claim
9798,8-1681,8-1681_v2_70@0,,Underlying data,,"Add,Other",Other
9799,8-1681,8-1681_v2_74@0,,Extended data,,"Add,Other",Other
9800,8-1681,8-1681_v2_77@0,,"Mean population receptive field (pRF) size (A), median cortical magnification factor CMF (B) and response amplitude (beta) (C) binned into eccentricity bands for the London 1.5T site (red) and the Auckland 3T site (black).",,"Add,Fact/Evidence",Fact/Evidence
9801,8-1681,8-1681_v2_77@1,,Panels in columns show different visual regions.,,"Add,Fact/Evidence",Fact/Evidence
9802,8-1681,8-1681_v2_77@2,,Panels in rows show the three participants.,,"Add,Fact/Evidence",Fact/Evidence
9803,8-1681,8-1681_v2_77@3,,Error bars denote 95% confidence intervals based on bootstrapping.,,"Add,Fact/Evidence",Fact/Evidence
9804,8-1681,8-1681_v2_80@0,,"Median goodness-of-fit (A), normalized goodness-of-fit (B), and noise ceiling (C) binned into eccentricity bands for the London 1.5T site (red) and the Auckland 3T site (black).",,"Add,Fact/Evidence",Fact/Evidence
9805,8-1681,8-1681_v2_80@1,,Panels in columns show different visual regions.,,"Add,Fact/Evidence",Fact/Evidence
9806,8-1681,8-1681_v2_80@2,,Panels in rows show the three participants.,,"Add,Fact/Evidence",Fact/Evidence
9807,8-1681,8-1681_v2_80@3,,Error bars denote 95% confidence intervals based on bootstrapping.,,"Add,Fact/Evidence",Fact/Evidence
9808,8-1681,8-1681_v2_83@0,,"Mean population receptive field (pRF) size (A) and median goodness-of-fit (B) binned into eccentricity bands for the Auckland data without filtering (black), and low-pass filtering with kernel 1 s (blue) and 2 s (green).",,"Add,Fact/Evidence",Fact/Evidence
9809,8-1681,8-1681_v2_83@1,,Panels in columns show different visual regions.,,"Add,Fact/Evidence",Fact/Evidence
9810,8-1681,8-1681_v2_83@2,,Panels in rows show the three participants.,,"Add,Fact/Evidence",Fact/Evidence
9811,8-1681,8-1681_v2_83@3,,Error bars denote 95% confidence intervals based on bootstrapping.,,"Add,Fact/Evidence",Fact/Evidence
9812,8-1681,8-1681_v2_25@6,8-1681_v1_25@6,"At the Auckland site, the scan had a TE of 30 ms, flip angle of 62°, a multiband/slice acceleration factor of 3, an in-plane/parallel imaging acceleration factor of 2 and rBW was 1680 Hz/Px.","At the Auckland site, the scan had a TE of 30 ms, flip angle of 62 o , a multiband/slice acceleration factor of 3, an in-plane/parallel imaging acceleration factor of 2 and rBW was 1680 Hz/Px.","Modify,Grammar",Grammar
9813,8-1681,8-1681_v2_31@4,8-1681_v1_31@4,"Subsequently, the time series was convolved with a canonical hemodynamic response function (HRF) determined from previous empirical data <REF-6> and z-standardized.","Subsequently, the time series was convolved with a canonical hemodynamic response function determined from previous empirical data <REF-6> and z-standardized.","Modify,Clarity",Clarity
9814,8-1681,8-1681_v2_33@2,8-1681_v1_33@2,"We then calculated the Pearson correlation between these split time series, r obs’ , and used the Spearman-Brown prophecy formula <REF-42> , <REF-43> to determine, r obs , the expected reliability for the average of all six runs:","We then calculated the Pearson correlation between these split time series, r obs ’, and used the Spearman-Brown prophecy formula <REF-42> , <REF-43> to determine, r obs , the expected reliability for the average of all six runs:","Modify,Grammar",Grammar
9815,8-1681,8-1681_v2_45@5,8-1681_v1_45@5,"CMF curves were very comparable for the two sites ( Figure 3B ), except for somewhat greater CMF at 3T in V1 and V2 in the very central visual field of participants P2 and P3 <REF-46> (see supplementary figure 1A and B <REF-46> )) for individual participants’ pRF size and CMF respectively).","CMF curves were very comparable for the two sites ( Figure 3B ), except for somewhat greater CMF at 3T in V1 and V2 in the very central visual field of participants P2 and P3.","Modify,Fact/Evidence",Fact/Evidence
9816,8-1681,8-1681_v2_48@1,8-1681_v1_48@1,This showed that in V1 and V2 goodness-of-fit was notably greater for the Auckland 3T site than the London 1.5T site.,This showed that in V1 and V2 goodness-of-fit was notably greater for the Auckland 3T site than the London 1.5T site in all 3 participants.,"Modify,Clarity",Clarity
9817,8-1681,8-1681_v2_48@3,8-1681_v1_48@3,"In V3A, the model fits (see Figure 3C last column) at both sites were similar, but generally lower than in the other regions and with greater variability.","In V3A, the model fits at both were similar, but generally lower than in the other regions and with greater variability.","Modify,Fact/Evidence",Fact/Evidence
9818,8-1681,8-1681_v2_48@5,8-1681_v1_48@5,"When we normalized model fits relative to the noise ceiling, ρ o 2, the maximum goodness-of-fit that could theoretically be achieved given the data from each site, we found no systematic difference in goodness-of-fit between sites ( Figure 3D ).","When we normalized model fits relative to the noise ceiling, ρ o 2 , the maximum goodness-of-fit that could theoretically be achieved given the data from each site, we found no systematic difference in goodness-of-fit between sites ( Figure 3B ).","Modify,Grammar",Grammar
9819,8-1681,8-1681_v2_48@6,8-1681_v1_48@6,The curves mostly overlapped for P1 and P2 except in V1 where the London data outperformed the Auckland data (see Figure 2B ).,The curves mostly overlapped for P1 and P2 except in V1 where the London data outperformed the Auckland data.,"Modify,Fact/Evidence",Fact/Evidence
9820,8-1681,8-1681_v2_62@1,8-1681_v1_63@1,"In London, images were projected onto a screen and this necessitated focusing and scaling the projected image to be of the exact size.","In London, images were projected onto a screen and this necessitated focussing and scaling the projected image to be of the exact size.","Modify,Grammar",Grammar
9821,8-1681,8-1681_v2_66@2,8-1681_v1_67@2,"A slower stimulus design where each bar position is stimulated for 2–3 s, or a random instead of an ordered stimulus sequence, could probably counteract this problem but this would come at the expense of longer scanning durations.","A slower stimulus design where each bar position is stimulated for 2-3 s, or a random instead of an ordered stimulus sequence, could probably counteract this problem but this would come at the expense of longer scanning durations.","Modify,Grammar",Grammar
9822,8-1681,8-1681_v2_84@0,8-1681_v1_74@0,Data are available under the terms of the Creative Commons Zero “No rights reserved” data waiver (CC0 1.0 Public domain dedication).,"Data are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).","Modify,Grammar",Grammar
10018,8-1983,8-1983_v2_2@1,,"In the dLGE, metrics of sizes and numbers of neurospheres generated using this assay has not been completely characterized.",,"Add,Claim",Claim
10019,8-1983,8-1983_v2_2@3,,The advantage of this protocol is that no expensive and specialized equipment is needed for tissue isolation.,,"Add,Claim",Claim
10020,8-1983,,8-1983_v1_2@1,,The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,"Delete,Claim",Claim
10021,8-1983,8-1983_v2_8@5,,"Additionally, many protocols do not provide metrics on expected numbers and sizes of neurospheres generated.",,"Add,Claim",Claim
10022,8-1983,8-1983_v2_8@6,,"Thus, it is unclear whether researchers can generate sufficient numbers of neurospheres in a particular range of sizes.",,"Add,Claim",Claim
10023,8-1983,8-1983_v2_8@10,,"Furthermore, using our method neurospheres small appears as early as 3 days.",,"Add,Claim",Claim
10024,8-1983,8-1983_v2_8@11,,Another advantage of this protocol is that it can generate neurospheres with average sizes of 50 μm - 100 μm after 5–7 days in vitro .,,"Add,Claim",Claim
10025,8-1983,,8-1983_v1_8@8,,Our method also provides an approximation of the expected sizes for neurospheres after a week in vitro .,"Delete,Fact/Evidence",Fact/Evidence
10026,8-1983,8-1983_v2_19@2,,The optimum magnification is approximately 5x with 3888 x 2592 dimensions.,,"Add,Fact/Evidence",Fact/Evidence
10027,8-1983,8-1983_v2_19@3,,"Camera was connected to the trinocular port of the stereomicroscope (Carl Zeiss Stemi 305; White Plains, NY) using Mount Adaptor EF-EOS (6098B007AA; Canon; Melville, NY).",,"Add,Fact/Evidence",Fact/Evidence
10028,8-1983,8-1983_v2_19@4,,The working distance was defined as the amount of room required between the top of the neurosphere and the bottom of the objective lens in order for the image to be in focus.,,"Add,Fact/Evidence",Fact/Evidence
10029,8-1983,8-1983_v2_19@5,,The steromicroscope was used at a working distance of ~110 mm.,,"Add,Fact/Evidence",Fact/Evidence
10030,8-1983,8-1983_v2_19@6,,"Due to the variation in neurosphere size, 110 mm should be adjusted to focus on the desire region of the neurosphere to provide optimal focus.",,"Add,Fact/Evidence",Fact/Evidence
10031,8-1983,8-1983_v2_27@1,,Sterilize all surgical instruments packed in aluminum foil in an autoclave at 121°C (15 psi) for 30 mins.,,"Add,Fact/Evidence",Fact/Evidence
10032,8-1983,8-1983_v2_27@2,,"This includes a scissors, forceps, and razor blades.",,"Add,Fact/Evidence",Fact/Evidence
10033,8-1983,8-1983_v2_27@3,,Before starting all premade solutions should be warmed to 37°C.,,"Add,Fact/Evidence",Fact/Evidence
10034,8-1983,8-1983_v2_37@1,,"Have a petri dish (100 mm) prepared with ice-cold Hank’s buffer kept on ice, which will be used to collect embryos after dissection.",,"Add,Fact/Evidence",Fact/Evidence
10035,8-1983,8-1983_v2_37@2,,"Afterwards, additional petri dishes will be needed to place in each of the dissected brains (35 mm) .",,"Add,Fact/Evidence",Fact/Evidence
10036,8-1983,8-1983_v2_46@1,,"In this section you will need the 18-gauge, 21-gauge, 23-gauge needle will be needed for trituration steps.",,"Add,Fact/Evidence",Fact/Evidence
10037,8-1983,8-1983_v2_46@2,,Trituration should be performed gently and slowly to avoid killing cells.,,"Add,Claim",Claim
10038,8-1983,8-1983_v2_46@3,,Hemocytometer will be needed to count cells .,,"Add,Fact/Evidence",Fact/Evidence
10039,8-1983,8-1983_v2_79@2,,Additional neurosphere staining using other antibodies can be found in previous published studies <REF-3> from our lab.,,"Add,Fact/Evidence",Fact/Evidence
10040,8-1983,8-1983_v2_83@4,,Another marker of an unhealthy culture is a large number of differentiated neurons surrounding neurospheres.,,"Add,Claim",Claim
10041,8-1983,8-1983_v2_83@5,,Indicators of differentiation are the large presence of axons and dendrites in your cultures.,,"Add,Claim",Claim
10042,8-1983,8-1983_v2_83@6,,This can be caused by depletion of growth factors.,,"Add,Claim",Claim
10043,8-1983,8-1983_v2_83@7,,"If this is the case, it is recommend that you increase the concentration of EGF.",,"Add,Claim",Claim
10044,8-1983,8-1983_v2_83@8,,Another cause of differentiation is too many cells in your prep.,,"Add,Claim",Claim
10045,8-1983,8-1983_v2_83@9,,This leads to over crowdedness.,,"Add,Claim",Claim
10046,8-1983,8-1983_v2_83@10,,It is recommend to split the culture to a lower density or decrease the number of neurons that are plated per 48 well.,,"Add,Claim",Claim
10047,8-1983,8-1983_v2_84@2,,"After trituration, if a uniform suspension has not been achieved an alternative method used in previous protocols are strainers <REF-15> .",,"Add,Fact/Evidence",Fact/Evidence
10048,8-1983,8-1983_v2_4@1,8-1983_v1_4@1,"Furthermore, we observed that this method yields an average range of neurospheres sizes greater than 50 μm, but less than 100 μm after 7 DIV.","Furthermore, we found this method yields different sizes of neurospheres.","Modify,Fact/Evidence",Fact/Evidence
10049,8-1983,8-1983_v2_4@2,8-1983_v1_4@2,"Lastly, using an anti-GFAP antibody, we show that these neurospheres can be stained, confirming their use in future immunocytochemistry studies.","Lastly, using an anti-GFAP antibody, we confirm that these neurospheres can be used for immunocytochemistry studies.","Modify,Clarity",Clarity
10050,8-1983,8-1983_v2_75@2,8-1983_v1_73@2,"Using similar analyses, we found significant differences between the size classification of neurospheres that were less than 50 μm, between 50–100 μm, and greater than 100 μm (F (2,379) = 424; p < 0.0001) ( Figure 3B ).","Using similar analyses, we found significant differences between the size classification of neurospheres that were less than 50 μm, between 50-100 μm, and greater than 100 μm (F (2,379) = 424; p < 0.0001) ( Figure 3B ).","Modify,Grammar",Grammar
10051,8-1983,8-1983_v2_75@3,8-1983_v1_73@3,Post hoc analysis using Bonferroni’s multiple comparison revealed a significant difference between the primary neurospheres that were greater than 100 μm compared to neurospheres that were less than 50 μm (p <0.0001) or between 50–100 μm (p <0.0001) ( Figure 3B ).,Post hoc analysis using Bonferroni’s multiple comparison revealed a significant difference between the primary neurospheres that were greater than 100 μm compared to neurospheres that were less than 50 μm (p <0.0001) or between 50-100 μm (p <0.0001) ( Figure 3B ).,"Modify,Grammar",Grammar
10052,8-1983,8-1983_v2_75@4,8-1983_v1_73@4,"Similarly, we found a substantial difference between primary neurospheres that were greater than 100 μm compared to neurospheres that were between 50–100 μm (p <0.0001) ( Figure 3B ).","Similarly, we found a substantial difference between primary neurospheres that were greater than 100 μm compared to neurospheres that were between 50 -100 μm (p <0.0001) ( Figure 3B ).","Modify,Grammar",Grammar
10053,8-1983,8-1983_v2_7@2,8-1983_v1_7@2,"Since its introduction some 25 years ago <REF-2> , neurospheres have been used to study neurogenesis <REF-3> , genes that regulate self-renewal <REF-4> , <REF-5> , and molecular mechanisms that control neuronal and glial differentiation <REF-3> , <REF-6> – <REF-9> .","Since its introduction some 25 years ago <REF-2> , neurospheres have been used to study neurogenesis <REF-3> , genes that regulate self-renewal <REF-4> , and molecular mechanisms that control neuronal and glial differentiation <REF-3> , <REF-5> – <REF-8> .","Modify,Fact/Evidence",Fact/Evidence
10054,8-1983,8-1983_v2_79@1,8-1983_v1_77@1,Figure 4 is a picture of a small ( Figure 4A-B ; arrowhead) and a larger ( Figure 4C ; arrowhead) primary neurosphere immunostained using an anti-GFAP antibody and counterstained with DAPI (Figure B; arrowhead).,Figure 4 is a picture of a primary neurosphere immunostained using an anti-GFAP ( Figure 4A ; arrowhead) antibody and counterstained with DAPI (Figure B; arrowhead).,"Modify,Fact/Evidence",Fact/Evidence
10055,8-1983,8-1983_v2_7@3,8-1983_v1_7@3,"Although there are many neurosphere protocols, the expected number and size of neurospheres generated after a week in vitro is not entirely characterized.","Although there are many neurosphere protocols, the expected number and size of neurospheres generated after a week in vivo is not fully understood.","Modify,Clarity",Clarity
10057,8-1983,8-1983_v2_83@3,8-1983_v1_81@2,A sign of bacteria contamination is reduced visibility of the media.,"This incubation time is necessary in order for the antibiotics in the media to inhibit growth of bacteria, and reduce subsequent contamination.","Split+Modify,Claim",Claim
10058,8-1983,8-1983_v2_92@0,8-1983_v1_89@0,- Spreadsheet containing numbers and sizes per field of view.xlsx (details on numbers and sizes of neurospheres produced from each mouse).,- Spreadsheet containing numbers and sizes per field of view.xlsx (details on numbers and sizes of neurospeheres produced from each mouse).,"Modify,Grammar",Grammar
10059,8-1983,8-1983_v2_8@3,8-1983_v1_8@3,The area surrounding the ventricle is then microdissected from a given slice of tissue to enrich for neural stem/progenitor cells.,The area surrounding the ventricle is then microdissected from a given slice to enrich for neural stem/progenitor cells.,"Modify,Clarity",Clarity
10060,8-1983,8-1983_v2_8@7,8-1983_v1_8@5,"In contrast, our approach requires no specialized equipment.","In our approach, no specialized equipment is needed.","Modify,Clarity",Clarity
10061,8-1983,8-1983_v2_8@9,8-1983_v1_8@7,"This method requires only half of a single brain, and generates reproducible numbers of neurospheres in a few days.","This method requires only half of a single brain, and generates reproducible numbers of neurospheres after a week.","Modify,Fact/Evidence",Fact/Evidence
10062,8-1983,8-1983_v2_2@2,8-1983_v1_2@2,The objective of this protocol is to provide a stepwise method from a single isolation that predicts the average number of neurospheres generated and to estimate an approximation of its sizes after several days in vitro .,This method predicts the average number of neurospheres and provides an approximation of its expected size after 7 days in vitro .,"Modify,Fact/Evidence",Fact/Evidence
10063,8-1983,8-1983_v2_2@4,8-1983_v1_2@3,"Estimates about the numbers and sizes of neurospheres will provide investigators with quantitative data to advise on how much starting dLGE tissue is required to generate the appropriate number of spheres for the implementation of downstream applications, including immunocytochemistry, self-renewal and differentiation assays.","Characterization of numbers and sizes will provide investigators with quantitative data to advise on the implementation of downstream applications, including immnocytochemistry, self-renewal and differentiation assays.","Modify,Claim",Claim
10064,8-1983,8-1983_v2_19@0,8-1983_v1_19@0,Images were taken with a Canon EOS Rebel XS camera.,Images were taken with a Canon EOS Rebel XS.,"Modify,Clarity",Clarity
10065,8-1983,8-1983_v2_19@7,8-1983_v1_19@1,The field of view represents a length of 783 μm and a width of 522 μm.,The field of view represent a length of 783 μm and a width of 522 μm.,"Modify,Grammar",Grammar
10066,8-1983,8-1983_v2_19@9,8-1983_v1_19@3,"Per animal, 3–4 wells were analyzed.","Per animal, 3-4 wells were analyzed.","Modify,Grammar",Grammar
10067,8-1983,8-1983_v2_19@10,8-1983_v1_19@4,A total of 5 individual animals were analyzed.,A total of 5 individual animal were analyzed.,"Modify,Grammar",Grammar
10068,8-1983,8-1983_v2_19@14,8-1983_v1_19@8,"If the main effect was significant (p < 0.05), Bonferroni’s multiple comparison post hoc test were used to compare the different replicates.","If the main effect was significant (p < 0.05), Bonferroni’s multiple comparision post hoc test were used to compare the different replicates.","Modify,Grammar",Grammar
10168,8-42,8-42_v2_20@2,,"Note that as of the time of publication, this is in the Bioconductor development version.",,"Add,Fact/Evidence",Fact/Evidence
10169,8-42,8-42_v2_20@3,,"For details, see the Software Availability section.",,"Add,Fact/Evidence",Fact/Evidence
10170,8-42,8-42_v2_31@0,,Note that the results of the queries using the API and the packages may change as we continue to update the OMA database.,,"Add,Claim",Claim
10171,8-42,8-42_v2_31@1,,The OMA database release of June 2018 was used to generate the examples below.,,"Add,Fact/Evidence",Fact/Evidence
10172,8-42,8-42_v2_34@0,,Here we first formulate our URL of interest and use it to send a GET request to the API.,,"Add,Fact/Evidence",Fact/Evidence
10173,8-42,8-42_v2_34@1,,"This gives us the response JSON object, which can then be parsed into an R list.",,"Add,Fact/Evidence",Fact/Evidence
10174,8-42,8-42_v2_39@0,,The identified targets can be found in the seq_annotation$targets.,,"Add,Fact/Evidence",Fact/Evidence
10175,8-42,8-42_v2_39@1,8-42_v1_37@0,"As the length of this object attribute is 1, in this example the sequence mapping identified a single target sequence.","In this example, the sequence mapping identified one target sequence.","Modify,Fact/Evidence",Fact/Evidence
10176,8-42,8-42_v2_39@2,8-42_v1_37@1,From this object further information can be obtained as follows:,From the seq_annotation object further information can be obtained as follows:,"Modify,Clarity",Clarity
10177,8-42,8-42_v2_45@0,8-42_v1_43@0,Note that it is also possible to access information on a HOG using the getHOG() function.,Note that it is also possible to access information on a HOG using the ID of one of its members.,"Split+Modify,Fact/Evidence",Fact/Evidence
10178,8-42,8-42_v2_45@1,8-42_v1_43@0,A HOG can be identified by its ID or the ID of one of its member proteins.,Note that it is also possible to access information on a HOG using the ID of one of its members.,"Split+Modify,Clarity",Clarity
10179,8-42,8-42_v2_92@3,8-42_v1_90@3,We will also continue to update the package and the API in sync with the OMA browser to incorporate new functionalities of OMA.,"We will also continue to update the package and API to incorporate new functionalities of OMA, such as support for local synteny which is currently under development.","Modify,Claim",Claim
10180,8-42,8-42_v2_14@0,8-42_v1_14@0,"Most data available through the OMA browser is now also accessible via the API, with the exception of the local synteny data.",Most data available through the OMA browser is now also accessible via the API.,"Modify,Fact/Evidence",Fact/Evidence
10181,8-42,8-42_v2_20@1,8-42_v1_20@1,"In the results section we showcase usage of the latest version of the package (v2.0), which requires R version >= 3.6 and Bioconductor version >= 3.9.","The package requires R version >= 3.6 and Bioconductor version >= 3.9, as well as a stable internet connection.","Modify,Fact/Evidence",Fact/Evidence
10182,8-42,8-42_v2_24@0,8-42_v1_24@0,"For Python users, we provide an analogous package named omadb .","For Python users, we provide an analogous package also named omadb .","Modify,Clarity",Clarity
10246,8-52,,8-52_v1_23@2,,"This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.","Delete,Claim",Claim
10247,8-52,8-52_v2_11@1,,"This genotype is known to produce more yield, but susceptible to geminivirus infection.",,"Add,Claim",Claim
10248,8-52,8-52_v2_11@2,,"Resistant genotype however, is not available in our collection so far, so comparison of both two genotypes is not possible to be performed.",,"Add,Claim",Claim
10249,8-52,8-52_v2_23@0,,"PCR based cloning strategy in combination with primer walking was applied to isolate the complete full length of putative distal promoter NPR1 gene region (5,950 bp).",,"Add,Fact/Evidence",Fact/Evidence
10250,8-52,8-52_v2_23@1,,This approach is considered to be the most appropriate since the sequencing read capacity used in this study is limited for about only 500 bp on average.,,"Add,Claim",Claim
10251,8-52,8-52_v2_23@2,,"Even though the KOD-Plus-Neo could amplify up to 24 kb according to manufacturer’s claim, but the full-length fragment (5,950 bp) still can not be sequenced in one step read due to limited reading capacity of the sequencing machine.",,"Add,Fact/Evidence",Fact/Evidence
10252,8-52,8-52_v2_23@3,,Validation of every single nucleotide data was confirmed by at least two overlapping validated segment.,,"Add,Fact/Evidence",Fact/Evidence
10253,8-52,8-52_v2_24@2,,"Furthermore, BLAST analysis showed no significant homology with another promotor sequence available in the NCBI database, indicating a limitation of promoter sequence availability.",,"Add,Claim",Claim
10254,8-52,8-52_v2_24@3,,The only promoter sequence showed homology is the promoter region of Capsicum annuum pathogenesis related protein-1 (PR-1) gene ( DQ201633.1 ) published by Lee et al . (2006) .,,"Add,Fact/Evidence",Fact/Evidence
10255,8-52,8-52_v2_24@4,,"However, the comparable nucleotide of both sequences spanned only 180 bp.",,"Add,Fact/Evidence",Fact/Evidence
10256,8-52,8-52_v2_27@0,,"In order to validate our claim, we constructed a contiguous segment with our isolated core promoter ( MK310185.1 ) and the NPR1 gene sequence isolated from a similar genotype Berangkai ( Nova et al ., 2019 ).",,"Add,Fact/Evidence",Fact/Evidence
10257,8-52,8-52_v2_27@1,,"BLAST analysis using the NPR1-Berangkai cDNA sequence exhibited 43 significant identity hits with other cDNA sequences of NPR1 gene for instance, Capsicum annuum ( NM_001325099.1 ), Capsicum chinense ( AM900559.1 ), Solanum lycopersicum ( KX198701.1 , NM_001247629.2 ), Nicotiana sp. (DQ837218.1, AF480488.1) Carica papaya (XM_022041103.1, AY550242.1) and some others.",,"Add,Fact/Evidence",Fact/Evidence
10258,8-52,8-52_v2_27@2,,"Tree analysis showed that our NPR1-Berangkai cDNA sequence clustered to similar clade with AM900559.1 and NM_001325099.1 and other three solanaceae ( S. lycopersicum - KX198701.1 , NM_001247629.2 ; S. tuberosum -XM_006357647.2; S. pennellii -XM_015227358.2 and S. torvum -KJ995663.1).",,"Add,Fact/Evidence",Fact/Evidence
10259,8-52,8-52_v2_27@3,,All those data obviously indicated that our segment landed in the right chromosome segment.,,"Add,Claim",Claim
10260,8-52,8-52_v2_29@2,,The PlantCare analysis successfully showed 2 other cis -acting element motifs namely 1 TCA motif and 3 CGTA motis which could not be shown by PLACE ( Figure 3 ).,,"Add,Fact/Evidence",Fact/Evidence
10261,8-52,8-52_v2_38@0,,"Analysis with PlantCare ( Lescot et al ., 2002 ) resulting 2 new additional elements, which are designated TCA element and CGTA-motif ( Figure 3 ), while analysis with PlantPAN 3.0 did not show any new different element compared to PLACE analysis.",,"Add,Fact/Evidence",Fact/Evidence
10262,8-52,8-52_v2_38@1,,The TCA element has a function as a cis -acting element which is involved in salicylic acid responsiveness.,,"Add,Fact/Evidence",Fact/Evidence
10263,8-52,8-52_v2_38@2,,Moreover we also found the CGTA motif playing a role in regulating me-JA responsiveness.,,"Add,Fact/Evidence",Fact/Evidence
10264,8-52,8-52_v2_38@3,,In our sequence only 1 single TCA element and 3 CGTA motifs could be detected.,,"Add,Fact/Evidence",Fact/Evidence
10265,8-52,8-52_v2_40@3,,"However, their role in the gene expression has to be confirmed empirically.",,"Add,Claim",Claim
10266,8-52,8-52_v2_40@4,,"For that reason, we are currently using the cis -acting elements found in this study for replicase ( Rep ) gene expression in the bacterial system.",,"Add,Fact/Evidence",Fact/Evidence
10267,8-52,8-52_v2_24@0,8-52_v1_23@0,Homology analysis between PD_CbNPR1 and its reference sequence via BLAST search showed 99% identity.,Homology analysis between PD_CbNPR1 and its reference sequence via BLAST search showed 99% homology.,"Modify,Clarity",Clarity
10268,8-52,8-52_v2_3@2,8-52_v1_3@2,"Identification of a cis -acting element was detected by PLACE, PlantCare, and PlantPAN.",Identification of a cis -acting element was detected by PLACE.,"Modify,Fact/Evidence",Fact/Evidence
10269,8-52,8-52_v2_33@3,8-52_v1_31@3,"The CAACA pattern was also found by Kagaya et al . (1999) in Arabidopsis thaliana , while the TGTTG pattern is similar to the OsNPR1 promoter ( Hwang & Hwang, 2010 ).","The CAACA pattern was also found by Kagaya et al . (1999) in Arabidopsis thaliana , while the TGTTG pattern is similar to the OsNPR1 promoter (Hwang dan Hwang, 2010).","Modify,Grammar",Grammar
10270,8-52,8-52_v2_4@1,8-52_v1_4@1,BLASTn search analysis indicated that PD_CbNPR1 sequence is highly conserved (99% identity) showing only a single nucleotide polymorphism (SNP) (base substitution) compared with its reference sequence.,BLASTn search analysis indicated that PD_CbNPR1 sequence is highly conserved (99% homology) showing only a single nucleotide polymorphism (SNP) (base substitution) compared with its reference sequence.,"Modify,Clarity",Clarity
10271,8-52,8-52_v2_36@0,8-52_v1_34@0,"We also found a gibberellin-responsive element (GARE) motif, which has been previously reported by Ogawa et al . (2003) as the binding site with some transcription factors induced by gibberellic acid.","We also found a gibberelline-responsive element (GARE) motif, which has been previously reported by Ogawa et al . (2003) as the binding site with some transcription factors induced by gibberellic acid.","Modify,Grammar",Grammar
10272,8-52,8-52_v2_7@0,8-52_v1_7@0,The non-expressor of pathogenesis related gene 1 ( NPR1 ) protein is the main regulator in the systemic acquired resistance response of many plants.,The non-expressor of pathogenesis related gene 1 ( NPR1 ) protein is a main regulator in the systemic acquired resistance response of many plants.,"Modify,Grammar",Grammar
10273,8-52,8-52_v2_7@1,8-52_v1_7@1,Overexpression by modifying the distal promoter in the W-box element of the OsNPR1 gene in rice could increase its resistance against Xanthomonas oryzae pv.,Over expression by modifying distal promoter in the W-box element of the OsNPR1 gene in rice could increase its resistance against Xanthomonas oryzae pv.,"Modify,Grammar",Grammar
10274,8-52,8-52_v2_11@3,8-52_v1_11@1,The plant was grown in a greenhouse and maintained for 8 weeks before being used for DNA isolation.,The chili pepper genotype Berangkai was grown in a greenhouse and maintained for 8 weeks before being used for DNA isolation.,"Modify,Clarity",Clarity
10275,8-52,8-52_v2_2@1,8-52_v1_2@1,"The cis -acting elements of its distal promoter gene are characterized by salicylic acid inducing elements such as the W-box, RAV1AAT and ASF1, accompanied by enhancer and silencer elements.","The cis -acting elements of its distal promoter gene are characterized by salicylic acid inducing elements such as the W-box, RAV1AAT and ASF1, accompanied with enhancer and silencer elements.","Modify,Grammar",Grammar
10276,8-52,8-52_v2_13@11,8-52_v1_13@11,The final extension was maintained at 72°C for 5 minutes.,Final extension was maintained at 72°C for 5 minutes.,"Modify,Grammar",Grammar
10277,8-52,8-52_v2_17@5,8-52_v1_17@5,"The cis -acting elements were identified using PLACE , developed by Higo et al . (1999) , PlantCARE ( Lescot et al ., 2002 ) and PlantPAN .","The cis -acting elements were identified using PLACE , developed by Higo et al . (1999) .","Modify,Fact/Evidence",Fact/Evidence
10531,8-80,,8-80_v1_101@1,,"Next steps include a more detailed look at the economic levers of tobacco preference, the perceived harms of cigarette smoking vs the objective realities of these harms, and a look at the characteristics and choices of ex-smokers.","Delete,Claim",Claim
10532,8-80,8-80_v2_19@4,,Face-to-face respondents were not compensated for their participation.,,"Add,Fact/Evidence",Fact/Evidence
10533,8-80,8-80_v2_19@5,,Online respondents were all members of an online panel company and received Reward Points.,,"Add,Fact/Evidence",Fact/Evidence
10534,8-80,8-80_v2_19@6,,"The number of points awarded for survey completion is based on survey length, complexity, and incidence rate.",,"Add,Fact/Evidence",Fact/Evidence
10535,8-80,8-80_v2_19@7,,"Once a points threshold is reached, panelists may redeem their points for online gift certificates or merchandise.",,"Add,Fact/Evidence",Fact/Evidence
10536,8-80,8-80_v2_19@8,,Each country has its own unique catalog.,,"Add,Fact/Evidence",Fact/Evidence
10537,8-80,8-80_v2_20@2,8-80_v1_20@2,"A rim weighting procedure was run against the population figures from the most recent national census to construct weight variables, with the procedure executed separately for each country.","A rim weighting procedure was run against the population figures to construct weight variables, with the procedure executed separately for each country.","Modify,Clarity",Clarity
10538,8-80,8-80_v2_7@0,8-80_v1_7@0,"While the worldwide rate of tobacco smoking has declined substantially in recent years, the absolute number of people currently smoking has increased from approximately 720 million smokers in 1980 to an estimated 1.1 billion today, the consequence of population growth outpacing declining smoking prevalence in many low and middle income countries (LMICs) <REF-1> – <REF-3> .","While the worldwide rate of tobacco smoking has declined substantially in recent years, the absolute number of people currently smoking has increased from approximately 720 million smokers in 1980 to an estimated 1.1 billion today, the consequence of population growth outpacing declining smoking prevalence in many low and middle income countries <REF-1> – <REF-3> .","Modify,Clarity",Clarity
10539,8-80,8-80_v2_35@0,8-80_v1_35@0,"In almost all countries, more current smokers than ex-smokers or nonsmokers were surrounded by people who also smoked, including parents, spouse/partner, close friends and colleagues ( Figure 5a–d ).","In almost all countries, more current smokers than ex-smokers or nonsmokers were surrounded by people who also smoked, including parents, spouse/partner, close friends and colleagues ( Figure 5a-d ).","Modify,Grammar",Grammar
10540,8-80,8-80_v2_40@2,8-80_v1_40@2,"Additionally, more than 60% of smokers and ex-smokers in India, Malawi and Brazil, had bought cigarettes when they knew the money could be spent better on household essentials like food.","Additionally, between 21% (Japan) and 87% (Brazil) of smokers and ex-smokers had bought cigarettes when they knew the money could be spent better on household essentials like food.","Modify,Fact/Evidence",Fact/Evidence
10541,8-80,8-80_v2_88@1,8-80_v1_88@1,RYO cigarettes vary in composition but have been shown to cause comparable exposure to known and suspected carcinogens <REF-15> .,"RYO cigarettes vary in composition but have been shown to have higher tar yields than boxed cigarettes, as well as comparable exposure to known and suspected carcinogens <REF-15> .","Modify,Fact/Evidence",Fact/Evidence
10542,8-80,8-80_v2_15@2,8-80_v1_15@2,"Smokers were defined as those who responded that they currently smoked cigarettes, cigars, cigarillos or a pipe (or bidis in India) “regularly” or “occasionally;” ex-smokers were defined as those who responded that they used to smoke but stopped; and never smokers were defined as those who responded that they had never smoked.","Smokers were defined as those who responded that they currently smoked cigarettes, cigars, cigarillos or a pipe “regularly” or “occasionally;” ex-smokers were defined as those who responded that they used to smoke but stopped; and never smokers were defined as those who responded that they had never smoked.","Modify,Clarity",Clarity
10543,8-80,8-80_v2_96@2,8-80_v1_96@2,Compared to other nicotine-non-tobacco products ENDS most closely simulate smoking regular cigarettes in how they are used.,"Compared to other nicotine, non-tobacco products, ENDS most closely simulate smoking regular cigarettes in how they are used.","Modify,Grammar",Grammar
10544,8-80,8-80_v2_106@0,8-80_v1_106@0,Data are available under the terms of the Creative Commons Zero “No rights reserved” data waiver (CC0 1.0 Public domain dedication).,"Data are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).","Modify,Grammar",Grammar
10776,9-1088,9-1088_v2_3@0,,Glossary,,"Add,Other",Other
10777,9-1088,9-1088_v2_28@3,,"Sunspots have been framed for communicable (CDs) and non-communicable disorders (NCDs), same as epidemics ( Hrushesky et al. , 2011 ; Stoupel et al. , 2003 ; Stienen et al. , 2015 ; Yeung, 2006 )",,"Add,Fact/Evidence",Fact/Evidence
10778,9-1088,9-1088_v2_31@4,,"Sunspot numbers correlation with other solar activity indices [UV/EUV, F10.7 flux (noontime measurement of the solar radio flux at a wavelength of 10.7 cm), Mg II] persisted at the same levels until 2000 ( Bruevich et al., 2014 ; Floyd et al., 2005 ).",,"Add,Fact/Evidence",Fact/Evidence
10779,9-1088,9-1088_v2_31@5,,"More importantly, they interact linearly except for the minima and maxima of the 11-year solar cycle, while these correlations vs sunspots and F10.7 flux, were shown to reach the lowest levels twice in each cycle ( Bruevich et al., 2014 ).",,"Add,Fact/Evidence",Fact/Evidence
10780,9-1088,9-1088_v2_31@6,,"Furthermore, the same study showed that a double-peak structure was observed in cycle 22 but not in cycle 21.",,"Add,Fact/Evidence",Fact/Evidence
10781,9-1088,9-1088_v2_31@7,,"These phenomena enhance the validity of our results, as the examined period comprise the minimum of the 21 st and the beginning of the maximum of the 22 nd cycle.",,"Add,Claim",Claim
10782,9-1088,9-1088_v2_31@8,,"It is established that the sunspot minima coincide with an increased flux of cosmic rays, whereas, in sunspot maxima the heliosphere is shielded by planetary magnetospheres- a phenomenon known as “Forbush decrease” ( Raghav et al., 2017 ).",,"Add,Fact/Evidence",Fact/Evidence
10783,9-1088,9-1088_v2_30@7,9-1088_v1_21@7,"The covariates related to diet, hygiene and culture were stable in this period and, thus, they could be safely assumed as such.","The covariates related to diet, hygiene and culture were stable in this period and, thus, they could be safely assumed.","Modify,Clarity",Clarity
10784,9-1088,9-1088_v2_32@2,9-1088_v1_23@2,"Our work postulates that there is an inverse relation in two time series, between the timing of sunspot numbers and stroke deaths, a hypothesis posed by previous investigations ( Geronikolou & Leontitsis, 2005 ; Geronikolou & Petropoulos, 1996 ; Stoupel et al., 1999 ).","Our work postulates that there is an inverse relation in two time series, between the timing of sunspot numbers and stroke deaths, a hypothesis posed by previous investigations ( Geronikolou & Leontisis, 2005 ; Geronikolou & Petropoulos, 1996 ; Stoupel et al., 1999 ).","Modify,Grammar",Grammar
10785,9-1088,9-1088_v2_33@2,9-1088_v1_24@2,"The molecular interactions network approach, where the inter-species functional interactome of nuclear steroid receptors (R1) constructed on orthologues was employed ( Geronikolou et al., 2018 ).","The molecular interactions network approach, where the inter-species functional interactome of nuclear steroid receptors (R1) was constructed on orthologues was employed ( Geronikolou et al., 2018 ).","Modify,Grammar",Grammar
10786,9-1088,9-1088_v2_33@3,9-1088_v1_24@3,"R1 has interspecies dimensions and thus has evolutionary value extending from insects to humans, that is, from early life eras till now.","R1 has interspecies dimensions and thus has evolutionary and historical value extending from insects to humans, that is, from early life eras till now.","Modify,Clarity",Clarity
10787,9-1088,9-1088_v2_33@6,9-1088_v1_24@6,"R1 includes genes and their products involved in circadian rhythms, while its major hub NCOR1 in macrophages blocks the pro-atherogenic functions of peroxisome proliferator-activated receptor gamma (PPARγ) ( Oppi et al., 2020 ), greatly implicating stroke pathophysiology.","R1 includes genes and their products involved in circadian rhythms, while its major hub NCOR1 in macrophages blocks the pro-atherogenic functions of peroxisome proliferator-activated receptor gamma (PPARγ) in atherosclerosis ( Oppi et al., 2020 ), greatly implicating stroke pathophysiology.","Modify,Clarity",Clarity
10788,9-1088,9-1088_v2_33@8,9-1088_v1_24@8,"It is likely that as soon as R1 is disrupted, the atherogenic and/or other pathological processes progress dramatically, with potentially lethal consequences.","It is likely that as soon as R1 is disrupted, the atherogenic and/or other pathological processes progress dramatically, with lethal consequences.","Modify,Claim",Claim
10789,9-1088,9-1088_v2_35@4,9-1088_v1_26@4,"Thus, future medical practice should probably take account of chronopathology ( Stienen et al., 2015 ) so as to prevent stroke mortality shifts (chronotherapy and chronoprevention plans).","Thus, future medical practice should probably take account of chronopathology so as to prevent stroke mortality shifts (chronotherapy and chronoprevention plans).","Modify,Fact/Evidence",Fact/Evidence
10790,9-1088,9-1088_v2_37@0,9-1088_v1_28@0,"Our work clearly established that sunspot numbers and stroke mortality were inversely correlated, and that a violent fluctuation of sunspot numbers over 35% shifted monthly mortality with a phase delay of two months.","Our work established clearly that of sunspot numbers and stroke mortality were inversely correlated, and that a violent fluctuation of sunspot numbers over 35% shifted monthly mortality to a phase delay of two months.","Modify,Clarity",Clarity
10791,9-1088,9-1088_v2_14@0,9-1088_v1_5@0,"Stroke has been previously associated with solar activity ( Halberg et al., 2001 ; Otsuka et al., 2001 ; Stoupel et al., 1995 ; Stoupel et al., 1996 ).","Stroke has been previously associated to solar activity ( Halgberg et al., 2001 ; Otsuka et al., 2001 ; Stoupel et al., 1995 ; Stoupel et al., 1996 ).","Modify,Grammar",Grammar
10792,9-1088,9-1088_v2_15@0,9-1088_v1_6@0,"Our aim was to investigate the dynamics and trends in the selected time series, to determine sunspot numbers vs. daily and monthly stroke deaths in synchronized periodicities with gradual time delays (chronomes), and to define a sunspot number threshold beyond which stroke death events may be influenced.","Our aim was to investigate the dynamics and trends in the time series, to determine sunspot numbers vs. daily and monthly stroke deaths in synchronized periodicities with gradual time delays (chronomes), and to define a sunspot number threshold for presence of stroke mortality.","Modify,Clarity",Clarity
10793,9-1088,9-1088_v2_17@0,9-1088_v1_8@0,"In this study we focused on monthly stroke mortality events between 1985 and 1989, based on the underlined cause of death data (ICD-9 Table 5.3: recode 430, Table 5.4: recode 200) from the archives of Piraeus Civil Registry ( Geronikolou & Zikos, 1991 ).","In this study we focused on monthly stroke mortality events between 1985 and 1989, based on the underlined cause of death data from the archives of Piraeus Civil Registry ( Geronikolou & Zikos, 1991 ).","Modify,Fact/Evidence",Fact/Evidence
10794,9-1088,9-1088_v2_19@0,9-1088_v1_10@0,"The stroke death rate in Piraeus, was calculated using the formula (number of all deaths per year per 1000 people in June 30 th , year x).","The stroke death rate in Piraeus, was calculated over the formula (number of all deaths per year per 1000 people in June 30 th , year x).","Modify,Grammar",Grammar
10795,9-1088,9-1088_v2_19@1,9-1088_v1_10@1,"The overall death rate was calculated with the denominators provided by the 1981 census ( Geronikolou & Zikos, 1991 ).",The overall death rate was calculated with the denominators provided by the 1981 census.,"Modify,Fact/Evidence",Fact/Evidence
10796,9-1088,9-1088_v2_23@4,9-1088_v1_14@4,"Thus, monthly sunspot numbers by squared root variation and their violent fluctuation of over 35% was correlated to monthly stroke mortality, establishing a negative correlation between the two time-series (sunspot numbers and deaths by stroke) ( Figure 1b, d ).","Thus, monthly sunspot numbers by squared root variation and their violent fluctuation of over 35% was correlated to monthly stroke mortality, establishing a negative correlation between the two time-series (sunspot numbers and deaths of strokes) ( Figure 1b, d ).","Modify,Grammar",Grammar
10798,9-1088,9-1088_v2_28@1,9-1088_v1_19@2,"Human biological clocks are intensively studied, as they represent adaptive body mechanisms necessary to assist with homeostatic changes caused by the environment, possibly including solar activity.",They represent adaptive body mechanisms necessary to assist with homeostatic changes caused by solar activity disturbances.,"Merge+Modify,Claim",Claim
10799,9-1088,9-1088_v2_29@4,9-1088_v1_20@4,"Socioeconomic and geographic disparities have been suspected, while heliomagnetic influences have been proposed as possible etiologic contributors to human pathology ( Halberg et al. , 1998 ; Stienen et al. , 2015 ; Stoupel et al. , 1996 ; Stoupel et al. , 2003 ).","Socioeconomic and geographic disparities have been suspected, while heliomagnetic influences have been proposed as possible etiologic contributors to human pathology.","Modify,Fact/Evidence",Fact/Evidence
10800,9-1088,9-1088_v2_30@2,9-1088_v1_21@2,"Moreover, its population is representative of the urban populations in Greece ( Geronikolou & Zikos 1991 ).","Moreover, its population is representative of the urban populations in Greece ( Geronikolou, 1991 ).","Modify,Fact/Evidence",Fact/Evidence
10801,9-1088,9-1088_v2_30@5,9-1088_v1_21@5,"The data used in this study were original and based on the reported underlined cause of death ( Geronikolou & Zikos, 1991 ).","The data used in this study were original and based on the underlined cause of death ( Geronikolou, 1991 ).","Modify,Fact/Evidence",Fact/Evidence
10911,9-1193,9-1193_v2_31@3,,"Regarding non-drug interventions, 28 investigated procedures (e.g. renal replacement therapy), eight devices (e.g. various respiratory devices), and 26 trials investigated other non-drug interventions (e.g. physical activity and pulmonary rehabilitation).",,"Add,Fact/Evidence",Fact/Evidence
10912,9-1193,9-1193_v2_56@4,,"Other objective outcomes commonly used according to the COVID-19 core outcome sets <REF-20> , such as hospitalization and mechanical ventilation, may still be impacted by subjective decisions and the awareness of the randomly allocated intervention, and thus may benefit from a blinded assessment.",,"Add,Claim",Claim
10913,9-1193,9-1193_v2_63@5,,"Fourth, we did not assess in details all the different outcomes being used and the blinding of the outcome collection was not systematically reported preventing us from fully apprehending the impact of the lack of blinding.",,"Add,Claim",Claim
10914,9-1193,9-1193_v2_69@0,,The dataset for this study is provided on the Open Science Framework.,,"Add,Fact/Evidence",Fact/Evidence
10915,9-1193,9-1193_v2_69@1,,It is based on continuously evolving data sources.,,"Add,Fact/Evidence",Fact/Evidence
10916,9-1193,9-1193_v2_69@2,,"With this second version, we corrected misclassifications/duplicates affecting 6 of the 689 previously included trials.",,"Add,Fact/Evidence",Fact/Evidence
10917,9-1193,9-1193_v2_69@3,,Subsequent updates of COVID-evidence and the other sources may provide different datasets.,,"Add,Claim",Claim
10918,9-1193,9-1193_v2_29@1,9-1193_v1_29@1,"Overall, 46.5% of the trials intended to use mortality as a primary (n=98) or secondary outcome (n=220; Table 1 ), and 53•4% (n=365) did not specify mortality as an outcome.","Overall, 46.4% of the trials intended to use mortality as a primary (n=98) or secondary outcome (n=369; Table 1 ).","Modify,Fact/Evidence",Fact/Evidence
10919,9-1193,9-1193_v2_29@2,9-1193_v1_29@2,"Out of the 521 inpatient trials, 55.7% (n=290) planned on reporting mortality as an outcome.","Out of the 525 inpatient trials, 55.6% (n=292) planned on reporting mortality as an outcome.","Modify,Fact/Evidence",Fact/Evidence
10920,9-1193,9-1193_v2_31@0,9-1193_v1_31@0,"Out of the 683 trials, 602 (88.1%) assessed treatment interventions (186,189 planned patients); drugs were more frequent (345 trials [57.3%]), encompassing a vast range of substances.","Out of the 689 trials, 607 (88.1%) assessed treatment interventions (187,209 planned patients); drugs were more frequent (349 trials [57.5%]), encompassing a vast range of substances.","Modify,Fact/Evidence",Fact/Evidence
10921,9-1193,9-1193_v2_31@1,9-1193_v1_31@1,The two most common pharmacological classes were antiviral drugs (assessed in 141 trials; e.g. lopinavir/ritonavir [n=45]) and antimalarial drugs (111 trials; e.g. hydroxychloroquine [n=84]).,The two most common pharmacological classes were antiviral drugs (assessed in 144 trials; e.g. lopinavir/ritonavir [n=45]) and antimalarial drugs (112 trials; e.g. hydroxychloroquine [n=83]).,"Modify,Fact/Evidence",Fact/Evidence
10922,9-1193,9-1193_v2_31@2,9-1193_v1_31@2,"There were 106 trials investigating traditional medicine and 70 exploring highly diverse pharmaceuticals of various classes, e.g. bismuth potassium citrate, ebastine, pirfenidone, dipyridamole and hydrogen peroxidase.","There were 106 trials investigating traditional medicine and 70 exploring highly diverse pharmaceuticals of various classes, e.g. bismuth potassium citrate, ebastine, pirfenidone, dipyridamole and hydrogen peroxidase ( Figure 1 and see Extended data ) <REF-12> .","Modify,Fact/Evidence",Fact/Evidence
10923,9-1193,9-1193_v2_31@5,9-1193_v1_31@3,"The comparators were predominantly standard of care or no intervention (47.2% [n=284]), placebo (17.1% [n=103]) or other interventions (17.9%; [n=108]) ( Table 1 ).","The comparators were predominantly standard of care or no intervention (47.1% [n=286]), placebo (17% [n=103]) or other interventions (18%; [n=109]) ( Table 1 ).","Modify,Fact/Evidence",Fact/Evidence
10924,9-1193,9-1193_v2_35@0,9-1193_v1_35@0,"Overall, 77 trials (11.3%) focused on prevention (204,641 planned participants), mainly prophylactic drug use (n=41), vaccines (n=14; 9 already started recruitment; see Extended data ) <REF-12> and non-pharmaceutical interventions (n=10) (e.g. masks or the use of media and influencers in people’s compliance to hygienic practices).","Overall, 78 trials (11.3%) focused on prevention (205,841 planned participants), mainly prophylactic drug use (n=41), vaccines (n=14; 9 already started recruitment; see Extended data ) <REF-12> and non-pharmaceutical interventions (n=10) (e.g. masks or the use of media and influencers in people’s compliance to hygienic practices).","Modify,Fact/Evidence",Fact/Evidence
10925,9-1193,9-1193_v2_37@0,9-1193_v1_37@0,"The number of trials increased rapidly; on average 0.5 trials per day were registered in January, 8.1 in February, 8.3 in March, and 17.2 in April 2020.","The number of trials increased rapidly; on average 0.5 trials per day were registered in January, 8.1 in February, 8.4 in March, and 17.6 in April 2020.","Modify,Fact/Evidence",Fact/Evidence
10926,9-1193,9-1193_v2_38@0,9-1193_v1_38@0,Trials were conducted in 41 countries and through international collaborations ( Table 1 ; see Extended data ) <REF-12> .,Trials were conducted in 42 countries and through international collaborations ( Table 1 ; see Extended data ) <REF-12> .,"Modify,Fact/Evidence",Fact/Evidence
10927,9-1193,9-1193_v2_38@1,9-1193_v1_38@1,"Half were from China (51.4% [n=351]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.","Half were from China (51.1% [n=352]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.","Modify,Fact/Evidence",Fact/Evidence
10928,9-1193,9-1193_v2_38@2,9-1193_v1_38@2,"Trial characteristics were similar across the five most frequent geographical locations (China, USA, France, Spain and international) contributing to 73.6% (n=503) of the global trial research ( Table 1 ).","Trial characteristics were similar across the five most frequent geographical locations (China, USA, France, Spain and international) contributing to 73.3% (n=505) of the global trial research ( Table 1 ).","Modify,Fact/Evidence",Fact/Evidence
10929,9-1193,9-1193_v2_38@3,9-1193_v1_38@3,Traditional medicine was assessed in 30.5% of trials from China (n=107) but rarely in other countries.,Traditional medicine was assessed in 30.4% of trials from China (n=107) but rarely in other countries.,"Modify,Fact/Evidence",Fact/Evidence
10930,9-1193,9-1193_v2_41@1,9-1193_v1_41@1,"In February, 5.1% of trials included more than 500 participants in contrast to 18.6% of trials in March ( Figure 3 ).","In February, fewer than 8% of trials included more than 500 participants in contrast to 29.6% of trials in March ( Figure 3 ).","Modify,Fact/Evidence",Fact/Evidence
10931,9-1193,9-1193_v2_41@4,9-1193_v1_41@4,"When the proportion of trials from China decreased, so did trials assessing traditional medicine (from 46.9% to 0.9%) while the proportion of trials assessing drugs rose (from 40.6% to 77.7%).","When the proportion of trials from China decreased, so did trials assessing traditional medicine (from 46.9% to 0.9%) while the proportion of trials assessing drugs rose (from 38.1% to 77.2%).","Modify,Fact/Evidence",Fact/Evidence
10932,9-1193,9-1193_v2_45@0,9-1193_v1_45@0,"Out of the 683 trials, 6.6% (n=45) planned to enroll 1,000 to 5,000 participants.","Out of the 689 trials, 6.7% (n=46) planned to enroll 1,000 to 5,000 participants.","Modify,Fact/Evidence",Fact/Evidence
10933,9-1193,9-1193_v2_45@1,9-1193_v1_45@1,"Most were randomized (88.9% [n=40]), assessed drugs (80%; n=36), and many were not blinded (53.3% [n=24]).","Most were randomized (89.1% [n=41]), assessed drugs (80.4%; n=37), and many were not blinded (52.2% [n=24]).","Modify,Fact/Evidence",Fact/Evidence
10934,9-1193,9-1193_v2_45@3,9-1193_v1_45@3,"The top three regions were the United States (22.2%; n=10), France (11.1% [n=5]) and international collaborations (11.1% [n=5]) (see Extended data ) <REF-12> .","The top three regions were the United States (21.7%; n=10), France (13% [n=6]) and international collaborations (10.9% [n=5]) (see Extended data ) <REF-12> .","Modify,Fact/Evidence",Fact/Evidence
10935,9-1193,9-1193_v2_46@0,9-1193_v1_46@0,"Eleven (1.6%) trials, registered between February and April 2020 (seven for treatment and four for prevention), planned to enroll over 5,000 participants (see Extended data ) <REF-12> .","Eleven (1.6%) trials, registered between February and April 2020, planned to enroll over 5,000 participants (see Extended data ) <REF-12> .","Modify,Fact/Evidence",Fact/Evidence
10936,9-1193,9-1193_v2_47@0,9-1193_v1_47@0,Five drug interventions tested in these 11 larger trials were simultaneously investigated in over 20 smaller trials (see Extended data ) <REF-12> .,Six drug interventions tested in these 11 larger trials (seven for treatment and four for prevention) were simultaneously investigated in at least 10 smaller trials (see Extended data ) <REF-12> .,"Modify,Fact/Evidence",Fact/Evidence
10937,9-1193,9-1193_v2_47@1,9-1193_v1_47@1,"Overall, 167 trials (141 for treatment, 24 for prevention and two for treatment and prevention) with fewer than 5,000 participants assessed at least one intervention that was also assessed in a larger trial (median sample size 223 [IQR 80 to 540]; 132 had fewer than 1000 participants).","Overall, 169 trials (143 for treatment, 24 for prevention and two for treatment and prevention) with fewer than 5,000 participants assessed at least one intervention that was also assessed in a larger trial (median sample size 273 [IQR 90 to 700]; 134 had fewer than 1000 participants).","Modify,Fact/Evidence",Fact/Evidence
10938,9-1193,9-1193_v2_47@2,9-1193_v1_47@2,For 103 of those (61.7%) the larger trial was registered before.,For 107 of those (63.5%) the larger trial was registered before.,"Modify,Fact/Evidence",Fact/Evidence
10939,9-1193,9-1193_v2_47@3,9-1193_v1_47@3,"For example, 104 trials with fewer than 5,000 participants tested hydroxychloroquine and 86 of them (82.7%) were registered after the first large trial testing this drug and 82 (78.8%) assessed hydroxychloroquine as treatment ( Figure 4 ; see Extended data ) <REF-12> .","For example, 106 trials with fewer than 5,000 participants tested hydroxychloroquine and 88 of them (83%) were registered after the first large trial testing this drug and 83 (77.6%) assessed hydroxychloroquine as treatment ( Figure 4 ; see Extended data ) <REF-12> .","Modify,Fact/Evidence",Fact/Evidence
10940,9-1193,9-1193_v2_47@4,9-1193_v1_47@4,"These 104 trials had a median sample size of 334, but cumulatively, they planned to enroll as many patients as the four larger trials testing hydroxychloroquine (75,217 vs 77,000).","These 106 trials had a median sample size of 334, but cumulatively, they planned to enroll as many patients as the four larger trials testing hydroxychloroquine (76,617 vs 77,000).","Modify,Fact/Evidence",Fact/Evidence
10941,9-1193,9-1193_v2_51@0,9-1193_v1_51@0,"By the end of 2020, 413 trials (60.5%) with a total of 159,957 planned participants were expected to be completed (i.e. last patient, last visit), including 232 drug trials (97,282 participants) and 22 over-1,000 participants trials and five over-5,000 participants trials.","By the end of 2020, 414 trials (60.1%) with a total of 160,107 planned participants were expected to be completed (i.e. last patient, last visit), including 240 drug trials (97,846 participants) and 22 over-1,000 participants trials and five over-5,000 participants trials.","Modify,Fact/Evidence",Fact/Evidence
10942,9-1193,9-1193_v2_54@3,9-1193_v1_54@3,"Few trials focused on prevention, but some were very large and focused on healthcare workers (i.e. 24.3% of planned trial participants are healthcare workers).","Few trials focused on prevention, but some were very large and focused on healthcare workers (i.e. 24.4% of planned trial participants are healthcare workers).","Modify,Fact/Evidence",Fact/Evidence
10943,9-1193,9-1193_v2_55@0,9-1193_v1_55@0,The emergence of 683 trials in a 100-day period is unparalleled.,The emergence of 689 trials in a 100-day period is unparalleled.,"Modify,Fact/Evidence",Fact/Evidence
10944,9-1193,9-1193_v2_56@0,9-1193_v1_56@0,"Thomas Chalmers highlighted in 1977 the need to ‘Randomize the first patient’ <REF-19> , and a reassuring 75.5% of trials are indeed randomized.","Thomas Chalmers highlighted in 1977 the need to ‘Randomize the first patient’ <REF-19> , and a reassuring 75% of trials are indeed randomized.","Modify,Fact/Evidence",Fact/Evidence
10945,9-1193,9-1193_v2_56@3,9-1193_v1_56@3,"Blinding may not be required for mortality outcomes; however, mortality was rarely a primary outcome.","Blinding may not be required for mortality outcomes; however, it was rarely a primary outcome.","Modify,Clarity",Clarity
10946,9-1193,9-1193_v2_56@5,9-1193_v1_56@4,Half of the trials include fewer than 118 patients and many small trials were initiated after public registration of very large trials addressing similar questions.,Half of the trials include fewer than 120 patients and many small trials were initiated after public registration of very large trials addressing similar questions.,"Modify,Fact/Evidence",Fact/Evidence
10947,9-1193,9-1193_v2_58@1,9-1193_v1_58@1,"Strongly endorsed and prioritized by authorities and medical representatives <REF-27> , it is running as a streamlined pragmatic platform trial in over 176 hospitals, randomizing over 12,000 patients in just over four months <REF-14> .","Strongly endorsed and prioritized by authorities and medical representatives <REF-26> , it is running as streamlined pragmatic platform trial in over 176 hospitals, randomizing over 12,000 patients in just over four months <REF-14> .","Modify,Grammar",Grammar
10948,9-1193,9-1193_v2_63@6,9-1193_v1_63@5,"Fifth, we may have missed a few cases of duplicate entries across registries or of multiple national parts of an international trial, thus slightly overestimating the number of trials but not affecting the overall interpretation.","Fourth, we may have missed a few cases of duplicate entries across registries or of multiple national parts of an international trial, thus slightly overestimating the number of trials but not affecting the overall interpretation.","Modify,Fact/Evidence",Fact/Evidence
10949,9-1193,9-1193_v2_63@7,9-1193_v1_63@6,"Sixth, we arbitrarily selected a period of the first 100 days, which is traditionally used to benchmark early outcomes of policies or presidencies.","Fifth, we arbitrarily selected a period of the first 100 days, which is traditionally used to benchmark early outcomes of policies or presidencies.","Modify,Fact/Evidence",Fact/Evidence
10950,9-1193,9-1193_v2_24@0,9-1193_v1_24@0,"We identified 683 trials registered or published over the pandemic’s first 100 days, testing interventions to treat or prevent COVID-19 (see Underlying data ) <REF-12> with a total planned sample size of 394,146 participants.","We identified 689 trials registered or published over the pandemic’s first 100 days, testing interventions to treat or prevent COVID-19 (see Underlying data ) <REF-12> with a total planned sample size of 396,366 participants.","Modify,Fact/Evidence",Fact/Evidence
10951,9-1193,9-1193_v2_24@2,9-1193_v1_24@2,"Twenty-nine (4.2%) were active but no longer recruiting (58,589 participants), 381 (55.8%) started recruiting (215,807 participants), 174 (25.5%) had not yet started (97,406 participants), 50 (7.3%) were discontinued (12,048 participants), and 4 (0.6%) were terminated (577 participants).","Thirty (4.4%) were active but no longer recruiting (59,259 participants), 384 (55.7%) started recruiting (217,357 participants), 174 (25.3%) had not yet started (97,406 participants), 50 (7.3%) were discontinued (12,048 participants), and 4 (0.6%) were terminated (577 participants).","Modify,Fact/Evidence",Fact/Evidence
10952,9-1193,9-1193_v2_24@3,9-1193_v1_24@3,The status was unknown for 10 (1.5%; 168 participants).,The status was unknown for 12 (1.7%; 168 participants).,"Modify,Fact/Evidence",Fact/Evidence
10953,9-1193,9-1193_v2_26@0,9-1193_v1_26@0,"The 683 trials’ median target sample size was 118 (IQR 60 to 300; Table 1 ); 40.7% (n=280) planned to enroll fewer than 100 participants, 8.2% (n=56) over 1,000, and 1.6% (n=11) over 5,000 (see Underlying data ) <REF-12> .","The 689 trials’ median target sample size was 120 (IQR 60 to 300; Table 1 ); 40.7% (n=280) planned to enroll fewer than 100 participants, 8.3% (n=57) over 1,000, and 1.6% (n=11) over 5,000 (see Underlying data ) <REF-12> .","Modify,Fact/Evidence",Fact/Evidence
10954,9-1193,9-1193_v2_26@1,9-1193_v1_26@1,75.5% (n=516) trials were randomized and 59.4% (n=406) did not use blinding ( Table 1 ).,75.8% (n=522) trials were randomized and 59.2% (n=408) did not use blinding ( Table 1 ).,"Modify,Fact/Evidence",Fact/Evidence
10955,9-1193,9-1193_v2_26@2,9-1193_v1_26@2,Randomized trials were on average almost three times larger than non-randomized trials (median sample size 144 vs. 50).,Randomized trials were on average three times larger than non-randomized trials (median sample size 150 vs. 50).,"Modify,Fact/Evidence",Fact/Evidence
11342,9-356,9-356_v2_21@4,9-356_v1_21@4,Such improvement allows researchers to culture small volume of a large number of Agrobacterium strains in parallel and use the diluted cultures to carry out high-throughput transformation of a large number of different constructs into Arabidopsis plants.,Such improvement allows researchers to culture small volume of a large number of Agrobacterium strains in parallel and use the diluted cultures to carry out high-throughput transformation of a large number different constructs into Arabidopsis plants.,"Modify,Grammar",Grammar
11343,9-356,9-356_v2_4@1,9-356_v1_4@1,"The development of different transformation protocols in various plants has enabled advances in plant molecular biology and crop improvements ( Saifi et al ., 2020 ).",The development of different transformation protocols in various plants has enabled advances in plant molecular biology and crop improvements.,"Modify,Fact/Evidence",Fact/Evidence
11344,9-356,9-356_v2_4@3,9-356_v1_4@3,"During 1980s and early 1990s, generating transgenic plants by leaf disc-based Agrobacterium -mediated transformation required laborious plant tissue culture and regeneration steps.","During 1980s and early 1990s, generating transgenic plants by leaf disc-based Agrobacterium -mediated transformation requires laborious plant tissue culture and regeneration steps.","Modify,Grammar",Grammar
11345,9-356,9-356_v2_4@8,9-356_v1_4@8,"Floral dip transformation may be feasible in plants such as wheat, maize, tomato, flax, Medicago truncatula and Setaria viridis ( Agarwal et al ., 2009 ; Bastaki & Cullis, 2014 ; Martins et al ., 2015 ; Mu et al ., 2012 ; Trieu et al ., 2000 ; Yasmeen et al ., 2009 ).","Floral dip transformation may be feasible in plants such as wheat and Setaria viridis ( Agarwal et al ., 2009 ).","Modify,Fact/Evidence",Fact/Evidence
11346,9-356,9-356_v2_5@2,9-356_v1_5@2,"Here, we tested whether a low concentration of Agrobacterium inoculum affects the plant transformation rate.","Here, we tested whether low concentration of Agrobacterium inoculum affects the plant transformation rate.","Modify,Grammar",Grammar
11347,9-356,9-356_v2_5@3,9-356_v1_5@3,"Our data showed that, contrary to our expectation, using an extremely low density of Agrobacterium inoculum (OD 600 =0.002) in the floral dip method still warrants relatively high transformation rate in Arabidopsis.","Our data showed that, in contrary to our expectation, using extremely low density of Agrobacterium inoculum (OD 600 =0.002) in floral dip method still warrants relatively high transformation rate in Arabidopsis.","Modify,Grammar",Grammar
11348,9-356,9-356_v2_10@3,9-356_v1_10@3,Then the overnight culture was diluted into 100 ml LB media with kanamycin (50 μg/ml) and allowed to grow further for 8 h (OD 600 =1.5~1.8) in the same shaker.,Then the overnight culture was diluted into 100 ml LB media with kanamycin (50 μg/ml) and allowed to grow further for 8 h in the same shaker.,"Modify,Fact/Evidence",Fact/Evidence
11349,9-356,9-356_v2_2@4,9-356_v1_2@4,Our data revealed that the floral dip method still guarantees a relatively high transformation rate in the Arabidopsis thaliana Col-0 ecotype even with very low Agrobacterium inoculum (OD 600 =0.002).,Our data revealed that the floral dip method still guarantees relatively high transformation rate in Arabidopsis thaliana Col-0 ecotype even with very low Agrobacterium inoculum (OD 600 =0.002).,"Modify,Grammar",Grammar
11364,9-47,,9-47_v1_25@3,,The C7 and C8 clusters also have a relatively high frequency.,"Delete,Claim",Claim
11365,9-47,9-47_v2_17@4,,More advance distribution analysis is also available using the clonesizeDistribution function based on recent work using Jensen-Shannon divergence.,,"Add,Claim",Claim
11366,9-47,9-47_v2_25@3,,This function also works with the SingleCellExperiment and monocle3 class of expression objects.,,"Add,Fact/Evidence",Fact/Evidence
11367,9-47,9-47_v2_29@3,,"Visualization functions in scRepertoire have a parameter, exportTable, allowing users to examine the quantifications underlying the generation of the graphs.",,"Add,Claim",Claim
11368,9-47,9-47_v2_24@0,9-47_v1_24@0,Expression interaction,Seurat interaction,"Modify,Other",Other
11369,9-47,9-47_v2_25@2,9-47_v1_25@2,"Using the combineExpression function in scRepertoire, we can look at the clonotypic frequencies of cells that comprise the UMAP-based clusters ( Figure 4C ).","Using the combineSeurat function in scRepertoire, we can look at the clonotypic frequencies of cells that comprise the UMAP-based clusters ( Figure 4C ), with notable expansion in the C2, C3, and C6 clusters ( Figure 4D ).","Modify,Fact/Evidence",Fact/Evidence
11370,9-47,9-47_v2_25@7,9-47_v1_25@6,"After combining both the clonotype and expression data, interaction between categories, such as cluster label and clonotype frequency can be visualized with the alluvialClonotypes function ( Figure 4E ).","After combining both the clonotype and expression data, interaction between categories, such as cluster label and clonotype frequency can be visualized with the alluvialGraph function.","Modify,Fact/Evidence",Fact/Evidence
11371,9-47,9-47_v2_5@1,9-47_v1_5@1,"Built using R, scRepertoire is a toolkit to assist in the analysis of immune profiles for both B and T cells, while interacting with the popular Seurat pipeline <REF-4> – <REF-6> , as well as SingleCellExperiment and monocle3 class expression objects.","Built using R, scRepertoire is a toolkit to assist in the analysis of immune profiles for both B and T cells, while interacting with the popular Seurat pipeline <REF-4> – <REF-6> .","Modify,Fact/Evidence",Fact/Evidence
11372,9-47,9-47_v2_2@4,9-47_v1_2@4,"Enabling users to easily combine mRNA and immune profiling, scRepertoire was built to process data derived from 10x Genomics Chromium Immune Profiling for both T-cell receptor (TCR) and immunoglobulin (Ig) enrichment workflows and subsequently interacts with a number of popular R packages for single-cell expression, such as Seurat.","Enabling users to easily combine mRNA and immune profiling, scRepertoire was built to process data derived from 10x Genomics Chromium Immune Profiling for both T-cell receptor (TCR) and immunoglobulin (Ig) enrichment workflows and subsequently interacts with the popular Seurat R package.","Modify,Clarity",Clarity
