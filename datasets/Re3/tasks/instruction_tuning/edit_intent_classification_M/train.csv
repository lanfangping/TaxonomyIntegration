edit_index,doc_name,node_ix_src,node_ix_tgt,text_src,text_tgt,gold,label
328,120-ARR,120-ARR_v2_28@8,120-ARR_v1_28@2,"They then defined a class of competent datasets, where the marginal probability for every feature is uniform over the class label, i.e., for any feature x i and label y ∈ Y , p(y|x i ) = 1 |Y | , thus limiting models from picking up any correlation between single features and output labels.","They then defined a class of competent datasets, where the marginal probability for every feature is uniform over the class label, i.e., ∀x ∈ X , y ∈ Y, p(y|x) = 1 |Y | , thus limiting models from picking up any correlation between single features and output labels.","Modify,Fact/Evidence",Fact/Evidence
329,120-ARR,120-ARR_v2_32@0,120-ARR_v1_32@0,"Consider the toy dataset for the task of sentiment analysis shown in Tab. 3, with vocabulary V ={good, bad, not, very}, and label set Y = { +, − }.","Consider the toy dataset for the task of sentiment analysis shown in Tab. 3, with vocabulary V ={good,bad,not,very}, and label set Y ={+,− }.","Modify,Grammar",Grammar
330,120-ARR,120-ARR_v2_35@2,120-ARR_v1_33@2,"In this case, the model would make the wrong prediction for the first test example in Test (as it has learned that very good is a feature that indicates positive sentiment), and similarly, will make a random prediction for the second test example, which does not contain any two-word feature seen during training.","In this case, the model would make the wrong prediction for the first test example in Test (as it has learned that very good is a feature that indicates positive sentiment), and similarly, will make a random prediction for the second test example, which does not contain any two-word features seen during training.","Modify,Grammar",Grammar
331,120-ARR,120-ARR_v2_43@0,120-ARR_v1_42@0,"Combining the two observations, we are left with the question of the potential intersection between balancing too much and balancing too little: does a sweet spot exist for which no spurious correlations are found in the dataset, but enough learnable signal is left? And even if so, would a balancing algorithm, whether by augmentation or filtering, be able to find it? We leave these questions for future work, but note that addressing them is a prerequisite for the theoretical and practical application of dataset balancing for mitigating spurious correlations.","Combining the two observations, we are left with the question of the potential intersection between balancing too much and balancing too little: does a sweet spot exist for which no spurious correlations are found in the dataset, but enough learnable signal is left? And even if so, would a balancing algorithm (whether by augmentation or filtering) be able to find it? We leave these questions for future work, but note that addressing them is a prerequisite for the theoretical and practical application of dataset balancing for mitigating spurious correlations.","Modify,Grammar",Grammar
332,120-ARR,120-ARR_v2_2@0,120-ARR_v1_2@0,"Recent work has shown that deep learning models in NLP are highly sensitive to lowlevel correlations between simple features and specific output labels, leading to overfitting and lack of generalization.","Recent work have shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to overfitting and lack of generalization.","Modify,Grammar",Grammar
333,120-ARR,120-ARR_v2_46@0,120-ARR_v1_44@1,"The practice of dataset balancing is designed to prevent models from learning that some words or expressions have a common fallback meaning that can stem from dataset artifacts (e.g., ""cat"" as an indicator of contradiction) but also from cultural and historical contexts (e.g., Biden is the U.S. president in 2022).","The practice of dataset balancing is designed to prevent models from learning that some words or expressions have a common fallback meaning that can stem from dataset idiosyncrasies (e.g., ""cat"" as an indicator of contradiction) but also from cultural and historical contexts (e.g., Biden is the U.S. president).","Modify,Fact/Evidence",Fact/Evidence
334,120-ARR,120-ARR_v2_46@3,120-ARR_v1_44@4,We argue that the ability to use them is a central ability of language understanding.,Here we argue that the ability to use them is a central ability of language understanding.,"Modify,Clarity",Clarity
335,120-ARR,120-ARR_v2_6@0,120-ARR_v1_6@0,"The year 2019 Donald Trump The West Wing, season 1 Josiah ""Jed"" Bartlet Table 1: Context, whether explicit or implicit, matters in textual understanding, as exemplified by the question ""who is the president of the U.S.?"". E.g., in the first line, given no other context, a QA system should provide the most sensible fallback answer (Joe Biden, at the time of writing).","The year 2019 Donald Trump The West Wing, season 1 Josiah ""Jed"" Bartlet Table 1: Context, whether explicit or implicit, matters in textual understanding, as exemplified by the question ""who is the president of the U.S.?"". E.g., in the first line, given no other context, a QA system should provide the most sensible fallback answer (Joe Biden).","Modify,Fact/Evidence",Fact/Evidence
336,120-ARR,120-ARR_v2_65@1,120-ARR_v1_61@2,"Interestingly, three different interpretation methods (simple gradient visualization, Simonyan et al., 2014;integrated gradient visualization, Sundararajan et al., 2017;and SmoothGrad, Smilkov et al., 2017) all find the word ""great"" or ""surprise"" to be one of the three most influential words on the model's prediction.","Interestingly, three different interpretation methods (simple gradient visualization, Simonyan et al., 2014;integrated gradient visualization, Sundararajan et al., 2017;and SmoothGrad, Smilkov et al., 2017) all find the word ""great"" to be one of the two most influential words on the model's prediction.","Modify,Fact/Evidence",Fact/Evidence
337,120-ARR,120-ARR_v2_63@2,120-ARR_v1_62@1,"The ability to detect when is it reasonable to make an educated guess is an important property of an intelligent agent, and an exciting research question.","The ability to detect when is it reasonable and when it is not to make an educated guess is an important property of an intelligent agent, and an exciting research question.","Modify,Clarity",Clarity
338,120-ARR,120-ARR_v2_68@1,120-ARR_v1_64@1,A naive way to mitigate spurious correlations is to stop using large-scale datasets altogether.,A naive way to eliminate spurious correlations is to stop using large-scale datasets altogether.,"Modify,Clarity",Clarity
339,120-ARR,120-ARR_v2_68@2,120-ARR_v1_64@2,"We echo recent calls and argue that for supervised learning (i.e., large-scale fine-tuning), recent advances in zero-and few-shot learning might make this option possible.","We argue that for supervised learning (i.e., large-scale fine-tuning), recent advances in zero-and few-shot learning might make this option possible.","Modify,Claim",Claim
340,120-ARR,120-ARR_v2_69@0,120-ARR_v1_65@0,"Large pretrained models such as T5 or GPT-3 (Brown et al., 2020), trained on vast amounts of data, arguably learn enough about the world to acquire many of the skills currently learned through supervised learning.","Large pretrained models such as T5 or GPT-3 (Brown et al., 2020), trained on vast amounts of data, arguably learn enough about the world to acquire many of the skills currently learned through supervised datasets.","Modify,Fact/Evidence",Fact/Evidence
341,120-ARR,120-ARR_v2_69@1,120-ARR_v1_65@1,"Indeed, the large increase in the size and capacity of pretrained language models has led to a new wave of few-shot and zero-shot methods Shin et al., 2020;Gu et al., 2022), which are able to reach human-level performance on certain tasks using only a few dozens of training examples .","Indeed, the large increase in the size and capacity of pretrained language models has led to a new wave of few-shot and zero-shot methods Shin et al., 2020;Gu et al., 2021), which are able to reach human-level performance on certain tasks using only a few dozens of training examples .","Modify,Fact/Evidence",Fact/Evidence
342,120-ARR,120-ARR_v2_71@1,120-ARR_v1_67@1,"It seems plausible that excelling in language modeling tasks requires mastering the skills that stand in the base of many NLP tasks, such as sentiment analysis, syntactic parsing, and NER.","It seems possible the excelling in language modeling tasks requires mastering the skills that stand in the base of many NLP tasks, such as sentiment analysis, syntactic parsing, and NER.","Modify,Claim",Claim
343,120-ARR,120-ARR_v2_71@2,120-ARR_v1_67@2,"However, it is similarly plausible that this is not the case for other tasks, e.g., summarization, simplification and dialogue.","However, it is similarly possible that this is not the case for other tasks, e.g., summarization, simplification and dialogue.","Modify,Clarity",Clarity
344,120-ARR,120-ARR_v2_71@3,120-ARR_v1_67@3,"We are cautious in making concrete recommendations for which tasks to apply this principle, but suggest the following intuitive rule of thumb: for datasets or tasks for which the state of the art is close to or surpasses the human baseline, we should consider moving to few-shot setups.","We are cautious to make concrete recommendations for which tasks to apply this principle, but suggest the following intuitive rule of thumb: for datasets or tasks for which the state of the art is close to or surpasses the human baseline, we should consider moving to few-shot setups.","Modify,Grammar",Grammar
345,120-ARR,120-ARR_v2_72@2,120-ARR_v1_68@2,"We suggest that instead of building large training sets and small validation and test sets, authors should consider building large test sets, as a means for achieving improved statistical power (Card et al., 2020).","We suggest instead of building large training sets and small validation and tests, authors should consider building large test sets, as a means for achieving improved statistical power (Card et al., 2020).","Modify,Clarity",Clarity
346,120-ARR,120-ARR_v2_77@0,120-ARR_v1_73@0,This paper discusses the arms-race between models and datasets.,This paper discussed the arms-race between models and datasets.,"Modify,Grammar",Grammar
347,120-ARR,120-ARR_v2_78@0,120-ARR_v1_74@0,"Raji et al. ( 2021) recently criticized the concept of benchmarks as a whole, arguing that they are only capturing specific skills and not ""general"" capabilities.","Finally, Raji et al. ( 2021) recently criticized the concept of benchmarks as a whole, arguing that they are only capturing specific skills and not ""general"" capabilities.","Modify,Clarity",Clarity
348,120-ARR,120-ARR_v2_78@1,120-ARR_v1_74@1,"Our paper raises related concerns about training sets implicitly containing spurious correlations, and suggests reconsidering the practice of building large-scale training sets.","Our paper raises relates concerns about training sets implicitly containing spurious correlations, and suggests reconsidering the practice of building large-scale training sets.","Modify,Grammar",Grammar
351,120-ARR,120-ARR_v2_9@7,120-ARR_v1_9@6,"Finally, we question the basic procedure of large-scale fine-tuning, and suggest focusing on zero-and few-shot learning instead .","We conclude by questioning the basic procedure of large-scale fine-tuning, and suggest focusing on zero-and few-shot learning.","Modify,Clarity",Clarity
352,120-ARR,120-ARR_v2_13@1,120-ARR_v1_14@1,"One approach is to add examples in order to balance the dataset (Goyal et al., 2017;Sharma et al., 2018;Hudson and Manning, 2019).","One approach, popular in visual question answering datasets, is to add examples in order to balance the dataset (Goyal et al., 2017;Hudson and Manning, 2019).","Modify,Fact/Evidence",Fact/Evidence
353,120-ARR,120-ARR_v2_14@0,120-ARR_v1_15@0,Filtering as balancing A complementary balancing approach to augmentation is filtering examples out from datasets such that spurious correlations are minimized.,Filtering as balancing A complementary approach to augmentation is filtering examples out from datasets such that spurious correlations are minimized.,"Modify,Clarity",Clarity
354,120-ARR,120-ARR_v2_14@3,120-ARR_v1_15@3,"The AF approach and similar approaches were picked up by many datasets such as ReCoRD (Zhang et al., 2018), DROP (Dua et al., 2019), HellaSWAG (Zellers et al., 2019), αNLI , and WinoGrande .","The AF approach was picked up by many follow-up datasets such as DROP (Dua et al., 2019), HellaSWAG (Zellers et al., 2019), and Wino-Grande .","Modify,Fact/Evidence",Fact/Evidence
355,120-ARR,120-ARR_v2_15@1,120-ARR_v1_16@1,"As this approach relies on an external model, applying it with ever stronger models with higher capacity, will allow these models to pick up on subtler correlations (Li et al., 2021).","As this approach relies on an external model, applying it with ever stronger models with higher capacity, will allow these models to pick up on subtler correlations.","Modify,Fact/Evidence",Fact/Evidence
356,120-ARR,120-ARR_v2_17@0,120-ARR_v1_18@0,"Evidently, a similar trend emerges for the previously mentioned datasets: (1) the first baselines, reflecting the state of the art at the time of dataset creation, perform relatively poorly, e.g., 59% on SWAG, 47% on ReCoRD, 47 F1 on DROP, 47% on HellaSWAG, 69% on αNLI, and 79% on Wino-Grande; (2) model developers introduce increasingly larger and heavily-parameterized models, hill-climbing on these datasets; and eventually (3) models essentially solve the dataset within a year or two, often outperforming humans: 86% on SWAG (Devlin et al., 2019), 94% on ReCoRD (He et al., 2021b), 88 F1 on DROP (Chen et al., 2020), 93% on HellaSWAG (He et al., 2021b), 92% on αNLI (He et al., 2021a), and 90% on Wino-Grande .","Evidently, a similar trend emerges for the previously mentioned datasets: (1) the first baselines, reflecting the state of the art at the time of dataset creation, perform poorly, e.g., 52% accuracy on SWAG, 47 F1 on DROP, 47% on Hel-laSWAG, and 53% AUC on WinoGrande; (2) model developers introduce increasingly larger and heavily-parameterized models, hill-climbing on these datasets; and eventually (3) models essentially solve the dataset within a year or two, often outperforming humans: 86% on SWAG (Devlin et al., 2019), 90 F1 on DROP , 93% on HellaSWAG (He et al., 2020), and 88% AUC on WinoGrande .","Modify,Fact/Evidence",Fact/Evidence
598,129-ARR,129-ARR_v2_23@0,129-ARR_v1_23@0,Region-aware Sampling for Active Learning on High-uncertainty Data,Region-aware Sampling for Active,"Modify,Other",Other
599,129-ARR,129-ARR_v2_24@1,129-ARR_v1_24@1,"However, directly sampling the most uncertain samples gives suboptimal results as it tends to query repetitive data (Ein-Dor et al., 2020) that represent the overall data distribution poorly.","However, directly sampling the most uncertain samples gives suboptimal result since uncertainty-based sampling tends to query repetitive data (Ein-Dor et al., 2020) and results in poor representativeness of the overall data distribution.","Modify,Clarity",Clarity
600,129-ARR,129-ARR_v2_25@2,129-ARR_v1_25@2,Denote by K the number of clusters and v (t) i = BERT(x i ) the representation of x i from the penultimate layer of BERT.,Denote K the number of clusters and v (t) i = BERT(x i ) the representation of x i from the penultimate layer of BERT.,"Modify,Grammar",Grammar
601,129-ARR,129-ARR_v2_25@3,129-ARR_v1_25@3,"The weighted K-means process first initializes the center of each each cluster µ i (1 ≤ i ≤ K) via K-Means++ (Arthur and Vassilvitskii, 2007).","The weighted K-means first initializes the center of each each cluster µ i (1 ≤ i ≤ K) via K-Means++ (Arthur and Vassilvitskii, 2007).","Modify,Clarity",Clarity
602,129-ARR,129-ARR_v2_43@0,129-ARR_v1_41@0,"Then, we rank the clusters in an ascending order in u (t) k .","Then, we rank the clusters in an ascending order according to u (t) k .","Modify,Clarity",Clarity
603,129-ARR,129-ARR_v2_43@1,129-ARR_v1_41@1,"A high score indicates high uncertainty of the model in these regions, and we need to actively annotate the member instances to reduce uncertainty and improve the model's performance.","A high score indicates high uncertainty of the model in these regions, and we need to actively annotate the associated instances to reduce uncertainty and improve the model's performance.","Modify,Clarity",Clarity
606,129-ARR,129-ARR_v2_53@0,129-ARR_v1_49@0,"Although the model is quite 'confident' on the class of x when we only consider the result of the round t, it gives contradictory predictions over these two consecutive rounds, which indicates that the model is actually uncertain to which class x belongs.","Although the model is quite 'confident' on the class of x when we only consider the result of the round t, it gives contradictory predictions over these two consecutive rounds, which indicates that the model is still uncertain to which class x belongs.","Modify,Clarity",Clarity
607,129-ARR,129-ARR_v2_4@2,129-ARR_v1_4@1,"However, there are still significant gaps between few-shot and fully-supervised PLM fine-tuning in many tasks.","However, there are still significant gaps between few-shot and fully-supervised PLM fine-tuning in many classification tasks.","Modify,Claim",Claim
608,129-ARR,129-ARR_v2_2@0,129-ARR_v1_2@0,"While pre-trained language model (PLM) finetuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data.","Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data.","Modify,Clarity",Clarity
609,129-ARR,129-ARR_v2_4@3,129-ARR_v1_4@2,"Besides, the performance of few-shot PLM finetuning can be sensitive to different sets of training data (Bragg et al., 2021).","Besides, the performance of few-shot PLM fine-tuning can vary substantially with different sets of training data (Bragg et al., 2021).","Modify,Clarity",Clarity
610,129-ARR,129-ARR_v2_4@4,129-ARR_v1_4@3,"Therefore, there is a crucial need for approaches that make PLM finetuning more label-efficient and robust to selection of training data, especially for applications where labeled data are scarce and expensive to obtain.","Therefore, there is a crucial need for PLM fine-tuning approaches with better label-efficiency and being robust to selection of training data, especially for applications where labeled data are scarce and expensive to obtain.","Modify,Clarity",Clarity
611,129-ARR,129-ARR_v2_74@0,129-ARR_v1_67@0,Weakly-supervised Learning,Extension to Weakly-supervised Learning,"Modify,Other",Other
612,129-ARR,129-ARR_v2_75@0,129-ARR_v1_68@0,"ACTUNE can be naturally used for weaklysupervised classification, where X l is a set of noisy labels derived from linguistic patterns or rules.","ACTUNE can be naturally extended to weaklysupervised classification, where X l is a set of data annotated by linguistic patterns or rules.","Modify,Fact/Evidence",Fact/Evidence
613,129-ARR,129-ARR_v2_75@1,129-ARR_v1_68@1,"Since the initial label set is noisy, the model trained with Eq. ( 1) can overfit the label noise (Zhang et al., 2022a), and we can actively query labeled data to refine the model.","Since the initial label set is noisy, then the model trained with Eq. ( 1) will overfit to the label noise, and we can actively query labeled data to refine the model.","Modify,Fact/Evidence",Fact/Evidence
614,129-ARR,129-ARR_v2_75@6,129-ARR_v1_70@2,We believe it is because CAL requires clean labels to calculate uncertainties.,We argue it is because CAL requires clean labels to calculate uncertainties.,"Modify,Clarity",Clarity
615,129-ARR,129-ARR_v2_5@0,129-ARR_v1_5@0,"Towards this goal, researchers have recently resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020;Margatina et al., 2021a,b;Yuan et al., 2020).","Towards this goal, researchers have resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020;Margatina et al., 2021a,b;Yuan et al., 2020).","Modify,Clarity",Clarity
616,129-ARR,129-ARR_v2_0@0,129-ARR_v1_0@0,ACTUNE: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models,ACTUNE: Uncertainty-Aware Active Self-Training for Active Fine-Tuning of Pretrained Language Models,"Modify,Other",Other
617,129-ARR,129-ARR_v2_85@1,129-ARR_v1_79@1,"Active learning has been widely applied to various NLP tasks (Yuan et al., 2020;Shelmanov et al., 2021;Karamcheti et al., 2021).","Active learning has been widely applied to various NLP tasks (Yuan et al., 2020;Zhao et al., 2020;Shelmanov et al., 2021;Karamcheti et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
618,129-ARR,129-ARR_v2_85@2,129-ARR_v1_79@2,"So far, AL methods can be categorized into uncertainty-based methods (Gal et al., 2017;Margatina et al., 2021a,b), diversity-based methods (Ru et al., 2020;Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020;Ash et al., 2020).","So far, AL methods can be categorized into uncertainty-based methods (Gal et al., 2017;Margatina et al., 2021a,b), diversitybased methods (Ru et al., 2020;Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020;Ash et al., 2020;Kirsch et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
621,129-ARR,129-ARR_v2_85@12,129-ARR_v1_79@10,"It first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability.","Self-training first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability (Rosenberg et al., 2005;Lee, 2013).","Modify,Fact/Evidence",Fact/Evidence
622,129-ARR,129-ARR_v2_86@0,129-ARR_v1_80@0,Conclusion and Discussion,Conclusion,"Modify,Other",Other
623,129-ARR,129-ARR_v2_5@4,129-ARR_v1_5@3,"Moreover, they usually rely on pseudo-labeling to utilize unlabeled data, which requires greater (yet often absent) care to denoise the pseudo labels, otherwise the errors could accumulate and hurt model performance.","Moreover, they usually rely on pseudo-labeling to utilize unlabeled data, which requires greater (yet often absent) care to denoise the pseudo labels, otherwise the errors could accumulate and deteriorate the model performance.","Modify,Clarity",Clarity
624,129-ARR,129-ARR_v2_5@5,129-ARR_v1_5@4,"This issue can be even more severe for PLMs, as the fine-tuning process is often sensitive to different weight initialization and data orderings (Dodge et al., 2020).","This phenomenon can be even more severe for PLMs, as the finetuning process often suffers from the instability issue caused by different weight initialization and data orders (Dodge et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
625,129-ARR,129-ARR_v2_88@5,129-ARR_v1_87@4,"Last, apart from the text classification task, we can also extend our work into other tasks such as sequence labeling and natural language inference (NLI).","Last, apart from the text classification task, we can also extend our work into other tasks such as sequence labeling and natural language inference.","Modify,Clarity",Clarity
626,129-ARR,129-ARR_v2_2@1,129-ARR_v1_2@1,"Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data.","Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data.","Modify,Clarity",Clarity
627,129-ARR,129-ARR_v2_83@0,129-ARR_v1_88@0,We give an example of our querying strategy on AG News dataset for the 1st round of active selftraining process in figure 6.,Here we give an example of our querying strategy on AG News and Pubmed dataset for the 1st round of active self-training process in figure 6.,"Modify,Fact/Evidence",Fact/Evidence
628,129-ARR,129-ARR_v2_83@2,129-ARR_v1_88@3,"We can see that the existing uncertainty-based methods such as Entropy and CAL, are suffered from the issue of limited diversity.","From the comparision, we can see that the existing uncertainty based methods such as Entropy and CAL, are suffered from the issue of limited diversity.","Modify,Clarity",Clarity
629,129-ARR,129-ARR_v2_6@0,129-ARR_v1_6@0,"To tackle the above challenges, we propose AC-TUNE, a new method that improves the label efficiency and robustness of active PLM fine-tuning.","To tackle above challenges, we propose AC-TUNE, a new method that improves the label efficiency and robustness of active PLM fine-tuning with self-training.","Modify,Clarity",Clarity
630,129-ARR,129-ARR_v2_6@2,129-ARR_v1_7@1,"Different from existing AL methods that only leverage uncertainty for querying labels, our uncertainty-driven self-training paradigm gradually leverages unlabeled data with low uncertainty via self-training, while reducing the chance of error propagation triggered by highly-uncertain mislabeled data.","Different from existing AL methods that only leverage uncertainty for querying labels, our uncertaintydriven self-training paradigm gradually unleash the data with low uncertainty via self-training, while reducing the chance of error propagation triggered by highly-uncertain mis-labeled data.","Modify,Clarity",Clarity
631,129-ARR,129-ARR_v2_7@0,129-ARR_v1_8@0,"To further boost model performance for AC-TUNE, we design two techniques to improve the query strategy and suppress label noise, namely region-aware sampling (RS) and momentum-based memory bank (MMB).","To further boost the performance on downstream tasks, we design two techniques, namely regionaware sampling (RS) and momentum-based memory bank (MMB) to improve the query strategies and suppress label noise for ACTUNE.","Modify,Clarity",Clarity
632,129-ARR,129-ARR_v2_7@1,129-ARR_v1_8@1,"Inspired by the fact that existing uncertainty-based AL methods often end up with choosing uncertain yet repetitive data (Ein-Dor et al., 2020;Margatina et al., 2021b), we design the region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs.","Inspired by the fact that existing uncertainty-based AL methods often end up choosing uncertain yet repetitive data (Ein-Dor et al., 2020;Margatina et al., 2021b), we design a region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs.","Modify,Grammar",Grammar
633,129-ARR,129-ARR_v2_2@2,129-ARR_v1_2@2,"We propose ACTUNE, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning.","We develop ACTUNE, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self training.","Modify,Fact/Evidence",Fact/Evidence
634,129-ARR,129-ARR_v2_8@2,129-ARR_v1_9@2,"However, previous approaches only select pseudo-labeled data based on the prediction of the current round and are thus less reliable.","However, previous approaches only select pseudo-labeled data based on the prediction of the current round and therefore are less reliable.","Modify,Clarity",Clarity
635,129-ARR,129-ARR_v2_8@5,129-ARR_v1_9@5,"As a result, only the samples with high prediction confidence over multiple rounds will be used for self-training, which mitigates the issue of label noise.","As a consequence, only the samples with high prediction confidence over multiple rounds will be used for self-training, which mitigates the issue of label noise.","Modify,Clarity",Clarity
636,129-ARR,129-ARR_v2_8@6,129-ARR_v1_9@6,"We highlight that our active self-training approach is an efficient substitution to existing AL methods, requiring little extra computational cost.","We highlight that our active self-training approach is an efficient substitution to existing AL methods, requiring ignorable extra computational cost.","Modify,Clarity",Clarity
637,129-ARR,129-ARR_v2_9@0,129-ARR_v1_10@0,Our key contributions are: (1) an active selftraining paradigm ACTUNE that integrates selftraining and active learning to minimize the labeling cost for fine-tuning PLMs; (2) a region-aware querying strategy to enforce both the informativeness and the diversity of queried samples during AL; (3) a simple and effective momentum-based method to leverage the predictions in preceding rounds to alleviate the label noise in self-training and (4) experiments on 6 benchmarks demonstrating ACTUNE improves the label efficiency over existing self-training and active learning baselines by 56.2%.,Our key contributions are: (1) an active selftraining paradigm ACTUNE that integrates the benefit of self-training and active learning in a principled way to minimize the labeling cost for finetuning PLMs; (2) a region-aware querying strategy to enforce both the informativeness and the diversity of queried samples during AL; (3) a simple and effective momentum-based method to harness the predictions for preceding rounds to alleviate the label noise in self-training and (4) experiments on 6 benchmarks demonstrating ACTUNE improves the label efficiency over existing self-training and active learning baselines by 56.2%.,"Modify,Clarity",Clarity
638,129-ARR,129-ARR_v2_2@3,129-ARR_v1_2@3,ACTUNE switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and lowuncertainty ones for model self-training.,"AC-TUNE switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from lowuncertainty regions are used for model selftraining.","Modify,Clarity",Clarity
639,129-ARR,129-ARR_v2_14@1,129-ARR_v1_15@1,"Here X = X l ∪ X u denotes all samples, and Y = {1, 2, • • • , C} is the label set where C is the number of classes.","Here X = X l ∪ X u denotes all samples and Y = {1, 2, • • • , C} is the label set, where C is the number of classes.","Modify,Grammar",Grammar
640,129-ARR,129-ARR_v2_18@0,129-ARR_v1_19@0,"In round t (1 ≤ t ≤ T ) of active self-training, we first calculate the uncertainty score based on a given function a (t) i = a(x i , θ (t) ) 1 for all x i ∈ X u .","In round t (1 ≤ t ≤ T ) of the active self-training procedure, we first calculate the uncertainty score based on a given function a (t) i = a(x i , θ (t) ) 1 for all x i ∈ X u .","Modify,Clarity",Clarity
641,129-ARR,129-ARR_v2_2@4,129-ARR_v1_2@4,"Under this framework, we design (1) a regionaware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training.","Additionally, we design (1) a regionaware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model's pseudo labels to suppress label noise in self-training.","Modify,Clarity",Clarity
831,136-ARR,136-ARR_v2_18@1,136-ARR_v1_21@0,"Given a set of triples X on the input, we:","We proceed as follows -given an input X, we:","Modify,Clarity",Clarity
832,136-ARR,136-ARR_v2_22@0,136-ARR_v1_29@0,Ordering the Facts,Ordering,"Modify,Other",Other
833,136-ARR,136-ARR_v2_23@0,136-ARR_v1_30@0,We assume that the default order of triples X is random and the same applies for the respective facts F .,We assume that the default order of triples X (and the respective facts F ) is random.,"Modify,Clarity",Clarity
834,136-ARR,136-ARR_v2_25@0,136-ARR_v1_31@0,Aggregating the Facts,Aggregation,"Modify,Other",Other
835,136-ARR,136-ARR_v2_27@1,136-ARR_v1_32@1,"Conversely, δ i "" 0 means that the facts should be aggregated and their corresponding sentences should be fused.","Conversely, δ i "" 0 means that the facts should be aggregated and their corresponding sentences should be fused (see §5.2 and §5.3).","Modify,Fact/Evidence",Fact/Evidence
836,136-ARR,136-ARR_v2_29@1,136-ARR_v1_34@1,"It has two main objectives: (a) fusing related sentences, i.e., sentences i and j in between which δ i "" 0, and (b) rephrasing the text to improve its fluency, e.g. fixing disfluencies in the templates, replacing noun phrases with refering expressions, etc.","The objectives of the model are two-fold: (a) fusing related sentences, i.e., sentences i and j in between which δ i "" 0, and (b) rephrasing the text to improve its fluency, e.g. fixing minor disfluencies in the templates, replacing noun phrases with refering expressions, etc.","Modify,Clarity",Clarity
837,136-ARR,136-ARR_v2_29@2,136-ARR_v1_34@2,"The goal of the task is to preserve the semantics of the text which is an already ordered sequence of sentences, so the edits will typically be minor.",The focus is on minor rephrasing since the goal is to preserve the semantics of the original text.,"Modify,Fact/Evidence",Fact/Evidence
838,136-ARR,136-ARR_v2_33@0,136-ARR_v1_36@1,Our goal is to cover a broad range of domains while capturing the sentence style in D2T generation with respect to both the input facts and the target descriptions.,"Our corpus needs to cover a broad range of domains while capturing the sentence style in D2T generation, both regarding the input templates and the target descriptions.","Modify,Clarity",Clarity
839,136-ARR,136-ARR_v2_42@6,136-ARR_v1_46@3,"To balance the length of inputs, we selected 250k examples each from 4 equally sized length ranges (30-130 characters, etc.).","To further ensure that the length of inputs is balanced, we selected 250k examples each from 4 equidistant length ranges (30-130 characters, etc.).","Modify,Clarity",Clarity
840,136-ARR,136-ARR_v2_44@0,136-ARR_v1_48@0,"To generate a set of simple sentences, we divide each paragraph into sentences using NLTK (Bird, 2006) and apply a split-and-rephrase model on each sentence.","For generating the target set of sentences, we divide each paragraph into sentences using NLTK (Bird, 2006) and apply a split-and-rephrase model on each sentence.","Modify,Clarity",Clarity
841,136-ARR,136-ARR_v2_45@0,136-ARR_v1_48@2,"We train our split-and-rephrase model on the large-scale WikiSplit corpus by Botha et al. (2018), containing human-made sentence splits from Wikipedia edit history.","We train our model on the large-scale WikiSplit corpus by Botha et al. (2018), containing human-made sentence splits from Wikipedia edit history.","Modify,Clarity",Clarity
842,136-ARR,136-ARR_v2_45@1,136-ARR_v1_48@3,"Following the same setup as for a paragraph compression model ( §3.4), we train BART-base (Lewis et al., 2020) on the WikiSplit dataset in a sequence-to-sequence setting.","Following the setup in the rest of our experiments, we train the encoder-decoder PLM BART-base (Lewis et al., 2020) on the WikiSplit dataset in a sequence-to-sequence setting.","Modify,Fact/Evidence",Fact/Evidence
843,136-ARR,136-ARR_v2_45@2,136-ARR_v1_48@4,"Next, we apply the trained split-and-rephrase model on each sentence in our Wikipedia-based corpus, uniformly randomly choosing between 0-2 recursive calls to ensure that the splits are not deterministic.","We apply the trained split-and-rephrase model on each sentence, uniformly randomly choosing between 0-2 recursive calls to ensure that the splits are not deterministic.","Modify,Fact/Evidence",Fact/Evidence
844,136-ARR,136-ARR_v2_47@0,136-ARR_v1_50@0,"As the next step, we concatenate the split sentences and apply a coreference resolution model (Gardner et al., 2018;Lee et al., 2018) in order to replace referring expressions with their antencendents (e.g., pronouns with noun phrases).","Next, we concatenate the split sentences and apply a coreference resolution model (Gardner et al., 2018) in order to replace referring expressions with their antencendents (e.g., pronouns with noun phrases).","Modify,Fact/Evidence",Fact/Evidence
845,136-ARR,136-ARR_v2_47@1,136-ARR_v1_50@1,"The motivation for this step is to match the style of the facts (see §3.1), which do not use pronouns since each fact describes a single triple only.",This allows to better follow the style of the templates in which the entities are always fully verbalized.,"Modify,Fact/Evidence",Fact/Evidence
846,136-ARR,136-ARR_v2_49@2,136-ARR_v1_52@2,"In a filtered version of the WIKIFLUENT corpus, we include only the examples without omissions or hallucinations (as computed by the model), reducing it to 714k examples (approximately 75% of the original size).","In a filtered version of the WIK-IFLUENT corpus, we include only the examples without omissions or hallucinations (as computed by the model), reducing it to approximately 3/4 of the original size.","Modify,Fact/Evidence",Fact/Evidence
847,136-ARR,136-ARR_v2_56@1,136-ARR_v1_56@2,We provide a short description of the model here; for details please refer to Calizzano et al. (2021).,We provide a short description of the model here; for details see Calizzano et al. (2021).,"Modify,Clarity",Clarity
848,136-ARR,136-ARR_v2_59@0,136-ARR_v1_59@0,"The pointer network computes the probability of a fact to be on the j-th position, using the encoder output E and the decoder output state d j .","The pointer network computes the probability of a fact to be on the j-th position, using the encoder output E and the decoder output d j .","Modify,Clarity",Clarity
849,136-ARR,136-ARR_v2_64@0,136-ARR_v1_61@1,"We train the model using the synthesized simple sentences in the WIKIFLUENT corpus, randomly shuffling the order of the sentences and training the model to restore their original order.","We train the model using the split sentences in the WIKIFLUENT corpus, randomly shuffling the order of the sentences and training the model to restore their original order.","Modify,Clarity",Clarity
850,136-ARR,136-ARR_v2_66@3,136-ARR_v1_63@3,We ignore the outputs for the nonseparator tokens while computing cross-entropy loss.,We ignore the outputs for the non-separator tokens while computing the cross-entropy loss.,"Modify,Grammar",Grammar
851,136-ARR,136-ARR_v2_67@0,136-ARR_v1_64@0,"We create the training examples using the synthesized sentences in the WIKIFLUENT corpus, in which we set δ i "" 0 for the sentences i, i`1 which were originally aggregated (i.e., are the result of splitting a single sentence) and δ i "" 1 otherwise.","We create the training examples using the split sentences in the WIKIFLUENT corpus, in which we set δ i "" 0 for the sentences i, i `1 which were originally aggregated (i.e., are the result of splitting a single sentence) and δ i "" 1 otherwise.","Modify,Clarity",Clarity
852,136-ARR,136-ARR_v2_69@1,136-ARR_v1_66@1,"We finetune the model on the WIK-IFLUENT corpus, concatenating the synthesized sentences on the input.","We train the model in a sequenceto-sequence setting on the WIKIFLUENT corpus, concatenating the split sentences on the input.","Modify,Clarity",Clarity
853,136-ARR,136-ARR_v2_69@2,136-ARR_v1_66@2,"We add delimiters between the sentences i and i `1 where δ i "" 1 using a special token <sep>, which we add to the model vocabulary.","We add delimiters between sentences i and i `1 where δ i "" 1 using a special token <sep>, which we add to the model vocabulary.","Modify,Grammar",Grammar
854,136-ARR,136-ARR_v2_70@1,136-ARR_v1_66@4,We evaluate how the model learns to respect the order and aggregation markers in §7.3.,We evaluate our model's behavior with respect to ordering and aggregation in §6.3.,"Modify,Clarity",Clarity
855,136-ARR,136-ARR_v2_6@1,136-ARR_v1_6@1,Gathering a large set of references for a particular domain is also costly and time-consuming as it usually requires collecting human-written references through crowdsourcing .,"Moreover, collecting a large set of references for a particular domain is costly and time-consuming, as the data are usually collected using crowdsourcing .","Modify,Clarity",Clarity
856,136-ARR,136-ARR_v2_73@0,136-ARR_v1_71@1,"Datasets The datasets differ in domain, size, textual style, and number of predicates (see Appendix A for details):","They differ in domain, size, textual style, and number of predicates (see Appendix A for details).","Modify,Clarity",Clarity
857,136-ARR,136-ARR_v2_78@0,136-ARR_v1_73@0,Evaluation and Discussion,Evaluation,"Modify,Other",Other
858,136-ARR,136-ARR_v2_79@1,136-ARR_v1_74@0,"We evaluate outputs from the {1,2,3}-STAGE variants of our pipeline using automatic metrics ( §7.1), and we perform a detailed manual error analysis of the model outputs ( §7.2).","We evaluate outputs from the {1,2,3}-STAGE variants of our pipeline automatically ( §6.1) and manually ( §6.2).","Modify,Fact/Evidence",Fact/Evidence
859,136-ARR,136-ARR_v2_79@2,136-ARR_v1_74@1,We also evaluate the performance of the content planning modules and the ability of the PC module to follow the content plan ( §7.3).,"Further, we evaluate the performance of the content planning modules and the ability of the PC module to follow the content plan ( §6.3).","Modify,Clarity",Clarity
861,136-ARR,136-ARR_v2_83@0,136-ARR_v1_79@0,"The automatic evaluation shows that our systems consistently outperform the COPY baseline (e.g., ""12 BLEU points for E2E), which is already strong thanks to our manually curated set of templates.","The automatic evaluation suggests that while our system lags behind state-of-the-art supervised systems, it shows considerable improvements compared to the COPY baseline (e.g., ""12 BLEU points pendix C for the details. for E2E) and matches performance of some older supervised systems.","Modify,Claim",Claim
863,136-ARR,136-ARR_v2_84@0,136-ARR_v1_79@2,"The 2-STAGE system is generally on par with the 3-STAGE system or better, which indicates that explicit aggregation using the AGG model may not be necessary.","The 2-STAGE system is generally on par with the 3-STAGE system (or better), which indicates that implicit aggregation using the PC-AGG model may be sufficient.","Modify,Claim",Claim
864,136-ARR,136-ARR_v2_84@2,136-ARR_v1_79@4,"The models using the filtered version of the corpus generally produce better results, although they also bring in a larger number of omissions.","The filtered version of the dataset generally brings better results, although it brings also an increase in the number of omissions.","Modify,Clarity",Clarity
865,136-ARR,136-ARR_v2_86@1,136-ARR_v1_81@0,"We counted the number of errors: factual (hallucinations, omissions, incorrect fact merging, redundancies) and grammatical.","We manually evaluated 100 outputs of the models regarding factual errors (hallucinations, omissions, incorrect fact merging, redundancies) as well as grammatical errors.","Modify,Fact/Evidence",Fact/Evidence
866,136-ARR,136-ARR_v2_87@1,136-ARR_v1_81@3,"These problems are largely eliminated with the 2-STAGE and 3-STAGE models, which produce Wildwood is a restaurant.","These problems are only slightly reduced in the filtered version, but they are largely eliminated with 2-STAGE and 3-STAGE models.","Modify,Claim",Claim
867,136-ARR,136-ARR_v2_89@1,136-ARR_v1_81@9,"Grammar errors and disfluencies stem mainly from over-eager paragraph compression or from artifacts in our templates and are relatively minor (e.g., missing ""is"" in ""serves French food and family-friendly"").","The grammar errors and disfluencies stem mainly from over-eager paragraph compression or from artifacts in our templates; they are relatively minor (e.g., missing ""is"" in ""serves French food and family-friendly"").","Modify,Clarity",Clarity
868,136-ARR,136-ARR_v2_91@0,136-ARR_v1_84@0,"Following and , we report the accuracy and BLEU-2 score of our ordering model on WebNLG against the humangenerated plans from Ferreira et al. (2018).","Following and , we report the accuracy (Acc) and BLEU-2 score (B-2) of our ordering model on WebNLG against the human-generated plans from Ferreira et al. (2018).","Modify,Clarity",Clarity
869,136-ARR,136-ARR_v2_2@1,136-ARR_v1_2@1,We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs.,We examine how to avoid finetuning the pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs.,"Modify,Grammar",Grammar
870,136-ARR,136-ARR_v2_7@0,136-ARR_v1_7@0,"In this paper, we present a zero-shot alternative to the traditional finetuning paradigm by formulating the D2T generation from RDF triples as a sequence of general-domain operations over text in natural language.","In this paper, we provide an alternative to this traditional paradigm by formulating the D2T generation from RDF triples as a sequence of generaldomain operations over text in natural language.","Modify,Clarity",Clarity
871,136-ARR,136-ARR_v2_92@0,136-ARR_v1_89@1,"Finally, we manually evaluate how the PC model follows the content plan (i.e., keeping the predefined order and aggregating the sentences according to the delimiters) using 100 randomly chosen examples with more than 1 triple on WebNLG and E2E.","Finally, we manually evaluate how the PC model follows the content plan using 100 randomly chosen examples with more than 1 triple on WebNLG and E2E.","Modify,Fact/Evidence",Fact/Evidence
872,136-ARR,136-ARR_v2_92@2,136-ARR_v1_89@3,The incorrect cases include a fact not properly mentioned or an extra boundary between sentences without a separator.,The incorrect cases include a fact not properly mentioned and an extra boundary between the sentences without a separator.,"Modify,Fact/Evidence",Fact/Evidence
873,136-ARR,136-ARR_v2_95@0,136-ARR_v1_90@0,Future Work,Discussion and Future Work,"Modify,Other",Other
874,136-ARR,136-ARR_v2_96@1,136-ARR_v1_92@1,"Automatic generation of facts without using hand-crafted templates (cf. §5.1) could allow applying zero-shot generation systems to datasets with a large number of predicates, such as ToTTo (Parikh et al., 2020).","Generating simple statements from the triples automatically, e.g., using the approach of Laha et al. (2020), could reduce the manual workload and allow applying our approach on datasets with a less constrained set of data attributes such as ToTTo (Parikh et al., 2020) or DART (Nan et al., 2021).","Modify,Claim",Claim
875,136-ARR,136-ARR_v2_97@0,136-ARR_v1_93@0,"More research is also needed regarding the main shortcoming of our approach, i.e., the semantic errors stemming from merging of facts in improper ways.",More research is also needed on semantic errors stemming from merging of facts in improper ways.,"Modify,Clarity",Clarity
876,136-ARR,136-ARR_v2_97@1,136-ARR_v1_93@1,"As we suggested in §7.2, explicitly controlling the semantics of sentence fusion could help to mitigate this issue, while still keeping the advantages of a zero-shot approach.","We suggest that explicitly controlling the semantics of sentence fusion (Ben-David et al., 2020) could help to mitigate this issue, while still keeping the advantages of a zero-shot approach.","Modify,Fact/Evidence",Fact/Evidence
877,136-ARR,136-ARR_v2_103@0,136-ARR_v1_95@0,"We implemented the models for split-and-rephrase, aggregation, and paragraph compression in Py-Torch Lightning (Paszke et al., 2019), using the PyTorch (Falcon et al., 2019) version of the BART and RoBERTa models from the Huggingface library (Wolf et al., 2019).","We implemented the models for split-and-rephrase, aggregation, and paragraph compression in Py-Torch Lightning (Paszke et al., 2019), using the PyTorch (Falcon, 2019) version of the BART and RoBERTa models from the Huggingface library (Wolf et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
878,136-ARR,136-ARR_v2_105@1,136-ARR_v1_97@1,We will integrate the ordering model into our framework.,We plan to fully integrate the ordering model into our framework in the future.,"Modify,Clarity",Clarity
882,136-ARR,136-ARR_v2_7@4,136-ARR_v1_7@4,Our approach allows generating natural language descriptions from RDF triples with a minimum amount of domain-specific rules or knowledge and without using training data from the D2T datasets.,Our approach allows generating natural language descriptions from triples with a minimum amount of domain-specific rules or knowledge and without using training data from the D2T datasets.,"Modify,Fact/Evidence",Fact/Evidence
883,136-ARR,136-ARR_v2_7@5,136-ARR_v1_7@5,"Although our approach is primarily a probe into the territory of zero-shot approaches and cannot yet match the quality of stateof-the-art models, we show that it can yield large improvements upon simple baselines and match older supervised systems on automatic metrics for text fluency.","We show that our approach can yield large improvements upon simple baselines and match older supervised systems in terms of fluency, while bringing potential for further improvements and advantages with respect to controllability.","Modify,Claim",Claim
884,136-ARR,136-ARR_v2_2@2,136-ARR_v1_2@2,"Inspired by pipeline approaches, we propose to generate text by transforming single-item descriptions with a sequence of modules trained on generaldomain text-based operations: ordering, aggregation, and paragraph compression.","Inspired by pipeline approaches, we propose to generate text by rephrasing single-item templates using a sequence of modules trained on general-domain text-based operations-ordering, aggregation, and paragraph compression.","Modify,Clarity",Clarity
885,136-ARR,136-ARR_v2_10@1,136-ARR_v1_10@1,"Following Chen et al. (2020b), other works adopted PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a).","Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a).","Modify,Fact/Evidence",Fact/Evidence
886,136-ARR,136-ARR_v2_12@0,136-ARR_v1_12@0,"Content Planning in D2T Generation Content planning, i.e. the task of ordering input facts and aggregating them into individual sentences, is one of the steps of the traditional D2T pipeline (Gatt and Krahmer, 2018).","Content Planning in D2T Generation Content planning, i.e. ordering input facts and aggregating them into individual sentences, is a traditional part of the D2T generation pipeline (Ferreira et al., 2019;Gatt and Krahmer, 2018;Reiter and Dale, 1997).","Modify,Fact/Evidence",Fact/Evidence
889,136-ARR,136-ARR_v2_14@0,136-ARR_v1_14@0,"Aggregating Input into Sentences Typically, multiple pieces of input information need to be merged into a single sentence.",Fact Aggregation The compact nature of the target text description results in aggregating multiple facts in a single sentence.,"Modify,Claim",Claim
890,136-ARR,136-ARR_v2_14@2,136-ARR_v1_14@2,"Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts (see §3.1), into which we selectively insert delimiters to mark sentence boundaries.","Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts, in which we selectively insert delimiters marking sentence boundaries.","Modify,Fact/Evidence",Fact/Evidence
891,136-ARR,136-ARR_v2_15@1,136-ARR_v1_14@4,"This task combines several standard natural-language tasks including sentence fusion, rephrasing, and coreference resolution.","As the last step in the pipeline, it is closely related to linguistic realisation, howeversince we already work with natural language in this step-the focus of our task is on sentence fusion, rephrasing, and coreference resolution.","Modify,Fact/Evidence",Fact/Evidence
892,136-ARR,136-ARR_v2_19@1,136-ARR_v1_16@1,"The individual steps are described in the following sections: transforming individual triples to text ( §3.1), ordering ( §3.2), aggregation ( §3.3), and paragraph compression ( §3.4).","Next, we describe the individual steps, starting by applying simple templates to transform data to text ( §3.2), followed by individual modules for ordering ( §3.3), aggregation ( §3.4), and paragraph compression ( §3.5).","Modify,Fact/Evidence",Fact/Evidence
1349,166-ARR,166-ARR_v2_20@1,166-ARR_v1_20@1,"While this ranker architecture approach is simple (and can also be used for the retrieval step via an approximate nearest neighbor search such as FAISS (Johnson et al., 2017), ScaNN (Guo et al., 2020) or the Pinecone managed service 2 ), the overall ranking quality is generally lower compared to methods that employ a query-document crossattention interaction.","While this ranker architecture approach is simple (and can also be used for the retrieval step via an approximate nearest neighbor search such as FAISS (Johnson et al., 2017) or ScaNN (Guo et al., 2020)), the overall ranking quality is generally lower compared to methods that employ a query-document cross-attention interaction.","Modify,Claim",Claim
1350,166-ARR,166-ARR_v2_38@0,166-ARR_v1_40@0,"To remedy this issue, we follow an approach similar to EDEN quantization (Vargaftik et al., 2022), which uses a randomized Hadamard transform prior to quantization.","To remediate this issue, we follow the DRIVE approach (Vargaftik et al., 2021a), which uses a randomized Hadamard transform prior to quantization.","Modify,Fact/Evidence",Fact/Evidence
1355,166-ARR,166-ARR_v2_48@2,166-ARR_v1_51@2,"For MSMARCO, we initialized the model from reduced width pre-trained weights 4 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.","For MS-MARCO, we initialized the model from reduced width pre-trained weights 2 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.","Modify,Fact/Evidence",Fact/Evidence
1356,166-ARR,166-ARR_v2_51@0,166-ARR_v1_54@0,"In the following sections, we denote the SDR variants as ""AESI-{c}-{B}b"" where {c} is replaced with the width of the encoded vector and {B} is replaced with the number of bits in the quantization scheme.","It the following sections, we denote the SDR variants as ""AESI-{c}-{B}b"" where {c} is replaced with the width of the encoded vector and {B} is replaced with the number of bits in the quantization scheme.","Modify,Grammar",Grammar
1357,166-ARR,166-ARR_v2_54@0,166-ARR_v1_57@0,Evaluation Results,Evaluation,"Modify,Other",Other
1358,166-ARR,166-ARR_v2_57@1,166-ARR_v1_62@0,The Distilbert model (full interaction architecture) has the highest quality and smallest index size (since it is only executed online).,The Distilbert model has the highest quality and smallest index size (since it is only executed online).,"Modify,Fact/Evidence",Fact/Evidence
1359,166-ARR,166-ARR_v2_59@2,166-ARR_v1_63@5,"The late interaction variants we consider have the same configuration as in the MS-MARCO case, where the baseline uses 24 features (with float16 quantization) and SDR uses 16 features (with 6 bits EDEN quantization).","The late interaction variants we consider have the same configuration as in the MS-MARCO case, where the baseline uses 24 features (with float16 quantization) and SDR uses 16 features (with 6 bits DRIVE quantization).","Modify,Fact/Evidence",Fact/Evidence
1360,166-ARR,166-ARR_v2_71@2,166-ARR_v1_73@2,"Using a quantization technique fitted to the Gaussian distribution of post randomized Hadamard transform data further improve quality, making the EDEN quantization superior to other quantization techniques in our case.","Using a quantization technique fitted to the Gaussian distribution of post randomized Hadamard transform data further improve quality, making the DRIVE quantization superior to other quantization techniques in our case.","Modify,Fact/Evidence",Fact/Evidence
1361,166-ARR,166-ARR_v2_73@0,166-ARR_v1_75@0,"To better understand the impact of side information, we measure the error rate between an input vector and its reconstructed vector (i.e., after encoding and decoding).","To better understand the impact of side information, we measure the error rate between an input vector and a reconstructed vector (i.e., after encoding and decoding) for different input vectors.","Modify,Fact/Evidence",Fact/Evidence
1362,166-ARR,166-ARR_v2_74@0,166-ARR_v1_76@0,"In IR, the document frequency of a token is known to be negatively correlated with the token's importance.","In the information retrieval field, the document frequency of a token is known to negatively correlated with the token's importance.","Modify,Clarity",Clarity
1363,166-ARR,166-ARR_v2_74@2,166-ARR_v1_76@2,This shows that the AESI scheme can better focus on tokens that are important for ranking.,This shows that the AESI scheme has a better focus on tokens that are important for ranking.,"Modify,Clarity",Clarity
1364,166-ARR,166-ARR_v2_74@3,166-ARR_v1_76@3,"A possible explanation for this phenomena is that the static embeddings for infrequent tokens are more informative (i.e., more helpful as side information) compared to static embeddings for frequent tokens (e.g., 'the').","A possible explanation for this phenomena is that the static embeddings for infrequent tokens is more informative (i.e., more helpful as side information) compared to static embeddings for frequent tokens (e.g., 'the').","Modify,Grammar",Grammar
1365,166-ARR,166-ARR_v2_74@4,166-ARR_v1_76@4,"We also found AESI excels more in compressing nouns, verbs, and adjectives, while AE-2L excels more in compressing punctuation, determiners, and adpositions.","We also found AESI excels more in compressing nouns, verbs, and adjectives, while AE-2L excels more in compressing punctuations, determiners, and adpositions.","Modify,Grammar",Grammar
1366,166-ARR,166-ARR_v2_74@6,166-ARR_v1_76@6,The details of this evaluation are provided in Appendix C.,The details of this evaluation appear in Appendix C.,"Modify,Clarity",Clarity
1367,166-ARR,166-ARR_v2_76@0,166-ARR_v1_78@0,"In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction transformer based models for passage re-ranking.","In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction models.","Modify,Fact/Evidence",Fact/Evidence
1368,166-ARR,166-ARR_v2_76@2,166-ARR_v1_78@2,"In addition, we explored different quantization techniques and showed that the recently proposed EDEN performs well in our use case and presented extensive experimentation.","In addition, we explored different quantization techniques and showed that the recently proposed DRIVE performs well in our use case and presented extensive experimentation.","Modify,Fact/Evidence",Fact/Evidence
1369,166-ARR,166-ARR_v2_87@2,166-ARR_v1_89@2,"In addition to EDEN (Appendix A, Algorithm 1), we consider the following quantization strategies: Deterministic Rounding (DR) (Gersho and Gray, 1992).","In addition to DRIVE (Appendix A, Algorithm 1), we consider the following quantization strategies: Deterministic Rounding (DR) (Gersho and Gray, 1992).","Modify,Fact/Evidence",Fact/Evidence
1370,166-ARR,166-ARR_v2_87@12,166-ARR_v1_89@12,"EDEN with Bias Correction (EDEN-BC) (Vargaftik et al., 2022, Section 2.3).","DRIVE with Bias Correction (DRIVE-BC) (Vargaftik et al., 2021b, Appendix C.3).","Modify,Fact/Evidence",Fact/Evidence
1371,166-ARR,166-ARR_v2_87@13,166-ARR_v1_89@13,"This variant of EDEN optimizes for lower bias over the mean squared error (MSE) by multiplying the dequantization result in Algorithm 1 by a bias correction scalar: x 2 2 / H(x), ŷ .",This variant of DRIVE optimizes for lower bias over the mean squared error (MSE) by multiplying the dequantization result in Algorithm 1 by a bias correction scalar: x 2 2 / ŷ 2 2 .,"Modify,Fact/Evidence",Fact/Evidence
1372,166-ARR,166-ARR_v2_88@2,166-ARR_v1_90@2,"Second, we see that EDEN performs better than all other schemes.","Second, we see that DRIVE performs better than all other schemes.","Modify,Fact/Evidence",Fact/Evidence
1373,166-ARR,166-ARR_v2_88@5,166-ARR_v1_90@5,"This conclusion follows by observing that EDEN and the deterministic rounding methods (DR, H-DR) are respectively better than EDEN-BC and the stochastic rounding methods (SR, H-SR).","This conclusion follows by observing that DRIVE and the deterministic rounding methods (DR, H-DR) are respectively better than DRIVE-BC and the stochastic rounding methods (SR, H-SR).","Modify,Fact/Evidence",Fact/Evidence
1374,166-ARR,166-ARR_v2_100@1,166-ARR_v1_102@1,"We compare the baseline -an autoencoder with 2 layers and float16 quantization -to SDR scheme, with the same number of features and EDEN 6bits quantization.","We compare the baseline -an autoencoder with 2 layers and float16 quantization -to SDR scheme, with the same number of features and DRIVE 6bits quantization.","Modify,Fact/Evidence",Fact/Evidence
1377,166-ARR,166-ARR_v2_100@6,166-ARR_v1_102@5,"Finally, with the largest size of features we tested, 64, the baseline reached a MAP@1K score of 0.313, similar to the score achieved by SDR-20 (0.314), demonstrating the effectiveness of the static embeddings as side information.","Finally, with the largest size of features we tested, 64, the baseline reached a MAP@1K score of 0.313, similar to the score achieved by SDR-20 (0.315), demonstrating the effectiveness of the static embeddings as side information.","Modify,Fact/Evidence",Fact/Evidence
1378,166-ARR,166-ARR_v2_13@2,166-ARR_v1_13@2,"To improve the compression-reliability tradeoff, we leverage static token embeddings, which are available since the ranker has access to the document text (as it needs to render it to the user), and are computationally cheap to obtain.","To improve the compression-reliability tradeoff, we leverage static token embeddings, which are available given that the ranker has access to the document text (as it needs to render it to the user), and are computationally cheap to obtain.","Modify,Clarity",Clarity
1379,166-ARR,166-ARR_v2_13@4,166-ARR_v1_13@4,Ablation tests verify that adding the static vectors significantly improves the compression rates for the same ranking accuracy.,Ablation tests verify that adding the static vectors significantly improves the compression rates for the same ranker accuracy.,"Modify,Grammar",Grammar
1380,166-ARR,166-ARR_v2_15@2,166-ARR_v1_15@2,"For the MSMARCO dataset, we used a distilled model with a reduced vector width (Hofstätter et al., 2020a) as the initial pre-trained weights for the late-interaction model.","For the MSMARCO dataset, we used a distilled model with a reduced vector width (Hofstätter et al., 2020a) as the initialized pre-trained weights for the late-interaction model.","Modify,Grammar",Grammar
1381,166-ARR,166-ARR_v2_16@0,166-ARR_v1_16@0,"To summarize, here are the contribution of this work 1 :",We make the following contributions:,"Modify,Clarity",Clarity
1500,172-ARR,172-ARR_v2_36@1,172-ARR_v1_40@1,"We tested our adaptation strategy on three different language encoders coupled with CLIP-T, including BERT-base, ELECTRA-base, and ELECTRA-large.","We tested our pretraining strategy on three different language encoders coupled with CLIP-T, including BERT-base, ELECTRA-base, and ELECTRA-large.","Modify,Clarity",Clarity
1501,172-ARR,172-ARR_v2_41@3,172-ARR_v1_45@3,"Analyzing the separated entries as a whole, we discovered that the betterperforming entries have a larger visually grounded ratio (Figure 4), as the quartile, median and mean values are generally higher for improved samples.",We found out that the better-performing entries have a shorter sentence length and their tokens have a larger visually grounded ratio (Figure 3).,"Modify,Fact/Evidence",Fact/Evidence
1503,172-ARR,172-ARR_v2_49@6,172-ARR_v1_55@6,"The issue is also present in the finetuning phase, and the maximum sequence length of GLUE and SWAG is 128; therefore we used 2 blocks of CLIP sub-sequences to model it.","The issue is also present in the finetuning phase, and the maxmum sequence length of GLUE and SWAG is 128; therefore we used 2 blocks of CLIP sub-sequences to model it.","Modify,Grammar",Grammar
1505,172-ARR,172-ARR_v2_43@2,172-ARR_v1_62@2,"Other experiments include changing the number of layers in the crossmodal encoder, training for longer, and swapping to a much larger wiki (14G).","Other attempts include changing the number of layers in the cross-modal encoder, training for longer, and swapping to a much larger wiki (14G).","Modify,Clarity",Clarity
1506,172-ARR,172-ARR_v2_44@0,172-ARR_v1_63@0,"Besides experimental evidence, we also justify the CLIPTC loss via further analysis, as the CLIPTC objective can theoretically be trivially solved by identity mapping.","Besides experimental evidence, we also justify the VC loss via further analysis because VC loss can theoretically be trivially solved by identity mapping.","Modify,Fact/Evidence",Fact/Evidence
1507,172-ARR,172-ARR_v2_44@1,172-ARR_v1_63@1,"Despite this possibility, we find that the loss is crucial to cross attention learning.",The importance of this loss is to balances out the cross-modal matching loss.,"Modify,Claim",Claim
1508,172-ARR,172-ARR_v2_44@2,172-ARR_v1_63@2,"Since we do not impose negative hard samples from sampled sentences, the MATCH objective can be solved sufficiently simply by guiding the cross attention to focus on common trivial words.","Without VC loss, the cross-modal matching is too easy, because relying on only a few common words is sufficient, even when the word is trivial to the meaning of the sentence.","Modify,Claim",Claim
1509,172-ARR,172-ARR_v2_44@4,172-ARR_v1_63@5,We show comparisons of the attention maps generated from the cross-modal encoders with a random sequence from RTE in Table 9 in the Appendix to verify this claim.,We show the attention of the cross-modal matching with a random sequence from RTE in Table 9.,"Modify,Fact/Evidence",Fact/Evidence
1510,172-ARR,172-ARR_v2_0@0,172-ARR_v1_0@0,XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding,XDBERT: Distilling Visual Information to BERT via Cross-Modal Encoders to Improve Language Understanding,"Modify,Clarity",Clarity
1511,172-ARR,172-ARR_v2_7@0,172-ARR_v1_7@0,"In this work, we establish the link between pretrained multimodal transformers and visuallygrounded language learning.","In this work, we established the link between pretrained multimodal transformers and visuallygrounded language learning.","Modify,Grammar",Grammar
1512,172-ARR,172-ARR_v2_7@1,172-ARR_v1_7@1,"We devise a way to distill visual information from components of a pretrained multimodal transformer (CLIP texttransfomer, abbreviated as CLIP-T) to pretrained language transformers (BERT/ELECTRA), to incorporate versatile perception of words into the model (Figure 1).","We devised a way to distill visual information from components of a pretrained multimodal transformer (CLIP texttransfomer, briefed by CLIP-T) to pretrained language transformers (BERT/ELECTRA).","Modify,Fact/Evidence",Fact/Evidence
1513,172-ARR,172-ARR_v2_8@0,172-ARR_v1_8@0,"Methodologically, we use the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot fewer than the original pretraining steps).","Methodologically, we used the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot smaller than the original pretraining steps).","Modify,Grammar",Grammar
1514,172-ARR,172-ARR_v2_8@3,172-ARR_v1_10@1,We do ablation studies to show that each of the task provides improvement (Section 5).,We do ablation studies to show that each of the task provides improvement (Appendix D).,"Modify,Fact/Evidence",Fact/Evidence
1515,172-ARR,172-ARR_v2_24@1,172-ARR_v1_30@1,"In these tasks, BERT and CLIP-T takes sentences A and B respectively as input, and losses are calculated from both BERT output and CLIP-T output.","In these tasks, BERT and CLIP-T takes sentences A and B respectively as input as we condition the output.","Modify,Fact/Evidence",Fact/Evidence
1516,172-ARR,172-ARR_v2_32@0,172-ARR_v1_36@0,"This is the MLM objective done on the CLIP-T side of the full model, omitting the masking part because CLIP has no mask token.","This is the MLM objective done on the CLIP-T side of the full model, but the CLIP-T sentence input tokens is neither being masked nor replaced.","Modify,Fact/Evidence",Fact/Evidence
1681,178-ARR,178-ARR_v2_17@2,178-ARR_v1_17@2,"Then, we will introduce our model including triaffine attention and triaffine scoring based on the proposed triaffine transformations.","Then, we will introduce our model based on the proposed triaffine transformations.","Modify,Fact/Evidence",Fact/Evidence
1682,178-ARR,178-ARR_v2_19@0,178-ARR_v1_19@0,"We define the deep triaffine transformation with vectors u, v, w ∈ R d and a tensor W ∈ R d+1 × R d × R d+1 which outputs a scalar by applying distinct MLP (multi-layer perceptron) transformations on input vectors and calculating tensor vector multiplications.","We define the deep triaffine transformation with vectors u, v, w ∈ R d and a tensor W ∈ R d+1 × R d × R d+1 which outputs a scalar by applying distinct MLP transformations on input vectors and calculating tensor vector multiplications.","Modify,Fact/Evidence",Fact/Evidence
1683,178-ARR,178-ARR_v2_42@0,178-ARR_v1_44@0,"Since h c i,j,r are composed by h ig,jg,r , we can decompose Equation 11 into following if and only if the layer of MLP transformation on h c i,j,r is 0:","Since h c i,j,r are composed by h ig,jg,r , we can decompose Equation 11 into:","Modify,Fact/Evidence",Fact/Evidence
1684,178-ARR,178-ARR_v2_57@0,178-ARR_v1_59@1,"To fairly compare with previous works, we follow the same dataset split with Lu and Roth (2015) for ACE2004 and ACE2005 and use the split from for GENIA and KBP2017 datasets.","To fairly compare with previous works, we follow the same dataset split with Lu and Roth (2015) for ACE2004 and ACE2005 datasets and use the split from for GENIA and KBP2017 datasets.","Modify,Clarity",Clarity
1685,178-ARR,178-ARR_v2_6@0,178-ARR_v1_6@0,"Although some of the factors may have been explored in previous works, to the best of our knowledge, it is the first work to fuse all these heterogeneous factors into a unified network.","Although some of the factors may be explored in previous works, to the best of our knowledge, it is the first work to fuse all these heterogeneous factors into a unified network.","Modify,Grammar",Grammar
1688,178-ARR,178-ARR_v2_2@4,178-ARR_v1_2@4,Triaffine attention uses boundaries and labels as queries and uses inside tokens and related spans as keys and values for span representations.,"Triaffine attention uses boundaries and labels as queries, and uses inside tokens and related spans as keys and values for span representations.","Modify,Grammar",Grammar
1689,178-ARR,178-ARR_v2_15@3,178-ARR_v1_15@3,"Except for the similar formula of vectors' interactions, the motivation and the use of triaffine are different in our paper.",There are two key differences between our triaffine transformation and theirs.,"Modify,Fact/Evidence",Fact/Evidence
1690,178-ARR,178-ARR_v2_15@4,178-ARR_v1_15@4,"Firstly, they only model the homogeneous features such as three tokens, but our triaffine transformation can model heterogeneous factors including labels, boundaries, and related spans.","Firstly, they only model the homogeneous features such as three tokens, but our triaffine transformation can model heterogeneous factors.","Modify,Fact/Evidence",Fact/Evidence
1964,192-ARR,192-ARR_v2_2@5,192-ARR_v1_2@5,"Specifically, an entity recognizer and a similarity evaluator are first trained in parallel as two teachers from the source domain.","Specifically, an entity recognizer and a similarity evaluator teachers are first trained in parallel from the source domain.","Modify,Clarity",Clarity
1965,192-ARR,192-ARR_v2_21@2,192-ARR_v1_23@2,"During the learning process, the samples from the target language are fed into the teacher model and the outputs are taken as the supervisory signal for two tasks in the student model.","During the learning process, the samples from target language are fed into the teacher model and the outputs are taken as the supervisory signal for two tasks in the student model.","Modify,Grammar",Grammar
1966,192-ARR,192-ARR_v2_2@6,192-ARR_v1_2@6,"Then, two tasks in the student model are supervised by these teachers simultaneously.","Then, two tasks in the student model are supervised by the two teachers simultaneously.","Modify,Clarity",Clarity
1967,192-ARR,192-ARR_v2_25@2,192-ARR_v1_27@2,We aim to find entity similarity to help the crosslingual NER model in the target language.,We aim to find entity similarity to help the cross-lingual NER model in target language.,"Modify,Grammar",Grammar
1968,192-ARR,192-ARR_v2_25@4,192-ARR_v1_27@4,"To address this challenge, we propose a binary classifier called similarity evaluator to leverage the labeled source language data for similarity prediction.","To address this challenge, we propose a binary classifier called similarity teacher to leverage the labeled source language data for similarity prediction.","Modify,Clarity",Clarity
1969,192-ARR,192-ARR_v2_2@7,192-ARR_v1_2@7,Empirical studies on the three datasets across 7 different languages confirm the effectiveness of the proposed model.,Empirical studies on the datasets across 7 different languages confirm the effectiveness of the proposed model.,"Modify,Fact/Evidence",Fact/Evidence
1970,192-ARR,192-ARR_v2_25@5,192-ARR_v1_27@5,"Our similarity evaluator model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.","Our similarity teacher model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.","Modify,Clarity",Clarity
1971,192-ARR,192-ARR_v2_26@0,192-ARR_v1_28@0,Entity Recognizer,Entity Recognizer Teacher,"Modify,Other",Other
1972,192-ARR,192-ARR_v2_33@0,192-ARR_v1_35@0,"To leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.","In order to leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.","Modify,Clarity",Clarity
1973,192-ARR,192-ARR_v2_36@0,192-ARR_v1_38@0,"The inter-entities similarity is measured on the hidden representations h i and h j of the tokens queried by the entity indices < i, j > on the sequences representations.","The inter-entities similarity is measured on the tokens hidden representations h i and h j , queried by the entity indices < i, j > on the sequences representations.","Modify,Clarity",Clarity
1974,192-ARR,192-ARR_v2_4@1,192-ARR_v1_4@1,"The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) makes this task achieve significant performances.","The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) make this task achieves significant performances.","Modify,Grammar",Grammar
1975,192-ARR,192-ARR_v2_42@0,192-ARR_v1_44@0,Teacher-student Distillation Learning,Teacher Student Distillation Learning,"Modify,Grammar",Grammar
1976,192-ARR,192-ARR_v2_43@0,192-ARR_v1_45@0,"In this section, we consider transferring the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task.","In this section, we consider to transfer the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task.","Modify,Grammar",Grammar
1977,192-ARR,192-ARR_v2_44@0,192-ARR_v1_46@0,"The mBERT is also used as an encoder for the sentence siamese pair, and the entity token feature is queried from the latent sequence encoding representation.","The multi-lingual BERT is also used as encoder for the sentence siamese pair, and the entity token feature queried from the latent sequence encoding representation.","Modify,Clarity",Clarity
1978,192-ARR,192-ARR_v2_4@2,192-ARR_v1_4@2,"However, since deep neural networks highly rely on a large amount of labelled training data, the annotation acquiring process is expensive and time consuming.","However, since deep neural networks highly relies on a large amount of labelled training data, the annotation acquiring process is expensive and time consuming.","Modify,Grammar",Grammar
1979,192-ARR,192-ARR_v2_4@3,192-ARR_v1_4@3,This situation is more severe for zero-resource languages.,This situation is more severe for low-resource languages.,"Modify,Claim",Claim
1980,192-ARR,192-ARR_v2_2@0,192-ARR_v1_2@0,Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages.,Cross-lingual named entity recognition task is one of the critical problem for evaluating the potential transfer learning techniques on low resource languages.,"Modify,Grammar",Grammar
1981,192-ARR,192-ARR_v2_56@2,192-ARR_v1_57@3,"We set the batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014).","We set batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014).","Modify,Grammar",Grammar
1982,192-ARR,192-ARR_v2_62@1,192-ARR_v1_63@1,"Specifically, compared with the remarkable RIKD, AdvPicker, and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].","Specifically, compared with the remarkable RIKD, AdvPicker and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].","Modify,Grammar",Grammar
1983,192-ARR,192-ARR_v2_63@0,192-ARR_v1_64@0,Note that BERT-f performs better than our model on the Chinese dataset due to their re-tokenization of the dataset.,Note that Bert-f performs better than our model on Chinese dataset due to their re-tokenization of the dataset.,"Modify,Grammar",Grammar
1984,192-ARR,192-ARR_v2_66@1,192-ARR_v1_67@1,"That is, the teacher model has the same as the neural network structure of the student model.","That is, both of the teacher and student have the same neural network structure.","Modify,Clarity",Clarity
1985,192-ARR,192-ARR_v2_66@2,192-ARR_v1_67@2,This causes a performance drop across all languages due to two single teachers cannot make a difference with the combination.,This causes a performance drop across all languages due to two single teachers cannot make a difference with combination.,"Modify,Grammar",Grammar
1986,192-ARR,192-ARR_v2_67@0,192-ARR_v1_68@0,"(2) MTMT w/o weighting, which set the α (•) , β and γ all to be 1 in the loss of student learning.","(2) MTMT w/o weighting, which set the α 1 ,α 2 , β and γ all to be 1 in the loss of student model learning.","Modify,Fact/Evidence",Fact/Evidence
1987,192-ARR,192-ARR_v2_67@1,192-ARR_v1_68@1,"It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es), which validates that weighting loss can bring more confident knowledge to the student model.","It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es), which validates that weighting loss can bring more confident knowledge to student model.","Modify,Grammar",Grammar
1988,192-ARR,192-ARR_v2_68@1,192-ARR_v1_69@1,"In this case, our approach degrades into the single teacherstudent learning model as in TSL (Wu et al., 2020a).","In this case, our approach degrades into the Single Teacher-Student learning model as in TSL (Wu et al., 2020a).","Modify,Grammar",Grammar
1989,192-ARR,192-ARR_v2_71@1,192-ARR_v1_72@1,"Specifically, if there is a set of tokens in which every two of them have a high Entity Similarity score, and one of the tokens is predicted to have a distinct label while other tokens have identical labels, then the one with the distinct label is predicted wrongly and is corrected by the student model to have the label of all other tokens.","Specifically, if there is a set of tokens in which every two of them have high Entity Similarity score, and one of the tokens is predicted to have a distinct label while other tokens have identical labels, then the one with the distinct label is predicted wrongly and is corrected by the student model to have the label of all other tokens.","Modify,Grammar",Grammar
1990,192-ARR,192-ARR_v2_72@0,192-ARR_v1_73@0,Embedding Distribution,Embeddings Distribution,"Modify,Grammar",Grammar
1991,192-ARR,192-ARR_v2_73@1,192-ARR_v1_74@1,"It can be seen that the embedding distribution of the student model is close to similarity evaluator teacher, as illustrated in Figure 5.","It can be seen that the embeddings distribution of student model is close to similarity evaluator teacher, as illustrated in Figure 5.","Modify,Grammar",Grammar
1992,192-ARR,192-ARR_v2_5@0,192-ARR_v1_6@0,Many studies have been done to solve this crosslingual NER problem.,Many studies have been done to solve this crosslanguage NER problem.,"Modify,Grammar",Grammar
1993,192-ARR,192-ARR_v2_73@2,192-ARR_v1_74@2,"We conjecture that the student model captures similarity knowledge from the similarity evaluator teacher, i.e. the same class of examples tend to cluster and the different class of examples tend to segregate in the embedding distribution.","We conjecture that the student model captures similarity knowledge from the similarity evaluator teacher, i.e. the same class of examples tend to cluster and the different class of examples tend to segregate in the embeddings distribution.","Modify,Grammar",Grammar
1994,192-ARR,192-ARR_v2_75@0,192-ARR_v1_76@0,"In this section, we evaluate the effectiveness of weight loss in student learning from a quantitative perspective.","In the section, we evaluate the effectiveness of the weighting loss in student learning from quantitative perspective.","Modify,Grammar",Grammar
1995,192-ARR,192-ARR_v2_76@1,192-ARR_v1_77@1,"Therefore, the student model is better suited to the target language with learning fewer low-confidence misrecognitions for the target language.","Therefore, the student model is better suited to target language with learning less low-confidence misrecognitions for the target language.","Modify,Clarity",Clarity
1996,192-ARR,192-ARR_v2_77@1,192-ARR_v1_78@1,The encoder of the student model obtains the clustering information of the target language with the help of β.,The encoder of student model obtains the clustering information of the target language with the help of β.,"Modify,Grammar",Grammar
1997,192-ARR,192-ARR_v2_78@2,192-ARR_v1_79@2,"The student model learns less from unreasonable results, and it can make more accurate entity recognition for the target language.","The student model learns less from unreasonable results, and it can make more accuracy entity recognition for the target language.","Modify,Grammar",Grammar
1998,192-ARR,192-ARR_v2_80@2,192-ARR_v1_81@2,"Moreover, to guarantee the student learning performance, we also propose a weighting strategy to take into consideration the reliability of the teachers.","Moreover, in order to guarantee the student learning performance, we also propose a weighting strategy to take consideration of the reliability of the teachers.","Modify,Grammar",Grammar
1999,192-ARR,192-ARR_v2_5@2,192-ARR_v1_6@2,"Shared feature space based models exploit language-independent features, which lacks the domain-specific features for the target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).","Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).","Modify,Grammar",Grammar
2000,192-ARR,192-ARR_v2_6@0,192-ARR_v1_7@0,"Although the above-mentioned models solve the cross-lingual NER problem to some extent, the auxiliary tasks, as in multi-task learning, have not been studied in this problem.","Although above mentioned models solve the cross-lingual NER problem in some extent, the auxiliary tasks, as in the multi-task learning, have not been studied in this problem.","Modify,Grammar",Grammar
2001,192-ARR,192-ARR_v2_2@1,192-ARR_v1_2@1,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer.,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority.,"Modify,Clarity",Clarity
2002,192-ARR,192-ARR_v2_6@1,192-ARR_v1_7@1,"Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target languages.","Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target language.","Modify,Grammar",Grammar
2003,192-ARR,192-ARR_v2_7@1,192-ARR_v1_8@1,"Given a Spanish sentence ""Arévalo (Avila), 23 may (EFE)."", the token ""Arévalo"" is recognized as ORG type using the learned model from the English domain.","Given a Spanish sentence ""Arévalo (Avila), 23 may (EFE)."", the token ""Arévalo"" is recognized as ORG type using the learned model from English domain.","Modify,Grammar",Grammar
2004,192-ARR,192-ARR_v2_2@2,192-ARR_v1_2@2,"However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains.","However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domain.","Modify,Grammar",Grammar
2005,192-ARR,192-ARR_v2_9@0,192-ARR_v1_10@0,We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results show the superiority of our presented MTMT model.,We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results shows the superiority our presented MTMT model.,"Modify,Grammar",Grammar
2006,192-ARR,192-ARR_v2_13@0,192-ARR_v1_14@0,"Our approach is closely related to the existing works on cross-lingual NER, knowledge distillation, and siamese network.","Our approach is closely related to the existing works on cross-lingual NER, knowledge distillation and siamese network.","Modify,Grammar",Grammar
2007,192-ARR,192-ARR_v2_15@1,192-ARR_v1_16@1,"Recently, the pre-trained multilingual language model is effective to address the challenge (Devlin et al., 2019).","Recently, the pre-trained multilingual language models mBERT is effective to address the challenge (Devlin et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
2008,192-ARR,192-ARR_v2_15@2,192-ARR_v1_16@2,"Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from the labeled source language to that of target languages (Keung et al., 2019).","Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019).","Modify,Grammar",Grammar
2009,192-ARR,192-ARR_v2_16@0,192-ARR_v1_17@0,Translation based models generally generate pseudo-labeled target data to alleviate target data scarcity.,Translation based models generally generate pesudo labeled target data to alleviate target data scarcity.,"Modify,Grammar",Grammar
2010,192-ARR,192-ARR_v2_2@4,192-ARR_v1_2@4,"In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on the target domain.","In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on target domain.","Modify,Grammar",Grammar
2011,192-ARR,192-ARR_v2_16@1,192-ARR_v1_17@1,"For example, (Wu et al., 2020b;Zhang et al., 2021) gain an improvement by translating the labeled source language to the target language word-by-word.","For example, (Wu et al., 2020b;Zhang et al., 2021) gain a improvement by translating the labeled source language to the target language word-by-word.","Modify,Grammar",Grammar
2012,192-ARR,192-ARR_v2_17@0,192-ARR_v1_18@0,"Knowledge distillation based models include a teacher model and a student model (Wu et al., 2020c).","Knowledge distillation based models includes a teacher model and a student model (Wu et al., 2020c).","Modify,Grammar",Grammar
2013,192-ARR,192-ARR_v2_17@1,192-ARR_v1_18@1,The teacher model is trained on the labeled source language.,The teacher model is trained on labeled source language.,"Modify,Grammar",Grammar
2014,192-ARR,192-ARR_v2_17@2,192-ARR_v1_18@2,The student model learns from the soft label predicted by the teacher model on unlabeled target language data.,The student model learns from the soft label predicted by teacher model on unlabeled target language data.,"Modify,Grammar",Grammar
2015,192-ARR,192-ARR_v2_18@2,192-ARR_v1_19@2,"However, there is a dilemma to adapt the siamese network to tokenlevel recognition tasks such as NER.","However, there is a dilemma to adapt siamese network to token-level recognition tasks such as NER.","Modify,Grammar",Grammar
2016,192-ARR,192-ARR_v2_20@1,192-ARR_v1_21@1,Our framework is consist of two models: teacher training model learned from the source language and teacher-student distillation learning model learned from the target language.,Our framework is consist of two models: teacher training model learned from source language and teacher-student distillation learning model learned from target language.,"Modify,Grammar",Grammar
2033,193-ARR,193-ARR_v2_2@4,193-ARR_v1_2@4,"Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks.","Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not enhance its performance on such tasks.","Modify,Clarity",Clarity
2034,193-ARR,193-ARR_v2_23@2,193-ARR_v1_25@2,"One possibility is that Arabic's rich morphology incentivizes the model to store more information about each token's character composition; however, it is also possible that AraBERT's different vocabulary, which allocates shorter character sequences to each token type, might explain this difference (we discuss the link between sequence length and accuracy in Appendix C).","One possibility is that Arabic's rich morphology incentivizes the model to store more information about each token's character composition, however it is also possible that AraBERT's different vocabulary, which allocates shorter character sequences to each token type, might explain this difference (we discuss the link between sequence length and accuracy later in this section).","Modify,Fact/Evidence",Fact/Evidence
2035,193-ARR,193-ARR_v2_30@4,193-ARR_v1_29@1,"8 We train RoBERTa-Large on English Wikipedia using the hyperparameter configuration of 24hBERT (Izsak et al., 2021), and cease training after 24 hours (approximately 16,000 steps).","6 We train RoBERTa-Large on English Wikipedia using the hyperparameter configuration of 24hBERT (Izsak et al., 2021), and cease training after 24 hours (∼16000 steps).","Modify,Clarity",Clarity
2036,193-ARR,193-ARR_v2_31@3,193-ARR_v1_30@3,"This result indicates that in this scenario, the model does not utilize the character information injected into the tokens' embeddings.",This result indicates that the model does not utilize the character information injected into the tokens' embeddings.,"Modify,Clarity",Clarity
2037,193-ARR,193-ARR_v2_32@1,193-ARR_v1_30@4,"Along with the results from Section 4, we hypothesize that the implicit notion of spelling that the model learns during pretraining might be sufficient for masked language modeling.","Along with the results from Section 4, we conjecture that the model learns an implicit notion of spelling during pretraining, which is sufficient for masked language modeling, and does not benefit from explicitly adding orthographic information.","Modify,Claim",Claim
2038,193-ARR,193-ARR_v2_7@1,193-ARR_v1_7@1,"We compare the pretraining process of the character-infused model to that of an identical model whose embedding layer is randomly initialized (and not pretrained), and find that both learning curves converge to virtually identical values within the first 1,000 gradient updates, a fraction of the total optimization process.","We compare the pretraining process of the character-infused model to that of an identical model whose embedding layer is randomly initialized (and not pretrained), and find that both learning curves converge to virtually identical values within the first 1000 gradient updates, a fraction of the total optimization process.","Modify,Grammar",Grammar
2039,193-ARR,193-ARR_v2_7@2,193-ARR_v1_7@2,"This experiment suggests that while language models may need to learn some notion of spelling to optimize their objectives, they might also be able to quickly acquire most of the character-level information they need from plain token sequences without directly observing the composition of each token.","This experiment suggests that while language models may need to learn some notion of spelling to optimize their objectives, they can quickly acquire all the character-level information they need without directly observing the composition of each token.","Modify,Claim",Claim
2040,193-ARR,193-ARR_v2_2@2,193-ARR_v1_2@2,Our results show that the embedding layers of RoBERTa and GPT2 each hold enough information to accurately spell up to a third of the vocabulary and reach high character ngram overlap across all token types.,Our results show that the embedding layer of RoBERTa holds enough information to accurately spell up to a third of the vocabulary and reach high average character ngram overlap on all token types.,"Modify,Fact/Evidence",Fact/Evidence
2041,193-ARR,193-ARR_v2_2@3,193-ARR_v1_2@3,"We further test whether enriching subword models with character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment.","We further test whether enriching subword models with additional character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment.","Modify,Clarity",Clarity
2042,193-ARR,193-ARR_v2_21@4,193-ARR_v1_23@3,"These results are persistent across different models and filters, strongly indicating that the embedding layer of pretrained models contains significant amounts of information about each token's character composition.","These results are persistent across different models and filters, strongly indicating that the embedding layer of pretrained models contain significant amounts of information about each token's character composition.","Modify,Grammar",Grammar
2066,195-ARR,195-ARR_v2_18@0,195-ARR_v1_12@4,MuST-SHE thus allows the identification and pinpointed evaluation of numerous and qualitatively different grammatical gender instances under authentic conditions.,"Thus, MuST-SHE allows the identification of numerous and qualitatively different grammatical gender instances under authentic conditions.","Modify,Fact/Evidence",Fact/Evidence
2067,195-ARR,195-ARR_v2_18@2,195-ARR_v1_12@6,"As a matter of fact, as Gygax et al. (2019) suggest, accounting for gender in languages with similar typological features allows for proper comparisons.","In fact, as Gygax et al. (2019) suggest, accounting for gender in languages with similar typological features allows for proper comparison.","Modify,Clarity",Clarity
2068,195-ARR,195-ARR_v2_2@5,195-ARR_v1_2@5,"By shedding light on model behaviours, gender bias, and its detection at several levels of granularity, our findings emphasize the value of dedicated analyses beyond aggregated overall results.","Our work sheds light on model behaviours, gender bias, and its detection at several levels of granularity for English-French/Italian/Spanish.","Modify,Claim",Claim
2069,195-ARR,195-ARR_v2_20@2,195-ARR_v1_14@2,"As shown in Table 1 (a-c), we differentiate among six POS categories: 7 i) articles, ii) pronouns, iii) nouns, and iv) verbs.","As shown in Table 1 (a-c), we differentiate among six POS categories: 6 i) articles, ii) pronouns, iii) nouns, iv) verbs.","Modify,Grammar",Grammar
2070,195-ARR,195-ARR_v2_21@5,195-ARR_v1_15@5,"9 This annotation lets us verify whether a model consistently picks the same gender paradigm for all words in the chain, enabling the assessment of its syntagmatic behaviour.","8 This annotation let us verify whether a model consistently picks the same gender paradigm for all words in the chain, enabling the assessment of its syntagmatic behaviour.","Modify,Grammar",Grammar
2071,195-ARR,195-ARR_v2_23@1,195-ARR_v1_17@1,"For each language pair, they annotated the whole corpus independently, based on detailed guidelines (see Appendix A).","For each language pair, they annotated the whole corpus independently, based on detailed guidelines (see Appendix §A).","Modify,Grammar",Grammar
2072,195-ARR,195-ARR_v2_26@0,195-ARR_v1_21@0,Speech Translation models,Speech Translation models.,"Modify,Grammar",Grammar
2073,195-ARR,195-ARR_v2_27@2,195-ARR_v1_21@3,"Working on WinoMT/ST, Kocmi et al. (2020) correlate higher BLEU scores and gender stereotyping, whereas Costa-jussà et al. (2020) show that systems with lower performance tend to produce fewer feminine translations for occupations, but rely less on stereotypical cues.","Working on WinoMT/ST, (Kocmi et al., 2020) correlates higher BLEU scores and gender stereotyping, whereas (Costa-jussà et al., 2020a) shows that systems with lower performance tend to produce fewer feminine translations for occupations, but rely less on stereotypical cues.","Modify,Fact/Evidence",Fact/Evidence
2074,195-ARR,195-ARR_v2_29@4,195-ARR_v1_22@7,"Thus, for the sake of comparison with , we train these systems in the same (controlled) data conditions i.e. on the MuST-C corpus only.","Thus, for the sake of comparison with , we train these systems in the same controlled data environment, i.e. on the MuST-C corpus only.","Modify,Clarity",Clarity
2075,195-ARR,195-ARR_v2_30@0,195-ARR_v1_23@0,Evaluation method,Evaluation method.,"Modify,Grammar",Grammar
2078,195-ARR,195-ARR_v2_32@2,195-ARR_v1_24@1,"14 Finally, since we aim at gaining qualitative insights into systems' behaviour, and at ensuring a sound and thorough multifaceted evaluation, we overcome the described coverage limitation of the automatic evaluation by complementing it with a manual analysis of all the gender-marked words and agreement chains that remained out of coverage.","13 Finally, since we aim at gaining exhaustive, qualitative insights into systems' behaviour, and at ensuring a sound and thorough multifaceted evaluation, we overcome the described coverage limitation of the automatic evaluation by complementing it with a manual analysis of all the words and agreement chains that remained out of coverage.","Modify,Fact/Evidence",Fact/Evidence
2079,195-ARR,195-ARR_v2_32@3,195-ARR_v1_24@2,"This extensive manual evaluation was carried out via a systematic annotation of systems' outputs, performed by the same linguists that enriched MuST-SHE, who provided the appropriate knowledge of both the resource and the evaluation task.","This extensive manual evaluation was carried out via a systematic annotation of systems' outputs, performed by the same linguists that enriched MuST-SHE, who provided the appropriate knowledge of both resource and evaluation tasks.","Modify,Grammar",Grammar
2080,195-ARR,195-ARR_v2_36@0,195-ARR_v1_31@0,"At a finer level of granularity, we use our extension of MuST-SHE to inspect gender bias across open and closed class words.","At a finer level of granularity, we use MuST-SHE extension to inspect gender bias across open and closed class words.","Modify,Clarity",Clarity
2081,195-ARR,195-ARR_v2_37@4,195-ARR_v1_33@5,"As discussed in Section 8, we believe that our breakdown per POS is informative inasmuch it prompts qualitative considerations on how to pursue gender bias mitigation in models and corpora (Czarnowska et al., 2021;Doughman et al., 2021).","As discussed in §8, we believe that our breakdown per POS is informative inasmuch it prompts qualitative considerations on how to pursue gender bias mitigation in models and corpora (Czarnowska et al., 2021;Doughman et al., 2021).","Modify,Clarity",Clarity
2082,195-ARR,195-ARR_v2_49@1,195-ARR_v1_42@1,"Similarly to the approach employed for single words (Section 5.3), we discern between OOC chains due to: i) translation errors (Err), and ii) alternative translations preserving the source meaning.","Similarly to the approach employed for single words ( §5.3), we discern between OOC chains due to: i) translation errors (Err), and ii) alternative translations preserving the source meaning.","Modify,Clarity",Clarity
2083,195-ARR,195-ARR_v2_49@4,195-ARR_v1_42@4,"Instead, when the generated alternative chain exhibits gender markings, we distinguish if the chosen gender is correct (C), wrong (W), or if the system produces a chain that does not respect gender agreement because it combines both feminine and masculine gender inflections (NO).","Instead, when the generated alternative chain exhibits gender markings, we distinguish if the chosen gender is correct (C), wrong (W), or if the system produces a chain that does not respect gender agreement, because it combines both feminine and masculine gender inflections (NO).","Modify,Grammar",Grammar
2084,195-ARR,195-ARR_v2_50@1,195-ARR_v1_43@1,"Interestingly, such results are only partially corroborating previous analyses.","Interestingly, such results are only partially corroborating analyses.","Modify,Clarity",Clarity
2085,195-ARR,195-ARR_v2_51@0,195-ARR_v1_43@2,"On the one hand, unlike the OOC words' results discussed in Section 5.3, we attest that CHAR models produce the highest proportion of translation errors.","On the one hand, unlike the OOC words' results discussed in §5.3, we attest that CHAR models produce the highest proportion of translation errors.","Modify,Clarity",Clarity
2086,195-ARR,195-ARR_v2_51@3,195-ARR_v1_44@0,"Finally, again in line with our automatic evaluation (Table 6), we confirm that respecting agreement is not an issue for our ST models: we identify only 3 cases (2 for en-fr BPE, 1 for en-fr CHAR) where concord is broken (NO).","Finally, in line with our automatic evaluation (Table 6), we confirm that respecting agreement is not an issue for our ST models: we identify only 3 cases (2 for en-fr BPE, 1 for en-fr CHAR) where concord is broken (NO).","Modify,Clarity",Clarity
2087,195-ARR,195-ARR_v2_51@7,195-ARR_v1_44@4,"However, the most common type among these outliers are constructions with semi-copula verbs (e.g. en: She... [became a vet]; it: ...E' [diventata F un M veterinatrio M ]), which -as discussed in Section 3.1 -exhibit a weaker agreement constraint.","However, the most common type among these outliers are constructions with semi-copula verbs (e.g. en: She... [became a vet]; it: ...E' [diventata F un M veterinatrio M ]), which -as discussed in §3.1 -exhibit a weaker agreement constraint.","Modify,Clarity",Clarity
2088,195-ARR,195-ARR_v2_53@3,195-ARR_v1_46@3,"On three language pairs (English-French/Italian/Spanish), our study shows that, while all POS are subject to masculine skews, they are not impacted to the same extent.","On 3 language pairs (en-es/fr/it) our study shows that, while all POS are subject to masculine skews, they are not impacted to the same extent.","Modify,Grammar",Grammar
2089,195-ARR,195-ARR_v2_54@1,195-ARR_v1_46@7,"Accordingly, our results are in line with previous studies showing that, in spite of lower generic performance, character-based segmentation exhibits a better capability at handling feminine translation at different levels of granularity.","Accordingly, our results are in line with previous studies showing that, in spite of lower generic performance, character-based segmentation favours feminine translation at different levels of granularity.","Modify,Clarity",Clarity
2090,195-ARR,195-ARR_v2_56@3,195-ARR_v1_48@3,"Specifically, our investigation on the relation between data size/segmentation technique and gender bias provides initial cues on which models and components to audit and implement toward the goal of reducing gender bias.","Specifically, our investigation on the relation between model size/segmentation technique and gender bias provides initial cues on which models and components to audit and implement toward the goal of reducing gender bias.","Modify,Clarity",Clarity
2091,195-ARR,195-ARR_v2_56@6,195-ARR_v1_48@6,"In fact, while it is known that the MuST-C corpus (Cattoni et al., 2021) used for training comprises a majority of masculine speakers, 21 the fact that certain lexical categories are more biased than others suggests that, on top of more coarse-grained quantitative attempts at gender balancing (Costa-jussà and de Jorge, 2020), data curation ought to account for more sensitive, nuanced, and qualitative asymmetries.","In fact, while it is known that the MuST-C corpus used for training comprises a majority of masculine speakers, 20 the fact that certain lexical categories are more biased than others suggests that, on top of more coarse-grained quantitative attempts at gender balancing (Costa-jussà and de Jorge, 2020), data curation ought to account for more sensitive, nuanced, and qualitative asymmetries.","Modify,Fact/Evidence",Fact/Evidence
2092,195-ARR,195-ARR_v2_7@1,195-ARR_v1_6@1,"2 (2) In light of recent studies exploring how model design and overall perfomance interplay with gender bias (Roberts et al., 2020;, we rely on our manually curated resource to compare three ST models, which are trained on varying amounts of data, and built with different segmentation techniques: character and byte-pairencoding (BPE) (Sennrich et al., 2016).","2 (2) In light of recent studies exploring how model design and overall perfomance interplay with gender bias (Roberts et al., 2020;, we rely on our manually curated resource to compare three ST models, which are trained on varying amounts of data, and built with different segmentation techniques: character and bytepair-encoding (BPE) (Sennrich et al., 2016).","Modify,Grammar",Grammar
2093,195-ARR,195-ARR_v2_58@0,195-ARR_v1_50@0,"While our experiments are limited to the binary linguistic forms represented in the used data, to the best of our knowledge, ST natural language corpora going beyond binarism do not yet exist.","While our experiments are largely limited to the binary linguistic forms represented in the used data, to the best of our knowledge, ST natural language corpora going beyond binarism do not yet exist.","Modify,Other",Other
2094,195-ARR,195-ARR_v2_2@1,195-ARR_v1_2@1,"However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions.","However, most evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions.","Modify,Clarity",Clarity
2095,195-ARR,195-ARR_v2_60@1,195-ARR_v1_53@1,"Successively, such guidelines have been refined and improved by means of discussions with the annotators, who had carried out a pilot annotation round to get acquainted with MuST-SHE language data.","Successively, such guidelines have been refined and improved by means of discussions with the annotators, who had carried out a trail annotation round to get acquainted with MuST-SHE language data.","Modify,Clarity",Clarity
2096,195-ARR,195-ARR_v2_61@1,195-ARR_v1_54@1,"We ensured that at least one annotator per language was a native speaker, whereas the second one had at least a C1 proficiency level of the assigned language.",We ensured that at least one annotator per language was a native speaker.,"Modify,Fact/Evidence",Fact/Evidence
2097,195-ARR,195-ARR_v2_62@0,195-ARR_v1_55@0,"In this section we describe in detail the ST models created for our study, whose source code is publicly released at: https://github.com/ mgaido91/FBK-fairseq-ST/tree/acl_2021.","In this section we describe in detail the ST models created for our study, whose source code will be publicly released upon acceptance of the paper.","Modify,Fact/Evidence",Fact/Evidence
2098,195-ARR,195-ARR_v2_64@0,195-ARR_v1_57@0,"Since the amount of ST data is limited (i.e. MuST-C - Cattoni et al. 2021-and Europarl-ST -Iranzo-Sánchez et al. 2020, knowledge transfer from the ASR and MT tasks is leveraged by respectively initializing the ST encoder with ASR pretrained weights (Weiss et al., 2017b;Bansal et al., 2019) and by distilling knowledge from a strong MT teacher (Liu et al., 2019).","Since the amount of ST data is limited (i.e. MuST-C -Cattoni et al. 2020-and Europarl-ST -Iranzo-Sánchez et al. 2020, knowledge transfer from the ASR and MT tasks is leveraged by respectively initializing the ST encoder with ASR pretrained weights (Weiss et al., 2017b;Bansal et al., 2019) and by distilling knowledge from a strong MT teacher (Liu et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
2099,195-ARR,195-ARR_v2_8@1,195-ARR_v1_6@3,"Finally, in line with concurring studies, we find that iv) character-based systems have an edge on translating gender phenomena, by favouring morphological and lexical diversity.","Finally, in line with concurring studies, we find that iv) character-based systems favour morphological and lexical diversity when translating gender phenomena.","Modify,Clarity",Clarity
2100,195-ARR,195-ARR_v2_10@1,195-ARR_v1_8@1,"Along this line, focus has been given to bias analysis in models' innards and outputs (Vig et al., 2020;Costajussà et al., 2022), and to ascertain the validity of bias measurement practices (Blodgett et al., 2021;Antoniak and Mimno, 2021;Goldfarb-Tarrant et al., 2021).","Along this line, focus has been given to bias analysis in models' innards and ouputs (Vig et al., 2020;Costajussà et al., 2020b), and to ascertain the validity of bias measurament practices (Blodgett et al., 2021;Antoniak and Mimno, 2021;Goldfarb-Tarrant et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
2101,195-ARR,195-ARR_v2_10@2,195-ARR_v1_8@2,"Complementary mounting evidence suggests that -rather than striving for generalizations -gender bias detection ought to incorporate contextual and linguistic specificity (González et al., 2020;Ciora et al., 2021;Matthews et al., 2021;Malik et al., 2021;Kurpicz-Briki and Leoni, 2021), which however receives little attention due to a heavy focus on English NLP (Bender and Friedman, 2018).","Complementary evidence suggests thatrather than striving for generalizations -gender bias detection ought to incorporate contextual and linguistic specificity (González et al., 2020;Ciora et al., 2021;Matthews et al., 2021;Malik et al., 2021;Kurpicz-Briki and Leoni, 2021), which however receives little attention due to a heavy focus on English NLP.","Modify,Fact/Evidence",Fact/Evidence
2102,195-ARR,195-ARR_v2_10@3,195-ARR_v1_8@3,"Purported agnostic approaches and evaluations (Bender, 2009) can prevent from drawing reliable conclusions and mitigating recommendations, as attested by monolingual studies on grammatical gender languages (Zhou et al., 2019;Gonen et al., 2019;Zmigrod et al., 2019) and in automatic translation scenarios (Vanmassenhove et al., 2018;Moryossef et al., 2019).","Purported agnostic approaches and evaluations (Bender, 2009) can prevent from drawing reliable conclusions and mitigating recommendations, as attested by monolingual studies on grammatical gender languages (Zhou et al., 2019;Gonen et al., 2019) and in automatic translation scenarios (Vanmassenhove et al., 2018;Moryossef et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
2103,195-ARR,195-ARR_v2_11@0,195-ARR_v1_8@4,"Unlike English, grammatical gender languages exhibit an elaborate morphological and syntactic system, where gender is overtly marked on numerous POS (e.g., verbs, determiners, nouns), and related words have to agree on the same gender features (see Figure 1 for an example).","Unlike English, grammatical gender languages exhibit an elaborate morphological and syntactic system, where gender is overtly marked on numerous POS (e.g., verbs, determiners, nouns), and related words have to agree on the same gender features.","Modify,Fact/Evidence",Fact/Evidence
2104,195-ARR,195-ARR_v2_12@1,195-ARR_v1_9@1,"It has been progressively enriched with new features Kocmi et al., 2020), and adapted for ST (Costa-jussà et al., 2020).","It has been progressively enriched with new features Kocmi et al., 2020), and adapted for ST (Costa-jussà et al., 2020a).","Modify,Fact/Evidence",Fact/Evidence
2105,195-ARR,195-ARR_v2_12@5,195-ARR_v1_9@5,"However, gender-marking involves also several other, so far less accounted POS categories, but if they are just as problematic is not clear yet.","However, gender-marking involves also several other, so far less accounted POS categories, but whether they are just as problematic is not clear yet.","Modify,Clarity",Clarity
2106,195-ARR,195-ARR_v2_13@1,195-ARR_v1_10@1,"As a result, they maximize lexical and contextual variability to inspect whether translation models yield feminine under-representation in real-world-like scenarios .","As a result, they maximize variability to inspect whether translation models yield feminine under-representation in real-world-like scenarios .","Modify,Fact/Evidence",Fact/Evidence
2107,195-ARR,195-ARR_v2_2@4,195-ARR_v1_2@4,"Focusing on speech translation, we conduct a multifaceted evaluation on three language directions (English-French/Italian/Spanish), with models trained on varying amounts of data and different word segmentation techniques.","On this basis, we conduct multifaceted automatic and manual evaluations for three speech translation models, trained on varying amounts of data and different word segmentation techniques.","Modify,Fact/Evidence",Fact/Evidence
2108,195-ARR,195-ARR_v2_14@3,195-ARR_v1_10@7,"Closer to our intent, analyze the output of different ST systems and note that their models seem to wrongly pick divergent gender inflections for unrelated words in the same sentence (e.g. en: As a researcher, professor; fr: En tant que chercheuse F , professeur M ) but not for dependency-related ones (e.g. en: The classic Asian student; it: [La classica studentessa asiatica] F ).","Closer to our intent, analyze the output of different ST systems and note that their models seem to wrongly pick divergent gender inflections for unrelated words in the same sentence (e.g. fr: En tant que chercheuse F , professeur M ) but not for dependency-related ones (e.g. it: [la classica studentessa asiatica] F ).","Modify,Fact/Evidence",Fact/Evidence
2109,195-ARR,195-ARR_v2_16@1,195-ARR_v1_12@1,"Rather than building it from scratch, we add two annotation layers to the existing MuST-SHE bench- mark , which is built on spoken language data retrieved from TED talks.","Rather than building it from scratch, we add two annotation layers to the existing TED-based MuST-SHE benchmark .","Modify,Clarity",Clarity
2110,195-ARR,195-ARR_v2_16@2,195-ARR_v1_12@2,"Available for en-es/fr/it, it represents the only multilingual MT and ST GBET 5 exhibiting a natural variety of gender phenomena, which are balanced across feminine and masculine forms.","4 Available for en-es/fr/it, it represents the only multilingual MT/ST GBET 5 exhibiting a natural variety of gender phenomena.","Modify,Fact/Evidence",Fact/Evidence
2111,195-ARR,195-ARR_v2_17@0,195-ARR_v1_12@3,"In the reference translations of the corpus, each target gender-marked word -corresponding to a neutral expression in the English source -is annotated with its alternative wrong gender form (e.g. en: the girl left; it: la<il> ragazza è an-data<andato> via).","In the reference translations, each gender-marked word -corresponding to a neutral expression in the source -is annotated with its alternative wrong gender form (e.g. en: the girl left; it: la<il> ragazza è andata<andato> via).","Modify,Clarity",Clarity
2296,207-ARR,207-ARR_v2_2@5,207-ARR_v1_2@5,"We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracy on downstream tasks (e.g. GLUE).","We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE).","Modify,Grammar",Grammar
2297,207-ARR,207-ARR_v2_30@3,207-ARR_v1_30@3,Hoory et al. (2021a) improved on these bounds by using Gaussian noise.,Hoory et al. (2021b) improved on these bounds by using Gaussian noise.,"Modify,Fact/Evidence",Fact/Evidence
2298,207-ARR,207-ARR_v2_46@3,207-ARR_v1_44@2,"For DP-SP we use the unigram Sentence-Piece algorithm (Kudo and Richardson, 2018).","For DP-SP we use the popular unigram SentencePiece algorithm (Kudo and Richardson, 2018).","Modify,Clarity",Clarity
2299,207-ARR,207-ARR_v2_58@2,207-ARR_v1_57@2,"Even for the most stringent guarantees of DP-Train ( of 6.06) which result in approx 20% of pretrain accuracy drop, on average GLUE fine-tuned performance is not significantly different from the baseline.","Even for the most stringent guarantees of DP-Train ( of 6.06) which result in approx 20% of pretrain accuracy drop, on average glue fine-tuned performance is not significantly different from the baseline.","Modify,Grammar",Grammar
2300,207-ARR,207-ARR_v2_58@4,207-ARR_v1_57@4,"On average, we also see that full-DP is not significantly better on subsequent fine-tuning tasks (e.g. mnli_m, mnli_msm, qnli etc), however for some tasks (e.g. cola) fine-tuning performance is significantly better than that of a (non-private) baseline.","On average, We also see that full-DP is not significantly better on subsequent fine-tuning tasks (e.g. mnli_m, mnli_msm, qnli etc), however for some tasks (e.g. cola) fine-tuning performance is significantly better than that of a (non-private) baseline.","Modify,Grammar",Grammar
2301,207-ARR,207-ARR_v2_58@5,207-ARR_v1_57@5,"On average, even DP-train models have approximately the same GLUE performance (difference insignificant).","On average, even DP-train models have approximately the same GLUE-fine tuning performance (the difference is insignificant).","Modify,Clarity",Clarity
2302,207-ARR,207-ARR_v2_62@0,207-ARR_v1_60@0,"One important caveat here is that the span corruption training objective was splitting a piece of text into input/target randomly, so it is possible a different definition of memorization would be more suitable.","One important caveat here is that the span corruption training objective was splitting a piece of text into input/target randomly, so it is possible a different definition of memorization would be more suitable here.","Modify,Clarity",Clarity
2303,207-ARR,207-ARR_v2_63@8,207-ARR_v1_62@4,For a large this protective effect is almost non-existent.,For a large this protective effect is almost non-existant.,"Modify,Grammar",Grammar
2304,207-ARR,207-ARR_v2_68@0,207-ARR_v1_67@0,"Theorem 1 (Hoory et al., 2021b) uses the notion of k and m (maximum L2 and infinity norms) of sentence-level (1-D vector) histogram.","Theorem 1 (Hoory et al., 2021a) uses the notion of k and m (maximum L2 and infinity norms) of sentence-level (1-D vector) histogram.","Modify,Fact/Evidence",Fact/Evidence
2305,207-ARR,207-ARR_v2_68@7,207-ARR_v1_67@7,"Just as in (Hoory et al., 2021b), we assume that sentence length is 256 words, which is a very pessimistic estimate-this translates into approximately 2.45 tokens per word.","Just as in (Hoory et al., 2021a), we assume that sentence length is 256 words, which is a very pessimistic estimate-this translates into approximately 2.45 tokens per word.","Modify,Fact/Evidence",Fact/Evidence
2306,207-ARR,207-ARR_v2_68@9,207-ARR_v1_67@9,"Additionally, (Hoory et al., 2021b) authors provide a Corollary that allows to obtain slightly better bounds while using approximately the same clipping norm and the same level of noise.","Additionally, (Hoory et al., 2021a) authors provide a Corollary that allows to obtain slightly better bounds while using approximately the same clipping norm and the same level of noise.","Modify,Fact/Evidence",Fact/Evidence
2307,207-ARR,207-ARR_v2_13@0,207-ARR_v1_13@0,"In §5.2 we verify how our models fare with respect to an empirical ""privacy"" definition, namely robustness to training data extraction attacks.","In §5.2 we verify how our models fair with respect to an empirical ""privacy"" definition, namely robustness to training data extraction attacks.","Modify,Grammar",Grammar
2308,207-ARR,207-ARR_v2_19@5,207-ARR_v1_19@5,"Hoory et al. (2021a) looked into DP fine-tuning of a (publicly) pre-trained BERT model (Devlin et al., 2019) in the medical domain and explored how DP fine tuning affects the performance and privacy of the models.","Hoory et al. (2021c) looked into DP fine-tuning of a (publicly) pre-trained BERT model (Devlin et al., 2019) in the medical domain and explored how DP fine tuning affects the performance and privacy of the models.","Modify,Fact/Evidence",Fact/Evidence
2395,21-ARR,21-ARR_v2_11@0,21-ARR_v1_11@0,"Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., , 2021bXu & McAuley, 2022).","Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks .","Modify,Fact/Evidence",Fact/Evidence
2396,21-ARR,21-ARR_v2_11@2,21-ARR_v1_11@2,"Hinton et al. (2015b) first introduced the idea of knowledge distillation to exploit the ""dark knowledge"" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model.","Hinton et al. (2015) first introduced the idea of knowledge distillation to exploit the ""dark knowledge"" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model.","Modify,Fact/Evidence",Fact/Evidence
2397,21-ARR,21-ARR_v2_4@1,21-ARR_v1_4@1,"Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015b) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015b;Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Ahn et al., 2019;Park et al., 2019;Passalis & Tefas, 2018;Heo et al., 2019;Kim et al., 2018;Shi et al., 2021;Sanh et al., 2019;Jiao et al., 2019;Wang et al., 2020b).","Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015;Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Ahn et al., 2019;Park et al., 2019;Passalis & Tefas, 2018;Heo et al., 2019;Kim et al., 2018;Shi et al., 2021;Sanh et al., 2019;Jiao et al., 2019;Wang et al., 2020b).","Modify,Fact/Evidence",Fact/Evidence
2398,21-ARR,21-ARR_v2_12@6,21-ARR_v1_12@6,"For example, meta pseudo labels (Pham et al., 2020) use meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model.","For example, meta pseudo labels (Pham et al., 2020) uses meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model.","Modify,Grammar",Grammar
2399,21-ARR,21-ARR_v2_12@10,21-ARR_v1_12@10,"In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problems, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.","In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problem, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.","Modify,Grammar",Grammar
2400,21-ARR,21-ARR_v2_13@3,21-ARR_v1_13@3,"Liu et al. (2020) proposed a self-distillation network which utilizes meta-learning to train a label-generator as a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers.","Liu et al. (2020) proposed a self-distillation network and utilizes meta-learning to train a label-generator, which is a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers.","Modify,Clarity",Clarity
2401,21-ARR,21-ARR_v2_20@2,21-ARR_v1_20@2,"Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error (MSE) between student and teacher logits, the similarity between the student and the teacher's attention distribution, etc.","Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error between student and teacher logits, the similarity between the student and the teacher's attention distribution, etc.","Modify,Clarity",Clarity
2402,21-ARR,21-ARR_v2_34@2,21-ARR_v1_29@2,"This ""learning to teach"" paradigm naturally fits the bi-level optimization framework in meta learning literature.","This ""learning to teach"" paradigm naturally fits the bi-level optimization framework in the meta learning literature.","Modify,Grammar",Grammar
2405,21-ARR,21-ARR_v2_9@5,21-ARR_v1_9@5,"Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably because of its ability to adjust the parameters of the teacher model.","Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably because of its ability to adjust its parameters.","Modify,Claim",Claim
2693,246-ARR,246-ARR_v2_35@0,246-ARR_v1_34@0,"As M approaches M test , the automatic metrics' τ values approach 1, which is much higher than the respective values at M jud , typically around 0.6-0.8.","As M approaches M test , the automatic metrics' τ values approach 1, which is significantly higher than the respective values at M jud , typically around 0.6-0.8.","Modify,Clarity",Clarity
2694,246-ARR,246-ARR_v2_36@1,246-ARR_v1_35@1,"We see that on both datasets the judgments' rankings are still quite variable, reaching a maximum τ of around 0.8-0.85.","We see that on both datasets the judgments' rankings are still quite variable, reaching a maximum of around 0.8-0.85 τ .","Modify,Clarity",Clarity
2695,246-ARR,246-ARR_v2_41@2,246-ARR_v1_40@2,"The largest decrease in width is in the ROUGE family of metrics on SummEval, potentially because that metric and dataset combination saw the biggest improvement in ranking stability (see Fig. 3).","The largest decrease in width is in the ROUGE family of metrics on SummEval, likely because that metric and dataset combination saw the biggest improvement in ranking stability (see Fig. 3).","Modify,Clarity",Clarity
2696,246-ARR,246-ARR_v2_53@2,246-ARR_v1_52@2,"Based on a survey of recent summarization papers in *ACL conferences (see Appendix C), we found that the average improvement over baseline/state-of-the-art models that was reported on the CNN/Dailymail dataset was on average 0.5 ROUGE-1.","Based on a survey of summarization papers in *ACL conferences over the past few years, we found that the average improvement over baseline/state-of-the-art models that was reported on the CNN/Dailymail dataset was on average 0.5 ROUGE-1.","Modify,Fact/Evidence",Fact/Evidence
2697,246-ARR,246-ARR_v2_54@0,246-ARR_v1_53@0,"To that end, we redefine a high quality evaluation metric to be one for which a small difference in scores reliably indicates a difference in quality.","To that extent, we redefine a high quality evaluation metric to be one for which a small difference in scores reliably indicates a difference in quality.","Modify,Clarity",Clarity
2698,246-ARR,246-ARR_v2_6@1,246-ARR_v1_6@1,"First, the metrics' scores which are used in practice are not the ones which are evaluated in system-level correlations; researchers compare systems based on metric scores calculated on the entire test set but calculate scores for system-level correlations when evaluating metrics on a much smaller subset of judged summaries.","First, the metrics' scores which are used in practice are not the ones which are evaluated in system-level correlations: Researchers compare systems based on metric scores calculated on the entire test set but calculate scores for system-level correlations when evaluating metrics on a much smaller subset of judged summaries.","Modify,Grammar",Grammar
2699,246-ARR,246-ARR_v2_65@1,246-ARR_v1_64@1,"We view our work as continuing this direction of research, described next.",We view our work as continuing this direction of research.,"Modify,Clarity",Clarity
2700,246-ARR,246-ARR_v2_9@2,246-ARR_v1_9@2,"This allows us to show, for example, that a ROUGE-1 score difference of less than 0.5 between systems has almost no correlation to how humans would rank the same two systems according to our best estimates ( §4.2).",This allows us to show that a ROUGE-1 score difference of less than 0.5 between systems has almost no correlation to how humans would rank the same two systems according to our best estimates ( §4.2).,"Modify,Clarity",Clarity
2701,246-ARR,246-ARR_v2_2@4,246-ARR_v1_2@4,"Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice.","Second, we propose to calculate correlations only on pairs of systems which are separated by differences in automatic scores that are commonly used to argue one system is of higher quality.","Modify,Claim",Claim
2709,249-ARR,249-ARR_v2_24@1,249-ARR_v1_25@1,"For machine learning as a service (MLaaS), A can be applied to perform a man-in-the-middle attack on the application programming interfaces.","For Machine Learning as a Service (MLaaS), A can be directly applied to perform a man-in-the-middle attack on the application programming interfaces.","Modify,Grammar",Grammar
2710,249-ARR,249-ARR_v2_24@2,249-ARR_v1_25@2,"Moreover, even if the raw data are protected and the transmission channel is secure, a curious service provider can train its attacker A to collect personas of service users.","Moreover, even if the raw data are protected and the transmission channel is secure, a curious service provider can train its attacker model A to collect personas of service users.","Modify,Clarity",Clarity
2711,249-ARR,249-ARR_v2_26@0,249-ARR_v1_27@0,The LM training objective in Equation 1 only considers the utility of chatbots.,The simple LM training objective only considers the utility of chatbots.,"Modify,Claim",Claim
2712,249-ARR,249-ARR_v2_27@0,249-ARR_v1_28@0,"Following the intuition that the adversary cannot obtain better results than a random guess, in Section 4.1, we propose KL loss that aims to flatten the persona predictor's estimated distribution.","Following the intuition that the adversary cannot obtain better results than a random guess, we propose KL loss that aims to flatten the persona predictor's estimated distribution.","Modify,Fact/Evidence",Fact/Evidence
2713,249-ARR,249-ARR_v2_27@1,249-ARR_v1_28@1,"Based on minimizing the mutual information between hidden states f (u) of chatbots and private persona attributes s, we propose MI loss in Section 4.2.","Based on minimizing the mutual information between hidden states f (u) of chatbots and private persona attributes s, we propose MI loss.","Modify,Fact/Evidence",Fact/Evidence
2714,249-ARR,249-ARR_v2_31@1,249-ARR_v1_30@2,"For optimization, we can leave out constant terms and the logarithm (Mireshghallah et al., 2021) to obtain the following loss function:","For optimization, we can leave out constant terms (Mireshghallah et al., 2021) and obtain the following loss function:","Modify,Fact/Evidence",Fact/Evidence
2715,249-ARR,249-ARR_v2_42@2,249-ARR_v1_44@2,"Instead, we use p Ψ (s|f (u)) to approximate q(s|f (u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2019):","Instead, we use p Ψ (s|f (u)) to approximate q(s|f (u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2018):","Modify,Fact/Evidence",Fact/Evidence
2716,249-ARR,249-ARR_v2_53@0,249-ARR_v1_55@0,Overall Objective,Overall Loss,"Modify,Other",Other
2717,249-ARR,249-ARR_v2_57@0,249-ARR_v1_59@0,Experiments,Experiment,"Modify,Grammar",Grammar
2718,249-ARR,249-ARR_v2_59@0,249-ARR_v1_61@0,Experimental Settings,Experimental Setting,"Modify,Grammar",Grammar
2721,249-ARR,249-ARR_v2_4@3,249-ARR_v1_4@3,"Unfortunately, large language models tend to memorize training data and some private data can be recovered from models (Pan et al., 2020;Carlini et al., 2021).","Unfortunately, large language models tend to memorize training data and some private data can be recovered from models via black-box training data extraction attacks (Carlini et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
2722,249-ARR,249-ARR_v2_68@1,249-ARR_v1_68@10,"Suppose the adversary knows the persona with Max-Ratio, then it can improve its guess by not predicting this persona, which is a threat for fewer persona labels (for example, binary classification).","Suppose the adversary knows the persona with Max-Ratio, then he can improve his guess by not predicting this persona, which is a threat for fewer persona labels (for example, binary classification).","Modify,Clarity",Clarity
2723,249-ARR,249-ARR_v2_7@8,249-ARR_v1_7@8,"We combine proposed KL divergence loss (KL loss) with mutual information loss (MI loss) (Song et al., 2019) as additional defense objectives to train the GPT-2 and decrease the attacker's persona inference accuracy to 0.53%.","We combine proposed KL divergence loss (KL loss) with mutual information loss (MI loss) (Song et al., 2018) as additional defense objectives to train the GPT-2 and decrease the attacker's persona inference accuracy to 0.53%.","Modify,Fact/Evidence",Fact/Evidence
2724,249-ARR,249-ARR_v2_12@7,249-ARR_v1_13@4,"Though the setup of this work is similar to ours, they merely consider simple cases of data recovery with given rules and suffer great utility degradation to obtain optimal defense performance.","Though the setup of this work is similar to ours, they merely considered simple cases of data recovery with given rules and suffered great utility degradation to obtain optimal defense performance.","Modify,Grammar",Grammar
2725,249-ARR,249-ARR_v2_13@0,249-ARR_v1_14@0,Attacking on Language Models,Attacking on Casual Language Model,"Modify,Clarity",Clarity
2762,250-ARR,250-ARR_v2_15@4,250-ARR_v1_14@4,"3 In other words, we don't compare with works like TinyBERT (Jiao et al., 2019) or MiniLM (Wang et al., 2020b), which use extra losses like self-attention distribution matching.","Otherwise said, we don't compare with works like TinyBERT (Jiao et al., 2019) or MiniLM (Wang et al., 2020b), which use extra losses like self-attention distribution matching.","Modify,Clarity",Clarity
2763,250-ARR,250-ARR_v2_22@2,250-ARR_v1_21@2,"Since in Sun et al. (2020a), the mean-pooling representation shows better results, we adopt it to compute the sentence representation of each layer.","Since in (Sun et al., 2020a), the mean-pooling representation shows better results, we adopt it to compute the sentence representation of each layer.","Modify,Grammar",Grammar
2764,250-ARR,250-ARR_v2_53@8,250-ARR_v1_47@3,"CoDIR results are adopted from their paper, and we followed their experimental protocol by not reporting scores on STS-B. On average, RAIL-KD performs on par with CoDIR (+0.2%) and outperforms it on 5 out of 8 datasets, while being almost two-times faster as shown in the next section.","CoDIR results are adopted from their paper, and we followed their experimental protocol by not reporting scores on STS-B. On average, RAIL-KD performs on par with CoDIR (+0.2%) and outperforms it on 5 out of 8 datasets, while being almost twice faster as shown in the next section.","Modify,Clarity",Clarity
2765,250-ARR,250-ARR_v2_55@2,250-ARR_v1_49@2,We used this configuration because CoDIR pretrained student models are not available and we can only run CoDIR code out-of-the-box.,We used this configuration because CODIR pretrained student models are not available and we can only run CODIR code out-of-the-box.,"Modify,Grammar",Grammar
2766,250-ARR,250-ARR_v2_68@1,250-ARR_v1_60@4,"Moreover, we observe that ALP-KD results have less similarity scores in the upper intermediate layers.","Moreover, we observe that ALP-KD results in less similarity scores in the upper intermediate layers.","Modify,Grammar",Grammar
2767,250-ARR,250-ARR_v2_12@0,250-ARR_v1_11@0,"In recent years, a wide range of methods have tried to expand knowledge transfer of transformerbased (Vaswani et al., 2017) NLU models beyond logits matching.","Recent years, have seen a wide range of methods have witnessed to expand knowledge transfer of transformer-based (Vaswani et al., 2017) NLU models beyond logits matching.","Modify,Grammar",Grammar
2768,250-ARR,250-ARR_v2_12@2,250-ARR_v1_11@2,"TinyBERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020b), and MiniLM (Wang et al., 2020b) matched the intermediate layers representations and self-attention distributions of the teacher and the student.","Tiny-BERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020b), and MiniLM (Wang et al., 2020b) matched the intermediate layers representations and selfattention distributions of the teacher and the student.","Modify,Grammar",Grammar
2769,250-ARR,250-ARR_v2_13@13,250-ARR_v1_12@13,"Alternatively, CoDIR (Sun et al., 2020a) exploited contrastive learning (Tian et al., 2019) to perform intermediate layers matching between the teacher and the student models with no deterministic mapping.","Alternatively, CODIR (Sun et al., 2020a) exploited contrastive learning (Tian et al., 2019) to perform intermediate layers matching between the teacher and the student models with no deterministic mapping.","Modify,Grammar",Grammar
2770,250-ARR,250-ARR_v2_14@0,250-ARR_v1_13@0,"Table 1 summarizes the main characteristics of the existing state-of-the-art intermediate layer distillation techniques (PKD, CKD, and CoDIR) used for pre-trained language models compared with our proposed RAIL-KD.","Table 1 summarizes the main characteristics of the existing state-of-the-art intermediate layer distillation techniques (PKD (Sun et al., 2019), CKD (Wu et al., 2020a), and CoDIR) used for pre-trained language models compared with our proposed RAIL-KD.","Modify,Fact/Evidence",Fact/Evidence
2771,250-ARR,250-ARR_v2_15@1,250-ARR_v1_14@1,"For instance, RAIL-KD is roughly two-times faster than CoDIR in a 24 to 6 layer compression.","For instance, RAIL-KD is roughly twice faster than CoDIR in a 24 to 6 layer compression.","Modify,Clarity",Clarity
2848,258-ARR,258-ARR_v2_4@2,258-ARR_v1_4@1,"However, designing high-quality cloze tests for language learning is a laborious process that involves finding an optimal distribution of gaps based on aspects such as function, distance, number of answers, etc. (ALTE, 2005;2011).","Designing high-quality cloze tests for language learning is a laborious process that involves finding an optimal distribution of gaps based on aspects such as function, distance, number of answers, etc. (ALTE, 2005;2011).","Modify,Clarity",Clarity
2849,258-ARR,258-ARR_v2_28@5,258-ARR_v1_25@5,A combination of gaps that yields lower KL divergence is assumed to be a better solution.,A combination of gaps that yields lower KL divergence is assumed to be a better selection.,"Modify,Clarity",Clarity
2850,258-ARR,258-ARR_v2_32@0,258-ARR_v1_28@2,"For this reason, we use a collection of expertly created open cloze tests at the B2 CEFR level that was kindly provided by Cambridge University Press & Assessment (CUP&A) for research purposes.","For this reason, we use a collection of expertly created open cloze tests at the B2 CEFR level that was kindly provided by <anonymised> for research purposes.","Modify,Fact/Evidence",Fact/Evidence
2851,258-ARR,258-ARR_v2_34@4,258-ARR_v1_33@3,"6 On top of the encoding layers, we have two branches that are being learned simultaneously (Figure 2).","6 On top of the encoding layers, we have two branches that are being learned simultaneously.","Modify,Fact/Evidence",Fact/Evidence
2852,258-ARR,258-ARR_v2_34@6,258-ARR_v1_34@1,"For the second branch, we add ELECTRA's generation layer plus a linear layer which aims to predict the best word from the whole vocabulary as an auxiliary task.","For the second branch, we add ELECTRA's generation layer plus a linear layer which aims to predict the best word from the whole vocabulary.","Modify,Fact/Evidence",Fact/Evidence
2853,258-ARR,258-ARR_v2_5@2,258-ARR_v1_5@2,"Our main objective is standard token classification, where we aim to minimise the error of classifying a token as a gap or not.","One objective is standard token classification, where we aim to minimise the error of classifying a token as a gap or not.","Modify,Fact/Evidence",Fact/Evidence
2854,258-ARR,258-ARR_v2_36@0,258-ARR_v1_36@0,"We compare our multi-objective ELECTRA model to other systems, namely:","We compare our ELECTRA-based model to other systems, namely:","Modify,Fact/Evidence",Fact/Evidence
2855,258-ARR,258-ARR_v2_39@0,258-ARR_v1_40@0,We use the pre-trained base model with standard parameters and fine-tune the weights of the whole architecture.,We use the base pre-trained model with standard parameters and fine-tune the weights of the whole architecture.,"Modify,Clarity",Clarity
2856,258-ARR,258-ARR_v2_40@2,258-ARR_v1_40@1,"Both random and Exercise Maker attempt to generate the same number of gaps per task as defined in the gold standard, although this is not always possible since the required conditions (such as specific words or PoS) are not always met.","Both random and Exercise Maker attempt to generate the same number of gaps per task as defined in the gold standard, although this is not always possible since the required conditions are not always met.","Modify,Claim",Claim
2857,258-ARR,258-ARR_v2_5@3,258-ARR_v1_5@3,The second and auxiliary objective is a language-model-based objective whereby we attempt to minimise the language model error when predicting the right answer for each gap.,The other objective is a language-modelbased objective whereby we attempt to minimise the language model error when predicting the right answer for each gap.,"Modify,Fact/Evidence",Fact/Evidence
2858,258-ARR,258-ARR_v2_44@0,258-ARR_v1_43@0,We also report human evaluation by three test experts from CUP&A who volunteered for the task.,"We also performed human evaluation, which was carried out by three test experts from <anonymised> who volunteered for the task.","Modify,Fact/Evidence",Fact/Evidence
2859,258-ARR,258-ARR_v2_47@1,258-ARR_v1_46@1,"Table 3 reports the results of our multi-objective ELECTRA model (enhanced with dependency information) as well as the random baseline, Exercise Maker, BERT, and the standard single-task ELECTRA.","Table 3 reports the results of our multi-objective ELECTRA model (enhanced with dependency information) as well as the random baseline, Exercise Maker and BERT.","Modify,Fact/Evidence",Fact/Evidence
2860,258-ARR,258-ARR_v2_2@0,258-ARR_v1_2@0,This paper presents the first multi-objective transformer model for constructing open cloze tests that exploits generation and discrimination capabilities to improve performance.,This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance.,"Modify,Clarity",Clarity
2861,258-ARR,258-ARR_v2_5@4,258-ARR_v1_5@4,"Our solution is based on a pre-trained ELECTRA (Clark et al., 2020) model that is finetuned on the two described objectives in a multitask scenario.","Our solution is based on a pre-trained ELECTRA (Clark et al., 2020) model that is fine-tuned on the two described objectives in a multi-task scenario.","Modify,Grammar",Grammar
2862,258-ARR,258-ARR_v2_48@4,258-ARR_v1_46@8,"Although the improvement of our multi-objective ELECTRA model over BERT does not seem to be very significant based simply on P, R and F 1 , a closer look at the results reveals that BERT produces a much higher number of repeated gaps (25 compared to 9 by multi-objective ELECTRA) as well as more cases of gaps in close proximity, as shown in Figure 3.","Although the improvement of our multi-objective ELECTRA model over BERT does not seem to be very significant based simply on P, R and F 1 , a closer look at the results reveals that BERT produces a much higher number of repeated gaps (25 compared to 9 by multi-objective ELECTRA) as well as more cases of gaps in close proximity as shown in Figure 3.","Modify,Grammar",Grammar
2863,258-ARR,258-ARR_v2_52@1,258-ARR_v1_50@1,"It also creates a better spread of gaps, as shown by a lower KL-divergence between the average PoS distribution of the output and that of the gold standard (0.55 with post-processing as opposed to 0.59 without it).","It also creates a better spread of gaps, as shown by a lower KL-divergence between the average POS distribution of the output and that of the gold standard (0.55 with post-processing as opposed to 0.59 without it).","Modify,Grammar",Grammar
2864,258-ARR,258-ARR_v2_53@1,258-ARR_v1_51@1,"Table 5 shows that they significantly improve R, which results in higher overall F 1 .","Table 5 shows that significantly improve R, which results in higher overall F 1 .","Modify,Clarity",Clarity
2865,258-ARR,258-ARR_v2_69@2,258-ARR_v1_66@2,"The worst performing classes are PRON (77%), NUM (77%) and VERB (75%) as opposed to the previous AUX and PART counterparts (now 79% and 83% respectively).","The worst performing classes are PRON (77%), NUM (77%) and VERB (77%) as opposed to the previous AUX and PART counterparts (now 79% and 83% respectively).","Modify,Fact/Evidence",Fact/Evidence
2866,258-ARR,258-ARR_v2_17@8,258-ARR_v1_14@6,The first objective is typical of any standard token classification model and constitutes our key task.,The first objective is typical of any standard token classification model.,"Modify,Fact/Evidence",Fact/Evidence
2867,258-ARR,258-ARR_v2_18@0,258-ARR_v1_15@0,The second and auxiliary objective attempts to model our preference for gaps with a restricted number of answers while also ensuring that the original word can be guessed from the context.,The second objective attempts to model our preference for gaps with a restricted number of answers while also ensuring that the original word can be guessed from the context.,"Modify,Fact/Evidence",Fact/Evidence
2868,258-ARR,258-ARR_v2_4@0,258-ARR_v1_4@0,"Open cloze (Taylor, 1953) tests are a common type of exercise where words are removed from a piece of text and must then be filled in by the students without any options to choose from.",Open cloze tests are a common type of learning exercise where a student must complete a sentence by filling in a gap without any options to choose from.,"Modify,Fact/Evidence",Fact/Evidence
2879,276-ARR,276-ARR_v2_43@3,276-ARR_v1_78@3,"However, F −1 as used in ( 9) is increasing only on interval [0, 0.5] (Fig. 1).","However, F −1 as used in ( 9) is increasing only on interval [0, 0.5].","Modify,Fact/Evidence",Fact/Evidence
2880,276-ARR,276-ARR_v2_43@4,276-ARR_v1_78@4,"For v ≥ 0.5, we get negative argument to ln which yields a complex function, whose real part is even decreasing.","For v ≥ 0.5, we get negative argument to ln which yields a complex function, whose real parts is even decreasing.","Modify,Grammar",Grammar
2881,276-ARR,276-ARR_v2_46@1,276-ARR_v1_80@1,"We rely on the standard proof of the Laplace mechanism as shown, e.g, by Habernal (2021).","We rely on the standard proof of the Laplace mechanism as shown, e.g, by (Dwork and Roth, 2013;Habernal, 2021).","Modify,Fact/Evidence",Fact/Evidence
2882,276-ARR,276-ARR_v2_0@0,276-ARR_v1_0@0,How reparametrization trick broke differentially-private text representation learning,How reparametrization trick broke differentially-private text representation leaning,"Modify,Grammar",Grammar
2883,276-ARR,276-ARR_v2_33@0,276-ARR_v1_35@0,"While both (4) and ( 7) generate samples from Lap(µ; b), note the substantial difference between u and v, since each is drawn from a different uniform distribution (see visualizations in Fig. 1).","While both (4) and ( 7) generate samples from Lap(µ; b), note the substantial difference between u and v, since each is drawn from a different uniform distribution.","Modify,Fact/Evidence",Fact/Evidence
3097,30-ARR,30-ARR_v2_21@2,30-ARR_v1_24@2,"Only adjectives, adverbs, nouns, and verbs were considered as rationales.","Only adjectives, adverbs, nouns, and verbs were considered.","Modify,Clarity",Clarity
3098,30-ARR,30-ARR_v2_21@5,30-ARR_v1_24@5,"We also limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams).","We limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams).","Modify,Clarity",Clarity
3099,30-ARR,30-ARR_v2_4@0,30-ARR_v1_4@0,"Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020;Srivastava et al., 2020) in datasets can cause sub-optimal model performance for neural networks.","Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020;Srivastava et al., 2020) in datasets cause sub-optimal model performance for neural networks.","Modify,Clarity",Clarity
3100,30-ARR,30-ARR_v2_22@4,30-ARR_v1_25@4,All re-annotated examples were approved only if all authors were happy with the quality of the annotations.,All re-annotated examples were approved only if all authors were happy with the quality of annotations.,"Modify,Grammar",Grammar
3101,30-ARR,30-ARR_v2_23@0,30-ARR_v1_26@0,"Our annotation procedure generated 5,073 rationales in 855 movie reviews involved in Section 3.1 and 3.3 (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).","Our annotation procedure generated 5,073 rationales in 855 movie reviews (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).","Modify,Fact/Evidence",Fact/Evidence
3102,30-ARR,30-ARR_v2_23@3,30-ARR_v1_26@3,"Note that our approach using 100 labelled examples can outperform manual CAD (Kaushik et al., 2020) using the entire training set of 1,707 examples (see Section 5.3), making our approach 300×1707 183.68×100 ≈ 27.88 times more efficient than manually generated CAD.","Note that our approach using 100 labelled examples can outperform the manual CAD (Kaushik et al., 2020) using the entire training set with 1,707 examples (see Section 5.3), making our approach 300×1707 183.68×100 ≈ 27.88 times more efficient than manually generated CAD.","Modify,Grammar",Grammar
3103,30-ARR,30-ARR_v2_4@1,30-ARR_v1_4@1,"As shown in Figure 1, the bold phrases-""100% bad"" and ""brain cell killing""-are underlying causes for a negative sentiment prediction that most human readers would recognise.","As shown in Figure 1, the phrases in bold-""100% bad"" and ""brain cell killing""-are underlying causes for a negative prediction most human readers would recognise.","Modify,Clarity",Clarity
3104,30-ARR,30-ARR_v2_26@1,30-ARR_v1_29@1,"To generate a semi-factual example, x ′ i , we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms.","To generate a semi-factual example, x i ', we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms.","Modify,Grammar",Grammar
3105,30-ARR,30-ARR_v2_27@0,30-ARR_v1_30@0,"In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x ′ i , with some replaced non-rationales and all the other tokens identical to x i .","In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x i ', with some replaced non-rationales and all the other tokens identical to x i .","Modify,Grammar",Grammar
3106,30-ARR,30-ARR_v2_2@0,30-ARR_v1_2@0,We present a novel rationale-centric framework with human-in-the-loop -Rationales-centric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios.,We present a novel rational-centric framework with human-in-the-loop -Rationales-centric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios.,"Modify,Grammar",Grammar
3107,30-ARR,30-ARR_v2_4@3,30-ARR_v1_4@3,"The underlined phrase-""acting and plot""has been incorrectly recognised as a causal term by the model used fort this example, and is referred to as a spurious pattern.","The underlined phrase-""acting and plot""-are incorrectly recognised as causal terms by the model, and are referred to as spurious patterns.","Modify,Clarity",Clarity
3108,30-ARR,30-ARR_v2_5@0,30-ARR_v1_5@0,"Spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021), and are usually useless, or even harmful, at test time.","These spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021) and are usually useless, or even harmful, at test time.","Modify,Clarity",Clarity
3109,30-ARR,30-ARR_v2_32@0,30-ARR_v1_35@0,Note that the two correction methods in dynamic human-intervened correction can operate in parallel and the generated examples are added to the small training set to re-train the model.,Note that the two correction methods in dynamic human-intervened correction can be operated in parallel and the generated examples are added to the small training set to re-train the model.,"Modify,Clarity",Clarity
3110,30-ARR,30-ARR_v2_37@0,30-ARR_v1_37@0,"Broadly speaking, our RDL framework takes advantage of invariance that makes a model less sensitive to non-rationale words or spurious patterns (Tu et al., 2020; in favour of focusing on useful mappings of rationales to labels.","Broadly speaking, our RDL framework acts like a sensible inductive bias (Wu et al., 2021b;Warstadt and Bowman, 2020) that restricts a model from superficially focusing on whole texts or spurious patterns (Tu et al., 2020; in favour of focusing on useful mappings of rationales to labels.","Modify,Fact/Evidence",Fact/Evidence
3111,30-ARR,30-ARR_v2_49@1,30-ARR_v1_53@1,"Following Kaushik et al. (2020), we fine-tune RoBERTa for up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).","Following Kaushik et al. (2020), we fine-tune RoBERTa up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).","Modify,Grammar",Grammar
3112,30-ARR,30-ARR_v2_49@4,30-ARR_v1_54@2,"We found that setting the learning rate to {5e-5, 5e-6 and 5e-6} could optimise Static, Static+n, and Full, respectively.","We found that setting the learning rate to 5e-5, 5e-6 and 5e-6 could optimise Static, Static+n, and Full, respectively.","Modify,Grammar",Grammar
3113,30-ARR,30-ARR_v2_51@1,30-ARR_v1_56@1,"Among all Static+n methods, Static+350 seems the best-performing method and exceeds Static with a 1.56% in-distribution improvement in average accuracy.","Among all Static+n methods, Static+350 seems the best-performing method that exceeds Static with a 1.56% in-distribution improvement in average accuracy.","Modify,Clarity",Clarity
3114,30-ARR,30-ARR_v2_51@2,30-ARR_v1_56@2,"Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, and 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets respectively.","Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets, respectively.","Modify,Clarity",Clarity
3115,30-ARR,30-ARR_v2_52@0,30-ARR_v1_57@0,"The Static+n methods can even outperform Full (i.e. normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset.","The Static+n methods can even outperform Full (i,e, the normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset.","Modify,Grammar",Grammar
3116,30-ARR,30-ARR_v2_52@4,30-ARR_v1_57@4,"It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp dataset (93.66±0.84) than on the in-distribution dataset (93.23±0.46), which could also be explained by the high similarity between the underlying distributions of these two datasets.","It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp (93.66±0.84) than on the in-distribution dataset (93.23±0.46), which could also be explained by the high similarity between the underlying distributions of these two test sets.","Modify,Clarity",Clarity
3117,30-ARR,30-ARR_v2_52@6,30-ARR_v1_57@6,"We believe that the duplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.","We believe that the multiplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.","Modify,Claim",Claim
3118,30-ARR,30-ARR_v2_5@3,30-ARR_v1_7@1,"In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu and MacNamee, 2020;Lu et al., 2021).","In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3119,30-ARR,30-ARR_v2_53@0,30-ARR_v1_58@0,"As shown in Table 3, RR is slightly better than the baseline Static approach.","As shown in Table 3, RR is slightly better than the baseline Static.","Modify,Clarity",Clarity
3120,30-ARR,30-ARR_v2_53@1,30-ARR_v1_58@1,"This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to the original data, introducing noise that helps prevent overfitting to some extent.","This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to original data, introducing noise that helps prevent overfitting to some extent.","Modify,Grammar",Grammar
3121,30-ARR,30-ARR_v2_53@2,30-ARR_v1_58@2,"However, the magnitude of improvement of the Static+n method is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.","However, the magnitude of improvement of Static+n methods is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.","Modify,Grammar",Grammar
3122,30-ARR,30-ARR_v2_59@1,30-ARR_v1_63@1,"To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter).","To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter), respectively.","Modify,Clarity",Clarity
3123,30-ARR,30-ARR_v2_61@3,30-ARR_v1_65@3,"Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, but no improvement for the SemEval dataset.","Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, except for the SemEval dataset.","Modify,Clarity",Clarity
3124,30-ARR,30-ARR_v2_61@4,30-ARR_v1_65@4,"Finally, the performance of our methods is better that the state-of-the-art manual CAD method in few-shot learning scenarios on all OOD datasets.","Finally, the performance of our methods outperforms another state-of-the-art manual CAD method in fewshot learning scenarios on all OOD datasets.","Modify,Clarity",Clarity
3125,30-ARR,30-ARR_v2_63@2,30-ARR_v1_68@2,"As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increase in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.","As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increasing in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.","Modify,Grammar",Grammar
3126,30-ARR,30-ARR_v2_2@1,30-ARR_v1_2@1,"By using static semi-factual generation and dynamic humanintervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.","By using static semi-factual generation and dynamic human-intervened correction, RDL, acting like a sensible ""inductive bias"", exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.","Modify,Fact/Evidence",Fact/Evidence
3127,30-ARR,30-ARR_v2_12@4,30-ARR_v1_13@4,"In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002;Kenny and Keane, 2021), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same).","In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same).","Modify,Fact/Evidence",Fact/Evidence
3128,30-ARR,30-ARR_v2_12@6,30-ARR_v1_13@6,"We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power, both for in-distribution and OOD predictions.","We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power both for in-distribution and OOD predictions.","Modify,Grammar",Grammar
3129,30-ARR,30-ARR_v2_2@2,30-ARR_v1_2@2,Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarks-especially for few-shot learning scenarios.,"Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests, especially for few-shot learning scenarios, compared to many state-of-the-art benchmarks.","Modify,Clarity",Clarity
3130,30-ARR,30-ARR_v2_13@7,30-ARR_v1_17@7,"Human-the-loop Machine Learning (Wu et al., 2021) has received increasing research attention.","Human-the-loop Machine Learning (Wu et al., 2021a) has received increasing research attention.","Modify,Fact/Evidence",Fact/Evidence
3131,30-ARR,30-ARR_v2_14@1,30-ARR_v1_17@12,"Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights.","Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of some model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights.","Modify,Clarity",Clarity
3132,30-ARR,30-ARR_v2_14@2,30-ARR_v1_17@13,"However, previous work focuses on increasing the explainability of AI systems for high-stakes domains such as health and finance (Li et al., 2020;Yang et al., 2020b), instead of improving model robustness or generalisation ability.","However, previous work focuses on increasing the explainability of AI systems for high-stake domains such as health and finance, instead of improving the model robustness or generalisation ability.","Modify,Fact/Evidence",Fact/Evidence
3299,315-ARR,315-ARR_v2_31@2,315-ARR_v1_32@7,"However, there are two noticeable performance drops around relative frequency ranges of 10%-30% (bucket 2) and 90%-100% (bucket 6), denoted as Drop#1 (-0.3 BLEU) and Drop#2 (-0.6 BLEU).","However, there are two noticeable performance drops around relative frequency ranges of 10%-30% and 90%-100%, denoted as Drop#1 (-0.3 BLEU) and Drop#2 (-0.6 BLEU).","Modify,Fact/Evidence",Fact/Evidence
3300,315-ARR,315-ARR_v2_38@1,315-ARR_v1_39@1,We first build pseudo terms from the target by sampling 0-3 words (more tokens after tokenization) from reference as the pre-defined constraints for training.,We first build pseudo terms from the target by sampling 0-3 words from reference as the pre-defined constraints for training.,"Modify,Fact/Evidence",Fact/Evidence
3301,315-ARR,315-ARR_v2_40@4,315-ARR_v1_42@2,"As illustrated in Figure 2, we propose to directly align the target-side constraints with the source words and prompt the alignment information to the model during both training and inference.",We propose to directly align the target-side constraints with the source words and prompt the alignment information to the model during both training and inference.,"Modify,Fact/Evidence",Fact/Evidence
3302,315-ARR,315-ARR_v2_54@2,315-ARR_v1_59@2,"When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do.","When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference.","Modify,Claim",Claim
3303,315-ARR,315-ARR_v2_21@0,315-ARR_v1_22@0,Iterative refinement by editing is an NAT paradigm that suits constrained translations due to its flexibility.,"For NATs, iterative refinement by editing is an NAT paradigm that suits constrained translations due to its flexibility.","Modify,Clarity",Clarity
3352,32-ARR,32-ARR_v2_42@2,32-ARR_v1_25@2,The algorithm is implemented in PyTorch-1.10 and experiments are conducted on Nvidia RTX-6000 and RTX-A6000 GPU.,The algorithm is implemented in PyTorch-1.10 and experiments are conducted on Nvidia RTX-A6000 GPU with 48GB of memory.,"Modify,Fact/Evidence",Fact/Evidence
3353,32-ARR,32-ARR_v2_4@2,32-ARR_v1_4@2,"Data augmentation (Wei and Zou, 2019), regularization and re-initialization further improve the results.","Data augmentation (Wei and Zou, 2019), regularization and re-initialization (Zhang et al., 2021) further improve the results.","Modify,Fact/Evidence",Fact/Evidence
3354,32-ARR,32-ARR_v2_7@0,32-ARR_v1_6@0,Prior work have proposed regularization methods to overcome this problem .,"Prior work have proposed regularization methods to overcome this problem Zhang et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3355,32-ARR,32-ARR_v2_0@0,32-ARR_v1_0@0,Embedding Hallucination for Few-Shot Language Fine-tuning,Embedding Hallucination for Few-shot Language Fine-tuning,"Modify,Grammar",Grammar
3356,32-ARR,32-ARR_v2_8@0,32-ARR_v1_7@0,"Current common text data augmentation methods, such as EDA (Wei and Zou, 2019) (which have been used in recent few-shot learning papers (Wei et al., 2021;Basu et al., 2021)) and AEDA (Karimi et al., 2021) operate at the lexical level, which while resulting in human readable texts, lead to limited diversity due to the discrete nature of the lexical space.","Current common text data augmentation methods, such as EDA (Wei and Zou, 2019) (which have been used in recent few-shot learning papers (Wei et al., 2021;Basu et al., 2021)) and AEDA (Karimi et al., 2021a) operate at the lexical level, which while resulting in human readable texts, lead to limited diversity due to the discrete nature of the lexical space.","Modify,Fact/Evidence",Fact/Evidence
3357,32-ARR,32-ARR_v2_8@8,32-ARR_v1_7@8,"We further experimentally show the overall superiority of EmbedHalluc when comparing to regularization methods proposed to address the problem of over-fitting during fine-tuning of LMs, such as Mixout and Re-Init .","We further experimentally show the overall superiority of EmbedHalluc when comparing to regularization methods proposed to address the problem of over-fitting during fine-tuning of LMs, such as Mixout and Re-Init (Zhang et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3358,32-ARR,32-ARR_v2_10@2,32-ARR_v1_9@2,Other tricks include bias correction in optimizer and re-initialization of top layers in Transformer .,"Other tricks include bias correction in optimizer and re-initialization of top layers in Transformer (Zhang et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3359,32-ARR,32-ARR_v2_10@8,32-ARR_v1_9@8,"In NLP, few-shot learning has been successfully applied to machine translation (Arthaud et al., 2021), abstract summarizing (Fabbri et al., 2021), question and answering (Hua et al., 2020;Ram et al., 2021), and entity recognition (de Lichy et al., 2021;Tong et al., 2021;Ding et al., 2021), by meta learning (Li and Zhang, 2021;Bansal et al., 2020;Sharaf et al., 2020), data augmentation (Wei et al., 2021;Wei and Zou, 2019;Karimi et al., 2021;, and prompts Tam et al., 2021).","Few-shot learning has been successfully applied to machine translation (Arthaud et al., 2021), abstract summarizing (Fabbri et al., 2021), question and answering (Hua et al., 2020;Ram et al., 2021), and entity recognition (de Lichy et al., 2021;Tong et al., 2021;Ding et al., 2021), by meta learning (Li and Zhang, 2021;Bansal et al., 2020;Sharaf et al., 2020), data augmentation (Wei et al., 2021;Wei and Zou, 2019;Karimi et al., 2021b), and prompts (Gao et al., 2021;Tam et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3360,32-ARR,32-ARR_v2_14@0,32-ARR_v1_13@0,"GAN (Goodfellow et al., 2014) has led the revolution of generative models to achieve impressive results in synthesizing images (Zhu et al., 2017) and higher dimensional data .","GAN (Goodfellow et al., 2014) has led the revolution of generative models to achieve impressive results in synthesizing images (Zhu et al., 2017) and higher dimensional data (Wang et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
3361,32-ARR,32-ARR_v2_18@3,32-ARR_v1_17@3,"We propose Label Calibration (LabelCalib) by pseudolabeling from a teacher model F GEN0 (LM 1 in Algorithm 1), where F GEN0 is first fine-tuned on the original training set (without augmentation).","We propose Label Calibration (LabelCalib) by pseudo-labeling from a teacher model F GEN0 , where F GEN0 is first fine-tuned on the original training set (without augmentation).","Modify,Fact/Evidence",Fact/Evidence
3394,326-ARR,326-ARR_v2_56@1,326-ARR_v1_51@1,"As seen, our model can give good aligned information, where preposition phrase ""around 50 years ago"" is arranged at the end of sentence in English, while its aligned phrase is at front in Chinese.","As seen, our model can give good aligned information, where preposition phrase ""around 50 years ago"" is arranged at the end of sentence in English, while its aligned phrase is at the front in Chinese.","Modify,Grammar",Grammar
3395,326-ARR,326-ARR_v2_60@4,326-ARR_v1_53@4,"Although we have shown the superiority of E-ATT, considering the whole TRANSFORMER block 2 , the use of E-ATT brings 17% energy reduction.","Although we have shown the superiority of E-ATT, considering the whole Transformer block 2 , the use of E-ATT brings 17% energy reduction.","Modify,Grammar",Grammar
3396,326-ARR,326-ARR_v2_60@6,326-ARR_v1_53@6,It is worth to further design techniques that reduce the energy cost of other modules in TRANSFORMER.,It is worth to further design techniques that reduce the energy cost of other modules in Transformer.,"Modify,Grammar",Grammar
3400,327-ARR,327-ARR_v2_40@1,327-ARR_v1_41@1,"This baseline helps reduce variance in REINFORCE (Williams, 1992).",This baseline helps reduce variance in REINFORCE.,"Modify,Fact/Evidence",Fact/Evidence
3401,327-ARR,327-ARR_v2_2@6,327-ARR_v1_2@7,"Additionally, we find that this model enjoys stable training relative to a non-RL setting.","In addition, we find that this approach enjoys stable training compared to a non-RL setting.","Modify,Clarity",Clarity
3402,327-ARR,327-ARR_v2_57@1,327-ARR_v1_58@1,It scores higher than XENT by 1.28 BLEU and 0.71 BLEURT points.,It scores higher than the XENT baseline by 1.28 BLEU and 0.71 BLEURT points.,"Modify,Clarity",Clarity
3403,327-ARR,327-ARR_v2_57@2,327-ARR_v1_58@2,"Although BL-R was specially trained to optimize BLEU, F DISC still outperforms it by over half a point on that metric.","Although BL-R was specially trained to optimize BLEU, F DISC still outperforms it by over half a point that metric.","Modify,Grammar",Grammar
3404,327-ARR,327-ARR_v2_65@0,327-ARR_v1_65@0,This paper proposes new F1 score metrics based on optimized rather than greedy alignments between predicted and reference tokens.,This paper proposed new F1 score metrics based on optimized rather than greedy alignments between predicted and reference tokens.,"Modify,Grammar",Grammar
3405,327-ARR,327-ARR_v2_65@1,327-ARR_v1_65@1,"Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extract alignments as a constrained optimization problem.","Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extracted alignments as a constrained optimization problem.","Modify,Grammar",Grammar
3406,327-ARR,327-ARR_v2_65@3,327-ARR_v1_65@3,"In the continuous case, we find alignments that minimize earth mover's distance between the two token embedding distributions.","In the continuous case, we found alignments that minimized earth mover's distance between the two token embedding distributions.","Modify,Grammar",Grammar
3407,327-ARR,327-ARR_v2_66@0,327-ARR_v1_66@0,"We apply new metrics as rewards during RLbased training for AMR-to-text generation, with F DISC outperforming both a cross-entropy baseline and one optimizing BLEU rewards.","We applied new metrics as rewards during RLbased training for AMR-to-text generation, with F DISC outperforming both a cross-entropy baseline and one optimizing BLEU rewards.","Modify,Grammar",Grammar
3408,327-ARR,327-ARR_v2_66@1,327-ARR_v1_66@1,"Despite being computed on a downstream model's token embeddings, the metrics still provide informative rewards during training without signs of overfitting.","Despite being computed on a downstream model's token embeddings, the metrics still provided informative rewards during training without signs of overfitting.","Modify,Grammar",Grammar
3409,327-ARR,327-ARR_v2_2@1,327-ARR_v1_2@1,Metrics leveraging contextualized embeddings appear more flexible than those that match n-grams and thus ideal as training rewards.,Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards.,"Modify,Clarity",Clarity
3410,327-ARR,327-ARR_v2_2@2,327-ARR_v1_2@2,"Yet metrics such as BERTSCORE greedily align candidate and reference tokens, which can give system outputs excess credit relative to a reference.","However, metrics such as BERTSCORE greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference.","Modify,Clarity",Clarity
3411,327-ARR,327-ARR_v2_2@3,327-ARR_v1_2@3,Past systems using such semantic similarity rewards further suffer from repetitive outputs and overfitting.,"Furthermore, past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting.","Modify,Clarity",Clarity
3412,327-ARR,327-ARR_v2_16@0,327-ARR_v1_17@0,"Aligning tokens between hypothesis and reference can be seen as an assignment problem, where a token pair (ŷ i , y j ) is highly weighted if it incurs low cost (i.e., distance).","Aligning tokens between hypothesis and reference can be seen as an assignment problem, where a token pair (ŷ i , y j ) is highly weighted if it incurs low cost (i.e. distance).","Modify,Grammar",Grammar
3413,327-ARR,327-ARR_v2_17@1,327-ARR_v1_18@1,"For the latter, we extract alignments from the earth mover's distance (EMD, Villani, 2009;Peyré and Cuturi, 2019) transport matrix.","For the latter, we extract alignments from the earth mover's distance (EMD, Villani, 2009;Peyré et al., 2019) transport matrix.","Modify,Fact/Evidence",Fact/Evidence
3414,327-ARR,327-ARR_v2_2@4,327-ARR_v1_2@4,"To address these issues, we propose metrics that replace the greedy alignments in BERTSCORE with optimized ones.",We address these issues by proposing metrics that replace the greedy alignments in BERTSCORE with optimized ones.,"Modify,Clarity",Clarity
3575,344-ARR,344-ARR_v2_16@1,344-ARR_v1_16@1,"4) Current NLU datasets do not combine a large set of fine-grained intents (again, with multiintent examples) and a large set of fine-grained slots, which prevents proper and more insightful evaluations of joint NLU models (Chen et al., 2019;Gangadharaiah and Narayanaswamy, 2019).","4) Current NLU datasets do not combine a large set of fine-grained intents (again, with multiintent examples) and a large set of fine-grained slots, which prevents proper and more insightful evaluations of joint NLU models Gangadharaiah and Narayanaswamy, 2019).","Modify,Fact/Evidence",Fact/Evidence
3576,344-ARR,344-ARR_v2_22@0,344-ARR_v1_22@0,"One of the main contributions of this work is the novel design of the intent space, defined in a highly modular manner that natively supports intent re-combinations and multi-intent annotations 7 .","One of the main contributions of this work is the design of the intent space, defined in a highly modular manner that natively supports intent recombinations and multi-intent annotations.","Modify,Clarity",Clarity
3577,344-ARR,344-ARR_v2_2@5,344-ARR_v1_3@1,"Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, the validity of 'intent modularisation', and call for further research on ToD NLU.","Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, and call for further research on ToD NLU.","Modify,Fact/Evidence",Fact/Evidence
3578,344-ARR,344-ARR_v2_23@1,344-ARR_v1_22@4,"1) The modular nature of the ontology allows for expressing a much more complex set of ideas through different combinations of intent modules (see Table 3), while reducing the overall size of the intent set compared to previous ID datasets 8 (see Table 1 and Table 5).","1) The modular nature of the ontology allows for expressing a much more complex set of ideas through different combinations of intent modules (see Table 2), while reducing the overall size of the intent set compared to previous ID datasets 7 (see Table 4).","Modify,Fact/Evidence",Fact/Evidence
3579,344-ARR,344-ARR_v2_23@2,344-ARR_v1_23@0,"2) It allows for the definition of partial intents (e.g., ""The savings one"").","2) It allows for the definition of partial intents (e.g. ""The savings one"").","Modify,Grammar",Grammar
3580,344-ARR,344-ARR_v2_23@3,344-ARR_v1_23@1,"This is crucial in multi-turn interactions, where the user often has to answer disambiguation questions (e.g., ""Which account would you like to close?"").","This is crucial in multi-turn interactions, where the user often has to answer disambiguation questions (e.g. ""Which account would you like to close?"").","Modify,Grammar",Grammar
3581,344-ARR,344-ARR_v2_27@0,344-ARR_v1_28@0,"Similarly to intent modules, slots can also be divided into the generic ones (e.g. time, date) and the domain-specific ones (e.g company_name, rooms, kids), see Table 10.","Similarly to intent modules, slots can also be divided into the generic ones (e.g. time, date) and the domain-specific ones (e.g company_name, rooms, kids) (see Table 9).","Modify,Clarity",Clarity
3582,344-ARR,344-ARR_v2_29@0,344-ARR_v1_30@0,"Previous NLU datasets have usually relied on crowdworkers, aiming to collect a large number of examples, and typically optimising for quantity over quality.","Previous NLU datasets have usually relied on crowdworkers, aiming to collect a large numbers of examples, and typically optimising for quantity over quality.","Modify,Grammar",Grammar
3583,344-ARR,344-ARR_v2_43@1,344-ARR_v1_44@1,"Another group of SotA ID baselines reformulates the ID task into the (extractive) question-answering (QA) problem (Namazifar et al., 2021;Fuisz et al., 2022).","Another group of SotA ID baselines reformulates the ID task into the (extractive) question-answering (QA) problem (Namazifar et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3584,344-ARR,344-ARR_v2_58@4,344-ARR_v1_58@4,"For instance, we observe particularly high scores for highly generic and reusable intent modules such as change, how, how_much, thank, when, and affirm, all with per-intent F 1 scores of ≥ 90.","For instance, we observe particularly high scores for highly generic and reusable intent modules such as change, how, how_much, thank, when, and affirm, all with per-intent F 1 scores ≥ 90.","Modify,Grammar",Grammar
3585,344-ARR,344-ARR_v2_62@3,344-ARR_v1_61@4,Future work includes rethinking the SL task for these slots.,Future work should also look into alternatives to fine-grained slot annotations for such slots.,"Modify,Clarity",Clarity
3586,344-ARR,344-ARR_v2_65@2,344-ARR_v1_62@2,"Upon collection, the dataset has undergone an additional check by the internal Ethics committee of the company.","Upon collection, the dataset has undergone an additional check by the internal Ethics committee.","Modify,Clarity",Clarity
3587,344-ARR,344-ARR_v2_65@3,344-ARR_v1_62@3,NLU++ is licensed under CC-BY-4.0.,It is licensed under CC BY 4.0.,"Modify,Clarity",Clarity
3588,344-ARR,344-ARR_v2_69@1,344-ARR_v1_65@1,"Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) using a dual-encoder framework (Yang et al., 2019) with larger embedding capacity (i.e., a shared multilingual vocabulary of 500k subwords).","Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) using a dual-encoder framework with larger embedding capacity (i.e., a shared multilingual vocabulary of 500k subwords).","Modify,Fact/Evidence",Fact/Evidence
3589,344-ARR,344-ARR_v2_6@9,344-ARR_v1_5@18,"Unlike previous ID datasets, examples are annotated with multiple labels, named intent modules 3 (see Table 1), with some examples naturally obtaining even up to 6-7 labels.","Unlike previous ID datasets, examples are annotated with multiple labels, with some examples naturally obtaining even up to 6-7 labels.","Modify,Fact/Evidence",Fact/Evidence
3590,344-ARR,344-ARR_v2_6@11,344-ARR_v1_5@19,"In addition, NLU++ defines a rich set of slots which are combined with the multi-intent sentences.",NLU++ defines a rich set of slots which are combined with multi-intent sentences.,"Modify,Clarity",Clarity
3591,344-ARR,344-ARR_v2_7@1,344-ARR_v1_6@1,"Our benchmark comparisons also demonstrate strong performance and shed new light on the (ability of) recently emerging QA-based NLU models (Namazifar et al., 2021;Fuisz et al., 2022), and warrant further research on ToD NLU.","Our benchmark comparisons also demonstrate strong performance and shed new light on the (ability of) recently emerging QA-based NLU models, and warrant further research on ToD NLU.","Modify,Fact/Evidence",Fact/Evidence
3592,344-ARR,344-ARR_v2_2@2,344-ARR_v1_2@2,"1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences, introducing and validating the idea of intent modules that can be combined into complex intents that convey complex user goals, combined with finer-grained and thus more challenging slot sets.",1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences combined with finer-grained and thus more challenging slot sets.,"Modify,Claim",Claim
3593,344-ARR,344-ARR_v2_10@0,344-ARR_v1_9@0,"In order to adapt to the increasing data requirements of deep learning models, increasingly larger dialogue datasets have been released in recent years (Budzianowski et al., 2018;Wei et al., 2018;Rastogi et al., 2019;Peskov et al., 2019).","To adapt to the increasing data requirements of deep learning models, increasingly larger dialogue datasets have been released in recent years (Budzianowski et al., 2018;Wei et al., 2018;Rastogi et al., 2019;Peskov et al., 2019).","Modify,Clarity",Clarity
3594,344-ARR,344-ARR_v2_11@0,344-ARR_v1_10@0,"2) The domain-specific ontologies required a lot of expertise for annotation, therefore many annotation mistakes were made (Eric et al., 2019;Zang et al., 2020).","2) The domain-specific ontologies required a lot of expertise for annotation, therefore many annotation mistakes were made (Eric et al., 2019;","Modify,Fact/Evidence",Fact/Evidence
3595,344-ARR,344-ARR_v2_2@3,344-ARR_v1_2@3,"2) The ontology is divided into domain-specific and generic (i.e., domainuniversal) intent modules that overlap across domains, promoting cross-domain reusability of annotated examples.","2) The ontology is divided into domain-specific and generic (i.e., domainuniversal) intents that overlap across domains, promoting cross-domain reusability of annotated examples.","Modify,Clarity",Clarity
3728,350-ARR,350-ARR_v2_37@5,350-ARR_v1_37@5,The test set is used only for measuring phoneme error rate and is not involved in any tuning.,The test set was used only for measuring phoneme error rate and was not involved in any tuning.,"Modify,Grammar",Grammar
3729,350-ARR,350-ARR_v2_4@0,350-ARR_v1_4@0,Recent progress in automatic speech recognition (ASR) was made by training neural networks on increasingly large amounts of annotated data.,Recent progress in automatic speech recognition (ASR) has been obtained by training neural networks on increasingly large amounts of annotated data.,"Modify,Clarity",Clarity
3730,350-ARR,350-ARR_v2_49@1,350-ARR_v1_49@1,"Note that the nru33 subset is used here rather than the full nru, to make it more comparable with other languages.","Note that the nru33 subset is used here rather than the full nru, to make it more comparable.","Modify,Clarity",Clarity
3731,350-ARR,350-ARR_v2_50@0,350-ARR_v1_49@3,Phoneme error rates (PER) reported are obtained using the speaker turn segmentation from the transcript.,Phoneme error rates reported are obtained using the speaker turn segmentation from the transcript.,"Modify,Clarity",Clarity
3732,350-ARR,350-ARR_v2_52@1,350-ARR_v1_52@0,"Here, phoneme error rate (PER) columns in Table 3 show that pretrained XLSR-53 outperforms other models for all languages in public datasets.","Looking at the phoneme error rate (PER) columns in Table 3, XLSR-53 is seen to outperform the other models for all languages in the public dataset, with an average of 13.6% PER.","Modify,Fact/Evidence",Fact/Evidence
3733,350-ARR,350-ARR_v2_52@2,350-ARR_v1_52@1,"In one case (mlv), it obtains 8.6% PER with only 20 minutes of training.","In one case, it obtains 8.6% PER with only 20 minutes of training.","Modify,Clarity",Clarity
3734,350-ARR,350-ARR_v2_52@3,350-ARR_v1_52@2,"Similarly in the private dataset, Table 4 shows XLSR-53 outperforming the other models for all languages.","Similarly in the private dataset, Table 4 shows XLSR-53 outperforming the other models for all languages, except for Tsuut'ina (srs).","Modify,Fact/Evidence",Fact/Evidence
3735,350-ARR,350-ARR_v2_52@4,350-ARR_v1_52@4,"Note that the HMM-GMM result for Cree (crl) is 13.0% PER, slightly better than for the HMM-BLSTM model without LM result from (Gupta and Boulianne, 2020).","Note that the HMM-GMM result for Cree (crl) is 14.2% PER, slightly better than previously reported for an HMM-BLSTM deep recursive model (Gupta and Boulianne, 2020).","Modify,Fact/Evidence",Fact/Evidence
3736,350-ARR,350-ARR_v2_54@1,350-ARR_v1_53@1,"This can be seen in Table 5, where we summarized results from Tables 3 and 4 Table 5 shows that with over 99 minutes, HMM-GMM, XLSR and k2-conf have a PER of 13.8% or less.","This can be seen in Table 5, where we summarized results from Tables 3 and 4 Table 5 shows that with over 99 minutes, HMM-GMM, XLSR and k2-conf have a PER of 13.9% or less (excluding the srs result for XLSR).","Modify,Fact/Evidence",Fact/Evidence
3737,350-ARR,350-ARR_v2_54@5,350-ARR_v1_53@5,The result for the full nru set from Wisniewski et al. (2020) is included for completeness.,The result for nru from Wisniewski et al. (2020) is included for completeness.,"Modify,Clarity",Clarity
3738,350-ARR,350-ARR_v2_54@7,350-ARR_v1_53@7,"Here a PER below 9% was obtained for all the languages in Tables 3 and 4 which had more than 99 minutes for training, so it looks like useful error rates are feasible with 1.7 hours of transcribed data.","Here a PER below 10% was obtained for all the languages in Tables 3 and 4 which had more than 99 minutes for training, so it looks like useful error rates are feasible with 1.5 hours of transcribed data.","Modify,Fact/Evidence",Fact/Evidence
3739,350-ARR,350-ARR_v2_55@2,350-ARR_v1_54@2,"So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware, and makes it possible to test many different hypothesis in a short time, for example about the phoneme inventory.","So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware in a short time.","Modify,Claim",Claim
3740,350-ARR,350-ARR_v2_58@0,350-ARR_v1_57@0,Fine-tuning a large pretrained multilingual model clearly outperformed the other approaches.,Fine-tuning a large pretrained multilingual model outperformed the other approaches (although failing in one case).,"Modify,Fact/Evidence",Fact/Evidence
3741,350-ARR,350-ARR_v2_6@1,350-ARR_v1_6@1,"In addition, none has yet evaluated fine-tuning recent large models pretrained on many languages, for example XLSR (Conneau et al., 2020) 3 , which are particularly well suited for low-resource languages.","In addition, none has yet evaluated the recent large models pretrained on many languages, for example XLSR (Conneau et al., 2020) 3 , which are particularly well suited for low-resource languages.","Modify,Claim",Claim
3742,350-ARR,350-ARR_v2_7@3,350-ARR_v1_7@3,"We more firmly establish feasibility of accurate phonemic transcription with 3 hours or less of transcribed data by reporting on 4 new languages, including Cree and highly polysynthetic Inuktitut, in addition to 7 other previously studied in the literature.","We more firmly establish the feasibility of accurate automatic phonemic transcription with 3 hours or less of transcribed data by reporting on 4 new languages, including Cree and highly polysynthetic Inuktitut, in addition to 7 other previously studied in the literature.","Modify,Clarity",Clarity
3743,350-ARR,350-ARR_v2_12@0,350-ARR_v1_12@0,Table 1 gives amounts of training and testing audio in minutes for each language in this dataset.,Table 1 gives the amount of training and testing audio in minutes for each language in this dataset.,"Modify,Grammar",Grammar
3744,350-ARR,350-ARR_v2_12@1,350-ARR_v1_12@1,"The language code is ISO-639-3 (International Organization for Standardization, 2018).","The language code is the 3-letter ISO-639-3 code (International Organization for Standardization, 2018).","Modify,Fact/Evidence",Fact/Evidence
3745,350-ARR,350-ARR_v2_12@2,350-ARR_v1_12@2,The number of phonemes depends on the particular rules for grapheme-to-phoneme conversion (more details in section 3.2).,The number of phonemes depends on the particular rules for grapheme-to-phoneme conversion (as described in more details in section 3.2).,"Modify,Clarity",Clarity
3746,350-ARR,350-ARR_v2_12@3,350-ARR_v1_12@3,"The IPA column says yes when the recording was transcribed in IPA phonemes, otherwise it was in orthographic text.","The IPA column contains yes if recording was transcribed in IPA phonemes, or no if transcribed in orthographic text.","Modify,Clarity",Clarity
3747,350-ARR,350-ARR_v2_14@2,350-ARR_v1_14@2,Transcribed recordings from a single speaker of Kurmanji Kurdish were kindly shared with us by Translators without Borders.,Transcribed recordings from a single speaker of Kurmanji Kurdish transcribed were kindly shared with us by Translators without Borders.,"Modify,Clarity",Clarity
4085,40-ARR,40-ARR_v2_21@2,40-ARR_v1_21@2,We provide further details on the optimization and model task performance in Appendix A.,We give optimization details and model task performance in Appendix A.,"Modify,Clarity",Clarity
4086,40-ARR,40-ARR_v2_4@0,40-ARR_v1_4@0,"The usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Zhang and Zhang, 2019;Klerke and Plank, 2019).","The usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Klerke and Plank, 2019).","Modify,Fact/Evidence",Fact/Evidence
4087,40-ARR,40-ARR_v2_25@3,40-ARR_v1_25@3,"For sentiment reading, the E-Z Reader and BNC show the highest correlations followed by the Transformer attention flow values (the ranking between E-Z/BNC and Transformer flows is significant at p < 0.05 ).","For sentiment reading, the E-Z Reader and BNC show the highest correlations followed by the Transformer attention flow values (the ranking between E-Z/BNC and Transformer flows is significant at p < .05 ).","Modify,Grammar",Grammar
4088,40-ARR,40-ARR_v2_25@4,40-ARR_v1_25@4,"For relation extraction, we see the highest correlation for BERTbase attention flows (with and without fine-tuning) and BERT-large followed by the E-Z Reader (ranking is significant at p < 0.05).","For relation extraction, we see the highest correlation for BERT-base attention flows (with and without fine-tuning) and BERT-large followed by the E-Z Reader (ranking is significant at p < .05).","Modify,Grammar",Grammar
4089,40-ARR,40-ARR_v2_25@8,40-ARR_v1_25@8,Correlations grouped by sentence length shows stable values around 0.6 (SST) and 0.4 − 0.6 (Wikipedia) except for shorter sentences where correlations fluctuate.,Correlations grouped by sentence length shows stable values around .6 (SST) and .4-.6 (Wikipedia) except for shorter sentences where correlations fluctuate.,"Modify,Grammar",Grammar
4090,40-ARR,40-ARR_v2_4@2,40-ARR_v1_4@2,"We compare the learned attention functions and the heuristic model across two taskspecific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available dataset with eye-tracking recordings of native speakers of English .","We compare the learned attention functions and the heuristic model across two task-specific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available data set with eye-tracking recordings of native speakers of English .","Modify,Grammar",Grammar
4091,40-ARR,40-ARR_v2_32@2,40-ARR_v1_31@2,"We can see that the Transformer models correlate better for more predictable words on both datasets whereas the E-Z Reader is less influenced by word predictability and already shows medium correlation on the most hard-to-predict words (0.3 − 0.4 for both, SST and Wikipedia).","We can see that the Transformer models correlate better for more predictable words on both datasets whereas the E-Z Reader is less influenced by word predictability and already shows medium correlation on the most hard-to-predict words (.3-.4 for both, SST and Wikipedia).","Modify,Grammar",Grammar
4092,40-ARR,40-ARR_v2_32@3,40-ARR_v1_31@3,"In fact, on SST, Transformers only pass the E-Z Reader on the most predictable tokens (word predictability > 0.03).","In fact, on SST, Transformers only pass the E-Z Reader on the most predictable tokens (word predictability >.03).","Modify,Grammar",Grammar
4093,40-ARR,40-ARR_v2_33@12,40-ARR_v1_33@12,The highest correlation can be observed when comparing human attention for task-specific and natural reading (0.72).,The highest correlation can be observed when comparing human attention for task-specific and natural reading (.72).,"Modify,Grammar",Grammar
4094,40-ARR,40-ARR_v2_43@1,40-ARR_v1_42@1,"It remains an open problem how best to interpret self-attention modules (Jain and Wallace, 2019;Wiegreffe and Pinter, 2019), and whether they provide meaningful explanations for model predictions.","It remains an open problem how best to interpret self-attention modules (Jain and Wallace, 2019;Wiegreffe and Pinter, 2019), and whether they provide explanations for model decisions.","Modify,Clarity",Clarity
4095,40-ARR,40-ARR_v2_43@4,40-ARR_v1_42@3,Faithfulness and practicality is often evaluated using automated procedures such as input reduction experiments or measuring time and model complexity.,Faithfulness and practicality can be evaluated using automated procedures such as input reduction experiments or measuring time and model complexity.,"Modify,Clarity",Clarity
4096,40-ARR,40-ARR_v2_48@5,40-ARR_v1_47@5,"Our input reduction suggest that in a sense, trained language models and humans have suboptimal, i.e., less sparse, task-solving strategies, and are heavily regularized by what is optimal in natural reading contexts.","Our input reduction experiments suggest that in a sense, both pretrained language models and humans have suboptimal, i.e., less sparse, task-solving strategies, and are heavily regularized by what is optimal in natural reading contexts.","Modify,Clarity",Clarity
4097,40-ARR,40-ARR_v2_5@7,40-ARR_v1_5@7,"In addition, we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns.",In addition we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns.,"Modify,Grammar",Grammar
4101,40-ARR,40-ARR_v2_5@10,40-ARR_v1_5@10,Our code is available at github.com/ oeberle/task_gaze_transformers.,Our code is available at github.com/anon.,"Modify,Fact/Evidence",Fact/Evidence
4102,40-ARR,40-ARR_v2_2@3,40-ARR_v1_2@3,"We find the predictiveness of large-scale pretrained self-attention for human attention depends on 'what is in the tail', e.g., the syntactic nature of rare contexts.","We find the predictiveness of large-scale pretrained self-attention for human attention depends on 'what is in the tail,' e.g., the syntactic nature of rare contexts.","Modify,Grammar",Grammar
4117,402-ARR,402-ARR_v2_15@5,402-ARR_v1_16@5,"Finally, the premise URLs are used to retrieve the premise articles.","Finally, the premise article URLs are used to retrieve the premise articles.","Modify,Clarity",Clarity
4118,402-ARR,402-ARR_v2_15@6,402-ARR_v1_16@6,"We try to directly retrieve the article where possible, but also use archive.org's API in case a premise article is no longer available online.","We try to directly retrieve the article where possible, but also use archive.org's APIs in case the premise article URL is no longer available online.","Modify,Grammar",Grammar
4119,402-ARR,402-ARR_v2_15@7,402-ARR_v1_16@7,We follow the same general procedure for data collection from Politifact and Snopes except that we directly crawl the respective websites instead of using Google's fact check tool APIs for collecting claims and associated metadata.,"We follow the same general procedure for data collection from Politifact and Snopes except that instead of using Google's fact check tool APIs for collecting claims and associated metadata, we directly crawl the respective websites to collect the data.","Modify,Clarity",Clarity
4120,402-ARR,402-ARR_v2_4@0,402-ARR_v1_4@0,"The rise of social media has led to a democratization of news, but it has also amplified issues related to fake news and misinformation.","The rise of social media has lead to a democratization of news, but it has also amplified issues related to fake news and misinformation.","Modify,Grammar",Grammar
4121,402-ARR,402-ARR_v2_16@0,402-ARR_v1_17@0,We perform some basic cleanup to the collected data before inclusion in the dataset.,We only perform some basic cleanup to the collected data before inclusion in the dataset.,"Modify,Clarity",Clarity
4122,402-ARR,402-ARR_v2_16@4,402-ARR_v1_17@4,"Admittedly, this does not eliminate auxiliary text such as navigation links, footer text, recommended links, etc.","Admittedly, this still does not eliminate the auxiliary text usually present in the web pages such as navigation links, footer text, recommended links, etc.","Modify,Clarity",Clarity
4123,402-ARR,402-ARR_v2_16@6,402-ARR_v1_17@7,"We map the numerous claim veracity labels used by the fact checking websites into three broad labels: True, Partially True/False, and False.","We also map the numerous claim veracity labels used by the fact checking websites into three broad labels: True, Partially True/False, and False.","Modify,Clarity",Clarity
4124,402-ARR,402-ARR_v2_18@0,402-ARR_v1_19@0,"The contributed dataset contains a total of 33,721 claims.","The contributed dataset contains a total of 33,697 claims.","Modify,Fact/Evidence",Fact/Evidence
4125,402-ARR,402-ARR_v2_18@1,402-ARR_v1_19@1,"We split those claims into the following three sets: training set containing 26,976 claims, validation set containing 3,372 claims, and test set containing 3,373 claims.","We split those claims into the following three sets: training set containing 26,957 claims, validation set containing 3369 claims, and test set containing 3371 claims.","Modify,Fact/Evidence",Fact/Evidence
4126,402-ARR,402-ARR_v2_18@3,402-ARR_v1_19@3,We provide the extracted text files for the review and premise articles.,We provide both the HTML and extracted text files for the review and premise articles.,"Modify,Fact/Evidence",Fact/Evidence
4127,402-ARR,402-ARR_v2_19@0,402-ARR_v1_20@0,Fig. 1 shows the number of claims per fact checking services.,Figure 1 shows the number of claims collected from each of the fact checking services.,"Modify,Clarity",Clarity
4128,402-ARR,402-ARR_v2_19@1,402-ARR_v1_20@1,Fig. 2 shows the claim rating distribution.,Figure 2 shows the claim rating distribution.,"Modify,Clarity",Clarity
4129,402-ARR,402-ARR_v2_19@2,402-ARR_v1_20@2,Claims in the Partially True/False and False categories significantly outnumber the claims in the True category.,We see that the claims in the Partially True/False and False categories significantly outnumber the claims in the True category.,"Modify,Clarity",Clarity
4130,402-ARR,402-ARR_v2_19@3,402-ARR_v1_20@3,"In reality, the number of true claims is much larger than the number of partially true/false and false claims, but fact checking services focus on debunking controversial claims and therefore the majority of the claims they investigate are false or partially true/false.","In reality, the number of true claims is much larger then the number of partially true/false and false claims, but fact checking services focus on debunking controversial claims and therefore the majority of the claims they investigate are false or partially true/false.","Modify,Grammar",Grammar
4131,402-ARR,402-ARR_v2_19@4,402-ARR_v1_20@4,This imbalance poses an important challenge.,This imbalance poses an important challenge for the models.,"Modify,Clarity",Clarity
4132,402-ARR,402-ARR_v2_22@2,402-ARR_v1_22@4,This reflects the real world task of automated veracity detection more truthfully due to the availability of the premise articles cited by the professional fact checkers in claim review articles.,We argue that our dataset reflects the real world task of automated veracity detection more truthfully due to the availability of the premise articles cited by the professional fact checkers in claim review articles.,"Modify,Clarity",Clarity
4133,402-ARR,402-ARR_v2_22@4,402-ARR_v1_22@6,Using a web search to retrieve evidence documents after a fact checking service has verified a claim is problematic since multiple news agencies often publish articles referencing the original fact checking review article.,"Using a web search to retrieve evidence documents for a claim is problematic due to the fact that once a fact checking service has fact checked a claim, we observe that multiple other news agency also publish articles referencing the original fact checking review article.","Modify,Clarity",Clarity
4136,402-ARR,402-ARR_v2_24@2,402-ARR_v1_24@2,"For the first stage, we evaluate two different approaches.","For the first stage, we use and evaluate two different approaches.","Modify,Clarity",Clarity
4137,402-ARR,402-ARR_v2_24@3,402-ARR_v1_24@3,"The first approach is term frequency inverse document frequency (TF-IDF), which is typically used by fact checking methods for sentence based retrieval (Aly et al., 2021).",The first approach is the well-known and commonly used basic text retrieval technique called term frequency inverse document frequency (TF-IDF).,"Modify,Fact/Evidence",Fact/Evidence
4138,402-ARR,402-ARR_v2_30@0,402-ARR_v1_31@0,A key step performed by professional fact checkers is examining the premise articles associated with a claim and extracting useful evidence from them to establish claim veracity.,One of the key steps in the fact checking process performed by human professional fact checkers is examining the premise articles associated with a claim and extracting useful evidence from them to establish claim veracity.,"Modify,Clarity",Clarity
4139,402-ARR,402-ARR_v2_32@0,402-ARR_v1_33@1,We measure TF-IDF similarity between the claim text and the premise article sentences to rank the sentence level evidence.,We use TF-IDF based similarity measure between the claim text and the premise article sentences to rank the sentence level evidence.,"Modify,Clarity",Clarity
4140,402-ARR,402-ARR_v2_34@0,402-ARR_v1_35@0,We propose a novel way of adapting the dense passage retrieval method proposed by Karpukhin et al. (2020) task of retrieving evidence sentences from premise articles.,We propose a novel way of adapting dense passage retrieval methods proposed by Karpukhin et al. (2020) for open domain question answering to the task of retrieving evidence sentences from premise articles.,"Modify,Fact/Evidence",Fact/Evidence
4141,402-ARR,402-ARR_v2_34@1,402-ARR_v1_35@1,Karpukhin et al.'s method uses a dual encoder architecture.,The dense passage retrieval method proposed by Karpukhin et al. (2020) uses a dual encoder architecture.,"Modify,Clarity",Clarity
4142,402-ARR,402-ARR_v2_34@3,402-ARR_v1_35@3,The question encoder E Q and the passage encoder E P embed question q and passage p into d-dimensional vectors.,The question encoder E Q and the passage encoder E P embed a given question q and passage p into d-dimensional real-valued vectors.,"Modify,Clarity",Clarity
4143,402-ARR,402-ARR_v2_36@0,402-ARR_v1_37@0,The model is then trained to learn embeddings such that the similarity score between relevant questionpassage pairs will be higher than irrelevant ones.,The model is then trained to learn embedding functions such that the similarity score between relevant pairs of questions and passages will be higher than irrelevant ones.,"Modify,Clarity",Clarity
4144,402-ARR,402-ARR_v2_36@1,402-ARR_v1_38@0,We adapt this method for our first stage by taking advantage of the fact that the review article published by fact checking websites (along with a claim) typically contains key evidence taken from the premise articles.,We adapt this method for our first stage by taking advantage of the fact that the review article published by fact checking websites along with a claim typically contains key evidence taken from the premise articles.,"Modify,Grammar",Grammar
4145,402-ARR,402-ARR_v2_36@2,402-ARR_v1_38@1,The evidence is usually paraphrased in order to form a coherent argument in support of the claim veracity verdict.,The evidence taken from the premise articles is usually paraphrased in order to form a coherent argument in support of the claim veracity verdict.,"Modify,Clarity",Clarity
4146,402-ARR,402-ARR_v2_39@1,402-ARR_v1_41@1,"Each instance is made up of a claim C i with one positive sentence from the associated review article s R+ i,j and n − 1 randomly chosen negative sentences s R− i,k .","Each instance is made up of a claim C i with one positive sentence from the associated review article s R+ i,j and n randomly chosen negative sentences s R− i,k .","Modify,Fact/Evidence",Fact/Evidence
4147,402-ARR,402-ARR_v2_42@0,402-ARR_v1_44@0,"After training, we use the encoders to encode the claim text and the sentences of the associated premise articles.","After training the model, we use the encoders to encode the claim text and the sentences of the associated premise articles.","Modify,Clarity",Clarity
4148,402-ARR,402-ARR_v2_42@2,402-ARR_v1_44@2,We use the top scoring sentences as evidence sentences in the next stage to perform claim veracity inference.,We use the top scoring sentences as evidence sentences in the next stage to perform the claim veracity inference.,"Modify,Grammar",Grammar
4149,402-ARR,402-ARR_v2_48@2,402-ARR_v1_50@2,Each sentence (claimant with claim text or each evidence sentence of the premise article) is embedded as a sequence of hidden vectors (one per word) by a bi-directional recurrent network (Bi-LSTM or Bi-GRU).,"At the bottom of a HAN, each sentence (claimant with claim text or each evidence sentence of the premise article) is embedded as a sequence of hidden vectors (one per word) by a bi-directional recurrent network (Bi-LSTM or Bi-GRU).","Modify,Clarity",Clarity
4150,402-ARR,402-ARR_v2_50@2,402-ARR_v1_52@2,The input sequence is encoded using the RoBERTa-base model and passed through a dense linear layer followed by a softmax to obtain the predicted claim veracity label distribution.,The input sequence is encoded using the RoBERTa-base model and passed through a dense linear layer followed by a softmax layer to obtain the predicted claim veracity label distribution.,"Modify,Clarity",Clarity
4151,402-ARR,402-ARR_v2_56@3,402-ARR_v1_57@3,The results clearly show that the DPR (dense passage retrieval) method outperforms the method based on TF-IDF.,The results clearly show that the DPR (dense passage retrieval) method outperforms the TF-IDF similarity based method.,"Modify,Clarity",Clarity
4152,402-ARR,402-ARR_v2_61@4,402-ARR_v1_60@1,The evidence sentences are then concatenated to the claim text and truncated to the maximum sequence length capability of the transformer model being used to perform claim veracity inference.,The sentences with top scores are then used to perform claim veracity inference.,"Modify,Fact/Evidence",Fact/Evidence
4153,402-ARR,402-ARR_v2_64@0,402-ARR_v1_64@0,Fig. 3 shows the number of claims investigated in each 6-month period in our dataset.,Figure 3 shows the number of claims investigated in each 6-month period in our dataset.,"Modify,Clarity",Clarity
4154,402-ARR,402-ARR_v2_64@4,402-ARR_v1_64@4,Fig. 4 shows the macro F1 results achieved by the top 4 algorithms with DPR evidence in each 6-month period.,Figure 4 shows the macro F1 results achieved by the top 4 algorithms with DPR evidence in each 6-month period.,"Modify,Clarity",Clarity
4155,402-ARR,402-ARR_v2_66@1,402-ARR_v1_66@1,WatClaimCheck includes premise articles used by professional fact checkers and therefore corresponds more closely to the task of claim veracity inference in automated fact checking.,It is the first dataset that includes premise articles used by professional fact checkers and therefore corresponds more closely to the task of claim veracity inference in automated fact checking.,"Modify,Claim",Claim
4158,402-ARR,402-ARR_v2_7@1,402-ARR_v1_7@1,"When the first stage fails to retrieve key passages, then the inferred verdict will be negatively affected regardless of how good the second stage is.","When the first stage fails to retrieve some key passages, then the inferred verdict will be negatively affected regardless of how good the second stage is.","Modify,Clarity",Clarity
4159,402-ARR,402-ARR_v2_9@10,402-ARR_v1_10@6,"Finally, Sect. 6 concludes and discusses possible future work.","Finally, Section 6 concludes and discusses possible future work.","Modify,Clarity",Clarity
4160,402-ARR,402-ARR_v2_11@0,402-ARR_v1_12@0,"There is an important line of work that focuses on claim verification (Kotonya and Toni, 2020a;Guo et al., 2022).",There is an important line of work that focuses on claim verification.,"Modify,Fact/Evidence",Fact/Evidence
4161,402-ARR,402-ARR_v2_11@1,402-ARR_v1_12@1,"This includes techniques that predict the veracity of a claim based on the text of the claim only (Rashkin et al., 2017), linguistic features (Popat et al., 2017), meta information about the claimant (e.g., name, job, party affiliation, veracity history) (Wang, 2017), review articles (Augenstein et al., 2019;Shu et al., 2018;Nakov et al., 2021), relevant articles returned by a search engine (Popat et al., 2018;Augenstein et al., 2019;Mishra and Setty, 2019) as well as premise articles (Aly et al., 2021;Kotonya and Toni, 2020b).","This includes techniques that predict the veracity of a claim based on the text of the claim only (Rashkin et al., 2017), linguistic features (Popat et al., 2017), meta information about the claimant (e.g., name, job, party affiliation, veracity history) (Wang, 2017b), review articles (Augenstein et al., 2019;Shu et al., 2018;Nakov et al., 2021), as well as relevant articles returned by a search engine (Popat et al., 2018;Augenstein et al., 2019;Mishra and Setty, 2019).","Modify,Fact/Evidence",Fact/Evidence
4162,402-ARR,402-ARR_v2_11@2,402-ARR_v1_12@3,There is an important distinction between articles returned by a search engine and premise articles.,There is an important distinction between articles returned by a search engine in previous work and the premise articles that we consider.,"Modify,Clarity",Clarity
4163,402-ARR,402-ARR_v2_12@4,402-ARR_v1_13@4,"An important task that can help the detection of fake news is stance detection (Borges et al., 2019;Jwa et al., 2019), i.e., does the content of an article agree or disagree with the title of the article?","An important task that can help the detection of fake news is the task of stance detection (Borges et al., 2019;Jwa et al., 2019), i.e., does the content of an article agree or disagree with the title of the article?","Modify,Clarity",Clarity
4164,402-ARR,402-ARR_v2_15@1,402-ARR_v1_16@1,We utilize Google's fact check tool APIs 2 to collect the claims' metadata for all fact checking services except Politifact and Snopes.,We utilize Google's fact check tool APIs 1 to collect the claims' metadata for all previously listed fact checking services except Politifact and Snopes.,"Modify,Fact/Evidence",Fact/Evidence
4165,402-ARR,402-ARR_v2_15@4,402-ARR_v1_16@4,"We parse the article body, retrieving the premise article URLs used in the review article to justify the claim veracity.","We carefully parse the article body, retrieving the premise article URLs used in the claim review article to justify the claim veracity.","Modify,Clarity",Clarity
4232,412-ARR,412-ARR_v2_28@2,412-ARR_v1_25@2,"A gradient reversal layer (Ganin and Lempitsky, 2015) is added between the feature concatenation and the discriminator to reverse the gradients through the backpropagation phase and support extracting general features.",A gradient reversal layer is added between the feature concatenation and the discriminator to reverse the gradients through the backpropagation phase and support extracting general features.,"Modify,Fact/Evidence",Fact/Evidence
4233,412-ARR,412-ARR_v2_2@6,412-ARR_v1_2@6,"Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset.",Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques when considering the CompLex from the Lexical Complexity Prediction 2021 dataset.,"Modify,Grammar",Grammar
4234,412-ARR,412-ARR_v2_31@0,412-ARR_v1_28@0,The following setups also include the Basic Domain Adaptation training setting.,The following setups also include the Basic Domain Adaptation training setup.,"Modify,Clarity",Clarity
4235,412-ARR,412-ARR_v2_32@1,412-ARR_v1_29@1,"The concatenation layer now contains the BiLSTM and Transformer features, along with the VAE encoder features (F v ), namely F t +F c +F v .","The concatenation layer now contains the BiLSTM and Transformer features, plus the VAE encoder features (F v ).","Modify,Fact/Evidence",Fact/Evidence
4236,412-ARR,412-ARR_v2_44@0,412-ARR_v1_41@0,"We rely on a Transformer-based model as the main feature extractor for the context of the target word (i.e., the full sentence), considering their superior performance on most natural language processing tasks.","We rely on a Transformer-based model as the main feature extractor for the context of the target word (i.e., the full sentence), considering their superior performance on most NLP tasks.","Modify,Clarity",Clarity
4237,412-ARR,412-ARR_v2_44@1,412-ARR_v1_41@1,"The selected model for the first dataset is RoBERTa (Liu et al., 2019), as it yields better results when compared to its counterpart, BERT.","The selected model for the first dataset is RoBERTa (Liu et al., 2019), inasmuch as it yields better results when compared to its counterpart, BERT.","Modify,Clarity",Clarity
4238,412-ARR,412-ARR_v2_44@2,412-ARR_v1_41@2,"RoBERTa is trained with higher learning rates and larger mini-batches, and it modifies the key hyper-parameters of BERT.","RoBERTa is trained with higher learning rates and larger minibatches, as well as it modifies the key hyperparameters of BERT.","Modify,Clarity",Clarity
4239,412-ARR,412-ARR_v2_46@0,412-ARR_v1_44@0,"We aim to further improve performance by adding extra features via Variational AutoEncoders (VAEs) (Kingma and Welling, 2014) to the context representation for a target word.","We aim to further improve performance by adding extra features via Variational AutoEncoders (VAEs) (Kingma and Welling, 2013) to the context representation for a target word.","Modify,Fact/Evidence",Fact/Evidence
4240,412-ARR,412-ARR_v2_52@1,412-ARR_v1_50@1,"Consequently, we introduced a generalization technique to extract only cross-domain features that do not present a bias towards a certain domain.","Consequently, we were introduced a generalization technique to extract only cross-domain features that do not present a bias towards a certain domain.","Modify,Grammar",Grammar
4241,412-ARR,412-ARR_v2_58@3,412-ARR_v1_56@3,"The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016a) 1 .","The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016) 1 .","Modify,Fact/Evidence",Fact/Evidence
4242,412-ARR,412-ARR_v2_4@2,412-ARR_v1_4@1,"However, complex word identification is a highly contextualized task, far from being trivial.","However, complex word identification (CWI) is a highly contextualized task, far from being trivial.","Modify,Clarity",Clarity
4243,412-ARR,412-ARR_v2_58@5,412-ARR_v1_56@5,"The corresponding flow is described in Algorithm 1, while the loss function is presented in Equation 7:","The corresponding flow is described in algorithm 1, while the loss function is presented in Equation 7.","Modify,Grammar",Grammar
4244,412-ARR,412-ARR_v2_68@1,412-ARR_v1_68@1,"The learning rate is set to 2e-5, while the loss functions used for the complexity task are the L1 loss (Janocha and Czarnecki, 2016) for the CompLex LCP dataset and the Mean Squared Error (MSE) loss (Kline and Berardi, 2005) for the CWI dataset.","The learning rate is set to 2e-5, while the loss functions used for the complexity task are the L1 loss (Janocha and Czarnecki, 2017) for the first dataset and the Mean Squared Error (MSE) loss (Kline and Berardi, 2005) for the second one.","Modify,Fact/Evidence",Fact/Evidence
4245,412-ARR,412-ARR_v2_68@3,412-ARR_v1_68@3,The λ parameter used for domain adaptation was updated according to Equation 11:,The λ parameter used for domain adaptation was updated according to Equation 11.,"Modify,Grammar",Grammar
4246,412-ARR,412-ARR_v2_12@2,412-ARR_v1_5@0,"Nevertheless, certain training techniques and auxiliary tasks help the model improve its generalization abilities, forcing it to focus only on the most relevant, general features (Schrom et al., 2021).","Nevertheless, certain training techniques and auxiliary tasks help the model improve its generalization abilities (Schrom et al., 2021), forcing it to focus only on the most relevant, general features.","Modify,Fact/Evidence",Fact/Evidence
4247,412-ARR,412-ARR_v2_73@2,412-ARR_v1_74@2,"Zaharia et al. (2021) created models that are based on target and context feature extractors, alongside features resulted from Graph Convolutional Networks, Capsule Networks, and pre-trained word embeddings.","Zaharia et al. (2021) created models that are based on target and context feature extractors, alongside features resulted from Graph Convolutional Networks, Capsule Networks, or pre-trained word embeddings.","Modify,Clarity",Clarity
4248,412-ARR,412-ARR_v2_12@3,412-ARR_v1_5@1,"Techniques like domain adaptation (Ganin et al., 2016) can be used for various tasks, with the purpose of selecting relevant features for follow-up processes.","Techniques like domain adaptation can be used for various tasks, with the purpose of selecting relevant features for follow-up processes.","Modify,Fact/Evidence",Fact/Evidence
4249,412-ARR,412-ARR_v2_76@2,412-ARR_v1_78@2,"The performance is evaluated in terms of MAE; however, we also report the Pearson Correlation Coefficient.","The performance is evaluated in terms of MAE; However, we also report the Pearson Correlation Coefficient.","Modify,Grammar",Grammar
4250,412-ARR,412-ARR_v2_79@0,412-ARR_v1_80@0,"The domain adaptation technique supports our model to learn general cross-domain or crosslanguage features, while achieving higher performance.","The domain adaptation technique supports our model to learn general cross-domain (or cross-language) features, while achieving higher performance.","Modify,Grammar",Grammar
4251,412-ARR,412-ARR_v2_80@0,412-ARR_v1_84@0,Conclusions and Future Work,Conclusion and Future Work,"Modify,Grammar",Grammar
4252,412-ARR,412-ARR_v2_81@0,412-ARR_v1_85@0,"This work proposes a series of training techniques, including domain adaptation, as well as multi-task adversarial learning, that can be used for improving the overall performance of the models for CWI.","This work proposes a series of training techniques, including adversarial domain adaptation, as well as multi-task learning, that can be used for improving the overall performance of the models for CWI.","Modify,Clarity",Clarity
4253,412-ARR,412-ARR_v2_81@2,412-ARR_v1_86@0,"Moreover, by jointly training the model on the CWI tasks and an auxiliary similar task (i.e., text simplification), the overall performance is improved.","Moreover, by jointly training the model on the CWI taks and an auxiliary similar task (i.e., text simplification), the overall performance is improved.","Modify,Grammar",Grammar
4254,412-ARR,412-ARR_v2_81@3,412-ARR_v1_86@1,"The task discriminator also ensures the extraction of general features, thus making the model more robust on the CWI dataset.","The task discriminator also ensures the extraction of general features, thus making the model more robust on the CWI task.","Modify,Clarity",Clarity
4255,412-ARR,412-ARR_v2_13@0,412-ARR_v1_7@0,"We propose several solutions to improve the performance of a model for CWI in a cross-domain or a cross-lingual setting, by adding auxiliary components (i.e., Transformer (Vaswani et al., 2017) decoders, Variational Auto Encoders -VAEs (Kingma and Welling, 2014)), as well as a domain adaptation training technique (Farahani et al., 2021).","We propose several solutions to improve the performance of a model for CWI in a cross-domain or a cross-lingual setting, by adding auxiliary components (i.e., Transformer (Vaswani et al., 2017) decoders, Variational Auto Encoders -VAEs (Kingma and Welling, 2013)), as well as a domain adaptation training technique (Farahani et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
4256,412-ARR,412-ARR_v2_16@1,412-ARR_v1_11@1,Several works employed domain adaptation to improve performance.,Several works employ domain adaptation to improve performance.,"Modify,Grammar",Grammar
4257,412-ARR,412-ARR_v2_16@2,412-ARR_v1_11@2,"For example, Du et al. (2020) approached the sentiment analysis task by using a BERT-based (Devlin et al., 2019) (Klimaszewski and Andruszkiewicz, 2019), mixup synthesis training (Tang et al., 2020), and effective regularization (Vernikos et al., 2020).","For example, Du et al. (2020) approach the sentiment analysis task by using a BERT-based (Devlin et al., 2019) (Klimaszewski and Andruszkiewicz, 2019), mixup synthesis training (Tang et al., 2020), and effective regularization (Vernikos et al., 2020).","Modify,Grammar",Grammar
4258,412-ARR,412-ARR_v2_17@2,412-ARR_v1_12@2,"The latter had the purpose of supporting the adversarial training setup, thus covering the scenario where the model was unable to detect whether the input language was from the source dataset or the target one.","The latter has the purpose of supporting the adversarial training setup, thus covering the scenario where the model is unable the detect whether the input language is from the source dataset or the target one.","Modify,Grammar",Grammar
4259,412-ARR,412-ARR_v2_17@3,412-ARR_v1_12@3,"A similar cross-lingual approach was adopted by Zhang et al. (2020), who developed a system to classify entries from the target language, while only labels from the source language were provided.","A similar cross-lingual approach is adopted by Zhang et al. (2020), who developed a system to classify entries from the target language, while only labels from the source language are provided.","Modify,Grammar",Grammar
4260,412-ARR,412-ARR_v2_18@0,412-ARR_v1_13@0,"Under a Named Entity Recognition training scenario, Kim et al. (2017) used features on two levels (i.e., word and characters), together with Recurrent Neural Networks and a language discriminator used for the domain-adversarial setup.","Under a Named Entity Recognition (NER) training scenario, Kim et al. (2017) use features on two levels (i.e., word and characters), together with Recurrent Neural Networks and a language discriminator used for the domain-adversarial setup.","Modify,Grammar",Grammar
4261,412-ARR,412-ARR_v2_2@3,412-ARR_v1_2@3,"In this paper, we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations.",In this paper we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations.,"Modify,Grammar",Grammar
4262,412-ARR,412-ARR_v2_18@1,412-ARR_v1_13@1,"Similarly, Huang et al. (2019) used target language discriminators during the process of training models for low-resource name tagging.","Similarly, Huang et al. (2019) use target language discriminators during the process of training models for low-resource name tagging.","Modify,Grammar",Grammar
4263,412-ARR,412-ARR_v2_19@1,412-ARR_v1_14@1,"Gooding and Kochmar (2019) based their implementation for CWI as a sequence labeling task on Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, inasmuch as the context helps towards proper identification of complex tokens.","Gooding and Kochmar (2019) base their implementation for CWI as a sequence labeling task on Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, inasmuch as the context helps towards proper identification of complex tokens.","Modify,Grammar",Grammar
4264,412-ARR,412-ARR_v2_19@3,412-ARR_v1_14@3,"Also adopting a sequence labeling approach, Finnimore et al. (2019) considered handcrafted features, including punctuation or syllables, that can properly identify complex structures.","Also adopting a sequence labeling approach, Finnimore et al. ( 2019) consider handcrafted features, including punctuation or syllables, that can properly identify complex structures.","Modify,Grammar",Grammar
4265,412-ARR,412-ARR_v2_20@2,412-ARR_v1_16@0,"At the same time, Zaharia et al. (2020) explored the power of Transformer-based models (Vaswani et al., 2017) in cross-lingual environments by using different training scenarios, depending on the scarcity of the resources: zero-shot, one-shot, as well as few-shot learning.","At the same time, Zaharia et al. (2020) explore the power of Transformer-based models (Vaswani et al., 2017) in cross-lingual environments by using different training scenarios, depending on the scarcity of the resources: zero-shot, one-shot, as well as few-shot learning.","Modify,Grammar",Grammar
4266,412-ARR,412-ARR_v2_20@4,412-ARR_v1_17@1,"For example, De Hertog and Tack (2018) introduced a series of architectures that combine deep learning features, as well as handcrafted features to address CWI as a regression problem.","For example, De Hertog and Tack (2018) introduce a series of architectures that combine deep learning features, as well as handcrafted features to solve a regression problem.","Modify,Fact/Evidence",Fact/Evidence
4267,412-ARR,412-ARR_v2_23@1,412-ARR_v1_20@1,"The entries of Com-pLex consist of a sentence in English and a target token, alongside the complexity of the token, given its context.","The entries of Com-pLex consist of a sentence and a target token, alongside the complexity of the token, given its context.","Modify,Fact/Evidence",Fact/Evidence
4268,412-ARR,412-ARR_v2_25@0,412-ARR_v1_22@0,"The CWI dataset was introduced in the CWI Shared Task 2018 (Yimam et al., 2018).","The Complex Word Identification (CWI) Shared Dataset was introduced in the CWI Shared Task 2018 (Yimam et al., 2018).","Modify,Clarity",Clarity
4281,413-ARR,413-ARR_v2_25@0,413-ARR_v1_23@0,"Following the experimental settings in NegBERT, we use the three standard benchmarks for negation cue detection and scope resolution tasks, i.e. BioScope (Vincze et al., 2008) (separated into two subsets, sourced from abstracts and full-text papers, resp.), the SFU product reviews dataset (Konstantinova et al., 2012), and the Sherlock dataset (Morante and Blanco, 2012).","Following the experiment settings in NegBERT, we use the three standard benchmarks for negation cue detection and scope resolution tasks, i.e. BioScope (Vincze et al., 2008) (separated into two subsets, sourced from abstracts and full-text papers, resp.), the SFU product reviews dataset (Konstantinova et al., 2012), and the Sherlock dataset (Morante and Blanco, 2012).","Modify,Grammar",Grammar
4282,413-ARR,413-ARR_v2_25@3,413-ARR_v1_23@3,"To investigate cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets.","To investigate the cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets.","Modify,Grammar",Grammar
4283,413-ARR,413-ARR_v2_28@1,413-ARR_v1_26@4,"Regarding the in-dataset setting, AugNB outperforms the baseline NegBERT on all datasets except for Sherlock.","Regarding the in-dataset setting (training and evaluating on the same dataset), AugNB outperforms the baseline NegBERT on all datasets except for Sherlock.","Modify,Clarity",Clarity
4284,413-ARR,413-ARR_v2_28@2,413-ARR_v1_26@5,"Gains are more noticeable over the biomedical datasets (BioScope, VetCompass).","Gains are more noticeable in the biomedical datasets (BioScope, VetCompass).","Modify,Grammar",Grammar
4285,413-ARR,413-ARR_v2_29@2,413-ARR_v1_27@2,"CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue helps the model learn better representations for negation cues and thus, better distinguish between cues and normal words.","CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue will help the model learn better representations for negation cues and thus, better distinguish between cues and normal words.","Modify,Grammar",Grammar
4286,413-ARR,413-ARR_v2_4@4,413-ARR_v1_4@4,"A recent study on English texts found that negation detection models do not transfer well across domains, due to variations in expression of negation (Khandelwal and Sawant, 2020).","A recent work on negation detection in English texts found that negation detection models do not transfer well across domains, due to variations in expression of negation (Khandelwal and Sawant, 2020).","Modify,Clarity",Clarity
4289,413-ARR,413-ARR_v2_2@1,413-ARR_v1_2@1,"Recent work has shown that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.","Recent works show that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.","Modify,Grammar",Grammar
4290,413-ARR,413-ARR_v2_11@1,413-ARR_v1_10@1,"Most work follows a common scheme in extracting various features from the sentence, and using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).","Most works follow a common scheme in extracting various features from the sentence, in using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).","Modify,Grammar",Grammar
4291,413-ARR,413-ARR_v2_15@1,413-ARR_v1_14@1,"To obtain sentences with negations, we extend the NegEx lexicon with additional negation cues obtained from biomedical texts (Morante, 2010), and apply it to sentences extracted from a corpus using the SpaCy English sentence tokenizer, keeping only those sentences with at least one identified negation cue.","To obtain sentences with negations, we extend the NegEx lexicon with additional negation cues obtained from biomedical texts (Morante, 2010), and apply it to sentences extracted from a corpus using the SpaCy English sentence tokenizer, keeping only those sentences with at least one identified negation.","Modify,Clarity",Clarity
4292,413-ARR,413-ARR_v2_16@2,413-ARR_v1_15@2,"Descriptions of clinical trials can be quite long, but a core aspect of the trial description is the patient inclusion/exclusion criteria, specifying what types of characteristics or conditions a patient must have/not have in order to be suitable for the trial.","Description of clinical trials can be quite long, but a core aspect of the trial description is the patient inclusion/exclusion criteria, specifying what types of characteristics or conditions a patient must have/not have in order to be suitable for the trial.","Modify,Grammar",Grammar
4293,413-ARR,413-ARR_v2_19@1,413-ARR_v1_18@1,"We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representations for sentences containing negation.","We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representation for sentences containing negation.","Modify,Grammar",Grammar
4294,413-ARR,413-ARR_v2_20@1,413-ARR_v1_19@1,"Inspired by work on entity and span masking (Joshi et al., 2020;Yamada et al., 2020), we explore explicitly incorporating information about negation cues into the model by masking these cues, and targeting prediction of the masked cue in the pretraining stage.","Inspired by various works on entity and span masking (Joshi et al., 2020;Yamada et al., 2020), we explore explicitly incorporating information about negation cues into the model by masking these cues, and targeting prediction of the masked cue in the pre-training stage.","Modify,Clarity",Clarity
4432,426-ARR,426-ARR_v2_4@0,426-ARR_v1_4@0,"The dominant approach in the design of current NLP solutions is (pre-)training a large neural language model, usually applying a Transformer architecture, such as GPT-2, RoBERTa or T5, and fine-tuning the model for specific tasks (Devlin et al., 2019;Raffel et al., 2019).","The dominant approach in the design of current NLP solutions consists in (pre-)training a large neural language model, usually applying a Transformer architecture, such as GPT-2, RoBERTa or T5, and fine-tuning the model for specific tasks (Devlin et al., 2018;Raffel et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
4433,426-ARR,426-ARR_v2_34@1,426-ARR_v1_31@1,"Secondly, the PDF files were downloaded and the English texts were processed into DjVu files (as this is the target format for the pipeline) using the pdf2djvu tool 5 .","Secondly, the PDF files were downloaded and the English texts were processed into DjVu files (as this is the target format for the pipeline) using the pdf2dvju tool 4 .","Modify,Grammar",Grammar
4434,426-ARR,426-ARR_v2_38@1,426-ARR_v1_34@1,"At present, the most popular architectures for language models are Transformer (Devlin et al., 2019) models (earlier, e.g. Word2vec (Mikolov et al., 2013) or LSTM models (Peters et al., 2017)).","At present, the most popular architectures for language models are Transformer (Devlin et al., 2018) models (earlier, e.g. Word2vec (Mikolov et al., 2013) or LSTM models (Peters et al., 2017)).","Modify,Fact/Evidence",Fact/Evidence
4435,426-ARR,426-ARR_v2_4@1,426-ARR_v1_4@1,"The solutions are evaluated on benchmarks such as GLUE (Wang et al., 2019b) or SuperGLUE (Wang et al., 2019a), which allow comparing the performance of various methods designed for the same purpose.","The solutions are evaluated on benchmarks such as GLUE ((Wang et al., 2018)) or SuperGLUE ((Wang et al., 2019)), which allow comparing the performance of various methods designed for the same purpose.","Modify,Fact/Evidence",Fact/Evidence
4436,426-ARR,426-ARR_v2_41@1,426-ARR_v1_37@1,The ChallAm models have the same number of parameters as the original RoBERTa Base (125M).,The ChallAm models have the same numbers of parameters as the original RoBERTa Base (125M).,"Modify,Grammar",Grammar
4437,426-ARR,426-ARR_v2_4@2,426-ARR_v1_4@2,An important feature of a good NLP benchmark is the clear separation between train and test sets.,A main feature of a good NLP benchmark is the clear separation between train and test sets.,"Modify,Clarity",Clarity
4438,426-ARR,426-ARR_v2_4@3,426-ARR_v1_4@3,"This requirement prevents data contamination, when the model (pre-)trained on huge data might have ""seen"" the test set in some form.","This requirement prevents data contamination, when the model (pre-)trained on huge data might have ""seen"" the test set.","Modify,Clarity",Clarity
4439,426-ARR,426-ARR_v2_46@10,426-ARR_v1_42@10,These data are used by the Gonito evaluation platform during submission evaluation.,These data are used by the evaluation platform during submission evaluation.,"Modify,Clarity",Clarity
4440,426-ARR,426-ARR_v2_52@1,426-ARR_v1_48@1,"They are released on the Gonito evaluation platform, which enables the calculation of metrics both offline and online, as well as the submission of solutions.","They are released on an evaluation platform, which enables the calculation of metrics both offline and online, as well as the submission of solutions.","Modify,Clarity",Clarity
4441,426-ARR,426-ARR_v2_58@1,426-ARR_v1_55@1,The expected format is the latitude and longitude.,The expected format is a latitude and longitude.,"Modify,Grammar",Grammar
4442,426-ARR,426-ARR_v2_61@3,426-ARR_v1_57@3,"The metric is perplexity; PerplexityHashed, to be precise, as implemented in the GEval evaluation tool (Graliński et al., 2019), the modification is analogous to LogLossHashed in (Graliński, 2017), its goal is to ensure proper evaluation in the competitive (shared-task) setup (i.e. avoid self-reported probabilities and ensure objective comparison of all reported solutions, including out-of-vocabulary words).","The metric is perplexity; PerplexityHashed, to be precise, as implemented in the GEval evaluation tool , the modification is analogous to LogLossHashed in (Graliński, 2017), its goal is to ensure proper evaluation in the competitive (shared-task) setup (i.e. avoid self-reported probabilities and ensure objective comparison of all reported solutions, including out-of-vocabulary words).","Modify,Fact/Evidence",Fact/Evidence
4443,426-ARR,426-ARR_v2_64@0,426-ARR_v1_61@0,Results,Baselines,"Modify,Other",Other
4444,426-ARR,426-ARR_v2_65@0,426-ARR_v1_62@0,Strong baselines for all three tasks are available at the Gonito evaluation platform.,Baselines for all three tasks are available at the evaluation platform.,"Modify,Clarity",Clarity
4445,426-ARR,426-ARR_v2_65@1,426-ARR_v1_62@1,"The baselines (see Tables 2 and 3) include, for each model, its score in the appropriate metric as well as the Git SHA1 reference code in the Gonito benchmark (in curly brackets).","5 The baselines (see Tables 2 and 3) include, for each model, its score in the appropriate metric as well as the Git SHA1 reference code (in curly brackets).","Modify,Fact/Evidence",Fact/Evidence
4446,426-ARR,426-ARR_v2_69@1,426-ARR_v1_64@1,RetroGeo requires two values (latitude and longitude) -we treat them separately and train two separate regression models for them.,RetroGeo requires two values (latitude and longitude) -we treat them separately and train two separate models for them.,"Modify,Fact/Evidence",Fact/Evidence
4447,426-ARR,426-ARR_v2_76@1,426-ARR_v1_71@1,"Since standard RoBERTa training does not incorporate any data, but text, we did not include temporal metadata during inference.","Since standard RoBERTa training does not incorporate any data, but text, we didn't include temporal metadata during inference.","Modify,Clarity",Clarity
4448,426-ARR,426-ARR_v2_78@1,426-ARR_v1_73@1,This means the incorporation of temporal metadata has a positive impact on the MLM task.,This means the incorporation of temporal metadata has a positive impact on MLM task.,"Modify,Grammar",Grammar
4449,426-ARR,426-ARR_v2_80@5,426-ARR_v1_75@5,"We use one of them, DeHateBERT (Aluru et al., 2020), to detect the abusive texts in the ChallAm dataset.","We use one of them, DeHateBERT (Aluru et al., 2020), to filter out the abusive texts in the ChallAm dataset.","Modify,Clarity",Clarity
4450,426-ARR,426-ARR_v2_80@6,426-ARR_v1_75@6,We tagged items that either (1) are marked as abusive speech by DeHateBERT with the probability greater than 0.75 or (2) contain words from a list of blocked words.,We filtered out items that either (1) are marked as abusive speech by DeHateBERT with the probability greater than 0.75 or (2) contain words from a list of blocked words.,"Modify,Clarity",Clarity
4451,426-ARR,426-ARR_v2_80@7,426-ARR_v1_75@7,The fraction of detected texts was 2.04-2.40 % (depending on the challenge and set).,The fraction of filtered out texts was 2.04-2.40% (depending on the challenge and set).,"Modify,Clarity",Clarity
4452,426-ARR,426-ARR_v2_90@0,426-ARR_v1_89@0,"See Table 4 and 5 for the list of top 30 features correlating most with, respectively, the worst and bad results in ChallAm challenges (as returned by the GEval tool with the option -worst-features -numerical-features (Graliński et al., 2019)).","See Table 4 and 5 for the list of top 30 features correlating most with, respectively, the worst and bad results in ChallAm challenges (as returned by the GEval tool with the option -worst-features -numerical-features ).","Modify,Fact/Evidence",Fact/Evidence
4453,426-ARR,426-ARR_v2_31@1,426-ARR_v1_23@1,We provide and make freely available training data from Chronicling America for three ML tasks.,We provide and make available training data from Chronicling America for three ML tasks.,"Modify,Clarity",Clarity
4454,426-ARR,426-ARR_v2_20@0,426-ARR_v1_24@0,Related Machine Learning datasets and challenges,Similar Machine Learning datasets and challenges,"Modify,Clarity",Clarity
4746,463-ARR,463-ARR_v2_19@1,463-ARR_v1_21@1,"When the dimension of the token's representation is r with m input tokens, the function can be defined as follows:","When the dimension of the token's representation is r with with m input tokens, the function can be defined as follows:","Modify,Grammar",Grammar
4747,463-ARR,463-ARR_v2_21@2,463-ARR_v1_24@2,"The dimension of r 1 , r 2 and r 3 is the dimension of the 2 nd feedforward layer, while the input dimension of the linear layer (L α ) is the output dimension of the first feed-forward layer with the token representation (r 1 , r 2 , r 3 ) as its inputs.","The dimension of r 1 , r 2 and r 3 is the dimension of the 2nd feedforward layer, while the dimension of the linear layer (L α ) is the output dimension of the first feed-forward layer with the token representation (r 1 , r 2 , r 3 ) as its inputs.","Modify,Clarity",Clarity
4748,463-ARR,463-ARR_v2_23@0,463-ARR_v1_26@0,"In this section, we experiment on two different methods to make AdapterBias more parameter efficient.","In this section, we experiment on two ways to make AdapterBias more parameter efficient.","Modify,Clarity",Clarity
4749,463-ARR,463-ARR_v2_25@0,463-ARR_v1_28@0,"Redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important (Houlsby et al., 2019).","Redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important.","Modify,Fact/Evidence",Fact/Evidence
4750,463-ARR,463-ARR_v2_38@0,463-ARR_v1_38@0,"In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters (Houlsby et al., 2019), Diff-pruning (Guo et al., 2020), BitFit (Ben Zaken et al., 2021), andLoRA (Hu et al., 2021).","In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters (Houlsby et al., 2019), Diff-pruning (Guo et al., 2020), andBitFit (Ben Zaken et al., 2021) 2019).","Modify,Fact/Evidence",Fact/Evidence
4751,463-ARR,463-ARR_v2_39@1,463-ARR_v1_38@1,"Although Diff-pruning (Guo et al., 2020) achieves the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM.","Although Diff-pruning (Guo et al., 2020) has the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM.","Modify,Clarity",Clarity
4752,463-ARR,463-ARR_v2_39@2,463-ARR_v1_38@2,"Thus, Diff-pruning requires 340M trainable parameters of BERT-large during the training stage, while AdapterBias only trains 0.17M parameters.","Thus, Diff-pruning requires 340M trainable parameters of BERT-large during the training stage, while AdapterBias only trains 0.23M parameters.","Modify,Fact/Evidence",Fact/Evidence
4753,463-ARR,463-ARR_v2_39@3,463-ARR_v1_38@3,"Furthermore, AdapterBias achieves comparable performance with BitFit and LoRA with fewer parameters needed per task.","Furthermore, AdapterBias achieves comparable performance with BitFit with fewer parameters needed per task.","Modify,Fact/Evidence",Fact/Evidence
4754,463-ARR,463-ARR_v2_5@0,463-ARR_v1_5@0,"To tackle these shortcomings, Adapters (Houlsby et al., 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al., 2017) have been proposed.","To tackle these shortcomings, Adapters (Houlsby et al., 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al., 2017) has been proposed.","Modify,Grammar",Grammar
4755,463-ARR,463-ARR_v2_48@0,463-ARR_v1_47@0,Experiments are conducted to examine whether AdapterBias can be more parameter-efficient by sharing its components across all layers.,Experiments are conducted to see whether Adapter-Bias can be more parameter-efficient by sharing its components across all layers.,"Modify,Clarity",Clarity
4756,463-ARR,463-ARR_v2_5@1,463-ARR_v1_5@1,"Instead of full fine-tuning the whole model, Adapters introduce extra tunable weights and freeze the original parameters of PLM.","Instead of full fine-tuning the whole model, Adapters introduces extra tunable weights and freezes the original parameters of PLM.","Modify,Grammar",Grammar
4757,463-ARR,463-ARR_v2_54@0,463-ARR_v1_53@0,"In Table 5, we use BERT-base (BB) and BERTlarge (BL) as the PLMs.","In Table 5, we use BERT-base (BB) and BERTlarge (BL) for the PLM.","Modify,Grammar",Grammar
4758,463-ARR,463-ARR_v2_56@1,463-ARR_v1_55@1,"Compared to the similar work Bit-Fit (Ben Zaken et al., 2021), where the shifts are identical for all tokens, AdapterBias adds tokendependent shifts to the output representation.","Compared to our similar work Bit-Fit (Ben Zaken et al., 2021), where the shifts are identical for all tokens, AdapterBias adds tokendependent shifts to the output representation.","Modify,Clarity",Clarity
4759,463-ARR,463-ARR_v2_58@4,463-ARR_v1_56@4,"They observe on a syntactic task with BShift (Conneau et al., 2018) that the ninth layer of BERT embeds a rich hierarchy of syntactic information.","Jawahar et al. (2019) also observe on a syntactic task with BShift (Conneau et al., 2018) that the ninth layer of BERT embeds a rich hierarchy of syntactic information.","Modify,Clarity",Clarity
4760,463-ARR,463-ARR_v2_58@6,463-ARR_v1_56@6,"For instance, RTE (Giampiccolo et al., 2007;Bentivogli et al., 2009) and MNLI (Williams et al., 2017), where both recognize textual entailment, have higher values in the upper layers than the lower ones.","For instance, RTE (Giampiccolo et al., 2007;Bentivogli et al., 2009) and MNLI (Williams et al., 2017), where both tasks recognize textual entailment, have higher values in the upper layers than those in the lower ones.","Modify,Clarity",Clarity
4761,463-ARR,463-ARR_v2_61@0,463-ARR_v1_59@0,"Since α i represents the weight of the representation shift for i th token in a transformer layer, we can observe the significance of i th token from the summation of α i in all the transformer layers.","Since α i represents the weight of the representation shift for ith token in a transformer layer, we can observe the significance of ith token from the summation of α i in all the transformer layers.","Modify,Grammar",Grammar
4762,463-ARR,463-ARR_v2_65@2,463-ARR_v1_63@2,"Through extensive experiments, not only does AdapterBias reach competitive results on the GLUE benchmark, but also obtain good performance on small-to-medium datasets.","Through extensive experiments, not only does AdapterBias reaches competitive results on the GLUE benchmark, but it also obtains good performance on small-to-medium datasets.","Modify,Clarity",Clarity
4763,463-ARR,463-ARR_v2_5@9,463-ARR_v1_6@1,"The results in Houlsby et al. (2019) have shown that the performance on GLUE benchmark (Wang et al., 2018) is almost the same when removing the Adapters in the lower layers, which indicates that not every adapter is useful.","The results in Houlsby et al. (2019) have shown that the performance on GLUE benchmark (Wang et al., 2018) is almost the same when removing the Adapters in the first layers, which indicates that not every adapter is useful.","Modify,Clarity",Clarity
4764,463-ARR,463-ARR_v2_5@10,463-ARR_v1_6@2,It raises the question of whether adapters can be even more parameter-efficient.,It leaves the question of whether adapters can be even more parameter-efficient.,"Modify,Clarity",Clarity
4765,463-ARR,463-ARR_v2_6@0,463-ARR_v1_7@0,"To develop practical and memory-efficient methods of utilizing PLMs, Diff pruning (Guo et al., 2020) enables parameter-efficient transfer learning that scales well with new tasks.","To develop practical and memory-efficient adapters, Diff pruning (Guo et al., 2020) enables parameter-efficient transfer learning that scales well with new tasks.","Modify,Fact/Evidence",Fact/Evidence
4766,463-ARR,463-ARR_v2_6@1,463-ARR_v1_7@1,"The approach learns a task-specific ""diff"" vector that extends the original pre-trained parameters and encourages the sparsity of the vector through L 0 -norm regularization.","The approach learns a taskspecific ""diff"" vector that extends the original pretrained parameters and encourages the sparsity of the vector through L 0 -norm regularization.","Modify,Grammar",Grammar
4767,463-ARR,463-ARR_v2_7@4,463-ARR_v1_8@4,"Bit-Fit assigns identical shifts to all the tokens, while AdapterBias adds more significant shifts to the representations that are related to the task.","Bit-Fit assigns identical shifts to all the tokens, while AdapterBias adds more significant shifts to the tokens related to the task.","Modify,Clarity",Clarity
4768,463-ARR,463-ARR_v2_8@0,463-ARR_v1_9@0,"With fewer trainable parameters required, AdapterBias achieves comparable performance on the GLUE benchmark with Houlsby et al. (2019); Pfeiffer et al. (2020a); Guo et al. (2020);Ben Zaken et al. (2021);Hu et al. (2021).","With fewer trainable parameters required, AdapterBias achieves comparable performance on the GLUE benchmark with Houlsby et al. (2019); Pfeiffer et al. (2020a); Guo et al. (2020); Ben Zaken et al. (2021).","Modify,Fact/Evidence",Fact/Evidence
4769,463-ARR,463-ARR_v2_13@0,463-ARR_v1_14@0,"Recently, studies start to focus on improving the parameter-efficiency of adaptation to a new task.","Recently, studies start to focus on improving the parameter-efficiency of adapters.","Modify,Clarity",Clarity
4772,463-ARR,463-ARR_v2_16@2,463-ARR_v1_17@2,AdapterBias produces a suitable weight for the bias based on the input token.,AdapterBias produces a suitable weight of the bias based on the input tokens.,"Modify,Grammar",Grammar
4773,463-ARR,463-ARR_v2_18@3,463-ARR_v1_20@3,The tokens which are more related to the task should be assigned larger representation shifts than other tokens.,"Since some tokens are more important to some tasks, these tokens should be assigned larger representation shifts than other tokens.","Modify,Clarity",Clarity
4788,465-ARR,465-ARR_v2_31@1,465-ARR_v1_29@1,"In the graph structure, we formulate each node as a MeSH label, and edges represent relationships in the MeSH hierarchy.","In the graph structure, we formulate each node as a MeSH label, and edges are implement MeSH hierarchies.","Modify,Clarity",Clarity
4789,465-ARR,465-ARR_v2_38@1,465-ARR_v1_36@1,We consider only a subset of the full MeSH list by employing a masked labelwise attention computes the element-wise multiplication of a mask matrix and an attention matrix for two reasons.,We only consider a subset of the full MeSH list and employ a masked label-wise attention that computes the element-wise multiplication of a mask matrix and an attention matrix for the following two reasons.,"Modify,Clarity",Clarity
4790,465-ARR,465-ARR_v2_39@0,465-ARR_v1_37@0,"For each article, selecting a subset of MeSH labels, namely a MeSH mask, downsamples the negative examples, which forces the classifier to concentrate on the candidate labels.","For each article, selecting a subset of MeSH labels, namely a MeSH mask, achieves down-sampling of the negative examples, which forces the classifier to concentrate on the candidate labels.","Modify,Clarity",Clarity
4791,465-ARR,465-ARR_v2_40@4,465-ARR_v1_38@4,"We build a journal-MeSH label cooccurrence matrix using conditional probabilities, i.e., P (L i | J j ), which denote the probabilities of occurrence of label L i when journal J j appears.","We build a co-occurrence matrix between journals and MeSH labels using conditional probabilities, i.e., P (L i | J j ), which denotes the probability of occurrence of label L i when journal J j appears.","Modify,Clarity",Clarity
4792,465-ARR,465-ARR_v2_42@1,465-ARR_v1_40@1,"To avoid the noise of rare co-occurrences, a threshold τ filters noisy correlations.","To avoid the noise of rare co-occurrences, we set a threshold τ to filter noisy correlations.","Modify,Clarity",Clarity
4793,465-ARR,465-ARR_v2_46@1,465-ARR_v1_45@1,"Next, we use KNN based on cosine similarity between abstracts to find the K nearest neighbours for each article in the training set.","Next, we calculate the cosine similarity between abstracts and use KNN to find k nearest neighbours for each article.","Modify,Clarity",Clarity
4794,465-ARR,465-ARR_v2_46@2,465-ARR_v1_45@2,"To form the unique MeSH mask for article a, we collect MeSH terms M a from the neighbours of a:","After that, we collect MeSH terms from neighbours and form as M n .","Modify,Fact/Evidence",Fact/Evidence
4795,465-ARR,465-ARR_v2_4@3,465-ARR_v1_4@3,"Currently, there are 29,369 main MeSH headings, and each MEDLINE citation has 13 MeSH indices, on average.","Currently, there are 29,369 main MeSH headings, and each MEDLINE citation html has 13 MeSH indices, on average.","Modify,Fact/Evidence",Fact/Evidence
4796,465-ARR,465-ARR_v2_68@1,465-ARR_v1_64@1,"For pre-processing, we removed nonalphanumeric characters, stop words, punctuation, and single character words, and we converted all words to lowercase.","For pre-processing, we removed nonalphanumeric characters, stop words, punctuation, and single character words, and we converted all words are lowercased.","Modify,Grammar",Grammar
4799,465-ARR,465-ARR_v2_78@7,465-ARR_v1_75@7,"Although our model is trained only on the abstract and title (which may suggest that it captures less complex semantics), it performs very well against more complex systems.",Although our model is trained only on the abstract and title (which may suggest that it can capture less complex semantics) it performs very well against more complex systems.,"Modify,Clarity",Clarity
4800,465-ARR,465-ARR_v2_8@2,465-ARR_v1_7@2,"This module combines a hybrid of information, at the levels of words and the latent representations of the semantic units, to capture local correlations and longterm dependencies from text.","This module combines a hybrid of information, at the levels of words and latent semantics, to capture local correlations and long-term dependencies from text.","Modify,Clarity",Clarity
4801,465-ARR,465-ARR_v2_8@3,465-ARR_v1_7@3,2. Our proposed method appears to be the first to employ graph convolutional neural networks that integrate information from the complete MeSH hierarchy to map label representations.,2. Our proposed method appears to be the first to employ graph convolutional neural networks that integrate MeSH hierarchical information to map label representations.,"Modify,Clarity",Clarity
4802,465-ARR,465-ARR_v2_11@8,465-ARR_v1_10@8,The main difference between AttentionMeSH and MeSHProbeNet is that the former uses a label-wise attention mechanism while the latter develops selfattentive MeSH probes to extract comprehensive aspects of information from the input articles.,The main difference between AttentionMeSH and MeSHProbeNet is that the former uses a label-wise attention mechanism while the latter develops selfattentive MeSH probes to extract comprehensive aspects of biomedical information from the input articles.,"Modify,Fact/Evidence",Fact/Evidence
4803,465-ARR,465-ARR_v2_13@0,465-ARR_v1_12@0,Graph Convolutional Networks in Natural Language Processing,Graph Convolutional Network in Text Classification,"Modify,Other",Other
4804,465-ARR,465-ARR_v2_14@0,465-ARR_v1_13@0,"Graph convolutional neural networks (GCN)s (Kipf and Welling, 2017) have received considerable attention and achieved remarkable success in natural language processing recently.","Graph convolutional neural networks (GCN)s (Kipf and Welling, 2017) have received considerable attention recently.","Modify,Claim",Claim
4805,465-ARR,465-ARR_v2_14@4,465-ARR_v1_13@4,"Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multi-label text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels, and Rios and Kavuluru (2018) built a multilabel classifier that was learned from a 2-layer GCN over the label hierarchy.","Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multilabel text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels.","Modify,Fact/Evidence",Fact/Evidence
4814,468-ARR,468-ARR_v2_18@4,468-ARR_v1_18@4,"In this scenario, the bias model's predictions are used to weight the main model's cross-entropy loss.","In this scenario, the bias model's predictions are used to weigh the main model's cross-entropy loss.","Modify,Grammar",Grammar
4815,468-ARR,468-ARR_v2_22@0,468-ARR_v1_22@0,"We work with the MLQE-PE dataset (Fomicheva et al., 2020) which was specifically designed for the training of MT QE models.",We work with the MLQE-PE dataset which was specifically designed for the training of MT QE models.,"Modify,Fact/Evidence",Fact/Evidence
4816,468-ARR,468-ARR_v2_22@3,468-ARR_v1_22@3,"A seventh dataset, Russian-English, was curated from Reddit posts and WikiQuotes.","A seventh dataset, Russian-English, was collected based on Reddit posts and WikiQuotes.","Modify,Clarity",Clarity
4817,468-ARR,468-ARR_v2_27@8,468-ARR_v1_27@8,"The Russian-English dataset is an exception where the source sentence is a good predictor for the translation quality, most likely due to the distinct nature of Reddit data and WikiQuotes (both user-generated).","Perhaps due to the distinct nature of Reddit data and WikiQuotes (both user-generated), the Russian-English dataset is an exception where the source sentence is a good predictor for the translation quality.","Modify,Clarity",Clarity
4818,468-ARR,468-ARR_v2_28@2,468-ARR_v1_28@2,"To achieve this, one of the authors, a German native speaker, manually annotated translations in the test set that are grammatically correct but do not preserve the meaning of the source.","To achieve this, we manually annotated translations in the test set that are grammatically correct but do not preserve the meaning of the source.","Modify,Fact/Evidence",Fact/Evidence
4819,468-ARR,468-ARR_v2_28@4,468-ARR_v1_29@1,"A key takeaway from the labelling process was that it is not only the models that have a partial input bias -human annotators clearly seem to over-rely on the target fluency, too.",A key takeaway from the labelling process was that it is not only the models that have a partial input bias -human annotators clearly seem to over-rely on the target fluency.,"Modify,Clarity",Clarity
4820,468-ARR,468-ARR_v2_28@5,468-ARR_v1_29@2,"Even if the instructions clearly specify that a DA score below 70 should be assigned to inadequate translations, 4 annotators tended to give higher scores if the sentence was fluent and appeared logical.","Despite the instructions clarly specify that a DA score below 70 should be assigned to inadequate translations, 3 annotators tended to give higher scores if the sentence was fluent and appeared logical.","Modify,Fact/Evidence",Fact/Evidence
4821,468-ARR,468-ARR_v2_33@8,468-ARR_v1_34@8,"6 In both cases, the main task optimises the MSE loss, and the auxiliary task is a binary classification problem using the binary cross-entropy loss.","5 In both cases, the main task optimises the MSE loss, however, the auxiliary task is a binary classification problem using the binary cross-entropy loss.","Modify,Clarity",Clarity
4822,468-ARR,468-ARR_v2_35@3,468-ARR_v1_36@3,"The shared layers, on the other hand, are penalised for learning a mapping between target sentence and scores.","The shared layers, however, are penalised for learning a mapping between target sentence and scores.","Modify,Clarity",Clarity
4823,468-ARR,468-ARR_v2_41@2,468-ARR_v1_42@2,"Since our QE task is formulated as a regression problem, we attempt to find an equivalent strategy to weigh down biased examples when working with MSE loss.","Since our QE task is formulated as a regression problem, we attempt to find an equivalent strategy to down-weigh biased examples when working with MSE loss.","Modify,Clarity",Clarity
4826,468-ARR,468-ARR_v2_5@3,468-ARR_v1_5@3,"We work with the recently published multilingual QE dataset MLQE-PE (Fomicheva et al., 2020), allowing us to test the generalisability of our approaches across different languages and quality scores.","We work with the recently published multilingual QE dataset MLQE-PE , allowing us to test the generalisability of our approaches across different languages and quality scores.","Modify,Fact/Evidence",Fact/Evidence
4829,468-ARR,468-ARR_v2_55@1,468-ARR_v1_56@1,"Considering the experimental design, the multitask architecture provides additional degrees of freedom that were not explored extensively, yet.","Firstly, the multitask architecture provides additional degrees of freedom that were not explored extensively, yet.","Modify,Clarity",Clarity
4830,468-ARR,468-ARR_v2_55@2,468-ARR_v1_56@2,"For example, one could vary the amount of training per task or learn the training schedule as a parameter which adapts dynamically during the training process (Kiperwasser and Ballesteros, 2018;Zaremoodi et al., 2018).","For example, one could vary the amount of training per task or even learn the training schedule as a parameter which adapts dynamically during the training process (Kiperwasser and Ballesteros, 2018;Zaremoodi et al., 2018).","Modify,Clarity",Clarity
4831,468-ARR,468-ARR_v2_56@0,468-ARR_v1_56@4,"Going beyond the field of Machine Translation Quality Estimation, it would be interesting to see the methods applied in adjacent areas of NLP.","Furthermore, it would be interesting to apply our proposed methods and architectures in adjacent fields of NLP.","Modify,Claim",Claim
4834,468-ARR,468-ARR_v2_12@3,468-ARR_v1_12@3,"Sentence-level QE has evolved from the first feature-heavy prediction models (Blatz et al., 2004) to neural architectures such as RNNs and Transformers (Vaswani et al., 2017), which accelerated the developments in the field by reducing the work of manual feature engineering and improving contextual representations (Kim et al., 2017;Wang et al., 2018;Fan et al., 2019).","Sentencelevel QE has evolved from the first feature-heavy prediction models in Blatz et al. (2004) to neural architectures such as RNNs and Transformers (Vaswani et al., 2017), which accelerated the developments in the field by reducing the work of manual feature engineering and improving contextual representations (Kim et al., 2017;Wang et al., 2018;Fan et al., 2019).","Modify,Grammar",Grammar
4835,468-ARR,468-ARR_v2_13@6,468-ARR_v1_13@6,"Following their work, in an attempt to reduce statistical artifacts, MLQE-PE (Fomicheva et al., 2020) -a new QE dataset diversifying the topics and languages covered -was created, which forms the basis of this work and will be described in more detail in Section 3.1.","Following their work, in an attempt to reduce statistical artifacts, MLQE-PE -a new QE dataset diversifying the topics and languages covered -was created, which forms the basis of this work and will be described in more detail in Section 3.1.","Modify,Fact/Evidence",Fact/Evidence
4836,468-ARR,468-ARR_v2_18@1,468-ARR_v1_18@1,The notion of focal loss was first introduced by Lin et al. (2017) as a means to improve classification results on imbalanced classes by weighing down the impact of samples that the model had already learned to classify well.,The notion of focal loss was first introduced by Lin et al. (2017) as a means to improve classification results on imbalanced classes by downweighting the impact of samples that the model had already learned to classify well.,"Modify,Clarity",Clarity
4967,473-ARR,473-ARR_v2_26@0,473-ARR_v1_27@0,"The SDG task is to generate a simple definition d sim for a given word and context (w * , c), where c = [w 1 , . . . , w * , . . . , w n ] is a sentence containing w * .","The SDG task consists in generating a simple definition d sim for a given word and context (w * , c), where c = [w 1 , . . . , w * , . . . , w n ] is a sentence containing w * .","Modify,Clarity",Clarity
4968,473-ARR,473-ARR_v2_26@1,473-ARR_v1_27@1,"This task is challenging because there is no corpus like {(w * i , c i , d sim i )} N i=1 and hence it is fully unsupervised.","This task is challenging because there is no corpus like {(w * i , c i , d sim i )} N i=1 and hence fully unsupervised.","Modify,Clarity",Clarity
4969,473-ARR,473-ARR_v2_2@8,473-ARR_v1_2@8,"Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87% 1 .","Our method outperforms the baseline model by a 1.6 SARI score on the English dataset, and the low level (HSK level 1-3) words in Chinese definitions raised by 5.03%.","Modify,Fact/Evidence",Fact/Evidence
4972,473-ARR,473-ARR_v2_33@2,473-ARR_v1_34@2,"Instead, we introduce the language modeling task (Section 3.2.3) to enhance the reconstruction decoder and make it more focused on simple text generation.","Instead, we introduce the language modeling task (Section 3.2.3) to enhance the reconstruction decoder and make it more focusd on simple text generation.","Modify,Grammar",Grammar
4973,473-ARR,473-ARR_v2_37@1,473-ARR_v1_38@1,The model is optimized using the following loss function.,The model is optimized using the following loss function:,"Modify,Grammar",Grammar
4974,473-ARR,473-ARR_v2_54@0,473-ARR_v1_56@0,"For parameters in the decoders, we divided them into two parts, which are complexity-independent and complexity-dependent parameters.","For parameters in the decoders, we dived them into two parts, which are complexity-independent and complexity-dependent parameters.","Modify,Grammar",Grammar
4975,473-ARR,473-ARR_v2_4@1,473-ARR_v1_4@1,"In recent years, researchers attempted to automatically generate definitions for words rather than formulating predefined worddefinition inventories (Ishiwatari et al., 2019;Huang et al., 2021).","In recent years, researchers attempted to automatically generate definitions for words rather than formulating predefined worddefinition inventories (Ishiwatari et al., 2019;Yang et al., 2020;Huang et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
4976,473-ARR,473-ARR_v2_59@0,473-ARR_v1_61@0,We believe that the required information may vary for different complexity.,We believe that the required information may be various for different complexity.,"Modify,Clarity",Clarity
4977,473-ARR,473-ARR_v2_67@2,473-ARR_v1_69@2,We list more statistics in Table 2.,We list more detailed statistics in Table 2.,"Modify,Clarity",Clarity
4978,473-ARR,473-ARR_v2_75@5,473-ARR_v1_78@5,"Note that we set the learning rate to 3e-4, warmup steps to 500 when fine-tuning both MASS and MASS-ZH.","Note that we set the learning rate to 3e-4, warmup steps to 500, and random seed to 1111 when fine-tuning both MASS and MASS-ZH.","Modify,Fact/Evidence",Fact/Evidence
4979,473-ARR,473-ARR_v2_76@3,473-ARR_v1_79@3,We tune the λ parameters in Eq. 7 on the validation set and adopt the same hyper-parameters as the baseline for comparison.,We adopt the same hyper-parameters as the baseline for comparison.,"Modify,Fact/Evidence",Fact/Evidence
4980,473-ARR,473-ARR_v2_87@2,473-ARR_v1_87@4,We also observe that complex definition generation also improves by 0.17 on BLEU and 1.09 on SemSim.,We also observe that complex definition generation also slightly improves by 0.31 on BLEU and 0.82 on SemSim.,"Modify,Fact/Evidence",Fact/Evidence
4981,473-ARR,473-ARR_v2_87@3,473-ARR_v1_87@5,This shows that SimpDefiner improves the ability to generate both complex and simple definitions.,This indicates that SimpDefiner improves the ability to generate both complex and simple definitions.,"Modify,Clarity",Clarity
4982,473-ARR,473-ARR_v2_5@3,473-ARR_v1_6@0,"Different from previous work (Noraset et al., 2017;Gadetsky et al., 2018;Mickus et al., 2019; that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).","Different from previous work (Noraset et al., 2017;Gadetsky et al., 2018;Mickus et al., 2019;Kong et al., 2020) that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).","Modify,Fact/Evidence",Fact/Evidence
4983,473-ARR,473-ARR_v2_88@1,473-ARR_v1_88@1,"We can see that the proportion of low-level (HSK level 1-3) words increases by 3.87%, and that of high-level (HSK level 7+) words decreases by 0.46%.","We can see that the proportion of low-level (HSK level 1-3) words increases by 5.03%, and that of high-level (HSK level 7+) words decreases by 1.61%.","Modify,Fact/Evidence",Fact/Evidence
4984,473-ARR,473-ARR_v2_91@2,473-ARR_v1_91@2,"For the layer normalization (LN) and query projection (QP) as parameter-shared layers, we ablate them by sharing their parameters between models.","For the layer normalization (LN) and query projection (QP) as parameter-shared layers, we ablate them by share their parameters between models.","Modify,Grammar",Grammar
4985,473-ARR,473-ARR_v2_5@4,473-ARR_v1_6@1,"Making the definitions easier to read and understand could benefit the language learners, low literacy readers, as well as helping people with aphasia or dyslexia.","Make the definitions easier to read and understand could benefit the language learners, low literacy readers, as well as helping people with aphasia or dyslexia.","Modify,Grammar",Grammar
4986,473-ARR,473-ARR_v2_5@5,473-ARR_v1_6@2,"For example, compared with the Oxford Dictionary (OD), the Oxford Advanced Learner's Dictionary (OALD) has simpler definitions, which are specifically designed for language learners.","For example, compared with the Oxford Dictionary (OD), the Oxford Advanced Learner's Dictionary (OALD) has simpler definitions, which is specifically designed for language learners.","Modify,Grammar",Grammar
4987,473-ARR,473-ARR_v2_2@2,473-ARR_v1_2@2,We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers.,We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers better.,"Modify,Clarity",Clarity
4988,473-ARR,473-ARR_v2_14@1,473-ARR_v1_15@1,"Gadetsky et al. (2018) tackled this problem by computing the AdaGram vectors (Bartunov et al., 2016) of input words, which are capable of learning different representations at desired semantic resolutions.","Gadetsky et al. (2018) tackled this problem by computing the AdaGram vectors (Bartunov et al., 2016) of input words, which is capable to learn different representations at desired semantic resolution.","Modify,Grammar",Grammar
4989,473-ARR,473-ARR_v2_14@4,473-ARR_v1_15@4,"Reid et al. (2020) initialized encoders with BERT (Devlin et al., 2019) and employed variational inference for estimation and leveraged contextualized word embeddings for improved performance.","Reid et al. (2020) initialized encoders with BERT (Devlin et al., 2019) and employed variational inference for estimation and leverage contextualized word embeddings for improved performance.","Modify,Grammar",Grammar
4990,473-ARR,473-ARR_v2_14@5,473-ARR_v1_15@5,Bevilacqua et al. (2020) employed a novel spanbased encoding scheme to fine-tune a pre-trained English encoder-decoder system to generate definitions.,Bevilacqua et al. (2020) employed a novel span-based encoding scheme to fine-tune a pre-trained English encoderdecoder system to generate definitions.,"Modify,Grammar",Grammar
4991,473-ARR,473-ARR_v2_15@2,473-ARR_v1_16@2,"Besides, our model is based on MASS (Song et al., 2019), which is a pre-trained encoder-decoder model and is suitable for generation tasks.","Besides, our model is based on MASS (Song et al., 2019), which is a pre-trained encoder-decoder model and is more suitable for generation tasks.","Modify,Grammar",Grammar
4992,473-ARR,473-ARR_v2_2@5,473-ARR_v1_2@5,We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders.,We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between the components.,"Modify,Clarity",Clarity
4993,473-ARR,473-ARR_v2_18@3,473-ARR_v1_19@3,"Martin et al. (2019) proposed to prepend additional prompt tokens to source sentences at train time, which enables the end-users to condition the simplifications returned by the model on attributes like length, lexical complexity, and syntactic complexity.","Martin et al. (2019) proposed to prepend additional prompt tokens to source sentences at train time, which enable the end-users to condition the simplifications returned by the model on attributes like length, lexical complexity, and syntactic complexity.","Modify,Grammar",Grammar
4994,473-ARR,473-ARR_v2_2@6,473-ARR_v1_2@6,"By jointly training these components, the framework can generate both complex and simple definitions simultaneously.","By joint training these components, the framework can generate both complex and simple definitions simultaneously.","Modify,Grammar",Grammar
4995,473-ARR,473-ARR_v2_19@1,473-ARR_v1_20@1,"Unlike the baseline, the SimpDefiner can generate simple definitions directly, alleviating the accumulated errors.","Unlike the baseline, the SimpDefiner can simultaneously generate complex and simple definitions without the need for aligned complex-simple sentence pairs.","Modify,Claim",Claim
5502,61-ARR,61-ARR_v2_26@4,61-ARR_v1_22@4,"In summary, relative slot accuracy enables relative comparison according to the distribution of the domain in a dialogue.","In summary, relative slot accuracy enables relative comparison according to the distribution of the domain in a dialog.","Modify,Grammar",Grammar
5503,61-ARR,61-ARR_v2_26@5,61-ARR_v1_23@0,"Dependency on Predefined Slots As discussed in Section 2.2, slot accuracy requiring total predefined slots is not a scalable method for evaluating the current dialogue dataset that contains a few domains in each dialogue.","Dependency on predefined slots As discussed in Section 2.2, slot accuracy requiring total predefined slots is not a scalable method for evaluating the current dialog dataset that contain a few domains in each dialog.","Modify,Grammar",Grammar
5504,61-ARR,61-ARR_v2_26@6,61-ARR_v1_23@1,"For example, when evaluating a dialogue sample that solely deals with the restaurant domain, even domains that never appear at all (i.e., hotel, train, attraction, and taxi) are involved in measuring performance, making deviations among different models trivial.","For example, when evaluating a dialog sample that solely deals with the restaurant domain, even domains that never appear at all (i.e., hotel, train, attraction, and taxi) are involved in measuring performance, making deviations among different models trivial.","Modify,Grammar",Grammar
5505,61-ARR,61-ARR_v2_26@7,61-ARR_v1_23@2,"However, relative slot accuracy can evaluate the model's predictive score without being affected by slots never seen in the current dialogue, which is a more realistic way, considering that each dialogue contains its own turn and slot composition.","However, relative slot accuracy can evaluate the model's predictive score without being affected by slots never seen in the current dialog, which is a more realistic way, considering that each dialog contains its own turn and slot composition.","Modify,Grammar",Grammar
5506,61-ARR,61-ARR_v2_29@2,61-ARR_v1_26@2,"This metric is not affected by unseen slots in the current dialogue situation, and compensates for the model's correct predic-tion.","This metric is not affected by unseen slots in the current dialog situation, and compensates for the model's correct prediction.","Modify,Grammar",Grammar
5507,61-ARR,61-ARR_v2_30@0,61-ARR_v1_27@0,"Our findings show that if the model makes an incorrect prediction, the error accumulates until the end of the dialogue, and the joint goal accuracy remains at zero.","Our findings show that if the model makes an incorrect prediction, the error accumulates until the end of the dialog, and the joint goal accuracy remains at zero.","Modify,Grammar",Grammar
5508,61-ARR,61-ARR_v2_30@1,61-ARR_v1_27@1,"In this section, we discuss a few cases of 59 dialogues that do not show the trend among 642 dialogues selected in Section 2.1; however, it is important to note that these few cases have negligible effect on the trend in Figure 1, solely changing the position where the joint goal accuracy first becomes zero.","In this section, we discuss a few cases of 59 dialogs that do not show the trend among 642 dialogs selected in Section 2.1; however, it is important to note that these few cases have negligible effect on the trend in Figure 1, solely changing the position where the joint goal accuracy first becomes zero.","Modify,Grammar",Grammar
5509,61-ARR,61-ARR_v2_31@0,61-ARR_v1_27@2,"We sampled dialogues of the MultiWOZ 2.1 test set in Table A1 and Table A2, and marked values appearing in the dialogue in bold.","We sampled dialogs of the MultiWOZ 2.1 test set in Table A1 and Table A2, and marked values appearing in the dialog in bold.","Modify,Grammar",Grammar
5510,61-ARR,61-ARR_v2_31@2,61-ARR_v1_27@4,"In the first dialogue presented in Table A1, the joint goal accuracy is measured as 1 at turn 2.","In the first dialog presented in Table A1, the joint goal accuracy is measured as 1 at turn 2.","Modify,Grammar",Grammar
5511,61-ARR,61-ARR_v2_32@0,61-ARR_v1_28@0,"The second dialogue presented in Table A2, reports the incorrect prediction according to the interpretation of annotations at turn 4.","The second dialog presented in Table A2, reports the incorrect prediction according to the interpretation of annotations at turn 4.","Modify,Grammar",Grammar
5512,61-ARR,61-ARR_v2_32@1,61-ARR_v1_28@1,"In other words, because the dialogue about the hotel-internet slot appears over turns 4 and 5, it is solely an error depending on the prediction timing of the model.","In other words, because the dialog about the hotel-internet slot appears over turns 4 and 5, it is solely an error depending on the prediction timing of the model.","Modify,Grammar",Grammar
5513,61-ARR,61-ARR_v2_33@1,61-ARR_v1_29@1,"Furthermore, the fact that the starting point of making the joint goal accuracy of subsequent turns to 0 mainly occurs at the beginning of the dialogue does not change.","Furthermore, the fact that the starting point of making the joint goal accuracy of subsequent turns to 0 mainly occurs at the beginning of the dialog does not change.","Modify,Grammar",Grammar
5514,61-ARR,61-ARR_v2_2@0,61-ARR_v1_2@0,Dialogue state tracking (DST) aims to extract essential information from multi-turn dialogue situations and take appropriate actions.,Dialogue state tracking (DST) aims to extract essential information from multi-turn dialog situations and take appropriate actions.,"Modify,Grammar",Grammar
5515,61-ARR,61-ARR_v2_6@1,61-ARR_v1_6@1,"To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ dataset.","To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ.","Modify,Clarity",Clarity
5516,61-ARR,61-ARR_v2_6@2,61-ARR_v1_6@2,"While slot accuracy has the challenge of overestimation by always considering all predefined slots in every turn, relative slot accuracy does not depend on predefined slots, and calculates a score that is affected solely by slots that appear in the current dialogue.","Because slot accuracy has the challenge of overestimation by always considering all predefined slots in every turn, relative slot accuracy does not depend on predefined slots, and calculates a score that is affected solely by slots that appear in the current dialog.","Modify,Clarity",Clarity
5517,61-ARR,61-ARR_v2_9@3,61-ARR_v1_9@3,"As illustrated in Figure 1, we measured the relative position of the turn causing this phenomenon for the dialogue.","As illustrated in Figure 1, we measured the relative position of the turn causing this phenomenon for the dialog.","Modify,Grammar",Grammar
5518,61-ARR,61-ARR_v2_9@6,61-ARR_v1_9@6,"Accordingly, the relative position where joint goal accuracy first became zero was mainly at the beginning of the dialogue 1 .","Accordingly, the relative position where joint goal accuracy first became zero was mainly at the beginning of the dialog 1 .","Modify,Grammar",Grammar
5519,61-ARR,61-ARR_v2_9@7,61-ARR_v1_9@7,"This means that the joint goal accuracy after the beginning of the dialogue is unconditionally measured as zero because of the initial misprediction, although the model may correctly predict new belief states at later turns.","This means that the joint goal accuracy after the beginning of the dialog is unconditionally measured as zero because of the initial misprediction, although the model may correctly predict new belief states at later turns.","Modify,Grammar",Grammar
5520,61-ARR,61-ARR_v2_9@8,61-ARR_v1_9@8,"Failure to measure the performance of the latter part means that it cannot consider various dialogue situations provided in the dataset, which is a critical issue in building a realistic DST model.","Failure to measure the performance of the latter part means that it cannot consider various dialog situations provided in the dataset, which is a critical issue in building a realistic DST model.","Modify,Grammar",Grammar
5521,61-ARR,61-ARR_v2_11@0,61-ARR_v1_11@0,Slot accuracy can compensate for situations where joint goal accuracy does not fully evaluate the dialogue situation.,Slot accuracy can compensate for situations where joint goal accuracy does not fully evaluate the dialog situation.,"Modify,Grammar",Grammar
5522,61-ARR,61-ARR_v2_13@1,61-ARR_v1_13@1,"Each value of x-axis in Figure 2 indicates the ""maximum"" number of slots that appear in a single dialogue, and we confirmed that approximately 85% of the test set utilized solely less than 12 of the 30 predefined slots in the experiment.","Each value of x-axis in Figure 2 indicates the ""maximum"" number of slots that appear in a single dialog, and we confirmed that approximately 85% of the test set utilized solely less than 12 of the 30 predefined slots in the experiment.","Modify,Grammar",Grammar
5523,61-ARR,61-ARR_v2_13@2,61-ARR_v1_13@2,"Because the number of belief states appearing in the early and middle turns of the dialogue are smaller, and even fewer states make false predictions, calculating slot accuracy using Equation 2 reduces the influence of M and W , and the final score is dominated by the total slot number T .","Because the number of belief states appearing in the early and middle turns of the dialog are smaller, and even fewer states make false predictions, calculating slot accuracy using Equation 2 reduces the influence of M and W , and the final score is dominated by the total slot number T .","Modify,Grammar",Grammar
5524,61-ARR,61-ARR_v2_18@1,61-ARR_v1_16@1,"The deviation among DST models will be even more minor when constructing datasets with various dialogue situations, because the number of predefined slots will continually in-crease.","The deviation among DST models will be even more minor when constructing datasets with various dialog situations, because the number of predefined slots will continually increase.","Modify,Grammar",Grammar
5525,61-ARR,61-ARR_v2_23@0,61-ARR_v1_20@0,"We measured MultiWOZ 2.1, an improved version of MultiWOZ 2.0 , which has been adopted in several studies, according to Table A5.","We measured MultiWOZ 2.1, an improved version of MultiWOZ 2.0 (Budzianowski et al., 2018), which has been adopted in several studies, according to Table A5.","Modify,Fact/Evidence",Fact/Evidence
5526,61-ARR,61-ARR_v2_2@4,61-ARR_v1_2@4,"Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialogue.","Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialog.","Modify,Grammar",Grammar
5623,68-ARR,68-ARR_v2_20@2,68-ARR_v1_20@2,"Once S 2 DM is trained, only the output of source language MLP network is fed into the linear output layer for MRC.","Once S 2 DM is trained, only the output of source language MLP network input to linear output layer for MRC.","Modify,Clarity",Clarity
5624,68-ARR,68-ARR_v2_25@0,68-ARR_v1_25@0,"The variational inference process of VG-VAE uses a factorized approximated posterior q ϕ (y|x)q ϕ (z|x) = q ϕ (y, z|x) with the objective function that maximizes a lower bound of the marginal log-likelihood:","The variational inference process of VG-VAE uses a factorized approximated posterior q φ (y|x)q φ (z|x) = q φ (y, z|x) with the objective function that maximizes a lower bound of the marginal log-likelihood:","Modify,Grammar",Grammar
5625,68-ARR,68-ARR_v2_30@3,68-ARR_v1_28@4,"As pre-trained representations are contextually-encoded token vectors, latent variable vectors obtained by sampling from the distributions need to be averaged so as to output sentence-level semantic and syntactic vector.","The pretrained representations of sentence are contextuallyencoded token vectors, so the latent variable vectors obtained by sampling from the distributions need to be averaged, then output sentence-level semantic vector and syntactic vector.","Modify,Clarity",Clarity
5626,68-ARR,68-ARR_v2_4@0,68-ARR_v1_4@0,"Multilingual pre-trained language models (PLMs) (Devlin et al., 2019;Conneau and Lample, 2019;Conneau et al., 2020) have been widely explored in cross-lingual understanding tasks.","Multilingual pre-trained language models (PLMs) (Devlin et al., 2019;Conneau and Lample, 2019;Conneau et al., 2020) have been widely utilized in cross-lingual understanding tasks.","Modify,Clarity",Clarity
5627,68-ARR,68-ARR_v2_34@0,68-ARR_v1_35@0,"In order to guide S 2 DM to disassociate syntactic information into the syntactic latent variable z, we also define three losses tailored for capturing different types of syntactic information.","In order to guide S 2 DM to disassociate syntactic information into the syntactic latent variable z, we also define three types of losses tailored for capturing different types of syntactic information.","Modify,Clarity",Clarity
5628,68-ARR,68-ARR_v2_34@1,68-ARR_v1_35@1,"First, we employ Word Position Loss (WPL) , defined as follows:","First, we employ Word Position Loss (WPL) , as follow:","Modify,Clarity",Clarity
5629,68-ARR,68-ARR_v2_4@1,68-ARR_v1_4@1,"However, zero-shot transfer method based on multilingual PLMs does not work well for low-resource language MRC.","However, the zero-shot transfer method based on multilingual PLM has limited effectiveness for low-resource languages MRC tasks.","Modify,Clarity",Clarity
5630,68-ARR,68-ARR_v2_4@2,68-ARR_v1_4@2,"Such multilingual MRC models could roughly detect answer spans but may fail to predict the precise boundaries of answers (Yuan et al., 2020).","This type of multilingual MRC model could roughly detect answer spans but fail to predict the precise boundaries of answers (Yuan et al., 2020).","Modify,Clarity",Clarity
5631,68-ARR,68-ARR_v2_49@0,68-ARR_v1_50@0,Generalization Analysis,Analysis,"Modify,Clarity",Clarity
5632,68-ARR,68-ARR_v2_55@1,68-ARR_v1_52@4,"In the same way, both p θ (x s , y s ) and p θ (x t , y t ) also fit to the same distribution, no matter what the target language is.","In the same way, p θ (x s , y s ) and p θ (x t , y t ) also fit to the same distribution, no matter what the target language is.","Modify,Clarity",Clarity
5633,68-ARR,68-ARR_v2_56@0,68-ARR_v1_54@0,"Furthermore, the semantic discrimination loss in Eq.( 5) guarantees that the semantic vectors of the source language and the target language are similar to each other.","Furthermore, the semantic discrimination loss Eq.( 5) guarantees that the semantic vectors of the source language and the target language are similar.","Modify,Clarity",Clarity
5634,68-ARR,68-ARR_v2_56@1,68-ARR_v1_54@1,"Minimizing Eq.( 5) can be equivalent to: sim(y s , y t ) > sim(y s , n t ) + δ sim(y s , y t ) > sim(n s , y t ) + δ which is to maximize sim(y s , y t ) to encourages the target semantic vector to approach parallel source semantic vector.","Minimizing Eq.( 5) can be equivalent to: sim(ys, yt) > sim(ys, nt) + δ sim(ys, yt) > sim(ns, yt) + δ which is to maximize sim(y s , y t ) to encourages the target semantic vector to approach parallel source semantic vector.","Modify,Grammar",Grammar
5635,68-ARR,68-ARR_v2_56@3,68-ARR_v1_55@1,"Therefore, our multilingual MRC model is suitable even for low-resource languages without training data for the decoupling model.","Therefore, our multilingual MRC model is suitable even for low-resource languages without training data of decoupling model.","Modify,Grammar",Grammar
5636,68-ARR,68-ARR_v2_59@0,68-ARR_v1_58@0,"XQuAD (Artetxe et al., 2020) consists of 11 datasets of different languages translated from the SQuAD v1.1 (Rajpurkar et al., 2016) development set, including Spanish (es), German (de), Greek (el), Russian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), and Romanian (ro).","XQuAD (Artetxe et al., 2020) consists of 11 languages corpus translated from the SQuAD v1.1 (Rajpurkar et al., 2016) development set, include : Spanish (es), German (de), Greek (el), Russian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), and Romanian (ro).","Modify,Clarity",Clarity
5637,68-ARR,68-ARR_v2_61@0,68-ARR_v1_60@0,"TyDi QA-GoldP is the gold passage task in TyDi QA (Clark et al., 2020) covering 9 typologically diverse languages: Arabic (ar), Bengali (bg), English (en), Finnish (fi), Indonesian (id), Korean (ko), Russian (ru), Swahili (sw), Telugu (te).","TyDi QA-GoldP is the gold passage task in Ty-Di QA (Clark et al., 2020) covers 9 typologically diverse languages: Arabic (ar), Bengali (bg), English (en), Finnish (fi), Indonesian (id), Korean (ko), Russian (ru), Swahili (sw), Telugu (te).","Modify,Grammar",Grammar
5638,68-ARR,68-ARR_v2_69@0,68-ARR_v1_66@0,"For S 2 DM, we collected 26k labelled parallel sentence pairs from the Universal Dependencies (UD 2.7) Corpus (Zeman et al., 2020) as the training set.","For S 2 DM, we collected approximately 26,000+ labelled parallel sentence pairs from the Universal Dependencies (UD 2.7) Corpus (Zeman et al., 2020) as the training set.","Modify,Clarity",Clarity
5639,68-ARR,68-ARR_v2_69@1,68-ARR_v1_66@1,The training set covers 20 languages and overlap with 13 languages of three MRC datasets.,The training set covers 20 languages and overlap with 13 languages of three MRC evaluation datasets.,"Modify,Clarity",Clarity
5640,68-ARR,68-ARR_v2_70@0,68-ARR_v1_67@0,"For our multilingual MRC models and two baseline models, we fine-tuned them on the SQuAD v1.1 (Rajpurkar et al., 2016) and evaluated them on the test data of the three multilingual MRC datasets.","For our multilingual MRC models and two baseline models, we fine-tuned them on the SQuAD v1.1 (Rajpurkar et al., 2016) and evaluated the test data of the three multilingual MRC datasets.","Modify,Clarity",Clarity
5641,68-ARR,68-ARR_v2_72@4,68-ARR_v1_69@4,"Especially, compared with baselines on the TyDi QA-Gold dataset, S 2 DM_SP based on XLM-100 and mBERT gains 4.1%, 4.2% EM improvements on average across 9 languages, respectively.","Especially, compared with baselines on TyDi QA-Gold datasets, S 2 DM_SP based on XLM-100 and mBERT gains 4.1%, 4.2% EM respectively improvements on average across 9 languages.","Modify,Clarity",Clarity
5642,68-ARR,68-ARR_v2_73@1,68-ARR_v1_70@1,"For cross-lingual transfer performance, our models are better than the two baselines in terms of either EM or F1 on all 11 low-resource target languages.","For cross-lingual transfer performance, our models are better than the two baselines on EM or F1 of all 11 low-resource target languages.","Modify,Clarity",Clarity
5643,68-ARR,68-ARR_v2_5@3,68-ARR_v1_4@5,"(b) A case from BiPaR (Jing et al., 2019) where the answer predicted by a model transferred from English to Chinese violates syntactic constituent boundaries in the target language.","(b) A case from BiPaR (Jing et al., 2019) where the answer predicted by the model transferred from English to Chinese violates syntactic constituent boundaries.","Modify,Fact/Evidence",Fact/Evidence
5644,68-ARR,68-ARR_v2_74@1,68-ARR_v1_70@3,"The results of TyDi QA-GoldP are shown in Table 4, and our models are superior to the baselines in terms of either EM or F1 for all 8 low-resource target languages.","The results of TyDi QA-GoldP are shown in Table 4, and our models are superior to the baselines on EM or F1 of all 8 low-resource target languages.","Modify,Clarity",Clarity
5645,68-ARR,68-ARR_v2_75@1,68-ARR_v1_71@1,The results on the three datasets show the effectiveness on five languages not included in the training target languages for S 2 DM.,The results in three datasets show the effectiveness of five languages not included in the training target languages for S 2 DM.,"Modify,Grammar",Grammar
5646,68-ARR,68-ARR_v2_78@1,68-ARR_v1_74@1,The results are shown in Figure 3.,The results are reported in Table 3.,"Modify,Clarity",Clarity
5647,68-ARR,68-ARR_v2_79@3,68-ARR_v1_75@3,"All ablation models do not exceed our best model, illustrating the importance of all proposed losses.","All ablation models do not exceed our best model, illustrating the importance of all our losses.","Modify,Clarity",Clarity
5648,68-ARR,68-ARR_v2_80@0,68-ARR_v1_78@0,Why the S 2 DM Works?,Why S 2 DM Works?,"Modify,Grammar",Grammar
5649,68-ARR,68-ARR_v2_6@1,68-ARR_v1_5@2,"Liang et al. (2021) present a boundary calibration model stacked over a base sequence labeling module, introducing a phrase boundary recovery task to pretrain the calibration module on large-scale multilingual datasets synthesized from Wikipedia documents.","Liang et al. (2021) propose a separate model for boundary calibration based on the output of a base zero-shot transfer model, introducing a phrase boundary recovery task to pretrain calibration module on large-scale multilingual datasets synthesized from Wikipedia documents.","Modify,Fact/Evidence",Fact/Evidence
5650,68-ARR,68-ARR_v2_86@2,68-ARR_v1_83@2,"For low-resource languages without training data for the decoupling model, our theoretical analysis and experiments verify the generalization of our multilingual MRC model.","For low-resource languages without training data of decoupling model, we theoretically analyze and experiment verify the generalization of our multilingual MRC model.","Modify,Clarity",Clarity
5651,68-ARR,68-ARR_v2_6@3,68-ARR_v1_6@1,"On four multilingual MRC datasets, we use Stanford CoreNLP 1 to collect syntax parse trees and calculate the percentages of ground-truth answers that respect syntactic constituent boundaries.","On four multilingual MRC evaluation datasets, we use Stanford CoreNLP 1 to collect syntax parse trees and calculate the percentages of ground-truth answers that respect syntactic constituent boundaries.","Modify,Clarity",Clarity
5652,68-ARR,68-ARR_v2_6@4,68-ARR_v1_6@2,"As shown in Table 1, over 87% of answer spans respect the syntactic constraint.","As shown in Table 1, over 87% of answer spans respect syntactic constraint.","Modify,Grammar",Grammar
5653,68-ARR,68-ARR_v2_7@1,68-ARR_v1_7@1,"For questions where the monolingual model correctly predicts the answer and respect syntactic constraint, 23.15% of them are incorrectly predicted by the transfer model, and the predicted answers violate the syntactic constraint, illustrated by the case in Figure 1 (b).","For answers that the monolingual model correctly predicts and respect syntactic constraint, 23.15% of them, which are incorrectly predicted by the transfer model, violate the syntactic constraint, illustrated by the case in Figure 1(b).","Modify,Clarity",Clarity
5654,68-ARR,68-ARR_v2_7@2,68-ARR_v1_7@2,"This suggests that the source language syntax may have a negative impact on the answer boundary detection in the target language during zero-shot transfer, due to the linguistic discrepancies between the two languages.","It suggests that the source language syntax may have a negative impact on the answer boundary detection on the target language during zero-shot transfer, due to the linguistic discrepancies between the two languages.","Modify,Grammar",Grammar
5655,68-ARR,68-ARR_v2_8@0,68-ARR_v1_8@0,"However, linguistic discrepancies are diverse and it is difficult to learn them.","However, the linguistic discrepancies are diverse and impossible to learn.","Modify,Claim",Claim
5656,68-ARR,68-ARR_v2_8@1,68-ARR_v1_8@1,"We hence propose to decouple semantics from syntax in pretrained models for multilingual MRC, transforming the learning of linguistic discrepancies into universal semantic information.","We propose to decouple semantics from syntax in pre-trained models for multilingual MRC, transforming the learning of linguistic discrepancies into universal semantic information.","Modify,Grammar",Grammar
5657,68-ARR,68-ARR_v2_9@0,68-ARR_v1_9@2,"To disassociate semantic and syntactic information in PLMs well, we introduce objective functions of learning cross-lingual reconstruction and semantic discrimination together with losses of incorporating word order information and syntax structure information (Part-of-Speech tags and syntax parse trees).","To disassociate semantic and syntactic information well, we introduce objective functions of learning cross-lingual reconstruction and semantic discrimination and losses of incorporating word order information and syntax structure information (Part-of-Speech tags and syntax parse trees), respectively.","Modify,Clarity",Clarity
5708,80-ARR,80-ARR_v2_2@0,80-ARR_v1_2@0,"We are interested in a novel task, singing voice beautification (SVB).","We are interested in a novel task, singing voice beautifying (SVB).","Modify,Grammar",Grammar
5709,80-ARR,80-ARR_v2_5@0,80-ARR_v1_5@0,"The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019;Blaauw and Bonada, 2020;Ren et al., 2020;Liu et al., 2021a) and Singing Voice Conversion (SVC) (Sisman and Li, 2020;Li et al., 2021;Wang et al., 2021a).","The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019;Blaauw and Bonada, 2020;Ren et al., 2020;Zhang et al., 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020;Li et al., 2021;Wang et al., 2021a).","Modify,Fact/Evidence",Fact/Evidence
5710,80-ARR,80-ARR_v2_5@1,80-ARR_v1_5@1,"However, the Singing Voice Beautification (SVB) remains an important and challenging endeavor for researchers.","However, the Singing Voice Beautifying (SVB) remains an important and challenging endeavor for researchers.","Modify,Grammar",Grammar
5711,80-ARR,80-ARR_v2_54@3,80-ARR_v1_53@3,The parallel setting could make sure that the personal vocal timbre will keep still during the beautification process.,The parallel setting could make sure that the personal vocal timbre will keep still during the beautifying process.,"Modify,Grammar",Grammar
5712,80-ARR,80-ARR_v2_54@7,80-ARR_v1_53@7,We randomly choose 6 songs in Chinese and 18 songs in English (from unseen speakers) for validation and test.,We randomly choose 617 pieces in English and 274 pieces in Chinese for validation and test.,"Modify,Fact/Evidence",Fact/Evidence
5713,80-ARR,80-ARR_v2_54@8,80-ARR_v1_53@8,"For subjective evaluations, we choose 60 samples in the test set from different singers, half in Chinese and English.","For subjective evaluations, we choose 60 samples in the test set from different singers, half in English and Chinese.","Modify,Clarity",Clarity
5714,80-ARR,80-ARR_v2_55@1,80-ARR_v1_54@1,"Besides PopBuTFy, we pre-train the ASR model (used for PPG extraction) leveraging the extra speech datasets: AISHELL-3 (Yao Shi et al., 2020) for Chinese and LibriTTS (Zen et al., 2019) for English.","Besides PopBuTFy, we pre-train the ASR model (used for PPG extraction) leveraging the extra speech datasets: AISHELL-3 (Yao Shi, 2020) for Chinese and LibriTTS (Zen et al., 2019) for English.","Modify,Grammar",Grammar
5715,80-ARR,80-ARR_v2_68@6,80-ARR_v1_70@6,We split evaluations for main experiments and ablation studies into several groups for them.,We split evaluations for experiments and ablation studies into several groups for them.,"Modify,Clarity",Clarity
5716,80-ARR,80-ARR_v2_9@0,80-ARR_v1_10@0,"• We propose the first generative model NSVB to solve the SVB task. NSVB not only corrects the pitch of amateur recordings, but also generates the audio with high audio quality and improved vocal tone, to which previous works typically pay little attention. • We propose Shape-Aware Dynamic Time Warping (SADTW) algorithm to synchronize the amateur recording with the template pitch curve, which ameliorates the robustness of the previous time-warping algorithm. • We propose a latent-mapping algorithm to convert the latent variable of the amateur vocal tone to the professional one's, and contribute a new dataset PopBuTFyto train the latent-mapping function. • We design NSVB as a CVAE model, which supports the semi-supervised learning to leverage unpaired, unlabeled singing data for better performance.","• We propose the first generative model NSVB to solve the SVB task. NSVB not only corrects the pitch of amateur recordings, but also generates the audio with high audio quality and improved vocal tone, to which previous works typically pay little attention. • We propose Shape-Aware Dynamic Time Warping (SADTW) algorithm to synchronize the amateur recording with the template pitch curve, which meliorates the robustness of the previous time-warping algorithm. • We propose a latent-mapping algorithm to convert the latent variable of the amateur vocal tone to the professional one's, and contribute a new dataset PopBuTFyto train the latent-mapping function, which will be released upon the paper is published. • We design NSVB as a CVAE model, which supports the semi-supervised learning to leverage unpaired, unlabeled singing data for better performance.","Modify,Clarity",Clarity
5717,80-ARR,80-ARR_v2_12@0,80-ARR_v1_13@0,"Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015;Serrà et al., 2019;Popov et al., 2021;, which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al., 2021).","Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015;Serrà et al., 2019;Popov et al., 2021), which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al., 2021).","Modify,Other",Other
5828,9-ARR,9-ARR_v2_22@3,9-ARR_v1_24@1,"The results on GLUE are shown in Fig. 2, and the results on XTREME show similar patterns.",The results on GLUE are shown in Fig. 1.,"Modify,Fact/Evidence",Fact/Evidence
5829,9-ARR,9-ARR_v2_22@4,9-ARR_v1_24@2,We find that adding global noise with the same distribution to all the PLM parameters will harm the model performance.,We find that adding global noise with same distributions to the PLM parameters will harm the model perfor- mance.,"Modify,Grammar",Grammar
5832,9-ARR,9-ARR_v2_22@8,9-ARR_v1_24@4,"In addition, we find an interesting phenomenon that adding uniform noise is better than Gaussian noise.","In addition, we find an interesting phenomenon that adding uniform noise is better than using Gaussian noise.","Modify,Clarity",Clarity
5833,9-ARR,9-ARR_v2_22@9,9-ARR_v1_24@5,This may be because Gaussian noise has wider ranges and some extreme values may affect the model performance.,This may be because Gaussian noise has wider ranges and some outliers may affect model performance.,"Modify,Clarity",Clarity
5834,9-ARR,9-ARR_v2_22@10,9-ARR_v1_24@6,"Thus, we use matrix-wise uniform noise in NoisyTune.","Thus, we prefer using matrix-wise uniform noise in our NoisyTune method.","Modify,Clarity",Clarity
5835,9-ARR,9-ARR_v2_25@0,9-ARR_v1_25@0,Empirical Analysis of NoisyTune,Analysis on NoiseTune,"Modify,Fact/Evidence",Fact/Evidence
5836,9-ARR,9-ARR_v2_26@0,9-ARR_v1_26@0,"Next, we empirically analyze why NoisyTune can help PLM finetuning.",We then analyze the influence of NoisyTune on finetuning.,"Modify,Clarity",Clarity
5837,9-ARR,9-ARR_v2_26@1,9-ARR_v1_26@1,We compare the accuracy of BERT with and without NoisyTune finetuned with different percentage of samples on the MRPC dataset.,We show the accuracy of the BERT model with or without NoisyTune on the MRPC dataset in Fig. 2.,"Modify,Fact/Evidence",Fact/Evidence
5838,9-ARR,9-ARR_v2_2@6,9-ARR_v1_2@5,Extensive experiments on both GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can consistently empower the finetuning of different PLMs on different downstream tasks.,Extensive experiments on the GLUE English benchmark and the XTREME multilingual benchmark show that NoisyTune can consistently improve the performance of different PLMs in many downstream tasks.,"Modify,Clarity",Clarity
5839,9-ARR,9-ARR_v2_26@3,9-ARR_v1_26@3,"We find NoisyTune can consistently improve PLMs under different amounts of data, especially when less training data is used.",We find NoisyTune can consistently improve PLMs at different finetuning steps.,"Modify,Fact/Evidence",Fact/Evidence
5840,9-ARR,9-ARR_v2_27@0,9-ARR_v1_27@0,"To further study the impact of NoisyTune on PLM finetuning, we show the relative changes of the L 1 -norms of different kinds of parameters in the BERT model during finetuning on the MRPC dataset in Fig. 5.","To further study the impact of NoisyTune on model finetuning, we show the relative changes of the L1-norms of different kinds of paramters in the BERT model during training on MRPC and STS-B in Fig. 3.","Modify,Fact/Evidence",Fact/Evidence
5841,9-ARR,9-ARR_v2_27@1,9-ARR_v1_27@1,"3 Since the noise we added to PLMs in NoisyTune is zero-mean uniform noise, the absolute parameter L 1 -norm will not change too much.","3 Since the noise we added is zeromean, the absolute parameter L1-norms will not be changed too much.","Modify,Clarity",Clarity
5842,9-ARR,9-ARR_v2_27@2,9-ARR_v1_27@2,"However, we can see that the relative change of L 1 -norms becomes smaller when Noisy-Tune is applied, which indicates that the PLMs can find the (sub)optimal parameters for downstream tasks more easily.","However, we can see that the relative change of L1-norms becomes smaller when NoisyTune is applied, which indicates that the model uses smaller paces towards convergence.","Modify,Fact/Evidence",Fact/Evidence
5845,9-ARR,9-ARR_v2_23@0,9-ARR_v1_28@0,Combination with Existing PLM Finetuning Methods,Empower Other Finetuning Methods,"Modify,Fact/Evidence",Fact/Evidence
5846,9-ARR,9-ARR_v2_24@2,9-ARR_v1_29@0,"In this section, we explore whether NoisyTune has the potential to empower the existing PLM finetuning techniques to achieve better performance.",Our NoisyTune method also has the potential to empower other PLM finetuning techniques.,"Modify,Clarity",Clarity
5847,9-ARR,9-ARR_v2_24@3,9-ARR_v1_29@1,"Here we select two well-known PLM finetuning for experiments, i.e., RecAdam and Mixout (Lee et al., 2020).","We compare the performance of the original RecAdam and Mixout (Lee et al., 2020) method and their variants combined with NoisyTune.","Modify,Fact/Evidence",Fact/Evidence
5848,9-ARR,9-ARR_v2_24@4,9-ARR_v1_29@2,The experimental results are summarized in Fig. 3.,The results are shown in Fig. 4.,"Modify,Clarity",Clarity
5849,9-ARR,9-ARR_v2_24@5,9-ARR_v1_29@3,We find that combining NoisyTune with existing PLM finetuning techniques can further improve their performance.,We find that combining NoisyTune with existing PLM finetuning techniques can further improve the performance.,"Modify,Grammar",Grammar
5852,9-ARR,9-ARR_v2_31@0,9-ARR_v1_31@0,"In this paper, we propose a very simple but effective method named NoisyTune, which can help better finetune PLMs on downstream tasks by adding a little noise to them before finetuning.","In this paper, we propose a very simple but effective method named NoisyTune, which adds a little noise to PLMs before finetuning for better transferability from pretraining tasks to downstream tasks.","Modify,Clarity",Clarity
5853,9-ARR,9-ARR_v2_31@1,9-ARR_v1_31@1,"In NoisyTune, we propose a matrix-wise perturbing method that adds noise with different intensities to different kinds of parameter matrices in PLMs according to their variances.","In NoisyTune, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of different parameter matrices in PLMs, which can consider the varied characteristics of different types of parameters.","Modify,Clarity",Clarity
5854,9-ARR,9-ARR_v2_31@3,9-ARR_v1_31@2,Extensive experiments on both monolingual GLUE benchmark and multilingual XTREME benchmark demonstrate NoisyTune can consistently empower the finetuning of different PLMs on various downstream tasks to achieve better performance.,Extensive experiments on the monolingual GLUE benchmark and the multilingual XTREME benchmark demonstrate the NoisyTune can consistently improve the performance of different PLMs in various downstream tasks.,"Modify,Clarity",Clarity
5855,9-ARR,9-ARR_v2_4@1,9-ARR_v1_4@1,"Many PLMs such as BERT (Devlin et al., 2019), RoBERTa and UniLM (Dong et al., 2019) which are pretrained from large-scale unlabeled corpus in a selfsupervised way, have significantly improve various downstream tasks such as reading comprehension , machine translation (Brown et al., 2020), text classification (Bao et al., 2020), dialog (Wu et al., 2020) and recommendation by finetuning on these tasks.","Many PLMs such as BERT (Devlin et al., 2019) and RoBERTa have played critical roles in various applications, such as reading comprehension, machine translation and text classification (Dong et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
5856,9-ARR,9-ARR_v2_5@0,9-ARR_v1_5@0,"How to effectively finetune PLMs to better empower downstream tasks is an important research problem (Zheng et al., 2021).","How to effectively finetune PLMs to better empower downstream tasks is an important research problem (Zheng et al., 2021a).","Modify,Fact/Evidence",Fact/Evidence
5857,9-ARR,9-ARR_v2_5@3,9-ARR_v1_5@1,"Only a few works explore more effective and robust PLM finetuning methods Lee et al., 2020;Aghajanyan et al., 2021;Xu et al., 2021).","Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods Jiang et al., 2020;Lee et al., 2020;Aghajanyan et al., 2021;Xu et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
5858,9-ARR,9-ARR_v2_5@5,9-ARR_v1_5@3,Lee et al. (2020) proposed Mixout which randomly replaces part of the parameters in the finetuned model with their original weights in the PLMs.,Lee et al. (2020) proposed a Mixout method to randomly replace parts of model parameters with their original pretrained weights.,"Modify,Clarity",Clarity
5859,9-ARR,9-ARR_v2_5@6,9-ARR_v1_5@4,These PLM finetuning methods mainly focus on preventing PLMs from overfitting the limited labeled data in downstream tasks.,These finetuning methods mainly focus on preventing PLMs from overfitting limited labeled data in downstream tasks.,"Modify,Clarity",Clarity
5860,9-ARR,9-ARR_v2_5@8,9-ARR_v1_5@5,"It is not easy for existing PLM finetuning methods to overcome such gap (Roberts et al., 2020), which may lead to suboptimal performance especially when labeled data in downstream tasks is insufficient.","However, PLMs have been well trained in the self-supervised pretraining tasks, and it can be difficult for them to overcome the barrier between pretraining and downstream tasks as well as the gaps of their domains during finetuning (Roberts et al., 2020), which may lead to a suboptimal performance especially when labeled data in downstream tasks is insufficient.","Modify,Fact/Evidence",Fact/Evidence
5861,9-ARR,9-ARR_v2_2@1,9-ARR_v1_2@1,"However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks.","However, PLMs may have risks in overfitting pretraining signals, and there are some gaps between downstream tasks and the pretraining tasks.","Modify,Clarity",Clarity
5862,9-ARR,9-ARR_v2_6@0,9-ARR_v1_6@0,"In order to handle this problem, in this paper we propose a very simple yet effective method named NoisyTune, which can help better finetune PLMs for downstream tasks.","In this paper, we propose a very simple yet effective method named NoisyTune, which can help better finetune PLMs for downstream tasks.","Modify,Clarity",Clarity
5863,9-ARR,9-ARR_v2_6@2,9-ARR_v1_6@1,"It can help prevent PLMs from overfitting the tasks and data in the pretraining stage, and reduce the gap between pretraining and downstream tasks.","The key idea of NoisyTune is to add a little noise to perturb PLMs parameters before finetuning, which can help prevent them from overfitting the signals in the pretraining tasks, and reduce the gap between pretraining and downstreaming tasks.","Modify,Claim",Claim
5864,9-ARR,9-ARR_v2_6@3,9-ARR_v1_6@2,"Since PLMs have different types of parameters which usually own different characteristics, in NoisyTune we use a matrix-wise perturbing method that adds uniform noise with different intensities to different parameter matrices according to their standard deviations for better adaptation.","Since different types of parameters in PLMs may have different characteristics, we propose a matrix-wise perturbing method that adds uniform noise with different intensities according to the standard deviations of different parameter matrices for better adaptation.","Modify,Clarity",Clarity
5865,9-ARR,9-ARR_v2_6@4,9-ARR_v1_6@3,"We conduct extensive experiments on two widely used NLP benchmarks, namely, GLUE (Wang et al., 2018) for English language understanding and XTREME (Hu et al., 2020) for multilingual language understanding.","We conduct experiments on two widely used NLP benchmarks, i.e., GLUE (Wang et al., 2018) for English language understanding and XTREME (Hu et al., 2020) for multilingual language understanding.","Modify,Clarity",Clarity
5866,9-ARR,9-ARR_v2_6@5,9-ARR_v1_6@4,The results show NoisyTune can empower the finetuning of different PLMs on many different downstream NLP tasks to consistently achieve better performance.,The results show that NoisyTune can consistently boost the performance of different PLMs in many downstream NLP tasks.,"Modify,Fact/Evidence",Fact/Evidence
5867,9-ARR,9-ARR_v2_8@2,9-ARR_v1_8@1,"It may be difficult for PLMs to effectively adapt to downstream tasks especially when labeled data in these tasks are limited, which is usually the case.","Since the parameters of PLMs are well tuned in the pretraining tasks and may overfit self-supervision signals, it may be difficult for them to adapt to downstream tasks especially when labeled data in downstream tasks are rather limited.","Modify,Claim",Claim
5868,9-ARR,9-ARR_v2_2@2,9-ARR_v1_2@2,Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance.,"It can be difficult for vanilla finetuning methods to overcome the barrier between pretraining and downstream tasks, which leads to suboptimal performance.","Modify,Clarity",Clarity
5869,9-ARR,9-ARR_v2_8@3,9-ARR_v1_8@2,"Motivated by the dueling bandits mechanism (Yue and Joachims, 2009) that adds randomness to the model for exploration, as shown in Fig. 1, we propose to add some noise to the parameters of PLMs before finetuning them on downstream tasks to do some ""exploration"" in parameter space and reduce the risk of overfitting the pretraining tasks and data.","Motivated by the dueling bandits mechanism (Yue and Joachims, 2009) that adds randomness to the model for exploration, we explore adding noise to PLMs before finetuning to ""explore"" other parameter spaces to reduce the problem of overfitting pretraining tasks.","Modify,Fact/Evidence",Fact/Evidence
5870,9-ARR,9-ARR_v2_9@1,9-ARR_v1_10@2,Different parameter matrices in the PLMs usually have different characteristics and scales.,"However, different parameter matrices in the PLM have very different characteristics.","Modify,Clarity",Clarity
5871,9-ARR,9-ARR_v2_9@2,9-ARR_v1_10@3,"For example, some researchers found that the self-attention parameters and the feed-forward network parameters in Transformers have very different properties, such as rank and density .","For example, the self-attention parameters and the feed-forward network parameters usually have very different properties .","Modify,Claim",Claim
5872,9-ARR,9-ARR_v2_9@3,9-ARR_v1_10@4,"Thus, adding unified noise to all parameter matrices in PLMs may not be optimal for keeping their good model utility.","Thus, adding global noise may not be optimal for keeping good model utility.","Modify,Clarity",Clarity
5873,9-ARR,9-ARR_v2_9@4,9-ARR_v1_10@5,"To handle this challenge, we propose a matrix-wise perturbing method that adds noise with different intensities to different parameter matrices according to their variances.","To solve this challenge, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of parameter matrices.","Modify,Clarity",Clarity
5874,9-ARR,9-ARR_v2_11@1,9-ARR_v1_10@6,"Denote the perturbed version of the parameter matrix W i as Wi , which is computed as follows:","We denote the perturbed version of the parameter matrix W i as Wi , which is computed as follows:","Modify,Clarity",Clarity
5875,9-ARR,9-ARR_v2_2@3,9-ARR_v1_2@3,"In this paper, we propose a very simple yet effective method named NoisyTune to help better finetune PLMs on downstream tasks by adding some noise to the parameters of PLMs before finetuning.","In this paper, we propose a very simple yet effective method named NoisyTune which can help better finetune PLMs in downstream tasks by adding some noise to the parameters of PLMs before finetuning.","Modify,Clarity",Clarity
5876,9-ARR,9-ARR_v2_14@0,9-ARR_v1_12@2,"In addition, in some PLMs there are some constant matrices, such as token type embeddings in RoBERTa .","In addition, in some PLMs there exist constant matrices, such as token type embeddings in RoBERTa .","Modify,Clarity",Clarity
5877,9-ARR,9-ARR_v2_14@2,9-ARR_v1_12@4,It can ensure that these constant matrices will not be accidentally activated by additional noise.,This will ensure that these constant matrices will not be accidentally activated by additional noise.,"Modify,Clarity",Clarity
5878,9-ARR,9-ARR_v2_17@1,9-ARR_v1_15@1,"The first one is GLUE (Wang et al., 2018), which is a benchmark for English language understanding that contains different tasks like natural language inference, sentiment analysis and sentence similarity evaluation.","The first one is GLUE (Wang et al., 2018), which is a benchmark for English language understanding that contains tasks like natural language inference, sentiment analysis and sentence similarity evaluation.","Modify,Clarity",Clarity
5879,9-ARR,9-ARR_v2_17@6,9-ARR_v1_17@2,"In order not to harm the alignment of token embeddings across different languages, we do not add noise to the token embeddings in multilingual PLMs.","In order not to harm the alignment of token embeddings across different languages, We do not add noise to the token embeddings in multilingual PLMs.","Modify,Grammar",Grammar
5882,9-ARR,9-ARR_v2_19@0,9-ARR_v1_19@0,"On the GLUE benchmark, we compare the performance of directly finetuning the base version of BERT (Devlin et al., 2019), XLNET , RoBERTa and ELEC-TRA (Clark et al., 2020) with that of finetuning them after applying NoisyTune.","On the GLUE benchmark, we compare directly finetuning the base version of BERT (Devlin et al., 2019), XLNET , RoBERTa and ELECTRA (Clark et al., 2020) as well as finetuning them after applying NoisyTune.","Modify,Clarity",Clarity
5883,9-ARR,9-ARR_v2_19@1,9-ARR_v1_20@0,"On the XTREME benchmark, we compare the performance of directly finetuning both base and large versions of XLM-R (Conneau et al., 2020) with that of their variants obtained by applying NoisyTune.","On the XTREME benchmark, we compare both base and large versions of XLM-R (Conneau et al., 2020) and their variants processed by NoisyTune.","Modify,Clarity",Clarity
5886,9-ARR,9-ARR_v2_20@0,9-ARR_v1_21@2,"According to these results, NoisyTune can consistently improve the performance of different PLMs on different tasks in both English and multilingual settings.","From the results, we can see that NoisyTune can consistently improve the performance of different PLMs on different tasks.","Modify,Clarity",Clarity
5887,9-ARR,9-ARR_v2_20@1,9-ARR_v1_21@3,"In addition, the performance improvement brought by NoisyTune is usually larger on relatively small datasets (e.g., RTE, CoLA and WNLI).","In addition, the performance improvement on relatively small datasets is usually larger (e.g., RTE, CoLA and WNLI).","Modify,Clarity",Clarity
5888,9-ARR,9-ARR_v2_20@2,9-ARR_v1_21@4,"These results indicate that when labeled data in downstream tasks is insufficient, it is quite difficult to effectively finetune PLMs starting from the original parameters which usually overfit the pretraining tasks and data.","This indicates that when labeled training data in downstream tasks is not redundant, it may be more difficult to well adapt PLMs to downstream tasks from the parameter space well tuned in pretraining tasks.","Modify,Claim",Claim
5889,9-ARR,9-ARR_v2_20@3,9-ARR_v1_21@5,"The experimental results validate that NoisyTune can properly perturb PLMs with a little noise to explore different parameter spaces and reduce the overfitting problem, making PLMs easier to be adapted to downstream tasks.","Thus, properly perturbing PLMs with noise can explore different parameter spaces and meanwhile keep useful knowledge encoded in pretraining tasks.","Modify,Claim",Claim
5890,9-ARR,9-ARR_v2_21@0,9-ARR_v1_22@0,Which Noise to Use and How?,Influence of Noise Type,"Modify,Clarity",Clarity
5891,9-ARR,9-ARR_v2_22@0,9-ARR_v1_23@0,In this section we study which kind of noise is more suitable for NoisyTune.,"Next, we study the influence of using different kinds of noise on NoisyTune.","Modify,Clarity",Clarity
5920,90-ARR,90-ARR_v2_4@1,90-ARR_v1_4@1,"However, dialogue can also be viewed as a sequential decision making process, which is well-suited to planning and reinforcement learning (RL) algorithms.","However, dialogue can also be viewed as a sequential decision making process which is well-suited to planning and reinforcement learning (RL) algorithms.","Modify,Grammar",Grammar
5921,90-ARR,90-ARR_v2_30@1,90-ARR_v1_33@1,"In the case of the information retrieval task that we consider in this paper, these high-level actions correspond to deciding which database entity to retrieve for the user (e.g., suggesting a flight to the customer that meets all of their needs).","In the case of the information retrieval task that we consider in this paper, these high-level actions correspond to deciding which database entity to retrieve for the user (e.g. suggesting a flight to the customer that meets all of their needs).","Modify,Grammar",Grammar
5925,90-ARR,90-ARR_v2_59@1,90-ARR_v1_62@1,"In terms of task success, CALM outperforms the prior SOTA (AirConcierge) by approximately 7%, achieving 88% task success when using greedy decoding from the language model (see Table 1).","In terms of task success, CALM outperforms the prior SOTA (AirConcierge) by approximately 7%, achieving 88% task success when using greedy decoding from the language model.","Modify,Fact/Evidence",Fact/Evidence
5926,90-ARR,90-ARR_v2_59@2,90-ARR_v1_62@2,"Compared with AirConcierge, where all reasoning about the task context is done outside of the language model, CALM does all of the filtering, selecting, and responding with relevant flight table entries within the language model, in a fully end-to-end manner.","Compared with AirConcierge, where all reasoning about the task context is done outside of the language model, CALM does all of the filtering, selecting, and responding with relevant flight table entries within the language model, in a fully endto-end manner.","Modify,Grammar",Grammar
5927,90-ARR,90-ARR_v2_76@3,90-ARR_v1_77@5,We release the code and model weights for our customer bot at https://sea-snell.github.io/CALM_LM_site/.,"To promote fair evaluations in future works that use the AirDialogue dataset, we will release the code and model weights for our customer bot upon acceptance.","Modify,Fact/Evidence",Fact/Evidence
5928,90-ARR,90-ARR_v2_79@0,90-ARR_v1_80@0,"2. We then execute this predicted information against the agent's flight flag, to produce a set of valid final actions.","2. We then execute this predicted information against the agent's flight table and reservation flag, to produce a set of valid final actions.","Modify,Clarity",Clarity
5929,90-ARR,90-ARR_v2_6@7,90-ARR_v1_7@1,"CALM unifies the traditional language modeling objective with task-specific supervision, where a language model is interpreted as a joint representation of dynamics and policies in an MDP, and the finetuning process utilizes a conditional imitation learning objective with a novel task relabeling strategy that teaches the model how to generate high-utility dialogues (see Figures 1 and 2).","CALM unifies the traditional language modeling objective with task-specific supervision, where a language model is interpreted as a joint representation of dynamics and policies in an MDP, and the finetuning process utilizes a conditional imitation learning objective with a novel task relabeling strategy that teaches the model how to generate high-utility dialogues.","Modify,Fact/Evidence",Fact/Evidence
5930,90-ARR,90-ARR_v2_9@3,90-ARR_v1_12@5,"When fine-tuning on this relabeled dataset, we then apply a Task Specific Auxiliary Loss on top of the standard language modeling objective; this helps the model learn to use the task context.","When fine-tuning on this relabeled dataset, we then apply a Task Specific Auxilary Loss on top of the standard language modeling objective; this helps the model learn to use the task context.","Modify,Grammar",Grammar
5931,90-ARR,90-ARR_v2_9@4,90-ARR_v1_12@6,"Once trained, CALM can consistently solve goal-directed dialogue tasks.","Once trained, CALM can consistently solve complex tasks in dialogue.","Modify,Clarity",Clarity
6194,2-131,2-131_v2_35@1,2-131_v1_30@1,The thickness measurements before and after correction were not statistically significant (P<.05) for any of the subfields and also when stratified by diagnosis.,The thickness measurements before and after correction were not statistically significant (p<0.05) for any of the subfields and also when stratified by diagnosis.,"Modify,Grammar",Grammar
6195,2-131,2-131_v2_9@0,2-131_v1_4@0,Results: Spectralis had the highest number of images needing manual correction.,Results: The Spectralis device had the highest number of images needing manual correction.,"Modify,Clarity",Clarity
6196,2-131,2-131_v2_37@3,2-131_v1_32@3,77% of the differences were less than 48μm and 50% were less than 10μm.,77% of the differences were less than 48 μm and 50% were less than 10 μm.,"Modify,Grammar",Grammar
6197,2-131,2-131_v2_41@0,2-131_v1_36@0,"The mean (±SD) of the macular thickness of all of the subfields, including the central 1mm subfield (FTH) for Stratus™, Cirrus™, and Spectralis™ before and after manual correction of scans, stratified by diagnosis of NV-AMD and NNV-AMD, is shown in Table 1 .","The mean (+SD) of the macular thickness of all of the subfields, including the central 1 mm subfield (foveal thickness; FTH) for Stratus™, Cirrus™, and Spectralis™ devices before and after manual correction of scans, stratified by diagnosis of NV-AMD and NNV-AMD, is shown in Table 1 .","Modify,Fact/Evidence",Fact/Evidence
6198,2-131,2-131_v2_41@1,2-131_v1_36@1,"For NV-AMD, the FTH values for central 1mm were 375µm (±129µm), 253µm (±74µm), 312µm (±110µm) for Spectralis™, Stratus™, and Cirrus™ respectively.","For NV-AMD, the FTH values for the central 1 mm were 375 µm (+129 µm), 253 µm (+74 µm), 312 µm (+110 µm) for Spectralis™, Stratus™, and Cirrus™ respectively.","Modify,Fact/Evidence",Fact/Evidence
6199,2-131,2-131_v2_41@2,2-131_v1_36@2,"After correction, the values were 335µm (±106µm) for Spectralis™ and 318µm (±110µm) for Cirrus™.","After correction, the values were 335 µm (+106 µm) for Spectralis™ and 318 µm (+110 µm) for Cirrus™.","Modify,Fact/Evidence",Fact/Evidence
6200,2-131,2-131_v2_41@3,2-131_v1_36@3,"On the other hand, the FTH values for NNV-AND in the central 1mm before correction were 298µm (87µm), 193µm (±32µm), and 229µm (±30µm) for Spectralis™, Stratus™, and Cirrus™ respectively.","On the other hand, the FTH values for NNV-AND in the central 1 mm before correction were 298 µm (+87 µm), 193 µm (+32 µm), and 229 µm (+30 µm) for Spectralis™, Stratus™, and Cirrus™ respectively.","Modify,Fact/Evidence",Fact/Evidence
6201,2-131,2-131_v2_41@4,2-131_v1_36@4,Spectralis™ was the only device to have a different FTH value of 248µm (±56µm) after correction.,Spectralis™ was the only device to have a different FTH value (248 µm +56 µm) after correction.,"Modify,Fact/Evidence",Fact/Evidence
6202,2-131,2-131_v2_41@6,2-131_v1_36@6,"The retinal thickness measurements obtained via the Cirrus™ were slightly less (range: 230 to 320µm), while Stratus™ had the lowest values, ranging from 190 to 270µm.","The retinal thickness measurements obtained via Cirrus™ were slightly less (range: 230 to 320 µm), while Stratus™ had the lowest values, ranging from 190 to 270 µm.","Modify,Grammar",Grammar
6203,2-131,2-131_v2_41@7,2-131_v1_36@7,There were no significant (p<.05) differences between the mean FTH of the first and second scans for each of the three devices.,There were no significant (p<0.05) differences between the mean FTH of the first and second scans for each of the three devices.,"Modify,Grammar",Grammar
6204,2-131,2-131_v2_9@2,2-131_v1_4@3,The CORs were lowest for Spectralis TM and similar and higher for Cirrus TM and Stratus TM .,The CORs were lowest for SpectralisTM and similar with each other and had higher values for CirrusTM and StratusTM.,"Modify,Clarity",Clarity
6205,2-131,2-131_v2_44@7,2-131_v1_39@7,"In the central subfield, Spectralis™ had a COR of 20µm NV-AMD which increased to 23µm; both Cirrus™ and Stratus™ had relatively larger CORs of 64µm (reduced to 49µm after correction) and 35µm, respectively.","In the central subfield, Spectralis™ had a COR of 20 µm NV-AMD which increased to 23 µm; both Cirrus™ and Stratus™ had relatively larger CORs of 64 µm (reduced to 49 µm after correction) and 35 µm, respectively.","Modify,Grammar",Grammar
6206,2-131,2-131_v2_44@8,2-131_v1_39@8,"For NNV-AMD, the COR for the central subfield was 15µm for both Cirrus™ and Spectralis™, and was 24µm for Stratus™.","For NNV-AMD, the COR for the central subfield was 15 µm for both Cirrus™ and Spectralis™, and was 24 µm for Stratus™.","Modify,Grammar",Grammar
6207,2-131,2-131_v2_44@9,2-131_v1_39@9,"After correction, the value decreased for Spectralis™ to 12µm and increased to 36µm for Cirrus™.","After correction, the value decreased for Spectralis™ to 12 µm and increased to 36 µm for Cirrus™.","Modify,Grammar",Grammar
6208,2-131,2-131_v2_12@2,2-131_v1_7@2,"One example is Age-related Macular Degeneration (AMD), a progressive, blinding disease that is mostly non-neovascular (NNV-AMD) but can be associated with choroidal neovascularization (NV-AMD).","One example is Age-related Macular Degeneration (AMD), a progressive, blinding disease that is mostly Non-Neovascular (NNV-AMD) but can be associated with choroidal Neovascularization (NV-AMD).","Modify,Grammar",Grammar
6209,2-131,2-131_v2_12@3,2-131_v1_7@3,"Currently, OCT is also being employed as an outcome measure in many multicenter clinical trials of AMD with Time Domain OCT (TD-OCT) device being the most common <REF-1> , <REF-2> .","Currently, OCT is also being employed as an outcome measure in many multicenter clinical trials of AMD with Time Domain OCT (TD-OCT) devices being the most common <REF-1> , <REF-2> .","Modify,Grammar",Grammar
6210,2-131,2-131_v2_2@0,2-131_v1_2@0,"Purpose: To evaluate the reproducibility and reliability of optical coherence tomography scans obtained using the time domain (TD-OCT) Stratus TM OCT, and the Spectral Domain (SD-OCT) Spectralis TM and Cirrus TM OCT devices before and after manual correction in eyes with either Neovascular (NV-AMD) or Non-Neovascular (NNV-AMD) age-related macular degeneration.","Purpose: To evaluate the reproducibility and reliability of Optical Coherence Tomography scans (OCT) obtained using the Time Domain (TD-OCT) StratusTM OCT, and the Spectral Domain (SD-OCT) SpectralisTM and CirrusTM OCT devices before and after manual correction in eyes with either Neovascular (NV-AMD) or Non-Neovascular (NNV-AMD) Age-related Macular Degeneration.","Modify,Clarity",Clarity
6211,2-131,2-131_v2_13@0,2-131_v1_8@0,"As this technology is increasingly being utilized by many ophthalmologists to evaluate and monitor patients and guide treatment decisions <REF-2> , it is important to understand the reliability and accuracy of thickness measurements obtained with various devices currently available.","As this technology is increasingly being utilized by many ophthalmologists to evaluate and monitor patients and guide treatment decisions <REF-2> , it is important to understand the reliability and accuracy of thickness measurements obtained with the various devices currently available.","Modify,Grammar",Grammar
6212,2-131,2-131_v2_49@2,2-131_v1_44@2,"The COR for Cirrus™ increased by 15–40µm after correction for NNV-AMD. Also, Cirrus™ COR values were 10–30µm higher than Stratus™ values for both NV-AMD and NNV-AMD.","The COR for Cirrus™ increased by 15–40 µm after correction for NNV-AMD. Also, Cirrus™ COR values were 10–30 µm higher than Stratus™ values for both NV-AMD and NNV-AMD.","Modify,Grammar",Grammar
6213,2-131,2-131_v2_13@1,2-131_v1_8@1,"Recently, studies have shown that in patients with AMD, there is a high frequency of errors in automated retinal thickness measurements due to incorrect segmentation of the retina in the TD-OCT machine specifically in NV-AMD <REF-2> , <REF-3> .","Recently, studies have shown that in patients with AMD, there is a high frequency of errors in automated retinal thickness measurements due to incorrect segmentation of the retina in the TD-OCT machine, specifically in NV-AMD <REF-2> , <REF-3> .","Modify,Grammar",Grammar
6214,2-131,2-131_v2_13@2,2-131_v1_8@2,"Using an Spectral Domain OCT (SD-OCT) device Menke et al. found that NNV-AMD had fewer errors than NV-AMD, mostly due to the pathology of the disease resulting in retinal pigment epithelial (RPE) layer changes <REF-4> .","Using a Spectral Domain OCT (SD-OCT) device Menke et al. found that retinal thickness measurements in NNV-AMD cases had fewer errors than in NV-AMD cases, mostly due to the pathology of the former disease resulting in retinal pigment epithelial (RPE) layer changes <REF-4> .","Modify,Clarity",Clarity
6215,2-131,2-131_v2_14@0,2-131_v1_9@0,"Manual correction of the algorithm is an option in newer generations of the review software and as more OCT devices are coming to the market, it is important to understand the clinical importance of manual correction of OCT algorithms and the agreement of thickness measurements from different machines before and after correction.","Manual correction of the OCT algorithm is an option in newer generations of the OCT review software and as more devices are coming to the market, it is important to understand the clinical importance of manual correction of OCT algorithms and the agreement of thickness measurements from different machines before and after correction.","Modify,Clarity",Clarity
6216,2-131,2-131_v2_54@0,2-131_v1_49@0,Figure 2a–f show Bland-Altman plots with 95% confidence intervals for the FTH comparison of the machines before and after correction.,Figure 2A–F show Bland-Altman plots with 95% confidence intervals for the FTH comparison of the machines before and after correction.,"Modify,Grammar",Grammar
6217,2-131,2-131_v2_54@1,2-131_v1_49@1,"Before correction, the mean difference between the machines was 32µm for Spectralis™ vs. Cirrus™, 52µm for Cirrus™ vs. Stratus™, and 84µm for Spectralis™ vs. Stratus™.","Before correction, the mean difference between the machines was 32 µm for Spectralis™ vs. Cirrus™, 52 µm for Cirrus™ vs. Stratus™, and 84 µm for Spectralis™ vs. Stratus™.","Modify,Grammar",Grammar
6218,2-131,2-131_v2_54@2,2-131_v1_49@2,"Manual correction reduced the differences, with it being 15µm for Spectralis™ vs. Cirrus™, 51µm for Cirrus™ vs. Stratus™, and 67µm for Spectralis™ vs. Stratus™.","Manual correction reduced the differences, with it being 15 µm for Spectralis™ vs. Cirrus™, 51 µm for Cirrus™ vs. Stratus™, and 67 µm for Spectralis™ vs. Stratus™.","Modify,Grammar",Grammar
6219,2-131,2-131_v2_54@3,2-131_v1_49@3,"When stratified by diagnoses, the values were 34µm and 29µm for Spectralis™ vs. Cirrus™, 53µm and 47µm for Cirrus™ vs. Stratus™, and 88µm and 79µm for Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD before correction, respectively.","When stratified by diagnoses, the values were 34 µm and 29 µm for Spectralis™ vs. Cirrus™, 53 µm and 47 µm for Cirrus™ vs. Stratus™, and 88 µm and 79 µm for Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD respectively, before correction.","Modify,Clarity",Clarity
6220,2-131,2-131_v2_54@4,2-131_v1_49@4,"After manual correction, the values reduced to 17µm and 14µm Spectralis™ vs. Cirrus™ and 70µm and 61µm Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD, respectively.","After manual correction, the values reduced to 17 µm and 14 µm Spectralis™ vs. Cirrus™ and 70 µm and 61 µm Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD, respectively.","Modify,Grammar",Grammar
6221,2-131,2-131_v2_54@5,2-131_v1_49@5,"The confidence interval widths, on average, were 5–10µm smaller than between an SD-OCT and TD-OCT machine.","The confidence interval widths, on average, were 5–10 µm smaller than when comparing between an SD-OCT and a TD-OCT machine.","Modify,Clarity",Clarity
6222,2-131,2-131_v2_58@2,2-131_v1_53@2,One such common and clinically relevant issue is the presence of a random error in the identification of the inner and outer boundaries of the retina by the algorithm.,One such common and clinically relevant issue is the presence of random errors in the identification of the inner and outer boundaries of the retina by the OCT algorithm.,"Modify,Clarity",Clarity
6223,2-131,2-131_v2_59@0,2-131_v1_54@0,Reasons for differences in our error rates compared to previous include a lack of standard definition of an algorithm error.,Reasons for differences in our error rates compared to previous studies include the lack of standard definition of an algorithm error.,"Modify,Clarity",Clarity
6224,2-131,2-131_v2_59@3,2-131_v1_54@3,"These differences may be due to the fact that our study was prospective and while acquiring scans, the operators tried their best to ensure no errors occurred during scan acquisition.","These differences may be due to the fact that our study was prospective and, while acquiring scans, the operators tried their best to ensure no errors occurred during scan acquisition.","Modify,Grammar",Grammar
6225,2-131,2-131_v2_60@2,2-131_v1_55@2,"For example, more than 50% of the Spectralis™ scans resulted in a 10µm or less change in the central subfield thickness.","For example, most of the Spectralis™ scans resulted in a 10 µm or less change in the central subfield thickness.","Modify,Fact/Evidence",Fact/Evidence
6226,2-131,2-131_v2_62@1,2-131_v1_55@4,"Correction reduced the difference of the thickness measurements between the two SD-OCT devices to less than 20um; in some cases as noted above, the difference was no longer statistically significant.","Correction reduced the difference of the thickness measurements between the two SD-OCT devices to less than 20 µm; in some cases as noted above, the difference was no longer statistically significant.","Modify,Grammar",Grammar
6227,2-131,2-131_v2_62@2,2-131_v1_55@5,"Other studies in normal and pathologic eyes including DME and macular degeneration have also demonstrated that the difference in retinal thickness between the SD machines can be attributed to the differences in segmentation of the automated algorithms <REF-7> , <REF-10> , <REF-12> .","Other studies in normal and pathologic eyes including Diabetic Macular Edema (DME) and macular degeneration have also demonstrated that the difference in retinal thickness between the SD machines can be attributed to the differences in segmentation of the automated algorithms <REF-7> , <REF-10> , <REF-11> .","Modify,Clarity",Clarity
6228,2-131,2-131_v2_16@1,2-131_v1_11@1,Informed consent was obtained from study subjects.,Written Informed consent was obtained from study subjects.,"Modify,Fact/Evidence",Fact/Evidence
6229,2-131,2-131_v2_63@3,2-131_v1_56@3,"The disease difference can be attributed to the pathology of NV-AMD disrupting the outer border, which makes it difficult for the automated algorithm to accurately segment the retinal layers <REF-13> , <REF-14> .","This difference between diseases can be attributed to the pathology of NV-AMD disrupting the outer border, which makes it difficult for the automated algorithm to accurately segment the retinal layers <REF-12> , <REF-13> .","Modify,Clarity",Clarity
6230,2-131,2-131_v2_63@7,2-131_v1_56@5,"Previous studies on normal eyes have reported a high repeatability of measurements with Spectralis™, with differences between repeated measurements being within 1µm <REF-12> , <REF-16> .","Previous studies on normal eyes have reported a high repeatability of measurements with Spectralis™, with differences between repeated measurements being within 1 µm <REF-11> , <REF-14> .","Modify,Grammar",Grammar
6231,2-131,2-131_v2_63@8,2-131_v1_56@6,"For Stratus™ OCT images, other studies have found central subfield repeatability values in patients with NV-AMD to be 50µm and 32–35µm for NNV-AMD patients after correction/exclusion of scans with errors <REF-8> , <REF-17> ; our study confirms this finding.","For Stratus™ OCT images, other studies have found central subfield repeatability values in patients with NV-AMD to be 50 µm and 32–35 µm for NNV-AMD patients after correction/exclusion of scans with errors <REF-8> , <REF-15> ; our study confirms this finding.","Modify,Grammar",Grammar
6232,2-131,2-131_v2_63@9,2-131_v1_56@7,"There has been one other published study looking at the repeatability of Cirrus™ OCT in NV-AMD, which found a central subfield repeatability value of 42um before correction and 26µm after exclusion of scans with significant segmentation errors <REF-18> .","There has been one other published study looking at the repeatability of Cirrus™ OCT in NV-AMD, which found a central subfield repeatability value of 42 µm before correction and 27 µm after exclusion <REF-16> .","Modify,Fact/Evidence",Fact/Evidence
6233,2-131,2-131_v2_18@0,2-131_v1_13@0,Patients with confirmed diagnosis of AMD were enrolled in the study.,Patients with a confirmed diagnosis of AMD were enrolled in the study.,"Modify,Grammar",Grammar
6234,2-131,2-131_v2_64@1,2-131_v1_57@1,"For NV-AMD, Cirrus had higher coefficients after correction, and for NNV-AMD, Cirrus™ had lower coefficients as compared to Stratus™.","For NV-AMD, Cirrus™ had higher coefficients after correction, and for NNV-AMD, Cirrus™ had lower coefficients compared to Stratus™.","Modify,Clarity",Clarity
6235,2-131,2-131_v2_65@2,2-131_v1_58@2,We found that 95% confidence intervals were narrower as compared to an SD-OCT and TD-OCT and correcting the algorithm errors further narrowed the intervals.,We found that 95% confidence intervals were narrower compared to an SD-OCT and TD-OCT and correcting the algorithm errors further narrowed the intervals.,"Modify,Clarity",Clarity
6236,2-131,2-131_v2_18@2,2-131_v1_13@2,Patients under treatment with intravitreal injections of anti-vascular endothelial growth factor (VEGF) agents were also allowed to participate in the study.,Patients under treatment with intravitreal injections of anti-Vascular Endothelial Growth Factor (VEGF) agents were also allowed to participate in the study.,"Modify,Grammar",Grammar
6237,2-131,2-131_v2_65@4,2-131_v1_58@4,This is mostly likely due to the effects of manually correcting the Spectralis™ images and that both machines have similar scanning technologies.,This is mostly likely due to the effects of manually correcting the Spectralis™ images and the fact that both machines have similar scanning technologies.,"Modify,Clarity",Clarity
6238,2-131,2-131_v2_65@6,2-131_v1_58@6,"Jaffe et al. reported similar results looking at NV-AMD, with limits of agreements being approximately 225um between a SD-OCT and TD-OCT <REF-7> .","Jaffe et al. reported similar results looking at NV-AMD, with limits of agreements being approximately 225 µm between a SD-OCT and TD-OCT <REF-7> .","Modify,Grammar",Grammar
6239,2-131,2-131_v2_65@7,2-131_v1_58@7,The poor agreement warrants caution for clinicians when trying to use the data from different machines interchangeably especially in the central 1mm of retina since most clinicians.,The poor agreement suggests that clinicians should exercise caution when trying to use the data from different machines interchangeably.,"Modify,Fact/Evidence",Fact/Evidence
6240,2-131,2-131_v2_66@2,2-131_v1_59@1,The version of software used for the Stratus™ images did not allow correction of segmentation errors and therefore these images had to be excluded from the analysis.,"First, the software version for the Stratus™ images would not allow correction of images.","Modify,Fact/Evidence",Fact/Evidence
6241,2-131,2-131_v2_66@3,2-131_v1_59@3,Two independent graders manually corrected all the images; this may have resulted in some inaccuracies in segmentation line correction.,"In addition, two people independently manually corrected the images, resulting in inaccuracies in segmentation line correction.","Modify,Clarity",Clarity
6242,2-131,2-131_v2_66@1,2-131_v1_59@4,All images were taken at a single imaging center; this might have introduced some bias.,"Finally, the images were only taken at one imaging center, which could have resulted in bias.","Modify,Clarity",Clarity
6243,2-131,2-131_v2_67@1,2-131_v1_60@1,Spectralis™ had the lowest COR values.,Spectralis™ had the lowest coefficient of repeatability values.,"Modify,Clarity",Clarity
6244,2-131,2-131_v2_19@0,2-131_v1_14@0,"Patients were scanned twice by certified OCT operators on a TD-OCT device (Stratus™ OCT) and two SD-OCT devices (Spectralis™, and Cirrus™ OCT) machines in random order and with 5–10 minutes between each device.",Patients were scanned twice by certified OCT operators on a TD-OCT device (Stratus™ OCT) and two SD-OCT devices (Spectralis™ and Cirrus™ OCT) machines in random order and with 5–10 minutes between each device.,"Modify,Grammar",Grammar
6245,2-131,2-131_v2_20@0,2-131_v1_15@0,Optical Coherence Tomography,Optical coherence tomography,"Modify,Grammar",Grammar
6246,2-131,2-131_v2_21@0,2-131_v1_16@0,"One TD-OCT machine, Stratus™ (software version 4), and two SD-OCT machines, Spectralis™ (software version 5.0 I and Cirrus™ (software version 5.0.0.326) were used.","One TD-OCT machine, Stratus™ (software version 4), and two SD-OCT machines, Spectralis™ (software version 5.01) and Cirrus™ (software version 5.0.0.326) were used.","Modify,Fact/Evidence",Fact/Evidence
6247,2-131,2-131_v2_21@3,2-131_v1_16@3,"Using the Stratus™, two fast macular thickness maps (FMTP) were acquired from each eye.","Using the Stratus™, two Fast Macular Thickness Maps (FMTM) were acquired from each eye.","Modify,Grammar",Grammar
6252,2-131,2-131_v2_24@0,2-131_v1_19@0,Scans from each of the three devices were reviewed at the Ocular Imaging Research and Reading Center at the Stanley M. Truhlsen Eye Institute by two independent graders.,Scans from each of the three devices were reviewed at the Retinal Imaging Research and Reading Center at the Wilmer Eye Institute by independent graders.,"Modify,Fact/Evidence",Fact/Evidence
6253,2-131,2-131_v2_24@1,2-131_v1_19@1,Segmentation errors due to incorrect identification of inner and outer retinal boundaries by automated algorithms in the Spectralis™ and Cirrus™ devices were identified and manually corrected by these graders.,Incorrect identification of inner and outer retinal boundaries by automated algorithms in Spectralis™ and Cirrus™ devices was manually corrected.,"Modify,Fact/Evidence",Fact/Evidence
6254,2-131,2-131_v2_24@2,2-131_v1_19@2,Stratus™ images could not be corrected due to the lack of editing capabilities in the operating system provided with the machine at the time of conducting the study.,Stratus™ images could not be corrected due to the lack of editing capabilities in the operating system provided with the machine at the time the study was conducted.,"Modify,Clarity",Clarity
6255,2-131,2-131_v2_24@3,2-131_v1_19@3,Only 5 patients required corrections and were excluded from the analysis.,Only five patients required corrections and were excluded from the analysis.,"Modify,Grammar",Grammar
6256,2-131,2-131_v2_24@5,2-131_v1_19@5,"Meanwhile each device identifies the inner limiting membrane (ILM) as the inner boundary of retina, identification of the outer boundary is different for each device.","Whereas each device identifies the inner limiting membrane (ILM) as the inner boundary of retina, identification of the outer boundary is different for each device.","Modify,Clarity",Clarity
6257,2-131,2-131_v2_7@0,2-131_v1_3@1,Procedure : OCT scans were taken simultaneously using one TD-OCT and two SD-OCT devices.,OCT scans were taken simultaneously using one TD-OCT and two SD-OCT devices.,"Modify,Clarity",Clarity
6258,2-131,2-131_v2_25@0,2-131_v1_20@0,"Whenever the foveal center could be identified, grids were repositioned for scans with off-center positioning of the ETDRS grid.","Whenever the foveal center could be identified, grids were repositioned for scans with off-center positioning of the Early Treatment Diabetic Retinopathy Study (ETDRS) grid.","Modify,Clarity",Clarity
6259,2-131,2-131_v2_8@0,2-131_v1_3@2,Main Outcome Measures : Macular thickness measurements were assessed before and after correction of the algorithm by constructing Bland-Altman plots for agreement and calculating intraclass correlation coefficients (ICCs) and coefficients of repeatability (COR) to evaluate intraclass repeatability.,Macular thickness measurements were assessed before and after correction of the OCT algorithm by constructing Bland-Altman plots for agreement and calculating intraclass correlation coefficients (ICCs) and coefficients of repeatability (COR) to evaluate intraclass repeatability.,"Modify,Clarity",Clarity
6260,2-131,2-131_v2_26@0,2-131_v1_21@0,The retinal thickness measurements of the nine standard ETDRS subfields ( Appendix A illustrates the nine-subfield abbreviations) were recorded from each device before and after correcting the errors in the scans algorithm.,The retinal thickness measurements of the nine standard ETDRS subfields ( Figure S1 illustrates the nine-subfield abbreviations) were recorded from each device before and after correcting the errors in the scans’ algorithm.,"Modify,Fact/Evidence",Fact/Evidence
6261,2-131,2-131_v2_28@4,2-131_v1_23@3,Statistical significance of difference in thickness before and after correction of images across devices was determined via student’s t-test with α = 0.05 with Bonferroni correction for multiple comparisons.,The statistical significance of difference in thickness before and after correction of images across devices was determined via the student’s t-test with α = 0.05 with Bonferroni correction for multiple comparisons.,"Modify,Grammar",Grammar
6307,2-180,2-180_v2_17@5,2-180_v1_16@4,"Secondly, even though traditional rTMS is considered to be focal, magnetic fields of lower intensity are delivered outside of the focal area <REF-33> , raising the possibility that low intensity stimulation may be contributing to therapeutic effects by acting on interconnected brain regions.","Secondly, even though traditional rTMS is considered to be focal, magnetic fields of lower intensity are delivered outside of the focal area, raising the possibility that low intensity stimulation may be contributing to therapeutic effects by acting on interconnected brain regions.","Modify,Fact/Evidence",Fact/Evidence
6308,2-180,2-180_v2_18@0,2-180_v1_17@0,"We chose a complex pattern of stimulation that is based on biomimetic principles (described in detail <REF-23> 59.9-ms trains of 20 pulses at 3 different frequencies as follows: 1 min warm-up at 6.71 Hz, 8 min treatment at 10.1 Hz, and 1 min cool down at 6.26 Hz) and has been shown to induce structural changes in mice <REF-23> .","We chose a complex pattern of stimulation because it has been shown to induce structural changes in mice <REF-19> (59.9-ms trains of 20 pulses at 3 different frequencies as follows: 1 min warm-up at 6.71 Hz, 8 min treatment at 10.1 Hz, and 1 min cool down at 6.26 Hz.","Modify,Fact/Evidence",Fact/Evidence
6309,2-180,2-180_v2_18@1,2-180_v1_17@1,The pulse was monophasic with a 300µs rise time and 100µs fall time.,"The pulse duration was 200 µs, which is within the range used in human rTMS.","Modify,Fact/Evidence",Fact/Evidence
6312,2-180,2-180_v2_4@1,2-180_v1_4@1,"Importantly, rTMS induces long term potentiation (LTP) in rodent hippocampus in vitro <REF-2> and several sessions of high-frequency rTMS increases the capacity to induce LTP compared to untreated controls, suggesting it may also regulate metaplasticity <REF-3> , <REF-4> .","Importantly, rTMS induces long term potentiation (LTP) in rodent hippocampus in vitro <REF-2> and several sessions of high-frequency rTMS increases the capacity to induce LTP (metaplasticity) compared to untreated controls <REF-3> , <REF-4> .","Modify,Claim",Claim
6313,2-180,2-180_v2_38@3,2-180_v1_37@3,"Although it is difficult to draw conclusions from the null results presented here, the absence of observed behavioural and structural change is consistent with previously reported rTMS specificity for abnormal systems <REF-8> , <REF-23> .","The absence of observed behavioural and structural change is consistent with previously reported rTMS specificity for abnormal systems 8 , <REF-19> .","Modify,Claim",Claim
6314,2-180,2-180_v2_38@4,2-180_v1_37@4,The lack of adverse effects in our long-term study suggests that up to 5 weeks of daily sessions of low intensity pulsed magnetic field stimulation at the parameters used in this study appears safe to use in healthy participants.,"Furthermore, the lack of adverse effects in our long term study contributes evidence that rTMS is safe to use in healthy control participants <REF-20> .","Modify,Claim",Claim
6315,2-180,2-180_v2_40@3,2-180_v1_39@3,"This is consistent with previous reports that long-term rTMS effects are specific to abnormal brain circuitry: two weeks of rTMS improved visual tracking, visual electrophysiological function and topographical accuracy in a different strain of mice (ephrin-A2A5 -/- double knockouts) with abnormal circuitry but produced no lasting effects in wildtype mice <REF-23> .","This is consistent with previous reports that long-term rTMS effects are specific to abnormal brain circuitry: two weeks of rTMS improved visual tracking, visual electrophysiological function and topographical accuracy in mice with abnormal circuitry but produced no lasting effects in wildtype mice <REF-19> .","Modify,Fact/Evidence",Fact/Evidence
6316,2-180,2-180_v2_0@0,2-180_v1_0@0,Long term delivery of pulsed magnetic fields does not alter visual discrimination learning or dendritic spine density in the mouse CA1 pyramidal or dentate gyrus neurons,Long term delivery of pulsed magnetic fields does not improve learning or alter dendritic spine density in the mouse hippocampus,"Modify,Other",Other
6317,2-180,2-180_v2_40@6,2-180_v1_39@6,"Furthermore, there is a lack of studies assessing cognitive effects of long-term rTMS in patients together with healthy controls, which presents a large gap in knowledge <REF-8> .","To our knowledge, there have been no studies assessing cognitive effects of long-term rTMS in patients and healthy controls, which presents a large gap in knowledge.","Modify,Clarity",Clarity
6318,2-180,2-180_v2_42@1,2-180_v1_41@1,"Importantly, we found similar spine densities in sham wildtype and ephrin-A2 -/- mice, suggesting that spine density is not solely dependent on ephrin-A2, in agreement with the literature <REF-19> – <REF-21> .","Importantly, we found similar spine densities in sham wildtype and ephrin-A2 -/- mice, suggesting that if present, deficits in spines are subtle in ephrin-A2 -/- mice.","Modify,Fact/Evidence",Fact/Evidence
6319,2-180,2-180_v2_8@1,2-180_v1_41@2,"In addition, although ephrin-A2 is expressed in the mouse hippocampus throughout life and has been implicated in its topographic organisation <REF-19> , <REF-20> , there is no evidence that it is involved in synaptic plasticity or spine dynamics <REF-21> .","Although ephrin-A2 is expressed in the mouse hippocampus throughout life and has been implicated in its topographic organisation <REF-35> , <REF-36> , to our knowledge, there is no evidence that ephrin-A2 is involved in synaptic plasticity or spine dynamics.","Modify,Clarity",Clarity
6320,2-180,2-180_v2_42@2,2-180_v1_41@4,"As such, the null effect of rTMS on dendritic spine density may be attributed to the absence of both a specific spine and learning deficit in both wildtype and ephrin-A2 -/- mice.","As such, the null effect of rTMS on dendritic spine density is in line with our behavioural results and may be attributed to the absence of a specific spine deficit for rTMS to correct in both wildtype and ephrin-A2 -/- mice.","Modify,Claim",Claim
6321,2-180,2-180_v2_42@5,2-180_v1_42@1,"However, it is surprising that dendritic spine density remains unaffected after long-term stimulation, given our previous results using the same stimulation parameters, demonstrating structural reorganisation in abnormal axon terminals following multiple stimulations, but not a single rTMS session <REF-23> .","However, it is surprising that dendritic spine density remains unaffected after long-term stimulation, given our previous results using the same stimulation parameters, demonstrating structural reorganisation in abnormal axon terminals following multiple, but not single rTMS stimulation sessions <REF-19> .","Modify,Clarity",Clarity
6322,2-180,2-180_v2_6@2,2-180_v1_6@2,"Given the significant structural changes induced in the mouse visual system following repeated stimulation sessions, and evidence for structural changes in the human brain <REF-17> , we hypothesised that a similar long-term rTMS regime in combination with a hippocampus-dependent learning task, would rescue impaired learning strategies previously found in ephrin-A2 -/- mice <REF-18> and alter spine density in the hippocampus.","Given the significant structural changes induced in the mouse visual system following repeated stimulation sessions, we hypothesised that a similar long-term rTMS regime in combination with a hippocampal learning task, would rescue impaired learning strategies previously found in ephrin-A2 -/- mice <REF-17> and alter spine density in the hippocampus.","Modify,Fact/Evidence",Fact/Evidence
6323,2-180,2-180_v2_2@2,2-180_v1_2@2,"We delivered 5 weeks of daily pulsed rTMS stimulation to adult ephrin-A2 -/- and wildtype (C57BI/6j) mice (n=10 per genotype) undergoing a visual learning task and analysed learning performance, as well as spine density, in the dentate gyrus molecular and CA1 pyramidal cell layers in Golgi-stained brain sections.","We delivered 5 weeks of daily pulsed rTMS stimulation to ephrin-A2 -/- and wildtype mice (n=10 per genotype) undergoing a visual learning task and analysed learning performance, as well as spine density, in the dentate gyrus molecular and CA1 pyramidal cell layers in Golgi-stained brain sections.","Modify,Fact/Evidence",Fact/Evidence
6324,2-180,2-180_v2_8@3,2-180_v1_7@1,"Although mice of both genotypes learned the task, their performance remained suboptimal due to lack of motivation to obtain food rewards through insufficient food restriction <REF-22> and neither learning behaviour, nor hippocampal spine density were affected by long term rTMS.","Although the mice learned the task, their performance remained suboptimal due to lack of motivation to obtain food rewards through insufficient food restriction <REF-18> and neither learning behaviour, nor hippocampal spine density were affected by long term rTMS.","Modify,Clarity",Clarity
6325,2-180,2-180_v2_11@6,2-180_v1_10@5,"Mice were age matched, aged 8–10 weeks old (equivalent to young sexually mature adult in humans) when commencing the experiment.","Mice were age matched, aged 8–10 weeks old when commencing the experiment.","Modify,Fact/Evidence",Fact/Evidence
6326,2-180,2-180_v2_11@7,2-180_v1_10@6,"For the duration of the study, mice were kept in standard caging in a controlled environment (12/12 light/dark cycle; temperature 22°C±2°C, separated into cages with clear plastic walls (17 cm × 19 cm base, 16 cm high) based on sex and genotype (2–4 per cage)).","For the duration of the study, mice were kept in standard caging in a controlled environment (12/12 light/dark cycle; temperature 22°C±2°C, separated into cages with clear plastic walls (17 cm x 19 cm base, 16 cm high) based on sex and genotype (2–4 per cage)).","Modify,Grammar",Grammar
6423,2-278,2-278_v2_2@0,2-278_v1_2@0,"In an article in Science on ""Bayes' Theorem in the 21st Century"", Bradley Efron uses Bayes' theorem to calculate the probability that twins are identical given that the sonogram shows twin boys.","In a recent article in Science on ""Bayes' Theorem in the 21st Century"", Bradley Efron uses Bayes' theorem to calculate the probability that twins are identical given that the sonogram shows twin boys.","Modify,Clarity",Clarity
6424,2-278,2-278_v2_5@2,2-278_v1_5@2,We argue that this example is relatively useless in illustrating Bayesian data analysis.,"We argue that this example is not only flawed, but useless in illustrating Bayesian data analysis because it does not rely on any data.","Modify,Claim",Claim
6425,2-278,2-278_v2_5@5,2-278_v1_5@4,"Rather, Efron combines different pieces of expert knowledge from the doctor and genetics using Bayes’ theorem.","Instead, Efron combines different pieces of expert knowledge from the doctor and genetics using Bayes’ theorem.","Modify,Clarity",Clarity
6426,2-278,2-278_v2_6@1,2-278_v1_7@0,"If we use the data point together with an uninformative uniform prior on P(A|B) (see Box 1 ) to determine the probability of identical twins given the twins are two boys, we obtain, with 95% certainty, a probability of between 0.01 and 0.84; if we use a highly informative prior based on information from the doctor and genetics, we obtain a probability of between 0.49 and 0.51.","If we use the data point together with an uninformative uniform prior on P(A|B) to determine the probability of identical twins given the twins are two boys (see Box 1 ), we obtain, with 95% certainty, a probability of between 0.01 and 0.84; if we use a highly informative prior based on information from the doctor and genetics, we obtain a probability of between 0.49 and 0.51.","Modify,Clarity",Clarity
6427,2-278,2-278_v2_2@2,2-278_v1_2@2,"While we agree that the choice of the prior is essential, we argue that the calculations on identical twins give a biased impression of the influence of uninformative priors in Bayesian data analyses.","We argue that this conclusion is problematic because Efron's example on identical twins does not use data, hence it is not Bayesian statistics; his priors are not appropriate and are not uninformative; and using the available data point and an uninformative prior actually leads to a reasonable posterior distribution.","Modify,Claim",Claim
6471,2-282,2-282_v2_25@0,2-282_v1_23@0,"Cortical activation occurring during the preparation of movement is detected by the EEG module thanks to a method based on the event-related desynchronization/synchronization (ERD/ERS) phenomenon <REF-7> , <REF-9> , <REF-10> .",Cortical activation occurring during the preparation of movement is detected by the EEG module thanks to a method based on the event-related desynchronization/synchronization (ERD/ERS) phenomenon <REF-6> .,"Modify,Fact/Evidence",Fact/Evidence
6472,2-282,2-282_v2_25@1,2-282_v1_23@1,We extracted a QP for the detection of intentionality of movement <REF-11> by considering: (i) the changes in the β²/α and β/α ratio (representing bursts of β-γ frequencies) during the pre-movement period; (ii) an appropriate threshold indicating which peaks of ratios are actually followed by a movement (and therefore may be considered as a predictor of movement); (iii) the number of movements executed.,"By considering: (i) the changes in the β²/α and β/α ratio (representing bursts of β-γ frequencies) during the pre-movement period; (ii) an appropriate threshold indicating which peaks of ratios are actually followed by a movement (and therefore may be considered as a predictor of movement); (iii) the number of movements executed, we extracted a QP for the detection of intentionality of movement <REF-8> .","Modify,Clarity",Clarity
6473,2-282,2-282_v2_26@0,2-282_v1_24@0,"Upsampled EEG data were processed with a Hamming window of 256 samples, using an overlap of 250 in the time domain.","Upsampled EEG data were processed with a Hamming window of 256 samples, an overlap of 250 in the time domain.","Modify,Clarity",Clarity
6474,2-282,2-282_v2_29@1,2-282_v1_27@1,"The pre-movement period (lasting 2 seconds) was defined according to the acoustic order given to the patients and the detection of the beginning of movement via the gyroscopes, by considering 2 seconds back from the point of detection of the beginning of movement.",The pre-movement period (lasting 2 seconds) was defined according to the acoustic order given to the patients and the detection of the beginning of movement via the gyroscopes.,"Modify,Fact/Evidence",Fact/Evidence
6475,2-282,2-282_v2_33@4,2-282_v1_31@4,"The peaks (β/α and β²/α ratios) higher than a defined threshold were considered as indicators of a potential voluntary movement <REF-11> , given that they represent the detection of the cortical motor preparation of the movement <REF-14> , <REF-15> .","The peaks (β/α and β²/α ratios) higher than a defined threshold were considered as indicators of a potential voluntary movement <REF-8> , given that they represent the detection of the cortical motor preparation of the movement.","Modify,Fact/Evidence",Fact/Evidence
6476,2-282,2-282_v2_33@6,2-282_v1_31@6,EEG QP is the geometric mean of the probability of movement (true positive stimulations) and the percentage of movements predicted <REF-11> .,QP is the geometric mean of the probability of movement (true positive stimulations) and the percentage of movements predicted <REF-8> .,"Modify,Fact/Evidence",Fact/Evidence
6477,2-282,2-282_v2_35@2,2-282_v1_33@2,"The Welch's averaged modified periodogram method was used to compute the magnitude squared coherence of an EEG channel and an EMG electrode along the frequency subbands <REF-7> , <REF-12> , <REF-16> .","The Welch's averaged modified periodogram method was used to compute the magnitude squared coherence of an EEG channel and an EMG electrode along the frequency band <REF-6> , <REF-9> , <REF-10> .","Modify,Clarity",Clarity
6478,2-282,2-282_v2_45@0,2-282_v1_43@0,The rise of low frequencies was used for a mathematical modelling which considered:,The rise of low frequencies was used for a mathematical modeling which considered:,"Modify,Grammar",Grammar
6479,2-282,2-282_v2_47@0,2-282_v1_45@0,- the max PSD in the low-frequency band over time.,- the max PSD in the low frequency band over time.,"Modify,Grammar",Grammar
6480,2-282,2-282_v2_76@0,2-282_v1_75@0,E. Simulation of ERD/ERS: which thresholds would be required to obtain high EEG QPs?,E. Simulation of ERD/ERS: which thresholds would be required to obtain high QPs?,"Modify,Other",Other
6481,2-282,2-282_v2_94@1,2-282_v1_90@1,"When all the possible combinations of EEG/EMG/kinematic QPs are tested, the probability trees could yield an optimal efficiency.","When all the possible combinations of EEG/EMG/kinematic are tested, the probability tree could yield an optimal efficiency.","Modify,Clarity",Clarity
6482,2-282,2-282_v2_94@2,2-282_v1_90@2,"An exhaustive list of the probabilities for the entire amount of data recorded is not provided, because of the huge amount of time that this analysis requires in terms of data processing.","An exhaustive list of the probabilities for the entire amount of data recorded is not provided, because of the huge amount of time that this analysis requires.","Modify,Fact/Evidence",Fact/Evidence
6485,2-282,2-282_v2_100@4,2-282_v1_95@4,"We propose that the EEG QP can be complemented by the QPs extracted from the cortico-muscular coherence and the QPs obtained by the analysis of the changes in the kinematic signals, which occur prior to the voluntary movements.",We propose that the QP can be complemented by the cortico-muscular coherence and the analysis of the changes in the kinematic signals which occur prior to the voluntary movements.,"Modify,Clarity",Clarity
6486,2-282,2-282_v2_100@5,2-282_v1_95@5,We suggest a fusion of the QP parameters in order to increase the likelihood to detect the intentionality of movement.,We suggest a fusion of the parameters.,"Modify,Claim",Claim
6487,2-282,2-282_v2_100@10,2-282_v1_95@9,"This is in agreement with adaptive methods which are being developed currently with the goal of improving the classification algorithms for BCI system in order to extract EEG patterns related to a cognitive or motor status <REF-6> , <REF-27> .",This is in agreement with adaptive methods which are being developed currently with the goal of improving the classifiers <REF-5> .,"Modify,Fact/Evidence",Fact/Evidence
6488,2-282,2-282_v2_100@11,2-282_v1_95@10,"Our approach will have to be tested in a large sample of patients in the future, in order to demonstrate its real clinical usefulness in daily practice.","Also, our approach will have to be tested in a large sample of patients in the future in order to demonstrate its real clinical usefulness in daily practice.","Modify,Clarity",Clarity
6491,2-282,2-282_v2_5@4,2-282_v1_5@2,"Why the use of a multimodal detection of the intentionality of movement? Although the potential for BCIs in neurological disorders is huge, the applicability of current BCI systems has been limited by several factors <REF-6> .","Why use a multimodal detection of the intentionality of movement? Although the potential for BCIs in neurological disorders is huge, the applicability of current BCI systems has been limited by several factors <REF-5> .","Modify,Clarity",Clarity
6492,2-282,2-282_v2_5@5,2-282_v1_5@3,One of them is the poor performance of BCIs based on EEG analysis only (also due to: inter-individual differences in the detectability of movement-related EEG-activity; differences in the way BCI users can voluntary modify their brain activity; and the fact that brain atrophy and neuroplastic changes occurring in patients affected with movement disorders makes it difficult to generalize EEG markers).,One of them is the poor performance of BCIs based on conventional EEG analysis.,"Modify,Fact/Evidence",Fact/Evidence
6493,2-282,2-282_v2_7@1,2-282_v1_7@1,"From each module, acting during different time-windows (EEG, kinematic and corticomuscular (described in detail in sections C–F) quality parameters (QPs) for the detection of the intentionality of movement or for the early detection of movement are extracted.","From each module (described in detail in sections B–F) quality parameters (QPs) for the detection of the intentionality of movement or the early detection of movement are extracted (we extract QPs and probabilities of stimulation for the EEG and Kinematic modules, and probability of stimulation for the corticomuscular module).","Modify,Fact/Evidence",Fact/Evidence
6494,2-282,2-282_v2_11@0,2-282_v1_11@0,"Acquisition of data was carried out on 4 neurological patients exhibiting a bilateral upper limb tremor (combinations of rest, postural and/or kinetic tremor), following approval of the Ethical Committee of ULB – Hôpital Erasme ( Table 1 ).","Acquisition of data was carried out on 4 neurological patients exhibiting a bilateral upper limb tremor (combinations of rest, postural and/or kinetic tremor), following approval of the Ethical Committee of ULB – Hôpital Erasme.","Modify,Fact/Evidence",Fact/Evidence
6495,2-282,2-282_v2_11@4,2-282_v1_11@4,Mean age of the patients was 62±20 years.,Mean age of the patients was 62 ± 20 years.,"Modify,Grammar",Grammar
6496,2-282,2-282_v2_11@6,2-282_v1_11@6,The ADL-T24 score range was 3–20/24 <REF-7> .,The ADL-T24 score range was 4–17/24 <REF-6> .,"Modify,Fact/Evidence",Fact/Evidence
6497,2-282,2-282_v2_11@7,2-282_v1_11@7,The Schwab and England ADL score ranged from 50 to 80% <REF-8> .,The Schwab and England ADL score ranged from 50 to 100% <REF-7> .,"Modify,Fact/Evidence",Fact/Evidence
6498,2-282,2-282_v2_15@5,2-282_v1_13@3,"After hearing an acoustic signal, they prepared themselves for the execution of movement by mental imagery of the movement.","After (1) hearing an acoustic signal, the patient (2) prepared themselves mentally for the execution of movement and (3) performed the task.","Modify,Clarity",Clarity
6499,2-282,2-282_v2_15@9,2-282_v1_13@8,"The nomenclature used for the recorded files–as reported in figures- is “pppFNnn” standing for patient’s code, task executed (""Finger-to-nose"") and run number, respectively.","The nomenclature used for the recorded files is “pppFNnn"" standing for patient’s code, task executed (""Finger-to-nose"") and run number, respectively.","Modify,Clarity",Clarity
6500,2-282,2-282_v2_17@0,2-282_v1_15@0,"(i) IMU sensors (inertial measurement units: tri-axial gyroscopes, accelerometers, magnetometers).","(i) IMU sensors (inertial measurement units; tri-axial gyroscopes, accelerometers, magnetometers).","Modify,Grammar",Grammar
6501,2-282,2-282_v2_21@1,2-282_v1_19@1,"In order to build a “movement window”, the signal from the magnetometer (which provides a very clean signal) is processed first.","In order to build a “movement window”, the signal from the magnetometer (which provides a very clean signal), is processed first.","Modify,Grammar",Grammar
7198,3-256,3-256_v2_76@3,3-256_v1_76@3,"Additionally, CR increased the plasma insulin content in GHR-KO mice [( p < 0.05), ( Figure 2F )].","Additionally, CR increased the plasma insulin content in GHR-KO mice [( p < 0.05, ( Figure 2F )].","Modify,Grammar",Grammar
7199,3-256,3-256_v2_98@2,3-256_v1_98@2,"Our insulin sensitivity results in GHR-KO mice on 30% CR differed from those obtained in a previous study, showing that caloric restriction promotes euglycemia in GHR-KO mice ( Figure 2C ).",Our insulin sensitivity results in GHR-KO mice on 30% CR differed from those obtained in a previous study showing that caloric restriction promotes euglycemia in GHR-KO mice ( Figure 2C ).,"Modify,Grammar",Grammar
7200,3-256,3-256_v2_98@5,3-256_v1_98@5,"Moreover, data from macromolecular analysis of insulin signaling in GHR-KO mice on CR, including decreased insulin receptor (INSR) and thymoma viral proto-oncogene 1/protein kinase b (AKT1/PKB) concentrations in the skeletal musculature of GHR-KO’s on CR, and decreased phosphatidylinositol-4,5-bisphosphate 3-kinase (PI3K) subunits content in the livers of GHR-KO’s on CR, (all relative to GHR-KO’s on AL), also corroborate and portend decreased insulin sensitivity in GHR-KO mice on CR [ Bonkowski et al. , 2009 ].","Moreover, data from macromolecular analysis of insulin signaling in GHR-KO mice on CR, including decreased insulin receptor (INSR) and thymoma viral proto-oncogene 1/protein kinase b (AKT1/PKB) concentrations in the skeletal musculature of GHR-KO’s on CR, and decreased phosphatidylinositol-4,5-bisphosphate 3-kinase (PI3K) subunits content in the livers of GHR-KO’s on CR (relative to GHR-KO’s on AL), also corroborate and portend decreased insulin sensitivity in GHR-KO mice on CR [ Bonkowski et al. , 2009 ].","Modify,Clarity",Clarity
7201,3-256,3-256_v2_98@6,3-256_v1_98@6,"Additionally, it is worth noting that a tight regulation of euglycemia would be more consistent with health and survival than a predilection for hypoglycemia [ Tan & Flanagan, 2013 ]; thus, “improving health”, as CR has been broadly documented as doing, might mean preventing the innate endocrinological/metabolic derangements that are merely coincident with the longevity of the GHR-KO mouse.","Additionally, it is worth noting that a tight regulation of euglycemia would be more consistent with health and survival than a predilection for hypoglycemia [ Tan & Flanagan, 2013 ], thus “improving health”, as CR has been broadly documented as doing, might mean preventing the innate endocrinological/metabolic derangements that are merely coincident with the longevity of the GHR-KO mouse.","Modify,Grammar",Grammar
7202,3-256,3-256_v2_99@1,3-256_v1_99@1,"We discovered that CR did not alter the metabolism or spontaneous activity of GHR-KO mice, and also revealed that CR has no effect on the anxiety or memory function of GHR-KO mice.",We discovered that CR did not alter the metabolism or spontaneous activity of GHR-KO mice and also revealed that CR has no effect on the anxiety or memory function of GHR-KO mice.,"Modify,Grammar",Grammar
7203,3-256,3-256_v2_103@0,3-256_v1_102@0,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2015 Arum O et al.,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2014 Arum O et al.,"Modify,Fact/Evidence",Fact/Evidence
7204,3-256,3-256_v2_16@0,3-256_v1_16@0,"The resulting mice had elements of a 129/Ola, a Balb/c, two C57Bl/6J, and two C3H/HeJ stocks; therefore, although lacking the methodological benefits of “reproducible genetic heterogeneity” [ Miller et al. , 1999 ], this stock possesses considerable genetic variation, and thus the results are likely applicable to other mouse populations.","The resulting mice had elements of a 129/Ola, a Balb/c, two C57Bl/6J, and two C3H/HeJ stocks; therefore, although lacking the methodological benefits of “reproducible genetic heterogeneity” [ Miller, et al. , 1999 ], this stock possesses considerable genetic variation, and thus the results are likely applicable to other mouse populations.","Modify,Grammar",Grammar
7320,4-143,4-143_v2_23@3,4-143_v1_23@3,"According to the TMT scores, thus, one may conclude that all of the children who had a concussion 5 to 8 days prior to their participation in this study did not seem to have neurocognitive impairments.","According to the TMT scores, thus, one may conclude that all these children who had a concussion 5 to 8 days prior to their participation in this study did not seem to have neurocognitive impairments.","Modify,Clarity",Clarity
7321,4-143,4-143_v2_24@0,4-143_v1_24@0,"The patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.","Their patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.","Modify,Clarity",Clarity
7322,4-143,4-143_v2_2@6,4-143_v1_2@6,Results showed that only one of the three concussed children showed a score from the trail making test that was worse than the scores obtained from control subjects.,Results showed that only one of the three concussed children showed a score from the trail making test that was worse than those obtained from control subjects.,"Modify,Clarity",Clarity
7323,4-143,4-143_v2_24@1,4-143_v1_24@1,"Specifically, the first block of DE in the adaptation session that was significantly different from the last block of DE of the baseline session occurred later in the patients than in the controls; and the rate of performance change was higher (i.e., slower adaptation) in the patients as well.","Specifically, the first block of DE in the adaptation session that is significantly different from the last block of DE of the baseline session occurred later in the patients than in the controls; and the rate of performance change was higher (i.e., slower adaptation) in the patients as well.","Modify,Grammar",Grammar
7324,4-143,4-143_v2_25@2,4-143_v1_25@2,"Second, we did not collect any information regarding our subjects’ demographic and social statuses, which could be considered as potential confounding characteristics.","Also, we did not collect from our subjects any information regarding their demographic and social status, which could be considered as potential confounding characteristics.","Modify,Clarity",Clarity
7325,4-143,4-143_v2_26@0,4-143_v1_26@0,"In conclusion, the results from this study provide preliminary data indicating that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.","In conclusion, the results from this study indicate that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.","Modify,Clarity",Clarity
7326,4-143,4-143_v2_2@9,4-143_v1_2@9,Our findings provide preliminary data that suggest that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.,Our findings collectively suggest that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.,"Modify,Clarity",Clarity
7327,4-143,4-143_v2_4@3,4-143_v1_4@3,"In this short observational/case study, we investigated the pattern of adaptation to a novel visuomotor condition in three children who suffered a concussion while playing sports, and compared their adaptation patterns to those obtained from three children without a concussion.","In this research, we investigated the pattern of adaptation to a novel visuomotor condition in three children who suffered a concussion while playing sports, and compared their adaptation patterns to those obtained from three children without a concussion.","Modify,Clarity",Clarity
7328,4-143,4-143_v2_6@0,4-143_v1_6@0,"The purpose of this observational/case study was to determine qualitatively whether concussed children with neurocognitive impairments, as indicated by the TMT scores, would also demonstrate sensorimotor deficits, as indicated by the visuomotor adaptation patterns.","The purpose of this observational study was to determine whether concussed children with neurocognitive impairments, as indicated by the TMT scores, would also demonstrate sensorimotor deficits, as indicated by the visuomotor adaptation patterns.","Modify,Clarity",Clarity
7329,4-143,4-143_v2_8@0,4-143_v1_8@0,"Three children (15 years old, one male (cc3)), who presented to the Emergency Department at the Children’s Hospital of Wisconsin within 24 hours from the time of injury and who received a diagnosis of concussion (Glasgow Coma Scale ≥ 14), participated in this study.","Three children (15 years old), who presented to the Emergency Department at the Children’s Hospital of Wisconsin within 24 hours from the time of injury and who received a diagnosis of concussion (Glasgow Coma Scale ≥ 14), participated in this study.","Modify,Fact/Evidence",Fact/Evidence
7330,4-143,4-143_v2_8@4,4-143_v1_8@2,"Three children (12 (cc1), 14 (cc3) and 17 (cc2) years old, all males), who were recruited from the Milwaukee Metropolitan area, served as controls.","Three children (12, 14 and 17 years old), who were recruited from the Milwaukee Metropolitan area, served as controls.","Modify,Fact/Evidence",Fact/Evidence
7331,4-143,4-143_v2_8@5,4-143_v1_8@3,"Selection criteria for subjects were the same for both patients and controls (except their concussion status), which were: subject was 10–17 years of age, regularly participated in an athletic activity, was English-speaking, was right handed, and had no neurological disease or peripheral disorder affecting movement of the right arm.","Selection criteria for subjects were the same between patients and controls (except their concussion status), which were: subject is 10–17 years of age, regularly participates in an athletic activity, is English-speaking, is right handed, and has no neurological disease or peripheral disorder affecting movement of the right arm.","Modify,Grammar",Grammar
7332,4-143,4-143_v2_2@2,4-143_v1_2@2,"In this observational/case study, we investigated the association between neurocognitive function, assessed using a trail making test, and sensorimotor function, assessed using a visuomotor adaptation task, in three children who suffered from a concussion while playing sports.","In this observational study, we investigated the association between neurocognitive function, assessed using a trail making test, and sensorimotor function, assessed using a visuomotor adaptation task, in three children who suffered from a concussion while playing sports.","Modify,Clarity",Clarity
7333,4-143,4-143_v2_14@0,4-143_v1_14@0,"Because of the nature of the present study (i.e., observational/qualitative/case study of concussed children), we only tried to recruit a small number of concussed children.","Because of the nature of the present study (i.e., observational/case study of concussed children), we only tried to recruit a small number of concussed children.","Modify,Clarity",Clarity
7538,5-1356,5-1356_v2_2@5,5-1356_v1_2@5,We propose a statistical framework based on the Dirichlet-multinomial distribution that can discover changes in isoform usage between conditions and SNPs that affect relative expression of transcripts using these quantifications.,We propose a statistical framework based on the Dirichlet-multinomial distribution that can discover changes in isoform usage between conditions and SNPs that affect splicing outcome using these quantifications.,"Modify,Claim",Claim
7539,5-1356,5-1356_v2_17@1,5-1356_v1_15@1,"Similarly, separate modeling and testing of exon junctions ( Altrans <REF-27> ) or splicing events ( rMATS <REF-29> , GLiMMPS <REF-32> , Jia et al. <REF-33> , Montgomery et al. <REF-49> ) of a gene leads to non-independent statistical tests, although the full effect of this on calibration (e.g., controlling the rate of false discoveries) is not known.","Similarly, separate modeling and testing of exon junctions ( Altrans <REF-26> ) or splicing events ( rMATS <REF-28> , GLiMMPS <REF-30> , <REF-31> , <REF-47> ) of a gene leads to non-independent statistical tests, although the full effect of this on calibration (e.g., controlling the rate of false discoveries) is not known.","Modify,Fact/Evidence",Fact/Evidence
7540,5-1356,5-1356_v2_24@1,5-1356_v1_26@1,"The mean and covariance matrix of random proportions Π are ( Π ) = γ / γ + = π and ( ∏ ) = { γ + diag ( γ ) − γ γ T } / { γ + 2 ( γ + + 1 ) } , respectively.","The mean and covariance matrix of random proportions Π are ( Π ) = γ/γ + = π and ( Π ) = {γ + diag( γ ) – γ γ T }/{ γ + 2 ( γ + + 1)}, respectively.","Modify,Grammar",Grammar
7541,5-1356,5-1356_v2_26@0,5-1356_v1_29@0,"The mean of Y is unchanged at ( Y ) = {( Y | Π )} = ( m Π ) = m γ / γ + = m π , while the covariance matrix of Y is given by ( Y ) = cm {diag( π ) − ππ T }, where c = ( m + γ + )/(1+ γ + ) is an additional factor when representing the Dirichlet-multinomial covariance to the ordinary multinomial covariance.","The mean of Y is unchanged at ( Y ) = {( Y | Π )} = ( m Π ) = m γ /γ + = m π , while the covariance matrix of Y is given by ( Y ) = cm {diag( π ) – ππ T }, where c = ( m +γ + )/(1+γ + ) is an additional factor when representing the Dirichlet-multinomial covariance to the ordinary multinomial covariance.","Modify,Grammar",Grammar
7542,5-1356,5-1356_v2_27@0,5-1356_v1_30@0,"We can represent the DM distribution using an alternative parameterization: π = γ / γ + and θ = 1/(1 + γ + ); then, the covariance of Y can be represented as ( Y ) = n {diag( π ) − ππ T } {1 + θ ( n − 1)}, where θ can be interpreted as a dispersion parameter.","We can represent the DM distribution using an alternative parameterization: π = γ /γ + and θ = 1/(1 + γ + ); then, the covariance of Y can be represented as ( Y ) = n {diag( π ) – ππ T }{1 + θ ( n – 1)}, where θ can be interpreted as a dispersion parameter.","Modify,Grammar",Grammar
7543,5-1356,5-1356_v2_29@10,5-1356_v1_36@1,"In comparisons across c groups, the number of degrees of freedom is ( c − 1) × ( q − 1).","In comparisons across c groups, the number of degrees of freedom is ( c – 1) × ( q – 1).","Modify,Grammar",Grammar
7544,5-1356,5-1356_v2_30@0,5-1356_v1_37@0,"In a DTU analysis, groups are defined by the design of an experiment and are the same for each gene.","In a DS analysis, groups are defined by the design of an experiment and are the same for each gene.","Modify,Clarity",Clarity
7545,5-1356,5-1356_v2_30@1,5-1356_v1_37@1,"In tuQTL analyses, the aim is to find nearby (bi-allelic) SNPs associated with transcript usage of a gene.","In sQTL analyses, the aim is to find nearby (bi-allelic) SNPs associated with alternative splicing of a gene.","Modify,Fact/Evidence",Fact/Evidence
7546,5-1356,5-1356_v2_30@3,5-1356_v1_37@3,"Thus, tuQTL analyses are similar to DTU analyses with the difference that multiple models are fitted and tested for each gene.","Thus, sQTL analyses are similar to DS analyses with the difference that multiple models are fitted and tested for each gene.","Modify,Clarity",Clarity
7547,5-1356,5-1356_v2_30@4,5-1356_v1_37@4,"Additional challenges to be handled in tuQTL analyses include a large number of tests per gene with highly variable allele frequencies (models) and linkage disequilibrium, which can be accounted for in the multiple testing corrections.","Additional challenges to be handled in sQTL analyses include a large number of tests per gene with highly variable allele frequencies (models) and linkage disequilibrium, which can be accounted for in the multiple testing corrections.","Modify,Clarity",Clarity
7548,5-1356,5-1356_v2_32@1,5-1356_v1_39@1,"Following the edgeR strategy <REF-1> , <REF-2> , <REF-53> , we propose multiple approaches for dispersion estimation, all based on the maximization and adjustment of the profile likelihood, since standard maximum likelihood (ML) is known to produce biased estimates as it tends to underestimate variance parameters by not allowing for the fact that other unknown parameters are estimated from the same data <REF-54> , <REF-55> .","Following the edgeR ideology <REF-1> , <REF-2> , <REF-51> , we propose multiple approaches for dispersion estimation, all based on the maximization and adjustment of the profile likelihood, since standard maximum likelihood (ML) is known to produce biased estimates as it tends to underestimate variance parameters by not allowing for the fact that other unknown parameters are estimated from the same data <REF-52> , <REF-53> .","Modify,Clarity",Clarity
7549,5-1356,5-1356_v2_44@1,5-1356_v1_54@1,We performed simulations that correspond to a two-group comparison with no DTU (i.e. null model) where feature counts were generated from the DM distribution with identical parameters in both groups.,We performed simulations that correspond to a two-group comparison with no DS (i.e. null model) where feature counts were generated from the DM distribution with identical parameters in both groups.,"Modify,Clarity",Clarity
7550,5-1356,5-1356_v2_45@1,5-1356_v1_55@1,"Additionally, the median error of concentration estimates for Cox-Reid APL is always lower than for PL or maximum likelihood (ML) used in the dirmult package <REF-7> ( Figure 1C , Figure S2 ).","Additionally, the median error of concentration estimates for Cox-Reid adjusted profile likelihood is always lower than for PL or maximum likelihood (ML) used in the dirmult package <REF-7> ( Figure 1B , Figure S2 ).","Modify,Clarity",Clarity
7551,5-1356,5-1356_v2_47@2,5-1356_v1_59@2,"Thus, sharing information about concentration (dispersion) between genes by moderating the gene-wise APL is applied.","Thus, sharing information about concentration (dispersion) between genes by moderating to the gene-wise (adjusted) profile likelihood is applied.","Modify,Clarity",Clarity
7552,5-1356,5-1356_v2_51@0,5-1356_v1_62@0,"The aim of these analyses is to compare the performance of DRIMSeq against DEXSeq , which emerged among the top performing methods for detection of DTU from RNA-seq data <REF-23> .","The aim of these analyses is to compare the performance of DRIMSeq against DEXSeq , which emerged among the top performing methods for detection of DS from RNA-seq data <REF-22> .","Modify,Fact/Evidence",Fact/Evidence
7553,5-1356,5-1356_v2_51@1,5-1356_v1_62@1,"For DRIMSeq , we consider different dispersion estimates: common, gene-wise with no moderation and with moderation-to-common and to-trended dispersion.","Additionally, for DRIMSeq , we consider different dispersion estimates: common, gene-wise with no moderation and with moderation-to-common and to-trended dispersion.","Modify,Clarity",Clarity
7554,5-1356,5-1356_v2_55@0,5-1356_v1_65@0,"As noted by Soneson et al . <REF-23> , detecting DTU in human is harder than in fruit fly due to the more complex transcriptome of the first one; all methods have much smaller false discovery rate (FDR).","As noted by Soneson et al . <REF-22> , detecting DS in fruit fly is easier than in human; all methods have much smaller false discovery rate (FDR).","Modify,Fact/Evidence",Fact/Evidence
7555,5-1356,5-1356_v2_56@4,5-1356_v1_66@4,"Additionally, we have considered how other filtering approaches affect DTU detection.","Additionally, we have considered how other filtering approaches affect DS detection.","Modify,Fact/Evidence",Fact/Evidence
7556,5-1356,5-1356_v2_59@0,5-1356_v1_68@0,The p-value distributions highlight a better fit of the DM model to transcript counts compared to exonic counts (it is more uniform with a sharp peak close to zero).,The p-value distributions highlight a better fit of the DM model to transcript counts compared to exonic counts (it is uniform with a sharp peak close to zero).,"Modify,Clarity",Clarity
7557,5-1356,5-1356_v2_60@0,5-1356_v1_69@0,DS analyses on real datasets,DS analysis,"Modify,Other",Other
7558,5-1356,5-1356_v2_62@4,5-1356_v1_71@4,"To not diminish DEXSeq for its ability to fit more complex models, we run it using a model that does the four control versus three knock-down comparison with library layout as an additional covariate (model full 2).","To not diminish DEXSeq for its ability to fit GLMs, we run it using a model that does the four control versus three knock-down comparison with library layout as an additional covariate (model full glm).","Modify,Fact/Evidence",Fact/Evidence
7559,5-1356,5-1356_v2_62@5,5-1356_v1_71@5,"For the adenocarcinoma data, we do a two-group comparison of six normal versus six cancer samples (model full) and for DEXSeq , we fit an extra model that takes into account patient effects (model full 2).","For the adenocarcinoma data, we do a two-group comparison of six normal versus six cancer samples (model full) and for DEXSeq , we fit an extra model that takes into account patient effects (model full glm).","Modify,Fact/Evidence",Fact/Evidence
7560,5-1356,5-1356_v2_63@1,5-1356_v1_72@1,"Accounting for covariates in DEXSeq (model full 2) or performing the analysis on a subgroup without covariates (model full paired) results in more DS genes detected ( Figure S28 , Figure S29 and Figure S30 ).",Accounting for covariates in DEXSeq using the GLM or performing the analysis on a subgroup without covariates (model full paired) results in more DS genes detected ( Figure S26 and Figure S27 ).,"Modify,Fact/Evidence",Fact/Evidence
7561,5-1356,5-1356_v2_6@4,5-1356_v1_6@4,"Notably, the beta-binomial distribution, such as those used in differential methylation from bisulphite sequencing data <REF-13> – <REF-15> , represent a special case of the DM.","Notably, the beta-binomial distribution, such as those used in differential methylation from bisulphite sequencing data <REF-13> – <REF-15> , represents a special case of the DM.","Modify,Grammar",Grammar
7562,5-1356,5-1356_v2_2@2,5-1356_v1_2@2,There are situations where differences (e.g. between normal and disease state) in the relative ratio of expressed isoforms may have significant phenotypic consequences or lead to prognostic capabilities.,"There are situations where the total abundance of gene expression does not change (e.g. between normal and disease state), but differences in the relative ratio of expressed isoforms may have significant phenotypic consequences or lead to prognostic capabilities.","Modify,Claim",Claim
7563,5-1356,5-1356_v2_65@3,5-1356_v1_74@3,"In particular, the p-value distributions under the null indicate that DM fits better to transcript counts than exon counts ( Figure S14 , Figure S31 and Figure S32 ).",Also the distributions of p-values indicate that DM fits better to the transcript counts ( Figure S28 and Figure S29 ).,"Modify,Claim",Claim
7564,5-1356,5-1356_v2_66@2,5-1356_v1_75@2,"Of course, these validations represent an incomplete truth, and ideally, large-scale independent validation would be needed to comprehensively compare the DTU detection methods.","Of course, these validations represent an incomplete truth, and ideally, large-scale independent validation would be needed to comprehensively compare DS detection methods.","Modify,Claim",Claim
7565,5-1356,5-1356_v2_70@0,5-1356_v1_79@0,"To demonstrate the application of DRIMSeq to tuQTL analysis, we use the data from the GEUVADIS project <REF-46> where 465 RNA-seq samples from lymphoblastoid cell lines were sequenced, 422 of which were sequenced in the 1000 Genomes Project Phase 1.","To demonstrate the application of DRIMSeq to sQTL analysis, we use the data from the GEUVADIS project <REF-44> where 465 RNA-seq samples from lymphoblastoid cell lines were sequenced, 422 of which were sequenced in the 1000 Genomes Project Phase 1.","Modify,Fact/Evidence",Fact/Evidence
7566,5-1356,5-1356_v2_7@1,5-1356_v1_7@1,"Hence, gene expression can be viewed as a multivariate expression of transcripts or exons and such a representation allows the study of not only the overall gene expression, but also the expressed variant composition.","Hence, gene expression can be viewed as a multivariate expression of transcripts or exons, and such representation allows to study not only the overall gene expression, but how it is composed from different isoforms.","Modify,Clarity",Clarity
7567,5-1356,5-1356_v2_71@4,5-1356_v1_80@4,"The numbers of tested and associated genes and tuQTLs are indicated in Figure 4 , Figure S38 and Figure S39 .","The numbers of tested and associated genes and sQTLs are indicated in Figure 4 , Figure S35 and Figure S36 .","Modify,Fact/Evidence",Fact/Evidence
7570,5-1356,5-1356_v2_78@1,5-1356_v1_87@1,We have shown that this framework can be used for detecting differential isoform usage between experimental conditions as well as for identifying tuQTLs.,We have shown that this framework can be used for detecting differential isoform usage between experimental conditions as well as for identifying sQTLs.,"Modify,Clarity",Clarity
7571,5-1356,5-1356_v2_80@3,5-1356_v1_89@3,"Thus, we incorporate estimation techniques analogous to those used in negative binomial frameworks, such as Cox-Reid APL; perhaps not surprisingly, raw profile likelihood or standard maximum likelihood approaches do not perform as well in our tests of estimation performance.","Thus, we incorporate estimation techniques analogous to those used in negative binomial frameworks, such as Cox-Reid adjusted profile likelihood; perhaps not surprisingly, raw profile likelihood or standard maximum likelihood approaches do not perform as well in our tests of estimation performance.","Modify,Clarity",Clarity
7572,5-1356,5-1356_v2_81@0,5-1356_v1_90@0,"In comparison to other available methods, DRIMSeq seems to be more conservative than both DEXSeq (using transcript counts) and sQTLseekeR , identifying fewer DTU genes and tuQTLs, respectively.","In comparison to other available methods, DRIMSeq seems to be more conservative than both DEXSeq (using transcript counts) and sQTLseekeR , identifying fewer DS genes and sQTLs, respectively.","Modify,Clarity",Clarity
7573,5-1356,5-1356_v2_81@2,5-1356_v1_90@2,"Moreover, the sQTL associations detected by DRIMSeq have more enrichment in splicing-related features than sQTLseekeR tuQTLs, which could be due to the fact that DRIMSeq accounts for the higher uncertainty of lowly expressed genes by using transcript counts instead of transcript ratios.","Moreover, the sQTL associations detected by DRIMSeq have more enrichment in splicing-related features than sQTLseekeR sQTLs, which could be due to the fact that DRIMSeq accounts for the higher uncertainty of lowly expressed genes by using transcript counts instead of transcript ratios.","Modify,Clarity",Clarity
7574,5-1356,5-1356_v2_83@2,5-1356_v1_92@2,"In the tuQTL analysis, it would allow studying samples from the pooled populations, with the subpopulation as a covariate, allowing larger sample sizes and increased power to detect interesting changes.","In the sQTL analysis, it would allow studying samples from the pooled populations, with the subpopulation as a covariate, allowing larger sample sizes and increased power to detect interesting changes.","Modify,Fact/Evidence",Fact/Evidence
7575,5-1356,5-1356_v2_85@1,5-1356_v1_94@1,"In addition to the user friendly workflow for the DTU and tuQTL analyses, it provides plotting functions that generate diagnostic figures such as the dispersion versus mean gene expression figures and histograms of p-values.","In addition to the user friendly workflow for the DS and sQTL analyses, it provides plotting functions that generate diagnostic figures such as the dispersion versus mean gene expression figures and histograms of p-values.","Modify,Fact/Evidence",Fact/Evidence
7576,5-1356,5-1356_v2_90@0,5-1356_v1_101@0,Data for the tuQTL analyses was downloaded from the GEUVADIS project website.,Data for the sQTL analyses was downloaded from the GEUVADIS project website.,"Modify,Fact/Evidence",Fact/Evidence
7577,5-1356,5-1356_v2_93@0,5-1356_v1_104@0,DRIMSeq analyses for this paper were done with version 0.3.3 available on Zenodo https://zenodo.org/record/53084 <REF-61> and Bioconductor release 3.2.,DRIMSeq analyses for this paper were done with version 0.3.3 available on Zenodo http://dx.doi.org/10.5281/zenodo.53084 <REF-59> and Bioconductor release 3.2.,"Modify,Fact/Evidence",Fact/Evidence
7578,5-1356,5-1356_v2_93@1,5-1356_v1_104@1,Source code used for the analyses in this paper is available on Zenodo https://zenodo.org/record/167305 <REF-62> .,Source code used for the analyses in this paper is available on Zenodo http://dx.doi.org/10.5281/zenodo.53059 <REF-60> .,"Modify,Fact/Evidence",Fact/Evidence
7579,5-1356,5-1356_v2_10@0,5-1356_v1_10@0,"The Dirichlet-multinomial framework, implemented as a Bioconductor R package called DRIMSeq , is applicable to both differential transcript usage (DTU) analysis between conditions and transcript usage quantitative trait loci (tuQTL) analysis.","The Dirichlet-multinomial framework, implemented as a Bioconductor R package called DRIMSeq , is oriented for both DS analysis and sQTL analysis.","Modify,Claim",Claim
7580,5-1356,5-1356_v2_2@3,5-1356_v1_2@3,"Similarly, knowledge of single nucleotide polymorphisms (SNPs) that affect splicing, so-called splicing quantitative trait loci (sQTL) will help to characterize the effects of genetic variation on gene expression.","Similarly, knowledge of single nucleotide polymorphisms (SNPs) that affect splicing, so-called splicing quantitative trait loci (sQTL), will help to characterize the effects of genetic variation on gene expression.","Modify,Grammar",Grammar
7581,5-1356,5-1356_v2_13@0,5-1356_v1_13@0,"DS can be studied in three main ways: as differential transcript usage (DTU) or, in a more local context, as differential exon or exon junction usage (DEU) or as specific splicing events (e.g., exon skipping), and all have their advantages and disadvantages.","DS can be studied in three main ways: as differential isoform usage or, in a more local context, as differential exon or exon junction usage or as specific splicing events (e.g., exon skipping), and all have their advantages and disadvantages.","Modify,Claim",Claim
7582,5-1356,5-1356_v2_13@6,5-1356_v1_13@6,"This issue is captured in Altrans <REF-27> , which quantifies exon-links (exon junctions) or in MISO <REF-28> , rMATS <REF-29> , SUPPA <REF-30> and SGSeq <REF-31> , all of which calculate splicing event inclusion levels expressed as percentage spliced in (PSI).","This issue is captured in Altrans <REF-26> , which quantifies exon-links (exon junctions) or in MISO <REF-27> , rMATS <REF-28> and SUPPA <REF-29> , all of which calculate splicing event inclusion levels expressed as percentage spliced in (PSI).","Modify,Fact/Evidence",Fact/Evidence
7583,5-1356,5-1356_v2_13@9,5-1356_v1_13@9,"However, there are (hypothetical) instances where changes in splicing pattern may not be captured by exon-level quantifications (Figure 1A in the paper by Monlog et al. <REF-35> ).","However, there are (hypothetical) instances where changes in splicing pattern may not be captured by exon-level quantifications (Figure 1A in Monlog et al. <REF-33> ).","Modify,Clarity",Clarity
7584,5-1356,5-1356_v2_13@10,5-1356_v1_13@10,"Furthermore, detection of more complex transcript variations remains a challenge for exon junction or PSI methods (see Figure S5 in the paper by Ongen et al. <REF-27> ).","Furthermore, detection of more complex transcript variations remains a challenge for exon junction or PSI methods (see Figure S5 in Ongen et al. <REF-26> ).","Modify,Clarity",Clarity
7585,5-1356,5-1356_v2_13@11,5-1356_v1_13@11,"Soneson et al. <REF-23> considered counting which accommodates various types of local splicing events, such as exon paths traced out by paired reads, junction counts or events that correspond to combinations of isoforms; in general, the default exon-based counting resulted in strongest performance for DS gene detection.","Soneson et al . <REF-22> considered counting to accommodate various types of local splicing events, such as exon paths traced out by paired reads, junction counts or events that correspond to combinations of isoforms; in general, the default exon-based counting resulted in strongest performance for DS gene detection.","Modify,Clarity",Clarity
7718,5-1822,5-1822_v2_15@2,5-1822_v1_15@2,"A total of 72 rats were randomly assigned into four experimental groups; PcTx1-treated ( n =12), saline-treated ( n =16), uninjured/untreated controls ( n =6) or blood-spinal cord barrier integrity ( n =38).","A total of 34 rats were randomly assigned into three treatment groups; PcTx1-treated ( n =12), saline-treated ( n =16) or uninjured/untreated controls ( n =6).","Modify,Fact/Evidence",Fact/Evidence
7719,5-1822,5-1822_v2_19@2,5-1822_v1_19@2,The role of the pump was to slowly and steadily release PcTx1 (1.08 μg/h) subcutaneously to compensate for estimated renal and tissue losses of PcTx1 over the first 48 h period.,The role of the pump was to slowly release PcTx1 (1.08 μg/h) subcutaneously to compensate for renal losses and maintain a stable plasma concentration of the drug over a 48 h period.,"Modify,Fact/Evidence",Fact/Evidence
7720,5-1822,5-1822_v2_21@0,5-1822_v1_21@0,An inherent feature of all spinal contusion models is inter-animal variations in the size of the initial spinal cord lesions produced.,An inherent feature of all spinal contusion models is inter-animal variance in the size of the spinal lesions produced.,"Modify,Clarity",Clarity
7721,5-1822,5-1822_v2_23@6,5-1822_v1_23@4,Animals were then video recorded during three attempts at crossing the ladder and the footage analysed by a blinded observer to count the number of hind limb foot faults (foot slipping below the ladder between rungs).,Animals were video recorded during three attempts at crossing the ladder and the footage analysed by a blinded observer to count the number of hind limb foot faults (foot slipping below the ladder between rungs).,"Modify,Clarity",Clarity
7722,5-1822,5-1822_v2_23@13,5-1822_v1_23@9,"Each animal was video recorded when swimming in a tank of water (27–31°C) and the recordings analysed to determine if the animal used their hind limbs to swim, indicative of preservation of supra-spinal connections ( Magnuson et al., 2009 ; Saunders et al. , 1998 ; Smith et al. , 2006 ), and whether there were alternating hind limb movements.","The limb pattern during swimming in a tank of water (27–31°C) was also video recorded and analysed to determine if the animals used alternating hind limb movements during swimming, indicative of supra-spinal connections.","Modify,Fact/Evidence",Fact/Evidence
7723,5-1822,5-1822_v2_25@0,5-1822_v1_25@0,"At the end of each experimental period (24 h or 6 weeks post-injury) injured animals (together with age-matched controls) were terminally anaesthetized with an overdose of inhaled isoflurane (Lyppard Australia), the chest cavity opened and transcardially perfused with 50 ml heparinised (5 IU/ml) PBS (80 ml/min/kg) followed by 150 ml of 4% paraformaldehyde solution.",At the end of each experimental period (24 h or 6 weeks post-injury) injured animals (together with age-matched controls) were terminally anaesthetized with an overdose of inhaled isoflurane (Lyppard Australia) and transcardially perfused with 50 ml heparinised (5 IU/ml) PBS (80 ml/min/kg) followed by 150 ml of 4% paraformaldehyde solution.,"Modify,Fact/Evidence",Fact/Evidence
7724,5-1822,5-1822_v2_29@0,5-1822_v1_29@0,Specific tissue regions containing some of the white matter tracts involved in hind limb motor function were outlined and measured separately.,Specific tissue regions containing white matter tracts involved in hind limb motor function were outlined and measured separately.,"Modify,Clarity",Clarity
7725,5-1822,5-1822_v2_35@0,5-1822_v1_35@0,"The functional performance of PcTx1-treated and saline-treated animals was compared using two-way analysis of covariance (ANCOVA, Prism v6, Graphpad, San Diego, USA).",The functional performance of PcTx1-treated and saline-treated animals was compared using two-way analysis of covariance (ANCOVA).,"Modify,Fact/Evidence",Fact/Evidence
7726,5-1822,5-1822_v2_35@1,5-1822_v1_35@1,This method uses the General Linear Model approach and fits least squares linear regression lines to the raw data for individual animals then compares the slopes (correlations) of the regression lines.,This method fits least squares linear regression lines to the raw data for individual animals and then compares the slopes and intercepts of the regression lines.,"Modify,Fact/Evidence",Fact/Evidence
7727,5-1822,5-1822_v2_35@3,5-1822_v1_35@2,Differences in the initial injury severity between animals within each group introduces a covariate that makes a substantial contribution to the total observed variance in each treatment group.,Inter-animal differences in the initial injury severity (SIS scores) introduce a covariate that makes a substantial contribution to the total observed variance in each treatment group.,"Modify,Clarity",Clarity
7728,5-1822,5-1822_v2_37@0,5-1822_v1_37@0,"Blood-spinal cord barrier (BSCB) function at the lesion site was assessed between 2 h and 7 days post-SCI for different size permeability tracers ( n = 2 per tracer and time point) in a separate series of 38 untreated, injured rats.","Blood-spinal cord barrier (BSCB) function at the lesion site was assessed between 2 h and 7 days post-SCI for different size permeability tracers ( n = 2–3 per tracer) in a separate series of untreated, injured rats.","Modify,Fact/Evidence",Fact/Evidence
7729,5-1822,5-1822_v2_50@2,5-1822_v1_50@2,"Linear regression and ANCOVA revealed that the data were best described by two separate lines having the same slope, but with the PcTx1-treated group having a significantly higher elevation compared to the saline-treated group (F 1,9 =16.1324, p=0.003).","Linear regression and ANCOVA analysis revealed that the data were best described by two separate lines with the same slope, but with the PcTx1-treated group having a significantly higher elevation compared to the saline-treated group (p=0.002, n =6–10).","Modify,Fact/Evidence",Fact/Evidence
7730,5-1822,5-1822_v2_54@1,5-1822_v1_54@1,There was no observable differences between the two treatment groups in the pattern and style of swimming.,There was no observable difference between the two treatment groups in the swimming test (not shown).,"Modify,Clarity",Clarity
7731,5-1822,5-1822_v2_62@2,5-1822_v1_62@2,"ANCOVA revealed two separate lines with the same slope best described the data with the PcTx1-treated group having a significantly higher elevation ( Figure 5 ; F 1,8 =12.9908, p=0.0069).","ANCOVA revealed two separate lines with the same slope best described the data with the PcTx1-treated group having a significantly higher elevation ( Figure 5 ; p=0.003, n =6).","Modify,Fact/Evidence",Fact/Evidence
7732,5-1822,5-1822_v2_83@0,5-1822_v1_83@0,"All of the gene transcripts with significant changes (i.e. FC>2 or FC<-2) for the two injury sizes (SIS 2.5 and 2.75) were separated into their biological categories ( Figure 9 ) using the ‘Panther’ gene classification system ( Mi et al. , 2013 ).","All of the gene transcripts with significant changes (i.e. FC>2 or FC<-2) for the two injury sizes (SIS 2.5 and 2.75) were separated into their biological categories ( ) using the ‘Panther’ gene classification system ( Mi et al. , 2013 ).","Modify,Fact/Evidence",Fact/Evidence
7902,5-392,5-392_v2_16@0,5-392_v1_16@0,The mtDNA of the diplonemid Diplonema papillatum is composed of numerous free DNA circles ( Figure 1 ) with a total size of about 600 kb <REF-31> .,The mtDNA of the diplonemid Diplonema papillatum is composed of numerous free DNA circles ( Figure 1 ) with a total coding capacity of about 600 kb <REF-31> .,"Modify,Clarity",Clarity
7906,5-392,5-392_v2_16@4,5-392_v1_16@4,"The small subunit (SSU) mito-rRNA has been identified only very recently because its sequence is extremely diverged, which has made its identification challenging <REF-34> , <REF-35> .","The absence of the small subunit (SSU) mito-rRNA is explained by its being diverged beyond recognition, as the LSU gene is fragmented and extremely diverged, which made its identification challenging <REF-34> .","Modify,Fact/Evidence",Fact/Evidence
7907,5-392,5-392_v2_19@2,5-392_v1_19@2,"In Diplonema , only a few insertions of blocks of uridines have been documented initially, but the recent comprehensive count amounts to ~200 <REF-31> , <REF-32> , <REF-35> .","In Diplonema , only a few insertions of blocks of uridines have been documented so far <REF-31> , <REF-32> .","Modify,Fact/Evidence",Fact/Evidence
7908,5-392,5-392_v2_22@0,5-392_v1_22@0,"The post-transcriptional processing is very different in Diplonema , where fragments of genes transcribed from individual DNA circles are trans -spliced <REF-31> , <REF-33> .","The post-transcriptional processing is very different in Diplonema , where fragments of genes transcribed from individual DNA circles are trans -spliced in a highly systematic 5′ to 3′ progression <REF-31> , <REF-33> .","Modify,Fact/Evidence",Fact/Evidence
7909,5-392,5-392_v2_25@0,5-392_v1_25@0,Why are mitochondrial genomes in Euglenozoa so diverse?,Why are mitochondrial genomes in Euglenozoa so diverse,"Modify,Grammar",Grammar
7910,5-392,5-392_v2_26@0,5-392_v1_26@0,"Soon after its discovery, RNA editing in kinetoplastids was explained as a remnant of the RNA world <REF-44> .","Soon after its discovery, RNA editing in kinetoplastids was explained as a remnant of the RNA world <REF-43> , but the absence of similar mechanisms in both sister clades <REF-33> , <REF-37> puts this scenario finally to rest.","Modify,Fact/Evidence",Fact/Evidence
7911,5-392,5-392_v2_26@3,5-392_v1_26@3,"The recent finding of a mt genome in Euglena <REF-38> implies that the irreversible scrambling originally implied for the mtDNA of the euglenozoan last common ancestor <REF-45> did happen at a later stage in evolution, probably in the predecessor of diplonemids and kinetoplastids.","The recent finding of a standard mt genome in Euglena <REF-37> implies that the irreversible scrambling originally implied for the mtDNA of the euglenozoan last common ancestor <REF-44> did happen at a later stage in evolution, probably in the predecessor of diplonemids and kinetoplastids.","Modify,Clarity",Clarity
7912,5-392,5-392_v2_26@4,5-392_v1_26@4,"Although despite the available sequence data the mutual relationships among the three euglenozoan lineages remain unresolved, we can predict, on the basis of their mt genomes and transcriptomes, that the mostly free-living photosynthetic euglenids constitute the earliest offshoot of the long euglenozoan branch.","Although despite the available sequence data, the mutual relationships among the three euglenozoan lineages remain unresolved, and we can predict, on the basis of their mt genomes and transcriptomes, that the mostly free-living photosynthetic euglenids constitute the earliest offshoot of the long euglenozoan branch.","Modify,Grammar",Grammar
7913,5-392,5-392_v2_8@1,5-392_v1_8@1,"Like all mitochondria of aerobic protists, this organelle contains mitochondrial DNA (mtDNA) <REF-7> .","As all mitochondria of aerobic protists, it contains mitochondrial DNA (mtDNA) <REF-7> .","Modify,Clarity",Clarity
7914,5-392,5-392_v2_10@0,5-392_v1_10@0,"Standard mt genomes are usually represented by a circular or linear DNA molecule encoding an average of fewer than two dozen genes ranging from 2 to 66 proteins in Chromera velia and Andalucia godoyi , respectively <REF-13> , <REF-14> .","Standard mt genomes are usually represented by a circular or linear DNA molecule encoding an average of fewer than two dozen genes ranging from 2 to 66 proteins in Chromera velia and Andalucia godoyi <REF-13> , <REF-14> .","Modify,Clarity",Clarity
7915,5-392,5-392_v2_10@2,5-392_v1_10@2,"In euglenids and diplonemids, mtDNA seems to be evenly distributed throughout the lumen of the organelle <REF-8> , <REF-15> , whereas in kinetoplastids, the picture is more complex ( Figure 2 ).","In euglenids and diplonemids, mtDNA seems to be evenly distributed throughout the lumen of the organelle <REF-8> , <REF-15> , and the picture is more complex in kinetoplastids ( Figure 2 ).","Modify,Clarity",Clarity
7916,5-392,5-392_v2_10@3,5-392_v1_10@3,"In the obligatory parasitic trypanosomatids mtDNA is invariably compacted into a single disk-shaped structure of concatenated DNA termed the kinetoplast DNA (kDNA), the free-living or commensalic bodonids have their kDNA distributed either evenly or in foci in the mt lumen <REF-16> .","Whereas in the obligatory parasitic trypanosomatids it is invariably compacted into a single disk-shaped structure of concatenated DNA termed the kinetoplast DNA (kDNA), the free-living or commensalic bodonids have their kDNA distributed either evenly or in foci in the mt lumen <REF-16> .","Modify,Clarity",Clarity
7917,5-392,5-392_v2_13@4,5-392_v1_13@4,"The conserved region carries 18 protein-coding genes, mostly subunits of respiratory complexes (complex I: nad1 , nad2 , nad3 , nad4 , nad5 , nad7 , nad8 , and nad9 ; complex III: cob ; complex IV: cox1 , cox2 , and cox3 ; complex V: atp6 ), one ribosomal protein ( rps12 ), small and large mito-rRNA genes ( 12S and 9S ), and four open reading frames of unknown function ( MURF2 , MURF5 , cr3 , and cr4 ) ( Figure 3 ) <REF-24> , <REF-25> .","The conserved region carries 18 protein-coding genes, mostly subunits of respiratory complexes (complex I: nad1 , nad2 , nad3 , nad4 , nad5 , nad7 , nad8 , and nad9 ; complex III: cob ; complex IV: cox1 , cox2 , and cox3 ; complex V: atp6 ), one ribosomal protein ( rpl12 ), small and large mito-rRNA genes ( 12S and 9S ), and four open reading frames of unknown function ( MURF2 , MURF5 , cr3 , and cr4 ) ( Figure 3 ) <REF-24> , <REF-25> .","Modify,Fact/Evidence",Fact/Evidence
8025,6-1302,6-1302_v2_20@10,6-1302_v1_20@10,"This classification resembles the one depicted in Gingeras, 2009 <REF-7> and can be summarized as follows:",This classification resembles the one depicted in <REF-7> and can be summarized as follows:,"Modify,Fact/Evidence",Fact/Evidence
8026,6-1302,6-1302_v2_37@2,6-1302_v1_37@2,All candidate fusion transcripts were validated using qPCR and Sanger sequencing except the class 3 overlap which requires systematic reconstruction of the fusion transcript for the design of primers.,All candidate fusion transcripts were validated using qPCR and Sanger sequencing except the class three overlap which requires systematic reconstruction of the fusion transcript for the design of primers.,"Modify,Grammar",Grammar
8027,6-1302,6-1302_v2_41@1,6-1302_v1_41@1,"1 µl of each cDNA sample (2ng/µl) was added to a 5 µl of reaction mix containing 3 µl of Master Mix (LightCycler ® 480 sybr green I Master, Roche Diagnostics, GERMANY) and 0.66 µM forward and reverse primers.","1 µl of each cDNA sample (2ng/µl) was added to a 5 µl of reaction mix containing 3 µl of Master Mix (LightCycler ® 480 sybr green I Master, Roche Diagnostics, GERMANY) and 0.33 µM forward and reverse primers.","Modify,Fact/Evidence",Fact/Evidence
8028,6-1302,6-1302_v2_41@8,6-1302_v1_41@8,"Ultimately, a melting curve analysis ranging from 60°C to 95°C was performed to control primer specificity.","Ultimately, a melting curve analysis ranging from 95°C to 60°C was performed to control primer specificity.","Modify,Clarity",Clarity
8029,6-1302,6-1302_v2_44@1,6-1302_v1_44@1,"A first step aging slide with the cytogenetic preparation was performed by immerging slides in 2xSSC solution (saline sodium citrate) for 30 minutes at 37°C, followed by dehydration in 3 baths of increasing ethanol concentration: 70%, 85% and 100%, each for 1 minute.","A first step aging slide with the cytogenetic preparation was performed by immerging slides in 2xSSC solution (saline sodium citrate) for 30 minutes at 37°C, followed by dehydration in 3 baths of increasing ethanol concentration: 70°, 85° and 100°, each for 1 minute.","Modify,Fact/Evidence",Fact/Evidence
8030,6-1302,6-1302_v2_54@5,6-1302_v1_54@5,"The CBFB-MYH11 and PML-RARA fusion transcripts expressed in the AML-inv16 and AML-t(15;17) samples were identified using both RNA-seq and qPCR analysis, confirming the reliability of RNA-seq and the Crac suite in this type of analysis.","The CBFB-MYH11 and PML-RAR fusion transcripts expressed in the AML-inv16 and AML-t(15;17) samples were identified using both RNA-seq and qPCR analysis, confirming the reliability of RNA-seq and the Crac suite in this type of analysis.","Modify,Clarity",Clarity
8031,6-1302,6-1302_v2_57@0,6-1302_v1_57@0,"Among the Class 1 chRNAs more frequently associated with genomic translocation, we identified 4 chRNAs associated with PML and RARA genes in patient OM110223 suffering from AML-t(15,17).","Among the Class 1 chRNAs more frequently associated with genomic translocation, we identified 4 chRNAs associated with PML and RAR genes in patient OM110223 suffering from AML-t(15,17).","Modify,Clarity",Clarity
8032,6-1302,6-1302_v2_8@2,6-1302_v1_8@2,"In some well-documented cases, gene fusions, in addition to contributing to neoplastic transformation, produce fusion RNA and proteins used as therapeutic targets (Mertens et al. , 2015 <REF-5> , Yoshihara et al. , 2015 <REF-6> and references therein).","In some well-documented cases, gene fusions, in addition to contributing to neoplastic transformation, produce fusion RNA and proteins used as therapeutic targets ( <REF-5> , <REF-6> and references therein).","Modify,Fact/Evidence",Fact/Evidence
8033,6-1302,6-1302_v2_65@0,6-1302_v1_65@0,New Class 1 PML-RARA variants,New Class 1 PML-RAR variants,"Modify,Clarity",Clarity
8034,6-1302,6-1302_v2_66@1,6-1302_v1_66@1,"Acute promyelocytic leukemia (APL) molecular diagnosis and minimal residual disease (MRD) monitoring are currently based on bcr1, bcr2 or bcr3 fusion transcript detection, depending on the DNA breakpoint <REF-21> , <REF-22> .","Acute promyelocytic leukemia (APL) molecular diagnosis and minimal residual disease (MDR) monitoring are currently based on bcr1, bcr2 or bcr3 fusion transcript detection, depending on the DNA breakpoint <REF-21> , <REF-22> .","Modify,Grammar",Grammar
8035,6-1302,6-1302_v2_82@0,6-1302_v1_82@0,"In order to validate our strategy on a large cohort, we analyzed a publicly available dataset of 125 AML and 17 normal CD34+ HSC RNA-seq using a tag search approach (see Materials and methods).","To further extend our analysis and in order to validate our strategy on a large cohort, we analyzed a publicly available dataset of 125 AML and 17 normal CD34+ HSC RNA-seq using a tag search approach (see Materials and methods).","Modify,Clarity",Clarity
8036,6-1302,6-1302_v2_83@0,6-1302_v1_83@0,"We identified four new types of tumor-specific chRNA; TRIM28-TRIM28, DHRS7B-TMEM11, PLXNB-BLRD1 and SLC16A3-METRNL, expressed in all AML groups ( Figure 5B ).","We identified four new types of tumor-specific chRNA; TRIM28-TRIM28, DHRS7B-TMEME11, PLXNB-BLRD1 and SLC16A3-METRNL, expressed in all AML groups ( Figure 5B ).","Modify,Fact/Evidence",Fact/Evidence
8037,6-1302,6-1302_v2_83@4,6-1302_v1_83@4,"FLT3, PML-RARA and CBFB-MYH11 are strong markers in AML, and also useful for prognostic and MRD monitoring.","FLT3, PML-RARA and CBFB-MYH11 are strong markers in AML, and also useful for prognostic and MDR monitoring.","Modify,Grammar",Grammar
8038,6-1302,6-1302_v2_88@0,6-1302_v1_88@0,"The use of RNA-seq to provide a detailed view of the transcriptome and to detect new RNA transcripts, opens up new opportunities for improving diagnosis and treatment of human diseases.","The use of RNA-seq to provide a detailed view of the transcriptome and to detect new RNA transcripts, opens up new opportunities for improving diagnosis and treatment of human disease.","Modify,Grammar",Grammar
8039,6-1302,6-1302_v2_88@2,6-1302_v1_88@2,"ChRNAs, also known as “fusion RNA” or “canonical chimeras” <REF-5> , <REF-26> , are already used in diagnosis, but many other chimeric fusion products generated by transcriptional mechanisms such as read-throughs, cis or trans-splicing <REF-5> , <REF-7> , <REF-9> , also have the potential to be used in diagnosis if correctly categorized.","ChRNAs, also known as “fusion RNA” or “canonical chimeras”( <REF-5> , <REF-26> ), are already used in diagnosis, but many other chimeric fusion products generated by transcriptional mechanisms such as read-throughs, cis or trans-splicing <REF-5> , <REF-7> , <REF-9> , also have the potential to be used in diagnosis if correctly categorized.","Modify,Grammar",Grammar
8040,6-1302,6-1302_v2_91@2,6-1302_v1_91@2,"We also identified novel PML-RARA isoforms, shorter than the isoforms currently used in diagnosis, which could again be used in patient follow-up.","We also identified novel PML-RAR isoforms, shorter than the isoforms currently used in diagnosis, which could again be used in patient follow-up.","Modify,Clarity",Clarity
8041,6-1302,6-1302_v2_91@5,6-1302_v1_91@5,"MRD and patient follow-up in APL is usually performed by PML-RARA transcript QPCR, and relapse is associated with an increase of bcr1, bcr2 or bcr3 fusion transcripts <REF-32> .","MRD and patient follow-up in APL is usually performed by PML-RAR transcript QPCR, and relapse is associated with an increase of bcr1, bcr2 or bcr3 fusion transcripts <REF-32> .","Modify,Clarity",Clarity
8042,6-1302,6-1302_v2_95@0,6-1302_v1_95@0,"Abbreviations: chRNA, chimeric RNA; AML, acute myeloid leukemia; NK, normal karyotype; UK, unknown karyotype; AK, abnormal karyotype; APL, Acute Promyelocytic Leukemia; PBMCs, peripheral blood mononuclear cells; Inv16, chromosome 16 inversion; t(15;17), translocation of chromosomes 15 and 17; qPCR, quantitative polymerase chain reaction; NONE, non-annotated region; LincRNA, Long intergenic noncoding RNAs; Bcr, break chromosomal region; FISH, Fluorescence in situ hybridization; ATRA, all-trans retinoic acid; VD, vitamin D; TGFβ, transforming growth factor beta; MRD, minimal residual disease.","Abbreviations: chRNA, chimeric RNA; AML, acute myeloid leukemia; NK, normal karyotype; UK, unknown karyotype; AK, abnormal karyotype; APL, Acute Promyelocytic Leukemia; PBMCs, peripheral blood mononuclear cells; Inv16, chromosome 16 inversion; t(15;17), translocation of chromosomes 15 and 17; qPCR, quantitative polymerase chain reaction; NONE, non-annotated region; LincRNA, Long intergenic noncoding RNAs; Bcr, break chromosomal region; FISH, Fluorescence in situ hybridization; ATRA, all-trans retinoic acid; VD, vitamin D; TGFβtransforming growth factor beta; MRD, minimal residual disease.","Modify,Grammar",Grammar
8043,6-1302,6-1302_v2_13@8,6-1302_v1_13@8,"For the latter, peripheral blood mononuclear cells were collected by ficoll-hypaque density gradient and cultured at a concentration of 1×10 6 /ml, with or without 0.1µM ATRA, for 3 days.","For the latter, peripheral blood mononuclear cells were collected by ficoll-hypaque density gradient and cultured at a concentration of 1×10 6 /ml, with or without 0,1µM ATRA, for 3 days.","Modify,Grammar",Grammar
8044,6-1302,6-1302_v2_13@13,6-1302_v1_13@13,"The chemical agents used for differentiation were 1µM all-trans retinoic acid (ATRA; Sigma-Aldrich, Gillingham, UK), or 0.1µM vitamin D (VD) associated with 500pg/ml transforming growth factor beta TGFβ (Promega Corporation, USA) for the NB4 cell line <REF-18> .","The chemical agents used for differentiation were 1µM all-trans retinoic acid (ATRA; Sigma-Aldrich, Gillingham, UK), or 0,1µM vitamin D (VD) associated with 500pg/ml transforming growth factor beta TGFβ (Promega Corporation, USA) for the NB4 cell line <REF-18> .","Modify,Grammar",Grammar
8045,6-1302,6-1302_v2_13@14,6-1302_v1_13@14,"For the U937 cell line, 0.1µM TTNPB associated with 1µM Targretin (LGD1069) and 0.1µM 1 alpha, 25 dihydroxyvitamin D3 (VD) were used.","For the U937 cell line, 0,1µM TTNPB associated with 1µM Targretin (LGD1069) and 0,1µM 1 alpha, 25 dihydroxyvitamin D3 (VD) were used.","Modify,Grammar",Grammar
8046,6-1302,6-1302_v2_13@15,6-1302_v1_13@15,"TTNPB, LGD1069 and VD were kindly provided by Dr Klaus (Hoffman-La Roche, Switzerland), JHiernaux (Glaxo-welcome Laboratories, France), and L Binderup (Leopharmaceutical products, Denmark), respectively.","TTNPB, LGD1069 and VD were kindly provided by Dr Klaus (Hoffman-La Roche,Switzerland), JHiernaux (Glaxo-welcome Laboratories, France), and L Binderup (Leopharmaceutical products,Denmark), respectively.","Modify,Grammar",Grammar
8361,6-577,6-577_v2_22@2,6-577_v1_20@2,The antibody directed against Thr202/Tyr204 di-phosphorylated active ERK was from Thermo Scientific Pierce (rabbit monoclonal; MA5-15174; batch no. OC1680806); the anti-ERK1/2 antibody was from Thermo Scientific Pierce (mouse monoclonal; MA5-15605; batch no. PH1895491).,The antibody directed against Thr2020/Tyr204 di-phosphorylated active ERK was from Thermo Scientific Pierce (rabbit monoclonal; MA5-15174; batch no. OC1680806); the anti-ERK1/2 antibody was from Thermo Scientific Pierce (mouse monoclonal; MA5-15605; batch no. PH1895491).,"Modify,Fact/Evidence",Fact/Evidence
8362,6-577,6-577_v2_22@3,6-577_v1_20@3,"After extensive washing (4×30 minutes) in PBS – 0.1% Tween 20, membranes were incubated for 2 hours at room temperature in the simultaneous presence of IRDye 680RD goat anti-mouse (925-68070) and IRDye 800CW goat anti-rabbit (925-32211; Li-COR Biotechnology GmbH, Bad Homburg, Germany) secondary antibodies, or with anti-mouse and anti-rabbit HRP-conjugated antibody.","After extensive washing in PBS – 0.1% Tween 20, membranes were incubated for 2 hours at room temperature in the simultaneous presence of IRDye 680RD goat anti-mouse (925-68070) and IRDye 800CW goat anti-rabbit (925-32211; Li-COR Biotechnology GmbH, Bad Homburg, Germany) secondary antibodies, or with anti-mouse and anti-rabbit HRP-conjugated antibody.","Modify,Fact/Evidence",Fact/Evidence
8363,6-577,6-577_v2_4@2,6-577_v1_4@2,"The ERK family is the most studied in mammals ( Boulton et al ., 1990 ; Dhillon et al ., 2007 ) because it is involved in meiosis, mitosis and post mitotic functions in differentiated cells, as well as in the oxidative stress response and wound healing ( Castellano et al. , 2014 ; Johnson & Lapadat, 2002 ; Matsubayashi et al ., 2004 ; Runchel et al ., 2011 ).","The ERK family is the most studied in mammals ( Boulton et al ., 1990 ; Dhillon et al ., 2007 ) because it is involved in meiosis, mitosis and post mitotic functions in differentiated cells, as well as in the oxidative stress response and wound healing ( Johnson & Lapadat, 2002 ; Matsubayashi et al ., 2004 ; Runchel et al ., 2011 ).","Modify,Fact/Evidence",Fact/Evidence
8364,6-577,6-577_v2_22@4,6-577_v1_20@4,"Another set of extensive rinsing (4×30 minutes) in PBS – 0.1% Tween 20 was performed before membranes were imaged with an Odyssey device (LI-COR Biosciences, Lincoln, Nebraska) to detect fluorescence and HRP activity using Millipore ECL.","Another set of extensive rinsing in PBS – 0.1% Tween 20 was performed before membranes were imaged with an Odyssey device (LI-COR Biosciences, Lincoln, Nebraska) to detect fluorescence and HRP activity using Millipore ECL.","Modify,Fact/Evidence",Fact/Evidence
8365,6-577,6-577_v2_25@0,6-577_v1_23@0,"In order to confirm the presence of an ERK ortholog in corals, the human protein sequence of ERK1 (NP_001035145) was compared to the transcriptome database of Stylophora pistillata using the BLAST software ( Altschul et al. , 1990 ; Karako-Lampert et al ., 2014 ).","In order to confirm the presence of an ERK ortholog in corals, the human protein sequence of ERK1 (NP_001035145) was compared to the transcriptome database of Stylophora pistillata using the BLAST software ( Altschul et al. , 1990 ; Liew et al ., 2014 ).","Modify,Fact/Evidence",Fact/Evidence
8366,6-577,6-577_v2_25@2,6-577_v1_23@2,"This sequence (hereafter referred to as Spi-ERK for S. pistillata ERK) is the only one that shows an homology as high as 81%, 80% and 78% with the protein sequences of the cnidarians Nematostella vectensis ERK (Nv-ERK; XP_001629498.1), Hydra vulgaris ERK (Hv-ERK; XP_002154499.3) and the human MAPK3/ERK1 (Hs-ERK1), respectively ( Figure 1 ) ( Krishna et al ., 2013 ; Putnam et al ., 2007 ).","This sequence (hereafter referred to as Spi-ERK for S. pistillata ERK) is the only one that shows an homology as high as 81%, 80% and 78% with the protein sequences of the cnidarians Nematostella vectensis ERK (Nv-ERK; XP_001629498.1), Hydra vulgaris ERK (Hv-ERK; XP_002154499.3) and the human MAPK8/JNK1 (Hs-ERK1), respectively ( Figure 1 ) ( Krishna et al ., 2013 ; Putnam et al ., 2007 ).","Modify,Fact/Evidence",Fact/Evidence
8367,6-577,6-577_v2_25@5,6-577_v1_23@5,"This result suggests that a single ortholog of ERK is present in these cnidarians, consistently with previous work where only one ERK ortholog was found ( Castellano et al. , 2014 ; Russo et al. , 2004 ) but as opposed to the two genes encoding ERKs in most mammalian genomes ( Ip & Davis, 1998 ).","This result suggests that a single ortholog of ERK is present in these cnidarians, as opposed to the two genes encoding ERKs in most mammalian genomes ( Ip & Davis, 1998 ).","Modify,Fact/Evidence",Fact/Evidence
8368,6-577,6-577_v2_25@7,6-577_v1_23@7,"Accordingly, we detected a single immune-reactive band with the total-ERK antibody by western blot on S. pistillata extracts ( Figure 2A and Supplementary Figure S1 ).","Accordingly, we detected a single immune-reactive band with the total-ERK antibody by western blot on S. pistillata extracts ( Figure 2A and Figure S1 ).","Modify,Clarity",Clarity
8369,6-577,6-577_v2_4@3,6-577_v1_4@3,"The ERK gene family is evolutionnarily conserved and is found in all eukaryotes, including yeasts, plants, vertebrates and invertebrates ( Chen et al ., 2001 ; Widmann et al ., 1999 ).","The ERK gene family is evolutionnarily conserved and is found in all eukaryotes, including yeasts, plants, vertebrates and anthozoans ( Chen et al ., 2001 ; Widmann et al ., 1999 ).","Modify,Fact/Evidence",Fact/Evidence
8370,6-577,6-577_v2_31@0,6-577_v1_29@0,"RNAi interference techniques are not available in coral, and the confirmation that the immune reactive bands observed here specifically correspond to ERK could not be obtained through this method.","RNAi interference techniques are not available in coral, and the confirmation that the immune reactive bands observed here specifically correspond to ERK could not be obtained through this medium.","Modify,Clarity",Clarity
8371,6-577,6-577_v2_31@4,6-577_v1_29@3,"When the inhibitor was added to the seawater, the intensity of the band detected by the anti-total ERK did not vary, while the intensity of the band detected with the anti-phosphorylated ERK antibody was significantly reduced ( Figure 2B and Supplementary Figure S1 ).","When the inhibitor was added to the seawater, the intensity of the band detected by the anti-total ERK did not vary, while the intensity of the band detected with the anti-phosphorylated ERK antibody was significantly reduced ( Figure 2B and Figure S1 ).","Modify,Clarity",Clarity
8372,6-577,6-577_v2_33@0,6-577_v1_30@0,"Finally, to assess the performance of these antibodies, we compared the signal obtained on S. pistillata and human fibroblasts protein extracts ( Figure 4 and Supplementary Figure S3 ).","To assess the performance of these antibodies, we compared the signal obtained on S. pistillata and human fibroblasts protein extracts ( Figure 3 and Figure S2 ).","Modify,Clarity",Clarity
8373,6-577,6-577_v2_46@0,6-577_v1_40@0,Supplementary Figure S1.,Figure S1.,"Modify,Other",Other
8374,6-577,6-577_v2_47@0,6-577_v1_41@0,Supplementary Figure S2.,Figure S2.,"Modify,Other",Other
8375,6-577,6-577_v2_47@1,6-577_v1_41@1,Uncropped blot images for Figure 3 and supplementary replicates.,Uncropped blot images for Figure 3 .,"Modify,Fact/Evidence",Fact/Evidence
8376,6-577,6-577_v2_5@2,6-577_v1_5@2,"According to the manufacturer’s instructions, the antibody used in this study and directed against the Thr202/Tyr204 di-phosphorylated active ERK (Thermo Scientific Pierce; MA5-15174) showed reactivity with fruit fly, human, mink, mouse, non-human primate, pig, rat and zebrafish.","According to the manufacturer’s instructions, the antibody used in this study and directed against the Thr2020/Tyr204 di-phosphorylated active ERK (Thermo Scientific Pierce; MA5-15174) showed reactivity with fruit fly, human, mink, mouse, non-human primate, pig, rat and zebrafish.","Modify,Fact/Evidence",Fact/Evidence
8377,6-577,6-577_v2_10@1,6-577_v1_10@1,"Two small nubbins (3–5 cm long) were cut off from each mother colony, and were allowed to heal for four weeks in 15 L open system tanks before the experiments. Corals were maintained in the same conditions as the mother colonies, i.e. at 25°C, under a photosynthetic active radiation of 200 µmol photon.m -2 .s -1 provided by 400 W metal halide lamps (HPIT, Philips) and were fed twice a week with Artemia salina nauplii .","Two small nubbins were cut from each mother colony, and were allowed to heal for four weeks in 15 L open system tanks before the experiments. Corals were maintained in the same conditions as the mother colonies, i.e. at 25°C, under a photosynthetic active radiation of 200 µmol photon.m -2 .s -1 provided by 400 W metal halide lamps (HPIT, Philips) and were fed twice a week with Artemia salina nauplii .","Modify,Fact/Evidence",Fact/Evidence
8378,6-577,6-577_v2_11@0,6-577_v1_11@0,"Immortalized skin fibroblasts (BJ-EHLT cells) were kindly provided by E. Gilson’s lab (IRCAN) and cultured in Dulbecco’s Modified Eagle’s Medium (Invitrogen, Villebon-sur-Yvette, France) supplemented with 10% heat-inactivated fetal calf serum (Dutscher, Brumath, France) at 37°C in an atmosphere of 5% CO 2 , as previously described ( Biroccio et al ., 2013 ).","Immortalized skin fibroblasts (BJ-EHLT cells) were kindly provided by E. Gilson’s lab (IRCAN) and cultured in Dulbecco's Modified Eagle's Medium (Invitrogen, Villebon-sur-Yvette, France) supplemented with 10% heat-inactivated fetal calf serum (Dutscher, Brumath, France) at 37°C in an atmosphere of 5% CO 2 , as previously described ( Biroccio et al ., 2013 ).","Modify,Grammar",Grammar
8379,6-577,6-577_v2_17@1,6-577_v1_15@1,"Briefly, nubbins were airbrushed in 1 mL Laemmli buffer (2% SDS, 10% glycerol, 50mM Tris HCL pH7), ( Laemmli, 1970 ) using an air-pick (5 bars) to remove the totality of the tissues surrounding the skeleton was removed from coral.","Briefly, coral tissue was removed from the skeleton in 1 mL Laemmli buffer (1.5 X, Laemmli, 1970 ) using an air-pick.","Modify,Fact/Evidence",Fact/Evidence
8434,7-1235,7-1235_v2_2@7,7-1235_v1_2@7,The code and processed data are open sourced and available on github and contains a tutorial built into the application for assisting users.,The code and processed data is open sourced and available on github and with a tutorial built into the application for assisting users.,"Modify,Clarity",Clarity
8435,7-1235,7-1235_v2_14@1,7-1235_v1_14@1,"The interactive plots are made using shiny (v1.1.0) and ggplot2 (v3.0.0). Plots can be downloaded as .png, .pdf, or .svg files. Data used to generate the individual plots can be downloaded as .csv files.","The interactive plots are made using shiny (v1.1.0) and ggplots2 (3.0.0). Plots can be downloaded as .png, .pdf, or .svg files. Data used to generate the individual plots can be downloaded as .csv files.","Modify,Clarity",Clarity
8436,7-1235,7-1235_v2_16@1,7-1235_v1_16@1,Kaplan-Meier curves are generated using the survival (v2.41-3) and the survminer (v0.4-1) R packages.,Kaplan-Meier curves are generated using the survival (v2.41-3) and the survminer (0.4-1) R packages.,"Modify,Clarity",Clarity
8437,7-1235,7-1235_v2_16@3,7-1235_v1_16@3,"Hazard ratios for two-group comparisons, either median or optimal cut-off, utilize the Cox proportional hazards regression model in the survival R package; with the reported hazard ratio comparing high versus low protein groups.","Hazard ratio for two-group comparisons, either median or optimal cut-off, utilize the Cox proportional hazards regression model in the survival R package; with the reported hazard ratio comparing high versus low protein groups.","Modify,Grammar",Grammar
8438,7-1235,7-1235_v2_4@0,7-1235_v1_4@0,Improving prognostic prediction and the identification of potential therapeutic targets is of particular interest to clinicians.,Improving prognostic predictions and the identification of potential therapeutic targets is of particular interest to clinicians.,"Modify,Grammar",Grammar
8439,7-1235,7-1235_v2_16@7,7-1235_v1_16@7,Clinical variables dependent on the cancer type selected can be used to filter patients into user-defined groupings.,"Clinical variables dependent on the cancer type selected, can be used to filter patients into user-defined groupings.","Modify,Grammar",Grammar
8440,7-1235,7-1235_v2_19@1,7-1235_v1_19@1,Hazard ratios and p-values are based on the Cox regression model.,Hazard ratios and P values are based on the Cox regression model.,"Modify,Grammar",Grammar
8441,7-1235,7-1235_v2_4@1,7-1235_v1_4@1,"Quantification of messenger RNA at a genome-wide level has proven valuable in the discovery of gene expression profiles, which can serve as biomarkers for clinical outcomes in cancer <REF-1> .","Quantification of messenger RNA levels at a genome-wide level has proven valuable in the discovery of gene expression profiles, which can serve as biomarkers for clinical outcomes in cancer <REF-1> .","Modify,Clarity",Clarity
8442,7-1235,7-1235_v2_21@0,7-1235_v1_21@0,"In order to demonstrate the functionality of TRGAted, we present a basic survival analysis examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.","In order to demonstrate the functionality of TRGAted, we present a basic survival analysis of examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.","Modify,Grammar",Grammar
8443,7-1235,7-1235_v2_22@3,7-1235_v1_22@3,"Samples can be divided into quartiles, tertiles, median or optimally for p-values based on the protein of interest ( Figure 2C ).","The division of samples is available into quartiles, tertiles, median or optimum based on the protein of interest ( Figure 2C ).","Modify,Clarity",Clarity
8444,7-1235,7-1235_v2_2@0,7-1235_v1_2@0,Reverse-phase protein arrays (RPPAs) are a highthroughput approach to protein quantification utilizing antibody-based micro-to-nano scale dot blot.,Reverse-phase protein arrays (RPPAs) are a highthroughput approach to protein quantification utilizing an antibody-based micro-to-nano scale dot blot.,"Modify,Grammar",Grammar
8445,7-1235,7-1235_v2_23@2,7-1235_v1_23@2,"Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low protein groups ( Figure 3B ).","Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low proportion groups ( Figure 3B ).","Modify,Fact/Evidence",Fact/Evidence
8446,7-1235,7-1235_v2_23@3,7-1235_v1_23@3,Protein labeling is adaptive for both the volcano plot and bar chart and will only label significant proteins (p-value ≤ 0.05).,Protein labeling is adaptive for both the volcano plot and bar chart and will only label significant proteins.,"Modify,Fact/Evidence",Fact/Evidence
8447,7-1235,7-1235_v2_26@1,7-1235_v1_26@1,"Here, RAD50 predicts poor survival in only five cancer types, prostate, adrenocortical, breast cancer, low-grade glioma, and head and neck cancers ( Figure 4A ).","Here, RAD50 predicts poor survival in only five cancer types, prostate, adrenocortical, breast cancer, low-grade glioma and head and neck cancers ( Figure 4A ).","Modify,Grammar",Grammar
8448,7-1235,7-1235_v2_4@3,7-1235_v1_4@3,The availability of protein-level quantifications for the TCGA cohort allows for more relevant clinical outcome predictions compared to mRNA levels.,The availability of protein-level quantification for the TCGA cohorts allow for more relevant clinical outcome predictions compared to mRNA levels.,"Modify,Grammar",Grammar
8449,7-1235,7-1235_v2_30@3,7-1235_v1_30@3,"Built on the R shiny framework, a literate code architecture, the code for TRGAted is annotated and easily modified from our GitHub repository.","Built on the R Shiny framework, a literate code architecture, the code for TRGAted is annotated and easily modified from our GitHub repository.","Modify,Grammar",Grammar
8450,7-1235,7-1235_v2_4@4,7-1235_v1_4@4,"Currently, TCGA-based applications provide entry-level analysis in correlational, differential, and survival modalities for the RPPA information.","Currently available applications provide entry-level analysis in correlational, differential, and survival modalities for the RPPA information.","Modify,Clarity",Clarity
8451,7-1235,7-1235_v2_2@1,7-1235_v1_2@1,"Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor and metastatic samples.","Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor or metastatic samples.","Modify,Grammar",Grammar
8452,7-1235,7-1235_v2_10@4,7-1235_v1_10@4,"Clinical and survival information for each cancer data set was downloaded from recent work by Liu, et al. <REF-5> .",Clinical and survival information for each cancer data set were downloaded from recently updated TCGA clinical data <REF-5> .,"Modify,Fact/Evidence",Fact/Evidence
8453,7-1235,7-1235_v2_2@2,7-1235_v1_2@2,Protein-level data has particular advantages in assessing putative prognostic or therapeutic targets in tumors.,This protein-level data has particular advantages in assessing putative prognostic or therapeutic targets in tumors.,"Modify,Grammar",Grammar
8454,7-1235,7-1235_v2_10@6,7-1235_v1_10@6,"Unlike other cancer types, metastatic samples were retained for skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.","Unlike other cancer types, metastatic samples were kept in the skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.","Modify,Clarity",Clarity
8455,7-1235,7-1235_v2_2@4,7-1235_v1_2@4,"We developed a cloud-based application, TRGAted to enable researchers to better examine patient survival based on single or multiple proteins across 31 cancer types in the TCGA.","We developed a cloud-based application, TRGAted to enable researchers to better examine survival based on single or multiple proteins across 31 cancer types in the TCGA.","Modify,Clarity",Clarity
8618,7-1299,7-1299_v2_16@2,7-1299_v1_16@2,"Plants also have homologues of SMG5/6/7, known as SMG7 and SMG7-like <REF-50> , and SMG1 homologues <REF-45> , <REF-51> .","Plants also have homologues of SMG5-7, known as SMG7 and SMG7-like <REF-45> , and SMG1 homologues <REF-43> , <REF-44> .","Modify,Clarity",Clarity
8619,7-1299,7-1299_v2_16@10,7-1299_v1_16@10,"One possibility is that an alteriave kinase has replaced SMG1 and might even be ancestral and operational in many species, allowing for the loss of SMG1 <REF-51> .","Alternatively, different mechanisms have replaced SMG1 in each independent loss of SMG1, which might explain why some organisms have retained S/TQ richness within their UPF1 protein sequences while others have not <REF-43> , <REF-44> .","Modify,Fact/Evidence",Fact/Evidence
8620,7-1299,7-1299_v2_17@0,7-1299_v1_17@0,"The SMG5/6/7 family split and diversified in the animal lineage, with the acquisition of the PIN domain in SMG5 and SMG6 <REF-27> , <REF-57> , <REF-58> .","The SMG5-7 family split and diversified in the animal lineage, with the acquisition of the PIN domain in SMG5 and SMG6 <REF-26> , <REF-55> , <REF-56> .","Modify,Clarity",Clarity
8621,7-1299,7-1299_v2_17@2,7-1299_v1_17@2,The SMG5/6/7 family also have a role in regulating telomere length <REF-59> .,The SMG5-7 family also have a role in regulating telomere length <REF-57> .,"Modify,Clarity",Clarity
8622,7-1299,7-1299_v2_17@3,7-1299_v1_17@3,"SMG5/6/7 homologues in plants are known as SMG7, given they lack the PIN domain of SMG5 and SMG6 <REF-50> .","SMG5-7 homologues in plants are known as SMG7, given they lack the PIN domain of SMG5 and SMG6 <REF-45> .","Modify,Grammar",Grammar
8623,7-1299,7-1299_v2_17@4,7-1299_v1_17@4,"SMG5/6/7 family members of baker’s yeast, EBS1 and EST1, also lack the PIN domain <REF-60> .","SMG5-7 family members of baker’s yeast, EBS1 and ETS1, also lack the PIN domain <REF-48> .","Modify,Clarity",Clarity
8624,7-1299,7-1299_v2_17@5,7-1299_v1_17@5,"In baker’s yeast, EST1 is implicated in telomere regulation but not NMD, while a knockout of EBS1 reveals a mild NMD phenotype <REF-60> , <REF-61> .","In baker’s yeast, ETS1 is implicated in telomere regulation but not NMD, while a knockout of EBS1 reveals a mild NMD phenotype <REF-48> .","Modify,Grammar",Grammar
8625,7-1299,7-1299_v2_17@7,7-1299_v1_17@7,"The UPF1 of baker’s yeast is depleted of S/TQ dipeptides <REF-45> , which once phosphorylated by SMG1, normally act as binding site for SMG5/6/7 <REF-24> .","The UPF1 of baker’s yeast is depleted of S/TQ dipeptides <REF-43> , which once phosphorylated by SMG1, normally act as binding site for SMG5-7 <REF-23> .","Modify,Clarity",Clarity
8626,7-1299,7-1299_v2_17@9,7-1299_v1_17@9,"Tyrosine phosphorylation of UPF1 in baker’s yeast has been observed and appears to regulate the RNA helicase activity of UPF1 <REF-62> , although the role in NMD, if any, and kinase responsible is still unknown.",Tyrosine phosphorylation of UPF1 in baker’s yeast has been observed and appears to regulate the RNA helicase activity of UPF1 <REF-58> .,"Modify,Claim",Claim
8627,7-1299,7-1299_v2_17@27,7-1299_v1_17@18,"However, it is unclear if SMG6L directly interacts with UPF1 or if it is via phosporylation, but there is no SMG1 and classical phosphorylation sites on UPF1 <REF-9> .","However, it is unclear how it is recruited to UPF1 given the lack of SMG1 and classical phosphorylation sites on UPF1.","Modify,Fact/Evidence",Fact/Evidence
8628,7-1299,7-1299_v2_19@1,7-1299_v1_19@1,"Generally speaking, these can be split into four major types and a spread across many unrelated eukaryotic lineages ( Figure 2 and Figure 3 ):","Generally speaking, these can be split into four major types ( Figure 2 and Figure 3 ):","Modify,Claim",Claim
8629,7-1299,7-1299_v2_20@0,7-1299_v1_20@0,"- 1) Classical SMG1-dependent NMD (As exemplified by humans, worms, and moss)","- 1) Classical SMG1-dependent NMD (As exemplified by C. elegans , humans, and moss)","Modify,Fact/Evidence",Fact/Evidence
8630,7-1299,7-1299_v2_4@1,7-1299_v1_4@1,"Analysis of mutant screens and genetic diseases identified mutations that introduced nonsense mutations, but surprisingly, these premature termination codons (PTCs) lead to a reduction in mRNA stability <REF-1> , <REF-2> .","Early mutant screens identified mutations that introduced nonsense mutations, but surprisingly, these premature termination codons (PTCs) lead to a reduction in mRNA stability <REF-1> , <REF-2> .","Modify,Fact/Evidence",Fact/Evidence
8631,7-1299,7-1299_v2_21@2,7-1299_v1_21@2,It is possible that the NMD pathways of some species with a type 1 NMD pathway in appearance might better resemble type 2 NMD (recent SMG1-independent NMD).,It is possible that the NMD pathways of some species with a type 1 NMD pathway in appearance might better resemble type 2 NMD (Recent SMG1-independent NMD).,"Modify,Grammar",Grammar
8632,7-1299,7-1299_v2_22@1,7-1299_v1_22@1,"However, UPF1 still maintains the relatively high level of phosphorylatable S/TQ motifs <REF-45> , and phospho-UPF1 binding protein SMG7 <REF-8> , <REF-50> .","However, UPF1 still maintains the relatively high level of S/TQ dipeptide phosphorylation sites <REF-43> , and phospho-UPF1 binding protein SMG7 <REF-8> , <REF-45> .","Modify,Clarity",Clarity
8633,7-1299,7-1299_v2_23@1,7-1299_v1_23@1,"These ancient losses of SMG1 lead to an NMD pathway without SMG1, without SMG8 and SMG9 <REF-45> , with a UPF1 depleted in S/TQ dipeptides <REF-45> , but a potential role for SMG5/6/7 proteins <REF-9> , <REF-60> , <REF-61> .","These ancient losses of SMG1 lead to an NMD pathway without SMG1, without SMG8 and SMG9 <REF-43> , with UPF1 depleted in S/TQ dipeptides <REF-43> , and an unclear role for SMG5-7 proteins <REF-48> , <REF-59> .","Modify,Fact/Evidence",Fact/Evidence
8634,7-1299,7-1299_v2_23@2,7-1299_v1_23@2,"Future work (see below) will be needed to better understand the exact molecular role of SMG5/6/7 proteins in type 3 NMD pathways, and to understand how the NMD pathway functions without the SMG1 activating UPF1.","Future work (see below) will be needed to better understand the exact molecular role of SMG5-7 proteins in type 3 NMD pathways, and to understand how the NMD pathway functions without the SMG1 activating UPF1.","Modify,Clarity",Clarity
8635,7-1299,7-1299_v2_4@3,7-1299_v1_4@3,"This pathway was termed nonsense-mediated mRNA decay (NMD) and is now known to regulate hundreds to thousands of transcripts in plants, animals, fungi and ciliates <REF-4> – <REF-10> .","This pathway was termed nonsense-mediated mRNA decay (NMD) and is now known to regulate hundreds to thousands of transcripts in plants, animals and fungi <REF-4> – <REF-8> .","Modify,Fact/Evidence",Fact/Evidence
8636,7-1299,7-1299_v2_24@6,7-1299_v1_24@12,It is certainly possible that the presence of these factors do not represent a fully functional form of an NMD pathway and instead reflect the molecular reminance of a former NMD pathway whose factors have now been co-opted for other functions.,It is certainly possible that the presence of these factors do not represent a fully functional form of an NMD pathway and instead reflect the molecular reminance of a former NMD pathway whose factors have not been co-opted for other functions.,"Modify,Grammar",Grammar
8637,7-1299,7-1299_v2_27@6,7-1299_v1_27@6,"The EJC has been lost from baker’s yeast and so cannot have a role in its NMD pathway, but the EJC is involved in the fungi Neurospora crassa’s NMD pathway <REF-84> .","The EJC has been lost from baker’s yeast and so cannot have a role in its NMD pathway, but the EJC is involved in the fungi Neurospora crassa ’s NMD pathway <REF-71> .","Modify,Grammar",Grammar
8638,7-1299,7-1299_v2_28@5,7-1299_v1_28@4,"An alternative, but not mutually exclusive model posits that longer 3’ UTRs are able to recruit more UPF1 directly bound to the 3’ UTR <REF-95> .",An alternative model posits that longer 3’ UTRs are able to recruit more UPF1 directly bound to the 3’ UTR <REF-81> .,"Modify,Fact/Evidence",Fact/Evidence
8639,7-1299,7-1299_v2_28@6,7-1299_v1_28@5,"It has been found that UPF1 coats transcripts but translation displaces UPF1 from all regions, except the 3’ UTRs <REF-96> .","It has been found that UPF1 coats transcripts, but translation displaces UPF1 from all regions, except the 3’ UTRs <REF-82> .","Modify,Grammar",Grammar
8640,7-1299,7-1299_v2_28@7,7-1299_v1_28@6,This model suggests that a higher level of UPF1 binding increases the chances of NMD being triggered during the termination of translation; naturally long 3’ UTRs that are resistant to NMD have been observed to bind less UPF1 than susceptible long 3’ UTR transcripts <REF-95> .,This model suggests that a higher level of UPF1 binding increases the chances of NMD being triggered during the termination of translation; Naturally long 3’ UTRs that are resistant to NMD have been observed to bind less UPF1 than susceptible long 3’ UTR transcripts <REF-81> .,"Modify,Grammar",Grammar
8641,7-1299,7-1299_v2_28@8,7-1299_v1_28@7,In fact some naturally long 3’ UTR transcripts in mammals appear to be protected from NMD by various features such as a recently identified cis-sequence element in the TRAM1 gene <REF-97> or the many genes found to bind PTBP1 near the stop codon to prevent NMD <REF-46> .,"In fact, some naturally long 3’ UTR transcripts appear to be protected from NMD by various features such as a recently identified cis -sequence element in the TRAM1 gene <REF-83> or the many genes found to bind PTBP1 near the stop codon to prevent NMD <REF-84> .","Modify,Fact/Evidence",Fact/Evidence
8642,7-1299,7-1299_v2_29@1,7-1299_v1_30@1,"While the EJC mode has been identified in plants, fungi, and animals, suggesting an ancient origin, there are many eukaryotic lineages where it has not been characterized, or does not function <REF-9> , <REF-99> .","While the EJC mode has been identified in both plants and animals, suggesting an ancient origin, there are many eukaryotic lineages where it has not been characterized, or does not function <REF-59> , <REF-87> .","Modify,Fact/Evidence",Fact/Evidence
8643,7-1299,7-1299_v2_31@1,7-1299_v1_32@1,"It appears that a rather complex NMD pathway, belonging to the type 1 group, existed in the last eukaryotic common ancestor (see above).","It is clear that a rather complex NMD pathway, belonging to the type 1 group, existed in the last eukaryotic common ancestor (see above).","Modify,Clarity",Clarity
8644,7-1299,7-1299_v2_31@8,7-1299_v1_32@8,"Expansion of group II introns has been proposed to have driven the evolution of the spliceosome to enhance the splicing of these selfish elements <REF-105> , the nucleus evolved to physically separate the processes of transcription and translation and allow for intron removal before translation <REF-106> , and NMD evolved to degrade intron-retaining transcripts that escaped the nucleus <REF-106> , <REF-107> .","Expansion of group II introns has been proposed to have driven the evolution of the spliceosome to enhance the splicing of these selfish elements <REF-93> , the nucleus evolved to physically separate the processes of transcription and translation and allow for intron removal before translation <REF-94> , and NMD evolved to degrade intron-retaining transcripts that escaped the nucleus <REF-94> .","Modify,Fact/Evidence",Fact/Evidence
8645,7-1299,7-1299_v2_31@9,7-1299_v1_32@9,These adaptations ensure that transcripts with retained introns do not undergo multiple rounds of translation.,These adaptations ensure that retention of these efficiently-spliced introns would not be repeatedly translation.,"Modify,Clarity",Clarity
8646,7-1299,7-1299_v2_2@1,7-1299_v1_2@1,"In most eukaryotes, thousands of transcripts are degraded by NMD, including many important regulators of developmental and stress response pathways.","In most eukaryotes, thousands of transcripts are degraded by NMD, including many important regulators of development and stress response pathways.","Modify,Grammar",Grammar
8647,7-1299,7-1299_v2_31@14,7-1299_v1_32@13,"The presence of NMD may act as a buffer for novel introns with weak splice sites <REF-107> , <REF-110> .",The presence of NMD may act as a buffer for novel introns with weak splice sites <REF-96> .,"Modify,Fact/Evidence",Fact/Evidence
8648,7-1299,7-1299_v2_34@0,7-1299_v1_35@0,- 1) Why is SMG1 repeatedly lost in different lineages? Is there a backup mechanism to activate UPF1 and is this conserved between the lineages that have recently lost (eg A. thaliana ) and more anciently lost (eg baker’s yeast and T . thermophila ) SMG1? Or are there multiple SMG1 replacement mechanisms?,- 1) Why is SMG1 repeatedly lost in different lineages? Is there a backup mechanism to activate UPF1 and is this conserved between the lineages that have recently lost (eg A. thaliana ) and more anciently (eg baker’s yeast and T . thermophila ) SMG1? Or are there multiple SMG1 replacement mechanisms?,"Modify,Grammar",Grammar
8649,7-1299,7-1299_v2_34@1,7-1299_v1_35@1,- 2) What recruits the RNA degradation machinery to UPF1 when SMG1 is lost and S/TQ dipeptides are depleted? Does it rely on the direct interactions of SMG5/6/7 family proteins with UPF1 or another mechanism?,- 2) What recruits the RNA degradation machinery to UPF1 when SMG1 is lost and S/TQ dipeptides are depleted? Does this still depend on the SMG5-7 family and UPF1 phosphorylation?,"Modify,Claim",Claim
8650,7-1299,7-1299_v2_34@3,7-1299_v1_35@3,"- 4) Can the EJC mode of PTC recognition exist without the involvement of the EJC, potentially in T. thermophila ? If so, what is the molecular basis for this and does it exist in other species?","- 4) What is the molecular basis of an EJC mode of PTC recognition when the EJC is not involved, such as in T . thermophila ?","Modify,Claim",Claim
8651,7-1299,7-1299_v2_34@4,7-1299_v1_35@4,- 5) What is the precise mechanistic roles of UPF2/UPF3 in relation to EJC mode and non-EJC mode NMD pathways? How do UPF2/UPF3 get recruited to NMD targets independently of the EJC?,- 5) What is the precise mechanistic roles of UPF2/UPF3 in relation to EJC mode and non-EJC mode NMD pathways?,"Modify,Claim",Claim
8652,7-1299,7-1299_v2_37@1,7-1299_v1_38@1,"I propose that the classical (type 1) NMD involves UPF1-3, the UPF1-kinase SMG1 and the SMG5/6/7 family.","I propose that the classical (type 1) NMD involves UPF1-3, the UPF1-kinase SMG1 and the SMG5-7 family.","Modify,Clarity",Clarity
8653,7-1299,7-1299_v2_6@1,7-1299_v1_6@1,"These factors were named UP-frameshift (UPF) 1, 2 and 3 in baker's yeast and Suppressors with Morphological defects on Genitalia (SMG) 2, 3 and 4 in C. elegans .","These factors were named UP-frameshift (UPF) 1, 2 and 3 in baker's yeast and suppressors with morphological defects on genitalia (SMG) 2, 3 and 4 in C. elegans .","Modify,Grammar",Grammar
8654,7-1299,7-1299_v2_6@6,7-1299_v1_6@6,"From these early studies in C. elegans , the different NMD factors were defined by their role in the phosphorylation of UPF1.","Initially, NMD factors were defined by their role in the phosphorylation of UPF1.","Modify,Clarity",Clarity
8655,7-1299,7-1299_v2_6@8,7-1299_v1_6@8,SMG5/6/7 bind to phosphorylated UPF1 <REF-24> and are active in the dephosphorylation of UPF1 by recruiting the PP2A phosphatase <REF-25> – <REF-27> .,SMG5-7 bind to phosphorylated UPF1 <REF-23> and are active in the dephosphorylation of UPF1 by recruiting the PP2A phosphatase <REF-24> – <REF-26> .,"Modify,Fact/Evidence",Fact/Evidence
8656,7-1299,7-1299_v2_6@10,7-1299_v1_6@10,SMG5/6/7 have a central role in recruiting the degradation machinery to degrade the NMD target <REF-28> – <REF-31> ( Figure 1 ).,SMG5-7 have a central role in recruiting the degradation machinery to degrade the NMD target ( Figure 1 ).,"Modify,Fact/Evidence",Fact/Evidence
8657,7-1299,7-1299_v2_6@16,7-1299_v1_6@16,Many NMD targets are degraded by specific “branches” of the NMD pathway that do not require UPF2 <REF-37> or UPF3b <REF-38> in mammals.,Many NMD targets use “branches” of the NMD pathway that do not require UPF2 <REF-35> or UPF3b <REF-36> .,"Modify,Fact/Evidence",Fact/Evidence
8658,7-1299,7-1299_v2_6@17,7-1299_v1_6@17,"However, all branches do involve UPF1, highlighting its central importance to the NMD pathway.","However, all branches do involve UPF1, highlightings its central importance to the NMD pathway.","Modify,Grammar",Grammar
8659,7-1299,7-1299_v2_9@0,7-1299_v1_9@0,"Together these studies, mostly using animal systems, paint a picture where multiple factors (UPF2, UPF3, SMG1, SMG8, and SMG9) assist in the activation of UPF1, while other factors (SMG5/6/7) act to degrade an NMD target and dephosphorylate UPF1.","Together these studies, mostly using animal systems, paint a picture where multiple factors (UPF2, UPF3, SMG1, SMG8, and SMG9) assist in the activation of UPF1, while other factors (SMG5-7) act to degrade an NMD target and dephosphorylate UPF1.","Modify,Clarity",Clarity
8660,7-1299,7-1299_v2_2@5,7-1299_v1_2@5,"Here, I detail the factors involved in NMD, our current understanding of their interactions and how they have evolved.","Here, I outline the factors involved in NMD, our current understanding of their interactions and how they have evolved.","Modify,Clarity",Clarity
8661,7-1299,7-1299_v2_11@9,7-1299_v1_11@9,"However, it is worth noting that parasites are known to have reduced genomes relative to free-living relatives <REF-44> , and that the non-parasitic excavata Naegleria gruberi does harbor the additional NMD factors of SMG1 and SMG9 <REF-45> .","However, it is worth noting that parasites are known to have reduced genomes relative to free-living relatives <REF-42> , and that the excavata Naegleria gruberi does harbor the additional NMD factors of SMG1 and SMG9 <REF-43> .","Modify,Fact/Evidence",Fact/Evidence
8677,7-1306,7-1306_v2_22@6,7-1306_v1_22@6,"Additional statistical information about each gene is provided in interactive plots, such as the log-fold change comparing the average expression of a gene in one cluster versus the average expression in all other cells, the percentage of cells within the cluster that express the gene, and the percentage of cells in other clusters that express the gene.","Additional statistical information about each gene is provided as an interactive table, such as the log-fold change comparing the average expression of a gene in one cluster versus the average expression in all other cells, the percentage of cells within the cluster that express the gene, and the percentage of cells in other clusters that express the gene.","Modify,Clarity",Clarity
8678,7-1306,7-1306_v2_26@1,7-1306_v1_28@1,First the data can be exported as a set of CSV (comma separated values) files suited for further independent analysis and data sharing.,"First, as a set of CSV (comma separated values) files suited for further independent analysis and data sharing.","Modify,Clarity",Clarity
8679,7-1306,7-1306_v2_26@3,7-1306_v1_28@3,"The data can also be exported as an H5AD file that can be re-imported into this notebook’s workflow, retaining the generated results.","Second, as an H5AD file that can be re-imported into this notebook’s workflow, retaining the generated results.","Modify,Clarity",Clarity
8680,7-1306,7-1306_v2_28@0,7-1306_v1_30@0,"To run this notebook, the user needs a GenePattern account or can create one on the GenePattern Notebook site .","To run the notebook, the user is required to have a GenePattern account that can be created on the GenePattern Notebook site .","Modify,Clarity",Clarity
8681,7-1306,7-1306_v2_36@4,7-1306_v1_35@2,"As the GenePattern Notebook user interface gains more features, the notebook will also be able to take advantage of these features.","As the GenePattern Notebook user interface gains more features, the notebook will also grow to take advantage of these features.","Modify,Clarity",Clarity
8682,7-1306,7-1306_v2_9@2,7-1306_v1_9@2,"Text files from read count quantification tools like HTSeq ( Anders et al ., 2015 ) and Kallisto ( Bray et al ., 2016 ) are supported as input.","Gene by cell matrices generated by the 10X Genomics Cell Ranger pipeline and flat text files from read count quantification tools like HTSeq ( Anders et al. , 2015 ) and kallisto ( Bray et al. , 2016 ) are supported as input.","Modify,Fact/Evidence",Fact/Evidence
8685,7-1306,7-1306_v2_14@7,7-1306_v1_14@7,We also give users the option to remove sources of technical variation by performing linear regression on the total number of molecules detected and the percentage of reads mapped to mitochondrial genes.,"To remove sources of technical variation, linear regression is used to diminish the effects of the number of detected molecules and the percentage of counts mapped to mitochondrial genes.","Modify,Fact/Evidence",Fact/Evidence
8686,7-1306,7-1306_v2_14@11,7-1306_v1_14@10,A plot showing the percent variance explained of each principal component is then displayed so the user may choose a reasonable number of principal components for use in clustering ( Figure 2B ).,A plot showing the standard deviation of each principal component is then displayed so the user may choose a reasonable number of principal components for use in clustering ( Figure 2B ).,"Modify,Fact/Evidence",Fact/Evidence
8755,7-1891,7-1891_v2_2@5,7-1891_v1_2@5,The proportion of active users of LARCs in Pameungpeuk is very low (10.66%).,The proportion of active users of LARCs in Pameungpeuk is also very low (10.66%).,"Modify,Clarity",Clarity
8756,7-1891,7-1891_v2_28@3,7-1891_v1_27@3,Privacy and confidentiality of the clients’ information was observed through the use of data collection with coded identification numbers.,Privacy and confidentiality of the clients' information was observed through the use of data collection with coded identification numbers.,"Modify,Grammar",Grammar
8757,7-1891,7-1891_v2_2@6,7-1891_v1_2@6,This study aimed to analyze factors associated with the utilization of LARCs among family planning clients at the Pameungpeuk Rural Hospital.,This study therefore aimed to analyze factors associated with the utilization of LARCs among family planning clients at the Pameungpeuk Rural Hospital.,"Modify,Clarity",Clarity
8758,7-1891,7-1891_v2_3@2,7-1891_v1_3@2,We performed statistical analyses using chi-square test.,We performed statistical analyses using a chi-square test.,"Modify,Grammar",Grammar
8759,7-1891,7-1891_v2_9@1,7-1891_v1_9@1,"One of the Indonesian government’s efforts to reduce population growth and its associated problems, including high mortality rates, are family planning programs promoting contraceptive use, which educate patients and clients regarding family planning and contraception itself <REF-4> .","One of the Indonesian government's efforts to reduce population growth and its associated problems, including high mortality rates, are family planning programs promoting contraceptive use, which educate patients and clients regarding family planning and contraception itself <REF-4> .","Modify,Grammar",Grammar
8760,7-1891,7-1891_v2_2@2,7-1891_v1_2@2,"However, the rate of termination of the use of short-acting contraceptives by family planning clients was higher than other methods, therefore the use of short-acting contraceptives is less efficient than long acting reversible contraceptives (LARCs) for longer term spacing because it is easy to skip a treatment for economic or other reasons, which can result in unintended pregnancy.","However, the rate of termination of the use of short-acting contraceptives by family planning clients was higher than other methods, therefore the use of short-acting contraceptive is not effective enough for use.","Modify,Claim",Claim
8761,7-1891,7-1891_v2_13@3,7-1891_v1_13@3,"Some people have negative beliefs and misunderstanding about LARCs, additionally the lack of knowledge and skills of health workers can also influence access to the use of LARCs methods <REF-20> , <REF-21> .","Widespread of myths and misunderstandings about LARCs and the lack of knowledge and skills of health workers can also influence access to the use of LARCs methods <REF-19> , <REF-20> .","Modify,Clarity",Clarity
8762,7-1891,7-1891_v2_2@3,7-1891_v1_2@3,"Therefore, the National Family Planning Program in Indonesia is encouraging the use of LARCs to control population growth.","In anticipating the decreased use of short-acting contraceptives while also seeking to control population growth, the National Family Planning Program in Indonesia is encouraging the use of long-acting reversible contraceptives (LARCs).","Modify,Claim",Claim
8763,7-1891,7-1891_v2_19@0,7-1891_v1_19@0,An interviewer-administered questionnaire <REF-22> was used to collect data for this study.,An interviewer-administered questionnaire ( Supplementary File 1 ) was used to collect data for this study.,"Modify,Fact/Evidence",Fact/Evidence
8764,7-1891,7-1891_v2_22@0,7-1891_v1_21@0,The interviewer-administered questionnaire <REF-22> was conducted with consenting consecutive clients as they accessed family planning services until the desired sample size was achieved.,The interviewer-administered questionnaire ( Supplementary File 1 ) was conducted with consenting consecutive clients as they accessed family planning services until the desired sample size was achieved.,"Modify,Fact/Evidence",Fact/Evidence
9715,8-1204,8-1204_v2_4@2,8-1204_v1_4@2,In two samples there were histological changes detected that might have suggested the underlying presence of a type IV collagen disorder.,In two samples there were histological changes detected that might have suggested the underlying presence of a collagen IV disorder.,"Modify,Clarity",Clarity
9716,8-1204,8-1204_v2_27@1,8-1204_v1_27@1,Two of these samples showed signs on electron microscopy that might be consistent with an underlying type IV collagen glomerular basement membrane disorder.,Two of these samples showed signs on electron microscopy that might be consistent with an underlying collagen IV glomerular basement membrane disorder.,"Modify,Clarity",Clarity
9717,8-1204,8-1204_v2_32@1,8-1204_v1_32@1,"Of those that did, two were reported to have characteristics that might be consistent with an underlying type IV collagen disorder.","Of those that did, two were reported to have characteristics that might be consistent with an underlying collagen IV disorder.","Modify,Clarity",Clarity
9718,8-1204,8-1204_v2_32@3,8-1204_v1_32@3,"The first sample, in which the patient had the tip variant of FSGS, was suggested to be consistent with TBMN whereas there was no pathological comment made about the second from a patient with FSGS NOS.",The first sample was suggested to be consistent with TBMN whereas there was no pathological comment made about the second.,"Modify,Fact/Evidence",Fact/Evidence
9719,8-1204,8-1204_v2_32@4,8-1204_v1_32@4,"FSGS (NOS) was the most common lesion described in this study, consistent with prior reports <REF-2> .","FSGS (NOS) was the most common lesion described in this study, which is consistent with prior reports <REF-2> .","Modify,Clarity",Clarity
9720,8-1204,8-1204_v2_32@5,8-1204_v1_32@5,"Notably, close to one in three cases of primary FSGS were not proceeding to electron microscopy despite an indication to do so and 1 in 20 cases within our cohort had structural changes that were consistent with an underlying type IV collagen variant.","Notably, close to one in three cases of primary FSGS were not proceeding to electron microscopy despite an indication to do so and 1 in 20 cases within our cohort had structural changes that were consistent with an underlying collagen IV variant.","Modify,Clarity",Clarity
9721,8-1204,8-1204_v2_33@0,8-1204_v1_33@0,"Curiously, neither of the two patients in whom electron microscopy was suggestive of an underlying type IV collagen disorder was noted to have haematuria on their urinalysis at the time of presentation.","Curiously, neither of the two patients in whom electron microscopy was suggestive of an underlying collagen IV disorder was noted to have haematuria on their urinalysis at the time of presentation.","Modify,Clarity",Clarity
9722,8-1204,8-1204_v2_33@3,8-1204_v1_33@2,Unfortunately due to the retrospective nature of this study we were unable to send any samples for immunostaining of type IV collagen.,"Unfortunately, due to the retrospective nature of this study we were unable to send any samples for immunostaining of collagen IV.","Modify,Clarity",Clarity
9723,8-1204,8-1204_v2_34@0,8-1204_v1_34@0,"There is an increasing body of evidence indicating that inheritable variants in COL4A may underlie a proportion of cases of FSGS, with up to 12.5% cases of autosomal dominant FSGS attributable to COL4A3 in some cohorts <REF-10> .","There is an increasing body of evidence indicating that inheritable variants in collagen IV genes may underlie a proportion of cases of FSGS, with up to 12.5% cases of autosomal dominant FSGS attributable to COL4A3 in some cohorts <REF-15> .","Modify,Clarity",Clarity
9724,8-1204,8-1204_v2_34@1,8-1204_v1_34@1,Not subjecting these renal biopsy samples to electron microscopy represents a potential gap in the investigation and subsequent management of such patients given they are much less likely to respond to immunosuppressive therapy <REF-16> which has otherwise been classically indicated ( Figure 2 ).,Not subjecting these renal biopsy samples to electron microscopy represents a potential gap in the investigation and subsequent management of such patients given they are much less likely to respond to immunosuppressive therapy <REF-16> which has otherwise been classically indicated.,"Modify,Fact/Evidence",Fact/Evidence
9725,8-1204,8-1204_v2_37@0,8-1204_v1_35@0,"This study was designed as a retrospective cohort study looking at the number of samples sent for electron microscopy, as well as any potential changes which might be consistent with a type IV collagen glomerular basement membrane disorder.","This study was designed as a retrospective cohort study looking at the number of samples sent for electron microscopy, as well as any potential changes which might be consistent with a collagen IV glomerular basement membrane disorder.","Modify,Clarity",Clarity
9726,8-1204,8-1204_v2_37@1,8-1204_v1_35@1,It is important to recognise that not all groups have found the characteristic changes associated with the type IV collagen disorders such as Alport’s Syndrome or TBMN on electron microscopy.,It is important to recognise that not all groups have found the characteristic changes associated with the collagen IV disorders such as Alport’s Syndrome or TBMN on electron microscopy.,"Modify,Clarity",Clarity
9727,8-1204,8-1204_v2_37@3,8-1204_v1_35@3,It is thus possible that a lack of classical findings for a type IV collagen glomerular basement membrane disorder may have accounted for the low number of those with GBM features on electron microscopy consistent with a type IV collagen disorder noted within our study.,It is thus possible that a lack of classical findings for a collagen IV glomerular basement membrane disorder may have accounted for the low number of those with GBM features on electron microscopy consistent with a collagen 4 disorder noted within our study.,"Modify,Clarity",Clarity
9728,8-1204,8-1204_v2_38@0,8-1204_v1_36@0,"The process by which variants within the COL4A genes might cause FSGS remains unclear, particularly given their clear association with Alport Syndrome and TBMN.","The process by which variants within the collagen IV genes might cause FSGS remains unclear, particularly given their clear association with Alport Syndrome and TBMN.","Modify,Clarity",Clarity
9729,8-1204,8-1204_v2_38@1,8-1204_v1_36@1,"One proposal is that the ultrastructural changes induced by the type IV collagen variants, perhaps under the influence of modifier genes such as laminin, result in impaired podocyte attachment to the glomerular basement membrane which leads to accelerated podocyte detachment, subsequent foot process effacement as a response to the increased shear stress induced by the denuded basement membrane and at a critical level of podocyte loss collapse of the capillary network with the appearance of the classical segmental sclerotic lesion <REF-2> , <REF-4> , <REF-19> .","One proposal is that the ultrastructural changes induced by the collagen IV variants, perhaps under the influence of modifier genes such as laminin, result in impaired podocyte attachment to the glomerular basement membrane which leads to accelerated podocyte detachment, subsequent foot process effacement as a response to the increased shear stress induced by the denuded basement membrane and at a critical level of podocyte loss collapse of the capillary network with the appearance of the classical segmental sclerotic lesion <REF-2> , <REF-4> , <REF-17> .","Modify,Clarity",Clarity
9730,8-1204,8-1204_v2_38@2,8-1204_v1_36@2,"It also remains unclear as to whether the changes of FSGS are a secondary process occurring in those with TBMN or whether the type IV collagen variants are capable of causing primary FSGS <REF-7> , <REF-20> .","It also remains unclear as to whether the changes of FSGS are a secondary process occurring in those with TBMN or whether the collagen 4 variants are capable of causing primary FSGS <REF-7> , <REF-18> .","Modify,Clarity",Clarity
9731,8-1204,8-1204_v2_38@3,8-1204_v1_36@3,"FSGS occurring as a secondary process to other basement membrane abnormalities may explain why immunosuppressive therapy has traditionally been less effective in inherited forms of FSGS, although there are case reports of the successful use of the calcineurin inhibitor cyclosporine for some patients harbouring inheritable type IV collagen disorders <REF-6> .","FSGS occurring as a secondary process to other basement membrane abnormalities may explain why immunosuppressive therapy has traditionally been less effective in inherited forms of FSGS, although there are case reports of the successful use of the calcineurin inhibitor cyclosporine for some patients harbouring inheritable collagen 4 disorders <REF-6> .","Modify,Clarity",Clarity
9732,8-1204,8-1204_v2_39@1,8-1204_v1_37@1,"This may have potentially led to inadvertently overlooking characteristic basement membrane abnormalities, which may suggest an underlying and heritable type IV collagen disorder.","This may have potentially led to inadvertently overlooking characteristic basement membrane abnormalities, which may suggest an underlying and heritable collagen IV disorder.","Modify,Clarity",Clarity
9733,8-1204,8-1204_v2_0@0,8-1204_v1_0@0,The use of electron microscopy in the diagnosis of focal segmental glomerulosclerosis: are current pathological techniques missing important abnormalities in the glomerular basement membrane?,An audit of electron microscopy in the diagnosis of focal segmental glomerulosclerosis: are current pathological techniques missing important abnormalities in the glomerular basement membrane?,"Modify,Other",Other
9734,8-1204,8-1204_v2_9@7,8-1204_v1_9@7,"Variants in COL4A3, COL4A4 and COL4A5 which encode the α3, α4 and α5 chains of type IV collagen respectively, the major constituent of the GBM previously linked to both Alport syndrome and thin basement membrane nephropathy (TBMN) may also underlie cases of FSGS <REF-4> .","Variants in COL4A3, COL4A4 and COL4A5 , which encode the α3, α4 and α5 chains of collagen IV, respectively, the major constituent of the GBM, previously linked to both Alport syndrome and thin basement membrane nephropathy (TBMN), may also underlie cases of FSGS <REF-4> .","Modify,Clarity",Clarity
9735,8-1204,8-1204_v2_10@0,8-1204_v1_10@0,"The relationship between the three renal conditions intertwined around variants in the COL4A genes, Alport syndrome, TBMN, and FSGS is complex and incompletely understood.","The relationship between the three renal conditions intertwined around variants in the collagen IV genes, Alport syndrome, TBMN, and FSGS is complex and incompletely understood.","Modify,Clarity",Clarity
9736,8-1204,8-1204_v2_10@1,8-1204_v1_10@1,"Whilst previously it was believed that patients heterozygous for variants in COL4A3 and COL4A4 would develop TBMN with persistent microscopic haematuria and an otherwise benign prognosis, this traditional thinking has been overturned by the discovery that some pedigrees with these variants will go on to develop significant proteinuria, FSGS lesions and the potential for progressive CKD or ESKD <REF-6> , <REF-7> .","Whilst previously it was believed that patients heterozygous for variants in COL4A3 and COL4A4 would develop TBMN with persistent microscopic haematuria and an otherwise benign prognosis, this traditional thinking has been overturned by the discovery that some pedigrees with these variants will go on to develop significant proteinuria, FSGS lesions and the potential for progressive CKD or ESRD <REF-6> , <REF-7> .","Modify,Fact/Evidence",Fact/Evidence
9737,8-1204,8-1204_v2_10@2,8-1204_v1_10@2,"More recently, targeted gene sequencing of adults thought to have primary FSGS or steroid resistant nephrotic syndrome (the paediatric equivalent) found that pathogenic or likely pathogenic variants in COL4A genes were present in up to 38% of families with familial FSGS, and 3% of those with sporadic FSGS <REF-3> , <REF-8> .","More recently, targeted gene sequencing of adults thought to have primary FSGS or steroid resistant nephrotic syndrome (the paediatric equivalent) found that pathogenic or likely pathogenic variants in collagen IV genes were present in up to 38% of families with familial FSGS, and 3% of those with sporadic FSGS <REF-3> , <REF-8> .","Modify,Clarity",Clarity
9738,8-1204,8-1204_v2_11@4,8-1204_v1_11@4,"The other potential renal lesions that may be caused by COL4A gene variants, Alport syndrome and TBMN, cause thinning, lamellation and fraying of the GBM and may also be associated with podocyte foot process effacement, all of which require electron microscopy to visualise <REF-4> .","The other potential renal lesions that may be caused by collagen IV gene variants, Alport syndrome and TBMN, cause thinning, lamellation and fraying of the GBM and may also be associated with podocyte foot process effacement, all of which require electron microscopy to visualise <REF-4> .","Modify,Clarity",Clarity
9739,8-1204,8-1204_v2_11@11,8-1204_v1_11@8,Given that the pathological diagnosis of FSGS does not routinely require electron microscopy it is therefore conceivable that GBM lesions potentially associated with an underlying type IV collagen variant may have been missed.,Given that the pathological diagnosis of FSGS does not routinely require electron microscopy it is therefore conceivable that GBM lesions potentially associated with an underlying collagen 4 variant may have been missed.,"Modify,Clarity",Clarity
9740,8-1204,8-1204_v2_11@12,8-1204_v1_11@9,This represents an opportunity to reflect on our prior clinical behaviour to see if our multidisciplinary diagnostic practice may require improvement.,This represents an opportunity to audit our prior clinical behaviour to see if our multidisciplinary diagnostic practice may require improvement.,"Modify,Clarity",Clarity
9741,8-1204,8-1204_v2_11@13,8-1204_v1_11@10,We conducted a retrospective cohort analysis of prior renal biopsy results in two tertiary centres to see how many of those that had been given a histological diagnosis of FSGS were sent for subsequent electron microscopy.,We conducted a retrospective audit of prior renal biopsy results in two tertiary centres to see how many of those that had been given a histological diagnosis of FSGS were sent for subsequent electron microscopy.,"Modify,Clarity",Clarity
9742,8-1204,8-1204_v2_14@1,8-1204_v1_14@1,"In addition, of those samples that were subjected to electron microscopy, we reviewed how many displayed evidence of a potential type IV collagen disorder.","In addition, of those samples that were subjected to electron microscopy, we reviewed how many displayed evidence of a potential collagen IV disorder.","Modify,Clarity",Clarity
9812,8-1681,8-1681_v2_25@6,8-1681_v1_25@6,"At the Auckland site, the scan had a TE of 30 ms, flip angle of 62°, a multiband/slice acceleration factor of 3, an in-plane/parallel imaging acceleration factor of 2 and rBW was 1680 Hz/Px.","At the Auckland site, the scan had a TE of 30 ms, flip angle of 62 o , a multiband/slice acceleration factor of 3, an in-plane/parallel imaging acceleration factor of 2 and rBW was 1680 Hz/Px.","Modify,Grammar",Grammar
9813,8-1681,8-1681_v2_31@4,8-1681_v1_31@4,"Subsequently, the time series was convolved with a canonical hemodynamic response function (HRF) determined from previous empirical data <REF-6> and z-standardized.","Subsequently, the time series was convolved with a canonical hemodynamic response function determined from previous empirical data <REF-6> and z-standardized.","Modify,Clarity",Clarity
9814,8-1681,8-1681_v2_33@2,8-1681_v1_33@2,"We then calculated the Pearson correlation between these split time series, r obs’ , and used the Spearman-Brown prophecy formula <REF-42> , <REF-43> to determine, r obs , the expected reliability for the average of all six runs:","We then calculated the Pearson correlation between these split time series, r obs ’, and used the Spearman-Brown prophecy formula <REF-42> , <REF-43> to determine, r obs , the expected reliability for the average of all six runs:","Modify,Grammar",Grammar
9815,8-1681,8-1681_v2_45@5,8-1681_v1_45@5,"CMF curves were very comparable for the two sites ( Figure 3B ), except for somewhat greater CMF at 3T in V1 and V2 in the very central visual field of participants P2 and P3 <REF-46> (see supplementary figure 1A and B <REF-46> )) for individual participants’ pRF size and CMF respectively).","CMF curves were very comparable for the two sites ( Figure 3B ), except for somewhat greater CMF at 3T in V1 and V2 in the very central visual field of participants P2 and P3.","Modify,Fact/Evidence",Fact/Evidence
9816,8-1681,8-1681_v2_48@1,8-1681_v1_48@1,This showed that in V1 and V2 goodness-of-fit was notably greater for the Auckland 3T site than the London 1.5T site.,This showed that in V1 and V2 goodness-of-fit was notably greater for the Auckland 3T site than the London 1.5T site in all 3 participants.,"Modify,Clarity",Clarity
9817,8-1681,8-1681_v2_48@3,8-1681_v1_48@3,"In V3A, the model fits (see Figure 3C last column) at both sites were similar, but generally lower than in the other regions and with greater variability.","In V3A, the model fits at both were similar, but generally lower than in the other regions and with greater variability.","Modify,Fact/Evidence",Fact/Evidence
9818,8-1681,8-1681_v2_48@5,8-1681_v1_48@5,"When we normalized model fits relative to the noise ceiling, ρ o 2, the maximum goodness-of-fit that could theoretically be achieved given the data from each site, we found no systematic difference in goodness-of-fit between sites ( Figure 3D ).","When we normalized model fits relative to the noise ceiling, ρ o 2 , the maximum goodness-of-fit that could theoretically be achieved given the data from each site, we found no systematic difference in goodness-of-fit between sites ( Figure 3B ).","Modify,Grammar",Grammar
9819,8-1681,8-1681_v2_48@6,8-1681_v1_48@6,The curves mostly overlapped for P1 and P2 except in V1 where the London data outperformed the Auckland data (see Figure 2B ).,The curves mostly overlapped for P1 and P2 except in V1 where the London data outperformed the Auckland data.,"Modify,Fact/Evidence",Fact/Evidence
9820,8-1681,8-1681_v2_62@1,8-1681_v1_63@1,"In London, images were projected onto a screen and this necessitated focusing and scaling the projected image to be of the exact size.","In London, images were projected onto a screen and this necessitated focussing and scaling the projected image to be of the exact size.","Modify,Grammar",Grammar
9821,8-1681,8-1681_v2_66@2,8-1681_v1_67@2,"A slower stimulus design where each bar position is stimulated for 2–3 s, or a random instead of an ordered stimulus sequence, could probably counteract this problem but this would come at the expense of longer scanning durations.","A slower stimulus design where each bar position is stimulated for 2-3 s, or a random instead of an ordered stimulus sequence, could probably counteract this problem but this would come at the expense of longer scanning durations.","Modify,Grammar",Grammar
9822,8-1681,8-1681_v2_84@0,8-1681_v1_74@0,Data are available under the terms of the Creative Commons Zero “No rights reserved” data waiver (CC0 1.0 Public domain dedication).,"Data are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).","Modify,Grammar",Grammar
10048,8-1983,8-1983_v2_4@1,8-1983_v1_4@1,"Furthermore, we observed that this method yields an average range of neurospheres sizes greater than 50 μm, but less than 100 μm after 7 DIV.","Furthermore, we found this method yields different sizes of neurospheres.","Modify,Fact/Evidence",Fact/Evidence
10049,8-1983,8-1983_v2_4@2,8-1983_v1_4@2,"Lastly, using an anti-GFAP antibody, we show that these neurospheres can be stained, confirming their use in future immunocytochemistry studies.","Lastly, using an anti-GFAP antibody, we confirm that these neurospheres can be used for immunocytochemistry studies.","Modify,Clarity",Clarity
10050,8-1983,8-1983_v2_75@2,8-1983_v1_73@2,"Using similar analyses, we found significant differences between the size classification of neurospheres that were less than 50 μm, between 50–100 μm, and greater than 100 μm (F (2,379) = 424; p < 0.0001) ( Figure 3B ).","Using similar analyses, we found significant differences between the size classification of neurospheres that were less than 50 μm, between 50-100 μm, and greater than 100 μm (F (2,379) = 424; p < 0.0001) ( Figure 3B ).","Modify,Grammar",Grammar
10051,8-1983,8-1983_v2_75@3,8-1983_v1_73@3,Post hoc analysis using Bonferroni’s multiple comparison revealed a significant difference between the primary neurospheres that were greater than 100 μm compared to neurospheres that were less than 50 μm (p <0.0001) or between 50–100 μm (p <0.0001) ( Figure 3B ).,Post hoc analysis using Bonferroni’s multiple comparison revealed a significant difference between the primary neurospheres that were greater than 100 μm compared to neurospheres that were less than 50 μm (p <0.0001) or between 50-100 μm (p <0.0001) ( Figure 3B ).,"Modify,Grammar",Grammar
10052,8-1983,8-1983_v2_75@4,8-1983_v1_73@4,"Similarly, we found a substantial difference between primary neurospheres that were greater than 100 μm compared to neurospheres that were between 50–100 μm (p <0.0001) ( Figure 3B ).","Similarly, we found a substantial difference between primary neurospheres that were greater than 100 μm compared to neurospheres that were between 50 -100 μm (p <0.0001) ( Figure 3B ).","Modify,Grammar",Grammar
10053,8-1983,8-1983_v2_7@2,8-1983_v1_7@2,"Since its introduction some 25 years ago <REF-2> , neurospheres have been used to study neurogenesis <REF-3> , genes that regulate self-renewal <REF-4> , <REF-5> , and molecular mechanisms that control neuronal and glial differentiation <REF-3> , <REF-6> – <REF-9> .","Since its introduction some 25 years ago <REF-2> , neurospheres have been used to study neurogenesis <REF-3> , genes that regulate self-renewal <REF-4> , and molecular mechanisms that control neuronal and glial differentiation <REF-3> , <REF-5> – <REF-8> .","Modify,Fact/Evidence",Fact/Evidence
10054,8-1983,8-1983_v2_79@1,8-1983_v1_77@1,Figure 4 is a picture of a small ( Figure 4A-B ; arrowhead) and a larger ( Figure 4C ; arrowhead) primary neurosphere immunostained using an anti-GFAP antibody and counterstained with DAPI (Figure B; arrowhead).,Figure 4 is a picture of a primary neurosphere immunostained using an anti-GFAP ( Figure 4A ; arrowhead) antibody and counterstained with DAPI (Figure B; arrowhead).,"Modify,Fact/Evidence",Fact/Evidence
10055,8-1983,8-1983_v2_7@3,8-1983_v1_7@3,"Although there are many neurosphere protocols, the expected number and size of neurospheres generated after a week in vitro is not entirely characterized.","Although there are many neurosphere protocols, the expected number and size of neurospheres generated after a week in vivo is not fully understood.","Modify,Clarity",Clarity
10058,8-1983,8-1983_v2_92@0,8-1983_v1_89@0,- Spreadsheet containing numbers and sizes per field of view.xlsx (details on numbers and sizes of neurospheres produced from each mouse).,- Spreadsheet containing numbers and sizes per field of view.xlsx (details on numbers and sizes of neurospeheres produced from each mouse).,"Modify,Grammar",Grammar
10059,8-1983,8-1983_v2_8@3,8-1983_v1_8@3,The area surrounding the ventricle is then microdissected from a given slice of tissue to enrich for neural stem/progenitor cells.,The area surrounding the ventricle is then microdissected from a given slice to enrich for neural stem/progenitor cells.,"Modify,Clarity",Clarity
10060,8-1983,8-1983_v2_8@7,8-1983_v1_8@5,"In contrast, our approach requires no specialized equipment.","In our approach, no specialized equipment is needed.","Modify,Clarity",Clarity
10061,8-1983,8-1983_v2_8@9,8-1983_v1_8@7,"This method requires only half of a single brain, and generates reproducible numbers of neurospheres in a few days.","This method requires only half of a single brain, and generates reproducible numbers of neurospheres after a week.","Modify,Fact/Evidence",Fact/Evidence
10062,8-1983,8-1983_v2_2@2,8-1983_v1_2@2,The objective of this protocol is to provide a stepwise method from a single isolation that predicts the average number of neurospheres generated and to estimate an approximation of its sizes after several days in vitro .,This method predicts the average number of neurospheres and provides an approximation of its expected size after 7 days in vitro .,"Modify,Fact/Evidence",Fact/Evidence
10063,8-1983,8-1983_v2_2@4,8-1983_v1_2@3,"Estimates about the numbers and sizes of neurospheres will provide investigators with quantitative data to advise on how much starting dLGE tissue is required to generate the appropriate number of spheres for the implementation of downstream applications, including immunocytochemistry, self-renewal and differentiation assays.","Characterization of numbers and sizes will provide investigators with quantitative data to advise on the implementation of downstream applications, including immnocytochemistry, self-renewal and differentiation assays.","Modify,Claim",Claim
10064,8-1983,8-1983_v2_19@0,8-1983_v1_19@0,Images were taken with a Canon EOS Rebel XS camera.,Images were taken with a Canon EOS Rebel XS.,"Modify,Clarity",Clarity
10065,8-1983,8-1983_v2_19@7,8-1983_v1_19@1,The field of view represents a length of 783 μm and a width of 522 μm.,The field of view represent a length of 783 μm and a width of 522 μm.,"Modify,Grammar",Grammar
10066,8-1983,8-1983_v2_19@9,8-1983_v1_19@3,"Per animal, 3–4 wells were analyzed.","Per animal, 3-4 wells were analyzed.","Modify,Grammar",Grammar
10067,8-1983,8-1983_v2_19@10,8-1983_v1_19@4,A total of 5 individual animals were analyzed.,A total of 5 individual animal were analyzed.,"Modify,Grammar",Grammar
10068,8-1983,8-1983_v2_19@14,8-1983_v1_19@8,"If the main effect was significant (p < 0.05), Bonferroni’s multiple comparison post hoc test were used to compare the different replicates.","If the main effect was significant (p < 0.05), Bonferroni’s multiple comparision post hoc test were used to compare the different replicates.","Modify,Grammar",Grammar
10175,8-42,8-42_v2_39@1,8-42_v1_37@0,"As the length of this object attribute is 1, in this example the sequence mapping identified a single target sequence.","In this example, the sequence mapping identified one target sequence.","Modify,Fact/Evidence",Fact/Evidence
10176,8-42,8-42_v2_39@2,8-42_v1_37@1,From this object further information can be obtained as follows:,From the seq_annotation object further information can be obtained as follows:,"Modify,Clarity",Clarity
10179,8-42,8-42_v2_92@3,8-42_v1_90@3,We will also continue to update the package and the API in sync with the OMA browser to incorporate new functionalities of OMA.,"We will also continue to update the package and API to incorporate new functionalities of OMA, such as support for local synteny which is currently under development.","Modify,Claim",Claim
10180,8-42,8-42_v2_14@0,8-42_v1_14@0,"Most data available through the OMA browser is now also accessible via the API, with the exception of the local synteny data.",Most data available through the OMA browser is now also accessible via the API.,"Modify,Fact/Evidence",Fact/Evidence
10181,8-42,8-42_v2_20@1,8-42_v1_20@1,"In the results section we showcase usage of the latest version of the package (v2.0), which requires R version >= 3.6 and Bioconductor version >= 3.9.","The package requires R version >= 3.6 and Bioconductor version >= 3.9, as well as a stable internet connection.","Modify,Fact/Evidence",Fact/Evidence
10182,8-42,8-42_v2_24@0,8-42_v1_24@0,"For Python users, we provide an analogous package named omadb .","For Python users, we provide an analogous package also named omadb .","Modify,Clarity",Clarity
10267,8-52,8-52_v2_24@0,8-52_v1_23@0,Homology analysis between PD_CbNPR1 and its reference sequence via BLAST search showed 99% identity.,Homology analysis between PD_CbNPR1 and its reference sequence via BLAST search showed 99% homology.,"Modify,Clarity",Clarity
10268,8-52,8-52_v2_3@2,8-52_v1_3@2,"Identification of a cis -acting element was detected by PLACE, PlantCare, and PlantPAN.",Identification of a cis -acting element was detected by PLACE.,"Modify,Fact/Evidence",Fact/Evidence
10269,8-52,8-52_v2_33@3,8-52_v1_31@3,"The CAACA pattern was also found by Kagaya et al . (1999) in Arabidopsis thaliana , while the TGTTG pattern is similar to the OsNPR1 promoter ( Hwang & Hwang, 2010 ).","The CAACA pattern was also found by Kagaya et al . (1999) in Arabidopsis thaliana , while the TGTTG pattern is similar to the OsNPR1 promoter (Hwang dan Hwang, 2010).","Modify,Grammar",Grammar
10270,8-52,8-52_v2_4@1,8-52_v1_4@1,BLASTn search analysis indicated that PD_CbNPR1 sequence is highly conserved (99% identity) showing only a single nucleotide polymorphism (SNP) (base substitution) compared with its reference sequence.,BLASTn search analysis indicated that PD_CbNPR1 sequence is highly conserved (99% homology) showing only a single nucleotide polymorphism (SNP) (base substitution) compared with its reference sequence.,"Modify,Clarity",Clarity
10271,8-52,8-52_v2_36@0,8-52_v1_34@0,"We also found a gibberellin-responsive element (GARE) motif, which has been previously reported by Ogawa et al . (2003) as the binding site with some transcription factors induced by gibberellic acid.","We also found a gibberelline-responsive element (GARE) motif, which has been previously reported by Ogawa et al . (2003) as the binding site with some transcription factors induced by gibberellic acid.","Modify,Grammar",Grammar
10272,8-52,8-52_v2_7@0,8-52_v1_7@0,The non-expressor of pathogenesis related gene 1 ( NPR1 ) protein is the main regulator in the systemic acquired resistance response of many plants.,The non-expressor of pathogenesis related gene 1 ( NPR1 ) protein is a main regulator in the systemic acquired resistance response of many plants.,"Modify,Grammar",Grammar
10273,8-52,8-52_v2_7@1,8-52_v1_7@1,Overexpression by modifying the distal promoter in the W-box element of the OsNPR1 gene in rice could increase its resistance against Xanthomonas oryzae pv.,Over expression by modifying distal promoter in the W-box element of the OsNPR1 gene in rice could increase its resistance against Xanthomonas oryzae pv.,"Modify,Grammar",Grammar
10274,8-52,8-52_v2_11@3,8-52_v1_11@1,The plant was grown in a greenhouse and maintained for 8 weeks before being used for DNA isolation.,The chili pepper genotype Berangkai was grown in a greenhouse and maintained for 8 weeks before being used for DNA isolation.,"Modify,Clarity",Clarity
10275,8-52,8-52_v2_2@1,8-52_v1_2@1,"The cis -acting elements of its distal promoter gene are characterized by salicylic acid inducing elements such as the W-box, RAV1AAT and ASF1, accompanied by enhancer and silencer elements.","The cis -acting elements of its distal promoter gene are characterized by salicylic acid inducing elements such as the W-box, RAV1AAT and ASF1, accompanied with enhancer and silencer elements.","Modify,Grammar",Grammar
10276,8-52,8-52_v2_13@11,8-52_v1_13@11,The final extension was maintained at 72°C for 5 minutes.,Final extension was maintained at 72°C for 5 minutes.,"Modify,Grammar",Grammar
10277,8-52,8-52_v2_17@5,8-52_v1_17@5,"The cis -acting elements were identified using PLACE , developed by Higo et al . (1999) , PlantCARE ( Lescot et al ., 2002 ) and PlantPAN .","The cis -acting elements were identified using PLACE , developed by Higo et al . (1999) .","Modify,Fact/Evidence",Fact/Evidence
10537,8-80,8-80_v2_20@2,8-80_v1_20@2,"A rim weighting procedure was run against the population figures from the most recent national census to construct weight variables, with the procedure executed separately for each country.","A rim weighting procedure was run against the population figures to construct weight variables, with the procedure executed separately for each country.","Modify,Clarity",Clarity
10538,8-80,8-80_v2_7@0,8-80_v1_7@0,"While the worldwide rate of tobacco smoking has declined substantially in recent years, the absolute number of people currently smoking has increased from approximately 720 million smokers in 1980 to an estimated 1.1 billion today, the consequence of population growth outpacing declining smoking prevalence in many low and middle income countries (LMICs) <REF-1> – <REF-3> .","While the worldwide rate of tobacco smoking has declined substantially in recent years, the absolute number of people currently smoking has increased from approximately 720 million smokers in 1980 to an estimated 1.1 billion today, the consequence of population growth outpacing declining smoking prevalence in many low and middle income countries <REF-1> – <REF-3> .","Modify,Clarity",Clarity
10539,8-80,8-80_v2_35@0,8-80_v1_35@0,"In almost all countries, more current smokers than ex-smokers or nonsmokers were surrounded by people who also smoked, including parents, spouse/partner, close friends and colleagues ( Figure 5a–d ).","In almost all countries, more current smokers than ex-smokers or nonsmokers were surrounded by people who also smoked, including parents, spouse/partner, close friends and colleagues ( Figure 5a-d ).","Modify,Grammar",Grammar
10540,8-80,8-80_v2_40@2,8-80_v1_40@2,"Additionally, more than 60% of smokers and ex-smokers in India, Malawi and Brazil, had bought cigarettes when they knew the money could be spent better on household essentials like food.","Additionally, between 21% (Japan) and 87% (Brazil) of smokers and ex-smokers had bought cigarettes when they knew the money could be spent better on household essentials like food.","Modify,Fact/Evidence",Fact/Evidence
10541,8-80,8-80_v2_88@1,8-80_v1_88@1,RYO cigarettes vary in composition but have been shown to cause comparable exposure to known and suspected carcinogens <REF-15> .,"RYO cigarettes vary in composition but have been shown to have higher tar yields than boxed cigarettes, as well as comparable exposure to known and suspected carcinogens <REF-15> .","Modify,Fact/Evidence",Fact/Evidence
10542,8-80,8-80_v2_15@2,8-80_v1_15@2,"Smokers were defined as those who responded that they currently smoked cigarettes, cigars, cigarillos or a pipe (or bidis in India) “regularly” or “occasionally;” ex-smokers were defined as those who responded that they used to smoke but stopped; and never smokers were defined as those who responded that they had never smoked.","Smokers were defined as those who responded that they currently smoked cigarettes, cigars, cigarillos or a pipe “regularly” or “occasionally;” ex-smokers were defined as those who responded that they used to smoke but stopped; and never smokers were defined as those who responded that they had never smoked.","Modify,Clarity",Clarity
10543,8-80,8-80_v2_96@2,8-80_v1_96@2,Compared to other nicotine-non-tobacco products ENDS most closely simulate smoking regular cigarettes in how they are used.,"Compared to other nicotine, non-tobacco products, ENDS most closely simulate smoking regular cigarettes in how they are used.","Modify,Grammar",Grammar
10544,8-80,8-80_v2_106@0,8-80_v1_106@0,Data are available under the terms of the Creative Commons Zero “No rights reserved” data waiver (CC0 1.0 Public domain dedication).,"Data are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).","Modify,Grammar",Grammar
10783,9-1088,9-1088_v2_30@7,9-1088_v1_21@7,"The covariates related to diet, hygiene and culture were stable in this period and, thus, they could be safely assumed as such.","The covariates related to diet, hygiene and culture were stable in this period and, thus, they could be safely assumed.","Modify,Clarity",Clarity
10784,9-1088,9-1088_v2_32@2,9-1088_v1_23@2,"Our work postulates that there is an inverse relation in two time series, between the timing of sunspot numbers and stroke deaths, a hypothesis posed by previous investigations ( Geronikolou & Leontitsis, 2005 ; Geronikolou & Petropoulos, 1996 ; Stoupel et al., 1999 ).","Our work postulates that there is an inverse relation in two time series, between the timing of sunspot numbers and stroke deaths, a hypothesis posed by previous investigations ( Geronikolou & Leontisis, 2005 ; Geronikolou & Petropoulos, 1996 ; Stoupel et al., 1999 ).","Modify,Grammar",Grammar
10785,9-1088,9-1088_v2_33@2,9-1088_v1_24@2,"The molecular interactions network approach, where the inter-species functional interactome of nuclear steroid receptors (R1) constructed on orthologues was employed ( Geronikolou et al., 2018 ).","The molecular interactions network approach, where the inter-species functional interactome of nuclear steroid receptors (R1) was constructed on orthologues was employed ( Geronikolou et al., 2018 ).","Modify,Grammar",Grammar
10786,9-1088,9-1088_v2_33@3,9-1088_v1_24@3,"R1 has interspecies dimensions and thus has evolutionary value extending from insects to humans, that is, from early life eras till now.","R1 has interspecies dimensions and thus has evolutionary and historical value extending from insects to humans, that is, from early life eras till now.","Modify,Clarity",Clarity
10787,9-1088,9-1088_v2_33@6,9-1088_v1_24@6,"R1 includes genes and their products involved in circadian rhythms, while its major hub NCOR1 in macrophages blocks the pro-atherogenic functions of peroxisome proliferator-activated receptor gamma (PPARγ) ( Oppi et al., 2020 ), greatly implicating stroke pathophysiology.","R1 includes genes and their products involved in circadian rhythms, while its major hub NCOR1 in macrophages blocks the pro-atherogenic functions of peroxisome proliferator-activated receptor gamma (PPARγ) in atherosclerosis ( Oppi et al., 2020 ), greatly implicating stroke pathophysiology.","Modify,Clarity",Clarity
10788,9-1088,9-1088_v2_33@8,9-1088_v1_24@8,"It is likely that as soon as R1 is disrupted, the atherogenic and/or other pathological processes progress dramatically, with potentially lethal consequences.","It is likely that as soon as R1 is disrupted, the atherogenic and/or other pathological processes progress dramatically, with lethal consequences.","Modify,Claim",Claim
10789,9-1088,9-1088_v2_35@4,9-1088_v1_26@4,"Thus, future medical practice should probably take account of chronopathology ( Stienen et al., 2015 ) so as to prevent stroke mortality shifts (chronotherapy and chronoprevention plans).","Thus, future medical practice should probably take account of chronopathology so as to prevent stroke mortality shifts (chronotherapy and chronoprevention plans).","Modify,Fact/Evidence",Fact/Evidence
10790,9-1088,9-1088_v2_37@0,9-1088_v1_28@0,"Our work clearly established that sunspot numbers and stroke mortality were inversely correlated, and that a violent fluctuation of sunspot numbers over 35% shifted monthly mortality with a phase delay of two months.","Our work established clearly that of sunspot numbers and stroke mortality were inversely correlated, and that a violent fluctuation of sunspot numbers over 35% shifted monthly mortality to a phase delay of two months.","Modify,Clarity",Clarity
10791,9-1088,9-1088_v2_14@0,9-1088_v1_5@0,"Stroke has been previously associated with solar activity ( Halberg et al., 2001 ; Otsuka et al., 2001 ; Stoupel et al., 1995 ; Stoupel et al., 1996 ).","Stroke has been previously associated to solar activity ( Halgberg et al., 2001 ; Otsuka et al., 2001 ; Stoupel et al., 1995 ; Stoupel et al., 1996 ).","Modify,Grammar",Grammar
10792,9-1088,9-1088_v2_15@0,9-1088_v1_6@0,"Our aim was to investigate the dynamics and trends in the selected time series, to determine sunspot numbers vs. daily and monthly stroke deaths in synchronized periodicities with gradual time delays (chronomes), and to define a sunspot number threshold beyond which stroke death events may be influenced.","Our aim was to investigate the dynamics and trends in the time series, to determine sunspot numbers vs. daily and monthly stroke deaths in synchronized periodicities with gradual time delays (chronomes), and to define a sunspot number threshold for presence of stroke mortality.","Modify,Clarity",Clarity
10793,9-1088,9-1088_v2_17@0,9-1088_v1_8@0,"In this study we focused on monthly stroke mortality events between 1985 and 1989, based on the underlined cause of death data (ICD-9 Table 5.3: recode 430, Table 5.4: recode 200) from the archives of Piraeus Civil Registry ( Geronikolou & Zikos, 1991 ).","In this study we focused on monthly stroke mortality events between 1985 and 1989, based on the underlined cause of death data from the archives of Piraeus Civil Registry ( Geronikolou & Zikos, 1991 ).","Modify,Fact/Evidence",Fact/Evidence
10794,9-1088,9-1088_v2_19@0,9-1088_v1_10@0,"The stroke death rate in Piraeus, was calculated using the formula (number of all deaths per year per 1000 people in June 30 th , year x).","The stroke death rate in Piraeus, was calculated over the formula (number of all deaths per year per 1000 people in June 30 th , year x).","Modify,Grammar",Grammar
10795,9-1088,9-1088_v2_19@1,9-1088_v1_10@1,"The overall death rate was calculated with the denominators provided by the 1981 census ( Geronikolou & Zikos, 1991 ).",The overall death rate was calculated with the denominators provided by the 1981 census.,"Modify,Fact/Evidence",Fact/Evidence
10796,9-1088,9-1088_v2_23@4,9-1088_v1_14@4,"Thus, monthly sunspot numbers by squared root variation and their violent fluctuation of over 35% was correlated to monthly stroke mortality, establishing a negative correlation between the two time-series (sunspot numbers and deaths by stroke) ( Figure 1b, d ).","Thus, monthly sunspot numbers by squared root variation and their violent fluctuation of over 35% was correlated to monthly stroke mortality, establishing a negative correlation between the two time-series (sunspot numbers and deaths of strokes) ( Figure 1b, d ).","Modify,Grammar",Grammar
10799,9-1088,9-1088_v2_29@4,9-1088_v1_20@4,"Socioeconomic and geographic disparities have been suspected, while heliomagnetic influences have been proposed as possible etiologic contributors to human pathology ( Halberg et al. , 1998 ; Stienen et al. , 2015 ; Stoupel et al. , 1996 ; Stoupel et al. , 2003 ).","Socioeconomic and geographic disparities have been suspected, while heliomagnetic influences have been proposed as possible etiologic contributors to human pathology.","Modify,Fact/Evidence",Fact/Evidence
10800,9-1088,9-1088_v2_30@2,9-1088_v1_21@2,"Moreover, its population is representative of the urban populations in Greece ( Geronikolou & Zikos 1991 ).","Moreover, its population is representative of the urban populations in Greece ( Geronikolou, 1991 ).","Modify,Fact/Evidence",Fact/Evidence
10801,9-1088,9-1088_v2_30@5,9-1088_v1_21@5,"The data used in this study were original and based on the reported underlined cause of death ( Geronikolou & Zikos, 1991 ).","The data used in this study were original and based on the underlined cause of death ( Geronikolou, 1991 ).","Modify,Fact/Evidence",Fact/Evidence
10918,9-1193,9-1193_v2_29@1,9-1193_v1_29@1,"Overall, 46.5% of the trials intended to use mortality as a primary (n=98) or secondary outcome (n=220; Table 1 ), and 53•4% (n=365) did not specify mortality as an outcome.","Overall, 46.4% of the trials intended to use mortality as a primary (n=98) or secondary outcome (n=369; Table 1 ).","Modify,Fact/Evidence",Fact/Evidence
10919,9-1193,9-1193_v2_29@2,9-1193_v1_29@2,"Out of the 521 inpatient trials, 55.7% (n=290) planned on reporting mortality as an outcome.","Out of the 525 inpatient trials, 55.6% (n=292) planned on reporting mortality as an outcome.","Modify,Fact/Evidence",Fact/Evidence
10920,9-1193,9-1193_v2_31@0,9-1193_v1_31@0,"Out of the 683 trials, 602 (88.1%) assessed treatment interventions (186,189 planned patients); drugs were more frequent (345 trials [57.3%]), encompassing a vast range of substances.","Out of the 689 trials, 607 (88.1%) assessed treatment interventions (187,209 planned patients); drugs were more frequent (349 trials [57.5%]), encompassing a vast range of substances.","Modify,Fact/Evidence",Fact/Evidence
10921,9-1193,9-1193_v2_31@1,9-1193_v1_31@1,The two most common pharmacological classes were antiviral drugs (assessed in 141 trials; e.g. lopinavir/ritonavir [n=45]) and antimalarial drugs (111 trials; e.g. hydroxychloroquine [n=84]).,The two most common pharmacological classes were antiviral drugs (assessed in 144 trials; e.g. lopinavir/ritonavir [n=45]) and antimalarial drugs (112 trials; e.g. hydroxychloroquine [n=83]).,"Modify,Fact/Evidence",Fact/Evidence
10922,9-1193,9-1193_v2_31@2,9-1193_v1_31@2,"There were 106 trials investigating traditional medicine and 70 exploring highly diverse pharmaceuticals of various classes, e.g. bismuth potassium citrate, ebastine, pirfenidone, dipyridamole and hydrogen peroxidase.","There were 106 trials investigating traditional medicine and 70 exploring highly diverse pharmaceuticals of various classes, e.g. bismuth potassium citrate, ebastine, pirfenidone, dipyridamole and hydrogen peroxidase ( Figure 1 and see Extended data ) <REF-12> .","Modify,Fact/Evidence",Fact/Evidence
10923,9-1193,9-1193_v2_31@5,9-1193_v1_31@3,"The comparators were predominantly standard of care or no intervention (47.2% [n=284]), placebo (17.1% [n=103]) or other interventions (17.9%; [n=108]) ( Table 1 ).","The comparators were predominantly standard of care or no intervention (47.1% [n=286]), placebo (17% [n=103]) or other interventions (18%; [n=109]) ( Table 1 ).","Modify,Fact/Evidence",Fact/Evidence
10924,9-1193,9-1193_v2_35@0,9-1193_v1_35@0,"Overall, 77 trials (11.3%) focused on prevention (204,641 planned participants), mainly prophylactic drug use (n=41), vaccines (n=14; 9 already started recruitment; see Extended data ) <REF-12> and non-pharmaceutical interventions (n=10) (e.g. masks or the use of media and influencers in people’s compliance to hygienic practices).","Overall, 78 trials (11.3%) focused on prevention (205,841 planned participants), mainly prophylactic drug use (n=41), vaccines (n=14; 9 already started recruitment; see Extended data ) <REF-12> and non-pharmaceutical interventions (n=10) (e.g. masks or the use of media and influencers in people’s compliance to hygienic practices).","Modify,Fact/Evidence",Fact/Evidence
10925,9-1193,9-1193_v2_37@0,9-1193_v1_37@0,"The number of trials increased rapidly; on average 0.5 trials per day were registered in January, 8.1 in February, 8.3 in March, and 17.2 in April 2020.","The number of trials increased rapidly; on average 0.5 trials per day were registered in January, 8.1 in February, 8.4 in March, and 17.6 in April 2020.","Modify,Fact/Evidence",Fact/Evidence
10926,9-1193,9-1193_v2_38@0,9-1193_v1_38@0,Trials were conducted in 41 countries and through international collaborations ( Table 1 ; see Extended data ) <REF-12> .,Trials were conducted in 42 countries and through international collaborations ( Table 1 ; see Extended data ) <REF-12> .,"Modify,Fact/Evidence",Fact/Evidence
10927,9-1193,9-1193_v2_38@1,9-1193_v1_38@1,"Half were from China (51.4% [n=351]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.","Half were from China (51.1% [n=352]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.","Modify,Fact/Evidence",Fact/Evidence
10928,9-1193,9-1193_v2_38@2,9-1193_v1_38@2,"Trial characteristics were similar across the five most frequent geographical locations (China, USA, France, Spain and international) contributing to 73.6% (n=503) of the global trial research ( Table 1 ).","Trial characteristics were similar across the five most frequent geographical locations (China, USA, France, Spain and international) contributing to 73.3% (n=505) of the global trial research ( Table 1 ).","Modify,Fact/Evidence",Fact/Evidence
10929,9-1193,9-1193_v2_38@3,9-1193_v1_38@3,Traditional medicine was assessed in 30.5% of trials from China (n=107) but rarely in other countries.,Traditional medicine was assessed in 30.4% of trials from China (n=107) but rarely in other countries.,"Modify,Fact/Evidence",Fact/Evidence
10930,9-1193,9-1193_v2_41@1,9-1193_v1_41@1,"In February, 5.1% of trials included more than 500 participants in contrast to 18.6% of trials in March ( Figure 3 ).","In February, fewer than 8% of trials included more than 500 participants in contrast to 29.6% of trials in March ( Figure 3 ).","Modify,Fact/Evidence",Fact/Evidence
10931,9-1193,9-1193_v2_41@4,9-1193_v1_41@4,"When the proportion of trials from China decreased, so did trials assessing traditional medicine (from 46.9% to 0.9%) while the proportion of trials assessing drugs rose (from 40.6% to 77.7%).","When the proportion of trials from China decreased, so did trials assessing traditional medicine (from 46.9% to 0.9%) while the proportion of trials assessing drugs rose (from 38.1% to 77.2%).","Modify,Fact/Evidence",Fact/Evidence
10932,9-1193,9-1193_v2_45@0,9-1193_v1_45@0,"Out of the 683 trials, 6.6% (n=45) planned to enroll 1,000 to 5,000 participants.","Out of the 689 trials, 6.7% (n=46) planned to enroll 1,000 to 5,000 participants.","Modify,Fact/Evidence",Fact/Evidence
10933,9-1193,9-1193_v2_45@1,9-1193_v1_45@1,"Most were randomized (88.9% [n=40]), assessed drugs (80%; n=36), and many were not blinded (53.3% [n=24]).","Most were randomized (89.1% [n=41]), assessed drugs (80.4%; n=37), and many were not blinded (52.2% [n=24]).","Modify,Fact/Evidence",Fact/Evidence
10934,9-1193,9-1193_v2_45@3,9-1193_v1_45@3,"The top three regions were the United States (22.2%; n=10), France (11.1% [n=5]) and international collaborations (11.1% [n=5]) (see Extended data ) <REF-12> .","The top three regions were the United States (21.7%; n=10), France (13% [n=6]) and international collaborations (10.9% [n=5]) (see Extended data ) <REF-12> .","Modify,Fact/Evidence",Fact/Evidence
10935,9-1193,9-1193_v2_46@0,9-1193_v1_46@0,"Eleven (1.6%) trials, registered between February and April 2020 (seven for treatment and four for prevention), planned to enroll over 5,000 participants (see Extended data ) <REF-12> .","Eleven (1.6%) trials, registered between February and April 2020, planned to enroll over 5,000 participants (see Extended data ) <REF-12> .","Modify,Fact/Evidence",Fact/Evidence
10936,9-1193,9-1193_v2_47@0,9-1193_v1_47@0,Five drug interventions tested in these 11 larger trials were simultaneously investigated in over 20 smaller trials (see Extended data ) <REF-12> .,Six drug interventions tested in these 11 larger trials (seven for treatment and four for prevention) were simultaneously investigated in at least 10 smaller trials (see Extended data ) <REF-12> .,"Modify,Fact/Evidence",Fact/Evidence
10937,9-1193,9-1193_v2_47@1,9-1193_v1_47@1,"Overall, 167 trials (141 for treatment, 24 for prevention and two for treatment and prevention) with fewer than 5,000 participants assessed at least one intervention that was also assessed in a larger trial (median sample size 223 [IQR 80 to 540]; 132 had fewer than 1000 participants).","Overall, 169 trials (143 for treatment, 24 for prevention and two for treatment and prevention) with fewer than 5,000 participants assessed at least one intervention that was also assessed in a larger trial (median sample size 273 [IQR 90 to 700]; 134 had fewer than 1000 participants).","Modify,Fact/Evidence",Fact/Evidence
10938,9-1193,9-1193_v2_47@2,9-1193_v1_47@2,For 103 of those (61.7%) the larger trial was registered before.,For 107 of those (63.5%) the larger trial was registered before.,"Modify,Fact/Evidence",Fact/Evidence
10939,9-1193,9-1193_v2_47@3,9-1193_v1_47@3,"For example, 104 trials with fewer than 5,000 participants tested hydroxychloroquine and 86 of them (82.7%) were registered after the first large trial testing this drug and 82 (78.8%) assessed hydroxychloroquine as treatment ( Figure 4 ; see Extended data ) <REF-12> .","For example, 106 trials with fewer than 5,000 participants tested hydroxychloroquine and 88 of them (83%) were registered after the first large trial testing this drug and 83 (77.6%) assessed hydroxychloroquine as treatment ( Figure 4 ; see Extended data ) <REF-12> .","Modify,Fact/Evidence",Fact/Evidence
10940,9-1193,9-1193_v2_47@4,9-1193_v1_47@4,"These 104 trials had a median sample size of 334, but cumulatively, they planned to enroll as many patients as the four larger trials testing hydroxychloroquine (75,217 vs 77,000).","These 106 trials had a median sample size of 334, but cumulatively, they planned to enroll as many patients as the four larger trials testing hydroxychloroquine (76,617 vs 77,000).","Modify,Fact/Evidence",Fact/Evidence
10941,9-1193,9-1193_v2_51@0,9-1193_v1_51@0,"By the end of 2020, 413 trials (60.5%) with a total of 159,957 planned participants were expected to be completed (i.e. last patient, last visit), including 232 drug trials (97,282 participants) and 22 over-1,000 participants trials and five over-5,000 participants trials.","By the end of 2020, 414 trials (60.1%) with a total of 160,107 planned participants were expected to be completed (i.e. last patient, last visit), including 240 drug trials (97,846 participants) and 22 over-1,000 participants trials and five over-5,000 participants trials.","Modify,Fact/Evidence",Fact/Evidence
10942,9-1193,9-1193_v2_54@3,9-1193_v1_54@3,"Few trials focused on prevention, but some were very large and focused on healthcare workers (i.e. 24.3% of planned trial participants are healthcare workers).","Few trials focused on prevention, but some were very large and focused on healthcare workers (i.e. 24.4% of planned trial participants are healthcare workers).","Modify,Fact/Evidence",Fact/Evidence
10943,9-1193,9-1193_v2_55@0,9-1193_v1_55@0,The emergence of 683 trials in a 100-day period is unparalleled.,The emergence of 689 trials in a 100-day period is unparalleled.,"Modify,Fact/Evidence",Fact/Evidence
10944,9-1193,9-1193_v2_56@0,9-1193_v1_56@0,"Thomas Chalmers highlighted in 1977 the need to ‘Randomize the first patient’ <REF-19> , and a reassuring 75.5% of trials are indeed randomized.","Thomas Chalmers highlighted in 1977 the need to ‘Randomize the first patient’ <REF-19> , and a reassuring 75% of trials are indeed randomized.","Modify,Fact/Evidence",Fact/Evidence
10945,9-1193,9-1193_v2_56@3,9-1193_v1_56@3,"Blinding may not be required for mortality outcomes; however, mortality was rarely a primary outcome.","Blinding may not be required for mortality outcomes; however, it was rarely a primary outcome.","Modify,Clarity",Clarity
10946,9-1193,9-1193_v2_56@5,9-1193_v1_56@4,Half of the trials include fewer than 118 patients and many small trials were initiated after public registration of very large trials addressing similar questions.,Half of the trials include fewer than 120 patients and many small trials were initiated after public registration of very large trials addressing similar questions.,"Modify,Fact/Evidence",Fact/Evidence
10947,9-1193,9-1193_v2_58@1,9-1193_v1_58@1,"Strongly endorsed and prioritized by authorities and medical representatives <REF-27> , it is running as a streamlined pragmatic platform trial in over 176 hospitals, randomizing over 12,000 patients in just over four months <REF-14> .","Strongly endorsed and prioritized by authorities and medical representatives <REF-26> , it is running as streamlined pragmatic platform trial in over 176 hospitals, randomizing over 12,000 patients in just over four months <REF-14> .","Modify,Grammar",Grammar
10948,9-1193,9-1193_v2_63@6,9-1193_v1_63@5,"Fifth, we may have missed a few cases of duplicate entries across registries or of multiple national parts of an international trial, thus slightly overestimating the number of trials but not affecting the overall interpretation.","Fourth, we may have missed a few cases of duplicate entries across registries or of multiple national parts of an international trial, thus slightly overestimating the number of trials but not affecting the overall interpretation.","Modify,Fact/Evidence",Fact/Evidence
10949,9-1193,9-1193_v2_63@7,9-1193_v1_63@6,"Sixth, we arbitrarily selected a period of the first 100 days, which is traditionally used to benchmark early outcomes of policies or presidencies.","Fifth, we arbitrarily selected a period of the first 100 days, which is traditionally used to benchmark early outcomes of policies or presidencies.","Modify,Fact/Evidence",Fact/Evidence
10950,9-1193,9-1193_v2_24@0,9-1193_v1_24@0,"We identified 683 trials registered or published over the pandemic’s first 100 days, testing interventions to treat or prevent COVID-19 (see Underlying data ) <REF-12> with a total planned sample size of 394,146 participants.","We identified 689 trials registered or published over the pandemic’s first 100 days, testing interventions to treat or prevent COVID-19 (see Underlying data ) <REF-12> with a total planned sample size of 396,366 participants.","Modify,Fact/Evidence",Fact/Evidence
10951,9-1193,9-1193_v2_24@2,9-1193_v1_24@2,"Twenty-nine (4.2%) were active but no longer recruiting (58,589 participants), 381 (55.8%) started recruiting (215,807 participants), 174 (25.5%) had not yet started (97,406 participants), 50 (7.3%) were discontinued (12,048 participants), and 4 (0.6%) were terminated (577 participants).","Thirty (4.4%) were active but no longer recruiting (59,259 participants), 384 (55.7%) started recruiting (217,357 participants), 174 (25.3%) had not yet started (97,406 participants), 50 (7.3%) were discontinued (12,048 participants), and 4 (0.6%) were terminated (577 participants).","Modify,Fact/Evidence",Fact/Evidence
10952,9-1193,9-1193_v2_24@3,9-1193_v1_24@3,The status was unknown for 10 (1.5%; 168 participants).,The status was unknown for 12 (1.7%; 168 participants).,"Modify,Fact/Evidence",Fact/Evidence
10953,9-1193,9-1193_v2_26@0,9-1193_v1_26@0,"The 683 trials’ median target sample size was 118 (IQR 60 to 300; Table 1 ); 40.7% (n=280) planned to enroll fewer than 100 participants, 8.2% (n=56) over 1,000, and 1.6% (n=11) over 5,000 (see Underlying data ) <REF-12> .","The 689 trials’ median target sample size was 120 (IQR 60 to 300; Table 1 ); 40.7% (n=280) planned to enroll fewer than 100 participants, 8.3% (n=57) over 1,000, and 1.6% (n=11) over 5,000 (see Underlying data ) <REF-12> .","Modify,Fact/Evidence",Fact/Evidence
10954,9-1193,9-1193_v2_26@1,9-1193_v1_26@1,75.5% (n=516) trials were randomized and 59.4% (n=406) did not use blinding ( Table 1 ).,75.8% (n=522) trials were randomized and 59.2% (n=408) did not use blinding ( Table 1 ).,"Modify,Fact/Evidence",Fact/Evidence
10955,9-1193,9-1193_v2_26@2,9-1193_v1_26@2,Randomized trials were on average almost three times larger than non-randomized trials (median sample size 144 vs. 50).,Randomized trials were on average three times larger than non-randomized trials (median sample size 150 vs. 50).,"Modify,Fact/Evidence",Fact/Evidence
11342,9-356,9-356_v2_21@4,9-356_v1_21@4,Such improvement allows researchers to culture small volume of a large number of Agrobacterium strains in parallel and use the diluted cultures to carry out high-throughput transformation of a large number of different constructs into Arabidopsis plants.,Such improvement allows researchers to culture small volume of a large number of Agrobacterium strains in parallel and use the diluted cultures to carry out high-throughput transformation of a large number different constructs into Arabidopsis plants.,"Modify,Grammar",Grammar
11343,9-356,9-356_v2_4@1,9-356_v1_4@1,"The development of different transformation protocols in various plants has enabled advances in plant molecular biology and crop improvements ( Saifi et al ., 2020 ).",The development of different transformation protocols in various plants has enabled advances in plant molecular biology and crop improvements.,"Modify,Fact/Evidence",Fact/Evidence
11344,9-356,9-356_v2_4@3,9-356_v1_4@3,"During 1980s and early 1990s, generating transgenic plants by leaf disc-based Agrobacterium -mediated transformation required laborious plant tissue culture and regeneration steps.","During 1980s and early 1990s, generating transgenic plants by leaf disc-based Agrobacterium -mediated transformation requires laborious plant tissue culture and regeneration steps.","Modify,Grammar",Grammar
11345,9-356,9-356_v2_4@8,9-356_v1_4@8,"Floral dip transformation may be feasible in plants such as wheat, maize, tomato, flax, Medicago truncatula and Setaria viridis ( Agarwal et al ., 2009 ; Bastaki & Cullis, 2014 ; Martins et al ., 2015 ; Mu et al ., 2012 ; Trieu et al ., 2000 ; Yasmeen et al ., 2009 ).","Floral dip transformation may be feasible in plants such as wheat and Setaria viridis ( Agarwal et al ., 2009 ).","Modify,Fact/Evidence",Fact/Evidence
11346,9-356,9-356_v2_5@2,9-356_v1_5@2,"Here, we tested whether a low concentration of Agrobacterium inoculum affects the plant transformation rate.","Here, we tested whether low concentration of Agrobacterium inoculum affects the plant transformation rate.","Modify,Grammar",Grammar
11347,9-356,9-356_v2_5@3,9-356_v1_5@3,"Our data showed that, contrary to our expectation, using an extremely low density of Agrobacterium inoculum (OD 600 =0.002) in the floral dip method still warrants relatively high transformation rate in Arabidopsis.","Our data showed that, in contrary to our expectation, using extremely low density of Agrobacterium inoculum (OD 600 =0.002) in floral dip method still warrants relatively high transformation rate in Arabidopsis.","Modify,Grammar",Grammar
11348,9-356,9-356_v2_10@3,9-356_v1_10@3,Then the overnight culture was diluted into 100 ml LB media with kanamycin (50 μg/ml) and allowed to grow further for 8 h (OD 600 =1.5~1.8) in the same shaker.,Then the overnight culture was diluted into 100 ml LB media with kanamycin (50 μg/ml) and allowed to grow further for 8 h in the same shaker.,"Modify,Fact/Evidence",Fact/Evidence
11349,9-356,9-356_v2_2@4,9-356_v1_2@4,Our data revealed that the floral dip method still guarantees a relatively high transformation rate in the Arabidopsis thaliana Col-0 ecotype even with very low Agrobacterium inoculum (OD 600 =0.002).,Our data revealed that the floral dip method still guarantees relatively high transformation rate in Arabidopsis thaliana Col-0 ecotype even with very low Agrobacterium inoculum (OD 600 =0.002).,"Modify,Grammar",Grammar
11368,9-47,9-47_v2_24@0,9-47_v1_24@0,Expression interaction,Seurat interaction,"Modify,Other",Other
11369,9-47,9-47_v2_25@2,9-47_v1_25@2,"Using the combineExpression function in scRepertoire, we can look at the clonotypic frequencies of cells that comprise the UMAP-based clusters ( Figure 4C ).","Using the combineSeurat function in scRepertoire, we can look at the clonotypic frequencies of cells that comprise the UMAP-based clusters ( Figure 4C ), with notable expansion in the C2, C3, and C6 clusters ( Figure 4D ).","Modify,Fact/Evidence",Fact/Evidence
11370,9-47,9-47_v2_25@7,9-47_v1_25@6,"After combining both the clonotype and expression data, interaction between categories, such as cluster label and clonotype frequency can be visualized with the alluvialClonotypes function ( Figure 4E ).","After combining both the clonotype and expression data, interaction between categories, such as cluster label and clonotype frequency can be visualized with the alluvialGraph function.","Modify,Fact/Evidence",Fact/Evidence
11371,9-47,9-47_v2_5@1,9-47_v1_5@1,"Built using R, scRepertoire is a toolkit to assist in the analysis of immune profiles for both B and T cells, while interacting with the popular Seurat pipeline <REF-4> – <REF-6> , as well as SingleCellExperiment and monocle3 class expression objects.","Built using R, scRepertoire is a toolkit to assist in the analysis of immune profiles for both B and T cells, while interacting with the popular Seurat pipeline <REF-4> – <REF-6> .","Modify,Fact/Evidence",Fact/Evidence
11372,9-47,9-47_v2_2@4,9-47_v1_2@4,"Enabling users to easily combine mRNA and immune profiling, scRepertoire was built to process data derived from 10x Genomics Chromium Immune Profiling for both T-cell receptor (TCR) and immunoglobulin (Ig) enrichment workflows and subsequently interacts with a number of popular R packages for single-cell expression, such as Seurat.","Enabling users to easily combine mRNA and immune profiling, scRepertoire was built to process data derived from 10x Genomics Chromium Immune Profiling for both T-cell receptor (TCR) and immunoglobulin (Ig) enrichment workflows and subsequently interacts with the popular Seurat R package.","Modify,Clarity",Clarity
