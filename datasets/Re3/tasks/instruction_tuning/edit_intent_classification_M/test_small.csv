edit_index,doc_name,node_ix_src,node_ix_tgt,text_src,text_tgt,gold,label
19,10-ARR,10-ARR_v2_24@1,10-ARR_v1_22@3,Each entity types identified during the extraction has a set of eleven distinct query templates as shown in Table 1.,Each entity types identified during the extraction has a set of distinct query templates as shown in Table 1.,"Modify,Fact/Evidence",Fact/Evidence
20,10-ARR,10-ARR_v2_25@2,10-ARR_v1_23@2,4 The resulting sentences with medical concepts are then considered as knowledge candidates during our next step.,4 The resulting sentences are then considered as knowledge candidates during our next step.,"Modify,Fact/Evidence",Fact/Evidence
21,10-ARR,10-ARR_v2_26@2,10-ARR_v1_24@2,"The positive samples used for this classifier consist of 1,331 sentences with cause-effect relationships (e.g., He had chest pains and headaches from mold in the bedrooms) from the SemEval10 Task 8 dataset (Hendrickx et al., 2010) and an equal amount of negative samples randomly selected from sentences containing other types of semantic relationships in the same dataset.","The positive samples used for this classifier consist of 1,331 cause-effect relationships (e.g., He had chest pains and headaches from mold in the bedrooms) from the SemEval10 Task 8 dataset (Hendrickx et 2010) and an equal amount of negative samples randomly selected from sentences containing other types of semantic relationships in the same dataset.","Modify,Clarity",Clarity
22,10-ARR,10-ARR_v2_5@0,10-ARR_v1_4@2,"Across different counseling styles, reflective listening has always been a fundamental procedure underlying effective counseling practices (Katz and McNulty, 1994).","Effective counseling practice calls for reflective listening as an essential skill (Katz and McNulty, 1994).","Modify,Fact/Evidence",Fact/Evidence
23,10-ARR,10-ARR_v2_5@2,10-ARR_v1_4@3,"If carried out the right way, it gives the client a sense of being understood and facilitates further self-exploration.","It requires the counselor to perceive the other's need or problem, and respond in a way letting the other know he is being understood.","Modify,Claim",Claim
24,10-ARR,10-ARR_v2_36@1,10-ARR_v1_33@6,"Following the categorization in (Hwang et al., 2021), we limit the relationships to the commonsense subset to reduce noise and to limit the number of generated knowledge triplets.","Following the categorization in (Hwang et al., 2020), we limit the relationships to the commonsense subset to reduce noise and to limit the number of generated knowledge triplets.","Modify,Fact/Evidence",Fact/Evidence
25,10-ARR,10-ARR_v2_5@6,10-ARR_v1_4@4,"Thus, counseling frequently calls for counselors to make inferences based on their prior knowledge.","This process frequently involves making inferences based on the counselor's prior knowledge (Miller and Rollnick, 2012).","Modify,Clarity",Clarity
26,10-ARR,10-ARR_v2_5@7,10-ARR_v1_4@5,"For example, when the client says I had a really hard time sticking to my diet this week, a plausible reflection may be You're wondering whether you'll be able to lose weight this way, which relates diet with losing weight as an inference based on commonsense knowledge.","For example, the client says I had a really hard time sticking to my diet this week, a plausible reflection may be You're wondering whether you'll be able to lose weight this way, which relates diet with losing weight as an inference.","Modify,Claim",Claim
27,10-ARR,10-ARR_v2_5@9,10-ARR_v1_4@7,"For example, to understand the client in Figure 1, the counselor needs to know that smoking can be a possible cause of emphysema, and Chantix is a medication for smoke cessation.","For example, to understand the client in Figure 1, the counselor needs to know that smoking can be a possible cause of emphysema, and Chantix is a medication for quit smoking.","Modify,Clarity",Clarity
28,10-ARR,10-ARR_v2_51@3,10-ARR_v1_48@0,"The inplace method, which inserts the relation r and the generated e 2 next to e 1 , shows a significant improvement over the baseline.","The inplace method, which inserts the relation r and the generated e 2 next to e1, shows a significant improvement over the baseline.","Modify,Grammar",Grammar
29,10-ARR,10-ARR_v2_6@1,10-ARR_v1_5@1,"This extra knowledge is needed since existing pre-trained language models struggle to produce coherent and informative responses that capture relevant knowledge, even if they have acquired some knowledge during the pre-training phase (Petroni et al., 2019a).","This is a challenging task since existing pre-trained language models struggle to produce coherent and informative responses that capture relevant knowledge, even if they have acquired some knowledge during the pretraining (Petroni et al., 2019a) phase.","Modify,Claim",Claim
30,10-ARR,10-ARR_v2_60@0,10-ARR_v1_56@0,"We conduct a human evaluation where we ask annotators to indicate their preferences between our best performing models from both the retrieval and the generative settings, and a model without knowledge enhancement.","We conduct a human evaluation where we ask annotators to indicate their preferences between our best performing models from both the retrieval and the generative settings ,and a model without knowledge enhancement.","Modify,Grammar",Grammar
31,10-ARR,10-ARR_v2_62@2,10-ARR_v1_58@2,7 The annotators had no information on which model generated the the response being annotated.,7 The annotators have no information on which model generates the the response being annotated.,"Modify,Grammar",Grammar
32,10-ARR,10-ARR_v2_6@2,10-ARR_v1_5@2,A system that generates accurate counseling reflections can serve as a tool to aid counseling training or assist counselors during a session by providing alternative reflections in response to client's statements.,A system that generates good counseling reflections can serve as a tool to aid counseling training or assist counselors during a session by providing candidate responses.,"Modify,Clarity",Clarity
33,10-ARR,10-ARR_v2_66@3,10-ARR_v1_62@3,"Through an ablation study, we found that commonsense related to intentional and causal relationships is essential for the counseling domain.","Through an ablation study, we found that commonsense related to intentional and causal relationships are essential for the counseling domain.","Modify,Grammar",Grammar
34,10-ARR,10-ARR_v2_7@1,10-ARR_v1_6@1,"The first is retrieval, which acquires sentences containing relevant knowledge based on the vector representations of sentences from the dialogue and assertions in the knowledge base using a BERT-based model (Reimers and Gurevych, 2019a).","The first is retrieval, which acquires sentences containing relevant knowledge using a BERT-based model (Reimers and Gurevych, 2019a) to get vector representations of sentences from the dialogue and assertions in the knowledge base.","Modify,Clarity",Clarity
35,10-ARR,10-ARR_v2_7@2,10-ARR_v1_6@2,"The second strategy is generative, where we first extract key phrases from the dialogue, and query a COMET model for plausible knowledge triplets with a predefined set of relations (Bosselut et al., 2019).","The second strategy is generative, where we first extract key phrases from the dialogue, and query a COMET model for plausible knowledge triplets with a defined set of relations (Bosselut et al., 2019).","Modify,Clarity",Clarity
36,10-ARR,10-ARR_v2_12@7,10-ARR_v1_12@5,We address a similar task but enhance the generation process by infusing commonsense and domain specific knowledge to better emulate what counselors do in practice.,"Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.","Modify,Claim",Claim
37,10-ARR,10-ARR_v2_13@4,10-ARR_v1_13@0,External knowledge resources have been found useful for enhancing language models.,"There are various types of knowledge resources that can be used to enhance language models, focusing on different aspects.","Modify,Claim",Claim
38,10-ARR,10-ARR_v2_13@5,10-ARR_v1_13@1,"For example, large-scale commonsense knowledge graphs (CSKG) that store structured commonsense knowledge in the form of knowledge triplets.","For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets.","Modify,Grammar",Grammar
39,10-ARR,10-ARR_v2_18@0,10-ARR_v1_18@0,"In the following section, we describe the method to obtain relevant knowledge k c and the approach we use to incorporate knowledge into the language model.","In the following section, we describe the method to obtain relevant knowledge k c and the approach we use to incorporate knowledge in the language model.","Modify,Grammar",Grammar
40,10-ARR,10-ARR_v2_20@0,10-ARR_v1_20@0,"Despite their large size, existing commonsense knowledge bases contain a limited amount of information on domain-specific concepts, especially for causal relationships such as the reason to take a medicine or its side effects.","Despite their large size, existing commonsense knowledge bases contain a limited amount of information on some domain-specific concepts, especially causal relationships such as the reason to take a medicine or its side effects.","Modify,Clarity",Clarity
49,103-ARR,103-ARR_v2_2@4,103-ARR_v1_2@4,"Thanks to the entailment, the multi-source transfer between ACE and WikiEvents further reduces annotation down to 10% and 5% (respectively) of the full training without transfer.","Thanks to entailment, the multi-source transfer between ACE and WikiEvents further reduces annotation down to 10% and 5% (respectively) of the full training without transfer.","Modify,Grammar",Grammar
50,103-ARR,103-ARR_v2_21@0,103-ARR_v1_22@0,"Inference takes into account three key factors to output the role label for an argument candidate: the entailment probabilities of each verbalization, the type constraints of the specific role, and a threshold.","Inference takes into account three key factors to output the role label for an argument candidate: the entailment probabilities of each verbalization, the type constraints of the specific role and a threshold.","Modify,Grammar",Grammar
51,103-ARR,103-ARR_v2_2@5,103-ARR_v1_2@5,Our analysis shows that the key to good results is the use of several entailment datasets to pre-train the entailment model.,Our analysis shows that key to good results is the use of several entailment datasets to pre-train the entailment model.,"Modify,Grammar",Grammar
52,103-ARR,103-ARR_v2_21@5,103-ARR_v1_23@2,"For this purpose, we convert the EAE training dataset into a NLI format, i.e we generate entailment, neutral and contradiction hypotheses heuristically from the data using the templates themselves.","For this purpose we convert the EAE training dataset into an NLI format, i.e we generate entailment, neutral and contradiction hypotheses heuristically from the data using the templates themselves.","Modify,Grammar",Grammar
53,103-ARR,103-ARR_v2_2@6,103-ARR_v1_2@6,"Similar to previous approaches, our method requires a small amount of effort for manual verbalization: only less than 15 minutes per event argument type is needed, and comparable results can be achieved with users with different level of expertise.","Similar to previous approaches, our method requires a small amount of effort for manual verbalization: only less than 15 minutes per event argument types is needed; comparable results can be achieved from users of different level of expertise.","Modify,Grammar",Grammar
54,103-ARR,103-ARR_v2_29@1,103-ARR_v1_31@1,"During the creation, the template developers had access to the guidelines that describe each of the roles (which can include one or two examples) and a NLI model that the developer could use to verify whether the generated verbalizations of these examples were entailed by the model.","During the creation, the template developers had access to the guidelines that describe each of the roles (which can include one or two examples) and an NLI model that the developer could use to verify whether the generated verbalizations of these examples are entailed by the model.","Modify,Grammar",Grammar
55,103-ARR,103-ARR_v2_31@3,103-ARR_v1_33@3,The WikiEvents dataset is instead more focused on document-level argument extraction task.,"The WikiEvents dataset instead, is more focused on document-level argument extraction task.","Modify,Clarity",Clarity
56,103-ARR,103-ARR_v2_4@0,103-ARR_v1_4@0,"Building Information Extraction (IE) systems for real-world applications is very costly and has suffered from data-scarcity problems, due in part to the expertise and time required to annotate training data at a large scale with sufficient consistency, but also due to poor transfer between domains: IE annotations depend on the schema used in each domain, and moving to new domains requires new schemas, new annotation guidelines and the manual annotation of new data.","Building Information Extraction (IE) systems for real-world applications is very costly and has suffered from data-scarcity problems, due in part to the expertise and time required to annotate training data at a large scale with sufficient consistency, but also due to poor transfer between domains: IE annotations depend on the schema used in each domain, and moving to new domains requires a new schemas, new annotation guidelines and manual annotation of new data.","Modify,Grammar",Grammar
57,103-ARR,103-ARR_v2_34@2,103-ARR_v1_36@2,"EM is a state-of-the-art (Zhou and Chen, 2021) model that uses ROBERTA LARGE as a backbone.",EM uses RoBERTa large.,"Modify,Fact/Evidence",Fact/Evidence
58,103-ARR,103-ARR_v2_2@0,103-ARR_v1_2@0,"Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as Textual Entailment tasks using verbalizations, with strong performance in zero-shot and fewshot settings thanks to pre-trained entailment models.","Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as a Textual Entailment tasks using verbalizations, with strong performance in zero-shot and few-shot settings thanks to pre-trained entailment models.","Modify,Grammar",Grammar
59,103-ARR,103-ARR_v2_37@2,103-ARR_v1_39@2,"In total, 464.56 hours (154.86 if only a single run is done) of computation time are required to reproduce all the experiments, that in our setting corresponds to 21.36 kgCO 2 eq carbon footprint 10 (roughly equivalent to the CO 2 emitted by 88.2 km driven by an average car).","In total, 464.56 hours (154.86 if only a single run is done) of computation time are required to reproduce all the experiments, that in our setting corresponds to 21.36 kgCO 2 eq carbon footprint 10 (roughly equivalent to the CO 2 emitted by 88.2Km driven by an average car).","Modify,Grammar",Grammar
60,103-ARR,103-ARR_v2_40@1,103-ARR_v1_42@1,"Sequentially fine-tuning our NLI model in TA-CRED and then in our target task shows small improvements on low-resource scenarios (0% split for ACE, 0% and 5% splits for WikiEvents).","Sequentially fine-tuning our NLI model in TA-CRED and then in our target task show small improvements on low-resource scenarios (0% split for ACE, 0% and 5% splits for WikiEvents).","Modify,Grammar",Grammar
61,103-ARR,103-ARR_v2_45@6,103-ARR_v1_47@5,"Our results suggest that new, more challenging NLI datasets, as well as NLI datasets automatically generated from other sources (as done in this work with WikiEvents and ACE) will yield more robust entailment models, and could further increase the performance of entailment-based EAE and IE.","Our results suggest that new, more challenging NLI datasets, as well as NLI datasets automatically generated from other sources (as done in this work with Wikievents and ACE) will yield more robust entailment models, and could further increase the performance of entailment-based EAE and IE.","Modify,Grammar",Grammar
62,103-ARR,103-ARR_v2_49@17,103-ARR_v1_51@17,"Based on our estimation, 9 hours would allow an annotator to annotate 5% of the dataset which yields a 37.5 F1 (Figure 5), while 5 hours of template building yields 40.6 F1-Score in the zero-shot setting.","Based on our estimation, 9 hours would allow an annotator to annotate 5% of the dataset which yields an 37.5 F1 (Figure 5), while 5 hours of template building yields 40.6 F1-Score in the zero-shot setting.","Modify,Grammar",Grammar
63,103-ARR,103-ARR_v2_49@19,103-ARR_v1_51@19,"Figure 5 plots the performance according to manual hours on ACE, showing the huge gains provided by the initial 5 hours writing templates, plus the reuse of WikiEvents annotations.","Figure 5 plots the performance according to manual hours on ACE, showing the huge gains provided by the initial 5 hours writing templates, plus the reuse of WikiEvent annotations.","Modify,Grammar",Grammar
64,103-ARR,103-ARR_v2_52@0,103-ARR_v1_54@0,"This paper shows the entailment-based approach for event argument extraction is extremely effective in zero-shot, few-shot and full train scenarios both on ACE and WikiEvents, outperforming previous methods.","This paper shows the entailment-base approach for event argument extraction is extremely effective in zero-shot, few-shot and full train scenarios both on ACE and WikiEvents, outperforming previous methods.","Modify,Grammar",Grammar
65,103-ARR,103-ARR_v2_54@4,103-ARR_v1_55@4,"For the future, we plan to test new hyperparameter sets that uses bigger batch-sizes, as recent works (Aribandi et al., 2022) suggest to be optimal for multi-task and -source learning experiments.","For the future, we plan to test new hyperparameter sets that uses bigger batch-sizes, as recent works (Aribandi et al., 2021) suggest to be optimal for multi-task and -source learning experiments.","Modify,Fact/Evidence",Fact/Evidence
66,103-ARR,103-ARR_v2_7@0,103-ARR_v1_7@0,"(1) We show that our method reduces schema dependency, as it improves the performance on the WikiEvents results using additional ACE training data and vice versa with no extra manual work.","(1) We show that our method reduces schema dependency, as it improves the performance on the Wikievents results using additional ACE training data and vice versa with no extra manual work.","Modify,Grammar",Grammar
67,103-ARR,103-ARR_v2_8@1,103-ARR_v1_8@1,"We make the code, templates and models publicly available.",We make the code and templates publicly available 2 .,"Modify,Fact/Evidence",Fact/Evidence
68,103-ARR,103-ARR_v2_2@2,103-ARR_v1_2@2,"In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE and WikiEvents respectively, while achieving the same performance as with full training.","In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE and WikiEvents, respectively, while achieving the same performance as with full training.","Modify,Grammar",Grammar
69,103-ARR,103-ARR_v2_12@0,103-ARR_v1_12@0,"Multi-task learning reformulates multiple tasks to a single and common task via prompting large pre-trained language models, leveraging multiple data sources to improve each task of interest.","Multi-task learning reformulates multiple tasks to a single and common task via prompting large pre-trained language models, leveraging multiple data sources to improve each tasks of interest.","Modify,Grammar",Grammar
70,103-ARR,103-ARR_v2_12@3,103-ARR_v1_13@2,Wei et al. (2021a) and Mishra et al. (2022) obtained contradictory results.,Wei et al. (2021a) and Mishra et al. (2021) obtained contradictory results.,"Modify,Fact/Evidence",Fact/Evidence
71,103-ARR,103-ARR_v2_12@5,103-ARR_v1_13@4,"In this work, we explore multi-source learning, where datasets from different or similar tasks are used to build a model for the target task.","In this work we explore multi-source learning, where datasets from different or similar tasks are used to build a model for the target task.","Modify,Grammar",Grammar
72,103-ARR,103-ARR_v2_13@0,103-ARR_v1_14@0,Event Argument Extraction is a sub-task of Event Extraction.,Event Argument Extraction (EAE) is a subtask of Event Extraction.,"Modify,Clarity",Clarity
73,103-ARR,103-ARR_v2_14@1,103-ARR_v1_15@1,"Lately, with the recent paradigm shift to prompt design learning (Min et al., 2021), several works reformulated the task as a Question Answering problem Feng et al., 2020;Du and Cardie, 2020b;Wei et al., 2021b;Lyu et al., 2021;Sulem et al., 2022) or as a Constrained Text Generation problem Du et al., 2021; using predefined prompts, questions or templates.","Lately, with the recent paradigm shift to prompt design learning (Min et al., 2021), several works reformulated the task as a Question Answering problem Feng et al., 2020;Du and Cardie, 2020b;Wei et al., 2021b;Lyu et al., 2021) or as a Constrained Text Generation problem Du et al., 2021; using predefined prompts, questions or templates.","Modify,Fact/Evidence",Fact/Evidence
74,103-ARR,103-ARR_v2_17@3,103-ARR_v1_18@2,"First, the possible roles are verbalized by means of predefined templates and the input, which comprises the context, trigger and argument candidate.","First, the possible roles are verbalized by means of predefined templates and the input, which comprises context, trigger and argument candidate.","Modify,Grammar",Grammar
85,110-ARR,110-ARR_v2_26@0,110-ARR_v1_27@0,"To reflect the prediction confidence score to the evaluation metric, we additionally define the weighted top-k hit rate (WHR@k) that uses the confidence score as weights.","To reflect the confidence score to the evaluation metric, we additionally define the weighted top-k hit rate (WHR@k) that uses the confidence score as weights.","Modify,Clarity",Clarity
86,110-ARR,110-ARR_v2_26@1,110-ARR_v1_27@1,It is worth to mention that lower metrics mean a better model performance in both cases as the metrics assess how likely the models make inaccurate answers that they must avoid.,It is worth to mention that lower metrics mean a better model performance in both cases.,"Modify,Fact/Evidence",Fact/Evidence
87,110-ARR,110-ARR_v2_30@2,110-ARR_v1_31@2,"We added the ELECTRA-small/base/large models (Clark et al., 2020) for the SAR task, but it is not used for the MKR-NQ and MWR experiments, as the discriminator of the ELECTRA models are trained with the replaced token prediction (RTP) training objective and have no MLM classifier.","We added the Electra-small/base/large models (Clark et al., 2020) for the SAR task, which are trained with the replaced token prediction (RTP) training objective.","Modify,Fact/Evidence",Fact/Evidence
88,110-ARR,110-ARR_v2_30@5,110-ARR_v1_31@5,"We use the AdamW optimiser (Loshchilov and Hutter, 2019) for training with a learning rate of 5e −6 and a batch size of 32.","We use the AdamW optimiser (Loshchilov and Hutter, 2017) for training with a learning rate of 5e −6 and a batch size of 32.","Modify,Fact/Evidence",Fact/Evidence
89,110-ARR,110-ARR_v2_4@0,110-ARR_v1_4@0,"Contemporary large-size PLMs, such as BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020), and GPT-2 and -3 (Radford et al., 2019;Brown et al., 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al., 2018) and Super-GLUE (Wang et al., 2019b) benchmark datasets.","Contemporary large-size PLMs, such as BERT (Devlin et al., 2019), Electra (Clark et al., 2020), and GPT-2 and -3 (Radford et al., 2019;Brown et al., 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) benchmark datasets.","Modify,Grammar",Grammar
90,110-ARR,110-ARR_v2_42@2,110-ARR_v1_42@4,"However, the difference between the large and small encoderfixed models is insignificant, except for the ELEC-TRA models that exhibit only a marginal improvement.","However, the difference between the large and small encoder-fixed models is insignificant, except for the Electra models that exhibit only a marginal improvement.","Modify,Grammar",Grammar
91,110-ARR,110-ARR_v2_42@3,110-ARR_v1_42@5,"The two phenomenons suggest that PLMs' outstanding performance is predicated on updating many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019;McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information.","The two phenomenons suggest that PLMs' outstanding performance is predicated on updating a great many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019;McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information.","Modify,Clarity",Clarity
94,110-ARR,110-ARR_v2_48@2,110-ARR_v1_48@3,"We can readily imagine sentences in which the two words appear in the same context, e.g., ""the little boy/girl cuddled the teddy bear closely"".","Despite their antonymy, we can readily imagine sentences in which the two words appear in the same context, e.g., ""The little boy/girl cuddled the teddy bear closely."".","Modify,Clarity",Clarity
95,110-ARR,110-ARR_v2_48@3,110-ARR_v1_48@4,"As a result, a model can learn their common functional meanings, i.e., young human beings, and the vector representations would be very similar if they were trained based on the distributional hypothesis.","As a result, the meaning of the two words would become quite similar if they were trained based on the distributional hypothesis.","Modify,Claim",Claim
96,110-ARR,110-ARR_v2_48@6,110-ARR_v1_48@6,"As a result, models cannot effectively learn the semantic meaning of words and negation expressions, provided they leverage only the text forms.","As a result, models can not learn the true meaning of words and negation expressions, provided they leverage only the text forms.","Modify,Clarity",Clarity
97,110-ARR,110-ARR_v2_5@1,110-ARR_v1_5@1,"Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2021;Gupta et al., 2021; Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2020;Gupta et al., 2021;Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
98,110-ARR,110-ARR_v2_54@7,110-ARR_v1_52@12,"We conjecture that a leading cause is that the dataset contains many words with similar meanings, mostly derived from the same stem.","We conjecture a leading cause is that the dataset contains many words with similar meanings, mostly derived from the same stem.","Modify,Clarity",Clarity
99,110-ARR,110-ARR_v2_59@1,110-ARR_v1_57@1,"After the intermediate training, all models are fine-tuned on the SAR task with the same hyperparameters described in Section 3.","For training, we use the same hyperparameters as described in Section 3.","Modify,Fact/Evidence",Fact/Evidence
100,110-ARR,110-ARR_v2_59@6,110-ARR_v1_58@3,Our results show that the proposed approach assists PLMs to learn enhanced representations with more abundant lexical semantic information.,Our results show that the proposed approach assists PLMs to learn enhanced contextualised representations with more abundant lexical semantic information.,"Modify,Clarity",Clarity
101,110-ARR,110-ARR_v2_60@0,110-ARR_v1_59@1,"We find that small PLMs, such as ELECTRA-small and ALBERT models, show no significant increase in performance or are negatively impacted.","We find that small PLMs, such as Electra-small and ALBERT models, show no significant increase in performance or are negatively impacted.","Modify,Grammar",Grammar
102,110-ARR,110-ARR_v2_62@1,110-ARR_v1_61@1,"We observe that the parameters of the ELECTRA-small model, which is negatively impacted, are changed considerably compared to other PLMs having parameters more than 100M.","We observe that the parameters of the Electra-small model, which is negatively impacted, are changed considerably compared to other PLMs having parameters more than 100M.","Modify,Grammar",Grammar
103,110-ARR,110-ARR_v2_67@1,110-ARR_v1_66@1,"To confirm whether the issue occurs, we compare the performance of BERT, RoBERTa, and ELECTRA-large on 7 GLUE benchmark datasets (Wang et al., 2018) with their IM 2 counterparts.","To confirm whether the issue occurs, we compare the performance of BERT, RoBERTa, and Electra-large on 7 GLUE benchmark datasets (Wang et al., 2018).","Modify,Fact/Evidence",Fact/Evidence
104,110-ARR,110-ARR_v2_68@1,110-ARR_v1_67@0,"We find no significant difference in performance for tasks with large datasets, such as MNLI, QNLI, QQP, and SST2.","We find no significant difference in performance for tasks with large datasets, such as the MNLI, QNLI, QQP, and SST2.","Modify,Grammar",Grammar
105,110-ARR,110-ARR_v2_68@2,110-ARR_v1_67@1,"On the contrary, tasks with small datasets, like MRPC and RTE, are slightly improved.","On the contrary, tasks with small datasets, like the MRPC and RTE, are slightly improved.","Modify,Grammar",Grammar
106,110-ARR,110-ARR_v2_68@5,110-ARR_v1_67@4,The result suggests that meaning-matching is a safe intermediate task that ensures a positive transfer with target downstream tasks.,The result suggests that the meaningmatching is a safe intermediate task that ensures positive transfer with target downstream tasks.,"Modify,Grammar",Grammar
107,110-ARR,110-ARR_v2_70@0,110-ARR_v1_69@0,"Finally, we conduct experiments on the NegNLI benchmark dataset (Hossain et al., 2020), where negation plays an important role for NLI tasks.","Finally, we conduct experiments on the NegNLI benchmark dataset (Hossain et al., 2020) where negation plays an important role for NLI tasks.","Modify,Grammar",Grammar
108,110-ARR,110-ARR_v2_71@0,110-ARR_v1_70@0,"For both SNLI and MNLI, we observe that our approach outperforms BERTNOT in the NegNLI datasets, while yielding a comparable performance in the original development datasets.","For both SNLI and MNLI, we observe that our approach outperforms BERTNOT in NegNLI datasets, while yielding a comparable performance in the original development datasets.","Modify,Grammar",Grammar
109,110-ARR,110-ARR_v2_73@3,110-ARR_v1_72@3,"Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2021;Gupta et al., 2021;Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2020;Gupta et al., 2021;Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
110,110-ARR,110-ARR_v2_74@1,110-ARR_v1_73@1,Ettinger (2020) check the ability of PLMs to understand the meaning of negation in given contexts.,Ettinger (2020) check the ability of PLMs to understand of the meaning of negation in given contexts.,"Modify,Grammar",Grammar
111,110-ARR,110-ARR_v2_75@1,110-ARR_v1_74@1,"In their remedy, they augment the language modelling objective with an unlikelihood objective (Welleck et al., 2020) based on negated sentences from the training corpus.","In their remedy, they augment the language modelling objective with an unlikelihood objective (Welleck et al., 2019) based on negated sentences from the training corpus.","Modify,Fact/Evidence",Fact/Evidence
112,110-ARR,110-ARR_v2_75@3,110-ARR_v1_74@3,"In this method, the dependency parse of the sentences, POS tags, and morphological information of each word are taken as input, and the negation of sentences is done using sets of dependency tree regular expression patterns, such as Semgrex (Chambers et al., 2007).","In this method, the dependency parse of the sentences, POS tags and morphological information of each word are taken as input and the negation of sentences is done using sets of dependency tree regular expression patterns, such as Semgrex (Chambers et al., 2007).","Modify,Grammar",Grammar
113,110-ARR,110-ARR_v2_76@0,110-ARR_v1_75@0,"Previous studies (e.g., Kassner and Schütze (2020)) have mostly limited the scope of the logical negation property only to the negation expressions (e.g., ""no"" and ""not"").","Previous studies, e.g., Kassner and Schütze (2020), have mostly limited the scope of the logical negation property only to the negation expressions (e.g., ""no"" and ""not"").","Modify,Grammar",Grammar
114,110-ARR,110-ARR_v2_76@1,110-ARR_v1_75@1,"However, the core spirit of this property is the opposite meaning, which is not only limited to the negation.","However, the core spirit of the property is opposite-meaning, which is not only limited to negation.","Modify,Grammar",Grammar
115,110-ARR,110-ARR_v2_76@2,110-ARR_v1_75@2,Welleck et al. (2020) consider negating sentences using dependency tree regular expression patterns.,Welleck et al. (2019) consider negating sentences using dependency tree regular expression patterns.,"Modify,Fact/Evidence",Fact/Evidence
116,110-ARR,110-ARR_v2_76@3,110-ARR_v1_75@3,"This widens the scope of negation, as it is not only limited to the negation expressions ""no"" and ""not"".","This widens the scope of negation, as it is not only limited to negation expressions ""no"" and ""not"".","Modify,Grammar",Grammar
117,110-ARR,110-ARR_v2_76@4,110-ARR_v1_75@4,"However, their approach relies on other components, such as Semgrex, and dependency and POS parsers, which could impact the quality of the data, hence impact the models' performance.","However, their approach relies on other components, such as Semgrex, and dependency and POS parsers which could impact the quality of the data, hence impact the models' performance.","Modify,Grammar",Grammar
118,110-ARR,110-ARR_v2_76@5,110-ARR_v1_75@5,"In this work, we consider other perturbation methods to generate the opposite-meaning sentences to investigate whether PLMs satisfy the logical negation property, and we propose a remedy, called intermediate-training on meaning-matching (IM 2 ), which hardly employs additional linguistic components.","In this work, we consider other perturbation methods to generate the opposite-meaning sentences to investigate whether PLMs satisfy the logical negation property, and we propose a remedy called intermediate-training on meaning-matching (IM 2 ) that hardly employs additional linguistic components.","Modify,Clarity",Clarity
119,110-ARR,110-ARR_v2_79@0,110-ARR_v1_78@0,We hypothesise that the distributional hypothesis is an insufficient basis for understanding the semantic meaning of texts.,We hypothesise that the distributional hypothesis results in PLMs' lack of understanding of the true meaning of texts.,"Modify,Clarity",Clarity
120,110-ARR,110-ARR_v2_8@1,110-ARR_v1_8@1,"Hosseini et al. (2021) recently employed data augmentation and unlikelihood training (Welleck et al., 2020) to prevent models from generating unwanted words, given the augmented negated data during masked language modelling (MLM).","Hosseini et al. (2021) recently employed data augmentation and unlikelihood training (Welleck et al., 2019) to prevent models from generating unwanted words, given the augmented negated data during masked language modelling (MLM).","Modify,Fact/Evidence",Fact/Evidence
121,110-ARR,110-ARR_v2_8@4,110-ARR_v1_8@4,"Second, the data augmentation method is contingent on many additional linguistic compo-nents, which causes the dependency of a model's performance on certain modules and precludes applying the method to other languages where such resources are unavailable.","Second, the data augmentation method is contingent on many additional linguistic components, which causes the dependency of model's performance on certain modules and precludes applying the method to other languages where such resources are unavailable.","Modify,Grammar",Grammar
122,110-ARR,110-ARR_v2_9@1,110-ARR_v1_10@0,"Next, we propose a remedy, called intermediate-training on meaning-matching (IM 2 ), which hardly employs additional linguistic components.","Next, we propose a remedy, called intermediatetraining on meaning-matching (IM 2 ), that hardly employs additional linguistic components.","Modify,Clarity",Clarity
123,110-ARR,110-ARR_v2_10@0,110-ARR_v1_11@0,"Our main contributions are as follows: (i) We extend the investigation of the LNP from negation to lexical semantics (Section 2), (ii) we reveal that PLMs are prone to violate the LNP (Section 3), (iii) we propose a novel remedy, named IM 2 , which is decoupled from the distributional hypothesis but learns meaning-text correspondence instead (Section 4), (iv) through experiments, we ascertain that the proposed approach improves the understanding of negation and lexical semantic information (Sections 5.1 and 5.2), and (v) we verify that meaningmatching is a stable and safe intermediate task that produces a similar or better performance in multiple downstream tasks (Sections 5.3 and 5.4).","Our main contributions are as follows: (i) We extend the investigation of the LNP from negation to lexical semantics (Section 2), (ii) we reveal that PLMs are prone to violate the LNP (Section 3), (iii) we propose a novel remedy named IM 2 that is decoupled from the distributional hypothesis but learns meaning-text correspondence instead (Section 4), (iv) through experiments, we ascertain that the proposed approach improves the understanding of negation and lexical semantic information (Sections 5.1 and 5.2), and (v) we verify that meaningmatching is a stable and safe intermediate task that produces a similar or better performance in multiple downstream tasks (Sections 5.3 and 5.4).","Modify,Clarity",Clarity
124,110-ARR,110-ARR_v2_2@5,110-ARR_v1_2@5,"To alleviate the issue, we propose a novel intermediate training task, named meaning-matching, designed to directly learn a meaning-text correspondence, instead of relying on the distributional hypothesis.","To alleviate the issue, we propose a novel intermediate training task, named meaningmatching, designed to directly learn a meaningtext correspondence, instead of relying on the distributional hypothesis.","Modify,Grammar",Grammar
168,111-ARR,111-ARR_v2_31@6,111-ARR_v1_26@6,"Moreover, token-level noise does not differentiate between content or function words, and cannot do syntactic changes like content reordering (Goyal and Durrett, 2020).","Moreover, token-level noise does not differentiate between content / function words, and cannot do syntactic changes like content reordering (Goyal and Durrett, 2020).","Modify,Grammar",Grammar
169,111-ARR,111-ARR_v2_3@2,111-ARR_v1_3@2,"Moreover, our method is better at controlling the style transfer magnitude using an input scalar knob.","Moreover, our method is better able to control the amount of style transfer using an input scalar knob.","Modify,Clarity",Clarity
170,111-ARR,111-ARR_v2_31@11,111-ARR_v1_27@3,"This has also been observed recently in Kreutzer et al. (2022), who audit 100 sentences in each language, and report 50% sentences in Marathi and 20% sentences in Hindi have the wrong language.","This has also been observed recently in Caswell et al. (2021), who audit 100 sentences in each language, and report 50% sentences in Marathi and 20% sentences in Hindi have the wrong language.","Modify,Fact/Evidence",Fact/Evidence
171,111-ARR,111-ARR_v2_3@3,111-ARR_v1_3@3,"We report promising qualitative results for several attribute transfer tasks (sentiment transfer, simplification, gender neutralization, text anonymization) all without retraining the model.","We report promising qualitative results for several attribute transfer directions, including sentiment transfer, text simplification, gender neutralization and text anonymization, all without retraining the model.","Modify,Clarity",Clarity
172,111-ARR,111-ARR_v2_3@4,111-ARR_v1_3@4,"Finally, we find model evaluation to be difficult due to the lack of datasets and metrics for many languages.",Finally we found model evaluation to be difficult due to the lack of evaluation datasets and metrics for many languages.,"Modify,Clarity",Clarity
173,111-ARR,111-ARR_v2_42@0,111-ARR_v1_38@0,"Paraphrases as a ""noise"" function: Instead of using random token-level noise (Issue #1 in Section 3.1), we paraphrase sentences to ""noise"" them during training.","Paraphrases as a ""noise"" function: Instead of using random token-level noise (issue #1 in Section 3.1), we paraphrase sentences to ""noise"" them during training.","Modify,Grammar",Grammar
174,111-ARR,111-ARR_v2_44@0,111-ARR_v1_40@0,"Using style vector differences for control: To fix the training / inference mismatch for style extraction (Issue #2 in Section 3.1), we propose using style vector differences between the output and input as the stylistic control.","Using style vector differences for control: To fix the training / inference mismatch for style extraction (issue #2 in Section 3.1), we propose using style vector differences between the output and input as the stylistic control.","Modify,Grammar",Grammar
175,111-ARR,111-ARR_v2_3@5,111-ARR_v1_3@5,"To facilitate future research we crowdsource formality annotations for 4000 sentence pairs in four Indic languages, and use this data to design our automatic evaluations.","To facilitate further research in formality transfer for Indic languages, we crowdsource annotations for 4000 sentence pairs in four languages, and use this dataset 1 to design our automatic evaluation suite.","Modify,Fact/Evidence",Fact/Evidence
176,111-ARR,111-ARR_v2_2@0,111-ARR_v1_2@0,Style transfer is the task of rewriting a sentence into a target style while approximately preserving content.,Style transfer is the task of rewriting an input sentence into a target style while approximately preserving its content.,"Modify,Clarity",Clarity
177,111-ARR,111-ARR_v2_52@0,111-ARR_v1_46@0,"To address the issue of no translation data (Issue #4 in Section 3.1), we train Indic variants of our models.","To address the issue of no translation data (issue #4 in Section 3.1), we train Indic variants of our models.","Modify,Grammar",Grammar
178,111-ARR,111-ARR_v2_54@0,111-ARR_v1_48@0,One issue with our DIFFUR-INDIC setup is usage of a stop-grad(•) to avoid verbatim copying from the input.,"One issue with our DIFFUR-INDIC setup is usage of a stop-grad(•), to avoid verbatim copying from the input.","Modify,Grammar",Grammar
179,111-ARR,111-ARR_v2_54@2,111-ARR_v1_48@2,To prevent this we simply multi-task between the exemplardriven denoising UR objective (Section 3) and the DIFFUR objective.,"To prevent this from happening, we simply do multi-task learning between the original Universal Rewriter objective (Section 3) and our DIFFUR-INDIC objective, using an equal number of minibatches for each objective.","Modify,Fact/Evidence",Fact/Evidence
180,111-ARR,111-ARR_v2_0@0,111-ARR_v1_0@0,Few-shot Controllable Style Transfer for Low-Resource Multilingual Settings,Few-shot Controllable Style Transfer for Low-Resource Multilinugal Settings,"Modify,Grammar",Grammar
181,111-ARR,111-ARR_v2_70@0,111-ARR_v1_62@1,"In other words, we are measuring the fraction of outputs which simultaneously transfer style, have a semantic similarity of at least L (our threshold in Section 5.3), and have the same language as the input.","In other words, we measure the fraction of outputs which simultaneously transfer style, have a semantic similarity of at least L (our threshold in Section 5.3), and have the same language as the input.","Modify,Grammar",Grammar
186,111-ARR,111-ARR_v2_90@1,111-ARR_v1_73@1,"Our methods outperform prior work in formality transfer & code-mixing for 7 languages, with promising qualitative results for several other attribute transfer tasks.","Our methods outperform prior work in formality transfer & codemixing for 7 languages, with promising qualitative results.","Modify,Claim",Claim
187,111-ARR,111-ARR_v2_90@2,111-ARR_v1_73@2,"Future work includes further improving systems for some attributes, and studying style transfer for languages where little / no translation data is available.","Future work includes further improving systems for some attributes, and considering languages where little / no translation data is available.","Modify,Clarity",Clarity
188,111-ARR,111-ARR_v2_92@3,111-ARR_v1_75@3,"The Google 2020 environment report mentions, 15 ""TPUs are highly efficient chips which have been specifically designed for machine learning applications"".","The Google 2020 environment report mentions, 13 ""TPUs are highly efficient chips which have been specifically designed for machine learning applications"".","Modify,Fact/Evidence",Fact/Evidence
189,111-ARR,111-ARR_v2_80@0,111-ARR_v1_75@5,"In our experiments, we compare the following models (training details are provided Appendix A):",We compare the following models:,"Modify,Fact/Evidence",Fact/Evidence
190,111-ARR,111-ARR_v2_2@1,111-ARR_v1_2@1,"While most prior literature assumes access to a large style-labelled corpus, recent work (Riley et al., 2021) has attempted ""few-shot"" style transfer using just 3-10 sentences at inference for style extraction.","While most prior literature assumes access to large stylelabelled corpora, recent work (Riley et al., 2021) has attempted ""few-shot"" style transfer using only 3-10 sentences at inference for extracting the target style.","Modify,Clarity",Clarity
191,111-ARR,111-ARR_v2_16@0,111-ARR_v1_10@1,"Most prior work either assumes access to supervised data with parallel sentences between the two styles (Jhamtani et al., 2017), or access to a large corpus of unpaired sentences with style labels (Prabhumoye et al., 2018;Subramanian et al., 2019).","Most prior work either assumes access to supervised data with parallel sentences between the two styles (Jhamtani et al., 2017), or access to large corpus of unpaired sentences with style labels (Prabhumoye et al., 2018;Subramanian et al., 2019).","Modify,Grammar",Grammar
192,111-ARR,111-ARR_v2_16@5,111-ARR_v1_10@6,"In this work, we take the first steps studying style transfer in seven languages 2 with nearly 1.5 billion speakers in total.","In this work, we take the first steps studying style transfer in seven languages 2 with nearly 1.5 billion speakers.","Modify,Clarity",Clarity
193,111-ARR,111-ARR_v2_16@7,111-ARR_v1_10@8,"Unfortunately, we find it often copies the inputs verbatim (Section 3.1), without changing their style.","Unfortunately, we found it often copied input sentences verbatim (Section 3.1) without transferring their style.","Modify,Clarity",Clarity
194,111-ARR,111-ARR_v2_17@1,111-ARR_v1_11@1,"To further boost performance we propose DIFFUR, 3 a novel algorithm using the recent finding that paraphrasing leads to stylistic changes (Krishna et al., 2020).","To further boost performance we propose DIFFUR, 3 an algorithm using the recent finding that paraphrasing leads to stylistic changes (Krishna et al., 2020).","Modify,Clarity",Clarity
195,111-ARR,111-ARR_v2_136@1,111-ARR_v1_125@1,"Consistent with Krishna et al. (2020), we find that top-p decoding usually gets higher style accuracy (r-ACC, a-ACC) and output diversity (1-g, COPY) scores, but lower semantic similarity (SIM) scores.","Consistent with Krishna et al. (2020), we find that top-p decoding usually gets higher style accuracy (r-ACC, a-ACC) and output diversity (1-g, COPY) scores, but lower similarity (SIM) scores.","Modify,Clarity",Clarity
196,111-ARR,111-ARR_v2_86@2,111-ARR_v1_130@2,"We find the lowest COPY (and lowest 1-g) for models with +BT (1%), which is due to two translation steps.","We find the lowest COPY (and lowest 1-g) for models with +BT (1%), which is due to two steps of translation.","Modify,Clarity",Clarity
197,111-ARR,111-ARR_v2_86@3,111-ARR_v1_130@3,"However, this lowers semantic similarity (also seen in Table 4) lowering the overall score (60.0 vs 78.1) compared to DIFFUR-MLT.","However, this lowers semantic similarity (also seen in Table 3) lowering the overall score compared to DIFFUR-MLT (60.0 vs 78.1 r-AGG).","Modify,Fact/Evidence",Fact/Evidence
198,111-ARR,111-ARR_v2_2@2,111-ARR_v1_2@2,"In this work, we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available.",In this work we study a relevant low-resource setting: style transfer for languages where no style-labelled corpora are available.,"Modify,Grammar",Grammar
199,111-ARR,111-ARR_v2_2@3,111-ARR_v1_2@3,"We notice that existing few-shot methods perform this task poorly, often copying inputs verbatim.","We find that existing fewshot methods perform this task poorly, with a strong tendency to copy inputs verbatim.","Modify,Clarity",Clarity
200,111-ARR,111-ARR_v2_22@1,111-ARR_v1_16@1,"At a high level, the UR model extracts a style vector s from an exemplar sentence e, which reflects the desired target style.","The UR model extracts a style vector s from an exemplar sentence e, which reflects the desired target style.","Modify,Clarity",Clarity
201,111-ARR,111-ARR_v2_24@2,111-ARR_v1_19@1,"Concretely, let x 1 and x 2 be two non-overlapping spans.","Concretely, let x 1 and x 2 be two non-overlapping spans in mC4.","Modify,Fact/Evidence",Fact/Evidence
202,111-ARR,111-ARR_v2_3@1,111-ARR_v1_3@1,"When compared to prior work, our model achieves 2-3x better performance in formality transfer and code-mixing addition across seven languages.","When compared to prior work using automatic and human evaluations, our model achieves 2-3x better performance and output diversity in formality transfer and code-mixing addition across seven languages.","Modify,Claim",Claim
219,114-ARR,114-ARR_v2_4@0,114-ARR_v1_4@0,"Event detection (ED) is the first and a crucial step of event extraction, which aims to identify events of certain types in plain texts (Ahn, 2006;Nguyen and Grishman, 2015;Mitamura et al., 2017).","Event detection (ED), the first and a crucial step of event extraction, aims to identify events of certain types in texts (Ahn, 2006;Nguyen and Grishman, 2015;Mitamura et al., 2017).","Modify,Clarity",Clarity
220,114-ARR,114-ARR_v2_4@2,114-ARR_v1_4@2,"Tasking the ACE benchmark as an example, we note the state-of-the-art ED model (Wadden et al., 2019) can strike 90% in F1 for the type DIVORCE, yet only 50% for the type START-POSITION, and it is more surprising that the training set of DIVORCE is eight times smaller than that of START-POSITION.","Tasking the ACE benchmark as an example, we note the state-of-the-art ED model (Wadden et al., 2019) can strike 90% in F1 for the type DIVORCE, yet only 50% for the type START-POSITION; it is more surprising that the training set of DIVORCE is 8 times smaller than that of START-POSITION.","Modify,Clarity",Clarity
221,114-ARR,114-ARR_v2_52@3,114-ARR_v1_60@3,"After 5 epochs, it achieves 74.8% in F1 on the ACE 2005 development set, matching the state-of-the-art performance (Liu et al., 2019c).","After 5 epochs, it achieves 74.8% in F1 on the ACE 2005 development set, matching the state-of-the-art performance .","Modify,Fact/Evidence",Fact/Evidence
222,114-ARR,114-ARR_v2_52@7,114-ARR_v1_60@8,"To allow for further investigation, we have made our code publicly available at https://github.com/ jianliu-ml/SaliencyED.","To allow for further investigation, we have made our code publicly available at http://anomynous.","Modify,Fact/Evidence",Fact/Evidence
223,114-ARR,114-ARR_v2_5@0,114-ARR_v1_5@0,In this study we take a fresh look at above problem and for the first time attribute the skewed performance to the contextual patterns of events.,This study takes a fresh look at the problem by attributing the skewed performance to the contextual patterns of events.,"Modify,Clarity",Clarity
224,114-ARR,114-ARR_v2_5@2,114-ARR_v1_5@2,"Intuitively, they demonstrate distinct patterns: the DI-VORCE event is more trigger-dependent, and the trigger word (i.e., ""divorced"") is very indicative of the event's occurrence; by contrast, the START-POSITION event is more context-dependent -the event semantic is primarily expressed by contexts rather than the trigger ""become"", which is a merely light verb.","Intuitively, they have distinct patterns: the DI-VORCE event is more trigger-dependent, because the trigger word (divorced) is very indicative of the event's occurrence; by contrast, the START-POSITION event is more context-dependent -the event semantic is primarily expressed by contexts rather than the trigger (become), which is a merely light verb.","Modify,Clarity",Clarity
225,114-ARR,114-ARR_v2_6@0,114-ARR_v1_6@0,"To address the first question, we introduce a brandy new concept called trigger saliency attribution, which can explicitly quantify an event's contextual pattern.",We introduce a brandy new concept called trigger saliency attribution that can explicitly quantify an event's contextual pattern.,"Modify,Clarity",Clarity
226,114-ARR,114-ARR_v2_6@1,114-ARR_v1_6@1,"Figure 2 illustrates the key idea: to determine how much an event is trigger-dependent or context-dependent, we measure the trigger's contribution to expressing overall the event semantic.","As shown in Figure 2, to determine how much an event depends on triggers/contexts, the key notion is to measure the trigger's contribution to expressing overall the event semantic.","Modify,Clarity",Clarity
227,114-ARR,114-ARR_v2_6@2,114-ARR_v1_6@2,"Specifically, we first assign each sentence a global event label that represents the overall event semantic.","To this end, we first assign each sentence a global event label that represents the overall event semantic.","Modify,Clarity",Clarity
228,114-ARR,114-ARR_v2_6@3,114-ARR_v1_6@3,"Then, inspired by the feature attribution method (Simonyan et al., 2014;Sundararajan et al., 2017), we regard each word as a feature and compute its contribution (i.e., saliency value) for predicting the global event label.","Then, inspired by the feature attribution method (Simonyan et al., 2014;Sundararajan et al., 2017), we regard each word as a feature and compute its saliency value (i.e., contribution) for predicting the global event label.","Modify,Clarity",Clarity
229,114-ARR,114-ARR_v2_6@4,114-ARR_v1_6@4,"Finally, by examining the ground-truth trigger's saliency value, we can tell how much an event depends on triggers or contexts: a higher value, for example, indicates that the trigger contributes more to the event, implying the event is more trigger-dependent.","Finally, by examining the ground-truth trigger's saliency value, we can determine how much an event depends on triggers or contexts: a higher value, for example, indicates that the trigger contributes more to the event, implying the event is more trigger-dependent.","Modify,Clarity",Clarity
230,114-ARR,114-ARR_v2_7@0,114-ARR_v1_7@0,"To answer the second question, we develop a new training mechanism based on trigger saliency attribution, which uses saliency as evidence to enhance learning.","We also develop a new training mechanism based on trigger saliency attribution, which uses saliency as evidence to enhance learning.","Modify,Clarity",Clarity
231,114-ARR,114-ARR_v2_7@1,114-ARR_v1_7@1,"Our method is simple and straightforward -instead of using a single model to detect all event types, we group event types with similar patterns together (assessed by trigger saliency attribution) and develop separate models for each group.","Our method is simple yet effective -instead of using a single model to detect all event types, we group event types with similar patterns together (assessed by trigger saliency attribution) and develop separate models for each group.","Modify,Clarity",Clarity
234,114-ARR,114-ARR_v2_7@3,114-ARR_v1_7@4,"To further boost learning, we also propose two saliency-exploration strategy to augment the above framework, which can explicitly integrate saliency information into learning and produce improved performance particularly for context-dependent types ( § 6.2).","Furthermore, we augment the above framework with two saliency-exploration strategy, which can explicitly integrate saliency information into learning and produce improved performance particularly for context-dependent types ( § 6.2).","Modify,Clarity",Clarity
235,114-ARR,114-ARR_v2_8@0,114-ARR_v1_8@0,"To verify the effectiveness of our approach, we have conducted extensive experiments on two ED benchmarks (i.e., ACE 2005(LDC, 2005 and MAVEN ).","We have conducted extensive experiments on two ED benchmarks (i.e., ACE 2005(LDC, 2005 and MAVEN ) to verify the effectiveness of our approach.","Modify,Clarity",Clarity
236,114-ARR,114-ARR_v2_8@1,114-ARR_v1_8@1,"According to the results: (i) Our trigger saliency attribution method can capture the underlying pattern and well explain the skewed performance, obtaining Spearman's correlation coefficients of 0.72 and 0.61 with per-type F1 on ACE 2005 and MAVEN respectively; (ii) Our new training regime based on saliency demonstrates improved results on the two benchmarks.","From the results: (i) Our trigger saliency attribution method does capture the underlying pattern and can well explain the skewed performance, obtaining Spearman's correlation coefficients of 0.72 and 0.61 with per-type F1 on ACE 2005 and MAVEN respectively; (ii) Our new training regime based on saliency demonstrates improved results on the two benchmarks.","Modify,Clarity",Clarity
237,114-ARR,114-ARR_v2_8@3,114-ARR_v1_8@3,"Finally, in ablation studies, we compare and highlight many significant characteristics (e.g., linguistic and lexical patterns) of triggerdependent and context-dependent event types; our work may inspire future research into their patterns.","Finally, we compare and emphasize several significant aspects (e.g., linguistic and lexical patterns) of trigger-dependent and contextdependent event types, and our work may inspire future research into their differences.","Modify,Clarity",Clarity
238,114-ARR,114-ARR_v2_20@0,114-ARR_v1_19@0,"• We highlight several diverse patterns of trigger-dependent and context-dependent event types, and our findings may stimulate future research into their differences.","• We highlight many distinct patterns of triggerdependent and context-dependent event types, and our findings suggest that the traditional ""one model fits all types"" paradigm may need to be revised.","Modify,Claim",Claim
239,114-ARR,114-ARR_v2_27@0,114-ARR_v1_27@0,"FA have been used to interpret model predictions in applications including image classification (Simonyan et al., 2014), machine translation (Ding et al., 2017), text classification , and others (Bastings and Filippova, 2020).","FA have been used to interpret model predictions in applications including image classification (Simonyan et al., 2014), machine translation (Ding et al., 2017), text classification (Chen et al., 2018), and others (Bastings and Filippova, 2020).","Modify,Fact/Evidence",Fact/Evidence
254,115-ARR,115-ARR_v2_68@0,115-ARR_v1_71@0,"2) Text-modality methods: These models use only textual information, including TextCNN (Kim, 2014), a deep learning model based on CNN for text classification; Bi-LSTM, a bidirectional LSTM network for text classification; SIARN (Tay et al., 2018), adopting inner-attention for textual sarcasm detection; SMSD (Xiong et al., 2019), exploring a self-matching network to capture textual incongruity information; and BERT (Devlin et al., 2019), the vanilla pre-trained uncased BERT-base taking '[CLS] text [SEP]' as input.","2) Text-modality methods These models use only textual information, including TextCNN (Kim, 2014), a deep learning model based on CNN for text classification; Bi-LSTM, a bidirectional LSTM network for text classification; SIARN (Tay et al., 2018), adopting inner-attention for textual sarcasm detection; SMSD (Xiong et al., 2019), exploring a self-matching network to capture textual incongruity information; and BERT (Devlin et al., 2019), the vanilla pre-trained uncased BERT-base taking '[CLS] text [SEP]' as input.","Modify,Grammar",Grammar
255,115-ARR,115-ARR_v2_69@0,115-ARR_v1_72@0,3) Multi-modal methods: These models take both text-and image-modality information.,3) Multi-modal methods These models take both text-and image-modality information.,"Modify,Grammar",Grammar
256,115-ARR,115-ARR_v2_69@1,115-ARR_v1_72@1,"Including HFM (Cai et al., 2019), a hierarchical multimodal features fusion model for multi-modal sarcasm detection; D&R Net (Xu et al., 2020), a Decomposition and Relation Network modeling both crossmodality contrast and semantic association; Res-BERT (Pan et al., 2020), concatenating image features and BERT-based text features for sarcasm prediction; Att- BERT (Pan et al., 2020), exploring an inter-modality attention and a co-attention to model the incongruity of multi-modal sarcasm detection; and InCrossMGs (Liang et al., 2021a), a graph-based model to leverage the sarcastic relations from both intra-and inter-modal perspectives.","Including HFM (Cai et al., 2019), a hierarchical multimodal features fusion model for multi-modal sarcasm detection; D&R Net , a Decomposition and Relation Network modeling both crossmodality contrast and semantic association; Res-BERT (Pan et al., 2020), concatenating image features and BERT-based text features for sarcasm prediction; Att- BERT (Pan et al., 2020), exploring an inter-modality attention and a co-attention to model the incongruity of multi-modal sarcasm detection; and InCrossMGs , a graph-based model to leverage the sarcastic relations from both intra-and inter-modal perspectives.","Modify,Fact/Evidence",Fact/Evidence
257,115-ARR,115-ARR_v2_5@4,115-ARR_v1_5@4,"This post however contains a sarcastic expression with negative sentiment, because it is accompanied by an image with ""thunderstorm clouds"".","This post however contains a sarcastic expression with negative sentiment, because it is accompanied by an image with thunderstorm clouds.","Modify,Grammar",Grammar
258,115-ARR,115-ARR_v2_86@0,115-ARR_v1_90@0,Conclusion and Future Work,Conclusion,"Modify,Other",Other
259,115-ARR,115-ARR_v2_87@1,115-ARR_v1_91@1,"Specifically, unlike previous research efforts that simply consider the visual information of the whole image, we attempt to recognize the important visual regions via object detection results, and further devise a novel cross-modal graph to explicitly establish the connections of scattered visual regions and the associated textual tokens.","Specifically, unlike previous research efforts that simply consider the visual information of the whole image, we attempt to recognize the important visual regions via object detection results.","Modify,Fact/Evidence",Fact/Evidence
260,115-ARR,115-ARR_v2_87@4,115-ARR_v1_91@4,"To the best of our knowledge, it is the first study of utilizing a cross-modal graph to extract intricate multi-modal sarcastic relations via object detection and sentiment cues from external knowledge bases.","To our knowledge, it is the first study of utilizing a cross-modal graph to extract intricate multi-modal sarcastic relations via object detection and sentiment cues from external knowledge bases.","Modify,Clarity",Clarity
261,115-ARR,115-ARR_v2_6@0,115-ARR_v1_6@0,"To perform multi-modal sarcasm detection on data composed of text and image, several related research efforts attempt to concatenate the textual and visual features to fuse sarcastic information (Schifanella et al., 2016), employ attention mechanism to implicitly fuse the features of different modalities based on external knowledge (Cai et al., 2019;Xu et al., 2020;Pan et al., 2020), or build interactive graphs to model the relations of different modalities (Liang et al., 2021a).","To perform multi-modal sarcasm detection on data composed of text and image, several related research efforts attempt to concatenate the textual and visual features to fuse sarcastic information (Schifanella et al., 2016), employ attention mechanism to implicitly fuse the features of different modalities based on external knowledge (Cai et al., 2019;Pan et al., 2020), or build interactive graphs to model the relations of different modalities .","Modify,Fact/Evidence",Fact/Evidence
262,115-ARR,115-ARR_v2_2@1,115-ARR_v1_2@1,"In this paper, we investigate multimodal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic relations between textual and visual modalities.","Different from existing research efforts that either simply consider the visual cues from the whole image or implicitly extract the sarcastic relations between different modalities purely via attention mechanism, in this paper, we investigate multi-modal sarcasm detection from a novel perspective by constructing a cross-modal graph for each instance to explicitly draw the ironic relations between textual and visual modalities.","Modify,Claim",Claim
263,115-ARR,115-ARR_v2_6@5,115-ARR_v1_6@5,"As such, it is essential to focus on drawing the intricate sentiment connections between text and image modalities, allowing a good exploitation of the contradictory sentiment information between modalities for learning sarcastic clues.","As such, it is essential to focus on drawing the intricate sentiment connections between modalities, allowing a good exploitation of the contradictory sentiments between modalities for learning sarcastic clues.","Modify,Clarity",Clarity
264,115-ARR,115-ARR_v2_7@3,115-ARR_v1_7@3,"Then, we explore a novel solution to assign weights to the edges of the cross-modal graph by means of computing the word similarities between the object descriptors of the attribute-object pairs and textual words based on the WordNet (Miller, 1992).","Then, we explore a novel solution to assign weights to the edges of the cross-modal graph by means of computing the word similarities between the object descriptors of the attribute-object pairs and textual words based on the WordNet (Miller, 1995).","Modify,Fact/Evidence",Fact/Evidence
265,115-ARR,115-ARR_v2_14@1,115-ARR_v1_16@1,"Further, there are also some research studies explored graph models to deal with the multi-modal tasks, such as multi-modal sentiment detection (Yang et al., 2021), multi-modal named entity recognition , cross-modal video moment retrieval (Zeng et al., 2021), multi-modal neural machine translation (Yin et al., 2020), and multimodal sarcasm detection (Liang et al., 2021a).","Correspondingly, there are also some multi-modal studies, such as multi-modal sentiment detection , multi-modal named entity recognition , and multi-modal sarcasm detection .","Modify,Fact/Evidence",Fact/Evidence
266,115-ARR,115-ARR_v2_17@0,115-ARR_v1_19@0,"As demonstrated in Figure 2, the architecture of the proposed CMGCN contains four main components: 1) Text-modality representation, which employs the pre-trained uncased BERT-base model (Devlin et al., 2019) as the text encoder to capture the hidden representation of the text-modality; 2) Image-modality representation, which deploys the pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2021) as the image encoder to capture the hidden representation of the image-modality with respect to each bounding box (visual region); 3) Cross-modal graph, which constructs a crossmodal graph for each multi-modal example based on the external affective knowledge source and the hidden representations of text and image modalities; 4) Multi-modal fusion, which fuses the representations from image and text modalities to capture the sarcastic features by means of a GCN structure and an attention mechanism.","As demonstrated in Figure 2, the architecture of the proposed CMGCN contains four main components: 1) Text-modality representation, which employs the pre-trained uncased BERT-base model (Devlin et al., 2019) as the text encoder to capture the hidden representation of the text-modality; 2) Image-modality representation, which deploys the pre-trained Vision Transformer (ViT) (Dosovitskiy et al., 2021) as the image encoder to capture the hidden representation of the image-modality with respect to each bounding box (visual region); 3) Cross-modal graph, which constructs a crossmodal graph for each multi-modal example based on the hidden representations of text and image modalities; 4) Multi-modal fusion, which fuses the representations from image and text modalities to capture the sarcastic features by means of a GCN structure and an attention mechanism.","Modify,Fact/Evidence",Fact/Evidence
279,118-ARR,118-ARR_v2_4@0,118-ARR_v1_4@0,"Named Entity Recognition (NER) aims to identify and classify entity mentions from unstructured text, e.g., extracting location mention ""Berlin"" from the sentence ""Berlin is wonderful in the winter"".","Named Entity Recognition(NER) aims to identify and classify entity mentions from unstructured text, e.g., extracting location mention ""Berlin"" from sentence ""Berlin is wonderful in the winter"".","Modify,Grammar",Grammar
280,118-ARR,118-ARR_v2_48@0,118-ARR_v1_47@0,"Motivated by IB (Tishby et al., 2000;Federici et al., 2020), we can subdivide I(X; Z) into two components by using the chain rule of mutual information(MI):","Motivated by IB (Tishby et al., 2000;Federici et al., 2020), we can subdividing I(X; Z) into two components by using the chain rule of mutual information(MI):","Modify,Grammar",Grammar
281,118-ARR,118-ARR_v2_72@0,118-ARR_v1_69@0,"In this section, we verify the performance of the proposed method on five OOV datasets, and compared it with other methods.","In this section, we verified the performance of the proposed method on five OOV datasets, and compared it with other methods.","Modify,Grammar",Grammar
282,118-ARR,118-ARR_v2_77@2,118-ARR_v1_78@2,"However, the work is neither open source nor reported on the same dataset, so this method cannot be compared with MINER.","However, the work is neither open source nor reported on the same data set, so this method is not compared with MINER.","Modify,Clarity",Clarity
283,118-ARR,118-ARR_v2_80@0,118-ARR_v1_81@0,"To verify the universality of our method, we measured its performance on various pre-trained models, i.e., Bert (Devlin et al., 2018), Roberta (Liu et al., 2019), Albert (Lan et al., 2019).","To verify the universality of our method, we measured its performance in various pre-trained models, i.e., Bert (Devlin et al., 2018), Roberta (Liu et al., 2019), Albert (Lan et al., 2019).","Modify,Grammar",Grammar
284,118-ARR,118-ARR_v2_82@2,118-ARR_v1_83@2,The output dim of the information bottleneck layer is 50.,The output dim of information bottleneck layer is 50.,"Modify,Grammar",Grammar
285,118-ARR,118-ARR_v2_82@4,118-ARR_v1_83@4,"On the other hand, we count the length distribution of entity length in different datasets, and finally choose 4 as the maximum enumerated entity length.","On the other hand, we count the length distribution of entity length in different datasets, and finally chose 4 as the maximum enumerated entity length.","Modify,Grammar",Grammar
286,118-ARR,118-ARR_v2_82@5,118-ARR_v1_83@5,The values of β and γ differ for different datasets.,The values of β and γ are different for different data sets.,"Modify,Clarity",Clarity
287,118-ARR,118-ARR_v2_82@7,118-ARR_v1_83@7,The model is trained in an NVIDIA GeForce RTX 2080Ti GPU.,The model is trained in a NVIDIA GeForce RTX 2080Ti GPU.,"Modify,Grammar",Grammar
288,118-ARR,118-ARR_v2_84@1,118-ARR_v1_85@1,"As shown in table 3, we conducted the following comparison and analysis:","As shown in table 3, we have the following observations and analysis:","Modify,Clarity",Clarity
289,118-ARR,118-ARR_v2_85@0,118-ARR_v1_86@0,"1) Our baseline model, i.e., SpanNER, does an excellent job of predicting OOV entities.","1) Our baseline model, i.e., SpanNER, does a good job at predicting OOV entities.","Modify,Other",Other
290,118-ARR,118-ARR_v2_90@4,118-ARR_v1_91@4,It probes the effectiveness of our proposed training objectives that enhances representation via deep understanding of context and entity surface forms and discourages representation from rote memorizing entity names or exploiting biased cues in data.,It probes the effectiveness of our proposed training objectives that enhances representation via deep understanding of context and entity surface forms and discourages representation from rotate memorizing entity names or exploiting biased cues in data.,"Modify,Grammar",Grammar
291,118-ARR,118-ARR_v2_90@5,118-ARR_v1_91@5,"As the coefficient rate increases continuously, the performance shows a declining trend, which means the over-constraint of L gi or L si will hurt the generalizing ability of predicting the OOV entities.","When the coefficient rate increases continuously, the performance shows a decline trend, which means the over-constraint of L gi or L si will hurt the generalizing ability of predicting the OOV entities.","Modify,Clarity",Clarity
292,118-ARR,118-ARR_v2_92@4,118-ARR_v1_93@4,"Take the attention weights of the entity ""State Street"" as an example, it is obvious that baseline model, i.e., SpanNER, focus on entity words themselves.","Take the attention weights of entity ""State Street"" as a example, it is obvious that baseline model, i.e., SpanNER, focus on entity words themselves.","Modify,Grammar",Grammar
293,118-ARR,118-ARR_v2_92@5,118-ARR_v1_93@5,"While the scores of our model are more average, it means that our method concerns more context information.","While the scores of our model is more average, means that our method concern more context information.","Modify,Grammar",Grammar
294,118-ARR,118-ARR_v2_95@0,118-ARR_v1_96@0,This group of methods makes it easier to predict OOV entities using external knowledge.,This of methods makes it easier to predict OOV entities using external knowledge.,"Modify,Grammar",Grammar
295,118-ARR,118-ARR_v2_95@1,118-ARR_v1_96@1,Zhang and Yang (2018) utilize a dictionary to list numerous entity mentions.,Zhang and Yang (2018) Use a dictionary to list numerous entity mentions.,"Modify,Clarity",Clarity
296,118-ARR,118-ARR_v2_95@3,118-ARR_v1_96@3,"To diminish the model's dependency on OOV embedding, introduce partof-speech tags.","To diminish the model's dependency on OOV embedding, introduces partof-speech tags.","Modify,Grammar",Grammar
297,118-ARR,118-ARR_v2_99@2,118-ARR_v1_100@2,Pre-trained models contextualized word embeddings via pretraining on large background corpora.,Pre-trained models contextualized word embbeddings via pretraining on large background corpora.,"Modify,Grammar",Grammar
298,118-ARR,118-ARR_v2_99@3,118-ARR_v1_100@3,"Furthermore, contextualized word embeddings can be provided by the pre-trained models, which are pre-trained on large background corpora (Peters et al., 2018;Devlin et al., 2018;Liu et al., 2019).","Furthermore, contextualized word embeddings can be provided by the pre-trained models which are pre-trained on large background corpora (Peters et al., 2018;Devlin et al., 2018;Liu et al., 2019).","Modify,Grammar",Grammar
299,118-ARR,118-ARR_v2_99@4,118-ARR_v1_100@4,Yan et al. (2021) shows that BERT is not always better at capturing context as compared to Gloe-based BiLSTM-CRFs.,Yan et al. (2021) shows that BERT are not always better at capturing context as compared to Gloe-based BiLSTM-CRFs.,"Modify,Grammar",Grammar
300,118-ARR,118-ARR_v2_99@5,118-ARR_v1_100@5,Their higher performance could be the result of learning the subword structure better.,Their higher performance could be the results of learning the subword structure better.,"Modify,Grammar",Grammar
301,118-ARR,118-ARR_v2_101@0,118-ARR_v1_102@0,"Based on the recent studies of NER, we analyze how to improve the OOV entity recognition.","Based on the recent studies of NER, we analyzed how to improve the OOV entity recognition.","Modify,Grammar",Grammar
302,118-ARR,118-ARR_v2_7@4,118-ARR_v1_7@4,"The strategy is learning a static OOV embedding representation, but not directly utilizing the context.","The strategy is learning a static OOV embedding representation, but not directly utilize the context.","Modify,Grammar",Grammar
303,118-ARR,118-ARR_v2_7@6,118-ARR_v1_7@6,"Unfortunately, Agarwal et al. (2021) shows that the higher performance of pretrained models could be the results of learning the subword structure better.","Unfortunately, Yan et al. (2021) shows that the higher performance of pretrained models could be the results of learning the subword structure better.","Modify,Fact/Evidence",Fact/Evidence
304,118-ARR,118-ARR_v2_8@2,118-ARR_v1_9@0,"Specifically, MINER contains two mutual information based learning objectives: i) generalizing information maximization, which aims to maximize the mutual information between representations and well-generalizing features, i.e., context and entity surface forms; ii) superfluous information minimization, which prevents the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.","Specifically, MINER contains two mutual information based learning objectives: i) generalizing information maximization, which aims to maximize the mutual information between representations and well-generalizing features, i.e., context and entity surface forms; ii) superfluous information minimization, which prevents the model from rote memorizing the entity names or exploiting biased cus via eliminating entity name information.","Modify,Grammar",Grammar
305,118-ARR,118-ARR_v2_10@0,118-ARR_v1_11@0,"1. We propose a novel learning framework, i.e., MINER, from an information theory perspective, aiming to improve the robustness of entity changes by eliminating entity-specific and maximizing wellgeneralizing information.","1. We propose a novel learning framework, i.e., MINER, from an information theory perspective, aiming to improve the robustness of entity changes by eliminating entity-specific and maximize wellgeneralizing information.","Modify,Grammar",Grammar
306,118-ARR,118-ARR_v2_13@1,118-ARR_v1_14@1,"Subsequently, the analysis of possible issues was provided when applying it to OOV entity recognition.","Subsequently, the analysis of possible issues when applying it to OOV entity recognition was provided.","Modify,Clarity",Clarity
307,118-ARR,118-ARR_v2_14@1,118-ARR_v1_15@1,It formulates the goal of representation learning as an information trade-off between predictive power and representation compression.,It formulates the goal of representation learning as an information trade-off between representation compression and predictive power.,"Modify,Clarity",Clarity
308,118-ARR,118-ARR_v2_2@3,118-ARR_v1_2@3,"The proposed approach contains two mutual information-based training objectives: i) generalizing information maximization, which enhances representation via deep understanding of context and entity surface forms; ii) superfluous information minimization, which discourages representation from rote memorizing entity names or exploiting biased cues in data.","The proposed approach contains two mutual information based training objectives: i) generalizing information maximization, which enhances representation via deep understanding of context and entity surface forms; ii) superfluous information minimization, which discourages representation from rotate memorizing entity names or exploiting biased cues in data.","Modify,Grammar",Grammar
309,118-ARR,118-ARR_v2_16@1,118-ARR_v1_17@1,The trade-off between the two MI terms is controlled by the Lagrange multiplier β.,The trade-off between the two MI terms is controlled by a Lagrange multiplier β.,"Modify,Grammar",Grammar
310,118-ARR,118-ARR_v2_18@3,118-ARR_v1_19@3,"Consequently, neural networks tend to use any accessible signal to do so (Ilyas et al., 2019), which is referred to as a shortcut learning problem (Geirhos et al., 2020).","Consequently, neural networks tend to use any accessible signal to do so (Ilyas et al., 2019), which is referred to as a shotcut learning problem (Geirhos et al., 2020).","Modify,Grammar",Grammar
311,118-ARR,118-ARR_v2_19@1,118-ARR_v1_20@1,"In Section 4, we demonstrate how we extend IB to the NER task and address these issues.","In Section 4, we demonstrate how we extend BN to the NER task and address these issues.","Modify,Fact/Evidence",Fact/Evidence
312,118-ARR,118-ARR_v2_21@2,118-ARR_v1_23@1,"2) Compared with sequence labeling, SpanNER does better in sentences with more OOV words (Fu et al., 2021).","2) compared with sequence labeling, SpanNER does better in sentences with more OOV words (Fu et al., 2021).","Modify,Grammar",Grammar
415,124-ARR,124-ARR_v2_19@2,124-ARR_v1_16@2,"Each meta-review sentence is independently labeled by 2 different annotators, and a third expert annotator resolves any disagreement between the first two annotators.","Each meta-review sentence is independently labeled by 2 different annotators, and a third annotator resolves any disagreement between the first two annotators.","Modify,Clarity",Clarity
416,124-ARR,124-ARR_v2_19@3,124-ARR_v1_16@3,"We label 45,929 sentences from 7,089 meta-reviews in total, and the Cohen's kappa is 0.778 between the first two annotators, showing that the annotation is of quite high quality.","We label 45,929 sentences from 7,089 meta-reviews in total, and the Cohen's kappa is 0.778 between the two annotators, showing that the annotation is of quite high quality.","Modify,Clarity",Clarity
417,124-ARR,124-ARR_v2_22@1,124-ARR_v1_19@1,"The number of sentences in different categories are shown in Figure 1, breakdown by the decision (i.e., accept or reject).","The sentence numbers in different categories are shown in Figure 1, breakdown by the decision (i.e., accept or reject).","Modify,Clarity",Clarity
418,124-ARR,124-ARR_v2_27@2,124-ARR_v1_24@2,"To achieve such flexibility, the task of structure-controllable text generation is defined as: given the text input (i.e., reviews) and a control sequence of the output structure, a model should generate a meta-review that is derivable from the reviews and presents the required structure.","To achieve such flexibility, the task of structure-controllable text generation is defined as: given the text input (i.e., reviews) and a control sequence of the output structure, a model should generate a meta-review which is derivable from the reviews and presents the required structure.","Modify,Clarity",Clarity
419,124-ARR,124-ARR_v2_31@1,124-ARR_v1_28@1,"One simple method, concat, is to concatenate all inputs one after another (Fabbri et al., 2019).","One simple method to combine multiple inputs for encoder-decoder models is to concatenate all inputs one after another (Fabbri et al., 2019).","Modify,Clarity",Clarity
420,124-ARR,124-ARR_v2_31@2,124-ARR_v1_28@2,"Besides the text inputs, the review rating, which cannot be found in the review passages but exists in the field of rating score, is also crucial information for writing meta-reviews.","Beside the text inputs, the review rating is also crucial information for writing meta reviews, which cannot be found in the review passages but exists in the field of rating score.","Modify,Clarity",Clarity
421,124-ARR,124-ARR_v2_32@2,124-ARR_v1_30@2,Sent-ctrl uses one control label per target sentence and controls generation on the sentence-level.,Sent-ctrl uses one control label per target sentence and controls generation on a sentence-level.,"Modify,Grammar",Grammar
422,124-ARR,124-ARR_v2_33@4,124-ARR_v1_31@4,"More specifically, we use the Py-Torch implementation in the open-source library Hugging Face Transformers (Wolf et al., 2020) and its hosted pretrained models 3 .","More specifically, we use the pytorch implementation in the open-source library Hugging Face Transformers (Wolf et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
423,124-ARR,124-ARR_v2_36@5,124-ARR_v1_34@5,"After ranking with each of the above models, we select sentences as output with different strategies according to the controlled and uncontrolled settings.","After ranking with the above models, we select sentences as output with different strategies according to the controlled and uncontrolled settings.","Modify,Clarity",Clarity
424,124-ARR,124-ARR_v2_36@8,124-ARR_v1_34@8,"To do so, we employ an LSTM-CRF (Lample et al., 2016) tagger trained on the labeled meta-reviews to predict the labels of each input review sentence.","To do so, we employ an LSTM-CRF (Lample et al., 2016) tagger trained on the labeled meta-reviews to predict the sentence labels of each input review.","Modify,Clarity",Clarity
425,124-ARR,124-ARR_v2_43@5,124-ARR_v1_39@5,"On the other hand, for bart-large-cnn, sent-ctrl is the best, followed by seg-ctrl.","On the other hand, for the Transformers, sent-ctrl is the best, followed by seg-ctrl.","Modify,Fact/Evidence",Fact/Evidence
426,124-ARR,124-ARR_v2_44@1,124-ARR_v1_39@8,"This is also validated by the ""Target Generic"" baseline's consistent improvement over the ""Source Generic"" baseline, which shows that generic sentences from meta-reviews can suit generation better than those in reviews.","This is again validated by the ""Target Generic"" baseline's significant improvement over the ""Source Generic"" baseline, which shows that generic sentences from meta-reviews can suit generation much better than those in reviews.","Modify,Clarity",Clarity
427,124-ARR,124-ARR_v2_63@1,124-ARR_v1_54@1,"Sentctrl also has better fluency and decision correctness, suggesting that having a better output structure can benefit readability and decision generation.","Sentctrl also has better fluency and decision correctness, suggesting that having a better output structure can benefit the readability and decision generation.","Modify,Grammar",Grammar
428,124-ARR,124-ARR_v2_65@2,124-ARR_v1_56@2,"In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely, structurecontrollable text generation.","In this paper, we explore text summarization in a new domain (i.e., the peer review domain) and provide a new dataset, i.e., MReD. Moreover, MReD's reference summaries (i.e., meta-reviews) are fully annotated and thus allow us to propose a new task, namely structurecontrollable text generation.","Modify,Grammar",Grammar
429,124-ARR,124-ARR_v2_66@3,124-ARR_v1_57@2,"There are also some datasets and annotation schemes on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain and cannot be easily transferred to our task.","There are also some explorations on research articles (Teufel et al., 1999;Liakata et al., 2010;Lauscher et al., 2018), which differ in nature from the peer review domain.","Modify,Claim",Claim
430,124-ARR,124-ARR_v2_7@0,124-ARR_v1_5@0,"To some extent, the existing task settings are not so adequate because they do not have a deep understanding of the domains they are working on, i.e., domain knowledge.","To some extent, the existing task settings are not so adequate because they do not have deep understanding of the domains they are working on, i.e., domain knowledge.","Modify,Grammar",Grammar
431,124-ARR,124-ARR_v2_7@1,124-ARR_v1_5@1,"Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail (Nallapati et al., 2016) is composed of the training pairs of news content and human-written summary bullets.","Taking text summarization as an example, the most well-experimented dataset CNN/Daily Mail (Nallapati et al., 2016) is composed of the training pairs of news content and news titles.","Modify,Fact/Evidence",Fact/Evidence
434,124-ARR,124-ARR_v2_7@2,124-ARR_v1_5@2,"However, it does not tell why a particular piece of news content should have that corresponding summary, for example for the same earnings report, why one media emphasizes its new business success in the summary, but another emphasizes its net income.","However, it does not tell why a particular piece of news content should have that corresponding title, for example for the same earnings report, why one media emphasizes its new business success in the title, but another emphasizes its net income.","Modify,Clarity",Clarity
435,124-ARR,124-ARR_v2_31@11,124-ARR_v1_76@6,"Additionally, we provide a longest-review baseline, which does not combine reviews but only uses the longest review as the input.","Additionally, we provide a baseline setting longestreview, which does not combine reviews but only uses the longest review as the input.","Modify,Clarity",Clarity
436,124-ARR,124-ARR_v2_31@10,124-ARR_v1_76@7,We further add rating sentences in front of the results of merge to obtain rate-merge.,"Moreover, we add rating sentences in front of the results of concat and merge to obtain rate-concat and rate-merge, respectively.","Modify,Fact/Evidence",Fact/Evidence
437,124-ARR,124-ARR_v2_46@1,124-ARR_v1_77@0,"The longest-review setting has the worst performance, thus validating that the review combination methods are necessary in order not to omit important information.","As shown in Table 12, the longest-review setting has the worst performance, thus validating that the review combination methods are necessary in order not to omit important information.","Modify,Fact/Evidence",Fact/Evidence
438,124-ARR,124-ARR_v2_75@7,124-ARR_v1_79@6,"RoBERTabase is the best performing model, therefore we use this model to predict review sentence labels.","RoBERTabase is the best performing model, therefore we use this model for review sentence label prediction.","Modify,Clarity",Clarity
439,124-ARR,124-ARR_v2_76@0,124-ARR_v1_80@0,"Besides the baselines of ""Source Generic"" and ""Target Generic"", we explore subsets of papers with high scores (average reviewers' rating ⩾ 7) or low scores (average reviewers' rating ⩽ 3) to obtain 4 generic baselines: ""Source High Score"", ""Source Low Score"", ""Target High Score"", ""Target Low Score"".","Besides the baselines of ""Source Generic"" and ""Target Generic"", we explore subsets of papers with high scores (average reviewers' rating 7) or low scores (average reviewers' rating 3) to obtain 4 additional generic baselines: ""Source High Score"", ""Source Low Score"", ""Target High Score"", ""Target Low Score"".","Modify,Fact/Evidence",Fact/Evidence
443,124-ARR,124-ARR_v2_78@0,124-ARR_v1_81@0,"After obtaining the generic sentence sets, we can create baseline generations using the sent-ctrl sequence on the corresponding high score paper test data.","After obtaining the generic sentence sets, we can create baseline generations using the sent-ctrl sequence.","Modify,Fact/Evidence",Fact/Evidence
445,124-ARR,124-ARR_v2_79@1,124-ARR_v1_82@1,"Both ""Target High Score"" and ""Target Low Score"" perform much better than the ""Target Genric"" baseline, suggesting that papers with very high or low scores tend to have more typical patterns in their meta-reviews.","The low score baselines perform the best amongst both source and target baselines, suggesting that the sentences from low score submissions are more typical for both reviews and meta-reviews.","Modify,Claim",Claim
446,124-ARR,124-ARR_v2_41@0,124-ARR_v1_85@0,"For bart-large-cnn, we first load the pretrained model and then fine-tune it on MReD. All experiments are conducted on single V100 GPUs, using a batch size of 1 in order to fit the large pretrained model on a single GPU.","For the Transformers, we first load the pretrained model and then fine-tune it on MReD. All experiments are conducted on single V100 GPUs, using a batch size of 1 in order to fit the large pretrained model on a single GPU.","Modify,Fact/Evidence",Fact/Evidence
447,124-ARR,124-ARR_v2_41@1,124-ARR_v1_85@1,"During fine-tuning, we set the hyperparameters of ""minimum_target_length"" to 20, and ""maximum_target_length"" to 400, according to our filter range on the meta-review lengths.","During finetuning, we set the Transformers' hyperparameters of ""minimum_target_length"" to 20, and ""maxi-mum_target_length"" to 400, according to our filter range on the meta-review lengths.","Modify,Clarity",Clarity
448,124-ARR,124-ARR_v2_80@1,124-ARR_v1_86@1,We further investigate the performance of different source truncation lengths under the setting of rate-concat.,We further investigate the performance of different source truncation lengths using rate-concat.,"Modify,Clarity",Clarity
449,124-ARR,124-ARR_v2_82@1,124-ARR_v1_88@1,"We use max-pooling to aggregate attention for same-sentence input tokens, because summation unfairly gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out.","We use max-pooling to aggregate attention for same-sentence input tokens, because summation gives high attention scores to excessively long sentences due to attention weight accumulation, whereas average-pooling disfavors long sentences containing a few relevant phrases by averaging the weights out.","Modify,Other",Other
450,124-ARR,124-ARR_v2_84@1,124-ARR_v1_91@1,"We then calculate the normalized token-level edit distance between the judge-annotated label sequence and the given control sequence, where each label is considered as a single token, and finally deduct this value from 1.","We then calculate the normalized token-level edit distance between the judge-annotated label sequence and the given control sequence, then deduct this value from 1.","Modify,Fact/Evidence",Fact/Evidence
451,124-ARR,124-ARR_v2_85@1,124-ARR_v1_92@1,"More specifically, we give 0 if the generation produces either contradictory decisions or a wrong decision, or if the generation does not show enough hints for rejection or acceptance.","More specifically, we give 0 if the generation produces contradictory decisions and a wrong decision, or the generation does not show enough hints for rejection or acceptance.","Modify,Grammar",Grammar
452,124-ARR,124-ARR_v2_8@2,124-ARR_v1_6@2,"Thus from the same input text, the trained generator can generate varied outputs according to the given control signals.","Thus from the same input text, the trained generator can generate varied outputs according to the given control signal.","Modify,Grammar",Grammar
453,124-ARR,124-ARR_v2_2@2,124-ARR_v1_2@2,"A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with a deep understanding of the domain knowledge.","A more useful text generator should leverage both the input text and the control signal to guide the generation, which can only be built with deep understanding of the domain knowledge.","Modify,Grammar",Grammar
454,124-ARR,124-ARR_v2_8@9,124-ARR_v1_6@9,"Our MReD is obviously different from the previous text generation/summarization datasets because, given the rich annotations of individual meta-review sentences, a model is allowed to learn more sophisticated generation behaviors to control the structure of the generated passage.","Our MReD is obviously different from previous text generation/summarization datasets because, given the rich annotations of individual meta-review sentences, a model is allowed to learn more sophisticated generation behaviors to control the structure of the generated passage.","Modify,Grammar",Grammar
455,124-ARR,124-ARR_v2_8@10,124-ARR_v1_6@10,"Our proposed task is also noticeably different from the existing controllable text generation tasks (e.g., text style transfer on sentiment polarity (Shen et al., 2017;Liao et al., 2018) and formality (Shang et al., 2019)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.","Our proposed task is also noticeably different from existing controllable text generation tasks (e.g., text style transfer on sentiment polarity (Shen et al., 2017;Liao et al., 2018) and formality (Shang et al., 2019)) because we focus on controlling the macro structure of the whole passage, rather than the wordings.","Modify,Grammar",Grammar
456,124-ARR,124-ARR_v2_14@1,124-ARR_v1_11@1,"Unlike the previous datasets that mainly focus on domains like news, the domain for meta-reviews is worth-studying because it contains essential and high-density opinions.","Unlike previous datasets that mainly focus on domains like news, meta-review is a worthstudying domain containing essential and highdensity opinions.","Modify,Clarity",Clarity
457,124-ARR,124-ARR_v2_16@0,124-ARR_v1_13@0,"We collect the meta-review related data of ICLR from an online peer-reviewing platform, i.e., Open-Review 2 from 2018 to 2021.",We collect the meta-review related data from an online peer reviewing platform for ICLR 2 from 2018 to 2021.,"Modify,Fact/Evidence",Fact/Evidence
458,124-ARR,124-ARR_v2_16@2,124-ARR_v1_13@2,"To prepare our dataset for controllable text generation, for each submission, we collect all of its corresponding official reviews with reviewer ratings and confidence scores, the final meta-review decision, and the meta-review passage.","To prepare our dataset for controllable text generation, for each submission, we collect multiple reviews with reviewer ratings and confidence scores, the final meta-review decision, and the meta-review passage.","Modify,Fact/Evidence",Fact/Evidence
503,125-ARR,125-ARR_v2_23@1,125-ARR_v1_20@0,The full list of EAE templates and the construction details can be found in Appendix A.,The full list of EAE templates and the constructing details can be found in Appendix A.,"Modify,Clarity",Clarity
504,125-ARR,125-ARR_v2_2@7,125-ARR_v1_2@7,"Moreover, DEGREE is capable of using additional weaklysupervised information, such as the description of events encoded in the prompts.","In addition, the proposed model is capable of using additional weakly-supervised information, such as the description of events.","Modify,Fact/Evidence",Fact/Evidence
505,125-ARR,125-ARR_v2_26@1,125-ARR_v1_22@1,"For the case that there are multiple triggers for the given event type in the input passage, DEGREE is trained to generate the output text that contains multiple E2E template such that each E2E template corresponds to one trigger and its argument roles.","For the case that there are multiple triggers for the given event type, DEGREE will generate the E2E template multiple times such that each E2E template corresponds to each trigger and its argument roles.","Modify,Fact/Evidence",Fact/Evidence
506,125-ARR,125-ARR_v2_28@1,125-ARR_v1_23@2,"After we obtain the generated sentences, we compare the outputs with E2E template to determine the predicted triggers and arguments in string format.","Then, we compare the generated output with the placeholders in E2E template to determine the predicted trigger spans and predicted argument spans.","Modify,Fact/Evidence",Fact/Evidence
507,125-ARR,125-ARR_v2_28@2,125-ARR_v1_23@3,"Finally, we apply string matching to convert the predicted string to span offsets in the passage.","Finally, we apply string matching to convert the word spans to the offsets in the passage.","Modify,Fact/Evidence",Fact/Evidence
508,125-ARR,125-ARR_v2_28@3,125-ARR_v1_23@4,"If the predicted string appears in the passage multiple times, we choose all span offsets that match for trigger predictions and choose the one closest to the given trigger span for argument predictions.","If the predicted span appears in the passage multiple times, we choose all that match for trigger predictions and choose the one being closest to the given trigger span for argument predictions.","Modify,Clarity",Clarity
509,125-ARR,125-ARR_v2_2@8,125-ARR_v1_2@8,"Finally, DE-GREE learns triggers and arguments jointly in an end-to-end manner, which encourages the model to better utilize the shared knowledge and dependencies among them.","Finally, learning triggers and argument roles in an end-toend manner encourages the model to better utilize the shared knowledge and dependencies between them.","Modify,Clarity",Clarity
510,125-ARR,125-ARR_v2_30@6,125-ARR_v1_24@7,"For example, DEGREE knows the relationship between the role Attacker and the role Target (who is attacking and who is attacked) due to E2E template.","For example, DEGREE knows the relation between the role Attacker and the role Target (who is attacking and who is attacked) because of the word ""attacked"" in E2E template.","Modify,Fact/Evidence",Fact/Evidence
511,125-ARR,125-ARR_v2_30@7,125-ARR_v1_24@8,This guidance helps DEGREE learn the dependencies between entities.,This guidance makes DEGREE learn the dependencies between entities well with less training data.,"Modify,Claim",Claim
512,125-ARR,125-ARR_v2_30@9,125-ARR_v1_24@10,This not only uses label semantics better but also makes the model easier to leverage the knowledge from the pre-trained decoder.,This not only utilizes label semantics better but also makes the model easier to leverage the knowledge from the pre-trained decoder.,"Modify,Clarity",Clarity
513,125-ARR,125-ARR_v2_2@9,125-ARR_v1_2@9,Our experimental results demonstrate the strong performance of DEGREE for low-resource event extraction.,Our experimental results and ablation studies demonstrate the strong performance of DEGREE for low-resource event extraction.,"Modify,Fact/Evidence",Fact/Evidence
514,125-ARR,125-ARR_v2_31@4,125-ARR_v1_26@4,"In fact, several prior works Du and Cardie, 2020;) also use constructed templates as weakly-supervised signals to improve models.","In fact, several prior works Du and Cardie, 2020;Li et al., 2020) also use constructed templates as weakly-supervised signals to improve models.","Modify,Fact/Evidence",Fact/Evidence
517,125-ARR,125-ARR_v2_4@0,125-ARR_v1_4@0,"Event extraction (EE) aims to extract events, each of which consists of a trigger and several participants (arguments) with their specific roles, from a given passage.","Event extraction (EE) aims to extract different types of events, each of which includes a trigger and several participants (arguments) with specific roles, from the given passage.","Modify,Clarity",Clarity
518,125-ARR,125-ARR_v2_44@1,125-ARR_v1_39@1,"We generate different proportions (1%, 2%, 3%, 5%, 10%, 20%, 30%, and 50%) of training data to study the influence of the size of the training set and use the original development set and test set for evaluation.","We generate different proportions (1%, 2%, 3%, 5%, 10%, 20%, 30%, and 50%) of training data to study the influence of the size of training set and use the original dev set and test set for evaluation.","Modify,Clarity",Clarity
519,125-ARR,125-ARR_v2_44@2,125-ARR_v1_39@2,Appendix C lists more details about the split generation process and the data statistics.,Appendix C lists more details about the split generating process and the data statistics.,"Modify,Clarity",Clarity
520,125-ARR,125-ARR_v2_46@6,125-ARR_v1_40@4,Note that the outputs of both generation-based baselines are not natural sentences.,Notice that the outputs of both generation-based baselines are not natural sentences.,"Modify,Clarity",Clarity
521,125-ARR,125-ARR_v2_4@1,125-ARR_v1_4@1,"For example, in Figure 1, a Justice:Execute event is triggered by the word ""execution"" and this event contains three argument roles, including an Agent (Indonesia) who carries out the execution, a Person who is executed (convicts), and a Place where the event occurs (not mentioned in the passage).","For example, in Figure 1, a Justice:Execute event is triggered by the word ""execution"" and this event contains three argument roles, including an Agent (Indonesia) who carries out the execution, a Person been executed (convicts), and a Place where the event occurs (not mentioned in the passage).","Modify,Grammar",Grammar
522,125-ARR,125-ARR_v2_48@0,125-ARR_v1_42@0,Table 2 shows the trigger classification F1-scores and the argument classification F1-scores in three data sets with different proportions of training data.,Table 1 shows the trigger classification F1-scores and the argument classification F1-scores across three datasets with different proportions of training data.,"Modify,Grammar",Grammar
523,125-ARR,125-ARR_v2_49@2,125-ARR_v1_43@2,"For example, when only 1% of the training data is available, DEGREE and DEGREE(PIPE) achieve more than 15 points of improvement in trigger classification F1 scores and more than 5 points in argument classification F1 scores.","For example, when only 1% of training data is available, DEGREE and DEGREE(PIPE) achieve more than 15 points of trigger classification F1scores improvement and more than 5 points of argument classification F1-scores.","Modify,Clarity",Clarity
524,125-ARR,125-ARR_v2_49@4,125-ARR_v1_43@4,"The generation-based model with carefully designed prompts is able to utilize the label semantics and the additional weakly supervised signals, thus helping learning under the low-resource regime.","The generationbased model with carefully designed prompts is able to utilize the label semantics and the additional weakly-supervised signals, thus, helps the learning under the low-resource regime.","Modify,Grammar",Grammar
525,125-ARR,125-ARR_v2_50@0,125-ARR_v1_44@0,Another interesting finding is that DEGREE and DEGREE(PIPE) seem to be more beneficial for predicting arguments than for predicting triggers.,Another interesting finding is that DEGREE and DEGREE(PIPE) seem to be more beneficial to argument prediction than trigger prediction.,"Modify,Clarity",Clarity
526,125-ARR,125-ARR_v2_50@1,125-ARR_v1_44@1,"For example, OneIE, the strongest baseline, requires 20% of training data to achieve competitive performance on trigger prediction to DEGREE and DEGREE(PIPE); however, it requires about 50% of training data to achieve competitive performance in predicting arguments.","For instance, OneIE, the strongest baseline, requires 20% of training data to achieve competitive performance on trigger prediction to DEGREE and DEGREE(PIPE); however, it requires about 50% of training data to achieve competitive performance on argument prediction.","Modify,Clarity",Clarity
527,125-ARR,125-ARR_v2_51@0,125-ARR_v1_45@0,"Furthermore, we observe that DEGREE is slightly better than DEGREE(PIPE) under the lowresource setting.","Finally, we observe that DEGREE is slightly better than DEGREE(PIPE) under the low-resource setting.","Modify,Clarity",Clarity
528,125-ARR,125-ARR_v2_51@1,125-ARR_v1_45@1,This provides empirical evidence on the benefit of jointly predicting triggers and arguments in a low-resource setting.,We hypothesize that DEGREE jointly predicts triggers and arguments and therefore can better take advantage of the output dependencies.,"Modify,Claim",Claim
529,125-ARR,125-ARR_v2_54@0,125-ARR_v1_47@0,"Although we focus on data-efficient learning for low-resource event extraction, to better understand the advantages and disadvantages of our model, we additionally study DEGREE in the high-resource setting for controlled comparisons.","While we focus on data-efficient learning for lowresource event extraction, to better understand the advantages and disadvantages of our model and make sure that it is indeed more data-efficient, rather than simply a stronger model, we additionally study DEGREE in the high-resource setting for controlled comparisons.","Modify,Fact/Evidence",Fact/Evidence
530,125-ARR,125-ARR_v2_56@0,125-ARR_v1_50@0,Ablation Studies,Ablation Study,"Modify,Grammar",Grammar
531,125-ARR,125-ARR_v2_5@0,125-ARR_v1_5@0,"Most prior works on EE rely on a large amount of annotated data for training (Nguyen and Grishman, 2015;Nguyen et al., 2016;Han et al., 2019b;Du and Cardie, 2020;Paolini et al., 2021).","Several previous EE approaches rely on a large amount of annotated data for training (Nguyen and Grishman, 2015;Nguyen et al., 2016;Du and Cardie, 2020;Paolini et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
532,125-ARR,125-ARR_v2_5@1,125-ARR_v1_5@1,"However, high-quality event annotations are expensive to obtain.","However, these high-quality event annotations are expensive to be obtained.","Modify,Clarity",Clarity
533,125-ARR,125-ARR_v2_5@2,125-ARR_v1_5@2,"For example, the ACE 2005 corpus (Doddington et al., 2004), one of the most widely used EE datasets, requires two rounds of annotations by linguistics experts.","For example, the ACE 2005 corpus (Doddington et al., 2004), one of the most common EE datasets, requires two rounds of annotations by linguistics experts.","Modify,Clarity",Clarity
534,125-ARR,125-ARR_v2_65@2,125-ARR_v1_59@6,Their predicted target-augmented language embed labels into the input passage via using brackets and vertical bar symbols.,"Their predicted targetaugmented language embed labels into the input passage via using brackets and vertical bar symbols, hindering the model from fully leveraging label semantics.","Modify,Claim",Claim
535,125-ARR,125-ARR_v2_65@6,125-ARR_v1_59@8,"They solve event extraction with a pipeline, which prevents knowledge sharing across subtasks.","Yet, similar to TANL, they solve event extraction with a pipeline, which prevents knowledge sharing across subtasks.","Modify,Fact/Evidence",Fact/Evidence
536,125-ARR,125-ARR_v2_66@1,125-ARR_v1_60@1,It has been a growing interest in event extraction in a scenario with less data.,It has been a rising interest in event extraction under less data scenario.,"Modify,Clarity",Clarity
537,125-ARR,125-ARR_v2_68@0,125-ARR_v1_62@0,Conclusion & Future Work,Conclusion,"Modify,Other",Other
538,125-ARR,125-ARR_v2_5@4,125-ARR_v1_5@4,"Therefore, how to learn a data-efficient EE model trained with only a few annotated examples is a crucial challenge.","Therefore, how to learn a data-efficient EE model trained with only a few annotated examples is a crucial research question.","Modify,Clarity",Clarity
539,125-ARR,125-ARR_v2_6@0,125-ARR_v1_6@0,"In this paper, we focus on low-resource event extraction, where only a small amount of training examples are available for training.","In this paper, we focus on low-resource event extraction, where only a small amount of training examples are available during training.","Modify,Clarity",Clarity
541,125-ARR,125-ARR_v2_12@2,125-ARR_v1_6@3,DEGREE enjoys the following advantages to learn well with less training data.,DEGREE enjoys the following three advantages to learn well with less training data.,"Modify,Fact/Evidence",Fact/Evidence
542,125-ARR,125-ARR_v2_95@6,125-ARR_v1_83@4,"We consider the following baselines: (1) BERT_QA (Du and Cardie, 2020) (2) OneIE (3) Matching baseline, a proposed baseline that makes trigger predictions by performing string matching between the input passage and the event keywords.","We consider the following baselines: (1) BERT_QA (Du and Cardie, 2020) (2) OneIE (Lin et al., 2020) (3) Matching baseline, a proposed baseline that makes trigger predictions by performing string matching between the input passage and the event keywords.","Modify,Fact/Evidence",Fact/Evidence
543,125-ARR,125-ARR_v2_70@0,125-ARR_v1_87@1,"DEGREE assumes that some weakly-supervised information (the description of events, similar keywords, and human-written templates) is accessible or not expensive for the users to craft.","DEGREE assumes that some weakly-supervised information (the description of events, similar keywords, and human-written templates) is accessible and not expensive.","Modify,Claim",Claim
544,125-ARR,125-ARR_v2_71@0,125-ARR_v1_88@1,"DEGREE fine-tunes the pre-trained generative language model (Lewis et al., 2020).","DEGREE fine-tunes the pretrained generative language (Lewis et al., 2020).","Modify,Clarity",Clarity
545,125-ARR,125-ARR_v2_10@2,125-ARR_v1_10@2,"The input of DEGREE consists of the given passage and our design prompt that contains an event type description, event keywords, and a E2E template.","The input of DEGREE consists of the given passage and our design prompt that contains a event type description, some event keywords, and a E2E template.","Modify,Grammar",Grammar
547,125-ARR,125-ARR_v2_12@5,125-ARR_v1_12@2,"In addition, the sentence structure of the template and the word ""attacked"" depict the semantic relation between the role Attacker and the role Target.","Also, the word ""attacked"" in the prompt depicts the relationship between the role Attacker and the role Target.","Modify,Claim",Claim
548,125-ARR,125-ARR_v2_12@6,125-ARR_v1_12@3,"With these kinds of guidance, DEGREE can make more accurate predictions with less training examples.","With these kinds of guidance, DEGREE can make accurate predictions without many training examples.","Modify,Clarity",Clarity
549,125-ARR,125-ARR_v2_12@7,125-ARR_v1_12@4,"Second, the prompts can incorporate additional weaksupervision signal about the task, such as the description of the event and similar keywords.","Second, the prompts can be further extended to include additional weakly-supervised information about the task, such as the description of the event and similar keywords.","Modify,Clarity",Clarity
550,125-ARR,125-ARR_v2_2@3,125-ARR_v1_2@3,"Given a passage and a manually designed prompt, DEGREE learns to summarize the events mentioned in the passage into a natural sentence that follows a predefined pattern.","Given a passage and a manually designed prompt, DEGREE learns to summarize the event happening in the passage into a natural sentence that follows a predefined pattern.","Modify,Clarity",Clarity
551,125-ARR,125-ARR_v2_12@10,125-ARR_v1_12@5,This information facilitates DEGREE to learn under a low-resource situation.,1 This information facilitates DEGREE to learn under the low-resource situation.,"Modify,Grammar",Grammar
552,125-ARR,125-ARR_v2_12@12,125-ARR_v1_12@7,Leveraging the shared knowledge and dependencies between the two tasks makes our model more data-efficient.,Utilizing the shared knowledge and dependencies between the two tasks makes DEGREE more dataefficient.,"Modify,Clarity",Clarity
553,125-ARR,125-ARR_v2_13@0,125-ARR_v1_13@0,Existing works on EE usually have only one or two of above-mentioned advantages.,Prior approaches on EE usually have only one or two above-mentioned advantages.,"Modify,Clarity",Clarity
554,125-ARR,125-ARR_v2_13@3,125-ARR_v1_13@3,"As a result, our model DEGREE can achieve significantly better performance than prior approaches on low-resource event extraction, as we will demonstrate in Section 3.","As a result, DEGREE can achieve significantly better performance than prior approaches on low-resource event extraction, as we will demonstrate in Section 3.","Modify,Clarity",Clarity
555,125-ARR,125-ARR_v2_2@4,125-ARR_v1_2@4,The final event predictions are then extracted from the generated sentence with a deterministic algorithm.,The final event structure predictions are then extracted from the generated sentence with a deterministic algorithm.,"Modify,Clarity",Clarity
556,125-ARR,125-ARR_v2_18@1,125-ARR_v1_16@1,"Unlike previous works , which separate event extraction into two pipelined tasks (event detection and event argument extraction), DEGREE is designed for the end-to-end event extraction and predict event triggers and arguments at the same time.","Unlike previous works (Wadden et al., 2019;Lin et al., 2020), which separate event extraction into two pipelined tasks (event detection and event argument extraction), DEGREE is designed for the end-to-end event extraction and makes trigger predictions and argument predictions at the same time.","Modify,Fact/Evidence",Fact/Evidence
557,125-ARR,125-ARR_v2_19@0,125-ARR_v1_17@0,The DEGREE Model,DEGREE,"Modify,Other",Other
558,125-ARR,125-ARR_v2_20@4,125-ARR_v1_18@4,"By designing appropriate prompts, we encourage DEGREE to better capture the dependencies between entities and, therefore, to reduce the number of training examples needed.","By designing appropriate prompts, we encourage DEGREE to better capture the dependencies between entities and therefore reduce the number of needed training examples.","Modify,Grammar",Grammar
559,125-ARR,125-ARR_v2_2@5,125-ARR_v1_2@5,DEGREE has three advantages to learn well with less training data.,DEGREE has the following advantages to learn well with less training data.,"Modify,Fact/Evidence",Fact/Evidence
560,125-ARR,125-ARR_v2_2@6,125-ARR_v1_2@6,"First, our designed prompts provide semantic guidance for DEGREE to leverage label semantics and thus better capture the event arguments.","First, with our design of prompts, DEGREE obtains semantic guidance by leveraging label semantics and thus better captures the argument roles.","Modify,Clarity",Clarity
661,13-ARR,13-ARR_v2_2@5,13-ARR_v1_2@5,"Unlike the conventional approach of fine-tuning, we introduce prompt-based learning to achieve fast adaptation for language embeddings, which substantially improves the learning efficiency by leveraging prior knowledge.","Unlike the conventional approach of fine-tuning, we introduce prompt tuning to achieve fast adaptation for language embeddings, which substantially improves the learning efficiency by leveraging prior knowledge.","Modify,Clarity",Clarity
662,13-ARR,13-ARR_v2_2@6,13-ARR_v1_2@6,"By automatically synthesizing trajectoryinstruction pairs in any environment without human supervision and efficient prompt-based learning, our model can adapt to diverse visionlanguage navigation tasks, including VLN and REVERIE.","By automatically synthesizing trajectoryinstruction pairs in any environment without human supervision and instruction prompt tuning, our model can adapt to diverse visionlanguage navigation tasks, including VLN and REVERIE.","Modify,Claim",Claim
663,13-ARR,13-ARR_v2_2@7,13-ARR_v1_2@7,Both qualitative and quantitative results show that our ProbES significantly improves the generalization ability of the navigation model * .,Both qualitative and quantitative results show that our ProbES significantly improves the generalization ability of the navigation model.,"Modify,Grammar",Grammar
664,13-ARR,13-ARR_v2_25@4,13-ARR_v1_26@4,"Then in the prompt tuning process, we only train E p and fix the parameters of E x for the language stream.","Then in the prompt tuning process, we only train E p and fix the parameters of E x .","Modify,Fact/Evidence",Fact/Evidence
665,13-ARR,13-ARR_v2_26@0,13-ARR_v1_26@5,"We sample hard negative paths based on distance in the environment for an instruction-trajectory pair, and the model is trained to choose the best path among them.","Similar to VLN-Bert (Devlin et al., 2018), we sample 3 hard negative paths using beam search for an instruction-trajectory pair, and the model is trained to choose the best path among them.","Modify,Fact/Evidence",Fact/Evidence
666,13-ARR,13-ARR_v2_30@0,13-ARR_v1_31@0,"We experiment with our proposed ProbES on two downstream tasks: goal-oriented navigation task (R2R (Anderson et al., 2018)), and objectoriented navigation task (REVERIE ).","We experiment with our proposed ProbES on two downstream tasks: goal-oriented navigation task (R2R ), and objectoriented navigation task (REVERIE ).","Modify,Fact/Evidence",Fact/Evidence
667,13-ARR,13-ARR_v2_4@2,13-ARR_v1_4@2,"The vision-language navigation (VLN) task (Anderson et al., 2018) is proposed where an agent is required to navigate in a photo-realistic environment stepby-step following a natural language instruction.",The vision-language navigation (VLN) task is proposed where an agent is required to navigate in a photo-realistic environment stepby-step following a natural language instruction.,"Modify,Fact/Evidence",Fact/Evidence
668,13-ARR,13-ARR_v2_33@0,13-ARR_v1_34@0,"We compare ProbES with previous state-of-the-art methods on the R2R dataset in the generative setting, which predicts actions sequentially, as shown in Table 2.","We compare ProbES with previous state-of-the-art methods on the R2R dataset in the generative setting, as shown in Table 2.","Modify,Fact/Evidence",Fact/Evidence
669,13-ARR,13-ARR_v2_34@0,13-ARR_v1_35@0,"We compare ProbES with VLN-BERT in the discriminative setting, which outputs scores for instruction-trajectory pairs, as in Table 4.",We compare ProbES with VLN-BERT in the discriminative setting as in Table 4.,"Modify,Fact/Evidence",Fact/Evidence
670,13-ARR,13-ARR_v2_4@3,13-ARR_v1_5@0,Recent tasks focus on target objects localization that asks an agent to identify an object in an unseen room.,"To solve a more practical problem, the REVERIE task focuses on target objects localization that asks an agent to identify an object in an unseen room.","Modify,Claim",Claim
671,13-ARR,13-ARR_v2_7@5,13-ARR_v1_7@12,"We evaluate ProbES on R2R (Anderson et al., 2018) and REVERIE datasets by discriminative and generative settings.",We evaluate ProbES on R2R and REVERIE datasets by discriminative and generative settings.,"Modify,Fact/Evidence",Fact/Evidence
672,13-ARR,13-ARR_v2_15@2,13-ARR_v1_12@2,"Inspired by BERT (Devlin et al., 2019), much work has extended it to process visual tokens and pretrain on large-scale image-text pairs for learning generic visio-linguistic representations.","Inspired by BERT (Devlin et al., 2018), much work has extended it to process visual tokens and pretrain on large-scale image-text pairs for learning generic visio-linguistic representations.","Modify,Fact/Evidence",Fact/Evidence
685,133-ARR,133-ARR_v2_16@9,133-ARR_v1_16@9,"Semantic hyperedges for word-word connections are derived from topic models (Blei et al., 2001).","Semantic hyperedges for word-word connections are derived from topic models (Blei et al., 2003).","Modify,Fact/Evidence",Fact/Evidence
686,133-ARR,133-ARR_v2_2@5,133-ARR_v1_2@5,"Finally, since Transformers need to compute O(L 2 ) attention weights with sequence length L, the MLP models show higher training and inference speeds on datasets with long sequences.","Finally, since Transformers need to compute O(L 2 ) attention weights with L sequence length, the MLP models show higher training and inference speeds on datasets with long sequences.","Modify,Clarity",Clarity
687,133-ARR,133-ARR_v2_18@4,133-ARR_v1_18@3,"Also Text-RCNN (Lai et al., 2015), a model combining recurrence and convolution uses only the 4 major categories in the 20ng dataset.","Also Tex-tRCNN (Lai et al., 2015), a model combining recurrence and convolution uses only the 4 major categories in the 20ng dataset.","Modify,Grammar",Grammar
688,133-ARR,133-ARR_v2_18@5,133-ARR_v1_18@4,The results of Text-RCNN are identical with BLSTM-2DCNN.,The results of Text-RCNN is identical with BLSTM-2DCNN.,"Modify,Grammar",Grammar
689,133-ARR,133-ARR_v2_18@6,133-ARR_v1_18@5,"For the MR dataset, BLSTM-2DCNN provides no information on the specific split of the dataset.","For the MR dataset, BLSTM-2DCNN provides no information on the specific splitting of the dataset.","Modify,Grammar",Grammar
690,133-ARR,133-ARR_v2_19@0,133-ARR_v1_19@0,"Sequence models: Transformers Surprisingly, only few works consider Transformer models for text categorization.","Sequence models: Transformers Surprisingly, only few works consider Transformer models for topical text classification.","Modify,Clarity",Clarity
691,133-ARR,133-ARR_v2_20@3,133-ARR_v1_20@3,"TinyBERT (Jiao et al., 2020) and Mo-bileBERT (Sun et al., 2020) would be similarly suitable alternatives, among others.","TinyBERT (Jiao et al., 2020) and Mo-bileBERT would be similarly suitable alternatives, among others.","Modify,Fact/Evidence",Fact/Evidence
692,133-ARR,133-ARR_v2_4@1,133-ARR_v1_4@1,"Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021;Li et al., 2020;Zhou et al., 2020;Kowsari et al., 2019;Kadhim, 2019).","Research on text categorization is a very active field as just the sheer amount of new methods in recent surveys shows (Bayer et al., 2021;Zhou et al., 2020;Kowsari et al., 2019;Kadhim, 2019).","Modify,Fact/Evidence",Fact/Evidence
693,133-ARR,133-ARR_v2_26@0,133-ARR_v1_22@0,"As BoW-based model, we consider a one hidden layer WideMLP (i. e., two layers in total).","As BoW-based models, we consider a one hidden layer WideMLP (i. e., two layers in total).","Modify,Grammar",Grammar
696,133-ARR,133-ARR_v2_26@3,133-ARR_v1_22@2,"We list the numbers for fastText, SWEM, and logistic regression from Ding et al. (2020) in our comparison.","We also list the numbers for fastText, SWEM, and logistic regression from Ding et al. (2020) in our comparison.","Modify,Clarity",Clarity
697,133-ARR,133-ARR_v2_28@1,133-ARR_v1_24@1,"For instance, in TextGCN the graph is set up in two parts: word-word connections are modeled by pointwise mutual information and word-document edges resemble that the word occurs in the document.","For instance, TextGCN the graph is set up in two parts: word-word connections modeled by pointwise mutual information and word-document edges resemble that the word occurs in the document.","Modify,Grammar",Grammar
698,133-ARR,133-ARR_v2_28@6,133-ARR_v1_24@6,A detailed discussion of the connection between TextGCN and MLP is provided in Appendix B.,A detailed discussion of the connection between TextGCN and MLP is provided in the Appendix B.,"Modify,Grammar",Grammar
699,133-ARR,133-ARR_v2_37@2,133-ARR_v1_33@2,"The mean sequence length is 551 words with a standard deviation (SD) of 2,047.","The mean sequence length is 551 words with a standard deviation of 2,047.","Modify,Clarity",Clarity
700,133-ARR,133-ARR_v2_5@1,133-ARR_v1_5@1,"Among them are Deep Averaging Networks (DAN) (Iyyer et al., 2015), a deep Multi-Layer Perceptron (MLP) model with n layers that relies on averaging the BoW, Simple Word Embedding Models (SWEM) (Shen et al., 2018) that explores different pooling strategies for pretrained word embeddings, and fastText (Bojanowski et al., 2017), which uses a linear layer on top of pretrained word embed-dings.","Among them are Deep Averaging Networks (DAN) (Iyyer et al., 2015), a deep Multi-Layer Perceptron (MLP) model with n layers that relies on averaging the BoW, Simple Word Embedding Models (SWEM) (Shen et al., 2018) that explores different pooling strategies for pretrained word embeddings, and fastText , which uses a linear layer on top of pretrained word embed-dings.","Modify,Fact/Evidence",Fact/Evidence
701,133-ARR,133-ARR_v2_43@7,133-ARR_v1_39@7,We use this augmentation strategy to increase the number of training examples by a factor of two (BERT w/ shuf. augm.).,We use this augmentation strategy to increase the number of training examples by a factor of two (BERT w/ shuffle augment).,"Modify,Clarity",Clarity
702,133-ARR,133-ARR_v2_6@1,133-ARR_v1_6@1,"Besides TextGCN, there are follow-up works like HeteGCN (Ragesh et al., 2021), TensorGCN (Liu et al., 2020), and HyperGAT (Ding et al., 2020), which we collectively call graph-based models.","Besides TextGCN, there are follow-up works like HeteGCN (Ragesh et al., 2021), TensorGCN , and HyperGAT (Ding et al., 2020), which we collectively call graph-based models.","Modify,Fact/Evidence",Fact/Evidence
703,133-ARR,133-ARR_v2_61@1,133-ARR_v1_57@1,"In such cases, the performance of graph neural networks is the state of the art (Kipf and Welling, 2017;Velickovic et al., 2018) and are superior to MLPs that use only the node features and not the graph structure (Shchur et al., 2018).","In such cases, the performance of graph neural networks is the state of the art (Kipf and Welling, 2017;Veličković et al., 2018) and are superior to MLPs that use only the node features and not the graph structure (Shchur et al., 2018).","Modify,Grammar",Grammar
704,133-ARR,133-ARR_v2_66@3,133-ARR_v1_62@3,"In computer vision, Tolstikhin et al. ( 2021) and Melas-Kyriazi (2021) proposed attention-free MLP models that are on par with the Vision Transformer .","In computer vision, Tolstikhin et al. (2021 and Melas-Kyriazi (2021) attentionfree MLP models are on par with the Vision Transformer (Dosovitskiy et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
705,133-ARR,133-ARR_v2_76@0,133-ARR_v1_70@0,"Depth vs. Width In text classification, width seems more important than depth.","Depth vs width In text classification, width seems more important than depth.","Modify,Grammar",Grammar
706,133-ARR,133-ARR_v2_78@0,133-ARR_v1_72@0,"In our experiments, we did not observe any improvement with more hidden layers (WideMLP-2), as suggested by Iyyer et al. (2015), but it might be beneficial for other, more challenging datasets.","In our experiments, we did not observe any improvement with more hidden layers (WideMLP-2), as suggested by Iyyer et al. (2015), but it might help for other, more challenging, datasets.","Modify,Clarity",Clarity
707,133-ARR,133-ARR_v2_81@1,133-ARR_v1_74@1,"A single layer Text-GCN is a BoW-MLP, except for the document embedding.","A single layer Text-GCN is a bow MLP, except for the document embedding.","Modify,Grammar",Grammar
708,133-ARR,133-ARR_v2_82@0,133-ARR_v1_75@0,The basic GCN equation H = σ( ÂXW ) reveals that the order of transformation and neighborhood aggregation is irrelevant.,The basic GCN equation reveals that the order of transformation and neighborhood aggregation is equivalent.,"Modify,Claim",Claim
709,133-ARR,133-ARR_v2_82@2,133-ARR_v1_75@2,"Truly new documents, as in inductive learning scenarios, would need a special treatment such as using an all zero embedding vector.",Truly new documents would need a special treatment such as using an all zero embedding vector.,"Modify,Clarity",Clarity
710,133-ARR,133-ARR_v2_83@1,133-ARR_v1_76@1,"On bag-of-words inputs, the first layer W (1) x + b (1) can be replaced by an equivalent embedding layer with weighting (e. g., TF-IDF or length normalization) being applied during aggregation of the embedding vectors.","The first layer can be replaced by an embedding layer such that H = XE, where X is the weighted term-document matrix.","Modify,Fact/Evidence",Fact/Evidence
711,133-ARR,133-ARR_v2_2@1,133-ARR_v1_2@1,We show that a wide multi-layer perceptron (MLP) using a Bag-of-Words (BoW) outperforms the recent graph-based models TextGCN and Hete-GCN in an inductive text classification setting and is comparable with HyperGAT.,"We show that a simple multi-layer perceptron (MLP) using a ""Bag of Words"" (BoW) outperforms the recent graph-based models TextGCN and Het-eGCN in an inductive text classification setting and is comparable with HyperGAT.","Modify,Clarity",Clarity
712,133-ARR,133-ARR_v2_88@2,133-ARR_v1_81@2,"Only with a second layer, TextGCN considers the embedding of other documents whose words are connected to the present documents' words.","Only with a second layer, TextGCN considers the embedding of other documents whose words are related to the present documents' words.","Modify,Clarity",Clarity
713,133-ARR,133-ARR_v2_12@6,133-ARR_v1_12@6,"We check whether modified versions of the datasets have been used (e. g., fewer classes), to avoid bias and wrongfully giving advantages.","We check whether modified versions of the datasets have been used (e. g., less classes), to avoid bias and wrongfully giving advantages.","Modify,Grammar",Grammar
714,133-ARR,133-ARR_v2_13@4,133-ARR_v1_13@4,"In fastText (Bojanowski et al., 2017;Joulin et al., 2017) a linear layer on top of pretrained embeddings is used for classification.",In fastText a linear layer on top of pretrained embeddings is used for classification.,"Modify,Fact/Evidence",Fact/Evidence
715,133-ARR,133-ARR_v2_16@1,133-ARR_v1_16@1,"Examples of GNN-based methods operating on a word-document co-occurence graph are TextGCN (Yao et al., 2019) and its successor TensorGCN (Liu et al., 2020) as well as Hete-GCN (Ragesh et al., 2021), HyperGAT (Ding et al., 2020), andDADGNN (Liu et al., 2020).","Examples of GNN-based methods operating on a word-document co-occurence graph are TextGCN (Yao et al., 2019) and its successor TensorGCN as well as Hete-GCN (Ragesh et al., 2021), HyperGAT (Ding et al., 2020), and DADGNN .","Modify,Fact/Evidence",Fact/Evidence
896,14-ARR,14-ARR_v2_16@0,14-ARR_v1_18@0,The construction of VHED is shown in Figure 2.,The construction of VHED is shown in Fig. 2.,"Modify,Clarity",Clarity
897,14-ARR,14-ARR_v2_28@2,14-ARR_v1_28@2,"SIMCSE uses contrastive learning with dropout as augmentation, then trained on natural langauge inference datasets to obtain better sentence embeddings from BERT (Devlin et al., 2019).","SIMCSE uses contrastive learning with dropout as augmentation, then trained on natural langauge inference datasets to obtain better sentence embeddings from BERT (Devlin et al., 2018).","Modify,Fact/Evidence",Fact/Evidence
898,14-ARR,14-ARR_v2_29@2,14-ARR_v1_29@2,"We hypothesize that utilizing this feature makes it possible to extract more information, making it easier for the model to learn human judgment.","We hypothesize that thus doing makes it possible to extract more information, making it easier for the model to learn human judgment for story pairs.","Modify,Clarity",Clarity
899,14-ARR,14-ARR_v2_29@3,14-ARR_v1_29@3,"However, due to the small amount of data available, high variance is likely (Mosbach et al., 2020) to occur during inference.","However, due to the small amount of data available, high variance is likely (Mosbach et al., 2021) to occur during inference.","Modify,Fact/Evidence",Fact/Evidence
900,14-ARR,14-ARR_v2_29@4,14-ARR_v1_29@4,"Hence, we used all data from VHED, including human agreement=3 to increase the stability of our model following Mosbach et al. (2020).","Hence, we used all data from VHED, including human agreement=3 to increase the stability of our model following Mosbach et al. (2021).","Modify,Fact/Evidence",Fact/Evidence
901,14-ARR,14-ARR_v2_33@3,14-ARR_v1_33@3,"Given the story pair (x 1 , x 2 ), the automatic metric being assessed predicts the corresponding story quality scores (s 1 , s 2 ) which we compare to the averaged ranks y 1 and y 2 of x 1 and x 1 from human evaluation.","Given the story pair (x 1 , x 2 ), the autometric being assessed predicts the corresponding story quality scores (s 1 , s 2 ) which we compare to the averaged ranks y 1 and y 2 of x 1 and x 1 from human evaluation.","Modify,Clarity",Clarity
902,14-ARR,14-ARR_v2_40@1,14-ARR_v1_39@1,"We also implement the more recent BERT-Score, BLEURT, and UNION as baseline metrics.","We also considered the more recent BERT-Score, BLEURT, and UNION as baseline metrics.","Modify,Clarity",Clarity
903,14-ARR,14-ARR_v2_40@2,14-ARR_v1_39@2,"In addition to the above automatic metrics, we also include a random baseline, denoted as Random in Table 4, to provide a random score for each story as the lower bound.","In addition to the above automatic metrics, we also included a random baseline to provide a random score for each story, shown as Random in Table 4, as the lower bound.","Modify,Clarity",Clarity
904,14-ARR,14-ARR_v2_0@0,14-ARR_v1_0@0,Learning to Rank Visual Stories from Human Ranking Data,Learning to Rank Visual Stories From Human Ranking Data,"Modify,Grammar",Grammar
905,14-ARR,14-ARR_v2_43@1,14-ARR_v1_42@1,"This algorithm only applies when evaluating story pairs containing references, i.e., reference-machine pairs in this paper.","The Reference Absent Algorithm only applies when evaluating story pairs containing references, i.e., reference-machine pairs in this paper.","Modify,Clarity",Clarity
906,14-ARR,14-ARR_v2_47@10,14-ARR_v1_47@10,"We also find that Vrank ranks correctly when machine is better than reference, showing that Vrank yields 26.5% recall when the other metrics have 0 recall without Eq. 3 and ∼18% with Eq. 3.",Another analysis to study ability of Vrank to rank correctly when machine is better than reference shows that Vrank yields 26.5% recall when the other metrics have 0 recall without Eq. 3 and ∼18% with Eq. 3.,"Modify,Clarity",Clarity
907,14-ARR,14-ARR_v2_50@2,14-ARR_v1_49@2,It is crucial for automatic metrics to also recognize errors to judge generated text.,It is crucial for automatic metrics to also recognize such errors to judge generated text.,"Modify,Clarity",Clarity
908,14-ARR,14-ARR_v2_54@1,14-ARR_v1_53@1,"To determine whether Vrank generalizes to textual stories, we selected MANS dataset (Guan et al., 2021), an imagefree storytelling dataset in which the stories are derived from the ROCStories corpus .","To determine whether Vrank generalizes to textual stories, we selected as the benchmark the MANS dataset (Guan et al., 2021), an image-free storytelling dataset in which the stories are derived from the ROCStories corpus.","Modify,Clarity",Clarity
909,14-ARR,14-ARR_v2_54@2,14-ARR_v1_53@2,"MANS includes 200 story prompts, where each prompt includes five model-generated stories and a reference.","This dataset includes 200 story prompts, where each prompt includes five model-generated stories and a reference.","Modify,Clarity",Clarity
910,14-ARR,14-ARR_v2_61@0,14-ARR_v1_60@0,"After applying Vrank to assess five recent VIST models, we present the results in Figure 4: the models are gradually approaching human-level writing, outlining an exciting development of NLG in VIST.","After applying Vrank to assess five recent VIST models, we present the results in Fig. 4: the models are gradually approaching human-level writing, outlining an exciting development of NLG in VIST.","Modify,Clarity",Clarity
911,14-ARR,14-ARR_v2_61@2,14-ARR_v1_60@2,We also show the correlation between different error types in Figure 5.,We also show the correlation between different error types in Fig. 5.,"Modify,Clarity",Clarity
912,14-ARR,14-ARR_v2_61@4,14-ARR_v1_60@4,"Ranking Gap Distribution The ranking gap distribution is shown in Figure 6, in which both the ranking gaps and the number of stories are normalized.","Ranking Gap Distribution The ranking gap distribution is shown in Fig. 6, in which both the ranking gaps and the number of stories are normalized.","Modify,Clarity",Clarity
913,14-ARR,14-ARR_v2_65@2,14-ARR_v1_64@2,"The batch size is set as 32 and the random seed for training can be set as 7,777 for reproduction.",The batch size is set as 32 and the random seed for training can be set as 7777 for reproduction.,"Modify,Grammar",Grammar
914,14-ARR,14-ARR_v2_8@1,14-ARR_v1_9@1,We then re-purposed VHED to create a better metric Vrank for VIST to rank visual stories.,We then re-purposed VHED to create a better metric for VIST named Vrank (VIST Ranker).,"Modify,Fact/Evidence",Fact/Evidence
915,14-ARR,14-ARR_v2_9@7,14-ARR_v1_10@7,"Indeed, 38% of machine-generated stories are better than the references, which suggests that the afore-mentioned assumption may need to be revisited .","Indeed, 38% of machine-generated stories are better than the references, which suggests that the afore-mentioned assumption may need to be revisited (Clark et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
918,14-ARR,14-ARR_v2_9@14,14-ARR_v1_11@0,"Specifically, we make three major contributions:",The contributions of this paper are threefold:,"Modify,Claim",Claim
945,141-ARR,141-ARR_v2_21@0,141-ARR_v1_20@0,"The hypothesis H is a logic program consisting of definite clauses b 1 ∧ ... ∧ b N ⇒ h where b 1 , ..., b N and h are logic atoms.","The hypothesis H is a logic program consisting of definite clauses b 1 ∧ ... ∧ b k ⇒ h where b 1 , ..., b k and h are logic atoms.","Modify,Fact/Evidence",Fact/Evidence
946,141-ARR,141-ARR_v2_29@0,141-ARR_v1_29@0,"Overall, DILR simulates a multi-hop reasoning process considering different number of inference steps.","Overall, DILR simulates multi-hop reasoning processes considering different number of inference steps.","Modify,Grammar",Grammar
947,141-ARR,141-ARR_v2_31@0,141-ARR_v1_31@0,"To avoid inevitable errors brought by off-the-shelf NER tools for named entity extraction, we propose to extract relevant information using an attentive reader.","To avoid inevitable errors brought by the NER tools for named entity extraction, we propose to learn to extract relevant information using an attentive reader.","Modify,Clarity",Clarity
948,141-ARR,141-ARR_v2_33@0,141-ARR_v1_33@0,"Given a query subject s with n s tokens, a candidate a with n a tokens, and a context c of length n c , we denote by S ∈ R ns×D , A ∈ R na×D and C ∈ R nc×D their word features after a biGRU layer, respectively.","Given a query subject s with n s tokens, a candidate a with n a tokens, and a context c of length n c , we denote by S ∈ R ns×D , A ∈ R na×D and C ∈ R nc×D as their word features after a biGRU layer, respectively.","Modify,Grammar",Grammar
949,141-ARR,141-ARR_v2_4@1,141-ARR_v1_4@1,"However, when the background knowledge is expressed in natural languages, as shown in the multi-hop reading comprehension problem with triplet-form questions (Welbl et al., 2018), it becomes difficult to conduct complex reasoning because the entities and relations are not explicitly labeled in the documents.","However, when the background knowledge is expressed in natural languages, as shown in the multi-hop reading comprehension problem with triplet-form questions (Welbl et al., 2018), it becomes difficult to conduct complex reasoning.","Modify,Claim",Claim
950,141-ARR,141-ARR_v2_35@1,141-ARR_v1_35@1,We obtain the normalized similarity score α s ij between the i-th token in the subject and the j-th token in the context via a softmax operation on each row of B s .,We obtain the normalized similarity score α s ij between i-th token in the subject and j-th token in the context via a softmax operation on each row of B s .,"Modify,Grammar",Grammar
951,141-ARR,141-ARR_v2_37@2,141-ARR_v1_37@2,"We denote by s = β s S, and a = β a A the feature representation of the query subject and the candidate entity, respectively.","We denote by s = β s S, and a = β a A the query subject and candidate representations, respectively.","Modify,Clarity",Clarity
952,141-ARR,141-ARR_v2_38@0,141-ARR_v1_38@0,"For (l + 1)-hop reasoning (l ≥ 0), it is desired to relocate to intermediate (bridging) entities that are related to the l-hop entities.","For (l + 1)-hop reasoning (l ≥ 0), it is desired to relocate to intermediate (bridging) entities related to the l-hop entities.","Modify,Clarity",Clarity
953,141-ARR,141-ARR_v2_2@0,141-ARR_v1_2@0,Multi-hop reading comprehension requires an ability to reason across multiple documents.,Multi-hop reading comprehension requires the ability to reason across multiple documents.,"Modify,Grammar",Grammar
954,141-ARR,141-ARR_v2_40@1,141-ARR_v1_40@1,We use α l+1 ij to denote a normalized attention score between the i-th and the j-th context tokens after applying a softmax operator over each row of B l+1 .,We use α l+1 ij to denote a normalized attention score between i-th and j-th context tokens after applying a softmax operator over each row of B l+1 .,"Modify,Grammar",Grammar
955,141-ARR,141-ARR_v2_54@0,141-ARR_v1_55@0,The multi-hop reasoner aims to conduct complex reasoning by first generating probable logic clauses and then evaluating each clause by instantiating the variables with relevant contexts obtained from the attentive reader.,The multi-hop reasoner aims to conduct complex reasoning by first generating probable logic clauses and then evaluating each clause by instantiating the variables.,"Modify,Fact/Evidence",Fact/Evidence
956,141-ARR,141-ARR_v2_54@2,141-ARR_v1_55@2,An illustration of the procedure is shown in Figure 1 and is elaborated in the following sub-section.,An illustration of the procedure is shown in Figure 1 and will be elaborated in the next section.,"Modify,Clarity",Clarity
957,141-ARR,141-ARR_v2_54@3,141-ARR_v1_55@3,"The clause evaluation process is then to instantiate variables in each atom with constants such as query subjects, candidate entities or bridging entities.","Then the clause evaluation process will ground each atom with query subjects, candidate entities or bridging entities.","Modify,Fact/Evidence",Fact/Evidence
958,141-ARR,141-ARR_v2_54@4,141-ARR_v1_55@4,"The outputs from the attentive reader, i.e., hs , ha and {h l k }'s (l > 0), can be used as feature representations for these constants to compute the atom scores for clause evaluation and updates.","The outputs from the attentive reader, i.e., hs , ha and {h l k }'s (l > 0), can be regarded as these constant representations to compute the atom scores for clause evaluation and updates.","Modify,Clarity",Clarity
959,141-ARR,141-ARR_v2_59@0,141-ARR_v1_58@0,"The clause generation process is divided into two stages: 1) to generate clauses defining invented predicates using only the existential predicates, and 2) to generate final clauses defining query relation q using only the invented predicates.",The clause generation process is divided into two stages: 1) generate clauses defining invented predicates using only the existential predicates; 2) generate final clauses defining query relation using only the invented predicates.,"Modify,Clarity",Clarity
960,141-ARR,141-ARR_v2_61@1,141-ARR_v1_60@2,"We use sparsemax, a sparse version of softmax (Martins and Astudillo, 2016), to select only a small number of predicates.","We use sparsemax which is a sparse version of softmax (Martins and Astudillo, 2016) to select only a small number of predicates.","Modify,Clarity",Clarity
961,141-ARR,141-ARR_v2_61@2,141-ARR_v1_60@3,"Intuitively, to learn to define a l-hop invented predicate r l m , ( 5) and ( 6) sequentially produce F t (X t , X t+1 ) at each step t ∈ {0, ..., l} to form the clause body by attending over all the existential predicates with attention weight S l t .","Intuitively, to learn to define a l-hop invented predicate r l m , ( 5) and (6) will sequentially produce F t (X t , X t+1 ) at each step t ∈ {0, ..., l} to form the clause body by attending over all the existential predicates with attention weight S l t .","Modify,Grammar",Grammar
962,141-ARR,141-ARR_v2_62@2,141-ARR_v1_62@2,"Given an embedding u q ∈ R D for the target relation q, we use a multi-head attention mechanism to compute a probability distribution s h over all the invented predicates for each head h ∈ {1, ..., H} to produce the h-th final clause:","Given an embedding u q ∈ R D for the target relation q, we use a multi-head attention mechanism to compute a probability distribution s i over all the invented predicates for each head i ∈ {1, ..., H} to produce the i-th final clause:","Modify,Fact/Evidence",Fact/Evidence
963,141-ARR,141-ARR_v2_64@1,141-ARR_v1_64@1,"For example, if s h selects r 0 1 and r 1 2 , the final clause becomes r 0 1 (X, Y ) ∧ r 1 2 (X, Y ) ⇒ q(X, Y ), which involves at most 1 inference step because r 1 2 is a 1-hop invented predicate.","For example, if s i selects r 0 1 and r 1 2 , the final clause becomes r 0 1 (X, Y ) ∧ r 1 2 (X, Y ) ⇒ q(X, Y ), which involves at most 1 inference step (r 12 ).","Modify,Fact/Evidence",Fact/Evidence
964,141-ARR,141-ARR_v2_66@0,141-ARR_v1_66@0,"Instantiation The clauses generated using the attentive memories need to be tested and refined against the given positive and negative examples, known as learning from entailment that tries to maximize the truth probabilities of positive examples and minimize those of negative examples.","Instantiation The clauses generated using the attentive memories will be tested and refined against the given positive and negative examples, known as learning from entailment that tries to maximize the truth probabilities of positive examples and minimize those of negative examples.","Modify,Clarity",Clarity
965,141-ARR,141-ARR_v2_66@1,141-ARR_v1_66@1,"The positive examples correspond to q(s, a) and the negative examples correspond to {q(s, a − )}'s, where s, a and a − refers to the query subject, correct answer and incorrect candidate, respectively.","The positive examples correspond to q(s, a) and the negative examples correspond to {q(s, a j )}'s, where s, a and a j refers to the query subject, correct answer and incorrect candidate, respectively.","Modify,Fact/Evidence",Fact/Evidence
966,141-ARR,141-ARR_v2_66@2,141-ARR_v1_66@2,"To obtain the truth probabilities of these atoms, we first instantiate the variables for each generated clause with constant contexts, e.g., X = s and Y = a (or Y = a − ) in q(X, Y ).","To obtain the truth probabilities of these atoms, we first instantiate the variables for each generated clause, e.g., X = s and Y = a (or Y = a j ) in q(X, Y ).","Modify,Fact/Evidence",Fact/Evidence
967,141-ARR,141-ARR_v2_66@4,141-ARR_v1_66@4,"Specifically, to instantiate each X l , we pick top-K contexts (documents) {c l 1 , ..., c l K } ⊆ C, namely X l = c l k , 1 ≤ k ≤ K with highest probabilities according to p l k computed via (4).","To avoid inaccurate selection, for each X l , we pick K contexts {c l 1 , ..., c l K } with highest probabilities according to p l k in (4).","Modify,Fact/Evidence",Fact/Evidence
968,141-ARR,141-ARR_v2_66@5,141-ARR_v1_66@5,"Neural Logic Operator Given a definite clause b 1 ∧ ... ∧ b N ⇒ h consisting of grounded atoms (e.g., b 1 = r 1 (s, a)), we could obtain the value for its head atom as µ(h) = µ(b 1 ∧ ... ∧ b N ).","Neural Logic Operator Given a definite clause b 1 ∧ ... ∧ b K ⇒ h consisting of grounded atoms (e.g., b 1 = r 1 (s, a)), we could obtain the value for its head atom as µ(h) = µ(b 1 ∧ ... ∧ b k ).","Modify,Fact/Evidence",Fact/Evidence
969,141-ARR,141-ARR_v2_85@1,141-ARR_v1_73@1,"When N = 2, the RHS of the inequality equals to 1/4 • µ n̸ =min , which makes G ∧ closer to µ min when µ n̸ =min is smaller.","When K = 2, the RHS of the inequality equals to 1/4 • µ k̸ =min , which makes G ∧ closer to µ min when µ k̸ =min is smaller.","Modify,Fact/Evidence",Fact/Evidence
970,141-ARR,141-ARR_v2_85@3,141-ARR_v1_73@4,"Moreover, It avoids exponential decay in the output when N > 1.","Moreover, It avoids exponential decay in the output when K > 1.","Modify,Fact/Evidence",Fact/Evidence
971,141-ARR,141-ARR_v2_8@4,141-ARR_v1_8@3,"However, DNNs only implicitly encode relevant contexts and fail to explicitly uncover the underlying relational compositions for complex inference.","However, DNNs only implicitly encode relevant contexts but fail to explicitly uncover the underlying relational compositions for complex inference.","Modify,Clarity",Clarity
972,141-ARR,141-ARR_v2_90@4,141-ARR_v1_75@4,We use a max operator to generate the maximum score over all possible instantiations in Z l to represent the final truth probability of each invented predicate.,We use a max operator to generate the maximum score over all possible bridging entities to represent the final truth probability of each invented predicate.,"Modify,Fact/Evidence",Fact/Evidence
973,141-ARR,141-ARR_v2_92@2,141-ARR_v1_77@1,"Here we organize the dataset according to subject-candidate pairs: (s, a).","Here we organize the dataset according to subject-candidate pairs: (s n , a n ).","Modify,Fact/Evidence",Fact/Evidence
974,141-ARR,141-ARR_v2_8@5,141-ARR_v1_8@4,"For instance, in the above example, DNNs may encode Bequia and Gladys Johnson into 1-hop features, given the fact that both entities co-occur with the query Moonhole.","With the above example, DNNs may encode Bequia and Gladys Johnson into 1-hop features, given both entities co-occur with the query Moonhole.","Modify,Clarity",Clarity
975,141-ARR,141-ARR_v2_92@3,141-ARR_v1_77@2,"We associate the ground-truth label y = 1 with (s, a) if a is the correct answer, otherwise, y = 0.","We associate the ground-truth label y n = 1 with (s n , a n ) if a n is the correct answer, otherwise, y n = 0.","Modify,Fact/Evidence",Fact/Evidence
976,141-ARR,141-ARR_v2_94@5,141-ARR_v1_79@5,"We define M = 10 relations as existential predicates and M l = 5 invented predicates for each hop with (Weber et al., 2019).","We define M = 10 relations as existential predicates and M l = 5 invented predicates for each hop with l = 0, 1, 2.","Modify,Fact/Evidence",Fact/Evidence
977,141-ARR,141-ARR_v2_8@7,141-ARR_v1_8@6,"In contrast, human beings would easily produce the correct answer given the knowledge ""if A is in B and B is part of country C, then A is in country C"" and by examining the relations between each entity pair co-occurred in the context.","However, a human would easily produce the correct answer given the knowledge ""if A is in B and B is part of country C, then A is in country C"" and by examining the relations between each entity pair co-occurred in the context.","Modify,Clarity",Clarity
978,141-ARR,141-ARR_v2_95@0,141-ARR_v1_81@14,"For a more thorough analysis, we take the entire WikiHop dataset and group the query relations in terms of the number of training instances.","For a more thorough analysis, we group the query relations in terms of the number of training instances.","Modify,Fact/Evidence",Fact/Evidence
981,141-ARR,141-ARR_v2_94@8,141-ARR_v1_81@20,"Even with well-trained contextualized word embeddings (DILR-BERT), our model still brings consistent performance gains.","Even with well-trained contextualized word embeddings, DILR still brings consistent performance gains.","Modify,Fact/Evidence",Fact/Evidence
982,141-ARR,141-ARR_v2_97@3,141-ARR_v1_83@3,"Clearly, ≤ 0 Hop and ≤ 3 Hop produce lower accuracies because ≤ 0 Hop fails to model the bridging entities and ≤ 3 Hop could overfit the model given most of the questions only involve at most 2 reasoning hops.","Clearly, ≤ 0 Hop and ≤ 3 Hop produce lower accuracies due to either missing bridging entities or overfitting with excessive inference steps.","Modify,Fact/Evidence",Fact/Evidence
983,141-ARR,141-ARR_v2_9@1,141-ARR_v1_9@1,"To answer the previous query, ILP could generate a rule as located_in(X, Z) ∧ country(Z, Y ) ⇒ country(X, Y ).","To answer the previous query, ILP could generate this rule: located_in(X, Z)∧ country(Z, Y ) ⇒ country(X, Y ).","Modify,Clarity",Clarity
984,141-ARR,141-ARR_v2_9@6,141-ARR_v1_9@6,"However, their work relies on the degree of precision for pre-extracted NERs and is limited by the number of rule templates.","However, their work relies on the accuracies of pre-extracted NERs and is limited by the number of rule templates.","Modify,Clarity",Clarity
985,141-ARR,141-ARR_v2_10@0,141-ARR_v1_10@0,"To address the aforementioned limitations, we propose a novel end-to-end integration of deep learning and logic reasoning termed Deep Inductive Logic Reasoning (DILR).","To address these limitations, we propose a novel end-to-end combination of deep learning and logic reasoning termed Deep Inductive Logic Reasoning (DILR).","Modify,Clarity",Clarity
986,141-ARR,141-ARR_v2_10@1,141-ARR_v1_10@1,"It consists of two components: 1) a hierarchical attentive reader that filters query-related and candidate-related information from given documents, and 2) a multihop reasoner that conducts inductive logic reasoning by attentively selecting proper predicates to form candidate rules and refines them upon given examples.",It consists of two components: 1) a hierarchical attentive reader that filters query-related and candidate-related information from given documents; 2) a multi-hop reasoner that conducts inductive logic reasoning by attentively selecting proper predicates to form candidate rules and refines them upon given examples.,"Modify,Grammar",Grammar
987,141-ARR,141-ARR_v2_14@1,141-ARR_v1_14@1,"To explicitly incorporate entity connections, De Cao et al. (2019), Ding et al. (2019), Qiu et al. (2019), Tang et al. (2020), Song et al. (2018) and Tu et al. (2019) proposed to build entity graphs and apply Graph Neural Networks for information propagation.","To explicitly incorporate entity connections, De Cao et al. ( 2019), Ding et al. (2019), Qiu et al. (2019), Tang et al. (2020), Song et al. (2018) and Tu et al. (2019) build entity graphs and apply Graph Neural Networks for information propagation.","Modify,Clarity",Clarity
988,141-ARR,141-ARR_v2_14@2,141-ARR_v1_14@2,Kundu et al. (2019) formalized reasoning as a path-finding problem with neural encoding to rank candidate paths.,Kundu et al. (2019) formalizes reasoning as a path-finding problem with neural encoding to rank candidate paths.,"Modify,Grammar",Grammar
989,141-ARR,141-ARR_v2_14@3,141-ARR_v1_14@3,Path modeling was also adopted in using pointer networks.,Path modeling is also adopted in using pointer networks.,"Modify,Grammar",Grammar
990,141-ARR,141-ARR_v2_14@4,141-ARR_v1_14@4,"However, these approaches only focus on local information without the ability to generalize, and some of them rely on off-the-shelf NER tools.","However, these approaches only focus on local information without the ability to generalize, and some of them rely on NER tools.","Modify,Clarity",Clarity
991,141-ARR,141-ARR_v2_14@5,141-ARR_v1_14@5,Dhingra et al. (2020) proposed to convert texts into a virtual knowledge base for retrieval using a pre-constructed entity database.,"Dhingra et al. (2020) converts texts into a virtual knowledge based for retrieval, but requires an entity database.","Modify,Clarity",Clarity
992,141-ARR,141-ARR_v2_18@1,141-ARR_v1_17@1,"Formally, for each RC problem, we are given a set of documents C = {c 1 , ..., c K }, a structured query in the form of a relational triplet (s, q, ?), where s denotes the subject of the relation q, and a list of candidate answers A = {a 1 , ..., a n }.","Formally, for each RC problem, we are given a set of documents D = {D 1 , ..., D n }, a structured query in the form of a relational triplet (s, q, ?) where s denotes the subject of the relation q, and a list of candidate answers A = {a 1 , ..., a m }.","Modify,Fact/Evidence",Fact/Evidence
1022,15-ARR,15-ARR_v2_21@2,15-ARR_v1_20@3,"The term ""behind"" reflects the fact that many arguments do not explicate values; for example, in the argument ""no matter they felt forced to commit it: anyone who commits a crime should be prosecuted"" no value is mentioned literally.","The term ""behind"" reflects the fact that many arguments do not explicate values; e.g., in the argument ""no matter they felt forced to commit it: anyone who commits a crime should be prosecuted"" no value is mentioned literally.","Modify,Clarity",Clarity
1023,15-ARR,15-ARR_v2_21@7,15-ARR_v1_20@6,The task studied in this paper is to draw this connection between arguments and values automatically.,The task at hand is to draw this connection automatically.,"Modify,Clarity",Clarity
1024,15-ARR,15-ARR_v2_22@2,15-ARR_v1_21@2,We also asked the annotators to comment on supposedly missing values (see Section 4).,We also asked the annotators to comment on supposedly missing values (cf. Section 4).,"Modify,Clarity",Clarity
1025,15-ARR,15-ARR_v2_22@3,15-ARR_v1_21@3,"For most of the additional 48 value descriptions that we received (be humane, be fair, be modern, etc.), we identified existing values or value combinations in the taxonomy that subsume them, suggesting to extend the value descriptions rather than adding new values.","For most of the additional 48 value descriptions that we received (be humane, be fair, be modern, etc.) we were able to identify in the proposed taxonomy existing values or value combinations that subsume them, suggesting to extend the value descriptions rather than adding new values.","Modify,Clarity",Clarity
1026,15-ARR,15-ARR_v2_22@4,15-ARR_v1_21@4,Only two of the added values are not directly related to the universal needs that Schwartz (1994) based the value categories on.,Only two of the added values are not directly related to the universal needs where Schwartz (1994) based the value categories on.,"Modify,Grammar",Grammar
1027,15-ARR,15-ARR_v2_22@6,15-ARR_v1_21@6,"We adopt a uniform naming scheme where the value names reflect the distinction of Rokeach (1973) into instrumental (be . . . ) and terminal (have . . . ) values, and are easy to embed in sentences, for example, ""it is good to be creative.""","We adopt a uniform naming scheme where the value names reflect the distinction of Rokeach (1973) into instrumental (be . . . ) and terminal (have . . . ) values, that can be easily embedded in sentences, for example, ""it is good to be creative.""","Modify,Clarity",Clarity
1028,15-ARR,15-ARR_v2_23@7,15-ARR_v1_22@1,"In addition, for the 41 values with a link to the World Values Survey (the WVS column in Table 1, Haerpfer et al., 2020), the corresponding dataset contains information on people's value priorities (i.e., value systems) collected rigorously for 51 territories, with the earliest survey from 1981 and the latest from 2020.","For example, for the 41 values with a link to the World Values Survey (the WVS column in Table 1, Haerpfer et al., 2020), the corresponding dataset contains information on people's value priorities (i.e., value systems) collected rigorously for 51 territories, with the earliest survey from 1981 and the latest from 2020.","Modify,Clarity",Clarity
1029,15-ARR,15-ARR_v2_25@2,15-ARR_v1_29@2,"The dataset, taxonomy description, and annotation interface are available online as Webis-ArgValues-22.","The dataset, a taxonomy description, and the annotation interface are available online.","Modify,Fact/Evidence",Fact/Evidence
1030,15-ARR,15-ARR_v2_27@1,15-ARR_v1_31@1,"Each argument consists of one premise, one conclusion, and a stance attribute indicating whether the premise is in favor of (pro) or against (con) the conclusion.","Each argument consists of one premise, one conclusions, and a stance attribute indicating whether the premise is in favor of (pro) or against (con) the conclusion.","Modify,Grammar",Grammar
1031,15-ARR,15-ARR_v2_27@2,15-ARR_v1_31@3,"As existing argument datasets are almost exclusively from a Western background, we had to collect new suitable arguments for the non-US parts, drastically limiting their size.","However, as existing argument datasets are almost exclusively from a Western background, we had to collect new suitable arguments for the other parts.","Modify,Claim",Claim
1032,15-ARR,15-ARR_v2_4@3,15-ARR_v1_4@3,"Some values tend to conflict and others to align (see Figure 1), which can cause disagreement on the best course forward, but also the support, if not formation, of political parties that promote the respective highly revered values.","Some values tend to conflict and others to align (cf. Figure 1), which can cause disagreement on the best course forward, but also the support, if not formation, of political parties that promote the respective highly revered values.","Modify,Clarity",Clarity
1033,15-ARR,15-ARR_v2_5@0,15-ARR_v1_5@0,"Due to their outlined importance, human values are studied both in the social sciences (Schwartz, 1994) and in formal argumentation (Bench-Capon, 2003) for decades.","Due to their outlined importance, human values are studied both in the social sciences (Schwartz, 1994) and formal argumentation (Bench-Capon, 2003) since decades.","Modify,Grammar",Grammar
1034,15-ARR,15-ARR_v2_50@10,15-ARR_v1_45@1,"Despite the difficulty of the annotation task, the crowdworker annotators reached an average value-wise agreement α of 0.49 (Krippendorff, 2004).","Despite the difficulty of the annotation task, the annotators reached an average value-wise agreement α of 0.49 (Krippendorff, 2004).","Modify,Clarity",Clarity
1035,15-ARR,15-ARR_v2_50@13,15-ARR_v1_45@2,"One step we implemented for quality assurance is that we manually checked the 48 arguments (<1%) to which MACE assigned more than 10 values, reducing their values to the most prevalent 5-7 ones.","Moreover, we manually checked the 48 arguments (<1%) to which MACE assigned more than 10 values, reducing their values to the most prevalent 5-7 ones.","Modify,Fact/Evidence",Fact/Evidence
1036,15-ARR,15-ARR_v2_50@14,15-ARR_v1_45@3,"The right side of Table 1 shows the frequency of each value in each dataset part, revealing that each value occurs at least once.",The right side of Table 1 shows the frequency of each value in each dataset part.,"Modify,Fact/Evidence",Fact/Evidence
1037,15-ARR,15-ARR_v2_56@1,15-ARR_v1_52@1,"The approaches are trained on the arguments from 60 unique conclusions (4240 arguments, ~85%), validated on 4 (277, ~5%), and tested on 7 (503, ~10%).","The approaches are trained on the arguments from 60 unique conclusions (4240 arguments), validated on 4 (277), and tested on 7 (503).","Modify,Fact/Evidence",Fact/Evidence
1040,15-ARR,15-ARR_v2_57@6,15-ARR_v1_53@2,"Moreover, while a F 1 -score of 0.25 at Level 1 is encouraging for largely out-of-the-box approaches, clearly more work is needed.","While a F 1 -score of 0.25 is encouraging for largely out-of-the-box approaches, clearly more work is needed.","Modify,Clarity",Clarity
1041,15-ARR,15-ARR_v2_57@7,15-ARR_v1_53@3,"Though a recall of 0.19 may be acceptable for applications that not rely on completeness, a precision of 0.40 is clearly too low for practical uses.","Though a recall of 0.19 may be acceptable for applications that not rely on completeness, a precision of 0.40 seems low for practical uses.","Modify,Other",Other
1042,15-ARR,15-ARR_v2_58@7,15-ARR_v1_54@4,The results are distributed alongside the dataset for follow-up analyses.,The complete results are distributed alongside the dataset.,"Modify,Fact/Evidence",Fact/Evidence
1043,15-ARR,15-ARR_v2_60@2,15-ARR_v1_56@2,"However, the 1-Baseline is equally affected by this lack, thus providing for a comparison with the previous setting.","However, the 1-Baseline is equally effected by this lack, thus providing for a comparison with the previous setting.","Modify,Grammar",Grammar
1044,15-ARR,15-ARR_v2_6@0,15-ARR_v1_7@0,"To understand the pragmatics of this argument, a reader has to acknowledge the belief (Point 1 in the definition above) that the ""end state"" (2) of having a comfortable life is desirable in general (3).","To understand the pragmatics of this statement, a reader has to acknowledge the belief (Point 1 in the definition above) that the ""end state"" (2) of having a comfortable life is desirable in general (3).","Modify,Clarity",Clarity
1045,15-ARR,15-ARR_v2_62@0,15-ARR_v1_57@4,"These findings constitute first evidence that using a cross-cultural value taxonomy could result in robust methods for identifying the values behind arguments, even though more data and research seem necessary to get there.",These findings constitute first evidence that using a cross-cultural value taxonomy could result in robust methods for identifying the values behind arguments.,"Modify,Claim",Claim
1046,15-ARR,15-ARR_v2_6@2,15-ARR_v1_7@2,"Within computational linguistics, human values thus provide the context to categorize, compare, and evaluate argumentative statements, creating several possibilities: to inform social science research on values through large-scale datasets; to assess argumentation with respect to scope and strength; to generate or select arguments based on the value system of a target audience; and to identify opposing and shared values on both sides of a controversial topic.","Within computational linguistics, human values thus provide the context to categorize, compare, and evaluate argumentative statements, creating several possibilities: to inform social science research on values through large-scale datasets; to assess argumentations with respect to scope and strength; and to generate or select arguments based on the value system of a target audience.","Modify,Claim",Claim
1047,15-ARR,15-ARR_v2_8@0,15-ARR_v1_9@0,"As a first endeavor on the automatic identification of values in written arguments, this paper makes three contributions: (1) a consolidated multilevel taxonomy of 54 human values taken from four authoritative cross-cultural social science studies (Section 3); (2) a dataset of 5270 arguments from the US (most arguments), Africa, China, and India, each of which manually annotated for all values by three annotators, corresponding to about 850k human judgments (Section 4); and (3) first classification results per taxonomy level, establishing a baseline and revealing promising results both within and across cultures (Section 5).","As a first endeavour on the automatic identification of values in written arguments, this paper makes three contributions: (1) a consolidated multilevel taxonomy of 54 human values taken from four authoritative cross-cultural social science studies (Section 3); (2) a dataset of 5270 arguments from the US (most arguments), Africa, China, and India, each of which manually annotated for all values by three annotators, corresponding to about 850k human judgments (Section 4); and (3) first classification results per taxonomy level, establishing a baseline and revealing promising results both within and across cultures (Section 5).","Modify,Grammar",Grammar
1048,15-ARR,15-ARR_v2_12@2,15-ARR_v1_13@2,"The paper at hand follows these definitions and targets the personal values behind arguments, that is, the values that the arguments, mostly implicitly, resort to.","The paper at hand targets the personal values behind arguments, meaning the values in the former sense that the arguments, mostly implicitly, resort to.","Modify,Fact/Evidence",Fact/Evidence
1049,15-ARR,15-ARR_v2_15@0,15-ARR_v1_14@0,"Other schemes, however, pertain to specific purposes, making them less suited for our study.",Several of the value schemes proposed in the literature pertain to specific purposes.,"Modify,Claim",Claim
1050,15-ARR,15-ARR_v2_13@0,15-ARR_v1_15@0,Several proposed value schemes are domainindependent and hence suited to analyze generic argumentation.,Other proposed value schemes are more generic.,"Modify,Claim",Claim
1051,15-ARR,15-ARR_v2_14@0,15-ARR_v1_15@2,"Specifically for cross-cultural analysis, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble.","Specifically for cross-cultural analyses, Schwartz et al. ( 2012) derived 48 value questions from the universal needs of individuals and societies, including obeying all the laws and to be humble.","Modify,Grammar",Grammar
1052,15-ARR,15-ARR_v2_14@4,15-ARR_v1_15@6,"However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further in this paper.","However, as the meta-inventory is strictly more coarse-grained than Schwartz et al.'s theory we do not investigate it further for this paper.","Modify,Grammar",Grammar
1053,15-ARR,15-ARR_v2_17@3,15-ARR_v1_17@3,This paper presents a first step towards the large-scale automatic application of these works as it takes values to argument mining.,This paper present a first step towards the large-scale automatic application of these works as it takes values to argument mining.,"Modify,Grammar",Grammar
1054,15-ARR,15-ARR_v2_18@0,15-ARR_v1_18@0,"Values overlap with idea of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993).","Partly, values overlap with ideas of framing in communication, that is, the selection and emphasis of specific aspects of (perceived) reality to promote a particular problem definition, causal interpretation, ethical evaluation, and/or recommendation (Entman, 1993).","Modify,Fact/Evidence",Fact/Evidence
1055,15-ARR,15-ARR_v2_18@1,15-ARR_v1_18@1,"In frames, values can define the costs and benefits of options (Entman, 1993), while common value systems are used for evaluation.","In frames, values can define the costs and benefits of options (Entman, 1993) whereas common value systems are used for evaluation.","Modify,Clarity",Clarity
1056,15-ARR,15-ARR_v2_18@2,15-ARR_v1_18@2,"Framing has often been studied computationally for news (Naderi and Hirst, 2015;Chen et al., 2021), but also for political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019).","Frames have been studied computationally for news (Naderi and Hirst, 2015), political speech (De Vreese, 2005), and argumentation (Ajjour et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
1057,15-ARR,15-ARR_v2_18@3,15-ARR_v1_18@3,"In the latter, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification.","In argumentation, some values are so prevalent that they constitute frames of their own, indicating a potential use of values in frame identification.","Modify,Clarity",Clarity
1086,151-ARR,151-ARR_v2_20@0,151-ARR_v1_20@0,"By receiving a data example composing of hypothesis, source, and reference segment, UniTE first modifies it into concatenated sequence following the given setting as REF , SRC , or SRC+REF :","By receiving a data example composing of hypothesis, source, and reference segment, UniTE first modifies it into concatenated sequence following the given setting as REF , SRC , or SRC+REF .","Modify,Grammar",Grammar
1087,151-ARR,151-ARR_v2_26@1,151-ARR_v1_26@1,"According to Ranasinghe et al. (2020b), we use the first output representation as the input of feedforward layer.","According to common practice (Ranasinghe et al., 2020b), we use the first output representation as the input of feedforward layer (see Appendix B).","Modify,Fact/Evidence",Fact/Evidence
1088,151-ARR,151-ARR_v2_27@0,151-ARR_v1_27@0,"Compared to existing methods (Zhang et al., 2020;Rei et al., 2020) which take sentence-level representations for evaluation, the advantages of our architecture design are as follows.","Compared to existing methods (Rei et al., 2020;Zhang et al., 2020) which only take the outputted representations of the topmost layer for evaluation, the advantages of our architecture design are as follows.","Modify,Fact/Evidence",Fact/Evidence
1089,151-ARR,151-ARR_v2_27@1,151-ARR_v1_27@1,"First, our UniTE model can benefit from layer-coordinated semantical interactions inside every one of PLM layers, which is proven effective on capturing diverse linguistic features (He et al., 2018;Lin et al., 2019;Jawahar et al., 2019;Tenney et al., 2019;Rogers et al., 2020).","First, our UniTE model can benefit from layer-coordinated semantical interactions inside every one layer of PLM, which is proven effective on capturing diverse linguistic features (He et al., 2018;Lin et al., 2019;Jawahar et al., 2019;Tenney et al., 2019;Rogers et al., 2020).","Modify,Clarity",Clarity
1090,151-ARR,151-ARR_v2_2@8,151-ARR_v1_2@7,Both source code and associated models are available at https://github.com/NLP2CT/UniTE.,Both source code and associated models will be released upon the acceptance of this paper.,"Modify,Fact/Evidence",Fact/Evidence
1091,151-ARR,151-ARR_v2_32@0,151-ARR_v1_31@0,"For training, we encourage the model to reduce the mean squared error with respect to given score q:","During training, we encourage the model to reduce the mean squared error between model prediction and given score q:","Modify,Clarity",Clarity
1092,151-ARR,151-ARR_v2_35@0,151-ARR_v1_33@0,"However, for the pretraining of most PLMs (e,g., XLM-R, Conneau et al., 2020), the input patterns are designed to receive two segments at most.","However, the input patterns for most multilingual PLMs (e,g., XLM-R, Conneau et al., 2020) are designed to receive two segments at most during pretraining.","Modify,Clarity",Clarity
1093,151-ARR,151-ARR_v2_35@2,151-ARR_v1_33@2,"Moreover, previous study (Takahashi et al., 2020) shows that directly training over SRC+REF by following such design leads to worse performance than REF scenario.","Moreover, previous studies (Takahashi et al., 2020) show that directly training over SRC+REF by following such design leads to slightly worse performance than REF scenarios.","Modify,Grammar",Grammar
1094,151-ARR,151-ARR_v2_4@0,151-ARR_v1_4@0,"Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020;Mathur et al., 2020a;Zhao et al., 2020;Kocmi et al., 2021).","Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020;Mathur et al., 2020;Zhao et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
1097,151-ARR,151-ARR_v2_37@0,151-ARR_v1_35@0,"To fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks.","In order to fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks.","Modify,Clarity",Clarity
1098,151-ARR,151-ARR_v2_37@3,151-ARR_v1_36@0,Considering the conventional attention module:,The conventional attention module can be expressed as:,"Modify,Clarity",Clarity
1099,151-ARR,151-ARR_v2_39@1,151-ARR_v1_39@1,"2 As to monotonic regional attention (MRA), we simply add a mask M to the softmax logits to control attention flows:","2 As to MRA, we simply add a mask M to the softmax logits to control attention flows:","Modify,Clarity",Clarity
1100,151-ARR,151-ARR_v2_43@0,151-ARR_v1_44@0,"To give more fine-grained designs, we propose two approaches for UniTE-MRA, which apply the MRA mechanism into UniTE model (Figure 2):","To give more fine-grained designs, we propose two approaches for UniTE-MRA, which apply the MRA mechanism into UniTE model:","Modify,Fact/Evidence",Fact/Evidence
1101,151-ARR,151-ARR_v2_4@1,151-ARR_v1_4@1,"Based on the input contexts, translation evaluation can be mainly categorized into three classes: 1) reference-only evaluation ( REF ) approaches like BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi-2 (Lo, 2019) and TransQuest (Ranasinghe et al., 2020b), which are also referred as quality estimation (QE).","According to the input contexts, translation evaluation can be mainly categorized into three classes: 1) reference-only evaluation ( REF ) approaches like BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi-2 (Lo, 2019) and TransQuest (Ranasinghe et al., 2020b), which are also referred as quality estimation (QE).","Modify,Clarity",Clarity
1102,151-ARR,151-ARR_v2_45@0,151-ARR_v1_46@0,"Note that, although the processing in source and reference may be affected because their positions are not indexed from the start, related studies on positional embeddings reveal that, PLM can well capture relative positional information (Wang and Chen, 2020), which dispels this concern.","Note here that, although the processing in source and reference segments may be affected because their position embeddings are not indexed from the start, related studies on positional embeddings reveal that PLM models can well capture relative positional information (Wang and Chen, 2020), which dispels this concern.","Modify,Clarity",Clarity
1103,151-ARR,151-ARR_v2_47@0,151-ARR_v1_48@0,"To further bridge the modeling gap between PLM and the joint training of UniTE mentioned in §3.1, we propose a unified pretraining strategy including the following main stages: 1) collecting and downgrading synthetic data; 2) labeling examples with a novel ranking-based strategy; 3) multi-task learning for unified pretraining and finetuning.","To further bridge the modeling gap between PLM and the joint training of UniTE as we mentioned in §3.1, we propose a unified pretraining strategy including the following main stages: 1) collecting and downgrading synthetic data; 2) labeling examples with a novel ranking-based strategy; 3) multi-task learning for unified pretraining and finetuning.","Modify,Clarity",Clarity
1104,151-ARR,151-ARR_v2_48@1,151-ARR_v1_49@1,"To further improve the diversity of synthetic data quality, we follow existing experiences (Sellam et al., 2020a;Wan et al., 2021) to apply the word and span dropping strategy to downgrade a portion of hypotheses.","In order to further improve the diversity of synthetic data quality, we follow Sellam et al. (2020a) to apply the word and span dropping strategy to downgrade a portion of hypotheses.","Modify,Fact/Evidence",Fact/Evidence
1105,151-ARR,151-ARR_v2_49@0,151-ARR_v1_50@0,"Specifically, we first use available approaches to derive the predicted score qi for each item, yielding labeled synthetic quadruple examples formed as D = { h i , s i , r i , qi } N i=1 .","To be concrete, for each data item, we first use existing evaluation approach to give prediction qi for each item, yielding labeled synthetic quadruple examples formed as D = { h i , s i , r i , qi } N i=1 .","Modify,Clarity",Clarity
1106,151-ARR,151-ARR_v2_49@1,151-ARR_v1_50@1,"Then, we tag each example with its rank index qi referring to qi :","After that, we descendingly tag each example with their rank index qi referring to qi :","Modify,Fact/Evidence",Fact/Evidence
1107,151-ARR,151-ARR_v2_53@4,151-ARR_v1_55@4,"For example, different methods may give scores with different distributions.","For example, different existing methods may output scores with different distributions.","Modify,Clarity",Clarity
1108,151-ARR,151-ARR_v2_4@2,151-ARR_v1_4@2,"These methods estimate the quality of the hypothesis based on the source sentence without using references; 3) source-reference-combined evaluation ( SRC+REF ) works like COMET (Rei et al., 2020), where the evaluation exploits information from both source and reference.","These methods estimate the quality of the hypothesis based on the source sentence without using references; 3) source-reference-combined evaluation ( SRC+REF ) works like COMET (Rei et al., 2020), where the model exploits information from both source and reference for the evaluation.","Modify,Clarity",Clarity
1109,151-ARR,151-ARR_v2_54@3,151-ARR_v1_60@1,"For the former, we follow the common practice in COMET 3 (Rei et al., 2020) to collect and preprocess the dataset.","We follow the common practice in COMET 3 (Rei et al., 2020) to collect and preprocess the dataset.","Modify,Clarity",Clarity
1110,151-ARR,151-ARR_v2_56@0,151-ARR_v1_60@4,"For SRC scenario, we further conduct results on WMT 2020 QE task (Specia et al., 2020) referring to Ranasinghe et al. (2020a) for data collection and preprocessing.","For SRC scenario, we further evaluate our method on WMT 2020 QE tasks (Specia et al., 2020), where we follow Ranasinghe et al. (2020a) for data collection and preprocessing.","Modify,Clarity",Clarity
1113,151-ARR,151-ARR_v2_58@2,151-ARR_v1_61@2,"Specifically, we follow Sellam et al. (2020a) to use TRANSFORMER-base (Vaswani et al., 2017) MT models to generate translation candidates, and use the checkpoints trained via UniTE-MRA approach for synthetic data labeling.","Specifically, we follow Sellam et al. (2020a) to use Transformer-base (Vaswani et al., 2017) model to generate translation candidates, and use the checkpoints trained via UniTE-MRA approach for synthetic data labeling.","Modify,Clarity",Clarity
1114,151-ARR,151-ARR_v2_58@3,151-ARR_v1_61@3,"We pretrain two kinds of models, one is pretrained on English-targeted language directions, and the other is a multilingual version trained using bidirectional data.","We pretrain two kinds of models, one is the English version which is pretrained on English-targeted language directions, the other is a multilingual version trained using bidirectional data.","Modify,Clarity",Clarity
1115,151-ARR,151-ARR_v2_59@1,151-ARR_v1_62@1,"For SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al., 2020), PRISM-src (Thompson and Post, 2020) and multilingual-to-multilingual MTran-sQuest (Ranasinghe et al., 2020b).","As to SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al., 2020), PRISMsrc (Thompson and Post, 2020) and multilingual-tomultilingual version of MTransQuest (Ranasinghe et al., 2020b).","Modify,Clarity",Clarity
1116,151-ARR,151-ARR_v2_59@2,151-ARR_v1_62@2,"For SRC+REF , we use XLM-R+Concat (Takahashi et al., 2020) and COMET (Rei et al., 2020) as strong baselines.","For SRC+REF scenario, we use XLM-R+Concat (Takahashi et al., 2020) and COMET (Rei et al., 2020) as strong baselines.","Modify,Clarity",Clarity
1117,151-ARR,151-ARR_v2_62@1,151-ARR_v1_64@1,"Among all involved baselines, for REF methods, BARTScore (Yuan et al., 2021) performs better than other statistical and model-based metrics.","For REF methods, BARTScore (Yuan et al., 2021) performs better than other statistical and model-based metrics.","Modify,Clarity",Clarity
1118,151-ARR,151-ARR_v2_63@2,151-ARR_v1_64@2,"Further, COMET (Rei et al., 2020) performs better than XLM-R+Concat (Takahashi et al., 2020) on SRC+REF scenario.","MTransQuest (Ranasinghe et al., 2020b) gives dominant performance on SRC scenario, and COMET (Rei et al., 2020) performs better than XLM-R+Concat on SRC+REF scenario.","Modify,Fact/Evidence",Fact/Evidence
1119,151-ARR,151-ARR_v2_63@3,151-ARR_v1_65@0,"As for our methods, we can see that, UniTE-MRA achieves better results on all tasks, demonstrating the effectiveness of monotonic attention flows for cross-lingual interactions.","As for our methods, UniTE-MRA approach achieves better results on all tasks, demonstrating the effectiveness of monotonic attention flow for cross-lingual interactions.","Modify,Clarity",Clarity
1120,151-ARR,151-ARR_v2_63@4,151-ARR_v1_65@1,"Moreover, the proposed model UniTE-UP, which unifies REF , SRC , and SRC+REF learning on both pretraining and finetuning, yields better results on all evaluation settings.","Moreover, our proposed model UniTE-UP, which unifies both pretraining and finetuning, can yield better results following all evaluation settings.","Modify,Fact/Evidence",Fact/Evidence
1121,151-ARR,151-ARR_v2_63@5,151-ARR_v1_65@2,"Most importantly, UniTE-UP is a single model which surpasses all the different state-of-the-art models on three tasks, showing its dominance on both convenience and effectiveness.","Most importantly, UniTE-UP is a single model which surpasses all the different state-of-the-art models on three tasks.","Modify,Claim",Claim
1122,151-ARR,151-ARR_v2_64@1,151-ARR_v1_68@2,"Besides, the UniTE-UP also gives dominant results, revealing an improvement of 0.6, 0.3 and 0.9 averaged Kendall's τ correlation scores, respectively.","Besides, it is encouraging to see that the UniTE-UP can also give dominant results, revealing an improvement of 0.6, 0.3 and 0.9 averaged Kendall's τ correlation scores, respectively.","Modify,Claim",Claim
1123,151-ARR,151-ARR_v2_64@2,151-ARR_v1_68@3,"However, we find that UniTE-MUP outperforms strong baselines but slightly worse than UniTE-UP on English-targeted translation directions (see Table 3).",Our further comparison indicates that UniTE-MUP also outperforms previous strong baselines but slightly worse than UniTE-UP on English-targeted translation directions.,"Modify,Clarity",Clarity
1124,151-ARR,151-ARR_v2_67@1,151-ARR_v1_70@1,All experiments are conducted by following English-targeted setting.,All experiments follow English-targeted setting on SRC+REF task.,"Modify,Fact/Evidence",Fact/Evidence
1125,151-ARR,151-ARR_v2_69@0,151-ARR_v1_72@0,"To investigate the effectiveness of MRA, we further collect experiments in Table 5.","To investigate the effectiveness of MRA, we further collect experiments for comparison.","Modify,Fact/Evidence",Fact/Evidence
1126,151-ARR,151-ARR_v2_69@1,151-ARR_v1_72@1,"As seen, MRA can give performance improvements than full attention, and preventing the interactions between hypothesis and source segment can improve the performance most.","As seen in Table 3, MRA can give performance improvements than full attention, and preventing the interactions between hypothesis and source segment can improve the performance most.","Modify,Fact/Evidence",Fact/Evidence
1127,151-ARR,151-ARR_v2_69@6,151-ARR_v1_73@1,Wang and Chen (2020) found that the positional embeddings in PLM are engaged with strong adjacent information.,As Wang and Chen (2020) found that the positional embeddings in PLM are engaged with strong adjacent information.,"Modify,Clarity",Clarity
1131,151-ARR,151-ARR_v2_5@2,151-ARR_v1_5@2,"We believe that it is valuable, as well as feasible, to unify the capabilities of all MT evaluation tasks ( REF , SRC and SRC+REF ) into one model.","Therefore, we believe that it is valuable to unify the capabilities of all MT evaluation tasks ( REF , SRC and SRC+REF ) into one model.","Modify,Clarity",Clarity
1132,151-ARR,151-ARR_v2_5@4,151-ARR_v1_5@4,"To achieve this, two important challenges need to be addressed: 1) How to design a model framework that can unify all translation evaluation tasks? 2) How to make the powerful PLMs better adapt to the unified evaluation model?","To achieve this idea, the following two important challenges need to be addressed: 1) how to design a model framework that can unify all translation evaluation tasks? 2) Considering the powerful capabilities of the PLM, how to make the PLM better adapt to the unified evaluation model?","Modify,Clarity",Clarity
1133,151-ARR,151-ARR_v2_2@2,151-ARR_v1_2@1,"Recent methods, despite their promising results, are specifically designed and optimized on one of them.","Recent methods, despite their promising results, are specifically designed and optimized on one of these three tasks.","Modify,Clarity",Clarity
1134,151-ARR,151-ARR_v2_6@1,151-ARR_v1_6@1,"To solve the first challenge as mentioned above, based on the multilingual PLM, we utilize layerwise coordination which concatenates all input segments into one sequence as the unified input form.","To solve the first challenge as mentioned above, based on the multilingual PLM (Conneau et al., 2020), we utilize layerwise coordination which concatenates all input segments into one sequence as the unified input form.","Modify,Fact/Evidence",Fact/Evidence
1135,151-ARR,151-ARR_v2_6@3,151-ARR_v1_6@3,"For the second challenge, a multi-task learning-based unified pretraining is proposed.","For the second challenge, a multi-task learning-xbased unified pretraining is proposed.","Modify,Grammar",Grammar
1136,151-ARR,151-ARR_v2_6@6,151-ARR_v1_6@6,"Finally, the multilingual PLM is continuously pretrained on synthetic dataset with multi-task learning manner.","Finally, The multilingual PLM is continuously pretrained on synthetic dataset with multi-task learning.","Modify,Clarity",Clarity
1137,151-ARR,151-ARR_v2_6@7,151-ARR_v1_6@7,"Besides, our proposed models, named UniTE-MRA and UniTE-UP respectively, can benefit from finetuning with human-annotated data over three tasks at once, not requiring extra task-specific training.","Besides, our proposed models, named as UniTE-MRA and UniTE-UP respectively, can benefit from finetuning with human-annotated data over three tasks at once, not requiring extra taskspecific training.","Modify,Grammar",Grammar
1138,151-ARR,151-ARR_v2_2@3,151-ARR_v1_2@2,"This limits the convenience of these methods, and overlooks the commonalities among tasks.",This limits the convenience of these methods and overlooks commonalities among tasks.,"Modify,Grammar",Grammar
1139,151-ARR,151-ARR_v2_7@1,151-ARR_v1_7@1,"Compared to various strong baseline systems on each task, UniTE, which unifies REF , SRC and SRC+REF tasks into one single model, achieves consistently absolute improvements of Kendall's τ correlations at 1.1, 2.3 and 1.1 scores on English-targeted translation directions of WMT 2019 Metric Shared task (Fonseca et al., 2019), respectively.","Compared to different strong baseline systems on each task, UniTE, which unifies REF , SRC and SRC+REF tasks into one single model, achieves consistently absolute improvements of Kendall's τ correlations at 1.1, 2.3 and 1.1 scores on English-targeted translation directions of WMT 2019 Metric Shared task (Fonseca et al., 2019), respectively.","Modify,Clarity",Clarity
1140,151-ARR,151-ARR_v2_11@4,151-ARR_v1_11@4,"However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020;Rei et al., 2020;Mathur et al., 2020a).","However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020;Rei et al., 2020;Mathur et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
1141,151-ARR,151-ARR_v2_12@0,151-ARR_v1_12@0,"Consequently, with the rapid development of PLMs, researchers have been paying their attention to model-based approaches.","Consequently, with the rapid development of PLMs, researchers have been paying their attention in model-based approaches.","Modify,Grammar",Grammar
1142,151-ARR,151-ARR_v2_12@1,151-ARR_v1_12@1,"The basic idea of these studies is to collect sentence representations for similarity calculation (BERTScore, Zhang et al., 2020) or evaluating probabilistic confidence (PRISM-ref, Thompson and Post, 2020;BARTScore, Yuan et al., 2021).","The basic idea of these studies is to collect sentences representations for similarity calculation (BERTScore, Zhang et al., 2020) or evaluating probabilistic confidence (PRISM-ref, Thompson and Post, 2020;BARTScore, Yuan et al., 2021).","Modify,Grammar",Grammar
1143,151-ARR,151-ARR_v2_2@5,151-ARR_v1_2@4,"Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task learning.","Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task training.","Modify,Clarity",Clarity
1144,151-ARR,151-ARR_v2_2@6,151-ARR_v1_2@5,We testify our framework on WMT 2019 Metrics and WMT 2020 Quality Estimation benchmarks.,We empirically testify our framework on WMT 2019 Metrics and WMT 20 Quality Estimation benchmarks.,"Modify,Fact/Evidence",Fact/Evidence
1152,152-ARR,152-ARR_v2_21@4,152-ARR_v1_20@4,"By direct analogy, sentence importance score measures the utility of a sentence with respect to the entities it contains.","By direct analogy, a sentence importance score measures the utility of a sentence respect to the entity tokens it contains.","Modify,Grammar",Grammar
1153,152-ARR,152-ARR_v2_4@0,152-ARR_v1_4@0,"In natural language processing, named entity recognition (NER) is an important task both on its own and for numerous downstream tasks such as entity linking and question answering.","In natural language processing, named entity recognition (NER) is an important task both on its own and supports numerous downstream tasks such as entity linking and question answering.","Modify,Clarity",Clarity
1154,152-ARR,152-ARR_v2_33@1,152-ARR_v1_32@1,By introducing the rareness of an entity type we propose another function called the smoothed resampling incorporating count and rareness (sCR):,By introducing rareness of an entity type we propose another function called smoothed resampling incorporating count and rareness (sCR):,"Modify,Grammar",Grammar
1155,152-ARR,152-ARR_v2_38@0,152-ARR_v1_38@0,We use √ l s instead of l s to slow down the decrease of f sCRD s when a sentence is too long.,We use √ l s instead of l s because to slow down the decrease of f sCRD s when a sentence is too long.,"Modify,Grammar",Grammar
1156,152-ARR,152-ARR_v2_40@0,152-ARR_v1_40@0,"Here, c(t, s) applies a sublinear increasing function on c(t, s) to implement the diminishing marginal utility when a sentence contains many tokens with the same type.","Here, c(t, s) applies a sublinearly increasing function on c(t, s) to implement the diminishing marginal utility when a sentence contains many tokens with the same type.","Modify,Grammar",Grammar
1157,152-ARR,152-ARR_v2_71@3,152-ARR_v1_69@3,"Similar observation was made by previous work (Devlin et al., 2018).","Similar observations was made by previous work (Devlin et al., 2018).","Modify,Grammar",Grammar
1158,152-ARR,152-ARR_v2_72@1,152-ARR_v1_70@1,These benefits become less salient on large corpus (CoNLL).,These benefit becomes less salient on large corpus (CoNLL).,"Modify,Grammar",Grammar
1159,152-ARR,152-ARR_v2_79@0,152-ARR_v1_77@0,Conclusion and Future Work,Conclusion,"Modify,Other",Other
1160,152-ARR,152-ARR_v2_83@0,152-ARR_v1_80@0,Various other avenues exist for future work.,There are multiple avenues for future work.,"Modify,Clarity",Clarity
1161,152-ARR,152-ARR_v2_83@1,152-ARR_v1_80@1,"First, further theoretical and empirical research can explore more effective resampling functions that deliver consistently better performance across corpora and base NER models.","First, further theoretical and empirical research can explore more effective resampling functions that deliver consistently better performance across corpora and base models.","Modify,Clarity",Clarity
1162,152-ARR,152-ARR_v2_83@2,152-ARR_v1_80@2,"Second, more corpora and models can be examined under these resampling strategies to evaluate their generalizability.","Second, more corpora and models can be examined under these resampling strategies.","Modify,Claim",Claim
1163,152-ARR,152-ARR_v2_83@4,152-ARR_v1_80@4,Future research may seek for corpora-level statistics that can assist practitioners in selecting the appropriate resampling methods.,Future research may seek for corpora-level statistics that can assist practitioners in the process of selecting the appropriate resampling method(s).,"Modify,Clarity",Clarity
1164,152-ARR,152-ARR_v2_6@4,152-ARR_v1_5@4,"Data augmentation was shown to be effective by enriching entity-bearing sentences through methods like segment shuffling and mention replacement (Dai and Adel, 2020;Issifu and Ganiz, 2021;Wang and Henao, 2021).","Data augmentation was shown to be effective by enriching entity-bearing sentences through methods like segment shuffling and mention replacement (Dai and Adel, 2020).","Modify,Fact/Evidence",Fact/Evidence
1165,152-ARR,152-ARR_v2_13@0,152-ARR_v1_11@0,"Researchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Garcia, 2009).","Researchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Ma, 2013).","Modify,Fact/Evidence",Fact/Evidence
1169,155-ARR,155-ARR_v2_48@1,155-ARR_v1_50@1,Take PGD-K as an instance.,Take PGD-K for instance.,"Modify,Grammar",Grammar
1170,155-ARR,155-ARR_v2_4@0,155-ARR_v1_4@0,"Deep neural networks (DNNs) have achieved great success on many natural language processing (NLP) tasks (Kim, 2014;Vaswani et al., 2017;Devlin et al., 2019).","Deep neural networks (DNNs) outperform humans on many natural language processing (NLP) tasks (Kim, 2014;Vaswani et al., 2017;Devlin et al., 2019).","Modify,Fact/Evidence",Fact/Evidence
1171,155-ARR,155-ARR_v2_4@1,155-ARR_v1_4@1,"However, recent studies (Szegedy et al., 2013;Goodfellow et al., 2015) have shown that DNNs are vulnerable to crafted adversarial examples .","However, recent studies have shown that DNNs are vulnerable to crafted adversarial examples (Szegedy et al., 2013;Goodfellow et al., 2014).","Modify,Fact/Evidence",Fact/Evidence
1172,155-ARR,155-ARR_v2_72@0,155-ARR_v1_74@0,"Regarding the training settings and hyperparameters, the optimizer is AdamW (Loshchilov and Hutter, 2019); the learning rate is 2e −5 ; the number of epochs is 10; the batch size is 64 for SST-2 and 24 for IMDb; the maximum sentence length kept for all the models is 40 for SST-2 and 200 for IMDb.","Regarding the training settings and hyperparameters, the optimizer is AdamW (Loshchilov and Hutter, 2019); the learning rate is 2e −5 ; the number of epochs is 10; the batch size is 64 for SST-2 and 24 for IMDb.","Modify,Fact/Evidence",Fact/Evidence
1173,155-ARR,155-ARR_v2_2@4,155-ARR_v1_2@4,"Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate friendly adversarial data.","Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate ""friendly"" adversarial data.","Modify,Grammar",Grammar
1174,155-ARR,155-ARR_v2_21@3,155-ARR_v1_22@3,Goodfellow et al. (2015) proposed fast gradient sign method (FGSM) to obtain δ by one step:,Goodfellow et al. (2014) proposed fast gradient sign method (FGSM) to obtain δ by one step:,"Modify,Fact/Evidence",Fact/Evidence
1175,155-ARR,155-ARR_v2_2@5,155-ARR_v1_2@5,"On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training on friendly adversarial data so that we can save a large number of search steps.","On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training (e.g., FGM) on friendly adversarial data so that we can save a large number of search steps.","Modify,Clarity",Clarity
1176,155-ARR,155-ARR_v2_28@1,155-ARR_v1_30@1,Miyato et al. (2017) find that adversarial and virtual adversarial training have good regularization performance.,Miyato et al. (2016) find that adversarial and virtual adversarial training have good regularization performance.,"Modify,Fact/Evidence",Fact/Evidence
1177,155-ARR,155-ARR_v2_30@0,155-ARR_v1_32@0,"Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2018;Li et al., 2019;Ren et al., 2019;Jin et al., 2019;Zang et al., 2020;Li et al., 2020;Garg and Ramakrishnan, 2020;Si et al., 2021).","Besides, adversarial data augmentation is another effective approach to improve robustness (Ebrahimi et al., 2017;Li et al., 2018;Ren et al., 2019;Jin et al., 2019;Zang et al., 2020;Li et al., 2020;Garg and Ramakrishnan, 2020;Si et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
1178,155-ARR,155-ARR_v2_2@6,155-ARR_v1_2@6,Comprehensive experiments across two widely used datasets and three pretrained language models demonstrate that GAT can obtain stronger robustness via fewer steps.,Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via less steps.,"Modify,Grammar",Grammar
1200,156-ARR,156-ARR_v2_86@0,156-ARR_v1_83@0,"As shown in Table 2 and Table 3, our model Di-alogVED is very competitive compared to PLATO and other models.","As shown in Table 2 and Table 6 (in Appendix A), our model DialogVED is very competitive compared to PLATO and other models.","Modify,Fact/Evidence",Fact/Evidence
1201,156-ARR,156-ARR_v2_105@1,156-ARR_v1_103@1,"These models introduce discourse-level diversity and are able to generate diverse dialog responses (Serban et al., 2017;Zhao et al., 2017a.","These models introduce discourse-level diversity and are able to generate diverse dialog responses (Serban et al., 2017;Zhao et al., 2017aZhao et al., , 2018.","Modify,Fact/Evidence",Fact/Evidence
1202,156-ARR,156-ARR_v2_87@2,156-ARR_v1_111@2,"However, DialogVED equipped with beam search or greedy search, can still easily beat PLATO even though it has a post-generation ranking component.","However, DialogVED equipped with beam or search, can still easily beat PLATO even though it has a post-generation ranking component.","Modify,Clarity",Clarity
1203,156-ARR,156-ARR_v2_10@0,156-ARR_v1_10@1,"The main contributions of this paper can be summarized as follows: 1) We propose a pretrained dialog model, which incorporates continuous latent variables into the enhanced encoder-decoder pre-training framework; 2) We explore the impact of latent variable sizes, different decoding strategies, and position embeddings for turns and roles in our model; 3) Extensive experiments show that the proposed model achieves the new state-of-theart (SOTA) in multiple downstream tasks, and our model has better performance both on relevance and diversity than previous SOTA in response generation.","The main contributions of this paper can be summarized as follows: 1) We propose a pretrained dialog model, which incorporates continuous latent variables into the enhanced encoder-decoder pre-training framework; We explore the impact of latent variable sizes, different decoding strategies, and position embeddings for turns and roles in our model; Extensive experiments show that the proposed model achieves the new state-of-the-art (SOTA) in multiple downstream tasks, and our model has better performance both on relevance and diversity than previous SOTA in response generation.","Modify,Grammar",Grammar
1209,157-ARR,157-ARR_v2_29@0,157-ARR_v1_28@0,"We consulted a domain expert (financial domain; specifically for SEC 10-K filings) to create a matrix of personas, who read such documents, and what kind of information they are interested in.",We consulted a domain expert (financial domain; specifically for SEC 10-K filings) to get an understanding of the parties or personas who read such documents and what kind of information are they generally interested in.,"Modify,Clarity",Clarity
1210,157-ARR,157-ARR_v2_29@1,157-ARR_v1_28@1,Figure 2 lists out the various stakeholders of a general 10-K filing against the different sections of the document each stakeholder is interested in.,We constructed a matrix listing out the various stakeholders of a general 10-K filing against the different sections of the document each stakeholder is interested in.,"Modify,Fact/Evidence",Fact/Evidence
1211,157-ARR,157-ARR_v2_29@2,157-ARR_v1_28@2,"The stakeholders are grouped together to form the personas used in DYNAMICTOC, viz. employees, business partners, investors and lendors, financial bodies and advisory and regulatory firms.","The stakeholders were grouped together to form the personas used in DYNAMICTOC, viz. employees, business partners, investors and lendors, financial bodies and advisory regulatory firms.","Modify,Grammar",Grammar
1212,157-ARR,157-ARR_v2_29@3,157-ARR_v1_28@3,"Similarly, the columns (headings) are grouped together according to similarity to create a mapping of topics of interest for each persona.","Similarly, the columns (headings) were grouped together according to similarity to create a mapping of topics of interest for each persona.","Modify,Grammar",Grammar
1213,157-ARR,157-ARR_v2_30@1,157-ARR_v1_29@0,"For this, the aspects obtained from the unsupervised technique are compared against the simplified column values from the constructed matrix.",The aspects we got from our unsupervised technique were compared against the simplified column values from the constructed matrix.,"Modify,Clarity",Clarity
1214,157-ARR,157-ARR_v2_30@2,157-ARR_v1_29@1,The columns with the greatest similarity (above a threshold) are associated with each persona.,The columns with the greatest similarity (above a threshold) were associated with each persona.,"Modify,Grammar",Grammar
1215,157-ARR,157-ARR_v2_30@3,157-ARR_v1_29@2,"For getting the personas interested in each paragraph, the paragraphs are first tagged for aspect.","For getting the personas interested in each paragraph, the paragraphs were first tagged for aspect.","Modify,Grammar",Grammar
1216,157-ARR,157-ARR_v2_30@4,157-ARR_v1_29@3,"From the resultant vector (which represents the confidence score of the text for each aspect), the combined score for each persona is calculated using the scores of its constituent aspects.","From the resultant vector (which represents the confidence score of the text for each aspect), the combined score for each persona was calculated using the scores of its constituent aspects.","Modify,Grammar",Grammar
1217,157-ARR,157-ARR_v2_31@1,157-ARR_v1_31@1,"Note that for financial documents, we were able to gather domain knowledge and leverage it to obtain the persona space.","The thing to note here is, for financial documents, we were able to get some domain knowledge and leveraged it to obtain the persona space.","Modify,Clarity",Clarity
1218,157-ARR,157-ARR_v2_31@2,157-ARR_v1_31@2,But the proposed technique is generalizable to other domains as well.,But the technique we are proposing is generalizable to other domains as well.,"Modify,Clarity",Clarity
1219,157-ARR,157-ARR_v2_34@5,157-ARR_v1_34@5,We use ELI5 dataset for training the model.,"We use ELI5 (Fan et al., 2019) dataset for training the model.","Modify,Fact/Evidence",Fact/Evidence
1220,157-ARR,157-ARR_v2_55@1,157-ARR_v1_60@1,"Financial documents are high value documents for businesses, and are often long and complex.","Financial documents and contracts are high value documents for business entities, and are often long and complex.","Modify,Claim",Claim
1221,157-ARR,157-ARR_v2_55@2,157-ARR_v1_60@2,The default ToC-based reading experience is quite limited and document consumption can be enhanced using intelligent technologies.,The default ToC-based reading experience is quite limited and there are immense opportunities to enhance the document consumption using intelligent technologies.,"Modify,Clarity",Clarity
1222,157-ARR,157-ARR_v2_55@3,157-ARR_v1_60@3,DY-NAMICTOC is one of the first works to pursue this exciting research direction.,"We believe that DYNAMICTOC is one of the first works to pursue this exciting research direction, and would enable further exploration in the area.","Modify,Claim",Claim
1223,157-ARR,157-ARR_v2_55@4,157-ARR_v1_61@0,DYNAMICTOC would benefit from in-domain learning of aspect keywords and questions.,"For the future direction, DYNAMICTOC would benefit from a better supervised in-domain learning of aspect keywords and questions.","Modify,Clarity",Clarity
1224,157-ARR,157-ARR_v2_55@5,157-ARR_v1_61@1,Evaluation of paragraph segmentation and mapping of personas to the aspects are future directions.,We would also like to work on evaluation of the paragraph segmentation and mapping of personas to the aspects in future.,"Modify,Clarity",Clarity
1225,157-ARR,157-ARR_v2_55@6,157-ARR_v1_61@2,A better understanding of personas would generalize the work to different domains.,A better understanding of personas or entities interested in consuming the document would help to generalize the work to different domains.,"Modify,Claim",Claim
1226,157-ARR,157-ARR_v2_7@2,157-ARR_v1_7@2,"Keyword detection (Liu et al., 2009;Tixier et al., 2016) & topic modeling (Blei et al., 2001) works aim is to describe the document by a few important words or topics for concise representation.","Keyword detection (Liu et al., 2009;Tixier et al., 2016) & topic modeling (Blei et al., 2003) works aim is to describe the document by a few important words or topics for concise representation.","Modify,Fact/Evidence",Fact/Evidence
1227,157-ARR,157-ARR_v2_7@4,157-ARR_v1_7@4,"Another task is compact and informative headline generation from a document (Dorr et al., 2003;Lopyrev, 2015).","Another task is compact and informative headline generation from a document (David and Zajic, 2003;Lopyrev, 2015).","Modify,Fact/Evidence",Fact/Evidence
1228,157-ARR,157-ARR_v2_8@4,157-ARR_v1_8@4,"Early unsupervised systems are dominated by Latent Dirichlet Allocation (LDA)-based topic models (García-Pablos et al., 2018;Shi et al., 2018;Álvarez-López et al., 2016).","Early unsupervised systems are dominated by Latent Dirichlet Allocation (LDA)-based topic models (García-Pablos et al., 2018;Shi et al., 2018;Alvarez-López et al., 2016).","Modify,Grammar",Grammar
2742,25-ARR,25-ARR_v2_35@2,25-ARR_v1_35@2,"Table 2 shows subsequent proportions (%) of workers, and the detailed instructions are introduced in Figure 5 in Appendix A.4.",Table 2 shows subsequent proportions (%) of workers.,"Modify,Fact/Evidence",Fact/Evidence
2743,25-ARR,25-ARR_v2_61@0,25-ARR_v1_47@0,"We compute the correlation between commonly applied automatic metrics and our human evaluation methods, including word-overlap-based metrics and reference-free metrics, as shown in Tables 5 and 6 respectively.","In this section, we compute the correlation between commonly applied automatic metrics and our human evaluation methods, including word-overlapbased metrics and reference-free metrics, as shown in Tables 6 and 5 respectively.","Modify,Clarity",Clarity
2744,25-ARR,25-ARR_v2_8@0,25-ARR_v1_8@0,"In terms of the live evaluation, competitions such as ConvAI2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.","In terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.","Modify,Grammar",Grammar
2745,25-ARR,25-ARR_v2_17@0,25-ARR_v1_17@0,"A continuous (0-100) rating scale is employed with three main motivation points (Graham et al., 2013;Novikova et al., 2018;Li et al., 2019;Santhanam and Shaikh, 2019;Barrault et al., 2020;Howcroft et al., 2020).","A continuous (0-100) rating scale is employed with three main motivation points (Graham et al., 2013;Mille et al., 2020;Barrault et al., 2020).","Modify,Fact/Evidence",Fact/Evidence
3463,334-ARR,334-ARR_v2_13@0,334-ARR_v1_11@0,Hindi Legal Documents Corpus,Hindi Legal Document Corpus,"Modify,Grammar",Grammar
3464,334-ARR,334-ARR_v2_14@0,334-ARR_v1_12@0,"Hindi Legal Documents Corpus (HLDC) is a corpus of 912,568 Indian legal case documents in the Hindi language.",Hindi Legal Document Corpus (HLDC) is a corpus of about 900K Indian legal case documents in the Hindi language.,"Modify,Fact/Evidence",Fact/Evidence
3465,334-ARR,334-ARR_v2_14@1,334-ARR_v1_13@0,The corpus is created by downloading data from the e-Courts website (a publicly available website: https:// districts.ecourts.gov.in/).,The corpus is created by scraping data from the e-Courts website (a publicly available website: https:// districts.ecourts.gov.in/).,"Modify,Clarity",Clarity
3466,334-ARR,334-ARR_v2_14@3,334-ARR_v1_13@2,We download case documents pertaining to the district courts located in the Indian northern state of Uttar Pradesh (U.P.).,We scrape case documents pertaining to the district courts located in the Indian northern state of Uttar Pradesh (U.P.).,"Modify,Clarity",Clarity
3467,334-ARR,334-ARR_v2_2@9,334-ARR_v1_2@9,Experiments with different models are indicative of the need for further research in this area.,Results on different models are indicative of the need for further research in this area.,"Modify,Clarity",Clarity
3468,334-ARR,334-ARR_v2_14@15,334-ARR_v1_13@14,The first step in HLDC creation is the downloading of documents from the e-Courts website.,The first step in HLDC creation is the scraping of documents from the e-Courts website.,"Modify,Clarity",Clarity
3469,334-ARR,334-ARR_v2_2@0,334-ARR_v1_2@0,Many populous countries including India are burdened with a considerable backlog of legal cases.,"Populous countries (e.g., India) are burdened with a considerable backlog of legal cases.","Modify,Clarity",Clarity
3470,334-ARR,334-ARR_v2_15@3,334-ARR_v1_14@3,"The header contains the meta-information related to the case, for example, case number, court identifier, and applicable sections of the law.","The header contains the meta-information related to the case, for example, case number, court identifier, applicable sections of the law, etc.","Modify,Clarity",Clarity
3471,334-ARR,334-ARR_v2_4@2,334-ARR_v1_4@2,"For example, if a system could readily extract the required information from a legal document for a legal practitioner, then it would help expedite the legal process.","For example, if a system could readily extract the required information from a legal document for a legal practitioner, then it would help them expedite the legal process.","Modify,Clarity",Clarity
3472,334-ARR,334-ARR_v2_18@1,334-ARR_v1_15@18,"Body is further segmented into Facts and Arguments, Judge's summary and Case Result.","The body is further segmented into Facts and Arguments, Judge's summary and Case Result.","Modify,Grammar",Grammar
3473,334-ARR,334-ARR_v2_4@4,334-ARR_v1_4@4,"For example, legal documents are typically quite long (tens of pages), highly unstructured and noisy (spelling and grammar mistakes since these are typed), use domainspecific language and jargon; consequently, pre-trained language models do not perform well on these (Malik et al., 2021b).","For example, legal documents are typically quite long (tens of pages), legal documents are highly unstructured and noisy (spelling and grammar mistakes, since these are typed), language in legal documents are domain-specific, and pre-trained language models do not perform well on these (Malik et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3474,334-ARR,334-ARR_v2_23@7,334-ARR_v1_19@7,"As observed in previous work (Malik et al., 2021b), anonymization of a judge's name is important as there is a correlation between a case outcome and a judge name.","As observed in previous work (Malik et al., 2021), anonymization of a judge's name is important as there is a correlation between a case outcome and a judge name.","Modify,Fact/Evidence",Fact/Evidence
3475,334-ARR,334-ARR_v2_23@12,334-ARR_v1_19@12,"Moreover, the Bail corpus and corresponding bail prediction systems can promote the development of explainable systems (Malik et al., 2021b), we leave research on such explainable systems for future work.","Moreover, the Bail corpus and corresponding bail prediction systems can promote the development of explainable systems (Malik et al., 2021), we leave research on such explainable systems for future work.","Modify,Fact/Evidence",Fact/Evidence
3476,334-ARR,334-ARR_v2_4@5,334-ARR_v1_5@0,"Thus, to develop legal text processing systems and address the challenges associated with the legal domain, there is a need for creating specialized legal domain corpora.","To develop legal text processing systems and address the challenges associated with the legal domain, there is a need for creating specialized legal domain corpora.","Modify,Clarity",Clarity
3477,334-ARR,334-ARR_v2_2@1,334-ARR_v1_2@1,Development of automated systems that could process legal documents and augment legal practitioners can mitigate this.,This calls for the development of automated systems that could process legal documents and augment legal practitioners.,"Modify,Claim",Claim
3481,334-ARR,334-ARR_v2_50@5,334-ARR_v1_45@5,Testing is done on a different set of 17 districts not present in train set.,Testing is done on a different set of 17 districts not present during training.,"Modify,Clarity",Clarity
3482,334-ARR,334-ARR_v2_52@4,334-ARR_v1_48@4,"Another thing to note from the results is that, in general, summarization based models perform better than Doc2Vec and transformer-based models, highlighting the importance of the summarization step in the bail prediction task.","Another thing to note from the results is that, in general, summarization models perform better than Doc2Vec and transformer-based models, highlighting the importance of the summarization step in the bail prediction task.","Modify,Clarity",Clarity
3483,334-ARR,334-ARR_v2_5@4,334-ARR_v1_5@3,"Hindi uses Devanagari script (Wikipedia contributors, 2021) for the writing system.","Hindi uses Devanagri (Wikipedia contributors, 2021) script for the writing system.","Modify,Clarity",Clarity
3484,334-ARR,334-ARR_v2_54@1,334-ARR_v1_51@1,"After examining the miss-classified examples, we observed the following.",We observe a couple of things looking at the misclassified examples.,"Modify,Clarity",Clarity
3485,334-ARR,334-ARR_v2_54@6,334-ARR_v1_51@6,"In some instances, we also observed that even if the facts of the cases are similar the judgements can differ.","In some instances, we also observe that even if the facts of the cases are similar the judgements can differ.","Modify,Grammar",Grammar
3486,334-ARR,334-ARR_v2_5@6,334-ARR_v1_5@6,Most of the lower (district) courts in northern India use Hindi as the official language.,Most of the lower (district) courts in Northern India use Hindi as the official language.,"Modify,Grammar",Grammar
3487,334-ARR,334-ARR_v2_5@7,334-ARR_v1_5@7,"However, most of the legal NLP systems that currently exist in India have been developed on English, and these do not work on Hindi legal documents (Malik et al., 2021b).","However, most of the legal NLP systems that currently exist in India have been developed on English, and these do not work on Hindi legal documents (Malik et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
3488,334-ARR,334-ARR_v2_5@8,334-ARR_v1_5@8,"To address this problem, in this paper, we release a large corpus of Hindi legal documents (HINDI LEGAL DOCUMENTS CORPUS or HLDC) that can be used for developing NLP systems that could augment the legal practitioners by automating some of the legal processes.","To address this problem, in this paper, we release a large corpus of Hindi legal documents (Hindi Legal Document Corpus) that can be used for developing NLP systems that could augment the legal practitioners by automating some of the legal processes.","Modify,Clarity",Clarity
3489,334-ARR,334-ARR_v2_58@8,334-ARR_v1_56@3,"Finally, all sentences labelled as judge's opinion either during reverse iteration or during paragraph level extension are extracted out as judge's summary and rest of the sentences form facts and opinions for further modelling.","Finally, all labelled as judge's opinion either during reverse iteration or during paragraph level extension are extracted out as judge's summary and rest of the sentences form facts and opinions for further modelling.","Modify,Clarity",Clarity
3490,334-ARR,334-ARR_v2_2@2,334-ARR_v1_2@2,"However, there is a dearth of high-quality corpora that is needed to develop such data-driven systems.","To develop such data-driven systems, there is a dearth of high-quality corpora.","Modify,Clarity",Clarity
3491,334-ARR,334-ARR_v2_6@0,334-ARR_v1_6@0,"India follows a Common Law system and has a three-tiered court system with District Courts (along with Subordinate Courts) at the lowest level (districts), followed by High Courts at the state level, and the Supreme Court of India at the highest level.","India follows a Common Law system and has a three-tiered court system with District Courts (along with Subordinate Courts) at the lowest levels of districts, followed by High Courts at the state level and the Supreme Court at the highest level.","Modify,Clarity",Clarity
3492,334-ARR,334-ARR_v2_6@1,334-ARR_v1_6@1,"In terms of number of cases, district courts handle the majority.","In terms of the number of cases, district courts handle the majority of the cases.","Modify,Clarity",Clarity
3493,334-ARR,334-ARR_v2_6@2,334-ARR_v1_6@2,"According to India's National Judicial Data Grid, as of November 2021, there are approximately 40 million cases pending in District Courts (National Judicial Data Grid, 2021) as opposed to 5 million cases pending in High Courts.","According to India's National Judicial Data Grid, as of November 2021, there are approximately 40 million cases pending in District courts (National Judicial Data Grid, 2021) as opposed to 5 million cases pending in High Courts.","Modify,Grammar",Grammar
3494,334-ARR,334-ARR_v2_6@3,334-ARR_v1_6@3,These statistics show an immediate need for developing models that could address the problems at the grass-root levels of the Indian legal system.,These statistics show an immediate need for developing systems that could address the problems at the grass-root levels of the Indian legal system.,"Modify,Clarity",Clarity
3495,334-ARR,334-ARR_v2_2@3,334-ARR_v1_2@3,The problem gets even more pronounced in the case of low resource languages such as Hindi.,"The problem gets even more pronounced in the case of low resource language (e.g., Hindi).","Modify,Clarity",Clarity
3496,334-ARR,334-ARR_v2_6@4,334-ARR_v1_6@4,"Out of the 40 million pending cases, approximately 20 million are from courts where the official language is Hindi (National Judicial Data Grid, 2021).","Out of 40 million pending cases, approximately 20 million cases are from courts where the official language is Hindi (National Judicial Data Grid, 2021).","Modify,Grammar",Grammar
3497,334-ARR,334-ARR_v2_6@5,334-ARR_v1_6@5,"In this resource paper, we create a large corpus of 912,568 Hindi legal documents.","In this resource paper, we create a large corpus of about 900K Hindi legal documents.","Modify,Fact/Evidence",Fact/Evidence
3498,334-ARR,334-ARR_v2_6@6,334-ARR_v1_6@6,"In particular, we collect documents from the state of Uttar Pradesh, the most populous state of India with a population of approximately 237 million (PopulationU, 2021).","In particular, we collect documents from the state of Uttar Pradesh (U.P.), the most populous state of India with a population of approximately 237 million (Popula-tionU, 2021).","Modify,Clarity",Clarity
3499,334-ARR,334-ARR_v2_6@7,334-ARR_v1_6@7,"The Hindi Legal Documents Corpus (HLDC) can be used for a number of legal applications, and as a use case, in this paper, we propose the task of Bail Prediction.","The Hindi Legal Document Corpus (HLDC) can be used for a number of legal applications, and in this paper, we propose the task of Bail Prediction.","Modify,Clarity",Clarity
3500,334-ARR,334-ARR_v2_2@4,334-ARR_v1_2@4,"In this resource paper, we introduce the Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents in Hindi.","In this resource paper, we introduce the Hindi Legal Documents Corpus (HLDC), a corpus of 900K legal documents in Hindi.","Modify,Fact/Evidence",Fact/Evidence
3501,334-ARR,334-ARR_v2_2@5,334-ARR_v1_2@5,Documents are cleaned and structured to enable the development of downstream applications.,The documents are cleaned and structured to enable the development of downstream applications.,"Modify,Grammar",Grammar
3502,334-ARR,334-ARR_v2_12@0,334-ARR_v1_10@0,"Majority of the work in the legal domain has focused on the higher court (Malik et al., 2021b;Strickson and De La Iglesia, 2020;Zhong et al., 2020b); however, the lower courts handle the maximum number of cases.","Majority of the work in the legal domain has focused on the higher court (Malik et al., 2021;Strickson and De La Iglesia, 2020;Zhong et al., 2020b); however, the lower courts handle the maximum number of cases.","Modify,Fact/Evidence",Fact/Evidence
3503,334-ARR,334-ARR_v2_2@6,334-ARR_v1_2@6,"Further, as a use-case for the corpus, we introduce the task of bail prediction.","Further, as a usecase for the corpus, we introduce the task of Bail Prediction.","Modify,Grammar",Grammar
3504,334-ARR,334-ARR_v2_12@3,334-ARR_v1_10@3,"The competition has two sub-tasks, a legal information retrieval task and an entailment identification task between law articles and queries.","The competition had two sub-tasks, a legal information retrieval task and an entailment identification task between law articles and queries.","Modify,Grammar",Grammar
5737,86-ARR,86-ARR_v2_21@1,86-ARR_v1_22@1,"ReLU(•) here denotes the ReLU activation function (Nair and Hinton, 2010), σ(•) represents the sigmoid function.","ReLU(.) here denotes the ReLU activation function (Nair and Hinton, 2010), σ(.) represents the sigmoid function.","Modify,Grammar",Grammar
5738,86-ARR,86-ARR_v2_35@2,86-ARR_v1_38@0,"The step-by-step explanation helps the model to do better inference, and the stepwise inference in turn guides the generation of better explanation.","Step-by-step interpretation helps the model to better inference, stepwise inference in turn guides the generation of better explanation.","Modify,Clarity",Clarity
5739,86-ARR,86-ARR_v2_40@0,86-ARR_v1_44@0,"Because of the intractability of directly estimating the mutual information in high-dimensional space, we approximate the optimization objective with a Variational Information Maximization lower bound (Chen et al., 2016b;Zhang et al., 2018;Poole et al., 2019):","Because of the intractability of directly estimating the mutual information in high-dimensional space, we approximate the optimization objective with a Variational Information Maximization (Chen et al., 2016b;Zhang et al., 2018) lower bound (Poole et al., 2019):","Modify,Clarity",Clarity
5740,86-ARR,86-ARR_v2_42@1,86-ARR_v1_46@1,"P θ (L, E|X) and Q ϕ (X|L, E) denote the forward network (generating L, E conditioned on X) and the backward network (generating X conditioned on L, E) respectively.","P θ (L, E|X) and Q ϕ (X|L, E) denote the forward network (generating L, R conditioned on X) and the backward network (generating X conditioned on L, E) respectively.","Modify,Fact/Evidence",Fact/Evidence
5741,86-ARR,86-ARR_v2_4@1,86-ARR_v1_4@1,"In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretations to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2019;Thorne et al., 2019;Kumar and Talukdar, 2020).","In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2018;Thorne et al., 2019;Kumar and Talukdar, 2020).","Modify,Fact/Evidence",Fact/Evidence
5742,86-ARR,86-ARR_v2_51@0,86-ARR_v1_55@0,"Besides, we add an objective term P θ (L, E|X) of maximize the negative likelihood of P θ to balance the positive samples as teacher-forcing algorithm (Li et al., 2017).","We also add an objective term P θ (L, E|X) of maximum the negative likelihood of P θ to balance the positive samples as teacher-forcing algorithm (Li et al., 2017).","Modify,Other",Other
5743,86-ARR,86-ARR_v2_57@0,86-ARR_v1_61@0,"We use six datasets as our testbeds: SNLI (Bowman et al., 2015), e-SNLI (Camburu et al., 2018), CQA (Talmor et al., 2019), CoS-E (Rajani et al., 2019, MultiNLI (Williams et al., 2018), and SICK-E (Marelli et al., 2014).","We use six datasets as our testbeds: SNLI (Bowman et al., 2015), e-SNLI (Camburu et al., 2018), CQA (Talmor et al., 2018), CoS-E (Rajani et al., 2019), MultiNLI (Williams et al., 2018), and SICK-E (Marelli et al., 2014).","Modify,Fact/Evidence",Fact/Evidence
5744,86-ARR,86-ARR_v2_58@3,86-ARR_v1_62@3,"SICK-e (Sentences Involving Compositional Knowledge for entailment) provides sentence pairs that are rich in the lexical, syntactic and semantic phenomena.","SICKe(Sentences Involving Compositional Knowledge for entailment) provides sentence pairs that are rich in the lexical, syntactic and semantic phenomena.","Modify,Grammar",Grammar
5745,86-ARR,86-ARR_v2_58@4,86-ARR_v1_62@4,The latter two datasets are used for out-of-domain evaluation.,The latter two datasets are used for out-of-domain test.,"Modify,Clarity",Clarity
5746,86-ARR,86-ARR_v2_60@3,86-ARR_v1_64@3,"The Transformer model (Vaswani et al., 2017) adds a MLP layer for making predictions.","The Transformer model (Vaswani et al., 2017) adds a MLP layer for generating sentencelevel interpretations.","Modify,Fact/Evidence",Fact/Evidence
5747,86-ARR,86-ARR_v2_67@0,86-ARR_v1_73@0,"As shown in Table 2, we evaluate our method with the Transformer baseline model on two outof-domain datasets: MultiNLI and SICK-E. The results show that our mutual promotion method enables the Transformer model to be more robust, and achieves about 3 absolute accuracy improvement on both of the out-of-domain datasets without fine-tuning.","As shown in Table 2, we evaluate our method with the Transformer baseline model on two outof-domain datasets: MultiNLI and SICK-E. The results show that our mutual promotion method enables the Transformer model to be more robust, and achieve more than 3 accuracy improvement on both of the out-of-domain datasets without fine-tuning.","Modify,Fact/Evidence",Fact/Evidence
5748,86-ARR,86-ARR_v2_67@2,86-ARR_v1_74@1,The ablation results demonstrate both the adversarial mutual information training strategy in AFiRe and deep integration in SIM is very effective to improve the model's generalization and robustness.,The ablation results demonstrate the adversarial mutual information training strategy in AFiRe is very effective to improve the model's generalization and robustness.,"Modify,Fact/Evidence",Fact/Evidence
5749,86-ARR,86-ARR_v2_8@1,86-ARR_v1_8@1,"Considering readability and comprehensibility for humans, some works turn to generate token-level explanations (Liu et al., 2019;Thorne et al., 2019), which are nevertheless prone to cause ambiguity.","Considering readability and comprehensibleness for human, some works turn to generate token-level explanations (Liu et al., 2018;Thorne et al., 2019), which nevertheless prone to cause ambiguity.","Modify,Fact/Evidence",Fact/Evidence
5750,86-ARR,86-ARR_v2_73@4,86-ARR_v1_80@1,"From the clear split of the red and blue lines when ""does"" and ""not"" are generated, we can see that the prediction is very sensitive to explanation, which demonstrates the faithfulness (Kumar and Talukdar, 2020).","From the clear split of the red and blue lines when 'does' and 'not' are generated, we can see that the prediction is very sensitive to explanation, which demonstrates the faithfulness (Kumar and Talukdar, 2020).","Modify,Grammar",Grammar
5751,86-ARR,86-ARR_v2_76@1,86-ARR_v1_82@1,"For the first example, CAGE makes wrong prediction, and generates explanation that obviously conflicts with common knowledge.","For the first exapmle, CAGE makes wrong prediction, and generates explanation that obviously conflicts with common knowledge.","Modify,Grammar",Grammar
5752,86-ARR,86-ARR_v2_79@0,86-ARR_v1_86@0,"With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2019;Thorne et al., 2019;Kumar and Talukdar, 2020).","With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2018;Thorne et al., 2019;Kumar and Talukdar, 2020).","Modify,Fact/Evidence",Fact/Evidence
5753,86-ARR,86-ARR_v2_8@6,86-ARR_v1_8@6,"Intuitively, human language sentence-level interpretations containing reasoning logic are the best form for human to understand.","Intuitively, human language sentence-level interpretations containing reasoning logic is the best form for human to understand.","Modify,Grammar",Grammar
5754,86-ARR,86-ARR_v2_9@0,86-ARR_v1_9@0,"With annotated natural language interpretation datasets available (Camburu et al., 2018;Rajani et al., 2019), methods of generating sentence-level interpretation have been explored recently.","With the annotated natural language interpretation datasets available (Camburu et al., 2018;Rajani et al., 2019), methods of generating sentencelevel interpretation have been explored recently.","Modify,Grammar",Grammar
5755,86-ARR,86-ARR_v2_9@2,86-ARR_v1_9@2,"Kumar and Talukdar (2020) proposed to first generate sentence-level interpretations with deep pre-trained language models (such as BERT and GPT), then fed those interpretations as extra knowledge to help improve inference performance.","Kumar and Talukdar (2020) proposed to first generate sentence-level interpretation with deep pre-trained language models (such as BERT and GPT), then fed those interpretation as extra knowledge to help improve inference performance.","Modify,Grammar",Grammar
5756,86-ARR,86-ARR_v2_12@0,86-ARR_v1_12@0,"• Different from the previous works that only include one-side promotion, we mutually promote the inference and sentence-level interpretation from both the model-level and the optimization-level. • We propose a Stepwise Integration Mechanism to tightly fuse latent prediction and interpretation information at every decoding step, and an Adversarial Fidelity Regularization to further improve the fidelity with the adversarial training strategy. • Experiment results show that our method achieves significant improvement in both inference accuracy and interpretation quality compared with baseline models.","• Different from the previous works that only include one-side promotion, we mutually promote the inference and sentence-level interpretation from both the model-level and the optimization-level. • We propose a Stepwise Integration Mechanism to tightly fuse latent prediction and interpretation information at every decoding step, and an Adversarial Fidelity Regularization to further improve the fidelity with the adversarial training strategy. • Experiment results show that our method achieve significant improvement in both inference accuracy and interpretation quality compared with baseline models.","Modify,Grammar",Grammar
5757,86-ARR,86-ARR_v2_14@1,86-ARR_v1_14@1,"Utilizing the autoregressive nature of Transformer decoder, SIM enables deep interaction at every decoding step between inference and interpretation.","Utilizing the autoregressive nature of Transformer decoder, SIM allows deep interaction at every decoding step between inference and interpretation.","Modify,Clarity",Clarity
5758,86-ARR,86-ARR_v2_14@2,86-ARR_v1_14@2,"With the adversarial training strategy, AFiRe enables further integration of latent semantic information between inference and interpretation, and also improves the quality of explanation sentences by bringing them closer to human expressions.","With the adversarial training strategy, AFiRe allows further integration of latent semantic information between inference and interpretation, and also improves the quality of explanation sentences, bringing them closer to human expressions.","Modify,Clarity",Clarity
5762,89-ARR,89-ARR_v2_19@1,89-ARR_v1_20@1,"Specifically, the boolean variables (for tweet and word selection) are relaxed into the continuous space so that they can be optimized by gradientbased methods over a convex hull.","Specifically, the boolean variables (for tweet and word selection) would be relaxed into the continuous space so that they can be optimized by gradient-based methods over a convex hull.","Modify,Clarity",Clarity
5763,89-ARR,89-ARR_v2_21@1,89-ARR_v1_22@1,"We evaluate our adversarial attack on a stock prediction dataset consisting of 10,824 instances including relevant tweets and numerical features of 88 stocks from 2014 to 2016 (Xu and Cohen, 2018).","We evaluate our adversarial attack on a stock prediction dataset consisting of 10824 instances including relevant tweets and numerical features of 88 stocks from 2014 to 2016 (Xu and Cohen, 2018).","Modify,Grammar",Grammar
5764,89-ARR,89-ARR_v2_4@0,89-ARR_v1_4@0,"The advance of deep learning based language models are playing a more and more important role in the financial context, including convolutional neutral network (CNN) (Ding et al., 2015), recurrent neutral network (RNN) (Minh et al., 2018), long short-term memory network (LSTM) (Hiew et al., 2019;Sawhney et al., 2021;Hochreiter and Schmidhuber, 1997), graph neutral network (GNN) (Sawhney et al., 2020a,b), transformer , autoencoder (Xu and Cohen, 2018), etc.","The advance of deep learning based language models are playing a more and more important role in the financial context, including convolutional neutral network (CNN) (Ding et al., 2015), recurrent neutral network (RNN) (Minh et al., 2018), long short-term memory network (LSTM) (Hiew et al., 2019;Sawhney et al., 2021;Hochreiter and Schmidhuber, 1997), graph neutral network (GNN) (Sawhney et al., 2020a,b), transformer (Yang et al., 2020), autoencoder (Xu and Cohen, 2018), etc.","Modify,Fact/Evidence",Fact/Evidence
5765,89-ARR,89-ARR_v2_21@3,89-ARR_v1_22@3,We apply our attack to instances on which the victim models make correct predictions.,We apply our attack to instances on which the victim models make correct prediction.,"Modify,Grammar",Grammar
5766,89-ARR,89-ARR_v2_24@2,89-ARR_v1_25@2,"For both JO and AGO, ASR increases by roughly 10% and F1 drops by 0.1 on average in comparison to the random attack.","As we can see, for both JO and AGO, ASR increases by roughly 10% and F1 drops by 0.1 on average in comparison to random attack.","Modify,Clarity",Clarity
5767,89-ARR,89-ARR_v2_24@6,89-ARR_v1_25@6,It appears that the attack performance becomes saturated if we keep increasing the attack budgets.,It appears that the attack performance becomes saturated if we keep increasing the attack budget.,"Modify,Grammar",Grammar
5768,89-ARR,89-ARR_v2_24@7,89-ARR_v1_25@7,"In fact, the attack with budget of one tweet and one word is the most cost effective, provided that it introduces minimum perturbation but achieves a relatively similar ASR.","In fact, the attack with budget of one tweet and one word is most cost effective, provided that it introduces minimum perturbation but achieves relatively similar ASR.","Modify,Grammar",Grammar
5771,89-ARR,89-ARR_v2_28@0,89-ARR_v1_29@1,This work demonstrates that our adversarial attack method consistently fools various financial forecast models even with physical constraints that the raw tweet can not be modified.,The experiments demonstrate that our adversarial attack method consistently fools various models.,"Modify,Claim",Claim
5772,89-ARR,89-ARR_v2_28@1,89-ARR_v1_29@2,"Adding a retweet with only one word replaced, the attack can cause 32% additional loss to our simulated investment portfolio.","Moreover, with replacement of a single word on one tweet, the attack can cause 32% additional loss to our simulated portfolio.","Modify,Fact/Evidence",Fact/Evidence
5775,89-ARR,89-ARR_v2_2@0,89-ARR_v1_2@0,"More and more investors and machine learning models rely on social media (e.g., Twitter and Reddit) to gather real-time information and sentiment to predict stock price movements.","More and more investors and machine learning models rely on social media (e.g., Twitter and Reddit) to gather information and predict movements stock prices.","Modify,Claim",Claim
5776,89-ARR,89-ARR_v2_40@1,89-ARR_v1_41@1,"An computationally efficient fashion is to optimize over a convex hull constructed with linear combination of candidate set, and the optimal replacement goes with word with highest weight (Dong et al., 2021).","An computationally efficient fashion is to optimize over a convex hull constructed with linear combination of candidate set, and optimal replacement goes with word with highest weight (Dong et al., 2021).","Modify,Grammar",Grammar
5777,89-ARR,89-ARR_v2_5@1,89-ARR_v1_5@1,"The perturbation can be at the sentence level (e.g., Xu et al., 2021;Iyyer et al., 2018;Ribeiro et al., 2018), the word level (e.g., Zhang et al., 2019;Alzantot et al., 2018;Zang et al., 2020;Jin et al., 2020;Lei et al., 2019;Zhang et al., 2021;Lin et al., 2021), or both (Chen et al., 2021).","The perturbation can be done at the sentence level (e.g., Xu et al., 2021;Iyyer et al., 2018;Ribeiro et al., 2018), the word level (e.g., Zhang et al., 2019;Alzantot et al., 2018;Zang et al., 2020;Jin et al., 2020;Lei et al., 2018;Zhang et al., 2021;Lin et al., 2021), or both (Chen et al., 2021).","Modify,Fact/Evidence",Fact/Evidence
5778,89-ARR,89-ARR_v2_5@2,89-ARR_v1_5@2,"We are interested in whether such adversarial attack vulnerability also exists in stock prediction models, as these models embrace more and more human-generated media data (e.g., Twitter, Reddit, Stocktwit, Yahoo News (Xu and Cohen, 2018;Sawhney et al., 2021)).","We are interested in whether such adversarial attack vulnerability also exists in stock prediction models, as these models embrace more and more user-generated public data (e.g., Twitter, Reddit, or Stocktwit (Xu and Cohen, 2018;Sawhney et al., 2021)).","Modify,Fact/Evidence",Fact/Evidence
5779,89-ARR,89-ARR_v2_5@3,89-ARR_v1_5@3,The adversarial robustness is a more critical issue in the context of stock prediction as anyone can post perturbed tweets or news to influence forecasting models.,The adversarial robustness may be a more critical topic in the context of stock prediction as anyone can post perturbed tweets to influence forecast models.,"Modify,Clarity",Clarity
5780,89-ARR,89-ARR_v2_5@4,89-ARR_v1_5@4,"For example, a fake news (""Two Explosions in the White House and Barack Obama is Injured"") posted by a hacker using the Associated-Press's Twitter account on 04/23/2013 erased $136 billion market value in just 60 seconds (Fisher, 2013).","As one example, a fake news (""Two Explosions in the White House and Barack Obama is Injured"") posted by a hacker using the AssociatedPress's Twitter account on 04/23/2013 erased $136 billion in stock market in just 60 seconds (Fisher, 2013).","Modify,Clarity",Clarity
5781,89-ARR,89-ARR_v2_5@5,89-ARR_v1_5@5,"Although the event doesn't fall into the category of adversarial attack, it rings the alarm for traders who use (social) media information for their trading decisions.","Although the event doesn't fall into the category of adversarial attack, it rings the alarm for traders who take information from social media to back their trading decision.","Modify,Clarity",Clarity
5782,89-ARR,89-ARR_v2_62@7,89-ARR_v1_63@7,The results show that loss smoothing does not contribute to attack performance in our experiment as it does in Srikant et al. (2021).,"The results show that loss smoothing does not contribute to attack performance in our experiment as it does in (Srikant et al., 2021).","Modify,Grammar",Grammar
5783,89-ARR,89-ARR_v2_63@2,89-ARR_v1_64@2,We then run Kmeans clustering on these 18 corpora based on the feature matrix from LIWC.,We then run Kmeans clustering these 18 corpora based on the feature matrix from LIWC.,"Modify,Grammar",Grammar
5784,89-ARR,89-ARR_v2_2@1,89-ARR_v1_2@1,"Although text-based models are known to be vulnerable to adversarial attacks, whether stock prediction models have similar vulnerability is underexplored.","Although textbased models are known to be vulnerable to adversarial attacks, whether stock prediction models have similar vulnerability given necessary constraints is underexplored.","Modify,Clarity",Clarity
5785,89-ARR,89-ARR_v2_6@1,89-ARR_v1_6@1,"Many attacks modify benign text directly (manipulation attack) and use them as model input; However, in our case, adversarial retweets enter the model along with benign tweets (concatenation attack), which is more realistic as malicious Twitter users can not modify others' tweets.","Many attack modifies benign text directly (manipulation attack) and use them as model input; However, in our case, adversarial retweets enter the model along with benign tweets (concatenation attack), which is more realistic as malicious Twitter users can not modify others' tweets.","Modify,Grammar",Grammar
5786,89-ARR,89-ARR_v2_6@2,89-ARR_v1_6@2,"In other words, we formulate the task as a text-concatenating attack (Jia and Liang, 2017;Le et al., 2021): we implement the attack by injecting new tweets instead of manipulating existing benign tweets.","In other words, we formulate the task as text-concatenating attack (Jia and Liang, 2017;Le et al., 2021): we implement the attack by injecting new tweets instead of manipulating existing benign tweets.","Modify,Grammar",Grammar
5787,89-ARR,89-ARR_v2_6@3,89-ARR_v1_6@3,"Our task is inspired and mimics the retweet function on social media, and uses it to feed the adversarial samples into the dataset.","Our task is inspired and mimics the retweet function on social media, and use it to feed the adversarial samples into the dataset.","Modify,Grammar",Grammar
5788,89-ARR,89-ARR_v2_6@4,89-ARR_v1_6@4,"Despite various algorithms are proposed to generate manipulation attack, literature of concatenation attack on classification models is rare, with exceptions Le et al. (2021), Song et al. (2021) and Wang et al. (2020).","Despite various algorithms are proposed to generate manipulation attack, literature of concatenation attack on classification model is rare, with exceptions Le et al. (2021), Song et al. (2021) and Wang et al. (2020).","Modify,Grammar",Grammar
5789,89-ARR,89-ARR_v2_6@5,89-ARR_v1_6@5,Our paper provides extra evidence of their difference by investigating their performances in the financial domain.,Our paper provides extra evidence of their difference by investigating their performances in the domain of finance.,"Modify,Clarity",Clarity
5790,89-ARR,89-ARR_v2_7@0,89-ARR_v1_7@0,The main challenge is to craft new and effective adversarial tweets.,The main challenge is to craft new adversarial tweets.,"Modify,Claim",Claim
5791,89-ARR,89-ARR_v2_7@1,89-ARR_v1_7@1,We solve the task by aligning the semantics with benign tweets so that the potential human and machine readers can't detect our adversarial tweets.,"While the adversarial tweets can be arbitrary given that they are newly posted, we solve the task by aligning the semantics with benign tweets so that potential human and machine readers can not detect our adversarial tweets.","Modify,Claim",Claim
5792,89-ARR,89-ARR_v2_7@3,89-ARR_v1_7@3,"Specific tweets are first selected, which are used as the target of perturbation on a limit number of words within the tweets.","Specific tweets are first selected, which are used as target of perturbation on a limit number of words within the tweets.","Modify,Grammar",Grammar
5793,89-ARR,89-ARR_v2_7@6,89-ARR_v1_7@6,"More astonishingly, the attack can cause additional loss of 23% to 32% if an investor trades on the predictions of the victim models (Fig. 4).","More astonishingly, the attack can cause additional loss of 23% to 32% if the investor trades on predictions of the victim models (Fig. 4).","Modify,Grammar",Grammar
5794,89-ARR,89-ARR_v2_10@3,89-ARR_v1_10@3,"Secondly, adversarial tweets are optimized to be semantically similar to the original tweets so that they are not counterfactual and very likely to fool human sanity checks as well as the Twitter's content moderation system.","Secondly, adversarial tweets are optimized to be semantically similar to original tweets so that they are not counterfactual and very likely fool human sanity checks as well as the Twitter's content moderator mechanisms.","Modify,Clarity",Clarity
5795,89-ARR,89-ARR_v2_11@1,89-ARR_v1_11@1,The challenge of our attack method centers around how to select the optimal tweets and the token perturbations with the constraints of semantic similarity.,The challenge of our attack method centers around how to select the optimal tweets and the token perturbations with constraints of semantic similarity.,"Modify,Grammar",Grammar
5796,89-ARR,89-ARR_v2_11@3,89-ARR_v1_11@3,"In the first step, a set of optimal tweets is first selected as the target tweets to be perturbed and retweeted.","In the first step, a set of optimal tweets is first selected as target tweets to be perturbed and retweeted.","Modify,Grammar",Grammar
5797,89-ARR,89-ARR_v2_11@4,89-ARR_v1_11@4,"For each selected tweet in the pool, the word selection problem is then solved to find one or more optimal words to apply perturbation.","For each selected tweet in the pool, the word selection problem is then solved to find one or more best words to apply perturbation.","Modify,Clarity",Clarity
5798,89-ARR,89-ARR_v2_11@5,89-ARR_v1_11@5,Word and tweet budgets are also introduced to quantify the strength of the perturbation.,Word and tweet budgets are also introduced to quantifies the strength of perturbation.,"Modify,Grammar",Grammar
5799,89-ARR,89-ARR_v2_12@0,89-ARR_v1_12@0,"We consider the word replacement and deletion for word perturbation (Garg and Ramakrishnan, 2020;Li et al., 2020).","We consider word replacement and deletion for word perturbation (Garg and Ramakrishnan, 2020;Li et al., 2020).","Modify,Grammar",Grammar
5800,89-ARR,89-ARR_v2_12@2,89-ARR_v1_12@2,"A synonym as replacement is widely adopted in the word-level attack since it is a natural choice to preserve semantics (Zang et al., 2020;Dong et al., 2021;Zhang et al., 2019;Jin et al., 2020).","Synonym as replacement is widely adopted in the word-level attack since it is a natural choice to preserve semantics (Zang et al., 2020;Dong et al., 2021;Zhang et al., 2019;Jin et al., 2020).","Modify,Grammar",Grammar
5801,89-ARR,89-ARR_v2_17@1,89-ARR_v1_18@1,"Consequently, given attack loss L, generation of adversarial retweets can be formulated as the optimization program min m,z,u and c) 1 T u i,j = 1, ∀i, j, where b s and b w denote the tweet and word budgets.","Consequently, given attack loss L, generation of adversarial retweets can be formulated as the optimization program min m,z,u and c) 1 T u i,j = 1, ∀i, j, where b s and b w denote the tweet and word budget.","Modify,Grammar",Grammar
5802,89-ARR,89-ARR_v2_17@2,89-ARR_v1_18@2,It is worth to stress that perturbation is only applied to the date (t) when the attack is implemented to preserve the temporal order.,It is worth to stress that perturbation is only applied to the date (t) when the attack is implemented to preserve temporal order.,"Modify,Grammar",Grammar
5995,1-12,1-12_v2_28@3,1-12_v1_28@3,Membrane proteins would be transported by vesicular transport from the PVM to Maurer's clefts where their release from the PI3P patches could be triggered by a PI3P-phosphatase.,Membrane proteins would be transported by vesicular transport from the PVM to Maurer’s clefts where their release from the PI3P patches could be triggered by a PI3P-phosphatase.,"Modify,Grammar",Grammar
5996,1-12,1-12_v2_32@6,1-12_v1_32@6,"The authors' interpretation of the data was that proteins must be unfolded in order to be transported across the PVM into the erythrocyte, that the PVM therefore contained a protein-conducting channel with similar requirements for transport as the Sec61 channel in the ER membrane, and that the time window after synthesis during which proteins were transport-competent was limited.","The authors’ interpretation of the data was that proteins must be unfolded in order to be transported across the PVM into the erythrocyte, that the PVM therefore contained a protein-conducting channel with similar requirements for transport as the Sec61 channel in the ER membrane, and that the time window after synthesis during which proteins were transport-competent was limited.","Modify,Grammar",Grammar
5997,1-12,1-12_v2_10@4,1-12_v1_10@4,- The uncleaved targeting signal binds PI3P at the ER membrane with the same specificity required for protease cleavage and host cell targeting; the cleaved signal no longer binds PI3P <REF-13> .,- The uncleaved targeting signal binds phosphoinositol-3-phosphate (PI3P) at the ER membrane with the same specificity required for protease cleavage and host cell targeting; the cleaved signal no longer binds PI3P <REF-13> .,"Modify,Clarity",Clarity
5998,1-12,1-12_v2_10@5,1-12_v1_10@5,"- A putative 'translocator' complex resides in the PV membrane (PVM); it consists of 5 proteins that coprecipitate some of the proteins bearing a host cell targeting signal, but a function of the complex has not been demonstrated in any way, nor has it been investigated whether the association of the complex and the PEXEL proteins is mediated by the PEXEL signal <REF-14> , <REF-15> .","- A putative ‘translocator’ complex resides in the PV membrane (PVM); it consists of 5 proteins that coprecipitate some of the proteins bearing a host cell targeting signal, but a function of the complex has not been demonstrated in any way, nor has it been investigated whether the association of the complex and the PEXEL proteins is mediated by the PEXEL signal <REF-14> , <REF-15> .","Modify,Grammar",Grammar
5999,1-12,1-12_v2_12@1,1-12_v1_12@1,"This cannot be right: N-acetylation is a cytosolic modification, based on the biochemical and sequence data characterizing the protease plasmepsin V, its active site is almost certainly on the cytoplasmic face of the membrane, and the only possible location for PI3P at the ER membrane is in the cytosolic leaflet.","This cannot be right: N-acetylation is a cytosolic modification, the active site of plasmepsin V is almost certainly on the cytoplasmic face of the ER membrane (based on the biochemical & sequence data characterizing the protease), and the only possible location for PI3P at the ER membrane is in the cytosolic leaflet and the only possible location for PI3P at the ER membrane is in the cytosolic leaflet.","Modify,Clarity",Clarity
6000,1-12,1-12_v2_16@2,1-12_v1_16@2,"Russo and colleagues showed later (2010) <REF-10> that fusion of this region of plasmepsin V to a fluorescent reporter protein resulted in ring-shaped staining around the parasite cytoplasm indicative of location in the PV or at the parasite plasma membrane, and suggesting entry of the protein into the ER and transport to the cell surface.","Russo and colleagues (2010) <REF-10> showed later that this region of plasmepsin V was not able to target the protein to the ER, which demonstrates that it is not a signal sequence.","Modify,Fact/Evidence",Fact/Evidence
6001,1-12,1-12_v2_16@5,1-12_v1_16@3,In the same paper Russo and colleagues demonstrated that the C-terminal hydrophobic region of plasmepsin V was required for ER retention of the protein <REF-10> .,In the same paper Russo and colleagues demonstrated that the C-terminal hydrophobic region of plasmepsin V was required to anchor the protein to the ER membrane.,"Modify,Fact/Evidence",Fact/Evidence
6002,1-12,1-12_v2_16@6,1-12_v1_16@4,"Both Russo and colleagues and Klemba and Goldberg describe this region as a transmembrane domain, but Klemba and Goldberg show that 50% of plasmepsin V can be extracted from the membrane by carbonate, pH 11.0 <REF-12> .","Both Russo and colleagues and Klemba and Goldberg describe this region as a transmembrane domain, but Klemba & Goldberg show that 50% of plasmepsin V can be extracted from the membrane by carbonate, pH 11.0 <REF-12> .","Modify,Grammar",Grammar
6003,1-12,1-12_v2_16@8,1-12_v1_16@6,"Altogether these data suggest that plasmepsin V is tethered to the ER membrane by hydrophobic regions at both termini, and that its active site is in the cytoplasm.",Altogether these data suggest that plasmepsin V is a carboxy-terminally membrane-anchored or membrane-associated protein with its entire N-terminal domain including the active site in the cytoplasm.,"Modify,Claim",Claim
6019,1-21,1-21_v2_16@0,1-21_v1_16@0,"We did not observe any difference in the impact of live phage on bacterial populations exposed to the different treatments (KW, df = 2, P = 0.153), suggesting that the inducible response does not alter bacteria resistance to phage predation.","We did not observe any difference in the impact of live phage on bacterial populations exposed to the different treatments (KW, df=2, P = 0.153), suggesting that the inducible response does not alter bacteria resistance to phage predation.","Modify,Grammar",Grammar
6020,1-21,1-21_v2_18@1,1-21_v1_18@1,We hypothesize that this response increases the survival chances of bacterial progeny under natural conditions and demonstrate that this behaviour comes at a fitness cost of reduced size of daughter cells.,We hypothesize that this response increases the survival chances of progeny under natural conditions and demonstrate that this behaviour comes at a fitness cost of reduced size of daughter cells.,"Modify,Claim",Claim
6021,1-21,1-21_v2_18@10,1-21_v1_19@0,"Furthermore, our results support and extend both theoretical <REF-33> and empirical <REF-34> , <REF-35> predictions that victims may lessen the fitness impact of their natural enemies through early reproduction, to cases where phenotypic responses are plastic and temporary.","These results support and extend both theoretical <REF-32> and empirical <REF-33> , <REF-34> predictions that victims may lessen the fitness impact of their natural enemies through early reproduction, to cases where phenotypic responses are plastic and temporary.","Modify,Clarity",Clarity
6022,1-21,1-21_v2_18@11,1-21_v1_19@1,Increased allocation to reproduction in stressful environments–termed “fecundity compensation” or “terminal investment” <REF-31> –although never studied in bacteria-phage associations to our knowledge–has been extensively studied for other host-parasite (or organism-stressor) interactions.,Increased allocation to reproduction in stressful environments–termed “fecundity compensation” or “terminal investment” <REF-31> – although never studied in bacteria-phage associations to our knowledge – has been extensively studied for other host-parasite (or organism - stressor) interactions.,"Modify,Grammar",Grammar
6023,1-21,1-21_v2_21@3,1-21_v1_22@3,"Assuming that the physiological mechanisms involved in fission rate increases are the same in the two experiments, this suggests that rapid multiplication is not adaptive for the bacterium, and indeed we report no advantage of being exposed to inactived phage in terms of a lessened population impact during live phage exposure.","Assuming that the physiological mechanisms involved in fission rate increases are the same in the two experiments, this suggests that rapid multiplication is not adaptive for the bacterium.","Modify,Fact/Evidence",Fact/Evidence
6024,1-21,1-21_v2_21@7,1-21_v1_22@7,"Future studies should therefore focus on the possible adaptive nature of this response for both bacterium and phage, by investigating in greater depth how it affects the mechanisms of infection, recovery, and resistance.",Future studies should therefore focus on the possible adaptive nature of this response for both bacterium and phage.,"Modify,Claim",Claim
6025,1-21,1-21_v2_28@4,1-21_v1_29@4,Inactivated phage were allowed 4 h to attach to the bacterial outer membrane.,Inactivated phage were allowed 4h to attach to the bacterial outer membrane.,"Modify,Grammar",Grammar
6026,1-21,1-21_v2_28@7,1-21_v1_29@7,"PCR was done using TPV1f (GATGTGAGAAAGCGATACACGG) and TPV1r (GAGAGAAGCGGGAGAGTGAA) sequences developed for this study, which selectively amplify a 550 bp fragment of the phage DNA and a 1200 bp fragment of the bacterial DNA (see Supplementary Figure 1 for detailed protocols).","PCR was done using TPV1f (GATGTGAGAAAGCGATACACGG) and TPV1r (GAGAGAAGCGGGAGAGTGAA) sequences developed for this study, which selectively amplify a 550 bp fragment of the phage DNA and a 1200 bp fragment of the bacterial DNA (see Supplement Figure 1 for detailed protocols).","Modify,Grammar",Grammar
6027,1-21,1-21_v2_28@8,1-21_v1_29@8,"We did not find any evidence that UV-inactivated phage was present in samples putatively containing bacteria only, thus confirming that (i) the DNA of inactivated phage was not incorporated in the bacterial cell and (ii) our centrifugation method removed both bound and unbound phage.","We did not find any evidence that UV-inactivated phage was present in samples putatively containing bacteria only, thus confirming that the DNA of inactivated phage was not incorporated in the bacterial cell.","Modify,Claim",Claim
6028,1-21,1-21_v2_30@1,1-21_v1_31@1,Fixed SBW25 bacteria of the smooth morphotype were first cultivated in 6 ml KB in 30 mL universal glass vials.,Fixed smooth SBW25 bacteria were first cultivated in 6 ml KB in 30 mL universal glass vials.,"Modify,Clarity",Clarity
6029,1-21,1-21_v2_34@1,1-21_v1_35@1,"Exponential phase was determined by conducting a series of windowed linear regressions over the full growth curve, and retaining the part of the curve with the largest slope (computer code given in Supplementary materials part 3 ).","Exponential phase was determined by conducting a series of windowed linear regressions over the full growth curve, and retaining the part of the curve with the largest slope (computer code given in suppl. materials part 3).","Modify,Clarity",Clarity
6030,1-21,1-21_v2_8@8,1-21_v1_8@7,"Terminal investment is well characterised for other host-parasite associations <REF-31> , but to our knowledge has not previously been observed in bacteria subject to phage infection.","Terminal investment is well characterised for other host-parasite associations <REF-31> , but to our knowledge has not previously been observed in bacteria and phage.","Modify,Claim",Claim
6031,1-21,1-21_v2_10@0,1-21_v1_10@0,"Bacteria exposed to UV-inactivated phage display a statistically significant higher growth rate over the first 24 hours post-exposure than non-phage controls (Kruskal-Wallis, df = 3, P = 0.006; Figure 1 ).","Bacteria exposed to UV-inactivated phage display a statistically significant higher growth rate over the first 24 hours post-exposure than non-phage controls (Kruskal-Wallis, df=3, P = 0.006; Figure 1 ).","Modify,Grammar",Grammar
6032,1-21,1-21_v2_10@3,1-21_v1_10@3,"During the fourth day post-exposure, control and treatment bacteria showed no significant differences in doubling time (KW, df = 3, P > 0.05).","During the fourth day post-exposure, control and treatment bacteria showed no significant differences in doubling time (KW, df=3, P > 0.05).","Modify,Grammar",Grammar
6033,1-21,1-21_v2_10@5,1-21_v1_10@5,"There was a marginally significant effect on population growth for bacteria exposed to different phage concentrations (KW, df = 2, P < 0.02), suggesting that the encounter rate between bacteria and phage is important in determining the population-level strength of the fission response.","There was a marginally significant effect on population growth for bacteria exposed to different phage concentrations (KW, df=2, P < 0.02), suggesting that the encounter rate between bacteria and phage is important in determining the population-level strength of the fission response.","Modify,Grammar",Grammar
6034,1-21,1-21_v2_13@0,1-21_v1_13@0,"We hypothesized that faster doubling times would come at a cost to cell size, since cells would have less time to metabolize and convert absorbed nutrients into cell structure twenty-four hours post-exposure, we found that phage-treated bacteria were two to three times smaller (as measured by mean cellular width) than the control (KW, df = 3, P < 0.0001; Figure 2 ).","We hypothesized that faster doubling times would come at a cost to cell size, since cells would have less time to metabolize and convert absorbed nutrients into cell structure Twenty-four hours post-exposure, we found that phage-treated bacteria were two to three times smaller (as measured by mean cellular width) than the control (KW, df=3, P < 0.0001; Figure 2 ).","Modify,Grammar",Grammar
6035,1-21,1-21_v2_13@2,1-21_v1_13@2,Analyses of the distribution of several flow cytometry profiles showed that a difference in cell shape is unlikely to explain this result (see Data File below).,Analyses of the distribution of several flow cytometry profiles showed that a difference in cell shape is unlikely to explain this result (see data associated to this article).,"Modify,Clarity",Clarity
6036,1-21,1-21_v2_13@4,1-21_v1_13@3,"This implies that bacterial shape remained unchanged throughout the experiment, and indeed, additional observations using a transmission electron microscope showed that the cells remained rod-shaped for all treatments.","Finally, observations using a transmission electron microscope showed that the cells remained rod-shaped for all treatments.","Modify,Claim",Claim
6048,1-23,1-23_v2_23@0,1-23_v1_24@0,Conclusions,Final remarks,"Modify,Other",Other
6049,1-23,1-23_v2_17@8,1-23_v1_17@8,Claims for increased Cesarean deliveries in cut women were attributed to obstructed labor most likely due to excessive scarring at the pelvic outlet probably resulting from the imperfect healing of the genital cutting and possible associated infection.,Claims for increased Cesarean deliveries in cut women were attributed to obstructed labor most likely due to excessive scarring at the pelvic outlet.,"Modify,Claim",Claim
6050,1-23,1-23_v2_17@9,1-23_v1_17@9,"However, the high Cesarean rate in this population cannot be attributed solely to obstruction due to excessive outlet scarring; obstructed labor may occur due to a variety of reasons.",The high Cesarean rate in this population cannot be attributed solely to obstruction due to excessive outlet scarring; obstructed labor may occur due to a variety of reasons.,"Modify,Clarity",Clarity
6103,1-62,1-62_v2_11@5,1-62_v1_17@0,"Further, the DNA sequencing of the ATP F o subunit of the selected isolate was performed via primer walking.","Further, the DNA sequencing of the ATP F o subunit of selected isolates was performed by primer walking method.","Modify,Clarity",Clarity
6104,1-62,1-62_v2_15@0,1-62_v1_19@0,Bacterial identification and ATP synthase F o subunit amplification,Bacterial identification,"Modify,Other",Other
6105,1-62,1-62_v2_8@3,1-62_v1_20@0,"Across the pH gradient, morphologically different pink, orange and white bacterial colonies were observed.","Across the pH gradient, growth of morphologically different bacterial colonies was observed.","Modify,Fact/Evidence",Fact/Evidence
6106,1-62,1-62_v2_16@0,1-62_v1_20@2,The orange pigmented bacterium was identified as Stenotrophomonas sp. based on a NCBI BLAST analysis of the 16S rRNA gene sequence and titled Stenotrophomonas sp. DL18 (GenBank Accession number: JN995612 ).,The orange pigmented bacterium was identified as Stenotrophomonas species based on BLAST analysis of 16S rRNA gene sequence and titled as Stenotrophomonas species DL18 (GenBank Accession number: JN995612 ).,"Modify,Fact/Evidence",Fact/Evidence
6109,1-62,1-62_v2_4@0,1-62_v1_4@0,"ATP is molecular currency for a living cell, which is not only growing and dividing but also continuously responding to external environmental stimuli.","ATP is the molecular currency for a living cell, which is not only merely growing and dividing but also continuously responding to external environmental stimuli.","Modify,Clarity",Clarity
6110,1-62,1-62_v2_18@0,1-62_v1_25@0,Comparative studies of ATP synthase a -subunit of Stenotrophomonas species DL18,ATP synthase a -subunit of Stenotrophomonas species DL18,"Modify,Other",Other
6111,1-62,1-62_v2_17@0,1-62_v1_26@0,A BLAST analysis of the ATP synthase a -subunit of the Stenotrophomonas species DL18 suggests maximum identity with Stenotrophomonas species SKA14 (GenBank Accession number: ZP_05136035 ) at the amino acid level (259 amino acid residues were identical from a total of 266 amino acid residues in the SKA14 species).,BLAST analysis of the ATP synthase a -subunit of the Stenotrophomonas species DL 18 suggests maximum identity at the amino acid level (259 identical amino acids from a total of 266 amino acids of Stenotrophomonas species SKA14; GenBank Accession number: ZP_05136035 ).,"Modify,Clarity",Clarity
6112,1-62,1-62_v2_19@1,1-62_v1_26@1,"The amino acid residue arginine, which was found to be conserved in almost all bacterial species in the a -subunit, was observed at position 200 (Arg 200 ) in DL18.","The amino acid residue arginine, which was found to be conserved in almost all bacterial species in the a -subunit, was observed at position 200 (Arg 200 ) (Supplementary figure 3) .","Modify,Fact/Evidence",Fact/Evidence
6113,1-62,1-62_v2_19@2,1-62_v1_26@2,"Moreover, other amino acid residues that were conserved in most of the bacteria include Leu 207 , Arg 210 , Leu 211 , Gly 213 , Asn 214 , Gly 218 , Gln 252 , Ala 253 and Phe 255 ( E. coli numbering system for ATP synthase a -subunit) in the trans-membrane helix-4 ( a TMH-4), a TMH-5 and the corresponding amino acids were also found in the DL18 alignment ( Figure 1 and Figure 2 ).","Moreover, other amino acids that were conserved in most of the bacteria include Leu 207 , Arg 210 , Leu 211 , Gly 213 , Asn 214 , Gly 218 , Gln 252 , Ala 253 , Phe 255 ( E. coli numbering system for ATP synthase a -subunit) in TMH-4 and TMH-5 and the corresponding amino acids were also found in the Stenotrophomonas species DL 18 in alignment ( Figure 1 , Figure 2 ).","Modify,Clarity",Clarity
6114,1-62,1-62_v2_4@1,1-62_v1_4@1,"To survive in extreme conditions, microorganisms devise specific adaptive mechanisms.","To sustain life in extreme conditions, microorganisms devise specific mechanisms for their adaptation.","Modify,Clarity",Clarity
6115,1-62,1-62_v2_4@2,1-62_v1_4@2,"Along with other transporter proteins, ATP synthase is widely considered one of the key molecules for adaptation at alkaline conditions.","Along with other transporter proteins, ATP synthase is widely considered as one of the key molecules for adaptation at alkaline conditions.","Modify,Grammar",Grammar
6116,1-62,1-62_v2_4@3,1-62_v1_5@0,"Hydrolysis of nucleoside triphosphates, specifically ATP, provides the chemical energy to drive a wide variety of cellular reactions.","Hydrolysis of nucleoside tri-phosphates, specifically ATP, provides the chemical energy to drive a wide variety of cellular reactions.","Modify,Grammar",Grammar
6117,1-62,1-62_v2_24@0,1-62_v1_34@0,It was observed that the most conserved arginine residue of the a -subunit (Arg 200 of Stenotrophomonas sp. DL18) was aligned with the expected position of the facultative alkaliphile Bacillus pseudofirmus OF4 (i.e. Arg 172 ).,It was observed that the most conserved arginine residue of the a -subunit (Arg 200 of Stenotrophomonas species DL18) was aligned with the expected position of the facultative alkaliphile Bacillus pseudofirmus OF4 (i.e. Arg 172 ; GenBank Accession number: YP_003426326 ).,"Modify,Fact/Evidence",Fact/Evidence
6118,1-62,1-62_v2_24@2,1-62_v1_34@2,Lys 180 in B. pseudofirmus OF4 was replaced by Gly 208 in the DL18 strain.,Amino acid Lys 180 of Bacillus pseudofirmus OF4 was replaced by Gly 208 in the Stenotrophomonas species DL 18.,"Modify,Clarity",Clarity
6119,1-62,1-62_v2_24@3,1-62_v1_34@3,"In addition, glycine and alanine residues were observed at the same position in other alkaliphiles including Gly 206 in Thioalkalimicrobium cyclicum ALM1, and Ala 230 in Theoalkalivibrio sp. K90mix, as shown in Figure 1 .","In addition, a glycine residue was observed at the same position in other alkaliphiles, T. cyclicum (Gly 206 ; GenBank Accession number: YP_004537849 ), and same amino acid family Ala 230 in Theoalkalivibrio species K90mix (GenBank Accession number: YP_003461818 ) as shown in Figure 2 .","Modify,Fact/Evidence",Fact/Evidence
6120,1-62,1-62_v2_24@5,1-62_v1_34@6,"As reported by Ivey DM et al. , Lys 180 and corresponding amino acids were located in a TMH-4 in Bacillus pseudofirmus OF4 and other alkaliphiles <REF-12> , <REF-13> .","Lys 180 and corresponding amino acids were located in transmembrane helix-4 ( a TMH-4) in Bacillus pseudofirmus OF4 and other alkaliphiles <REF-23> , <REF-24> .","Modify,Fact/Evidence",Fact/Evidence
6121,1-62,1-62_v2_24@6,1-62_v1_34@7,"In the DL18 strain, a histidine residue, which is conserved in other reference species of the same genus (e.g. Stenotrophomonas species K279a, Stenotrophomonas sp. SKA14 and other alkaliphiles including T. cyclicum ALM1 (His 244 ), and Theoalkalivibrio sp. K90mix (His 262 ), was present at position 240 (His 240 ) ( Figure 1 ).","In the Stenotrophomonas species DL 18, a histidine residue, which is conserved in other reference species of the same genus ( S. maltophilia K279a GenBank Accession number: YP_001973793 ; and S . sp. SKA14) and other alkaliphiles T. cyclicum (His244), and Theoalkalivibrio species K90mix (His 262 ), was present at position 240 (His 240 ) (Figure 2) .","Modify,Fact/Evidence",Fact/Evidence
6122,1-62,1-62_v2_4@4,1-62_v1_5@1,ATP synthesis is central to ATP production during oxidative phosphorylation.,ATP synthases are central to ATP production during oxidative phosphorylation.,"Modify,Grammar",Grammar
6123,1-62,1-62_v2_26@0,1-62_v1_34@8,"E. coli K12 DH10B considered a neutrophile, can adapt to slightly alkaline conditions up to pH 8.0 and this may be due to the presence of His 245 .","However, E. coli K12 DH10B (GenBank Accession number: YP_001732559 ), considered as neutrophile, can adapt to slightly alkaline conditions i.e. up to pH 8.0 and this may be due to the presence of His 245 .","Modify,Fact/Evidence",Fact/Evidence
6124,1-62,1-62_v2_25@0,1-62_v1_34@9,It has been proposed that Gly 120 and Lys 180 forms a channel for the proton uptake pathway of the a -subunit through which protons pass onto the neighboring c -subunit in B. pseudofirmus OF4 <REF-7> .,It was proposed that Gly 120 and Lys 180 form a channel residing within the proton uptake pathway of the a -subunit through which protons pass onto the neighboring c -subunit in Bacillus pseudofirmus OF4 <REF-18> .,"Modify,Clarity",Clarity
6125,1-62,1-62_v2_2@0,1-62_v1_2@0,"Lonar Lake, an Indian soda lake with high alkaline conditions of pH 10.5, is well known for its biodiversity of extremophiles including alkaliphiles.","Lonar Lake, an Indian Soda Lake, is well known for its biodiversity of extremophiles including alkaliphiles.","Modify,Fact/Evidence",Fact/Evidence
6126,1-62,1-62_v2_4@6,1-62_v1_5@3,"The F o integral membrane protein complex provides a transmembrane pore for protons, whereas the peripheral protein F <REF-1> is involved in catalysis <REF-1> .","The F o integral membrane protein complex (the subscript ‘o’ denotes its inhibition by the drug oligomycin) provides a transmembrane pore for protons, whereas the peripheral protein F <REF-1> (the subscript ‘1’ indicates that it was the first of several factors isolated from mitochondria) is involved in catalysis <REF-1> .","Modify,Fact/Evidence",Fact/Evidence
6127,1-62,1-62_v2_25@1,1-62_v1_37@3,"However, one study has reported that His 245 along with the Gly 218 and Glu 219 positioning plays a critical role in F o ion translocations in E. coli <REF-8> , and in the present study on Stenotrophomonas sp. DL18, His 240 , Gly 208 and Glu 209 were observed at the same corresponding positions as shown in Figure 1 .","However, one of the studies reported the His 245 along with the Glu 219 positioning plays a critical role in ion translocation in E. coli <REF-25> , while in Stenotrophomonas species DL 18 these were at His 240 and Glu 209 as shown in Figure 2 .","Modify,Fact/Evidence",Fact/Evidence
6128,1-62,1-62_v2_25@2,1-62_v1_38@0,McMillan <REF-14> et al demonstrated the importance of residues with a basic side chain along with its pKa value in ATP synthesis in alkaline environments.,McMillan et al <REF-25> showed the importance of the residue with a basic side chain along with its pKa value in ATP synthesis linked with alkaline environment.,"Modify,Clarity",Clarity
6129,1-62,1-62_v2_25@3,1-62_v1_38@1,"However, that mutation study was carried out in the Bacillus sp. TA2.A1, specifically involving the 180 th residue in the a -subunit.","However, these studies were carried out with a mutation study in the Bacillus pseudofirmus OF4 a -subunit, specifically with the position of the residue at 180.","Modify,Fact/Evidence",Fact/Evidence
6130,1-62,1-62_v2_25@4,1-62_v1_38@2,"In that amino acid substitution study, amino acid residue Lys 180 in the a -subunit was mutated to Gly 180 , His 180 and Arg 180 .","These studies included mutations at Lys 180 position as Gly 180 , His 180 and Arg 180 .","Modify,Fact/Evidence",Fact/Evidence
6131,1-62,1-62_v2_26@3,1-62_v1_38@5,"Hence, these studies in E. coli signified that the positions Gly 218 and His 245 along with Glu 219 had a critical interaction with the F o subunit function.","Similar studies in E. coli showed that the positions Gly 218 and His 245 along with G1u 219 had a critical interaction with F o function <REF-26> , <REF-27> .","Modify,Fact/Evidence",Fact/Evidence
6132,1-62,1-62_v2_27@0,1-62_v1_39@1,"From alignment, Glu 209 of the DL18 strain was conserved in alkaliphiles, neutrophiles and some acidophiles except His 186 in Acidiphilium cryptum ( Figure 1 ).","From alignment, Glu 209 of the Stenotrophomonas species DL 18 was conserved in alkaliphiles, neutrophiles and some acidophiles except His 186 Acidiphilium cryptum .","Modify,Fact/Evidence",Fact/Evidence
6133,1-62,1-62_v2_27@1,1-62_v1_39@2,"In addition, the position of Gly 212 in B. pseudofirmus OF4 corresponds to Ser 212 in Enterococcus hirae ATCC 9790 (neutrophile), His 245 in E. coli and His 240 in the Stenotrophomonas sp. DL18 while glutamate in acidophiles is positioned at Glu 222 in A. ferrooxidans and Glu 225 in A. cryptum as shown in Figure 1 .","In addition, the position of Gly 212 in Bacillus pseudofirmus OF4 corresponds to Ser 212 in E. hirae (neutrophile; GenBank Accession number: YP_006487510 ), His 245 in E. coli and His 240 in the Stenotrophomonas species DL 18 while glutamate in acidophiles, Glu 222 in A. ferrooxidans and Glu 225 in A. cryptum as shown in Figure 3 .","Modify,Fact/Evidence",Fact/Evidence
6134,1-62,1-62_v2_27@2,1-62_v1_39@3,"This showed that channel formation may involve a glycine residue along with other residues, specifically those with acidic, basic and neutral side chains, which play vital roles in ATP synthesis in acidophile, alkaliphiles and neutrophiles.","This showed that the channel formation may involve a glycine residue along with other residues, specifically acidic, basic and neutral side chain, which play vital roles in ATP synthesis in acidophile, alkaliphiles and neutrophiles.","Modify,Clarity",Clarity
6135,1-62,1-62_v2_27@3,1-62_v1_39@4,"Hence, these residues are found to be critical in channel formation <REF-7> – <REF-9> .","Hence, these residues are found to be critical in channel formation.","Modify,Fact/Evidence",Fact/Evidence
6136,1-62,1-62_v2_27@5,1-62_v1_39@5,Gly 208 and His 240 may form the proton translocation channel.,"In the same scenario, the Gly 208 and His 240 may be form the proton translocation channel in Stenotrophomonas species DL 18.","Modify,Clarity",Clarity
6137,1-62,1-62_v2_4@7,1-62_v1_5@4,"F <REF-1> consists of five subunits, α <REF-3> β <REF-3> γ <REF-1> δ <REF-1> ε <REF-1> , with a ring of α- and β-subunits alternating around a single γ-subunit <REF-2> .","F <REF-1> consists of five subunits α <REF-3> β <REF-3> γ <REF-1> δ <REF-1> ε <REF-1> , with a ring of α- and β-subunits alternating around a single γ-subunit <REF-2> .","Modify,Grammar",Grammar
6138,1-62,1-62_v2_4@9,1-62_v1_5@6,"Out of these subunits, a -subunit is a stator and c -ring is a rotor ring through which ions (H + or Na + ) are translocated <REF-3> – <REF-6> .","Out of these subunits, the a -subunit is a stator and c-ring is a rotor ring through which ions (H + or Na + ) are translocated <REF-3> – <REF-6> .","Modify,Grammar",Grammar
6139,1-62,1-62_v2_4@10,1-62_v1_5@7,Each c -chain from the ring consists of two α-helices traversing the membrane and the polar loop extends out of the membrane to interact with the γ-and ε-subunits.,"Each c -chain from the ring consists of two α-helices traversing the membrane, and the polar loop extends out of the membrane to interact with the γ-and ε-subunits.","Modify,Grammar",Grammar
6140,1-62,1-62_v2_4@11,1-62_v1_5@8,A cytoplasmic F <REF-1> catalytic domain is connected by a membrane-embedded F o domain by a central (γε) and peripheral (b <REF-2> δ) stalk <REF-2> – <REF-6> .,A cytoplasmic F <REF-1> catalytic domain is connected with a membrane-embedded F o domain by a central (γε) and peripheral (b <REF-2> δ) stalk <REF-2> – <REF-6> .,"Modify,Grammar",Grammar
6141,1-62,1-62_v2_4@12,1-62_v1_6@0,"In general, downhill ion translocation across the membrane through F o causes rotation of the c -ring, which induces conformational changes in the catalytic β-subunit and results in ATP synthesis.","Downhill ion translocation across the membrane through F o causes rotation of the c-ring, which induces conformational changes in the catalytic β-subunit and results in ATP synthesis.","Modify,Clarity",Clarity
6142,1-62,1-62_v2_2@1,1-62_v1_2@1,Most of the molecular studies on Lonar Lake alkaliphiles are based on identification by 16S ribosomal RNA (16S rRNA).,Most of the molecular studies on Lonar Lake alkaliphiles are based on molecular identification by 16S ribosomal RNA along with numerous applications in the biotechnology industry.,"Modify,Fact/Evidence",Fact/Evidence
6143,1-62,1-62_v2_4@13,1-62_v1_6@8,"However, in alkaline conditions, the external pH is high i.e. above pH 8.0, which poses a major thermodynamic problem for ATP synthesis.","However, alkaliphiles grow at high environmental pH, which poses the thermodynamic problem of synthesizing ATP with ATP synthase.","Modify,Fact/Evidence",Fact/Evidence
6144,1-62,1-62_v2_2@3,1-62_v1_2@2,"As the data on the alkaliphilic nature of bacteria from Lonar Lake is incompletely understood, the present report comprised of isolation and identification of alkaliphiles from Lonar Lake.","However, molecular basis of adaptation of these alkaliphiles to high alkaline conditions is incompletely understood.","Modify,Fact/Evidence",Fact/Evidence
6145,1-62,1-62_v2_5@0,1-62_v1_8@0,Most studies have focused on the proton translocation channel in the a -subunit of ATP synthase <REF-7> – <REF-9> .,Most studies have focused on the proton translocation channel in the a -subunit of ATP synthase.,"Modify,Fact/Evidence",Fact/Evidence
6146,1-62,1-62_v2_5@1,1-62_v1_8@3,"However, similar experimental evidence from other geographic locations such as highly alkaline soda lakes needs further exploration to understand pH homeostasis in facultative alkaliphiles.","However, similar experimental evidence from other geographic locations such as highly alkaline soda lakes need further exploration to understand pH homeostasis in facultative alkaliphiles.","Modify,Grammar",Grammar
6147,1-62,1-62_v2_5@2,1-62_v1_8@4,This study explores the ATP synthase a -subunit of a facultative alkaliphilic aerobe isolated from Lonar Lake.,This study explores the comparison of the ATP synthase a -subunit of facultative alkaliphilic aerobes isolated from Lonar Lake with established and reported alkaliphiles.,"Modify,Fact/Evidence",Fact/Evidence
6148,1-62,1-62_v2_6@0,1-62_v1_9@0,Methods,Materials and methods,"Modify,Other",Other
6149,1-62,1-62_v2_7@0,1-62_v1_10@0,Isolation and identification of bacteria from the soda lake,Isolation and culture of bacteria,"Modify,Other",Other
6150,1-62,1-62_v2_8@0,1-62_v1_11@0,"In the present study, underwater sediment soil samples were collected 350 meters away from the Kamalaja Devi Temple end of Lonar Lake, Buldhana, Maharashtra, India.","Underwater sediment soil samples were collected from 350 meters away from Kamalaja Devi Temple end, Lonar Lake, Buldhana, Maharashtra, India.","Modify,Clarity",Clarity
6151,1-62,1-62_v2_8@4,1-62_v1_13@0,The bacterial genomic DNA of optimally grown bacteria at pH 9.5 with orange pigmentation was isolated by the DNAzol method <REF-10> .,Bacterial genomic DNA was isolated by the DNAzol method <REF-19> .,"Modify,Fact/Evidence",Fact/Evidence
6152,1-62,1-62_v2_2@4,1-62_v1_2@4,"Further, we studied the F1FoATP synthase a- subunit, with reference to alkaliphile specific domains, of one of the facultative alkaliphiles, Stenotrophomonas sp. DL18.","One facultative alkaliphile, Stenotrophomonas species DL18, was studied for F 1 F o ATP synthase a-subunit with reference to alkaliphile-specific domains.","Modify,Clarity",Clarity
6153,1-62,1-62_v2_11@1,1-62_v1_14@1,"Briefly, a PCR reaction mixture of 100 µl was prepared: 10x PCR buffer, 50 mM MgCl 2 , 10 mM dNTP mix, 100 picomoles each of forward and reverse primers, 5 units of Taq DNA polymerase (Chromous Biotech, India) with Pfu and 200 ng of bacterial genomic DNA template in nuclease free water.","Briefly, PCR reaction mixture of 100 µl was prepared as 10x PCR Rxn Buffer (Invitrogen), 50mM MgCl 2 (Invitrogen) 1.8 µl, 10 mM dNTP mix 3.0 µl (Merck), 100 picomoles of each forward and reverse primers (Integrated DNA technologies, USA), 5U Taq DNA polymerase (Invitrogen) with pfu (Chromous biotech, India) and bacterial genomic DNA templates 200 ng, with remaining nuclease free water (Merck, India).","Modify,Fact/Evidence",Fact/Evidence
6154,1-62,1-62_v2_11@3,1-62_v1_14@3,A final extension was carried out at 72°C for 5 min.,Final extension was carried out at 72°C for 5 min and stored at 4°C.,"Modify,Fact/Evidence",Fact/Evidence
6155,1-62,1-62_v2_10@0,1-62_v1_15@0,PCR and DNA sequencing of ATP synthase a -subunit of Stenotrophomonas species DL18,DNA sequencing and analysis,"Modify,Other",Other
6159,10-25,10-25_v2_3@1,10-25_v1_3@1,The diets were given to juvenile silver pompano (with average body weight of 8.56 ± 0.18 g) and stocked in 15 similar 20-L plastic jars with 10 fish per jar in a density of 100 L capacity container.,The diets were given to juvenile silver pompano (with average body weight of 8.56 ± 0.18 g) and stocked with 10 fish in a 100 L capacity container.,"Modify,Fact/Evidence",Fact/Evidence
6160,10-25,10-25_v2_24@0,10-25_v1_24@0,"The juvenile silver pompano were purchased from Batam Marine Aquaculture Center (BPBL) located in Setoko islands, Batam city were placed and acclimatised in 400- L plastic tanks for a week.","Ten juvenile silver pompano were purchased from Batam Marine Aquaculture Center (BPBL) located in Setoko islands, Batam city with mean weight of 8.56±0.18 g were placed in 100 L plastic tanks.","Modify,Fact/Evidence",Fact/Evidence
6161,10-25,10-25_v2_58@2,10-25_v1_58@2,Chor et al. <REF-24> also stated that the lipase and protease activity of omnivorous fish was higher than amylase.,Chor et al. <REF-24> also stated that the lipase and protease activity of omnivorous fish including star pomfret was higher than amylase.,"Modify,Fact/Evidence",Fact/Evidence
6162,10-25,10-25_v2_68@0,10-25_v1_68@0,"The use of FFM in feed resulted in increased activity of digestive enzymes (protease, lipase) in silver pompano.","The use of FFM in feed resulted in increased activity of digestive enzymes (protease, lipase, and amylase) in silver pompano.","Modify,Fact/Evidence",Fact/Evidence
6163,10-25,10-25_v2_2@3,10-25_v1_2@3,This study aimed to determine the digestibility of fermented feather meal (FFM) in silver pompano ( Trachinotus blochii ) diets and to observe the histological structure of their intestines after digestion.,This study aimed to determine the digestibility of fermented feather meal (FFM) in silver pompano diets and to observe the histological structure of their intestines after digestion.,"Modify,Clarity",Clarity
6166,10-77,10-77_v2_39@0,10-77_v1_39@0,"The population density of A. fulica reported in this study was between 0.0019–0.6818 ind/m 2 , these density values are similar to those obtained by De la Ossa et al .","The population density of A. fulica reported in this study was between 0.0019–0.6818 ind/m <REF-2> , these density values are similar to those obtained by De la Ossa et al .","Modify,Fact/Evidence",Fact/Evidence
6167,10-77,10-77_v2_39@2,10-77_v1_39@2,"Other similar values were reported in a study performed in Ilha Porchat, Brazil, obtaining a density of 0.07 ind/m 2 <REF-3> .","Other similar values were reported in a study performed in Ilha Porchat, Brazil, obtaining a density of 0.07 ind/m <REF-2> <REF-3> .","Modify,Fact/Evidence",Fact/Evidence
6168,10-77,10-77_v2_39@3,10-77_v1_39@3,"Investigations such as those carried out in Havana, Cuba, showed a considerably lower density of snails (0.00015 ind/m 2 ) <REF-26> .","Investigations such as those carried out in Havana, Cuba, showed a considerably lower density of snails (0.00015 ind/m <REF-2> ) <REF-25> .","Modify,Fact/Evidence",Fact/Evidence
6169,10-77,10-77_v2_39@4,10-77_v1_39@4,"In contrast, densities of 1.1–4.6 ind/m 2 have been reported in different departments of Colombia <REF-27> , 0.06–8 ind/m 2 in Northeast Brazil <REF-28> , and an average of 8.4 ind/m 2 in Puyo, Ecuador <REF-22> .","In contrast, densities of 1.1–4.6 ind/m <REF-2> have been reported in different departments of Colombia <REF-26> , 0.06–8 ind/m <REF-2> in Northeast Brazil <REF-27> , and an average of 8.4 ind/m <REF-2> in Puyo, Ecuador <REF-21> .","Modify,Fact/Evidence",Fact/Evidence
6170,10-77,10-77_v2_39@5,10-77_v1_39@5,"On the other hand, studies performed in Puerto Iguazú y Corrientes, Argentina, recorded a much higher average density, which reaches 107.6 ind/m 2 and 118.6 ind/m 2 , respectively <REF-15> , <REF-29> .","On the other hand, studies performed in Puerto Iguazú y Corrientes, Argentina, recorded a much higher average density, which reaches 107.6 ind/m <REF-2> and 118.6 ind/m <REF-2> , respectively <REF-14> , <REF-28> .","Modify,Fact/Evidence",Fact/Evidence
6171,10-77,10-77_v2_39@6,10-77_v1_39@6,"It has been mentioned that the areas highly affected by the giant African snail present densities of 10 ind/m 2 or more <REF-12> , <REF-30> ; this similarly occurs with regards to biomass, where devastating values of up to 780 kg/ha are estimated for areas which are highly affected <REF-12> , <REF-31> .","It has been mentioned that the areas highly affected by the giant African snail present densities of 10 ind/m <REF-2> or more <REF-11> , <REF-29> ; this similarly occurs with regards to biomass, where devastating values of up to 780 kg/ha are estimated for areas which are highly affected <REF-11> , <REF-30> .","Modify,Fact/Evidence",Fact/Evidence
6172,10-77,10-77_v2_9@0,10-77_v1_9@0,"In Colombia, the presence of A. fulica was registered for the first time in 2009 <REF-11> , since then the mollusk has been distributed in more than 20 departments, registering in all regions of the country <REF-12> .","In Colombia, the presence of A. fulica was registered for the first time in 2010, since then the mollusk has been distributed in more than 20 departments, registering in all regions of the country <REF-11> .","Modify,Fact/Evidence",Fact/Evidence
6173,10-77,10-77_v2_14@1,10-77_v1_14@1,The sampling consisted of visiting the zones within the neighborhoods of the city Cartagena previously informed by citizens affected by the presence of the snail in their homes and surroundings.,The sampling consisted of visiting the sites within the city of Cartagena previously informed by citizens affected by the presence of the snail in their homes and surroundings.,"Modify,Clarity",Clarity
6174,10-77,10-77_v2_14@5,10-77_v1_14@4,"As this was a monitoring study, all the snails available during collections were included for analyzes, no exclusion criteria were applied for the collection of the snails.","As this was a monitoring study, all the snails available in the study period were included for analyzes, no exclusion criteria were applied for the collection of the snails.","Modify,Clarity",Clarity
6175,10-77,10-77_v2_24@0,10-77_v1_24@0,"During the period studied 204 snails were collected, distributed in four zones of four Cartagena neighborhoods, for which the presence of A. fulica was indicated by the community ( Figure 1 ).","During the period studied 204 snails were collected, distributed in four sites in Cartagena for which the presence of A. fulica was indicated by the community ( Figure 1 ).","Modify,Clarity",Clarity
6176,10-77,10-77_v2_24@1,10-77_v1_24@1,"The inspected sites where the snails were collected corresponded principally to extensive gardens and other spaces inside of private properties and residential condominiums (Manga, Las Gavias, and Serena del Mar).","The inspected sites where the snails were collected corresponded essentially to the gardens of private properties and residential condominiums (Manga, Las Gavias, and Serena del Mar).","Modify,Clarity",Clarity
6177,10-77,10-77_v2_24@2,10-77_v1_24@2,"At these sites the snails were found in different substrates, such as plants, tree roots, grasses, under litter, flowerpots, and ornamental plants; In Zaragocilla, snails were found aggregated in rubble inside an educational institute.","At these sites the snails were found in different substrates, such as plants, tree roots, grasses, under litter, flowerpots and ornamental plants; In Zaragocilla, aggregates were found in the rubble.","Modify,Clarity",Clarity
6265,2-147,2-147_v2_26@1,2-147_v1_26@1,"Cartilage discs were digested in papain (Sigma-Aldrich, Gillingham, UK) for 16 hours.","Cartilage discs were digested in papain(Sigma-Aldrich, Gillingham, UK) for 16 hours.","Modify,Grammar",Grammar
6266,2-147,2-147_v2_30@6,2-147_v1_30@6,Densitometric quantification of MMP-3 bands was performed using ImageJ software.,Densitometric quantification of MMP-3 bands was performed using Image J software.,"Modify,Grammar",Grammar
6267,2-147,2-147_v2_66@0,2-147_v1_66@0,"In summary, although this study found that curcumin at concentrations of 25μM and above is cytotoxic to monolayer chondrocytes after five days in culture, lower concentrations effectively antagonize PG and PGE 2 release in vitro and exert a potent anti-inflammatory effect on cartilage explants treated with IL-1β.","In summary, although this study found that curcumin at concentrations of 25μM and above is cytotoxic to monolayer chondrocytes after five days in culture, lower concentrations effectively antagonize PG and PGE2 release in vitro and exert a potent anti-inflammatory effect on cartilage explants treated with IL-1β.","Modify,Grammar",Grammar
6268,2-147,2-147_v2_11@2,2-147_v1_11@1,The joint tissues were sourced from UK-based abattoirs and veterinary practices.,The joint tissues were sourced from two UK-based abattoirs.,"Modify,Fact/Evidence",Fact/Evidence
6269,2-147,2-147_v2_11@3,2-147_v1_11@2,Animals were euthanized for non-research purposes either in accordance with Welfare of Animals (Slaughter or Killing) Regulations 1995 or the Veterinary Surgeons Act with owner consent.,Animals were euthanized for non-research purposes having been stunned before slaughter for meat in accordance with Welfare of Animals (Slaughter or Killing) Regulations 1995 .,"Modify,Fact/Evidence",Fact/Evidence
6270,2-147,2-147_v2_17@6,2-147_v1_17@6,"Live and dead cells were counted with ImageJ Software (National Institutes of Health, Bethesda, MD) and the percentage of dead cells (expressed as a percentage of the total number of cells present) was calculated at 24 hours, 48 hours and five days for each treatment.","Live and dead cells were counted with Image J Software (National Institutes of Health, Bethesda, MD) and the percentage of dead cells (expressed as a percentage of the total number of cells present) was calculated at 24 hours, 48 hours and five days for each treatment.","Modify,Grammar",Grammar
6277,2-155,2-155_v2_29@0,2-155_v1_29@0,"Poultry growers use antibiotics both for therapeutic purposes and for growth promotion <REF-33> , <REF-34> .","Antibiotic use is widespread in the production of chicken both for therapeutic and non-therapeutic purposes (e.g., growth promotion).","Modify,Fact/Evidence",Fact/Evidence
6278,2-155,2-155_v2_29@3,2-155_v1_29@1,"The use of antibiotics in poultry production can select for antibiotic-resistant microorganisms including Salmonella , Campylobacter , Enterococcus , and extra-intestinal pathogenic E. coli <REF-38> .","The use of antibiotics in poultry production selects for antibiotic-resistant microorganisms including Salmonella , Campylobacter , Enterococcus , and extra-intestinal pathogenic E. coli <REF-33> .","Modify,Clarity",Clarity
6279,2-155,2-155_v2_35@2,2-155_v1_35@2,Our final sample size was limited (n=184) but not atypical for the field <REF-48> – <REF-51> .,Our sample size was limited (n=184) but not atypical for the field <REF-43> – <REF-46> .,"Modify,Clarity",Clarity
6280,2-155,2-155_v2_2@1,2-155_v1_2@1,"Consumers have a range of choices for poultry, including conventional, organic, kosher, and raised without antibiotics (RWA) – designations that are perceived to indicate differences in quality and safety.","Consumers have a range of choices for poultry, including conventional, organic, kosher, and raised without antibiotics (RWA)-designations that are perceived to indicate differences in quality and safety.","Modify,Grammar",Grammar
6281,2-155,2-155_v2_12@3,2-155_v1_12@3,"The plate was incubated at 37°C for 2 h and then at 44°C for 22 h, along with QA/QC strains ATCC E. coli 35218, Klebsiella pneumoniae , Hafnia alvei , Citrobacter freundii and Serratia plymuthica.","The plate was incubated at 37°C for 2 h and then at 44°C for 22 h, along with QA/QC strains ATCC E. coli 35218, Klebsiella pneumoniae , Hafnia alvei , Citrobacter freundii, and Serratia plymuthica.","Modify,Grammar",Grammar
6284,2-173,2-173_v2_62@2,2-173_v1_62@2,"As such, additional studies using animal models and relevant samples from human patients need to be evaluated in order to conclusively confirm or reject the hypothesis that CYP3A genes are regulated in human subjects, particularly asthmatics, in response to glucocorticoid treatment since current in vitro models remain unexplainably limited in value for such studies.","As such, additional studies using animal models and relevant samples from human patients need to be evaluated in order to conclusively confirm or reject the hypothesis that CYP3A genes are regulated in human lung cells in response to glucocorticoid treatment since current in vitro models remain unexplainably limited in value for such studies.","Modify,Claim",Claim
6285,2-173,2-173_v2_63@0,2-173_v1_63@0,"In summary, the data presented herein demonstrate that, in A549 cells, glucocorticoid binding to the glucocorticoid receptor regulates the expression of CYP3A5.","In summary, the data presented herein demonstrate that, in A549 cells, glucocorticoid binding to the glucocorticoid receptor regulates the expression of CYP3A5, and therefore, corroborates the hypothesis that increased metabolism of glucocorticoids may occur in some patients.","Modify,Claim",Claim
6286,2-173,2-173_v2_63@1,2-173_v1_63@1,"However, further research is needed to determine if changes in CYP3A5 expression occur in the human respiratory tissue similar to A549 cells, the precise mechanism by which this process occurs, and whether changes in the local metabolism of glucocorticoids by CYP3A5 ultimately impact glucocorticoid efficiency in asthma patients refractory to glucocorticoid treatment.","However, further research is needed to determine if changes in CYP3A5 expression occur in the human respiratory tissue similar to A549 cells, the precise mechanism by which this process occurs, and whether changes in the local metabolism of glucocorticoids by CYP3A5 ultimately impact glucocorticoid efficiency.","Modify,Claim",Claim
6287,2-173,2-173_v2_16@6,2-173_v1_16@6,All cells except A549 cells were plated in 12-well plates pre-coated with LHC basal medium (Life Technologies) and cultured in the presence of hydrocortisone.,All cells except A549 cells were plated in 12-well plates pre-coated with LHC basal medium (Life Technologies).,"Modify,Fact/Evidence",Fact/Evidence
6343,2-238,2-238_v2_22@1,2-238_v1_22@1,"Enzymatic assay was carried out in triplicates using the fluorogenic Class I HDAC substrate, Boc-Lys (Ac)-AMC (Cat. No. I1875, Bachem AG, Bubendorf, Switzerland).","Enzymatic assay was carried out using the fluorogenic Class I HDAC substrate, Boc-Lys (Ac)-AMC (Cat. No. I1875, Bachem AG, Bubendorf, Switzerland).","Modify,Fact/Evidence",Fact/Evidence
6344,2-238,2-238_v2_22@2,2-238_v1_22@2,"Briefly, 0.5 μg of purified recombinant protein in a volume of 10 μl of assay buffer was incubated with 50 μl of appropriate concentration of HDAC inhibitors and 20 μM of substrate at 37°C for 1 hr in 100 μl reaction volume.","Briefly, 0.5 μg of purified recombinant protein in a volume of 10 μl of assay buffer was incubated with 50 μl of appropriate concentration of HDAC inhibitors and 20 μM of substrate at 37°C for 1 hr.","Modify,Fact/Evidence",Fact/Evidence
6345,2-238,2-238_v2_24@0,2-238_v1_24@0,"Female BALB/c mice were purchased from The Jackson Laboratory (Bar Harbor, ME) at 4 weeks of age and then housed at the animal facility, Orchid Chemicals and Pharmaceuticals for 2 weeks in a specific-pathogen free facility with a 12 h light cycle (6 am–6 pm) and a 12 h dark cycle (6 pm–6 am).","Female BALB/c mice were purchased from The Jackson Laboratory (Bar Harbor, ME) at 4 weeks of age and then housed at the animal facility, Orchid chemicals and Pharmaceuticals for 2 weeks in a specific-pathogen free facility with a 12 h light cycle (6 am–6 pm) and a 12 h dark cycle (6 pm–6 am).","Modify,Grammar",Grammar
6346,2-238,2-238_v2_30@0,2-238_v1_30@0,"C. albicans ATCC 90028 mycelia (~5 gm wet weight) were washed with water, centrifuged at 10,000 rpm for 10 minutes at 4°C and the mycelial pellet was resuspended in 50 ml of 0.1 mM Tris-HCl, pH 9.4, 10 mM DTT.","C. albicans ATCC 90028 mycelia (~ 5 gm wet weight) were washed with water, centrifuged at 10,000 rpm for 10 minutes at 4°C and the mycelial pellet was resuspended in 50 ml of 0.1 mM Tris-HCl, pH 9.4, 10 mM DTT.","Modify,Grammar",Grammar
6347,2-238,2-238_v2_2@0,2-238_v1_2@0,"Candida albicans is a mucosal commensal organism capable of causing superficial (oral and vaginal thrush) infections in immune normal hosts, but is a major pathogen causing systemic and mucosal infections in immunocompromised individuals.","Candida albicans is a mucosal commensal organism in normal individuals, but is a major pathogen causing systemic and mucosal infections in immunocompromised individuals.","Modify,Claim",Claim
6348,2-238,2-238_v2_32@3,2-238_v1_32@3,"Deacetylation assays were carried out in 100 μl reaction volume for 1 hr at 37°C in reaction buffer (50 mM Tris Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 , 1 mg/ml BSA).","Deacetylation assays were carried out for 1 hr at 37°C in reaction buffer (50 mM Tris Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 , 1 mg/ml BSA).","Modify,Fact/Evidence",Fact/Evidence
6349,2-238,2-238_v2_34@2,2-238_v1_34@2,"Deacetylation assays were carried out in 100 μl reaction volume for 3 hr at 37°C in reaction buffer (50 mM Tris-Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 ).","Deacetylation assays were carried out for 3 hr at 37°C in reaction buffer (50 mM Tris-Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 ).","Modify,Fact/Evidence",Fact/Evidence
6350,2-238,2-238_v2_4@0,2-238_v1_4@0,Candida albicans is a commensal organism found in the mucosa and gastrointestinal tract of most healthy individuals but can cause superficial (oral and vaginal thrush) infections in immune-normal hosts and severe systemic infection in immunocompromised patients <REF-1> .,"Candida albicans is a commensal organism found in the mucosa and gastrointestinal tract of most healthy individuals but can cause severe systemic/superficial infections, especially in immunocompromised patients <REF-1> .","Modify,Fact/Evidence",Fact/Evidence
6351,2-238,2-238_v2_38@0,2-238_v1_38@0,"HeLa nuclear extract (2 μg) or recombinant Hos2 enzyme (300 ng) was incubated with NAD+ (500 μM), Fluor de Lys ® −Sirt1 (Enzo lifescience), varying concentrations of resveratrol (Cat. No. 0219605205, MP Biomedicals, Ohio, USA) (30, 100, 250 and 500 μM) in presence or absence of the pan-HDAC inhibitor trichostatin, in 50 μl reaction volume at 37°C for 30 min.","HeLa nuclear extract (2 μg) or recombinant Hos2 enzyme (300 ng) was incubated with NAD+ (500 μM), Fluor de Lys ® −Sirt1 (Enzo lifescience), varying concentrations of resveratrol (Cat. No. 0219605205, MP Biomedicals, Ohio, USA) (30, 100, 250 and 500 μM) in presence or absence of the pan-HDAC inhibitor trichostatin, at 37°C for 30 min.","Modify,Fact/Evidence",Fact/Evidence
6352,2-238,2-238_v2_38@1,2-238_v1_38@1,The reaction was carried out in triplicate following manufacturer’s protocol (Enzo lifescience).,The reaction was carried out following manufacturer’s protocol (Enzo lifescience).,"Modify,Fact/Evidence",Fact/Evidence
6353,2-238,2-238_v2_45@0,2-238_v1_43@0,"The Hos2 enzyme was expressed in the baculoviral-insect cell expression system as a NH 2 -terminal hexa histidine tagged fusion protein, which is detected on Western blot as a ~52 kDa protein using our own polyclonal anti-Hos2 anti-sera raised against Hos2 protein in mice.","The Hos2 enzyme was expressed in the baculoviral-insect cell expression system as a NH 2 -terminal hexa histidine tagged fusion protein, which is detected on Western blot as a ~ 52 kDa protein using our own polyclonal anti-Hos2 anti-sera raised against Hos2 protein in mice.","Modify,Grammar",Grammar
6354,2-238,2-238_v2_45@1,2-238_v1_43@1,SDS-PAGE analysis of purified protein revealed a major band at ~52 kDa ( Figure 1A ).,SDS-PAGE analysis of purified protein revealed a major band at ~ 52 kDa ( Figure 1A ).,"Modify,Grammar",Grammar
6355,2-238,2-238_v2_49@1,2-238_v1_47@1,The total activity with Boc-Lys (ac) AMC showed the enzyme to be active in deacetylating the lysine residue and the activity increased significantly ( P < 0.05 ) with an increase in Hos2 concentration ( Figure 1B ).,The total activity with Boc-Lys (ac) AMC showed the enzyme to be active in deacetylating the lysine residue and the activity increased with an increase in Hos2 concentration ( Figure 1B ).,"Modify,Fact/Evidence",Fact/Evidence
6356,2-238,2-238_v2_5@0,2-238_v1_5@0,"Azole resistance in Candida sp. is mediated by up regulation of genes encoding ERG11 , a lanosterol demethylase <REF-4> – <REF-6> , MDR1 <REF-5> , <REF-7> and by CDR (Candida drug resistance) efflux pumps <REF-5> , <REF-7> – <REF-9> .","Azole resistance in Candida sp. is mediated by up regulation of genes encoding ERG11, a lanosterol demethylase <REF-4> – <REF-6> , MDR1 <REF-5> , <REF-7> and by CDR (Candida drug resistance) efflux pumps <REF-5> , <REF-7> – <REF-9> .","Modify,Grammar",Grammar
6357,2-238,2-238_v2_69@2,2-238_v1_67@0,"HDAC inhibitors, by virtue of their ability to prevent antifungal resistance in vitro, have been proposed as antifungal adjuvants.",HDAC inhibitors by virtue of their ability to prevent antifungal resistance in vitro have been proposed as antifungal adjuvants.,"Modify,Grammar",Grammar
6358,2-238,2-238_v2_74@2,2-238_v1_71@2,"Our studies with the Class I HDAC inhibitor MS-275 showed that this inhibitor did not inhibit Hos2 deacetylase as effectively as the pan HDAC inhibitors SAHA or TSA, suggesting that Candida Hos2 is more similar to Class II deacetylases.","Our studies with the class I HDAC inhibitor MS-275 showed that this inhibitor did not inhibit Hos2 deacetylase as effectively as the pan HDAC inhibitors SAHA or TSA, suggesting that Candida Hos2 is more similar to class II deacetylases.","Modify,Grammar",Grammar
6359,2-238,2-238_v2_75@3,2-238_v1_72@2,"Hos2 in essence resembles the Class II mammalian HDACs, specifically HDAC6 in its preference for tubulin deacetylation.","Hos2 in essence resembles the class II mammalian HDACs, specifically HDAC6 in its preference for tubulin deacetylation.","Modify,Grammar",Grammar
6360,2-238,2-238_v2_76@6,2-238_v1_73@6,"We did not observe any significant ( P value 0.5317 and 0.4411, in the presence and absence of trichostatin respectively) activation of NAD+ dependent deacetylase activity with the fluor-de-lys substrate.",We did not observe any significant activation of NAD+ dependent deacetylase activity with the fluor-de-lys substrate.,"Modify,Fact/Evidence",Fact/Evidence
6361,2-238,2-238_v2_7@1,2-238_v1_7@1,"For example the HDAC Class I specific inhibitor MS-275 is in advanced clinical trials (clinical trial Nos. NCT00020579, NCT00866333) for several forms of cancer, and the HDAC Class II specific inhibitor ACY-1215 is at an advanced clinical phase (clinical trial Nos. NCT01323751 , NCT01583283 ) for myeloma.","For example the HDAC Class I specific inhibitor MS-275 is in advanced clinical trials (clinical trial Nos. NCT00020579, NCT00866333) for several forms of cancer, and the HDAC class II specific inhibitor ACY-1215 is at an advanced clinical phase (clinical trial Nos. NCT01323751 , NCT01583283 ) for myeloma.","Modify,Grammar",Grammar
6362,2-238,2-238_v2_9@0,2-238_v1_9@0,"Hos2, a Class I HDAC enzyme plays an important role in gene activation in the yeast Saccharomyces cerevisiae by binding to open reading frames (ORFs) of active genes <REF-20> .","Hos2, a class I HDAC enzyme plays an important role in gene activation in the yeast Saccharomyces cerevisiae by binding to open reading frames (ORFs) of active genes <REF-20> .","Modify,Grammar",Grammar
6363,2-238,2-238_v2_2@5,2-238_v1_2@5,"Inhibition studies showed that Hos2 is susceptible to pan inhibitors such as trichostatin A (TSA) and suberoylanilide hydroxamic acid (SAHA), but is not inhibited by class I inhibitors such as MS-275.","Inhibition studies showed that Hos2 is susceptible to pan inhibitors such as trichostatin A (TSA) and suberoylanilide hydroxamic acid (SAHA), but is not inhibited by class I inhibitors such MS-275.","Modify,Grammar",Grammar
6366,2-242,2-242_v2_54@5,2-242_v1_54@4,"However, there are other possible reasons why there is a poor association of heart defects with the ‘jogging ortholog’ gene list.","However, there are numerous other reasons why there is a poor association of heart defects with the ‘jogging ortholog’ gene list.","Modify,Clarity",Clarity
6367,2-242,2-242_v2_13@0,2-242_v1_13@0,A list of 30 zebrafish genes that affect heart jogging was compiled using a variety of approaches.,A list of 30 zebrafish heart jogging genes was compiled using a variety of approaches.,"Modify,Clarity",Clarity
6368,2-242,2-242_v2_13@1,2-242_v1_13@1,"Twelve zebrafish proteins were identified as they were already annotated to the ‘heart jogging’ GO terms, the remaining 18 proteins were then identified using the ZFIN ( http://zfin.org/ ) Site Search, with the search phrase 'heart jogging', and filtering using the 'Expression/Phenotypes' category.","Twelve zebrafish proteins were identified as they were already annotated to the ‘heart jogging’ GO terms, a further 18 proteins were then identified using the ZFIN database, using a keyword search (heart jogging).","Modify,Fact/Evidence",Fact/Evidence
6371,2-242,2-242_v2_13@4,2-242_v1_13@3,"Each of the papers identified in this way were reviewed; of the 23 zebrafish genes identified in these papers five (Bmpr1aa, Tbx1, unm_hu119, unm_hu202, unm_hu304) were eliminated, as none of these papers provided experimental evidence for the involvement of these genes in heart jogging.","This search identified a further 23 zebrafish genes, however manual review of these publications led to 5 being disregarded, as the evidence for an involvement in heart jogging was not strong enough.","Modify,Fact/Evidence",Fact/Evidence
6378,2-259,2-259_v2_8@0,2-259_v1_8@0,"We report the case of a 36-year-old French man, Caucasian truck driver, without any significant medical history.","We report the case of a 36-year-old French, Caucasian truck driver, without any significant medical history.","Modify,Clarity",Clarity
6379,2-259,2-259_v2_0@0,2-259_v1_0@0,Case Report: Severe bilateral amyotrophic neuralgia associated with major dysphagia secondary to acute hepatitis E,Severe bilateral amyotrophic neuralgia associated with major dysphagia secondary to acute hepatitis E,"Modify,Other",Other
6380,2-259,2-259_v2_9@5,2-259_v1_9@5,"The cerebrospinal fluid (CSF) was normal (2 white blood cells/mm3; CSF Protein= 0.37 g/L) and there was no intrathecal antibody synthesis, although it was not tested for the synthesis of specific anti-HEV antibodies.",The cerebrospinal fluid (CSF) was normal (2 white blood cells/mm 3 ; CSF Protein= 0.37 g/L) and there was no intrathecal antibody synthesis.,"Modify,Fact/Evidence",Fact/Evidence
6381,2-259,2-259_v2_9@9,2-259_v1_9@10,The PCR was negative in the CSF and was not initially performed in stools.,The PCR was negative in the CSF.,"Modify,Fact/Evidence",Fact/Evidence
6382,2-259,2-259_v2_10@0,2-259_v1_10@0,"A treatment with intravenous immunoglobulins (Tegeline ® , LFB laboratory, France; 0.4 g/kg/day) was given for 5 days.","A treatment with intravenous immunoglobulins (Tegeline®, LFB laboratory, France; 0.4 g/kg/day) was given for 5 days.","Modify,Grammar",Grammar
6383,2-259,2-259_v2_10@2,2-259_v1_10@2,"Nine days after ribavirin initiation, the PCR showed 2.02 log-copies/ml and was negative after 18 days in both blood and stools.","Nine days after ribavirin initiation, the PCR showed 2.02 log-copies/ml and was negative after 18 days.","Modify,Fact/Evidence",Fact/Evidence
6384,2-259,2-259_v2_12@6,2-259_v1_12@6,The only proposed treatment (with a low level of evidence) is corticosteroids but this may have been dangerous in this case of acute hepatitis E <REF-3> .,The only validated treatment is corticosteroids but this may have been dangerous in this case of acute hepatitis E <REF-3> .,"Modify,Claim",Claim
6385,2-259,2-259_v2_16@0,2-259_v1_16@0,Post-infectious neurological diseases following HEV infection must be recognized to avoid unnecessary and potentially invasive procedures such as liver biopsy.,Post-infectious neurological diseases following HEV infection must be recognized to avoid unnecessary and potentially invasive procedures.,"Modify,Claim",Claim
6399,2-260,2-260_v2_15@2,2-260_v1_13@2,"For example in a catalytic site consisting of n residues, the existence of a congruent n −1 motif does not imply that it is easy or even possible to add another residue in the structure and obtain the n residue motif.","For example in a catalytic site consisting of n residues, the existence of a congruent n − 1 motif does not imply that it is easy or even possible to add another residue in the structure and obtain the n residue motif.","Modify,Grammar",Grammar
6400,2-260,2-260_v2_15@3,2-260_v1_13@3,"This complexity is best exemplified in the failure to induce β -lactamase activity in a penicillin-binding protein (PBP-5) from E. coli <REF-58> , <REF-59> by generating the L153E mutant of this protein, as proposed by our previous analysis <REF-47> (unpublished results).","This complexity is best exemplified in the failure to induce β -lactamase activity in a penicillin-binding protein (PBP-5) from E. coli <REF-43> , <REF-44> by generating the L153E mutant of this protein, as proposed by our previous analysis <REF-35> (and unpublished results).","Modify,Clarity",Clarity
6401,2-260,2-260_v2_7@3,2-260_v1_5@3,"For example, we detected the presence of the serine protease (SPASE) catalytic triad motif (Ser195, His57, Asp102) in alkaline phosphatases (AP) from various organisms using the spatial and electrostatic congruence, and validated this by inhibition of the native phosphatase activity using inhibitors (AEBSF/PMSF) <REF-32> , known to be active on many serine proteases by reaction with the nucleophilic serine <REF-36> .","For example, we detected the presence of the serine protease (SPASE) catalytic triad motif (Ser195, His57, Asp102) in alkaline phosphatases (AP) from various organisms using the spatial and electrostatic congruence, and validated this by inhibition of the native phosphatase activity using serine protease inhibitors (AEBSF/PMSF) <REF-20> .","Modify,Fact/Evidence",Fact/Evidence
6402,2-260,2-260_v2_7@5,2-260_v1_5@5,"Recently, the crown domain in the E. coli expressed rat intestinal AP protein was shown to be prone to protease cleavage, which the authors have ascribed to self-cleavage <REF-37> .","Recently, the crown domain in the Escherichia coli expressed rat intestinal AP protein was shown to be prone to protease cleavage, which the authors have ascribed to self-cleavage <REF-24> .","Modify,Clarity",Clarity
6403,2-260,2-260_v2_8@3,2-260_v1_6@3,"We thus concluded that one should exert caution before ruling out protease activity in an enzyme since theoretically proteases have a large number of possible substrates due to the possible variation in residues flanking the sissile bond, and the corresponding folds that harbor a recognition site for a particular protease <REF-39> .",We thus concluded that one should exert caution before ruling out protease activity in an enzyme since theoretically proteases have unenumerable number of possible substrates due to the infinite possible DNA sequences that can result in proteins and their corresponding infinite folds <REF-27> .,"Modify,Fact/Evidence",Fact/Evidence
6404,2-260,2-260_v2_8@4,2-260_v1_6@4,"Thus, it is possible that we have not found the ideal proteolytic substrate for APs <REF-32> .","Thus, it is possible that we have not found the correct proteolytic substrate for APs <REF-20> .","Modify,Clarity",Clarity
6405,2-260,2-260_v2_9@2,2-260_v1_8@2,"Several β -lactam compounds failed to inhibit E. coli or shrimp AP, as was expected by the lower congruence indicated by CLASP as compared to VAP.","Several β -lactam compounds failed to inhibit E. coli or shrimp AP, as was expected by the lower congruence indicated by CLASP.","Modify,Clarity",Clarity
6406,2-260,2-260_v2_10@1,2-260_v1_9@1,The search for an elastase-like motif in a plant protein <REF-47> led us to the pathogenesis-related protein P14a <REF-49> .,The search for an elastase-like motif in a plant protein led us to the pathogenesis-related protein P14a <REF-37> .,"Modify,Fact/Evidence",Fact/Evidence
6593,2-288,2-288_v2_31@0,2-288_v1_31@0,"JSim is designed centrally for evaluating models against experimental data, for describing biological systems, for designing experiments, and for the teaching of integrative systems approaches to biological, chemical and physical systems.","JSim is quite general, and while designed for evaluating models against experimental data, it also serves pure model development quite well.","Modify,Fact/Evidence",Fact/Evidence
6594,2-288,2-288_v2_32@0,2-288_v1_32@0,MML (Mathematical Modeling Language) is a declarative modeling language developed for JSim and used for composing models.,MML (Mathematical Modeling Language) is the declarative modeling language developed for JSim and used for composing models.,"Modify,Grammar",Grammar
6595,2-288,2-288_v2_37@0,2-288_v1_36@0,"JSim’s mathematical modeling language, MML","JSim’s Mathematical Modeling Language, MML","Modify,Grammar",Grammar
6596,2-288,2-288_v2_38@1,2-288_v1_37@1,"When JSim imports other model formats (e.g. SBML, CellML, Antimony ( Smith et al. , 2013 )), it translates them to MML.","When JSim imports other model formats (e.g. SBML, CellML, Antimony), it translates them to MML.","Modify,Fact/Evidence",Fact/Evidence
6597,2-288,2-288_v2_44@2,2-288_v1_41@2,At present JSim provides 8 algorithms for solving ODEs [ Table 1 ) and 3 for PDEs [ Table 2 ).,At present JSim provides 8 algorithms for solving ODEs ( Table 1 ) and 3 for PDEs ( Table 2 ).,"Modify,Grammar",Grammar
6598,2-288,2-288_v2_49@1,2-288_v1_46@1,"Partial differential equations also require boundary conditions, as seen in the code for a two-region convection-diffusion-permeation-reaction model [ Box 2 ).","Partial differential equations require also boundary conditions, as seen in the code for a two-region convection-diffusion-permeation-reaction model ( Box 2 ).","Modify,Grammar",Grammar
6599,2-288,2-288_v2_51@4,2-288_v1_48@4,"When there is no consumption and the system is linear (output area equals input) and stationary (response same at another time), then the output function is the convolution of the operator’s transfer function (the response to an infinitely short pulse input) with the input function.","When the system is linear (output area equals input) and stationary (response same at another time), then the output is the convolution of the operator’s transfer function (the response to an infinitely short pulse input) with the input function.","Modify,Fact/Evidence",Fact/Evidence
6600,2-288,2-288_v2_4@2,2-288_v1_4@2,"By so doing, at least one of the hypotheses must then be rejected; the rejection marks a stepping-stone in science.","By so doing, at least one of the hypotheses must then be rejected: the rejection marks a stepping-stone in science.","Modify,Grammar",Grammar
6601,2-288,2-288_v2_63@0,2-288_v1_62@0,"Nested plots [ Figure 3 ) are JSim’s version of worlds-within-worlds graphics ( Harris et al. , 1994 ).","Nested plots ( Figure 3 ) are JSim’s version of worlds-within-worlds graphics ( Harris et al. , 1994 ).","Modify,Grammar",Grammar
6602,2-288,2-288_v2_5@2,2-288_v1_5@2,"Given that the experiment tests whether or not the working hypothesis is compatible with experimental data, then failure to fit the data within a defined level of goodness of fit leads to skepticism about the accuracy of the data or more often, about the structure of the model and leads to its modification or to its outright rejection.","Given that the experiment tests whether or not the working hypothesis is compatible with experimental data, then failure to fit leads to rejection.","Modify,Claim",Claim
6603,2-288,2-288_v2_71@1,2-288_v1_70@1,Automated parameter optimization is usually much faster; eight methods are provided [See Table 3 ]; we recommend testing several in order to test speed and reliability with respect to the particular types of data and model.,Automated parameter optimization is usually much faster; eight methods are provided (See Table 3 ); we recommend testing several in order to test speed and reliability with respect to the particular types of data and model.,"Modify,Grammar",Grammar
6604,2-288,2-288_v2_7@0,2-288_v1_7@0,"As in physics, models are posed in order to gain deeper understanding and to make predictions.","As in physics, models are posed in order to gain deeper understanding.","Modify,Claim",Claim
6607,2-288,2-288_v2_79@4,2-288_v1_78@5,"From these many results, one obtains a histogram of estimates for each optimized parameter, and robust confidence limits can be drawn directly from these histograms without assuming symmetry and linearity as in the Jacobian method.","From these results, one obtains a histogram of estimates for each optimized parameter, and robust confidence limits can be drawn directly from these histograms without assuming symmetry and linearity as in the Jacobian method.","Modify,Clarity",Clarity
6608,2-288,2-288_v2_79@5,2-288_v1_78@6,JSim also displays these results in the form of 2-parameter scatter plots to show covariance.,"JSim displays these histograms to show the distributions of parameter estimates in full detail, and 2-parameter scatter plots to show covariance.","Modify,Fact/Evidence",Fact/Evidence
6609,2-288,2-288_v2_81@3,2-288_v1_80@3,"This feature is based on work by Yngve ( Yngve et al. , 2007 ).","This capability is based on work by Yngve ( Yngve et al. , 2007 ).","Modify,Clarity",Clarity
6610,2-288,2-288_v2_85@0,2-288_v1_84@0,"JSim is implemented in the Java computer language ( Gosling & McGilton, 1996 ).","JSim is implemented in the Java computer language ( Gosling & McGilton, 2003 ).","Modify,Fact/Evidence",Fact/Evidence
6611,2-288,2-288_v2_85@1,2-288_v1_84@1,"The major factors affecting this choice are Java’s platform independent GUI (allowing Windows, Macintosh and Linux versions to be developed in a single code base), object-oriented features and garbage collection (simplifying complex coding), advanced utilities (associative arrays, dynamic linking, remote procedure calls), strong type checking (allowing many common coding errors to be caught at compile time) and robust exception mechanism (simplifying coding and enabling a virtually crash-proof GUI).","The major factors affecting this choice are Java's platform independent GUI (allowing Windows, Macintosh and Linux versions to be developed in a single code base), object-oriented features and garbage collection (simplifying complex coding), advanced utilities (associative arrays, dynamic linking, remote procedure calls), strong type checking (allowing many common coding errors to be caught at compile time) and robust exception mechanism (simplifying coding and enabling a virtually crash-proof GUI).","Modify,Grammar",Grammar
6612,2-288,2-288_v2_86@1,2-288_v1_85@1,"These tools, similar to the classic Unix lex and yacc ( Aho & Sethi, 1988 ), were among the few parser generation tools available for Java when JSim was first developed.","These tools, similar to the classic Unix lex and yacc ( Aho et al. , 1988 ), were among the few parser generation tools available for Java when JSim was first developed.","Modify,Fact/Evidence",Fact/Evidence
6613,2-288,2-288_v2_86@3,2-288_v1_85@3,MML’s declarative structure is an intuitive expression of a model’s underlying mathematics (simplifying the modeler’s learning) and allows the overall structure of the model to be examined for mathematical correctness (detecting overspecification or underspecification) in a way that is not possible with a procedural specification.,MML's declarative structure is an intuitive expression of a model's underlying mathematics (simplifying the modeler’s learning) and allows the overall structure of the model to be examined for mathematical correctness (detecting overspecification or underspecification) in a way that is not possible with a procedural specification.,"Modify,Grammar",Grammar
6614,2-288,2-288_v2_7@4,2-288_v1_7@3,"Standard statistical methods are not central to deciding whether or not to reject the hypothesis, but are indeed helpful in assessing goodness of fit, estimating confidence ranges and co-variances among parameters, and in guiding the investigator in identifying errors or in finding ways to simplify the model.","Standard statistical methods are not central to deciding whether or not to reject the hypothesis, but are indeed very helpful in assessing goodness of fit, estimating confidence ranges and co-variances among parameters, and in guiding the investigator in identifying errors or in finding ways to simplify the model.","Modify,Clarity",Clarity
6615,2-288,2-288_v2_87@4,2-288_v1_86@4,"JSim’s remote computation is implemented using Java Remote Method Invocation (RMI) ( Harold, 1997 ), providing reliable access to networked computational servers.","JSim's remote computation is implemented using Java Remote Method Invocation (RMI) ( Harold, 1997 ), providing reliable access to networked computational servers.","Modify,Grammar",Grammar
6616,2-288,2-288_v2_104@0,2-288_v1_88@0,The all-too-frequent failures to reproduce published results are a critical problem in advancing the biological sciences.,"The issue of reproducibility, or should we say the all-too-frequent failures of attempts to reproduce published results, are beginning to be recognized as a critical problem in advancing the biological sciences.","Modify,Clarity",Clarity
6617,2-288,2-288_v2_104@1,2-288_v1_88@1,"It is easy to understand that biological laboratory studies, with inherently great variability in materials and analysis procedures, should be less exact than those in the physical sciences, but it is not so forgivable that reproducing mathematical models of biological systems is a major problem.","It is easy to understand biological studies, with inherently great variability in materials and analysis procedures, should be less exact than those in the physical sciences, but it is not so forgivable that reproducing mathematical models of biological systems is a major problem.","Modify,Clarity",Clarity
6618,2-288,2-288_v2_104@3,2-288_v1_88@3,"These models all used algebraic, ODEs, or differential-algebraic equations and so must be regarded as relatively simple, computationally, compared to finite-element models or spatially dependent models.","These models all used algebraic, ODEs, or differential-algebraic equations and so must be regarded as relatively simple computationally compared to finite-element models or spatially dependent models.","Modify,Grammar",Grammar
6619,2-288,2-288_v2_105@0,2-288_v1_89@0,Project files as vehicles for reproducible modeling and data analysis,Project files,"Modify,Other",Other
6620,2-288,2-288_v2_106@8,2-288_v1_90@8,"They are editable in any word processor, but one avoids doing that since it is easier to enter code and notes under JSim editor directly and not disturb the format in the XMML file that JSim reads.","They are editable in any word processor, but one avoids doing that since it is easier to enter code and notes under JSim and not risk disturbing the format in the XMML file that JSim reads.","Modify,Clarity",Clarity
6621,2-288,2-288_v2_107@1,2-288_v1_91@1,"Examples are that of Kuikka et al on glucose uptake by myocardium ( Kuikka et al. , 1986 , models 163 and 173), xanthine oxidase reactions ( Bassingthwaighte & Chinn, 2013 , model 324), and lung endothelial serotonin uptake ( Jardine & Bassingthwaighte, 2013 , model 198).","Examples are that of Kuikka et al. on glucose uptake by myocardium ( Kuikka et al. , 1986 ), [models 163 and 173], xanthine oxidase reactions ( Bassingthwaighte & Chinn, 2013 ), [model 324], and lung endothelial serotonin uptake ( Jardine & Bassingthwaighte, 2013 ), [model 198].","Modify,Grammar",Grammar
6622,2-288,2-288_v2_122@3,2-288_v1_96@3,"First, when it only takes a few seconds to modify a model, re-run it, and view the results, researchers are more likely to explore many “what if” scenarios and develop a deeper understanding of model behavior, and in turn, a deeper understanding of the system being modeled.","First, when it only takes a few seconds to tweak a model, re-run it, and view the results, researchers are more likely to explore many “what if” scenarios and develop a deeper understanding of model behavior, and in turn, a deeper understanding of the system being modeled.","Modify,Clarity",Clarity
6623,2-288,2-288_v2_124@0,2-288_v1_98@0,Future developments:,Future developments,"Modify,Grammar",Grammar
6624,2-288,2-288_v2_18@0,2-288_v1_18@0,(7) Explore model behavior over wide ranges of parameter values in state-space. (We think of “state space” as being the N-dimensional space enclosing the ranges of values of all of the parameters within which the model is correct numerically and sensible scientifically).,(7) Explore model behavior over wide ranges of parameter values in state-space. (We think of “state space” as being the N-dimensional space enclosing the ranges of values of all of the parameters within which the model is correct numerically and sensible scientifically.),"Modify,Grammar",Grammar
6632,2-29,2-29_v2_4@1,2-29_v1_4@1,Paraovarian cysts constitute 10–20% of all adnexal masses <REF-2> .,"Paraovarian cysts, constitute 10–20% of all adnexal masses <REF-2> .","Modify,Grammar",Grammar
6633,2-29,2-29_v2_4@3,2-29_v1_4@3,"These cysts are usually benign and rarely malignant <REF-4> , <REF-5> .","These cysts are usually benign with rare incidence of malignant types <REF-4> , <REF-5> .","Modify,Clarity",Clarity
6634,2-29,2-29_v2_4@4,2-29_v1_4@4,"In this report, we present how we surgically managed a case with an abnormally huge paraovarian cyst.","Here, we present a case of unusually extensive proportions.","Modify,Fact/Evidence",Fact/Evidence
6635,2-29,2-29_v2_18@0,2-29_v1_7@0,Written informed consent for publication of the clinical details and clinical images was obtained from the patient.,Written informed consent for publication of the clinical details and/or clinical images was obtained from the patient.,"Modify,Clarity",Clarity
6636,2-29,2-29_v2_6@1,2-29_v1_8@1,History revealed a gradual increase of an abdominal swelling over the preceding 6 months.,History revealed a gradual increase in an abdominal swelling over the preceding 6 months.,"Modify,Grammar",Grammar
6637,2-29,2-29_v2_6@2,2-29_v1_8@2,"Physical examination showed a non tender, tense cystic pelvi-abdominal mass of 36 weeks gestational size.",Physical examination showed a non tender tense cystic pelviabdominal mass of 36 weeks gestational size.,"Modify,Grammar",Grammar
6638,2-29,2-29_v2_6@3,2-29_v1_8@3,Computerised tomography revealed a 25 × 26 cm left ovarian simple cyst with clear contents and no septae.,Computerised tomography revealed 25×26 cm left ovarian simple cyst with clear contents and no septae.,"Modify,Grammar",Grammar
6639,2-29,2-29_v2_6@4,2-29_v1_8@4,Serum CA125 level was normal.,Serum CA125 levels were normal.,"Modify,Grammar",Grammar
6640,2-29,2-29_v2_0@0,2-29_v1_0@0,Case Report: Laparoscopic trocar management of a giant paraovarian cyst: a case report,Laparoscopic trocar management of a giant paraovarian cyst: a case report,"Modify,Other",Other
6643,2-29,2-29_v2_6@13,2-29_v1_8@12,"The collapsed cyst was found to be left paraovarian, which was exteriorized and the trocar sleeve was removed.",The collapsed cyst was found to be left paraovarian which was exteriorized and the trocar sleeve was removed.,"Modify,Grammar",Grammar
6644,2-29,2-29_v2_6@15,2-29_v1_8@14,The left broad ligament was opened and the cyst wall was completely removed from the broad ligament ( Figure 1 ).,"The left broad ligament was opened and the cyst wall was completely removed from the broad ligament, Figure 1 .","Modify,Grammar",Grammar
6645,2-29,2-29_v2_6@16,2-29_v1_8@15,The redundant peritoneum of the ligament was excised and subsequently reconstructed with preservation of the tubal integrity as seen in Figure 2 .,The redundant ligament peritoneum was excised and subsequently reconstructed with preservation of the tubal integrity as seen in Figure 2 .,"Modify,Clarity",Clarity
6648,2-29,2-29_v2_13@3,2-29_v1_15@2,Only one report for three adolescents with large paraovarian cysts (with a range of 20–26 cm) had addressed decompression techniques before cyst exteriorization and excision using a suction cannula <REF-9> .,"However, a case report for three adolescents with large paraovarian cysts had addressed decompression technique before cyst externalization and excision but in a different way <REF-9> .","Modify,Fact/Evidence",Fact/Evidence
6649,2-29,2-29_v2_14@2,2-29_v1_15@4,Darwish et al. reported a series of smaller paraovarian cysts which had been excised laproscopically; the largest was not more than 13 cm <REF-12> .,"Concerning the endoscopic role, Darwish et al. reported a series of paraovarian cysts which had been excised laproscopically but were smaller in size with the largest not more than 13 cm <REF-10> .","Modify,Clarity",Clarity
6650,2-29,2-29_v2_14@4,2-29_v1_15@7,Laparoscopy would have been technically difficult in our case due to the huge size of the cyst reaching close to xiphesternum.,It was thought that laparoscopy would be technically difficult in this case due to huge size of the cyst reaching close to xiphesternum.,"Modify,Clarity",Clarity
6651,2-29,2-29_v2_14@5,2-29_v1_15@8,Direct abdominal entry with a Veress needle or trocar might have traumatized the cyst leading to spillage of its content.,Direct abdominal entry with a Veress needle or trocar may have traumatized the cyst leading to risk of spillage of its content.,"Modify,Grammar",Grammar
6652,2-29,2-29_v2_14@7,2-29_v1_15@9,"We therefore, through laparotomy and a small surgical incision, employed a closed drainage system to safely aspirate the cyst followed by excision.",Through laparotomy we employed a closed drainage system and safely aspirated the cyst without spillage of its content.,"Modify,Fact/Evidence",Fact/Evidence
6653,2-29,2-29_v2_4@0,2-29_v1_4@0,"Paraovarian cysts occur in the broad ligament between the ovary and the tube, predominantly arising from mesothelium covering the peritoneum (mesothelial cyst) but also occasionally arise from the paramesonephric tissue (paramesonephric cysts or Mullerian cysts) and rarely from mesonephric remnants (mesonephric cysts or Wolffian cysts) <REF-1> .","Paraovarian cysts occur in the broad ligament between the ovary and the tube, predominantly arising from mesothelium covering the peritoneum (mesothelial cyst) but occasionally also from para mesonephric tissue (paramesonephric cysts or Mullerian cysts) and rarely from mesonephric remnants (mesonephric cysts or Wolffian cysts) <REF-1> .","Modify,Clarity",Clarity
6654,2-30,2-30_v2_13@2,2-30_v1_13@2,"An example can be seen in Figure 1a) , where logical definitions are used to automatically infer that Hypoglycemia is a subclass of Decreased aldohexose concentration (blood) based on the asserted subclass relationship between 'glucose' and 'aldohexose' in ChEBI.","An example can be seen in Figure 1 a) , where logical definitions are used to automatically infer that Hypoglycemia is a subclass of Decreased aldohexose concentration (blood) based on the asserted subclass relationship between 'glucose' and 'aldohexose' in ChEBI.","Modify,Grammar",Grammar
6655,2-30,2-30_v2_27@4,2-30_v1_27@4,The translation is shown in Manchester syntax in Figure 1a) .,The translation is shown in Manchester syntax in Figure 1 a) .,"Modify,Grammar",Grammar
6656,2-30,2-30_v2_6@4,2-30_v1_6@4,"Similar approaches are being taken in zebrafish ( Danio rerio ) by the Zebrafish Mutation Project (ZMP, http://www.sanger.ac.uk/Projects/D_rerio/zmp/ ) and the data is being made available through the Zebrafish Model Organism Database (ZFIN <REF-5> ).","Similar approaches are being taken in zebrafish ( Danio rerio ) by the Zebrafish Mutation Project (ZMP, http://www.sanger.ac.uk/Projects/D_rerio/zmp/ ) and the data is being made available through the The Zebrafish Model Organism Database (ZFIN <REF-5> ).","Modify,Grammar",Grammar
6657,2-30,2-30_v2_43@0,2-30_v1_43@0,"Here, the GO process eye pigmentation ( GO:0048069 ) is logically defined as being equivalent to everything that is a pigmentation ( GO:0043473 ) and also ""occurs_in"" an eye ( UBERON:0000970 ).","Here, the GO process eye pigmentation ( G0:0048069 ) is logically defined as being equivalent to everything that is a pigmentation ( GO:0043473 ) and also ""occurs_in"" an eye ( UBERON:0000970 ).","Modify,Grammar",Grammar
6658,2-30,2-30_v2_68@0,2-30_v1_68@0,"An excerpt of the Uberpheno ontology is shown in ( Figure 1b ), demonstrating how the phenotype descriptions from different ontologies are combined and automatically organised into a single, integrated hierarchy.","An excerpt of the Uberpheno ontology is shown in ( Figure 1 b ), demonstrating how the phenotype descriptions from different ontologies are combined and automatically organised into a single, integrated hierarchy.","Modify,Grammar",Grammar
6659,2-30,2-30_v2_3@0,2-30_v1_3@0,"We have generated a cross-species phenotype ontology for human, mouse and zebrafish that contains classes from the Human Phenotype Ontology, Mammalian Phenotype Ontology, and generated classes for zebrafish phenotypes.","We have generated a cross-species phenotype ontology for human, mouse and zebra fish that contains zebrafish phenotypes.","Modify,Fact/Evidence",Fact/Evidence
6660,2-57,2-57_v2_24@0,2-57_v1_24@0,Response of cotesia vestalis females to Blend A vs. Blend B,Response of Cotesia vestalis females to Blend A vs. Blend B,"Modify,Grammar",Grammar
6661,2-57,2-57_v2_29@0,2-57_v1_29@0,Response of cotesia vestalis females to Blend A vs. Blend C,Response of Cotesia vestalis females to Blend A vs. Blend C,"Modify,Grammar",Grammar
6662,2-57,2-57_v2_15@2,2-57_v1_15@2,"Next, to test whether female wasps could discriminate the ratios of the four compounds in the blend (quantitative differences in the blend), we prepared a third blend (Blend C) of sabinene, n -heptanal, α -pinene, and ( Z )-3-hexenyl acetate at a ratio of 1.0:1.0:1.0:1.0.","Next, to test whether female wasps could discriminate the ratios of the four compounds in the blend, we prepared a third blend (Blend C) of sabinene, n -heptanal, α -pinene, and ( Z )-3-hexenyl acetate at a ratio of 1.0:1.0:1.0:1.0.","Modify,Clarity",Clarity
6674,2-58,2-58_v2_38@0,2-58_v1_38@0,"A) The size of the binding ring had a significant influence on the perceived size of the array (main effect of binding ring size-repeated measures ANOVA: F (4, 36) = 31.22, p < 0.001).","A) The size of the binding ring had a significant influence on the perceived size of the array (main effect of binding ring size - repeated measures ANOVA: F (4, 36) = 31.22, p < 0.001).","Modify,Grammar",Grammar
6675,2-58,2-58_v2_45@1,2-58_v1_45@1,"It is suggested that some visual illusions, including the Müller-Lyer <REF-19> and Oppel-Kundt <REF-20> , <REF-21> or filled area illusion <REF-22> , are mediated by differential processing of low- and high-spatial frequency information <REF-23> – <REF-27> .","It is suggested that some visual illusions, including the the Müller-Lyer <REF-17> , are mediated by differential processing of low- and high-spatial frequency information <REF-18> – <REF-22> .","Modify,Fact/Evidence",Fact/Evidence
6676,2-58,2-58_v2_4@3,2-58_v1_4@3,"These constructive processes are revealed through a number of classic size illusions such as the Ebbinghaus Illusion <REF-4> ( Figure 1A ), the Delboeuf Illusion <REF-5> , <REF-6> ( Figure 1B ), the Müller-Lyer Illusion <REF-7> ( Figure 1C ) and several others that illustrate how mechanisms that underlie size constancy sometimes lead to illusory percepts resulting from a discrepancy between retinal and perceived size.","These constructive processes are revealed through a number of classic size illusions such as the Ebbinghaus Illusion <REF-4> ( Figure 1A ) and the Delboeuf Illusion <REF-5> , <REF-6> ( Figure 1B ).","Modify,Fact/Evidence",Fact/Evidence
6677,2-58,2-58_v2_4@4,2-58_v1_4@4,"In each of these illusions, the perceived size of an explicitly defined object is influenced by the context in which it is presented.","In each of these illusions, the perceived size of an explicitly defined object (the inner circle) is influenced by the context in which it is presented.","Modify,Clarity",Clarity
6678,2-58,2-58_v2_52@0,2-58_v1_52@0,Experiment 4a,Experiment 4,"Modify,Other",Other
6679,2-58,2-58_v2_53@1,2-58_v1_53@1,"A number of visual illusions can be attributed to the processes of local configural features <REF-3> , <REF-8> , <REF-28> .",A number of visual illusions can be attributed to the processes of local configural features <REF-23> .,"Modify,Fact/Evidence",Fact/Evidence
6680,2-58,2-58_v2_19@2,2-58_v1_19@2,"Participants indicated by pressing one of two buttons (two-alternative forced choice), which of the two stimuli had appeared larger.","Participants indicated by pressing one of two buttons (two-alternative forced choice; AFC), which of the two stimuli had appeared larger.","Modify,Clarity",Clarity
6697,2-9,2-9_v2_11@0,2-9_v1_12@0,Two recipient mice of the 5 used in the two accompanying articles were assessed in this experiment.,Two recipient mice of the 5 used in the two accompanying articles were assessed in this report.,"Modify,Clarity",Clarity
6698,2-9,2-9_v2_15@1,2-9_v1_16@1,They were housed in the SKCC animal care facilitywith controlled 12/12 hr light/dark cycle and temperature maintained at + 22°C.,The mice were housed in the SKCC animal care facility.,"Modify,Fact/Evidence",Fact/Evidence
6699,2-9,2-9_v2_16@1,2-9_v1_17@1,J. Lustgarten and P. Borgstrom and used to form tumor spheroids by culturing 2×10 5 cells per well for 2–3 days prior to implantation.,J. Lustgarten and P. Borgstrom and used to form tumor spheroids by culturing 2 × 10 5 cells per well for 2–3 days prior to implantation.,"Modify,Grammar",Grammar
6700,2-9,2-9_v2_16@4,2-9_v1_17@4,"Their final size was about 1–3 mm in ""diameter'.",Their final size was about 1–3 mm in diameter.,"Modify,Grammar",Grammar
6701,2-9,2-9_v2_16@5,2-9_v1_17@5,The GFP-specific rabbit polyclonal IgG (ab290) was from Abcam; and non-reactive IgG (sc-34284) were from Santa Cruz.,The GFP-specific rabbit polyclonal IgG (ab290) was from Abcam; and non-reactive goat polyclonal IgG (sc-34284) were from Santa Cruz.,"Modify,Fact/Evidence",Fact/Evidence
6702,2-9,2-9_v2_18@0,2-9_v1_19@0,The tumors with some surrounding tissues were dissected out and cut in halves perpendicularly to the host skin surface while immersed in cold fixative (4% paraformaldehyde in 0.1 M Na cacodylate pH 7.4).,The tumors with some surrounding tissues were dissected out and cut in halves perpendicular to the host skin surface while immersed in cold fixative (4% paraformaldehyde in 0.1 M Na cacodylate pH 7.4).,"Modify,Grammar",Grammar
6703,2-9,2-9_v2_20@1,2-9_v1_21@1,"They were cut into 1 mm thick slices in planes perpendicular to the plain of the first cut and to the skin surface, finally, into ~ 1 mm 3 blocks, transferred into fresh portion of the fixative in which they were cut and incubated for 2 hrs at 4°C.","They were cut into 1 mm thick slices in planes perpendicular to the plain of the first cut and to the skin surface, finally, into ~ 1 mm 3 blocks, transferred into a fresh portion of the fixative in which they were cut and incubated for 2 hrs at 4°C.","Modify,Grammar",Grammar
6704,2-9,2-9_v2_20@2,2-9_v1_21@2,"The fixed tissue blocks were washed with 0.1 M Na cacodylate–HCl buffer pH 7.4 (3 × 15 min.) and post fixed in 1% OsO 4 in 0.1 M Na cacodylate buffer, pH 7.0 for 60 min on ice, washed with water and stained with 1% uranyl acetate at RT for one hour.","The fixed tissue blocks were washed with 0.1 M Na cacodylate – HCl buffer pH 7.4 (3 × 15 min) and post fixed in 1% OsO 4 in 0.1 M Na cacodylate buffer, pH 7.0 for 60 min. on ice, washed with water and stained with 1% uranyl acetate at RT for one hour.","Modify,Grammar",Grammar
6705,2-9,2-9_v2_20@4,2-9_v1_21@4,The resin embedded tissues were cut into 60 nm sections on Leica Ultracut UCT ultramicrotome and viewed without further contrasting.,"The resin-embedded tissues were cut into 60 nm sections, on a Leica Ultracut UCT ultramicrotome and stained with lead citrate <REF-42> or viewed without further contrasting.","Modify,Fact/Evidence",Fact/Evidence
6706,2-9,2-9_v2_22@0,2-9_v1_23@0,"During cutting into ~ 1 mm 3 blocks as described above, the tissues were kept in the mild fixative to protect the antigenic epitopes (4% paraformaldehyde in 0.1 M Na cacodylate pH 7.4).","During cutting into ~ 1 mm 3 blocks as described above, the tissues were kept in the mild fixative to protect the antigenic epitops (4% paraformaldehyde in 0.1 M Na cacodylate pH 7.4).","Modify,Grammar",Grammar
6707,2-9,2-9_v2_22@1,2-9_v1_23@1,"The tissue blocks were vitrified by infiltrating the pieces with 50% PVP (polyvinylpyrrolidone) containing 2.3 M sucrose in 0.1 M Na cacodylate buffer, pH 7.4, for 2 hrs or over night, mounted on metal pins and frozen in liquid nitrogen, as described by Tokuyasu <REF-47> .","The tissue blocks were vitrified by infiltrating the pieces with 50% PVP (polyvinylpyrrolidone) containing 2.3 M sucrose in 0.1 M Na-cacodylate buffer, pH 7.4, for 2 hrs or overnight, mounted on metal pins and frozen in liquid nitrogen, as described by Tokuyasu <REF-43> .","Modify,Grammar",Grammar
6708,2-9,2-9_v2_22@2,2-9_v1_23@2,"Frozen tissues were cut into 70 nm sections, on Leica Ultracut UCT ultramicrotome with the cryo-attachment.","Frozen tissues were cut into 70 nm sections, on a Leica Ultracut UCT ultramicrotome with the cryo-attachment.","Modify,Grammar",Grammar
6709,2-9,2-9_v2_22@6,2-9_v1_23@6,"After rinsing three times with water the immunostained cryosections were contrasted with mixture of uranyl acetate and methyl cellulose (25 centipoises, Sigma M-6385) in water, at final concentration of 1.3% each, for 10 min at RT.","After rinsing three times with water, the immunostained cryosections were contrasted with a mixture of uranyl acetate and methyl cellulose (25 centipoises, Sigma M-6385) in water, at final concentration of 1.3% each, for 10 min at RT.","Modify,Grammar",Grammar
6710,2-9,2-9_v2_22@7,2-9_v1_23@7,Excess of the liquid was removed and the sections were dried at RT.,Excess liquid was removed and the sections were dried at RT.,"Modify,Clarity",Clarity
6711,2-9,2-9_v2_24@4,2-9_v1_25@1,The images were transmitted from the microscope camera to iTEM imaging platform from Olympus Soft Imaging Solutions and archived in a designated Data Base.,Images were transmitted from the microscope camera to an iTEM imaging platform from Olympus Soft Imaging Solutions and archived in a designated database.,"Modify,Grammar",Grammar
6712,2-9,2-9_v2_24@6,2-9_v1_25@2,"We used graphics editing program, Adobe PhotoShop, to add cell type specific color-coding shown in the twin set of images included in the Supplement.","We used the graphics editing program, Adobe PhotoShop, to add cell type-specific color-coding shown in the twin set of images included in the Supplement.","Modify,Grammar",Grammar
6713,2-9,2-9_v2_26@0,2-9_v1_27@0,"Three weeks after the ectopic implantation of tumor spheroids, the vasculature formation, i.e., formation of tumor-supporting blood and vessels, was evidently retarded in comparison to pseudo-orthotopically implanted tumors described elsewhere <REF-41> .","Three weeks after the ectopic implantation of tumor spheroids, the vasculature formation, i.e., formation of tumor-supporting blood and vessels was evidently retarded in comparison to pseudo-orthotopicly implanted tumors described elsewhere <REF-36> .","Modify,Grammar",Grammar
6714,2-9,2-9_v2_26@2,2-9_v1_27@2,"A multi-cellular layer of connective tissue was growing between tumor and glass wall of the chamber, therefore, it was also generating its own vasculature ([A] in Figure 1 & Figure S1 ).","A multi-cellular layer of connective tissue was growing between the tumor and the glass wall of the chamber, therefore, it was also generating its own vasculature ([A] in Figure 1 & Figure S1 ).","Modify,Grammar",Grammar
6715,2-9,2-9_v2_26@3,2-9_v1_27@3,"Here, the term ""vasculature"" includes vessels and blood, and the term ""erythrosome"" is used as synonym for the ""erythrocyte"", because the latter is not a cell <REF-41> .","Here, the term ""vasculature"" includes vessels and blood and ""erythrosome"" is used as a synonym for the ""erythrocyte"", because the latter is not a cell <REF-36> .","Modify,Clarity",Clarity
6716,2-9,2-9_v2_26@7,2-9_v1_27@7,"Outside the tumor capsule, a primitive forming vessel morphologically resembled some of those seen around pseudo-orthotopically implanted tumors after only five days ([D] in Figure 1 & Figure S1 ).","Outside the tumor capsule, a primitive forming vessel morphologically resembled some of those seen around pseudo-orthotopicly implanted tumors after only five days ([D] in Figure 1 & Figure S1 ).","Modify,Grammar",Grammar
6717,2-9,2-9_v2_26@12,2-9_v1_27@12,Yet some tumor cells (CSCs?) began regenerating their vasculogenic potential that had been dormant during the years of in vitro culturing ([F & G] in Figure 1 & Figure S1 ).,Some tumor cells (CSCs?) did however begin regenerating their vasculogenic potential that had been dormant during the years of in vitro culturing ([F & G] in Figure 1 & Figure S1 ).,"Modify,Clarity",Clarity
6718,2-9,2-9_v2_26@14,2-9_v1_27@14,"Until that time, some tumor cells survived at the expense of the others.","Until that time, some tumor cells survived at the expense of others.","Modify,Grammar",Grammar
6719,2-9,2-9_v2_29@1,2-9_v1_30@1,In some locations hypoxic tumor nodules were breaking apart via prominent anoikis (loss of attachment between cells <REF-48> ) with abundant nano-tentacles.,"In some locations, hypoxic tumor nodules were breaking apart via prominent anoikis (loss of attachment between cells <REF-44> ) with abundant nano-tentacles ([A–C] in Figure 2 & Figure S2 ).","Modify,Fact/Evidence",Fact/Evidence
6720,2-9,2-9_v2_29@3,2-9_v1_30@3,They were either losing their internal cristae without shrinking and thus generating electron lucent vacuoles (seemingly empty or containing whorled membranes that might be intermediate stages of the internal membranes degradation) or becoming smaller and electron dense ([A–C] in Figure 2 & Figure S2 ).,They were either loosing their internal cristae without shrinking and thus generating electron-lucent vacuoles (seemingly empty or containing whorled membranes that might be intermediate stages of the internal membranes degradation) or becoming smaller and electron dense.,"Modify,Fact/Evidence",Fact/Evidence
6721,2-9,2-9_v2_29@5,2-9_v1_30@5,"The second type at first resembled appearance of mitochondria during mitosis and later, they were indistinguishable from the dark granules in erythroblasts ([D & F] in Figure 2 & Figure S2 ) and consistent with published images of peroxisomes <REF-50> – <REF-53> .","The second type at first resembled the appearance of mitochondria during mitosis and later, they were indistinguishable from the dark granules in erythroblasts ([D–F] in Figure 2 & Figure S2 ) and consistent with published images of peroxisomes <REF-46> – <REF-49> .","Modify,Grammar",Grammar
6722,2-9,2-9_v2_29@8,2-9_v1_30@8,That is because erythrosomes are capable of secreting anaerobically generated ATP <REF-54> .,That is because erythrosomes are capable of secreting anaerobicly generated ATP <REF-50> .,"Modify,Grammar",Grammar
6723,2-9,2-9_v2_29@10,2-9_v1_30@10,"Initially, electron dense regions of tumor cell nucleus contained chromatin in both cases.","Initially, electron-dense regions of the tumor cell nucleus contained chromatin in both cases.","Modify,Grammar",Grammar
6724,2-9,2-9_v2_29@14,2-9_v1_30@17,"The GFP-labeled mitotic chromosomes identified the tumor cell whereas the unlabeled fibroblast, on the other side of the collagen layer separating the two, must have been of host origin ([F] in Figure 2 & Figure S2 ).","The GFP-labeled mitotic chromosomes identified the tumor cell whereas the unlabeled fibroblast, on the other side of collagen layer separating the two, must have been of host origin ([F] in Figure 2 & Figure S2 ).","Modify,Grammar",Grammar
6725,2-9,2-9_v2_29@15,2-9_v1_30@18,"Together, the host fibroblasts and the encapsulated tumor-derived blood elements created the capsular vaso-mimicry that morphologically resembled veins ([G-I] in Figure 1 & Figure S1 ).","Together, the host fibroblasts and the encapsulated tumor-derived blood elements created the capsular vaso-mimicry that morphologically resembled veins ([G–I] in Figure 1 & Figure S1 ).","Modify,Grammar",Grammar
6726,2-9,2-9_v2_0@0,2-9_v1_0@0,II. Capsular vaso-mimicry formed by transgenic mammary tumor spheroids implanted ectopically into mouse dorsal skin fold: implications for cellular mechanisms of metastasis,II. Capsular vaso-mimicry formed by transgenic mammary tumor spheroids implanted ectopically into mouse dorsal skin fold: cellular mechanisms of metastasis,"Modify,Clarity",Clarity
6727,2-9,2-9_v2_32@3,2-9_v1_33@3,"The elongated cells like the one shown between the tumor nodules ( Figure 3 & Figure S3 ) had large nuclei undergoing conversion into erythrosomes and sparse, metabolically active cytoplasm generating energy and synthesizing protein.","The elongated cells like the one shown between the tumor nodules ( Figure 3 & Figure S3 ) had large nuclei undergoing conversion into erythrosomes and a sparse, metabolically active cytoplasm generating energy and synthesizing protein.","Modify,Grammar",Grammar
6728,2-9,2-9_v2_32@4,2-9_v1_33@4,"What appeared in two-dimensional image as a single file of erythrosomes between tumor nodules was not ""a rouleau of circulating erythrocytes"" <REF-18> .",What appeared in the two-dimensional image as a single file of erythrosomes between the tumor nodules was not a rouleau of circulating erythrocytes.,"Modify,Fact/Evidence",Fact/Evidence
6729,2-9,2-9_v2_35@2,2-9_v1_36@2,In some regions ECs converting into erythrosomes (hemogenic endothelium) were also seen (Figure 2 in <REF-57> ).,"In some regions, ECs converting into erythrosomes (hemogenic endothelium) were also seen (Figure 8 in <REF-54> ).","Modify,Grammar",Grammar
6730,2-9,2-9_v2_43@0,2-9_v1_41@0,Survival of the ectopically implanted breast tumor cells for three weeks without support of host circulatory system was possible due to the erythrogenic autophagy <REF-41> .,Survival of the ectopically implanted breast tumor cells for three weeks without the support of a host circulatory system was possible due to the erythrogenic autophagy <REF-36> .,"Modify,Grammar",Grammar
6731,2-9,2-9_v2_43@4,2-9_v1_42@3,"Non-vasculogenic tumors do not grow over several weeks although the tumor cells keep proliferating at rate similar to that of vasculogenic tumors; ""They have no or non-functional vessels"" <REF-59> .","Non-vasculogenic tumors do not grow over several weeks although the tumor cells keep proliferating at a rate similar to that of vasculogenic tumors; ""They have no or non-functional vessels"" <REF-55> .","Modify,Grammar",Grammar
6732,2-9,2-9_v2_43@6,2-9_v1_42@5,"That was, in fact, another experimental result demonstrating that some tumor cells could survive at the expense of the others.","That was, in fact, an experimental result demonstrating that some tumor cells could survive at the expense of others.","Modify,Clarity",Clarity
6733,2-9,2-9_v2_52@0,2-9_v1_42@6,The postnatal extramedullar erythropoiesis at the location other than bone marrow ( medulla ossea ) was observed previously in spleen <REF-75> – <REF-78> and adipocytic tissues <REF-79> .,Such postnatal extramedullar erythropoiesis at a location other than the bone marrow ( medulla ossea ) was not unprecedented; spleen <REF-56> – <REF-59> and adipocytic tissues <REF-60> have that potential as well.,"Modify,Clarity",Clarity
6734,2-9,2-9_v2_44@0,2-9_v1_43@0,The relevance of the variable metabolism within a single tumor nodule was that tumor derived erythrosomes might indeed extend viability of adjacent tumor cells by supplying them with vital energy in absence of vasculature.,The relevance of the variable metabolism within a single tumor nodule was that tumor-derived erythrosomes might indeed extend viability of adjacent tumor cells by supplying them with vital energy in the absence of vasculature.,"Modify,Grammar",Grammar
6735,2-9,2-9_v2_44@4,2-9_v1_43@1,"Hemoglobin has evolved to bind oxygen cooperatively, i.e., most efficiently when it is abundant (in lungs where the oxygen concentration is about 100 torr) and gradually less and less efficiently as erythrosomes move through arteries and veins (in peripheral tissues the oxygen concentration is about 20 torr) <REF-61> .","Hemoglobin has evolved to bind oxygen cooperatively, i.e., most efficiently when it is abundant (in lungs where the oxygen concentration is about 100 torr) and gradually less and less efficiently as erythrosomes move through arteries and veins (in peripheral tissues, the oxygen concentration is about 20 torr) <REF-61> .","Modify,Grammar",Grammar
6736,2-9,2-9_v2_5@1,2-9_v1_5@1,"A developmental regulatory program involved in embryo implantation, referred to as the ""epithelial-mesenchymal transition"" (EMT) <REF-1> – <REF-4> was adopted to explain how transformed epithelial cells could acquire ability to metastasize, i.e., to invade surrounding nonmalignant tissues and to disseminate, in a multistep process including entering and leaving the circulatory system <REF-5> , <REF-6> .","A developmental regulatory program involved in embryo implantation, referred to as the ""epithelial-mesenchymal transition"" (EMT) <REF-1> – <REF-4> was adopted to explain how transformed epithelial cells could acquire the ability to metastasize, i.e., to invade surrounding nonmalignant tissues and to disseminate, in a multistep process including entering and leaving the circulatory system <REF-5> , <REF-6> .","Modify,Grammar",Grammar
6737,2-9,2-9_v2_45@4,2-9_v1_44@0,The potential to convert into erythrosomes was not limited to erythropoietic lineage derived from myeloid precursors.,The ability to convert into erythrosomes was not limited to erythropoietic lineage derived from myeloid precursors.,"Modify,Clarity",Clarity
6738,2-9,2-9_v2_45@5,2-9_v1_44@1,"Understandably so, given that the inducing factor, hypoxia, affected the most fundamental function of living cells, i.e., the respiration, generating vital energy aerobically.","Understandably so, because the inducing factor, hypoxia, affected the most fundamental function of all living cells, i.e., the respiration, generating vital energy aerobically.","Modify,Clarity",Clarity
6739,2-9,2-9_v2_45@7,2-9_v1_44@3,That metabolic requirement being shared by all cells experiencing hypoxia imposed formation of similar ultrastructural features on all of them (convergence).,That metabolic pathway being shared by all cells experiencing hypoxia imposed the formation of similar ultrastructural features on all of them (convergence).,"Modify,Clarity",Clarity
6740,2-9,2-9_v2_47@3,2-9_v1_46@3,"In other words, the heterologous environment did not kill the transplanted tumor but gradually the exogenous tissue acquired the ability to engage in the paracrine dialog with local TSCs, (perhaps by acquiring proper cell adhesion molecules <REF-62> ) or the tumor activated its own SCs (CSCs).","In other words, the heterologous environment did not kill the transplanted tumor but gradually the exogenous tissue acquired the ability to engage in the paracrine dialog with local TSCs (perhaps by acquiring proper cell adhesion molecules <REF-62> ), or the tumor activated its own SCs (CSCs).","Modify,Grammar",Grammar
6741,2-9,2-9_v2_47@4,2-9_v1_46@4,"Trans-differentiation of tumor SCs into ECs was observed here and also reported earlier in glioblastoma <REF-14> , <REF-63> – <REF-65> .",Trans-differentiation of tumor SCs into ECs was also reported earlier in glioblastoma <REF-63> – <REF-66> .,"Modify,Fact/Evidence",Fact/Evidence
6742,2-9,2-9_v2_47@7,2-9_v1_46@7,"Reported dormant tumors had < 1 mm “diameter”, possibly including fibroblastic coats and necrotic centers <REF-66> .","Reported dormant tumors had < 1 mm diameter, possibly including fibroblastic coats and necrotic centers <REF-67> .","Modify,Grammar",Grammar
6743,2-9,2-9_v2_48@0,2-9_v1_47@0,Capsular vaso-mimicry as cellular mechanism of metastasis,Capsular vaso-mimicry as a cellular mechanism of metastasis,"Modify,Grammar",Grammar
6744,2-9,2-9_v2_49@0,2-9_v1_48@0,"Two cellular mechanisms normally beneficial to the organism when acting independently, one involved in tissue nourishing and the other in healing, i.e., erythrogenesis and scar formation (or foreign body encapsulation <REF-55> ) respectively, became deleterious by creating the capsular vaso-mimicry when they coincided in the ectopically implanted tumor.","Two cellular mechanisms normally beneficial to the organism when acting independently, one involved in tissue nourishment and the other in healing, i.e., erythrogenesis and scar formation (or foreign body encapsulation) respectively, became deleterious by creating the capsular vaso-mimicry when they coincided in the ectopically implanted tumor.","Modify,Fact/Evidence",Fact/Evidence
6745,2-9,2-9_v2_49@8,2-9_v1_48@8,"At the same time, the loss of attachment to other cells could facilitate their dissemination by breaking tumor tissue into small cell clusters or single cells that could be carried away by blood flow.","At the same time, the loss of attachment to other cells could facilitate their dissemination by breaking the tumor tissue into small cell clusters or single cells that could be carried away by blood flow.","Modify,Grammar",Grammar
6746,2-9,2-9_v2_5@2,2-9_v1_5@2,"However, no satisfactory mechanism for the spreading of non-epithelial tumors to secondary locations was proposed.","However, no satisfactory mechanism for the spread of non-epithelial tumors to secondary locations was proposed.","Modify,Grammar",Grammar
6747,2-9,2-9_v2_50@2,2-9_v1_49@2,"Clusters of the primary tumor cells could be passively carried to different tissues by blood flow and become immobilized when they reached vessels narrower than their own dimensions, in a tissue non-specific manner.","Clusters of primary tumor cells could be passively carried to different tissues by blood flow and become immobilized when they reached vessels narrower than their own dimensions, in a tissue non-specific manner.","Modify,Grammar",Grammar
6748,2-9,2-9_v2_50@5,2-9_v1_49@5,"Liver being formed relatively early during embryogenesis and later maintaining primitive vasculature might be most compatible with tumors for that reason and therefore most prone to metastases, as observed clinically and shown experimentally <REF-70> .","Liver being formed relatively early during embryogenesis and later maintaining primitive vasculature might be most compatible with tumors for that reason and therefore most prone to metastases, as observed clinically.","Modify,Fact/Evidence",Fact/Evidence
6749,2-9,2-9_v2_5@3,2-9_v1_5@3,"Therefore, an alternative to EMT regulatory programs playing a role in invasiveness of carcinoma cells should also be considered, as pointed out elsewhere <REF-7> .","Therefore, an alternative to EMT regulatory programs playing a role in invasiveness of carcinoma cells should be considered, as pointed out elsewhere <REF-7> .","Modify,Clarity",Clarity
6750,2-9,2-9_v2_51@4,2-9_v1_49@12,That is how anatomically distant but phenotypically compatible tissue could become activated by the tumor before metastasizing cells got there.,That is how anatomically distant but phenotypicly compatible tissue could become activated by the tumor before metastasizing cells got there.,"Modify,Grammar",Grammar
6751,2-9,2-9_v2_54@1,2-9_v1_51@1,"The remnants of cells that produced erythrosomes could be responsible for PAS staining due to their glyco-lipid components and, more importantly, for fusion with capillaries of main circulatory system, at stages later than analyzed here.","The remnants of cells that produced erythrosomes could be responsible for PAS staining due to their glyco-lipid components and, more importantly, for fusion with capillaries of the main circulatory system, at stages later than analyzed here.","Modify,Grammar",Grammar
6752,2-9,2-9_v2_6@0,2-9_v1_6@0,"Attempts made to elucidate the cellular mechanism of metastasis-initiating events included retrospective extrapolation from the distribution of established metastases, namely the preference of specific tumors to metastasize in certain organs but not in others.","Attempts made to elucidate the cellular mechanism of metastasis-initiating events have included retrospective extrapolation from the distribution of established metastases, namely the preference of specific tumors to metastasize in certain organs but not in others.","Modify,Grammar",Grammar
6753,2-9,2-9_v2_54@2,2-9_v1_51@2,"Our tumors were significantly smaller (""diameter"" of < 1 mm) than those described in the literature (1 cm or more).",Our tumors were significantly smaller (diameter of < 1 mm) than those described in the literature (1 cm or more).,"Modify,Grammar",Grammar
6754,2-9,2-9_v2_54@3,2-9_v1_51@3,Lack of hierarchy in the network pattern of the aggressive tumors suggested a lack of blood circulation.,A lack of hierarchy in the network pattern of the aggressive tumors suggested a lack of blood circulation.,"Modify,Grammar",Grammar
6755,2-9,2-9_v2_54@6,2-9_v1_51@6,"Therefore, that kind of mimicry is probably a form of erythrogenic autophagy of fibroblasts associated with the presence of metastatic tumors.","Therefore, that kind of mimicry probably is a form of fibroblastic autophagy associated with the presence of metastatic tumors.","Modify,Claim",Claim
6756,2-9,2-9_v2_58@5,2-9_v1_53@4,"Shunting of inspired oxygen into tumor venules, presumed to occur due to arterio-venous anastomoses (Figure 10 in <REF-83> ) could alternatively be explained by stable saturation of hemoglobin located in non-circulating erythrosomes within tumor capsule as well as within the regions mimicking vessels ( Figure 1 & Figure S1 ).","Shunting of inspired oxygen into tumor venules, presumed to occur due to arterio-venous anastomoses (Figure 10 in <REF-76> ) could alternatively be explained by stable saturation of hemoglobin located in non-circulating erythrosomes within the tumor capsule, as well as within the regions mimicking vessels ( Figure 1 & Figure S1 ).","Modify,Grammar",Grammar
6757,2-9,2-9_v2_58@10,2-9_v1_53@9,"Consequently, it should not be surprising that increased oxygenation of breast adenocarcinoma by treatment with, for example, darbepoetin alpha, had no desirable effect on the tumor’s responsiveness to radiotherapy <REF-86> .","Consequently, it shouldn’t be surprising that increased oxygenation of breast adenocarcinoma by treatment with, for example, darbepoetin alpha, had no desirable effect on the tumor’s responsiveness to radiotherapy <REF-79> .","Modify,Grammar",Grammar
6758,2-9,2-9_v2_56@0,2-9_v1_55@0,"The distance between capillaries in tissue sections suggested that, within the 100–200 µm zones, cell membranes did not present a barrier for diffusion of nutrients as well as oxygen.","The distance between capillaries in tissue sections suggested that, within the 100–200 µm zones, cell membranes did not present a barrier for diffusion of nutrients or for oxygen.","Modify,Clarity",Clarity
6759,2-9,2-9_v2_56@4,2-9_v1_55@4,"Considering what we now know about cytoevolution leading to ECs differentiation <REF-41> , one could make a premise that luminal surface of the polarized endothelial cell was a functional equivalent of the inner membrane.","Considering what we now know about cytoevolution leading to ECs differentiation <REF-36> , one could make a premise that the luminal surface of the polarized endothelial cell was a functional equivalent of the inner membrane.","Modify,Grammar",Grammar
6760,2-9,2-9_v2_56@8,2-9_v1_55@8,Vascular lumen in that context would be a functional equivalent of intracellular vesicle.,Vascular lumen in that context would be a functional equivalent of an intracellular vesicle.,"Modify,Grammar",Grammar
6761,2-9,2-9_v2_56@9,2-9_v1_55@9,One could conclude that host fibroblasts encapsulating the tumor and creating the capsular vaso-mimicry by positioning themselves around erythrosomes should not present a barrier for the diffusion process because they did not go through the process of cytoevolution resulting in polarization of outer cell membrane into luminal and abluminal.,One could conclude that host fibroblasts encapsulating the tumor and creating the capsular vaso-mimicry by positioning themselves around erythrosomes should not present a barrier for the diffusion process because they did not go through the process of cytoevolution resulting in polarization of the outer cell membrane into luminal and abluminal.,"Modify,Grammar",Grammar
6762,2-9,2-9_v2_60@4,2-9_v1_57@4,"That way, they could salvage the remaining tumor cells in two ways: by secreting lactic acid <REF-56> or ATP <REF-54> (similarly to muscle cells and erythrocytes, respectively) and by initiating the vaso-mimicry.","That way, they could salvage the remaining tumor cells: by secreting lactic acid <REF-53> or ATP <REF-50> (similar to muscle cells and erythrocytes, respectively), and by initiating the vaso-mimicry.","Modify,Clarity",Clarity
6763,2-9,2-9_v2_60@6,2-9_v1_57@6,Creating the capsular vaso-mimicry would require sufficient numbers of cancer cells in the initiating nodule.,Establishing the metastatic tumors by creating the capsular vaso-mimicry required sufficient numbers of cancer cells in the initiating nodule.,"Modify,Claim",Claim
6764,2-9,2-9_v2_60@9,2-9_v1_57@9,By definition they could be referred to as cancer stem cells (CSCs).,"By definition, they could be referred to as cancer stem cells (CSCs).","Modify,Grammar",Grammar
6765,2-9,2-9_v2_60@11,2-9_v1_57@11,"If the enzyme plays a role in degradation of chromatin during the erythrogenic conversion of erythroblasts it could be associated with growth of any tissue, not only malignant.","If the enzyme plays a role in degradation of chromatin during the erythrogenic conversion of erythroblasts it could be associated with growth of any tissue, not only malignant tissue.","Modify,Clarity",Clarity
6766,2-9,2-9_v2_61@0,2-9_v1_58@0,"Eventually, the heterologous host TSCs also engaged into paracrine dialog with the tumor (via cytokines and growth factors <REF-71> ).","Eventually, the heterologous host TSCs also engaged in paracrine dialog with the tumor (via cytokines and growth factors <REF-72> ).","Modify,Grammar",Grammar
6767,2-9,2-9_v2_61@3,2-9_v1_58@3,"Such interpretation regarding the role of homologous TSCs in neo-vasculature morphogenesis was consistent with earlier reports stating that not bone marrow derived EC precursors <REF-89> , <REF-90> but TSCs from tumor microenvironment differentiated into vasculature that supported tumor growth <REF-91> .","Such interpretation regarding the role of homologous TSCs in neo-vasculature morphogenesis was consistent with earlier reports stating that it was not bone marrow-derived EC precursors <REF-83> , <REF-84> but rather TSCs from the tumor microenvironment differentiated into vasculature that supported the tumor growth <REF-85> .","Modify,Clarity",Clarity
6768,2-9,2-9_v2_62@0,2-9_v1_59@0,"On the other hand, after pseudo-orthotopic implantation, TSCs from grafted breast tissue formed vasculature for the breast tumor sooner because malignant tissues maintained some characteristics of their tissue of origin.","On the other hand, after pseudo-orthotopic implantation, TSCs from grafted breast tissue formed vasculature for the breast tumor because malignant tissues maintained some characteristics of their tissue of origin.","Modify,Clarity",Clarity
6769,2-9,2-9_v2_62@1,2-9_v1_59@1,The two related cell types were immediately ready to cooperate in executing the tissue self-organizing potential <REF-41> .,The two related cell types cooperated in executing the tissue self-organizing potential <REF-36> .,"Modify,Fact/Evidence",Fact/Evidence
6770,2-9,2-9_v2_62@3,2-9_v1_59@3,"However, in each case hematopoiesis supporting the growing tissues was extramedullar.","However, in each case, hematopoiesis supporting the growing tissues was extramedullar.","Modify,Grammar",Grammar
6771,2-9,2-9_v2_6@5,2-9_v1_6@5,"Metastases of particularly aggressive cancers of different types (ovarian <REF-10> , <REF-11> , prostate <REF-12> , <REF-13> , glioblastoma <REF-14> , as well as melanoma <REF-15> ) were associated with patterned vasculogenic mimicry, i.e., a network of periodic acid Schiff stained (glycoproteins containing <REF-16> ) ""loops"" that represented blood-containing micro-vascular ""channels"", generated by the aggressive tumor cells without participation of endothelial cells (ECs) and independently of angiogenesis <REF-17> , <REF-18> .","Metastases of particularly aggressive cancers of different types (not only melanoma <REF-10> ) were associated with patterned vasculogenic mimicry, i.e., a network of periodic acid Schiff-stained (glycoproteins containing <REF-11> ) ""loops"" that represented blood-containing micro-vascular ""channels"", generated by the aggressive tumor cells without participation of endothelial cells (ECs) and independently of angiogenesis <REF-12> , <REF-13> .","Modify,Fact/Evidence",Fact/Evidence
6772,2-9,2-9_v2_6@7,2-9_v1_6@7,"Elevated incidence of metastasis was also correlated with autophagy of internal organelles in tumor cells, although by what mechanism was not clear <REF-21> .","Elevated incidence of metastasis was also correlated with autophagy of internal organelles in tumor cells, although the mechanism behind this was unclear <REF-16> .","Modify,Clarity",Clarity
6773,2-9,2-9_v2_6@8,2-9_v1_6@8,"Reports based on different experiments suggested that depending on the context, autophagy could either stimulate or prevent cancer <REF-22> .","Reports based on a variety of experiments have suggested that, depending on the context, autophagy could either stimulate or prevent cancer <REF-17> .","Modify,Clarity",Clarity
6774,2-9,2-9_v2_6@9,2-9_v1_6@9,"Thus, the question regarding the way in which autophagy influenced metastasis remained unanswered <REF-23> .","Thus, the question regarding the way in which autophagy influences metastasis has remained unanswered <REF-18> .","Modify,Grammar",Grammar
6775,2-9,2-9_v2_6@10,2-9_v1_6@10,Two other intriguing issues were inefficiency of tumor formation in experimental settings and targeting of a selected sub-population of tumor cells by an anticancer drug.,Two other intriguing issues were the inefficiency of tumor formation in experimental settings and the targeting of a selected sub-population of tumor cells by an anticancer drug.,"Modify,Grammar",Grammar
6776,2-9,2-9_v2_6@11,2-9_v1_6@11,"(1) Theoretically a single cell could be capable of establishing the tumor but large numbers and a latent period were actually required <REF-24> , <REF-25> .","(1) Theoretically, a single cell could be capable of establishing the tumor but large numbers and a latent period were actually required <REF-19> , <REF-20> .","Modify,Grammar",Grammar
6777,2-9,2-9_v2_6@13,2-9_v1_6@13,"Those observations together with the heterogeneity of tumors, known for a long time but not fully understood <REF-25> , <REF-27> , suggested the existence of cancer stem cells (CSCs) in spite of the undifferentiated phenotype of the malignant cells.","Those observations, together with the heterogeneity of tumors known for a long time but not fully understood <REF-20> , <REF-22> , suggest the existence of cancer stem cells (CSCs) in spite of the undifferentiated phenotype of the malignant cells.","Modify,Grammar",Grammar
6778,2-9,2-9_v2_7@2,2-9_v1_7@2,"However, crossing the endothelial barrier by molecules that successfully reached the intended vascular destinations turned out to be another problem.","However, how molecules cross the endothelial barrier and successfully reach the intended vascular destination has turned out to be another problem.","Modify,Clarity",Clarity
6779,2-9,2-9_v2_7@5,2-9_v1_7@5,The issue of the tumor vessels permeability is rather perplexing.,The issue of the tumor vessels’ permeability is rather perplexing.,"Modify,Grammar",Grammar
6780,2-9,2-9_v2_7@6,2-9_v1_7@6,"On one hand the vessels prevent anticancer drugs from penetrating the tumors, and on the other they are known to be abnormally leaky <REF-39> , <REF-40> .","On the one hand, the vessels prevent anticancer drugs from penetrating the tumors, while on the other hand, they are known to be abnormally leaky <REF-34> , <REF-35> .","Modify,Clarity",Clarity
6781,2-9,2-9_v2_8@0,2-9_v1_8@0,We had observed earlier that in our model formation of the tumor vasculature (vessels and blood) could be accelerated by availability of homologous tissue stem cells (TSCs) from co-implanted graft <REF-41> .,"We had observed earlier that in our model, formation of the tumor vasculature (vessels and blood) could be accelerated by availability of homologous tissue stem cells (TSCs) from a co-implanted graft <REF-36> .","Modify,Grammar",Grammar
6782,2-9,2-9_v2_8@3,2-9_v1_8@3,"In addition to providing answers to those questions, the results shown below suggested a new cellular mechanism for initiating metastasis.","In addition to providing answers to those questions, the results shown below suggest a new cellular mechanism for initiating metastasis.","Modify,Grammar",Grammar
6783,2-9,2-9_v2_8@8,2-9_v1_9@2,"If correct, it would bring the focus of the future studies to the energy metabolism-related initial steps (as discussed elsewhere <REF-41> ) and could result in finding new ways for inhibiting some of them before the angiogenic switch has had a chance to evolve; therefore, potentially preventing the metastases.","If correct, it would bring the focus of future studies to the energy metabolism-related initial steps (as discussed elsewhere <REF-36> ) and could result in the identification of new methods of inhibiting some of these steps before the angiogenic switch has had a chance to evolve, therefore, potentially preventing the metastases.","Modify,Clarity",Clarity
6789,3-101,3-101_v2_19@6,3-101_v1_19@6,"This solution contains transcendental functions (sines and cosines), which are computable to any desired precision.","But this solution contains transcendental functions (sines and cosines), which are not computable and therefore must be replaced by computable approximations, e.g. power-series expansions.","Modify,Claim",Claim
6790,3-101,3-101_v2_19@7,3-101_v1_19@7,"However, when three or more celestial bodies are included in the model, no analytical solution is available and the differential equations must be approximated by finite difference equations <REF-12> .","When three or more celestial bodies are included in the model, no analytical solution is available and the differential equations must be approximated by finite difference equations <REF-12> .","Modify,Clarity",Clarity
6791,3-101,3-101_v2_21@4,3-101_v1_21@4,"The very fact that a program runs and produces results proves that the model specification is complete and unambiguous, assuming that the computing system itself (hardware, operating system, compiler, etc.) works correctly and that the programming language it is written in has clearly defined semantics (which, unfortunately, is not the case for widely used languages such as C <REF-15> .","The very fact that a program runs and produces results proves that the model specification is complete and unambiguous, assuming that the computing system itself (hardware, operating system, compiler, etc.) works correctly.","Modify,Fact/Evidence",Fact/Evidence
6792,3-101,3-101_v2_25@1,3-101_v1_25@1,"I use the term “tool” in a general sense that includes both physical objects (e.g. microscopes, lasers, etc.) and mathematical theorems or procedures (e.g. calculus or algebra), but not mathematical axioms and definitions, which form the language of mathematics rather than its toolbox.","I use the term “tool” in a general sense that includes both physical objects (e.g. microscopes, lasers, etc.) and mathematical theorems or procedures (e.g. calculus or algebra).","Modify,Fact/Evidence",Fact/Evidence
6793,3-101,3-101_v2_29@4,3-101_v1_28@10,The role of such heuristics in computational science remains to be clarified.,The role of such probabilistic heuristics in computational science remains to be clarified.,"Modify,Clarity",Clarity
6794,3-101,3-101_v2_45@5,3-101_v1_44@5,This doesn’t mean that knowledge is lost rapidly.,This doesn’t mean of course that knowledge is lost rapidly.,"Modify,Clarity",Clarity
6795,3-101,3-101_v2_56@3,3-101_v1_55@3,"In particular, tools that are very similar in spirit to today’s computer algebra systems can be used to create approximations and combinations of scientific models.","In particular, tools that very similar in spirit to today’s computer algebra systems can be used to create approximations and combinations of scientific models.","Modify,Grammar",Grammar
6796,3-101,3-101_v2_60@0,3-101_v1_59@0,"The specificity of floating-point arithmetic deserves a special discussion, both because of its central role in much of scientific software and because of its reputation of being the source of intractable problems.","The specificity of floating-point arithmetic deserve a special discussion, both because of its central role in much of scientific software and because of its reputation of being the source of intractable problems.","Modify,Grammar",Grammar
6797,3-101,3-101_v2_2@4,3-101_v1_2@4,"As a consequence, these crucial pieces of information no longer enter the scientific record.","As a consequence, these crucial pieces of information have disappeared from the scientific record.","Modify,Clarity",Clarity
9293,7-407,7-407_v2_24@0,7-407_v1_21@0,"Excluded records were studies where the psychophysiological variables were analysed only after and not before the stimuli presentations ( Jin et al. , 2013 ) and with an unusual procedure ( Tressoldi et al. , 2015 ), i.e. using heart rate feedback to inform a voluntary decision to predict random positive or negative events.","Excluded records were studies were the psychophysiological variables were analysed only after and not before the stimuli presentations ( Jin et al. , 2013 ) and with an unusual procedure ( Tressoldi et al. , 2015 ), i.e. using heart rate feedback to inform a voluntary decision to predict random positive or negative events.","Modify,Grammar",Grammar
9294,7-407,7-407_v2_7@0,7-407_v1_4@2,The statistical estimation of the publication bias by using the Copas selection model suggest that the main findings are not contaminated by publication bias.,The statistical estimation of the publication bias by using the Copas model suggest that the main findings are not contaminated by publication bias.,"Modify,Clarity",Clarity
9295,7-407,7-407_v2_29@1,7-407_v1_26@1,The database along with all 19 papers are available from Tressoldi (2017) .,The database along with all 18 papers are available from Tressoldi (2017) .,"Modify,Fact/Evidence",Fact/Evidence
9296,7-407,7-407_v2_36@0,7-407_v1_33@0,"In order to control the reliability of the results, a second analysis was carried out by using a multilevel approach as suggested by Assink & Wibbelink (2016) implemented with the metafor package ( Viechtbauer, 2010 ) and reported in the Table S2 in the Supplementary File 3 .","In order to control the reliability of the results, a second analysis was carried out by using a multilevel approach as suggested by ( Assink & Wibbelink, 2016 ) implemented with the metafor package ( Viechtbauer, 2010 ) and reported in the Table S2 in the Supplementary File 3 .","Modify,Clarity",Clarity
9297,7-407,7-407_v2_37@0,7-407_v1_34@0,"A Bayesian meta-analysis was implemented with the brms package ( Bürkner, 2017 ).","The Bayesian meta-analysis was implemented with the brms package ( Bürkner, 2017 ).","Modify,Grammar",Grammar
9298,7-407,7-407_v2_52@0,7-407_v1_50@0,Another “sensitivity analysis” was carried out excluding the new Mossbridge and Tressoldi studies in order to control whether different authors could obtain similar results.,Another “sensitivity analysis” was carried out excluding the Mossbridge and the Tressoldi studies in order to control whether different authors could obtain similar results.,"Modify,Clarity",Clarity
9299,7-407,7-407_v2_55@0,7-407_v1_53@0,"Both the frequentist and the Bayesian analyses support the evidence of an overall main effect of approximately .28, and a small difference between the peer and non-peer reviewed studies.","Both the frequentist and the Bayesian analyses support the evidence of an overall main effect of approximately .29, and a small difference between the peer and non-peer reviewed studies.","Modify,Fact/Evidence",Fact/Evidence
9300,7-407,7-407_v2_65@2,7-407_v1_57@1,"Anyway, we applied the Copas selection model which is recommended by Jin et al . (2015) .",We hence applied the Copas selection model which is recommended by Jin et al . (2015) .,"Modify,Clarity",Clarity
9301,7-407,7-407_v2_10@3,7-407_v1_7@3,"Now imagine if such prognosticating ability was possible without any sensory or other inferential cues (see Mossbridge & Radin, 2018 for a review).",Now imagine if such prognosticating ability was possible without any sensory or other inferential cues.,"Modify,Fact/Evidence",Fact/Evidence
9302,7-407,7-407_v2_69@0,7-407_v1_62@0,"This update of the Mossbridge et al. (2012) meta-analysis related to the so called predictive anticipatory activity (PAA) responses to future random stimuli, covers the period January 2008- July 2018.","This update of the Mossbridge et al. (2012) meta-analysis related to the so called predictive anticipatory activity (PAA) responses to future random stimuli, covers the years 2008- October 2017.","Modify,Fact/Evidence",Fact/Evidence
9303,7-407,7-407_v2_69@1,7-407_v1_62@1,"Overall, we found 19 new studies describing a total of 36 effect sizes.","Overall, we found 18 new studies describing a total of 34 effect sizes.","Modify,Fact/Evidence",Fact/Evidence
9304,7-407,7-407_v2_0@0,7-407_v1_0@0,Predictive physiological anticipatory activity preceding seemingly unpredictable stimuli: An update of Mossbridge et al ’s meta-analysis,Predictive physiological anticipation preceding seemingly unpredictable stimuli: An update of Mossbridge et al ’s meta-analysis,"Modify,Clarity",Clarity
9305,7-407,7-407_v2_72@0,7-407_v1_65@0,"Furthermore, we did not find substantial differences between peer and non-peer reviewed papers as in the original paper, as the confidence intervals of their mean effect size, overlap considerably.","Furthermore, we did not find substantial differences between peer and not-peer reviewed papers as in the original paper.","Modify,Fact/Evidence",Fact/Evidence
9306,7-407,7-407_v2_74@0,7-407_v1_69@0,This update confirms the main results reported in Mossbridge et al . (2012) original meta-analysis and gives further support to the hypothesis of predictive physiological anticipatory activity of future random events.,This update confirms the main results reported in Mossbridge et al . (2012) original meta-analysis and gives further support to the hypothesis of predictive physiological anticipation of future random events.,"Modify,Clarity",Clarity
9307,7-407,7-407_v2_75@0,7-407_v1_70@0,The limitations of the present meta-analysis are similar to most meta-analyses which include non-preregistered studies.,"The limitations of the present meta-analysis are similar to most meta-analyses which include non pre-registered studies that cannot be controlled for the degree of freedoms in the methodology and data analysis in the course of their implementations, making them prone, for example, to the so-called “questionable research practices” ( John et al ., 2012 ).","Modify,Claim",Claim
9308,7-407,7-407_v2_75@1,7-407_v1_71@0,"The solution is that of prospective meta-analyses ( Watt & Kennedy, 2017 ), based on all preregistered studies where the methods and data analyses have been declared and made public beforehand.","The solution is that of prospective meta-analyses ( Watt & Kennedy, 2017 ), based on preregistered studies where the methods and data analyses have been declared and made public beforehand.","Modify,Clarity",Clarity
9309,7-407,7-407_v2_10@7,7-407_v1_7@7,"Disturbingly, moments before the stimulus is presented there are physiological changes ahead of time.","Disturbingly, moments before the stimulus is presented there are murmurings of activity, as if the body is predicting moments ahead of time.","Modify,Claim",Claim
9310,7-407,7-407_v2_11@1,7-407_v1_8@1,"In both of these approaches it is difficult to envision mundane strategies that might explain the anomalous pre-stimulus effects observed, and indeed, Mossbridge et al ., went to significant lengths in refuting the leading candidate – expectancy effects, both in the 2012 meta-analysis and in post-review exchanges with sceptical psychologists and physiologists.","In both of these approaches it is difficult to envision mundane strategies that might explain the anomalous pre-stimulus effects observed, and indeed, Mossbridge et al , went to significant lengths in refuting the leading candidate – expectancy effects, both in the 2012 meta-analysis and in post-review exchanges with sceptical psychologists and physiologists.","Modify,Grammar",Grammar
9311,7-407,7-407_v2_11@4,7-407_v1_8@4,The presentiment hypothesis calls for a difference between the pre-stimulus responses of the two stimulus categories and this is calculated across sessions.,The presentiment hypothesis calls for a difference between arousing and neutral pre-stimulus responses and this is calculated across sessions.,"Modify,Clarity",Clarity
9312,7-407,7-407_v2_12@0,7-407_v1_9@0,"Because of the high profile nature of Mossbridge et al ., (over 93,000 views as of January 2018) there has been a good number of replications in the few years since publication.","Because of the high profile nature of Mossbridge et al , (over 93,000 views as of January 2018) there has been a good number of replications in the few years since publication.","Modify,Grammar",Grammar
9313,7-407,7-407_v2_12@3,7-407_v1_9@3,"Because expectancy effects have been proposed as a potential mechanism to explain at least some of the presentiment effect, it is noteworthy that several experiments in this fresh cohort of studies tackle this head on by only analysing the first trial of a run.","Because expectancy effects have been forwarded to explain at least some of the presentiment effect, it is noteworthy that several experiments in this fresh cohort of studies tackle this head on by only analysing the first trial of a run.","Modify,Claim",Claim
9314,7-407,7-407_v2_12@6,7-407_v1_9@6,This provides another objective measure of the validity of the presentiment effect.,This provides a second objective measure of the validity of the presentiment effect.,"Modify,Clarity",Clarity
9315,7-407,7-407_v2_14@0,7-407_v1_11@0,"The whole procedure followed both the APA Meta-Analysis Reporting Standards ( APA Publications and Communications Board Working Group on Journal Article Reporting Standards, 2008 ), the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols (PRISMA) 2015 ( Moher et al ., 2015 ) and the reporting standards for literature searches and report inclusion ( Atkinson et al ., 2015 ).","The whole procedure followed both the APA Meta-Analysis Reporting Standards ( APA Publications and Communications Board Working Group on Journal Article Reporting Standards, 2008 ), the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols 2015 ( Moher et al ., 2015 ) and the reporting standards for literature searches and report inclusion ( Atkinson et al ., 2015 ).","Modify,Clarity",Clarity
9316,7-407,7-407_v2_17@0,7-407_v1_14@0,It is important to point out that these eligibility criteria are different from those used by Mossbridge et al. Those authors selected only studies where the anticipatory signals mirrored the post-stimulus ones.,It is important to point out that these eligibility criteria are different from those used by Mossbridge et al. Those authors selected only studies were the anticipatory signals mirrored the post-stimulus ones.,"Modify,Grammar",Grammar
9317,7-407,7-407_v2_17@1,7-407_v1_14@1,In addition we included all studies that used anticipatory signals to predict future events independently of the presence of post-stimulus physiological signals.,Differently we included all studies that used anticipatory signals to predict future events independently of the presence of post-stimulus physiological signals.,"Modify,Clarity",Clarity
9318,7-407,7-407_v2_17@2,7-407_v1_14@2,"For example, some authors, e.g. Mossbridge (2015) used heart rate variability to predict winning i.e. $4, versus losing outcomes without recording the post-stimulus physiological activity associated with hits and misses.","For example, some authors, e.g. Mossbridge (2015) used heart rate variability to predict winning i.e. $4, versus losing outcomes.","Modify,Fact/Evidence",Fact/Evidence
9319,7-407,7-407_v2_19@1,7-407_v1_16@1,"Furthermore, we emailed a request of the data of completed studies to all authors we knew were involved in this type of research.","Furthermore, we emailed a request of the data of completed studies to all authors we knew were involved in this type of investigations.","Modify,Clarity",Clarity
9320,7-407,7-407_v2_19@3,7-407_v1_16@3,"We searched all completed studies, both peer reviewed and non-peer reviewed, e.g. Ph.D dissertations, from January 2008 to June 2018.","We searched all completed studies, both peer reviewed and non-peer reviewed, e.g. Ph.D dissertations, from January 2008 to October 2017.","Modify,Fact/Evidence",Fact/Evidence
10762,9-1032,9-1032_v2_20@9,9-1032_v1_20@9,Culex larvae possess a siphon on the posterior part of their abdomen for breathing whereas Anopheles larvae have no siphon and rest horizontal to the water body <REF-29> .,Culex larvae possess siphon on the posterior part of their abdomen for breathing through at the interface of the water surface during resting whereas Anopheles larvae have no siphon and rest horizontal to the water body <REF-28> .,"Modify,Fact/Evidence",Fact/Evidence
10763,9-1032,9-1032_v2_20@13,9-1032_v1_20@13,Rearing of the field collected larvae was done in 1 L plastic containers.,"Rearing of the field collected larvae was done in 1 L plastic rectangle food mate (H67 × W126 × L184 mm, Kenpoly manufacturer, Nairobi, Kenya).","Modify,Fact/Evidence",Fact/Evidence
10764,9-1032,9-1032_v2_20@20,9-1032_v1_20@20,"Extraction of DNA was done for each mosquito separately using Tissue Kit (Quagen, GmbH Hilden, Germany).","Extraction of genomic DNA was done for each mosquito separately using Tissue Kit (Quagen, GmbH Hilden, Germany).","Modify,Clarity",Clarity
10765,9-1032,9-1032_v2_20@21,9-1032_v1_20@21,"The PCR was prepared by mixing PCR mix of 2 µl of 5XHot Firepol Blended Master Mix (Ready to Load), primers (0.5 µM each), DNA template (2 µl) and nuclease-free water (5 µl).","The PCR in a 10 µl (per sample) was prepared by mixing PCR mix of 2 µl of 5XHot Firepol Blended Master Mix (Ready to Load), primers (0.5 µM each), DNA template (2 µl) and nuclease-free water (5 µl).","Modify,Fact/Evidence",Fact/Evidence
10766,9-1032,9-1032_v2_20@23,9-1032_v1_20@23,"We used a Kyratec Thermal Cycler (SC300T-R2, Australia) for the thermal reactions.","We used Kyratec Thermal Cycler (SC300T-R2, Australia) for the thermal reactions.","Modify,Grammar",Grammar
10767,9-1032,9-1032_v2_21@2,9-1032_v1_21@2,"Vegetation coverage was estimated visually, always by the same field worker, as the proportion of the habitats covered with vegetations and categorized as (1) 1–25% (2) 25–50% (3) 50–75% (4) 75–100%.",Vegetation coverage was estimated visually as the proportion of the habitats covered with vegetations and categorized as (1) 1–25% (2) 25–50% (3) 50–75% (4) 75–100%.,"Modify,Fact/Evidence",Fact/Evidence
10768,9-1032,9-1032_v2_21@4,9-1032_v1_21@4,"The graminoid plants were identified to family using the morphology of their leaves (two or three-ranked; open or closed sheaths), and their stem type (three-sided or round; hollow or solid) using Revuelta <REF-33> .","The graminoid plants were identified to family using morphology of their leaves (two or three-ranked; open or closed sheaths), and their stem type (three-sided or round; hollow or solid) using Revuelta <REF-32> .","Modify,Grammar",Grammar
10769,9-1032,9-1032_v2_34@0,9-1032_v1_34@0,All the swamp habitats were bordered by graminoid plants along the water edges and had a high surface coverage.,All the swamp habitats were boarded by graminoid plants along the water edges and had a high surface coverage.,"Modify,Grammar",Grammar
10770,9-1032,9-1032_v2_4@8,9-1032_v1_4@8,"The presence of early instar larvae was significantly and positively associated with swamp habitat types (OR=22, 95% CI=6-86, P<0.001) and abundance of late Anopheles larvae (OR=359, CI=33-3941, P<0.001), and negatively associated with the presence of tadpoles (OR=0.1, CI=0.0.01-0.5, P=0.008).","The presence of early instar larvae was significantly and positively associated with swamp habitat types (OR=22, 95% CI=6-86, P<0.001) and abundance of late Anopheles larvae (OR=359, CI=33-3941, P<0.001), whilst the association was negative with tadpole presence (OR=0.1, CI=0.0.01-0.5, P=0.008).","Modify,Clarity",Clarity
10771,9-1032,9-1032_v2_42@3,9-1032_v1_42@3,All six species of Anopheles mosquitoes were recorded in swamp habitats.,These six species of Anopheles mosquitoes were recorded in swamp habitats.,"Modify,Clarity",Clarity
10772,9-1032,9-1032_v2_42@5,9-1032_v1_42@5,"However, only three species of Anopheles mosquitoes ( An. arabiensis , An. ziemanni, and An. pharoensis ) were collected in habitats sparsely (1–25%) covered by graminoid plants.","However, only three species of Anopheles mosquitoes ( An. arabiensis , An. ziemanni, and An. pharoensis ) were collected in habitats sparsely (1–25%) covered by graminoids.","Modify,Clarity",Clarity
10773,9-1032,9-1032_v2_59@0,9-1032_v1_59@0,The presence and abundance of early instar Anopheles larvae were negatively associated with the presence of tadpoles.,The presence and abundance of early instar Anopheles larvae was negatively associated with the presence of tadpoles.,"Modify,Grammar",Grammar
10774,9-1032,9-1032_v2_62@3,9-1032_v1_62@3,"The habitats covered by these vegetations were abundantly colonized by early instar Anopheles larvae even though no specific preference for any of these could be detected, likely due to study limitations.","The habitats covered by this vegetation were abundantly colonized by early instar Anopheles larvae even though no specific preference for any of these could be detected, likely due to study limitations.","Modify,Grammar",Grammar
10775,9-1032,9-1032_v2_19@1,9-1032_v1_19@1,"The perimeter of every habitat was estimated, always by the same field worker for uniformity, by walking in large steps around the habitat.",The perimeter of every habitat was estimated by walking in large steps around the habitat.,"Modify,Fact/Evidence",Fact/Evidence
10817,9-113,9-113_v2_20@7,9-113_v1_25@7,In the second configuration the parameter “gamma” was additionally set to the value “auto”.,"In the second configuration the parameter ""gamma"" was additionally set to the value ""auto"".","Modify,Grammar",Grammar
10818,9-113,9-113_v2_20@9,9-113_v1_25@9,"In the second configuration, early stopping was additionally activated, where 10% of the training data was separated as validation data.","In the second configuration, early stopping was additionally activated, where 10% of the training data is separated as validation data.","Modify,Grammar",Grammar
10819,9-113,9-113_v2_20@10,9-113_v1_25@10,"If the error of the validation data did not improve by more than 0.001 over ten training epochs, the training is stopped early to avoid overtraining.","If the error of the validation data does not improve by more than 0.001 over ten training epochs, the training is stopped early to avoid overtraining.","Modify,Grammar",Grammar
10820,9-113,9-113_v2_20@14,9-113_v1_25@14,This resulted in a total of seven different machine learning configurations.,This results in a total of seven different machine learning configurations.,"Modify,Grammar",Grammar
10821,9-113,9-113_v2_26@0,9-113_v1_31@0,First of all a working Miniconda/Anaconda installation is needed.,First of all you need a working Miniconda/Anaconda installation.,"Modify,Clarity",Clarity
10822,9-113,9-113_v2_26@1,9-113_v1_31@1,Miniconda can be downloaded at https://conda.io/en/latest/miniconda.html .,You can get Miniconda at https://conda.io/en/latest/miniconda.html .,"Modify,Clarity",Clarity
10823,9-113,9-113_v2_27@0,9-113_v1_32@0,"Now an environment named ""ml_pka"" with all needed dependencies can be created and activated with:","Now you can create an environment named ""ml_pka"" with all needed dependencies and activate it with:","Modify,Clarity",Clarity
10824,9-113,9-113_v2_29@0,9-113_v1_34@0,"Alternatively, a new environment can be created manually without the environment.yml file:",You can also create a new environment by yourself and install all dependencies without the environment.yml file:,"Modify,Claim",Claim
10825,9-113,9-113_v2_36@1,9-113_v1_41@1,To use the data preparation pipeline the repository folder hast to be entered and the created conda environment must be activated.,To use the data preparation pipeline you have to be in the repository folder and your conda environment have to be activated.,"Modify,Clarity",Clarity
10826,9-113,9-113_v2_36@2,9-113_v1_41@2,Additionally the Marvin <REF-10> commandline tool cxcalc and the QUACPAC <REF-26> commandline tool tautomers have to be added to the PATH variable.,Additionally the Marvin <REF-10> commandline tool cxcalc and the QUACPAC <REF-28> commandline tool tautomers have to be set in your PATH variable.,"Modify,Clarity",Clarity
10827,9-113,9-113_v2_37@0,9-113_v1_42@0,"Also the environment variables OE_LICENSE (containing the path to the OpenEye license file) and JAVA_HOME (referring to the Java installation folder, which is needed for cxcalc ) have to be set.","Also the environment variables OE_LICENSE (containing the path to your OpenEye license file) and JAVA_HOME (referring to the Java installation folder, which is needed for cxcalc ) have to be set.","Modify,Clarity",Clarity
10828,9-113,9-113_v2_38@0,9-113_v1_43@0,After preparation a small usage information can be displayed with bash run_pipeline.sh -h .,After preparation you can display a small usage information with bash run_pipeline.sh -h .,"Modify,Clarity",Clarity
10829,9-113,9-113_v2_40@1,9-113_v1_45@1,First of all the repository folder has to be entered and the created conda environment must be activated.,First of all you have to be in the repository folder and your conda environment have to be activated.,"Modify,Clarity",Clarity
10830,9-113,9-113_v2_40@2,9-113_v1_45@2,To use the prediction tool the machine learning model has to be retrained.,To use the prediction tool you have to retrain the machine learning model.,"Modify,Clarity",Clarity
10831,9-113,9-113_v2_40@3,9-113_v1_45@3,"To do so the training script should be called, it will train the 5-fold cross-validated Random Forest machine learning model using 12 cpu cores.","Therefore just call the training script, it will train the 5-fold cross-validated Random Forest machine learning model using 12 cpu cores.","Modify,Clarity",Clarity
10832,9-113,9-113_v2_40@4,9-113_v1_45@4,If the number of cores has to be adjusted the train_model.py can be edited by changing the value of the variable EST_JOBS .,If you want to adjust the number of cores you can edit the train_model.py by changing the value of the variable EST_JOBS .,"Modify,Clarity",Clarity
10833,9-113,9-113_v2_42@0,9-113_v1_47@0,To use the prediction tool with the trained model QUACPAC/Tautomers have to be available as mentioned in the section above.,To use the prediction tool with the trained model QUACPAC/Tautomers have to be available as it was mentioned in the section above.,"Modify,Clarity",Clarity
10834,9-113,9-113_v2_43@0,9-113_v1_48@0,Now the python script can be called with an SDF file and an output path:,Now you can call the python script with an SDF file and an output path:,"Modify,Clarity",Clarity
10835,9-113,9-113_v2_62@1,9-113_v1_52@1,"In terms of the mean absolute error, a random forest with scaled MorganFeatures (radius=3) and descriptors gave the best performing model (MAE=0.682, RMSE=1.032, r 2 =0.82).","In terms of the mean absolute error, a random forest with scaled MorganFeatures (radius=3) and descriptors gives the best performing model (MAE=0.682, RMSE=1.032, r 2 =0.82).","Modify,Grammar",Grammar
10836,9-113,9-113_v2_71@0,9-113_v1_52@4,"This showed that our model had a slightly better performance than Marvin for the LiteratureCompilation, but Marvin performed better for the Novartis dataset.","This shows that our model slightly outcompetes Marvin for the LiteratureCompilation, but Marvin performs better for the Novartis dataset.","Modify,Clarity",Clarity
10837,9-113,9-113_v2_71@1,9-113_v1_52@5,"For both data sets, our models <REF-17> had a better predictive performance than the OPERA tool.","For both data sets, our models <REF-17> have a better predictive performance than the OPERA tool.","Modify,Grammar",Grammar
10838,9-113,9-113_v2_4@6,9-113_v1_4@6,"In particular, the publication by Williams et al . <REF-15> makes use of a publicly available data set provided by the application DataWarrior <REF-16> and provides a freely available pK a prediction tool called OPERA.","In particular, the publication by Williams et al . <REF-15> make use of a publicly available data set provided by the application DataWarrior <REF-16> and provides a freely available pK a prediction tool called OPERA.","Modify,Grammar",Grammar
10839,9-113,9-113_v2_48@0,9-113_v1_8@0,"One crucial point in the field of pK a measurements (and its usage for pK a predictions) was linked to the different experimental methods <REF-25> , <REF-30> .","One crucial point in the field of pK a measurements (and its usage for pK a predictions) is linked to the different experimental methods <REF-25> , <REF-26> .","Modify,Grammar",Grammar
10840,9-113,9-113_v2_48@1,9-113_v1_8@1,"Based on the Novartis set, the correlation between capillary electrophoresis and potentiometric measurements (for 15 data points) was convincing enough (mean absolute error (MAE)=0.202, root mean squared error (RMSE)=0.264, correlation coefficient r 2 =0.981) for us to combine pK a measurements from these different experimental methods (see Figure 3 ).","Based on the Novartis set, the correlation between capillary electrophoresis and potentiometric measurements (for 15 data points) is convincing enough (mean absolute error (MAE)=0.202, root mean squared error (RMSE)=0.264, correlation coefficient r 2 =0.981) for us to combine pK a measurements from these different experimental methods (see Figure 1 ).","Modify,Grammar",Grammar
10841,9-113,9-113_v2_52@0,9-113_v1_11@0,We also compared the pK a values of 187 monoprotic molecules contained in both the ChEMBL and DataWarrior data sets.,"We also compare the overlap of the filtered (see next section) ChEMBL and DataWarrior data sets, 187 monoprotic molecules could be identified in both sources.","Modify,Fact/Evidence",Fact/Evidence
10842,9-113,9-113_v2_52@1,9-113_v1_11@1,"Due to the missing annotation, it remained unclear if different experimental methods were used or multiple measurements with the same experimental method have been performed (or a mixture of both).","Due to the missing annotation, it remains unclear if different experimental methods were used or multiple measurements with the same experimental method have been performed (or a mixture of both).","Modify,Grammar",Grammar
10843,9-113,9-113_v2_52@2,9-113_v1_11@2,"Either way, this comparison was an additional proof-of-concept that the ChEMBL and DataWarrior pK a data sources can be combined after careful curation.","Either way, this comparison is an additional proof-of-concept that the ChEMBL and DataWarrior pK a data sources can be combined after careful curation.","Modify,Grammar",Grammar
10844,9-113,9-113_v2_52@4,9-113_v1_11@4,"The correlation coefficient between the annotated pK a values for these two data sets r 2 was 0.949, the MAE was 0.275, and the RMSE was 0.576.","The correlation coefficient between the annotated pK a values for these two data sets r 2 is 0.949, the MAE is 0.275, and the RMSE is 0.576.","Modify,Grammar",Grammar
10845,9-113,9-113_v2_73@4,9-113_v1_58@0,The good performance of Marvin on the Novartis set is interesting to note: the RMSE was almost 0.4 units better than our top performing model.,The good performance of Marvin on the Novartis set is interesting to note: the RMSE is almost 0.4 units better than our top performing model.,"Modify,Grammar",Grammar
10846,9-113,9-113_v2_73@7,9-113_v1_58@3,"In contrast, Marvin performed slightly worse than our top model on the LiteratureCompilation.","In contrast, Marvin performs slightly worse than our top model on the LiteratureCompilation.","Modify,Grammar",Grammar
10847,9-113,9-113_v2_73@8,9-113_v1_58@4,The OPERA tool performed significantly worse than our model on both external test sets.,The OPERA tool performs significantly worse than our model on both external test sets.,"Modify,Grammar",Grammar
10848,9-113,9-113_v2_73@9,9-113_v1_58@5,We assume that the addition of 2470 ChEMBL pK a – datapoints to our training set which were not part of the OPERA training set led to this drop in predictive performance.,We assume that the addition of 2470 ChEMBL pKa-datapoints to our training set which are not part of the OPERA training set leads to this drop in predictive performance.,"Modify,Grammar",Grammar
10849,9-113,9-113_v2_73@10,9-113_v1_58@6,"In addition, the pre-processing of the data was performed differently by OPERA in comparison to our pre-processing procedure.","In addition, the pre-processing of the data is performed differently by OPERA in comparison to our pre-processing procedure.","Modify,Grammar",Grammar
10850,9-113,9-113_v2_8@1,9-113_v1_15@1,"The following restrictions were made: it must be a physicochemical assay, the measurements must be taken from scientific literature, the assay must be in “small-molecule physicochemical format” and the organism taxonomy must be set to “N/A”.","The following restrictions were made: it must be a physicochemical assay, the measurements must be taken from scientific literature, the assay must be in ""small-molecule physicochemical format"" and the organism taxonomy must be set to ""N/A"".","Modify,Grammar",Grammar
10851,9-113,9-113_v2_8@4,9-113_v1_15@4,"Only pK a measurements, i.e. ChEMBL activities, were taken into account that were specified as exact (“standard_relation” equals “=”) and for which one of the following names was specified as “standard_type”: “pka”, “pka value”, “pka1”, “pka2”, “pka3” or “pka4” (case-insensitive).","Only pKa measurements, i.e. ChEMBL activities, were taken into account that were specified as exact (""standard_relation"" equals ""="") and for which one of the following names was specified as ""standard_type"": ""pka"", ""pka value"", ""pka1"", ""pka2"", ""pka3"" or ""pka4"" (case-insensitive).","Modify,Grammar",Grammar
10852,9-113,9-113_v2_11@2,9-113_v1_18@2,- Filtering by Lipinski‘s rule of five (one violation allowed),- Filter by Lipinski‘s rule of five (one violation allowed),"Modify,Grammar",Grammar
10853,9-113,9-113_v2_13@2,9-113_v1_20@2,All molecules were then combined on the basis of the canonical isomeric SMILES.,All molecules were then combined on the basis of the isomeric SMILES.,"Modify,Clarity",Clarity
10854,9-113,9-113_v2_20@0,9-113_v1_25@0,"First, to simplify cross-validation, a class “CVRegressor” was defined, which can serve as a wrapper for any regressor implementing the Scikit-Learn <REF-27> interface.","First, to simplify cross-validation, a class ""CVRegressor"" was defined, which can serve as a wrapper for any regressor implementing the Scikit-Learn <REF-29> interface.","Modify,Fact/Evidence",Fact/Evidence
10855,9-113,9-113_v2_20@2,9-113_v1_25@2,"Next, 196 of the 200 available RDKit descriptors (“MaxPartialCharge”, “MinPartialCharge”, “MaxAbsPartialCharge” and “MinAbsPartialCharge” were not used because they are computed as “NaN” for many molecules), and a 4096-bit long MorganFeature fingerprint with radius 3 were calculated for the training data set.","Next, 196 of the 200 available RDKit descriptors (""MaxPartialCharge"", ""MinPartialCharge"", ""MaxAbsPartialCharge"" and ""MinAbsPartialCharge"" were not used because they are computed as ""NaN"" for many molecules), and a 4096-bit long MorganFeature fingerprint with radius 3 were calculated for the training data set.","Modify,Grammar",Grammar
