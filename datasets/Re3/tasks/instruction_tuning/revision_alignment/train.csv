edit_index,doc_name,node_ix_src,node_ix_tgt,text_src,text_tgt,label
328,120-ARR,120-ARR_v2_28@8,120-ARR_v1_28@2,"They then defined a class of competent datasets, where the marginal probability for every feature is uniform over the class label, i.e., for any feature x i and label y ∈ Y , p(y|x i ) = 1 |Y | , thus limiting models from picking up any correlation between single features and output labels.","They then defined a class of competent datasets, where the marginal probability for every feature is uniform over the class label, i.e., ∀x ∈ X , y ∈ Y, p(y|x) = 1 |Y | , thus limiting models from picking up any correlation between single features and output labels.",yes
329,120-ARR,120-ARR_v2_32@0,120-ARR_v1_32@0,"Consider the toy dataset for the task of sentiment analysis shown in Tab. 3, with vocabulary V ={good, bad, not, very}, and label set Y = { +, − }.","Consider the toy dataset for the task of sentiment analysis shown in Tab. 3, with vocabulary V ={good,bad,not,very}, and label set Y ={+,− }.",yes
330,120-ARR,120-ARR_v2_35@2,120-ARR_v1_33@2,"In this case, the model would make the wrong prediction for the first test example in Test (as it has learned that very good is a feature that indicates positive sentiment), and similarly, will make a random prediction for the second test example, which does not contain any two-word feature seen during training.","In this case, the model would make the wrong prediction for the first test example in Test (as it has learned that very good is a feature that indicates positive sentiment), and similarly, will make a random prediction for the second test example, which does not contain any two-word features seen during training.",yes
331,120-ARR,120-ARR_v2_43@0,120-ARR_v1_42@0,"Combining the two observations, we are left with the question of the potential intersection between balancing too much and balancing too little: does a sweet spot exist for which no spurious correlations are found in the dataset, but enough learnable signal is left? And even if so, would a balancing algorithm, whether by augmentation or filtering, be able to find it? We leave these questions for future work, but note that addressing them is a prerequisite for the theoretical and practical application of dataset balancing for mitigating spurious correlations.","Combining the two observations, we are left with the question of the potential intersection between balancing too much and balancing too little: does a sweet spot exist for which no spurious correlations are found in the dataset, but enough learnable signal is left? And even if so, would a balancing algorithm (whether by augmentation or filtering) be able to find it? We leave these questions for future work, but note that addressing them is a prerequisite for the theoretical and practical application of dataset balancing for mitigating spurious correlations.",yes
332,120-ARR,120-ARR_v2_2@0,120-ARR_v1_2@0,"Recent work has shown that deep learning models in NLP are highly sensitive to lowlevel correlations between simple features and specific output labels, leading to overfitting and lack of generalization.","Recent work have shown that deep learning models in NLP are highly sensitive to low-level correlations between simple features and specific output labels, leading to overfitting and lack of generalization.",yes
333,120-ARR,120-ARR_v2_46@0,120-ARR_v1_44@1,"The practice of dataset balancing is designed to prevent models from learning that some words or expressions have a common fallback meaning that can stem from dataset artifacts (e.g., ""cat"" as an indicator of contradiction) but also from cultural and historical contexts (e.g., Biden is the U.S. president in 2022).","The practice of dataset balancing is designed to prevent models from learning that some words or expressions have a common fallback meaning that can stem from dataset idiosyncrasies (e.g., ""cat"" as an indicator of contradiction) but also from cultural and historical contexts (e.g., Biden is the U.S. president).",yes
334,120-ARR,120-ARR_v2_46@3,120-ARR_v1_44@4,We argue that the ability to use them is a central ability of language understanding.,Here we argue that the ability to use them is a central ability of language understanding.,yes
335,120-ARR,120-ARR_v2_6@0,120-ARR_v1_6@0,"The year 2019 Donald Trump The West Wing, season 1 Josiah ""Jed"" Bartlet Table 1: Context, whether explicit or implicit, matters in textual understanding, as exemplified by the question ""who is the president of the U.S.?"". E.g., in the first line, given no other context, a QA system should provide the most sensible fallback answer (Joe Biden, at the time of writing).","The year 2019 Donald Trump The West Wing, season 1 Josiah ""Jed"" Bartlet Table 1: Context, whether explicit or implicit, matters in textual understanding, as exemplified by the question ""who is the president of the U.S.?"". E.g., in the first line, given no other context, a QA system should provide the most sensible fallback answer (Joe Biden).",yes
336,120-ARR,120-ARR_v2_65@1,120-ARR_v1_61@2,"Interestingly, three different interpretation methods (simple gradient visualization, Simonyan et al., 2014;integrated gradient visualization, Sundararajan et al., 2017;and SmoothGrad, Smilkov et al., 2017) all find the word ""great"" or ""surprise"" to be one of the three most influential words on the model's prediction.","Interestingly, three different interpretation methods (simple gradient visualization, Simonyan et al., 2014;integrated gradient visualization, Sundararajan et al., 2017;and SmoothGrad, Smilkov et al., 2017) all find the word ""great"" to be one of the two most influential words on the model's prediction.",yes
337,120-ARR,120-ARR_v2_63@2,120-ARR_v1_62@1,"The ability to detect when is it reasonable to make an educated guess is an important property of an intelligent agent, and an exciting research question.","The ability to detect when is it reasonable and when it is not to make an educated guess is an important property of an intelligent agent, and an exciting research question.",yes
338,120-ARR,120-ARR_v2_68@1,120-ARR_v1_64@1,A naive way to mitigate spurious correlations is to stop using large-scale datasets altogether.,A naive way to eliminate spurious correlations is to stop using large-scale datasets altogether.,yes
339,120-ARR,120-ARR_v2_68@2,120-ARR_v1_64@2,"We echo recent calls and argue that for supervised learning (i.e., large-scale fine-tuning), recent advances in zero-and few-shot learning might make this option possible.","We argue that for supervised learning (i.e., large-scale fine-tuning), recent advances in zero-and few-shot learning might make this option possible.",yes
340,120-ARR,120-ARR_v2_69@0,120-ARR_v1_65@0,"Large pretrained models such as T5 or GPT-3 (Brown et al., 2020), trained on vast amounts of data, arguably learn enough about the world to acquire many of the skills currently learned through supervised learning.","Large pretrained models such as T5 or GPT-3 (Brown et al., 2020), trained on vast amounts of data, arguably learn enough about the world to acquire many of the skills currently learned through supervised datasets.",yes
341,120-ARR,120-ARR_v2_69@1,120-ARR_v1_65@1,"Indeed, the large increase in the size and capacity of pretrained language models has led to a new wave of few-shot and zero-shot methods Shin et al., 2020;Gu et al., 2022), which are able to reach human-level performance on certain tasks using only a few dozens of training examples .","Indeed, the large increase in the size and capacity of pretrained language models has led to a new wave of few-shot and zero-shot methods Shin et al., 2020;Gu et al., 2021), which are able to reach human-level performance on certain tasks using only a few dozens of training examples .",yes
342,120-ARR,120-ARR_v2_71@1,120-ARR_v1_67@1,"It seems plausible that excelling in language modeling tasks requires mastering the skills that stand in the base of many NLP tasks, such as sentiment analysis, syntactic parsing, and NER.","It seems possible the excelling in language modeling tasks requires mastering the skills that stand in the base of many NLP tasks, such as sentiment analysis, syntactic parsing, and NER.",yes
343,120-ARR,120-ARR_v2_71@2,120-ARR_v1_67@2,"However, it is similarly plausible that this is not the case for other tasks, e.g., summarization, simplification and dialogue.","However, it is similarly possible that this is not the case for other tasks, e.g., summarization, simplification and dialogue.",yes
344,120-ARR,120-ARR_v2_71@3,120-ARR_v1_67@3,"We are cautious in making concrete recommendations for which tasks to apply this principle, but suggest the following intuitive rule of thumb: for datasets or tasks for which the state of the art is close to or surpasses the human baseline, we should consider moving to few-shot setups.","We are cautious to make concrete recommendations for which tasks to apply this principle, but suggest the following intuitive rule of thumb: for datasets or tasks for which the state of the art is close to or surpasses the human baseline, we should consider moving to few-shot setups.",yes
345,120-ARR,120-ARR_v2_72@2,120-ARR_v1_68@2,"We suggest that instead of building large training sets and small validation and test sets, authors should consider building large test sets, as a means for achieving improved statistical power (Card et al., 2020).","We suggest instead of building large training sets and small validation and tests, authors should consider building large test sets, as a means for achieving improved statistical power (Card et al., 2020).",yes
346,120-ARR,120-ARR_v2_77@0,120-ARR_v1_73@0,This paper discusses the arms-race between models and datasets.,This paper discussed the arms-race between models and datasets.,yes
347,120-ARR,120-ARR_v2_78@0,120-ARR_v1_74@0,"Raji et al. ( 2021) recently criticized the concept of benchmarks as a whole, arguing that they are only capturing specific skills and not ""general"" capabilities.","Finally, Raji et al. ( 2021) recently criticized the concept of benchmarks as a whole, arguing that they are only capturing specific skills and not ""general"" capabilities.",yes
348,120-ARR,120-ARR_v2_78@1,120-ARR_v1_74@1,"Our paper raises related concerns about training sets implicitly containing spurious correlations, and suggests reconsidering the practice of building large-scale training sets.","Our paper raises relates concerns about training sets implicitly containing spurious correlations, and suggests reconsidering the practice of building large-scale training sets.",yes
351,120-ARR,120-ARR_v2_9@7,120-ARR_v1_9@6,"Finally, we question the basic procedure of large-scale fine-tuning, and suggest focusing on zero-and few-shot learning instead .","We conclude by questioning the basic procedure of large-scale fine-tuning, and suggest focusing on zero-and few-shot learning.",yes
352,120-ARR,120-ARR_v2_13@1,120-ARR_v1_14@1,"One approach is to add examples in order to balance the dataset (Goyal et al., 2017;Sharma et al., 2018;Hudson and Manning, 2019).","One approach, popular in visual question answering datasets, is to add examples in order to balance the dataset (Goyal et al., 2017;Hudson and Manning, 2019).",yes
353,120-ARR,120-ARR_v2_14@0,120-ARR_v1_15@0,Filtering as balancing A complementary balancing approach to augmentation is filtering examples out from datasets such that spurious correlations are minimized.,Filtering as balancing A complementary approach to augmentation is filtering examples out from datasets such that spurious correlations are minimized.,yes
354,120-ARR,120-ARR_v2_14@3,120-ARR_v1_15@3,"The AF approach and similar approaches were picked up by many datasets such as ReCoRD (Zhang et al., 2018), DROP (Dua et al., 2019), HellaSWAG (Zellers et al., 2019), αNLI , and WinoGrande .","The AF approach was picked up by many follow-up datasets such as DROP (Dua et al., 2019), HellaSWAG (Zellers et al., 2019), and Wino-Grande .",yes
355,120-ARR,120-ARR_v2_15@1,120-ARR_v1_16@1,"As this approach relies on an external model, applying it with ever stronger models with higher capacity, will allow these models to pick up on subtler correlations (Li et al., 2021).","As this approach relies on an external model, applying it with ever stronger models with higher capacity, will allow these models to pick up on subtler correlations.",yes
356,120-ARR,120-ARR_v2_17@0,120-ARR_v1_18@0,"Evidently, a similar trend emerges for the previously mentioned datasets: (1) the first baselines, reflecting the state of the art at the time of dataset creation, perform relatively poorly, e.g., 59% on SWAG, 47% on ReCoRD, 47 F1 on DROP, 47% on HellaSWAG, 69% on αNLI, and 79% on Wino-Grande; (2) model developers introduce increasingly larger and heavily-parameterized models, hill-climbing on these datasets; and eventually (3) models essentially solve the dataset within a year or two, often outperforming humans: 86% on SWAG (Devlin et al., 2019), 94% on ReCoRD (He et al., 2021b), 88 F1 on DROP (Chen et al., 2020), 93% on HellaSWAG (He et al., 2021b), 92% on αNLI (He et al., 2021a), and 90% on Wino-Grande .","Evidently, a similar trend emerges for the previously mentioned datasets: (1) the first baselines, reflecting the state of the art at the time of dataset creation, perform poorly, e.g., 52% accuracy on SWAG, 47 F1 on DROP, 47% on Hel-laSWAG, and 53% AUC on WinoGrande; (2) model developers introduce increasingly larger and heavily-parameterized models, hill-climbing on these datasets; and eventually (3) models essentially solve the dataset within a year or two, often outperforming humans: 86% on SWAG (Devlin et al., 2019), 90 F1 on DROP , 93% on HellaSWAG (He et al., 2020), and 88% AUC on WinoGrande .",yes
,120-ARR,120-ARR_v2_27@2,120-ARR_v1_62@2,The basic definition is a set of features that are correlated but not causally related.,8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_27@2,120-ARR_v1_11@2,The basic definition is a set of features that are correlated but not causally related.,"For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_28@5,120-ARR_v1_62@2,"This definition does not address the nature of the feature (genuine or not), but does make an implicit assumption that such features are of high importance (e.g., high pointwise mutual information values with the corresponding label; Gururangan et al., 2018).",8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_28@5,120-ARR_v1_11@2,"This definition does not address the nature of the feature (genuine or not), but does make an implicit assumption that such features are of high importance (e.g., high pointwise mutual information values with the corresponding label; Gururangan et al., 2018).","For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_28@6,120-ARR_v1_62@2,"This definition is no longer subjective in terms of the genuineness of the feature, but is still subjective in the level of effect on generalizability (i.e., what is a high value of PMI?).",8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_28@6,120-ARR_v1_11@2,"This definition is no longer subjective in terms of the genuineness of the feature, but is still subjective in the level of effect on generalizability (i.e., what is a high value of PMI?).","For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_51@2,120-ARR_v1_62@2,"We argue that doing so is a desired strategy in many cases (though a preferred strategy might be to interact of abstain from making a decisive prediction, see Sec. 4.2).",8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_51@2,120-ARR_v1_11@2,"We argue that doing so is a desired strategy in many cases (though a preferred strategy might be to interact of abstain from making a decisive prediction, see Sec. 4.2).","For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_52@0,120-ARR_v1_62@2,We also acknowledge that correlations in the real world can be misleading.,8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_52@0,120-ARR_v1_11@2,We also acknowledge that correlations in the real world can be misleading.,"For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_52@1,120-ARR_v1_62@2,"For instance, people often mistake the biggest commercial city in some countries for their capital (e.g., Istanbul in Turkey), potentially due to the high correlation between the two.",8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_52@1,120-ARR_v1_11@2,"For instance, people often mistake the biggest commercial city in some countries for their capital (e.g., Istanbul in Turkey), potentially due to the high correlation between the two.","For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_52@2,120-ARR_v1_62@2,"In such cases, relying on the fallback option might lead to prediction error.",8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_52@2,120-ARR_v1_11@2,"In such cases, relying on the fallback option might lead to prediction error.","For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_52@3,120-ARR_v1_62@2,"However, we argue that following the human strategy of relying on a fallback option in cases of uncertainty will promote models' communication abilities.",8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_52@3,120-ARR_v1_11@2,"However, we argue that following the human strategy of relying on a fallback option in cases of uncertainty will promote models' communication abilities.","For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_62@0,120-ARR_v1_62@2,"7 Though we should continually assess the challenge negation poses on the most recent models (Bowman, 2022).",8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_62@0,120-ARR_v1_11@2,"7 Though we should continually assess the challenge negation poses on the most recent models (Bowman, 2022).","For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_62@1,120-ARR_v1_62@2,"8 We recognize that editing pretrained corpora poses significant challenges due to their immense size, as demonstrated by recent efforts such as corpus analysis and deduplication (Lee et al., 2022).",8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_62@1,120-ARR_v1_11@2,"8 We recognize that editing pretrained corpora poses significant challenges due to their immense size, as demonstrated by recent efforts such as corpus analysis and deduplication (Lee et al., 2022).","For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_63@0,120-ARR_v1_62@2,9 See Abend and Rappoport (2017) for a survey.,8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_63@0,120-ARR_v1_11@2,9 See Abend and Rappoport (2017) for a survey.,"For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_79@0,120-ARR_v1_62@2,"Finally, concurrent to this work, Eisenstein (2022) discussed several types of spurious correlations in the context of causality theory (Pearl, 2009), and used a toy example to demonstrate their different effects on models.",8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_79@0,120-ARR_v1_11@2,"Finally, concurrent to this work, Eisenstein (2022) discussed several types of spurious correlations in the context of causality theory (Pearl, 2009), and used a toy example to demonstrate their different effects on models.","For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_79@1,120-ARR_v1_62@2,"They concluded that domain knowledge is required to identify the correlations that are indeed spurious, i.e., those that might challenge the generalization ability of models.",8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,no
,120-ARR,120-ARR_v2_79@1,120-ARR_v1_11@2,"They concluded that domain knowledge is required to identify the correlations that are indeed spurious, i.e., those that might challenge the generalization ability of models.","For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).",no
,120-ARR,120-ARR_v2_43@0,120-ARR_v1_6@0,"Combining the two observations, we are left with the question of the potential intersection between balancing too much and balancing too little: does a sweet spot exist for which no spurious correlations are found in the dataset, but enough learnable signal is left? And even if so, would a balancing algorithm, whether by augmentation or filtering, be able to find it? We leave these questions for future work, but note that addressing them is a prerequisite for the theoretical and practical application of dataset balancing for mitigating spurious correlations.","The year 2019 Donald Trump The West Wing, season 1 Josiah ""Jed"" Bartlet Table 1: Context, whether explicit or implicit, matters in textual understanding, as exemplified by the question ""who is the president of the U.S.?"". E.g., in the first line, given no other context, a QA system should provide the most sensible fallback answer (Joe Biden).",no
598,129-ARR,129-ARR_v2_23@0,129-ARR_v1_23@0,Region-aware Sampling for Active Learning on High-uncertainty Data,Region-aware Sampling for Active,yes
599,129-ARR,129-ARR_v2_24@1,129-ARR_v1_24@1,"However, directly sampling the most uncertain samples gives suboptimal results as it tends to query repetitive data (Ein-Dor et al., 2020) that represent the overall data distribution poorly.","However, directly sampling the most uncertain samples gives suboptimal result since uncertainty-based sampling tends to query repetitive data (Ein-Dor et al., 2020) and results in poor representativeness of the overall data distribution.",yes
600,129-ARR,129-ARR_v2_25@2,129-ARR_v1_25@2,Denote by K the number of clusters and v (t) i = BERT(x i ) the representation of x i from the penultimate layer of BERT.,Denote K the number of clusters and v (t) i = BERT(x i ) the representation of x i from the penultimate layer of BERT.,yes
601,129-ARR,129-ARR_v2_25@3,129-ARR_v1_25@3,"The weighted K-means process first initializes the center of each each cluster µ i (1 ≤ i ≤ K) via K-Means++ (Arthur and Vassilvitskii, 2007).","The weighted K-means first initializes the center of each each cluster µ i (1 ≤ i ≤ K) via K-Means++ (Arthur and Vassilvitskii, 2007).",yes
602,129-ARR,129-ARR_v2_43@0,129-ARR_v1_41@0,"Then, we rank the clusters in an ascending order in u (t) k .","Then, we rank the clusters in an ascending order according to u (t) k .",yes
603,129-ARR,129-ARR_v2_43@1,129-ARR_v1_41@1,"A high score indicates high uncertainty of the model in these regions, and we need to actively annotate the member instances to reduce uncertainty and improve the model's performance.","A high score indicates high uncertainty of the model in these regions, and we need to actively annotate the associated instances to reduce uncertainty and improve the model's performance.",yes
606,129-ARR,129-ARR_v2_53@0,129-ARR_v1_49@0,"Although the model is quite 'confident' on the class of x when we only consider the result of the round t, it gives contradictory predictions over these two consecutive rounds, which indicates that the model is actually uncertain to which class x belongs.","Although the model is quite 'confident' on the class of x when we only consider the result of the round t, it gives contradictory predictions over these two consecutive rounds, which indicates that the model is still uncertain to which class x belongs.",yes
607,129-ARR,129-ARR_v2_4@2,129-ARR_v1_4@1,"However, there are still significant gaps between few-shot and fully-supervised PLM fine-tuning in many tasks.","However, there are still significant gaps between few-shot and fully-supervised PLM fine-tuning in many classification tasks.",yes
608,129-ARR,129-ARR_v2_2@0,129-ARR_v1_2@0,"While pre-trained language model (PLM) finetuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data.","Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data.",yes
609,129-ARR,129-ARR_v2_4@3,129-ARR_v1_4@2,"Besides, the performance of few-shot PLM finetuning can be sensitive to different sets of training data (Bragg et al., 2021).","Besides, the performance of few-shot PLM fine-tuning can vary substantially with different sets of training data (Bragg et al., 2021).",yes
610,129-ARR,129-ARR_v2_4@4,129-ARR_v1_4@3,"Therefore, there is a crucial need for approaches that make PLM finetuning more label-efficient and robust to selection of training data, especially for applications where labeled data are scarce and expensive to obtain.","Therefore, there is a crucial need for PLM fine-tuning approaches with better label-efficiency and being robust to selection of training data, especially for applications where labeled data are scarce and expensive to obtain.",yes
611,129-ARR,129-ARR_v2_74@0,129-ARR_v1_67@0,Weakly-supervised Learning,Extension to Weakly-supervised Learning,yes
612,129-ARR,129-ARR_v2_75@0,129-ARR_v1_68@0,"ACTUNE can be naturally used for weaklysupervised classification, where X l is a set of noisy labels derived from linguistic patterns or rules.","ACTUNE can be naturally extended to weaklysupervised classification, where X l is a set of data annotated by linguistic patterns or rules.",yes
613,129-ARR,129-ARR_v2_75@1,129-ARR_v1_68@1,"Since the initial label set is noisy, the model trained with Eq. ( 1) can overfit the label noise (Zhang et al., 2022a), and we can actively query labeled data to refine the model.","Since the initial label set is noisy, then the model trained with Eq. ( 1) will overfit to the label noise, and we can actively query labeled data to refine the model.",yes
614,129-ARR,129-ARR_v2_75@6,129-ARR_v1_70@2,We believe it is because CAL requires clean labels to calculate uncertainties.,We argue it is because CAL requires clean labels to calculate uncertainties.,yes
615,129-ARR,129-ARR_v2_5@0,129-ARR_v1_5@0,"Towards this goal, researchers have recently resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020;Margatina et al., 2021a,b;Yuan et al., 2020).","Towards this goal, researchers have resorted to active fine-tuning of PLMs and achieved comparable performance to fully-supervised methods with much less annotated samples (Ein-Dor et al., 2020;Margatina et al., 2021a,b;Yuan et al., 2020).",yes
616,129-ARR,129-ARR_v2_0@0,129-ARR_v1_0@0,ACTUNE: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models,ACTUNE: Uncertainty-Aware Active Self-Training for Active Fine-Tuning of Pretrained Language Models,yes
617,129-ARR,129-ARR_v2_85@1,129-ARR_v1_79@1,"Active learning has been widely applied to various NLP tasks (Yuan et al., 2020;Shelmanov et al., 2021;Karamcheti et al., 2021).","Active learning has been widely applied to various NLP tasks (Yuan et al., 2020;Zhao et al., 2020;Shelmanov et al., 2021;Karamcheti et al., 2021).",yes
618,129-ARR,129-ARR_v2_85@2,129-ARR_v1_79@2,"So far, AL methods can be categorized into uncertainty-based methods (Gal et al., 2017;Margatina et al., 2021a,b), diversity-based methods (Ru et al., 2020;Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020;Ash et al., 2020).","So far, AL methods can be categorized into uncertainty-based methods (Gal et al., 2017;Margatina et al., 2021a,b), diversitybased methods (Ru et al., 2020;Sener and Savarese, 2018) and hybrid methods (Yuan et al., 2020;Ash et al., 2020;Kirsch et al., 2019).",yes
621,129-ARR,129-ARR_v2_85@12,129-ARR_v1_79@10,"It first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability.","Self-training first generates pseudo labels for high-confidence samples, then fits a new model on pseudo labeled data to improve the generalization ability (Rosenberg et al., 2005;Lee, 2013).",yes
622,129-ARR,129-ARR_v2_86@0,129-ARR_v1_80@0,Conclusion and Discussion,Conclusion,yes
623,129-ARR,129-ARR_v2_5@4,129-ARR_v1_5@3,"Moreover, they usually rely on pseudo-labeling to utilize unlabeled data, which requires greater (yet often absent) care to denoise the pseudo labels, otherwise the errors could accumulate and hurt model performance.","Moreover, they usually rely on pseudo-labeling to utilize unlabeled data, which requires greater (yet often absent) care to denoise the pseudo labels, otherwise the errors could accumulate and deteriorate the model performance.",yes
624,129-ARR,129-ARR_v2_5@5,129-ARR_v1_5@4,"This issue can be even more severe for PLMs, as the fine-tuning process is often sensitive to different weight initialization and data orderings (Dodge et al., 2020).","This phenomenon can be even more severe for PLMs, as the finetuning process often suffers from the instability issue caused by different weight initialization and data orders (Dodge et al., 2020).",yes
625,129-ARR,129-ARR_v2_88@5,129-ARR_v1_87@4,"Last, apart from the text classification task, we can also extend our work into other tasks such as sequence labeling and natural language inference (NLI).","Last, apart from the text classification task, we can also extend our work into other tasks such as sequence labeling and natural language inference.",yes
626,129-ARR,129-ARR_v2_2@1,129-ARR_v1_2@1,"Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data.","Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data.",yes
627,129-ARR,129-ARR_v2_83@0,129-ARR_v1_88@0,We give an example of our querying strategy on AG News dataset for the 1st round of active selftraining process in figure 6.,Here we give an example of our querying strategy on AG News and Pubmed dataset for the 1st round of active self-training process in figure 6.,yes
628,129-ARR,129-ARR_v2_83@2,129-ARR_v1_88@3,"We can see that the existing uncertainty-based methods such as Entropy and CAL, are suffered from the issue of limited diversity.","From the comparision, we can see that the existing uncertainty based methods such as Entropy and CAL, are suffered from the issue of limited diversity.",yes
629,129-ARR,129-ARR_v2_6@0,129-ARR_v1_6@0,"To tackle the above challenges, we propose AC-TUNE, a new method that improves the label efficiency and robustness of active PLM fine-tuning.","To tackle above challenges, we propose AC-TUNE, a new method that improves the label efficiency and robustness of active PLM fine-tuning with self-training.",yes
630,129-ARR,129-ARR_v2_6@2,129-ARR_v1_7@1,"Different from existing AL methods that only leverage uncertainty for querying labels, our uncertainty-driven self-training paradigm gradually leverages unlabeled data with low uncertainty via self-training, while reducing the chance of error propagation triggered by highly-uncertain mislabeled data.","Different from existing AL methods that only leverage uncertainty for querying labels, our uncertaintydriven self-training paradigm gradually unleash the data with low uncertainty via self-training, while reducing the chance of error propagation triggered by highly-uncertain mis-labeled data.",yes
631,129-ARR,129-ARR_v2_7@0,129-ARR_v1_8@0,"To further boost model performance for AC-TUNE, we design two techniques to improve the query strategy and suppress label noise, namely region-aware sampling (RS) and momentum-based memory bank (MMB).","To further boost the performance on downstream tasks, we design two techniques, namely regionaware sampling (RS) and momentum-based memory bank (MMB) to improve the query strategies and suppress label noise for ACTUNE.",yes
632,129-ARR,129-ARR_v2_7@1,129-ARR_v1_8@1,"Inspired by the fact that existing uncertainty-based AL methods often end up with choosing uncertain yet repetitive data (Ein-Dor et al., 2020;Margatina et al., 2021b), we design the region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs.","Inspired by the fact that existing uncertainty-based AL methods often end up choosing uncertain yet repetitive data (Ein-Dor et al., 2020;Margatina et al., 2021b), we design a region-aware sampling technique to promote both diversity and representativeness by leveraging the representation power of PLMs.",yes
633,129-ARR,129-ARR_v2_2@2,129-ARR_v1_2@2,"We propose ACTUNE, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning.","We develop ACTUNE, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self training.",yes
634,129-ARR,129-ARR_v2_8@2,129-ARR_v1_9@2,"However, previous approaches only select pseudo-labeled data based on the prediction of the current round and are thus less reliable.","However, previous approaches only select pseudo-labeled data based on the prediction of the current round and therefore are less reliable.",yes
635,129-ARR,129-ARR_v2_8@5,129-ARR_v1_9@5,"As a result, only the samples with high prediction confidence over multiple rounds will be used for self-training, which mitigates the issue of label noise.","As a consequence, only the samples with high prediction confidence over multiple rounds will be used for self-training, which mitigates the issue of label noise.",yes
636,129-ARR,129-ARR_v2_8@6,129-ARR_v1_9@6,"We highlight that our active self-training approach is an efficient substitution to existing AL methods, requiring little extra computational cost.","We highlight that our active self-training approach is an efficient substitution to existing AL methods, requiring ignorable extra computational cost.",yes
637,129-ARR,129-ARR_v2_9@0,129-ARR_v1_10@0,Our key contributions are: (1) an active selftraining paradigm ACTUNE that integrates selftraining and active learning to minimize the labeling cost for fine-tuning PLMs; (2) a region-aware querying strategy to enforce both the informativeness and the diversity of queried samples during AL; (3) a simple and effective momentum-based method to leverage the predictions in preceding rounds to alleviate the label noise in self-training and (4) experiments on 6 benchmarks demonstrating ACTUNE improves the label efficiency over existing self-training and active learning baselines by 56.2%.,Our key contributions are: (1) an active selftraining paradigm ACTUNE that integrates the benefit of self-training and active learning in a principled way to minimize the labeling cost for finetuning PLMs; (2) a region-aware querying strategy to enforce both the informativeness and the diversity of queried samples during AL; (3) a simple and effective momentum-based method to harness the predictions for preceding rounds to alleviate the label noise in self-training and (4) experiments on 6 benchmarks demonstrating ACTUNE improves the label efficiency over existing self-training and active learning baselines by 56.2%.,yes
638,129-ARR,129-ARR_v2_2@3,129-ARR_v1_2@3,ACTUNE switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and lowuncertainty ones for model self-training.,"AC-TUNE switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from lowuncertainty regions are used for model selftraining.",yes
639,129-ARR,129-ARR_v2_14@1,129-ARR_v1_15@1,"Here X = X l ∪ X u denotes all samples, and Y = {1, 2, • • • , C} is the label set where C is the number of classes.","Here X = X l ∪ X u denotes all samples and Y = {1, 2, • • • , C} is the label set, where C is the number of classes.",yes
640,129-ARR,129-ARR_v2_18@0,129-ARR_v1_19@0,"In round t (1 ≤ t ≤ T ) of active self-training, we first calculate the uncertainty score based on a given function a (t) i = a(x i , θ (t) ) 1 for all x i ∈ X u .","In round t (1 ≤ t ≤ T ) of the active self-training procedure, we first calculate the uncertainty score based on a given function a (t) i = a(x i , θ (t) ) 1 for all x i ∈ X u .",yes
641,129-ARR,129-ARR_v2_2@4,129-ARR_v1_2@4,"Under this framework, we design (1) a regionaware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training.","Additionally, we design (1) a regionaware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model's pseudo labels to suppress label noise in self-training.",yes
,129-ARR,129-ARR_v2_85@4,129-ARR_v1_79@6,"Very recently, there are also several works attempted to query labeling functions for weakly-supervised learning (Boecking et al., 2020;Hsieh et al., 2022;Zhang et al., 2022b).","Gao et al. (2020); Song et al. (2019); Guo et al. (2021) design query strategies for specific semi-supervised methods, Tomanek and Hahn (2009); Rottmann et al. (2018); Siméoni et al. (2020) exploit the mostcertain samples from the unlabeled with pseudolabeling to augment the training set.",no
,129-ARR,129-ARR_v2_88@0,129-ARR_v1_84@3,There are several directions to improve ACTUNE.,"For ST (Lee, 2013), CEAL (Wang et al., 2016) & BASS (Rottmann et al., 2018), it uses a threshold δ for selecting high-confidence data.",no
,129-ARR,129-ARR_v2_70@6,129-ARR_v1_63@3,"We choose RoBERTabase (Liu et al., 2019) from the HuggingFace codebase (Wolf et al., 2020) as the backbone for AC-TUNE and all baselines except for Pubmed and Chemprot, where we use SciBERT (Beltagy et al., 2019), a BERT model pre-trained on scientific cor-pora.","For self-training, the size of the memory bank g(x; θ) is proportional to |X u |, while the extra computation of maintaining this dictionary is ignorable since we do not inference over the unlabeled data for multiple times in each round as BALD (Gal et al., 2017) does.",no
,129-ARR,129-ARR_v2_2@6,129-ARR_v1_83@0,Our implementation is available at https://github.,"In our experiments, we keep β = 0.5, λ = 1 for all datasets.",no
,129-ARR,129-ARR_v2_85@11,129-ARR_v1_82@2,"Self-training is one of the earliest and simplest approaches to semi-supervised learning (Lee, 2013).",All results are reported as the average over three runs.,no
,129-ARR,129-ARR_v2_85@11,129-ARR_v1_63@1,"Self-training is one of the earliest and simplest approaches to semi-supervised learning (Lee, 2013).",Complexity Analysis.,no
,129-ARR,129-ARR_v2_82@0,129-ARR_v1_85@0,Case Study,C Runtime Analysis.,no
,129-ARR,129-ARR_v2_70@4,129-ARR_v1_83@0,"For weakly-supervised text classification, since the datasets are much smaller, we keep the labeling budget and the size of development set to b = 500.","In our experiments, we keep β = 0.5, λ = 1 for all datasets.",no
,129-ARR,129-ARR_v2_2@6,129-ARR_v1_79@13,Our implementation is available at https://github.,"Note that although Mukherjee and Awadallah (2020); Rizve et al. ( 2021) also leverage uncertainty information for self-training, their focus is on developing better self-training methods, while we aim to jointly query high-uncertainty samples and generate pseudo-labels for low-uncertainty samples.",no
,129-ARR,129-ARR_v2_82@0,129-ARR_v1_55@1,Case Study,"For methods that do not directly use prediction for uncertainty estimation, we aggregate the uncertainty value as g(x; θ (t) ) = m t ×a(x; θ (t) )+(1−m t )×g(x; θ (t−1) ).",no
,129-ARR,129-ARR_v2_85@11,129-ARR_v1_83@3,"Self-training is one of the earliest and simplest approaches to semi-supervised learning (Lee, 2013).","For AC-TUNE with Entropy, we use probability based aggregation and for ACTUNE with CAL, we use value based aggregation by default.",no
,129-ARR,129-ARR_v2_70@1,129-ARR_v1_82@1,"Following (Yuan et al., 2020), we set the number of rounds T = 10, the overall budget for all datasets b = 1000 and the initial size of the labeled |X l | is set to 100.","Specifically, we search T 1 from 10 to 2000, T 2 from 1000 to 5000, T 3 from 10 to 500, ξ from 0 to 1, and λ from 0 to 0.5.",no
,129-ARR,129-ARR_v2_85@4,129-ARR_v1_84@5,"Very recently, there are also several works attempted to query labeling functions for weakly-supervised learning (Boecking et al., 2020;Hsieh et al., 2022;Zhang et al., 2022b).","For UST (Mukherjee and Awadallah, 2020), we tune the number of lowuncertainty samples used in the next round from [1024,2048,4096].",no
,129-ARR,129-ARR_v2_70@0,129-ARR_v1_84@7,Active Learning Setups.,"For REVIVAL (Guo et al., 2021), it calculates uncertainty with adversarial perturbation, we tune the size of the perturbation from [1e − 3, 1e − 4, 1e − 5].",no
,129-ARR,129-ARR_v2_82@0,129-ARR_v1_82@1,Case Study,"Specifically, we search T 1 from 10 to 2000, T 2 from 1000 to 5000, T 3 from 10 to 500, ξ from 0 to 1, and λ from 0 to 0.5.",no
,129-ARR,129-ARR_v2_85@4,129-ARR_v1_85@0,"Very recently, there are also several works attempted to query labeling functions for weakly-supervised learning (Boecking et al., 2020;Hsieh et al., 2022;Zhang et al., 2022b).",C Runtime Analysis.,no
,129-ARR,129-ARR_v2_88@4,129-ARR_v1_84@1,"Moreover, we can explore more advanced uncertainty estimation approach (Kong et al., 2020) into ACTUNE to further improve the performance.","Note that Entropy (Holub et al., 2008), BALD (Gal et al., 2017), ALPS (Yuan et al., 2020), BADGE (Ash et al., 2020) do not introduce any new hyperparameters.",no
,129-ARR,129-ARR_v2_85@4,129-ARR_v1_83@2,"Very recently, there are also several works attempted to query labeling functions for weakly-supervised learning (Boecking et al., 2020;Hsieh et al., 2022;Zhang et al., 2022b).","Specifically, we search γ from [0.5, 0.6, 0.7], m L from [0.6, 0.7, 0.8], m H from [0.8, 0.9, 1].",no
,129-ARR,129-ARR_v2_70@5,129-ARR_v1_86@3,Implementation Details.,"This is because we use FAISS (Johnson et al., 2019) instead of SKLearn (Pedregosa et al., 2011) for clustering, which accelerates the clustering step significantly.",no
,129-ARR,129-ARR_v2_70@6,129-ARR_v1_84@2,"We choose RoBERTabase (Liu et al., 2019) from the HuggingFace codebase (Wolf et al., 2020) as the backbone for AC-TUNE and all baselines except for Pubmed and Chemprot, where we use SciBERT (Beltagy et al., 2019), a BERT model pre-trained on scientific cor-pora.","For CAL (Margatina et al., 2021b), we tune the number for KNN k from [5,10,20] and report the best performance.",no
,129-ARR,129-ARR_v2_70@2,129-ARR_v1_79@6,"In each AL round, we sample a batch of 100 samples from the unlabeled set X u and query their labels.","Gao et al. (2020); Song et al. (2019); Guo et al. (2021) design query strategies for specific semi-supervised methods, Tomanek and Hahn (2009); Rottmann et al. (2018); Siméoni et al. (2020) exploit the mostcertain samples from the unlabeled with pseudolabeling to augment the training set.",no
,129-ARR,129-ARR_v2_70@4,129-ARR_v1_84@0,"For weakly-supervised text classification, since the datasets are much smaller, we keep the labeling budget and the size of development set to b = 500.","For other SSAL methods, we mainly tune their key hyperparameters.",no
,129-ARR,129-ARR_v2_85@11,129-ARR_v1_88@2,"Self-training is one of the earliest and simplest approaches to semi-supervised learning (Lee, 2013).",Different colors stands for different classes.,no
,129-ARR,129-ARR_v2_88@0,129-ARR_v1_55@1,There are several directions to improve ACTUNE.,"For methods that do not directly use prediction for uncertainty estimation, we aggregate the uncertainty value as g(x; θ (t) ) = m t ×a(x; θ (t) )+(1−m t )×g(x; θ (t−1) ).",no
,129-ARR,129-ARR_v2_70@4,129-ARR_v1_79@12,"For weakly-supervised text classification, since the datasets are much smaller, we keep the labeling budget and the size of development set to b = 500.","To alleviate this, we adopt a simple momentumbased method to select high confidence samples, effectively reducing the pseudo labels noise for active learning.",no
,129-ARR,129-ARR_v2_70@0,129-ARR_v1_84@4,Active Learning Setups.,"We tune δ from [0.6, 0.7, 0.8, 0.9] to report the best performance.",no
,129-ARR,129-ARR_v2_88@4,129-ARR_v1_79@14,"Moreover, we can explore more advanced uncertainty estimation approach (Kong et al., 2020) into ACTUNE to further improve the performance.","The experiments in Sec. 3 show that with appropriate querying methods, ACTUNE can further improve the performance of self-training.",no
,129-ARR,129-ARR_v2_88@4,129-ARR_v1_84@7,"Moreover, we can explore more advanced uncertainty estimation approach (Kong et al., 2020) into ACTUNE to further improve the performance.","For REVIVAL (Guo et al., 2021), it calculates uncertainty with adversarial perturbation, we tune the size of the perturbation from [1e − 3, 1e − 4, 1e − 5].",no
,129-ARR,129-ARR_v2_70@6,129-ARR_v1_82@1,"We choose RoBERTabase (Liu et al., 2019) from the HuggingFace codebase (Wolf et al., 2020) as the backbone for AC-TUNE and all baselines except for Pubmed and Chemprot, where we use SciBERT (Beltagy et al., 2019), a BERT model pre-trained on scientific cor-pora.","Specifically, we search T 1 from 10 to 2000, T 2 from 1000 to 5000, T 3 from 10 to 500, ξ from 0 to 1, and λ from 0 to 0.5.",no
,129-ARR,129-ARR_v2_85@11,129-ARR_v1_84@0,"Self-training is one of the earliest and simplest approaches to semi-supervised learning (Lee, 2013).","For other SSAL methods, we mainly tune their key hyperparameters.",no
,129-ARR,129-ARR_v2_2@6,129-ARR_v1_84@4,Our implementation is available at https://github.,"We tune δ from [0.6, 0.7, 0.8, 0.9] to report the best performance.",no
,129-ARR,129-ARR_v2_70@6,129-ARR_v1_84@1,"We choose RoBERTabase (Liu et al., 2019) from the HuggingFace codebase (Wolf et al., 2020) as the backbone for AC-TUNE and all baselines except for Pubmed and Chemprot, where we use SciBERT (Beltagy et al., 2019), a BERT model pre-trained on scientific cor-pora.","Note that Entropy (Holub et al., 2008), BALD (Gal et al., 2017), ALPS (Yuan et al., 2020), BADGE (Ash et al., 2020) do not introduce any new hyperparameters.",no
,129-ARR,129-ARR_v2_70@0,129-ARR_v1_82@2,Active Learning Setups.,All results are reported as the average over three runs.,no
,129-ARR,129-ARR_v2_2@6,129-ARR_v1_88@2,Our implementation is available at https://github.,Different colors stands for different classes.,no
,129-ARR,129-ARR_v2_85@4,129-ARR_v1_84@2,"Very recently, there are also several works attempted to query labeling functions for weakly-supervised learning (Boecking et al., 2020;Hsieh et al., 2022;Zhang et al., 2022b).","For CAL (Margatina et al., 2021b), we tune the number for KNN k from [5,10,20] and report the best performance.",no
,129-ARR,129-ARR_v2_70@4,129-ARR_v1_84@6,"For weakly-supervised text classification, since the datasets are much smaller, we keep the labeling budget and the size of development set to b = 500.","For COSINE (Yu et al., 2021), we set the weight for confidence regularization λ as 0.1, the threshold τ for selecting high-confidence data from [0.7, 0.9] and the update period of selftraining from [50,100,150].",no
,129-ARR,129-ARR_v2_2@6,129-ARR_v1_82@1,Our implementation is available at https://github.,"Specifically, we search T 1 from 10 to 2000, T 2 from 1000 to 5000, T 3 from 10 to 500, ξ from 0 to 1, and λ from 0 to 0.5.",no
,129-ARR,129-ARR_v2_70@2,129-ARR_v1_84@2,"In each AL round, we sample a batch of 100 samples from the unlabeled set X u and query their labels.","For CAL (Margatina et al., 2021b), we tune the number for KNN k from [5,10,20] and report the best performance.",no
,129-ARR,129-ARR_v2_70@0,129-ARR_v1_85@0,Active Learning Setups.,C Runtime Analysis.,no
,129-ARR,129-ARR_v2_70@1,129-ARR_v1_86@3,"Following (Yuan et al., 2020), we set the number of rounds T = 10, the overall budget for all datasets b = 1000 and the initial size of the labeled |X l | is set to 100.","This is because we use FAISS (Johnson et al., 2019) instead of SKLearn (Pedregosa et al., 2011) for clustering, which accelerates the clustering step significantly.",no
831,136-ARR,136-ARR_v2_18@1,136-ARR_v1_21@0,"Given a set of triples X on the input, we:","We proceed as follows -given an input X, we:",yes
832,136-ARR,136-ARR_v2_22@0,136-ARR_v1_29@0,Ordering the Facts,Ordering,yes
833,136-ARR,136-ARR_v2_23@0,136-ARR_v1_30@0,We assume that the default order of triples X is random and the same applies for the respective facts F .,We assume that the default order of triples X (and the respective facts F ) is random.,yes
834,136-ARR,136-ARR_v2_25@0,136-ARR_v1_31@0,Aggregating the Facts,Aggregation,yes
835,136-ARR,136-ARR_v2_27@1,136-ARR_v1_32@1,"Conversely, δ i "" 0 means that the facts should be aggregated and their corresponding sentences should be fused.","Conversely, δ i "" 0 means that the facts should be aggregated and their corresponding sentences should be fused (see §5.2 and §5.3).",yes
836,136-ARR,136-ARR_v2_29@1,136-ARR_v1_34@1,"It has two main objectives: (a) fusing related sentences, i.e., sentences i and j in between which δ i "" 0, and (b) rephrasing the text to improve its fluency, e.g. fixing disfluencies in the templates, replacing noun phrases with refering expressions, etc.","The objectives of the model are two-fold: (a) fusing related sentences, i.e., sentences i and j in between which δ i "" 0, and (b) rephrasing the text to improve its fluency, e.g. fixing minor disfluencies in the templates, replacing noun phrases with refering expressions, etc.",yes
837,136-ARR,136-ARR_v2_29@2,136-ARR_v1_34@2,"The goal of the task is to preserve the semantics of the text which is an already ordered sequence of sentences, so the edits will typically be minor.",The focus is on minor rephrasing since the goal is to preserve the semantics of the original text.,yes
838,136-ARR,136-ARR_v2_33@0,136-ARR_v1_36@1,Our goal is to cover a broad range of domains while capturing the sentence style in D2T generation with respect to both the input facts and the target descriptions.,"Our corpus needs to cover a broad range of domains while capturing the sentence style in D2T generation, both regarding the input templates and the target descriptions.",yes
839,136-ARR,136-ARR_v2_42@6,136-ARR_v1_46@3,"To balance the length of inputs, we selected 250k examples each from 4 equally sized length ranges (30-130 characters, etc.).","To further ensure that the length of inputs is balanced, we selected 250k examples each from 4 equidistant length ranges (30-130 characters, etc.).",yes
840,136-ARR,136-ARR_v2_44@0,136-ARR_v1_48@0,"To generate a set of simple sentences, we divide each paragraph into sentences using NLTK (Bird, 2006) and apply a split-and-rephrase model on each sentence.","For generating the target set of sentences, we divide each paragraph into sentences using NLTK (Bird, 2006) and apply a split-and-rephrase model on each sentence.",yes
841,136-ARR,136-ARR_v2_45@0,136-ARR_v1_48@2,"We train our split-and-rephrase model on the large-scale WikiSplit corpus by Botha et al. (2018), containing human-made sentence splits from Wikipedia edit history.","We train our model on the large-scale WikiSplit corpus by Botha et al. (2018), containing human-made sentence splits from Wikipedia edit history.",yes
842,136-ARR,136-ARR_v2_45@1,136-ARR_v1_48@3,"Following the same setup as for a paragraph compression model ( §3.4), we train BART-base (Lewis et al., 2020) on the WikiSplit dataset in a sequence-to-sequence setting.","Following the setup in the rest of our experiments, we train the encoder-decoder PLM BART-base (Lewis et al., 2020) on the WikiSplit dataset in a sequence-to-sequence setting.",yes
843,136-ARR,136-ARR_v2_45@2,136-ARR_v1_48@4,"Next, we apply the trained split-and-rephrase model on each sentence in our Wikipedia-based corpus, uniformly randomly choosing between 0-2 recursive calls to ensure that the splits are not deterministic.","We apply the trained split-and-rephrase model on each sentence, uniformly randomly choosing between 0-2 recursive calls to ensure that the splits are not deterministic.",yes
844,136-ARR,136-ARR_v2_47@0,136-ARR_v1_50@0,"As the next step, we concatenate the split sentences and apply a coreference resolution model (Gardner et al., 2018;Lee et al., 2018) in order to replace referring expressions with their antencendents (e.g., pronouns with noun phrases).","Next, we concatenate the split sentences and apply a coreference resolution model (Gardner et al., 2018) in order to replace referring expressions with their antencendents (e.g., pronouns with noun phrases).",yes
845,136-ARR,136-ARR_v2_47@1,136-ARR_v1_50@1,"The motivation for this step is to match the style of the facts (see §3.1), which do not use pronouns since each fact describes a single triple only.",This allows to better follow the style of the templates in which the entities are always fully verbalized.,yes
846,136-ARR,136-ARR_v2_49@2,136-ARR_v1_52@2,"In a filtered version of the WIKIFLUENT corpus, we include only the examples without omissions or hallucinations (as computed by the model), reducing it to 714k examples (approximately 75% of the original size).","In a filtered version of the WIK-IFLUENT corpus, we include only the examples without omissions or hallucinations (as computed by the model), reducing it to approximately 3/4 of the original size.",yes
847,136-ARR,136-ARR_v2_56@1,136-ARR_v1_56@2,We provide a short description of the model here; for details please refer to Calizzano et al. (2021).,We provide a short description of the model here; for details see Calizzano et al. (2021).,yes
848,136-ARR,136-ARR_v2_59@0,136-ARR_v1_59@0,"The pointer network computes the probability of a fact to be on the j-th position, using the encoder output E and the decoder output state d j .","The pointer network computes the probability of a fact to be on the j-th position, using the encoder output E and the decoder output d j .",yes
849,136-ARR,136-ARR_v2_64@0,136-ARR_v1_61@1,"We train the model using the synthesized simple sentences in the WIKIFLUENT corpus, randomly shuffling the order of the sentences and training the model to restore their original order.","We train the model using the split sentences in the WIKIFLUENT corpus, randomly shuffling the order of the sentences and training the model to restore their original order.",yes
850,136-ARR,136-ARR_v2_66@3,136-ARR_v1_63@3,We ignore the outputs for the nonseparator tokens while computing cross-entropy loss.,We ignore the outputs for the non-separator tokens while computing the cross-entropy loss.,yes
851,136-ARR,136-ARR_v2_67@0,136-ARR_v1_64@0,"We create the training examples using the synthesized sentences in the WIKIFLUENT corpus, in which we set δ i "" 0 for the sentences i, i`1 which were originally aggregated (i.e., are the result of splitting a single sentence) and δ i "" 1 otherwise.","We create the training examples using the split sentences in the WIKIFLUENT corpus, in which we set δ i "" 0 for the sentences i, i `1 which were originally aggregated (i.e., are the result of splitting a single sentence) and δ i "" 1 otherwise.",yes
852,136-ARR,136-ARR_v2_69@1,136-ARR_v1_66@1,"We finetune the model on the WIK-IFLUENT corpus, concatenating the synthesized sentences on the input.","We train the model in a sequenceto-sequence setting on the WIKIFLUENT corpus, concatenating the split sentences on the input.",yes
853,136-ARR,136-ARR_v2_69@2,136-ARR_v1_66@2,"We add delimiters between the sentences i and i `1 where δ i "" 1 using a special token <sep>, which we add to the model vocabulary.","We add delimiters between sentences i and i `1 where δ i "" 1 using a special token <sep>, which we add to the model vocabulary.",yes
854,136-ARR,136-ARR_v2_70@1,136-ARR_v1_66@4,We evaluate how the model learns to respect the order and aggregation markers in §7.3.,We evaluate our model's behavior with respect to ordering and aggregation in §6.3.,yes
855,136-ARR,136-ARR_v2_6@1,136-ARR_v1_6@1,Gathering a large set of references for a particular domain is also costly and time-consuming as it usually requires collecting human-written references through crowdsourcing .,"Moreover, collecting a large set of references for a particular domain is costly and time-consuming, as the data are usually collected using crowdsourcing .",yes
856,136-ARR,136-ARR_v2_73@0,136-ARR_v1_71@1,"Datasets The datasets differ in domain, size, textual style, and number of predicates (see Appendix A for details):","They differ in domain, size, textual style, and number of predicates (see Appendix A for details).",yes
857,136-ARR,136-ARR_v2_78@0,136-ARR_v1_73@0,Evaluation and Discussion,Evaluation,yes
858,136-ARR,136-ARR_v2_79@1,136-ARR_v1_74@0,"We evaluate outputs from the {1,2,3}-STAGE variants of our pipeline using automatic metrics ( §7.1), and we perform a detailed manual error analysis of the model outputs ( §7.2).","We evaluate outputs from the {1,2,3}-STAGE variants of our pipeline automatically ( §6.1) and manually ( §6.2).",yes
859,136-ARR,136-ARR_v2_79@2,136-ARR_v1_74@1,We also evaluate the performance of the content planning modules and the ability of the PC module to follow the content plan ( §7.3).,"Further, we evaluate the performance of the content planning modules and the ability of the PC module to follow the content plan ( §6.3).",yes
861,136-ARR,136-ARR_v2_83@0,136-ARR_v1_79@0,"The automatic evaluation shows that our systems consistently outperform the COPY baseline (e.g., ""12 BLEU points for E2E), which is already strong thanks to our manually curated set of templates.","The automatic evaluation suggests that while our system lags behind state-of-the-art supervised systems, it shows considerable improvements compared to the COPY baseline (e.g., ""12 BLEU points pendix C for the details. for E2E) and matches performance of some older supervised systems.",yes
863,136-ARR,136-ARR_v2_84@0,136-ARR_v1_79@2,"The 2-STAGE system is generally on par with the 3-STAGE system or better, which indicates that explicit aggregation using the AGG model may not be necessary.","The 2-STAGE system is generally on par with the 3-STAGE system (or better), which indicates that implicit aggregation using the PC-AGG model may be sufficient.",yes
864,136-ARR,136-ARR_v2_84@2,136-ARR_v1_79@4,"The models using the filtered version of the corpus generally produce better results, although they also bring in a larger number of omissions.","The filtered version of the dataset generally brings better results, although it brings also an increase in the number of omissions.",yes
865,136-ARR,136-ARR_v2_86@1,136-ARR_v1_81@0,"We counted the number of errors: factual (hallucinations, omissions, incorrect fact merging, redundancies) and grammatical.","We manually evaluated 100 outputs of the models regarding factual errors (hallucinations, omissions, incorrect fact merging, redundancies) as well as grammatical errors.",yes
866,136-ARR,136-ARR_v2_87@1,136-ARR_v1_81@3,"These problems are largely eliminated with the 2-STAGE and 3-STAGE models, which produce Wildwood is a restaurant.","These problems are only slightly reduced in the filtered version, but they are largely eliminated with 2-STAGE and 3-STAGE models.",yes
867,136-ARR,136-ARR_v2_89@1,136-ARR_v1_81@9,"Grammar errors and disfluencies stem mainly from over-eager paragraph compression or from artifacts in our templates and are relatively minor (e.g., missing ""is"" in ""serves French food and family-friendly"").","The grammar errors and disfluencies stem mainly from over-eager paragraph compression or from artifacts in our templates; they are relatively minor (e.g., missing ""is"" in ""serves French food and family-friendly"").",yes
868,136-ARR,136-ARR_v2_91@0,136-ARR_v1_84@0,"Following and , we report the accuracy and BLEU-2 score of our ordering model on WebNLG against the humangenerated plans from Ferreira et al. (2018).","Following and , we report the accuracy (Acc) and BLEU-2 score (B-2) of our ordering model on WebNLG against the human-generated plans from Ferreira et al. (2018).",yes
869,136-ARR,136-ARR_v2_2@1,136-ARR_v1_2@1,We examine how to avoid finetuning pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs.,We examine how to avoid finetuning the pretrained language models (PLMs) on D2T generation datasets while still taking advantage of surface realization capabilities of PLMs.,yes
870,136-ARR,136-ARR_v2_7@0,136-ARR_v1_7@0,"In this paper, we present a zero-shot alternative to the traditional finetuning paradigm by formulating the D2T generation from RDF triples as a sequence of general-domain operations over text in natural language.","In this paper, we provide an alternative to this traditional paradigm by formulating the D2T generation from RDF triples as a sequence of generaldomain operations over text in natural language.",yes
871,136-ARR,136-ARR_v2_92@0,136-ARR_v1_89@1,"Finally, we manually evaluate how the PC model follows the content plan (i.e., keeping the predefined order and aggregating the sentences according to the delimiters) using 100 randomly chosen examples with more than 1 triple on WebNLG and E2E.","Finally, we manually evaluate how the PC model follows the content plan using 100 randomly chosen examples with more than 1 triple on WebNLG and E2E.",yes
872,136-ARR,136-ARR_v2_92@2,136-ARR_v1_89@3,The incorrect cases include a fact not properly mentioned or an extra boundary between sentences without a separator.,The incorrect cases include a fact not properly mentioned and an extra boundary between the sentences without a separator.,yes
873,136-ARR,136-ARR_v2_95@0,136-ARR_v1_90@0,Future Work,Discussion and Future Work,yes
874,136-ARR,136-ARR_v2_96@1,136-ARR_v1_92@1,"Automatic generation of facts without using hand-crafted templates (cf. §5.1) could allow applying zero-shot generation systems to datasets with a large number of predicates, such as ToTTo (Parikh et al., 2020).","Generating simple statements from the triples automatically, e.g., using the approach of Laha et al. (2020), could reduce the manual workload and allow applying our approach on datasets with a less constrained set of data attributes such as ToTTo (Parikh et al., 2020) or DART (Nan et al., 2021).",yes
875,136-ARR,136-ARR_v2_97@0,136-ARR_v1_93@0,"More research is also needed regarding the main shortcoming of our approach, i.e., the semantic errors stemming from merging of facts in improper ways.",More research is also needed on semantic errors stemming from merging of facts in improper ways.,yes
876,136-ARR,136-ARR_v2_97@1,136-ARR_v1_93@1,"As we suggested in §7.2, explicitly controlling the semantics of sentence fusion could help to mitigate this issue, while still keeping the advantages of a zero-shot approach.","We suggest that explicitly controlling the semantics of sentence fusion (Ben-David et al., 2020) could help to mitigate this issue, while still keeping the advantages of a zero-shot approach.",yes
877,136-ARR,136-ARR_v2_103@0,136-ARR_v1_95@0,"We implemented the models for split-and-rephrase, aggregation, and paragraph compression in Py-Torch Lightning (Paszke et al., 2019), using the PyTorch (Falcon et al., 2019) version of the BART and RoBERTa models from the Huggingface library (Wolf et al., 2019).","We implemented the models for split-and-rephrase, aggregation, and paragraph compression in Py-Torch Lightning (Paszke et al., 2019), using the PyTorch (Falcon, 2019) version of the BART and RoBERTa models from the Huggingface library (Wolf et al., 2019).",yes
878,136-ARR,136-ARR_v2_105@1,136-ARR_v1_97@1,We will integrate the ordering model into our framework.,We plan to fully integrate the ordering model into our framework in the future.,yes
882,136-ARR,136-ARR_v2_7@4,136-ARR_v1_7@4,Our approach allows generating natural language descriptions from RDF triples with a minimum amount of domain-specific rules or knowledge and without using training data from the D2T datasets.,Our approach allows generating natural language descriptions from triples with a minimum amount of domain-specific rules or knowledge and without using training data from the D2T datasets.,yes
883,136-ARR,136-ARR_v2_7@5,136-ARR_v1_7@5,"Although our approach is primarily a probe into the territory of zero-shot approaches and cannot yet match the quality of stateof-the-art models, we show that it can yield large improvements upon simple baselines and match older supervised systems on automatic metrics for text fluency.","We show that our approach can yield large improvements upon simple baselines and match older supervised systems in terms of fluency, while bringing potential for further improvements and advantages with respect to controllability.",yes
884,136-ARR,136-ARR_v2_2@2,136-ARR_v1_2@2,"Inspired by pipeline approaches, we propose to generate text by transforming single-item descriptions with a sequence of modules trained on generaldomain text-based operations: ordering, aggregation, and paragraph compression.","Inspired by pipeline approaches, we propose to generate text by rephrasing single-item templates using a sequence of modules trained on general-domain text-based operations-ordering, aggregation, and paragraph compression.",yes
885,136-ARR,136-ARR_v2_10@1,136-ARR_v1_10@1,"Following Chen et al. (2020b), other works adopted PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a).","Following Chen et al. (2020c), other works adopt PLMs for few-shot D2T generation (Chang et al., 2021b;Su et al., 2021a).",yes
886,136-ARR,136-ARR_v2_12@0,136-ARR_v1_12@0,"Content Planning in D2T Generation Content planning, i.e. the task of ordering input facts and aggregating them into individual sentences, is one of the steps of the traditional D2T pipeline (Gatt and Krahmer, 2018).","Content Planning in D2T Generation Content planning, i.e. ordering input facts and aggregating them into individual sentences, is a traditional part of the D2T generation pipeline (Ferreira et al., 2019;Gatt and Krahmer, 2018;Reiter and Dale, 1997).",yes
889,136-ARR,136-ARR_v2_14@0,136-ARR_v1_14@0,"Aggregating Input into Sentences Typically, multiple pieces of input information need to be merged into a single sentence.",Fact Aggregation The compact nature of the target text description results in aggregating multiple facts in a single sentence.,yes
890,136-ARR,136-ARR_v2_14@2,136-ARR_v1_14@2,"Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts (see §3.1), into which we selectively insert delimiters to mark sentence boundaries.","Unlike these works, we adopt a simpler scenario using an already ordered sequence of facts, in which we selectively insert delimiters marking sentence boundaries.",yes
891,136-ARR,136-ARR_v2_15@1,136-ARR_v1_14@4,"This task combines several standard natural-language tasks including sentence fusion, rephrasing, and coreference resolution.","As the last step in the pipeline, it is closely related to linguistic realisation, howeversince we already work with natural language in this step-the focus of our task is on sentence fusion, rephrasing, and coreference resolution.",yes
892,136-ARR,136-ARR_v2_19@1,136-ARR_v1_16@1,"The individual steps are described in the following sections: transforming individual triples to text ( §3.1), ordering ( §3.2), aggregation ( §3.3), and paragraph compression ( §3.4).","Next, we describe the individual steps, starting by applying simple templates to transform data to text ( §3.2), followed by individual modules for ordering ( §3.3), aggregation ( §3.4), and paragraph compression ( §3.5).",yes
,136-ARR,136-ARR_v2_99@0,136-ARR_v1_71@0,We presented an approach for zero-shot D2T generation.,"We test our approach on two English D2T datasets, WebNLG and E2E.",no
,136-ARR,136-ARR_v2_72@0,136-ARR_v1_92@0,We train our pipeline modules on the WIKIFLU-ENT corpus as described in §5.,Our approach regarding handcrafting a single template for each predicate is quite basic.,no
,136-ARR,136-ARR_v2_32@0,136-ARR_v1_17@0,Here we descibe the process of building a largescale synthetic corpus WIKIFLUENT.,Method Overview,no
,136-ARR,136-ARR_v2_19@0,136-ARR_v1_94@0,"(1) transform the triples into facts, which are sentences in natural language, (2) sort the facts using an ordering module, (3) insert sentence delimiters between the sorted facts using an aggregation module, (4) input the ordered sequence of facts with delimiters into a paragraph compression module, which generates the final description Y .",Statistics for the datasets described in the paper are listed in Table 7.,no
,136-ARR,136-ARR_v2_94@4,136-ARR_v1_17@0,Accuracy of the ordering and aggregation modules is comparable to their performance on D2T datasets.,Method Overview,no
,136-ARR,136-ARR_v2_47@2,136-ARR_v1_78@2,Note that this procedure replaces the referring expressions only in the synthesized sentences (which are used as input) and keeps them in the original paragraphs (which are used as ground truth).,"For WebNLG (see Table 2), we include the results of UPF-FORGe and MELBOURNE systems from the first run of WebNLG Challenge which are comparable in terms of automatic metrics and semantic errors, and the results of Ke et al. (2021), which is a state-of-theart system using structure-aware encoder and taskspecific pretraining.",no
,136-ARR,136-ARR_v2_26@1,136-ARR_v1_30@1,"Considering the previous example, occupation is likely to be mentioned separately, while birth date and birth place are likely to be mentioned together.","To maximize the coherency of the resulting description, we apply an ordering model O to get an ordered sequence of facts: F o "" tf o 1 , . . . , f on u.",no
,136-ARR,136-ARR_v2_44@2,136-ARR_v1_16@0,The process is illustrated in the upper part of Figure 2.,We first give an overview of our neural D2T generation pipeline ( §3.1).,no
,136-ARR,136-ARR_v2_35@3,136-ARR_v1_72@6,"We used the first paragraphs of Wikipedia entries, which contain mostly concise, fact-based descriptions.","We use the cleaned version of the dataset (Dušek et al., 2019).",no
,136-ARR,136-ARR_v2_88@9,136-ARR_v1_72@6,"As a possible remedy, we propose explicitly controlling the semantics of sentence fusion (Ben-David et al., 2020), e.g. using a variant of constrained decoding (Balakrishnan et al., 2019;.","We use the cleaned version of the dataset (Dušek et al., 2019).",no
,136-ARR,136-ARR_v2_42@3,136-ARR_v1_72@7,The result is a set of simple sentences which together convey the same meaning as the original paragraph.,"Following previous work, we transformed the attribute-value pairs into RDF triples (using the restaurant name as a subject) and then applied the same setup as for WebNLG.",no
,136-ARR,136-ARR_v2_29@0,136-ARR_v1_67@0,The paragraph compression (PC) model is a generative model which outputs the final text description.,Ablation Study,no
,136-ARR,136-ARR_v2_96@2,136-ARR_v1_26@1,"The task of paragraph compression could be used as a task-specific pretraining (Gururangan et al., 2020) for more efficient finetuning of D2T models, e.g., with a small amount of clean data.",We need at least one template for each predicate.,no
,136-ARR,136-ARR_v2_47@3,136-ARR_v1_54@0,"As a consequence, the paragraph compression module is implicitly trained to generate referring expressions in the final description.",We show how we build our pipeline ( §5.1-5.4) and discuss the D2T generation datasets which we use for our experiments ( §5.5).,no
,136-ARR,136-ARR_v2_11@1,136-ARR_v1_46@1,"Our approach is inspired by the pipeline approaches, in particular the pipelines utilizing neural modules (Ferreira et al., 2019).","The paragraphs contain mostly concise, factbased descriptions from a wide range of domains.",no
,136-ARR,136-ARR_v2_94@2,136-ARR_v1_72@2,The results for both full and filtered test sets are summarized in Table 7.,Templates for WebNLG could be extracted from the training data by delexicalizing single-triple examples.,no
,136-ARR,136-ARR_v2_81@3,136-ARR_v1_72@7,"For WebNLG (see Table 3), we compare our systems with the results of:","Following previous work, we transformed the attribute-value pairs into RDF triples (using the restaurant name as a subject) and then applied the same setup as for WebNLG.",no
,136-ARR,136-ARR_v2_101@1,136-ARR_v1_54@1,"Although the task of D2T generation has numerous applications, using neural models for D2T generation (especially in the zero-shot context) is still limited to experimental settings (Dale, 2020).",The details of our training setup are included in Appendix B.,no
,136-ARR,136-ARR_v2_102@0,136-ARR_v1_30@2,"On the flip side, our approach helps to reduce the number of omissions and hallucinations stemming from noise in human-written references.","The coherence of the final text will also depend on the paragraph compression step, but grouping related facts together (e.g. facts mentioning birth date and birth place) helps the paragraph compression model to focus only on fusing and rephrasing the neighboring sentences.",no
,136-ARR,136-ARR_v2_11@2,136-ARR_v1_72@2,"In contrast with these approaches, our pipeline works with unstructured data in natural language and it operates in zero-shot setting, i.e. without using any training data from target D2T datasets.",Templates for WebNLG could be extracted from the training data by delexicalizing single-triple examples.,no
,136-ARR,136-ARR_v2_53@0,136-ARR_v1_92@0,We transform triples into facts ( §3.1) using a singletriple template t i for each predicate.,Our approach regarding handcrafting a single template for each predicate is quite basic.,no
,136-ARR,136-ARR_v2_83@3,136-ARR_v1_92@2,"For this reason, we further focus on manual error analysis in §7.2 to pinpoint the current shortcomings of our approach.","Moreover, explicitly including a denoising task for the paragraph compression model could help to tackle the disfluencies in the templates.",no
,136-ARR,136-ARR_v2_18@0,136-ARR_v1_94@0,Our pipeline proceeds as follows.,Statistics for the datasets described in the paper are listed in Table 7.,no
,136-ARR,136-ARR_v2_51@0,136-ARR_v1_30@1,"In this section, we describe how we implement our pipeline modules ( §3) using simple template transformations ( §5.1) and neural models trained on the WIKIFLUENT dataset ( §5.2-5.4).","To maximize the coherency of the resulting description, we apply an ordering model O to get an ordered sequence of facts: F o "" tf o 1 , . . . , f on u.",no
,136-ARR,136-ARR_v2_101@2,136-ARR_v1_85@2,"Similarly to other recent approaches for D2T generation, our approach relies on PLMs, which are known to reflect the biases in their pretraining corpus (Bender et al., 2021;Rogers, 2021).","The results show that although our approach lacks behind stateof-the-art supervised approaches, it can outperform both the random baseline and the Transformerbased approach from Ferreira et al. (2019) while not using any training examples from WebNLG.",no
,136-ARR,136-ARR_v2_11@3,136-ARR_v1_78@2,Laha et al. (2019) introduce a three-step pipeline for zero-shot D2T generation similar to ours.,"For WebNLG (see Table 2), we include the results of UPF-FORGe and MELBOURNE systems from the first run of WebNLG Challenge which are comparable in terms of automatic metrics and semantic errors, and the results of Ke et al. (2021), which is a state-of-theart system using structure-aware encoder and taskspecific pretraining.",no
,136-ARR,136-ARR_v2_99@1,136-ARR_v1_78@2,The approach uses a pipeline of PLMs trained on general-domain lexical operations over natural language.,"For WebNLG (see Table 2), we include the results of UPF-FORGe and MELBOURNE systems from the first run of WebNLG Challenge which are comparable in terms of automatic metrics and semantic errors, and the results of Ke et al. (2021), which is a state-of-theart system using structure-aware encoder and taskspecific pretraining.",no
,136-ARR,136-ARR_v2_11@0,136-ARR_v1_72@6,"Pipeline-based D2T Generation Until the recent surge of end-to-end approaches , using several modules connected in a pipeline was a major approach for D2T generation (Gatt and Krahmer, 2018;Reiter, 2007;Reiter and Dale, 1997).","We use the cleaned version of the dataset (Dušek et al., 2019).",no
,136-ARR,136-ARR_v2_47@2,136-ARR_v1_27@2,Note that this procedure replaces the referring expressions only in the synthesized sentences (which are used as input) and keeps them in the original paragraphs (which are used as ground truth).,See §5.5 for our approach to gathering the templates and Figure 2 for examples of the templates we use in our datasets.,no
,136-ARR,136-ARR_v2_88@8,136-ARR_v1_26@1,This behavior is the main obstacle to ensure factual consistency of the output.,We need at least one template for each predicate.,no
,136-ARR,136-ARR_v2_93@0,136-ARR_v1_92@2,Intrinsic Evaluation,"Moreover, explicitly including a denoising task for the paragraph compression model could help to tackle the disfluencies in the templates.",no
,136-ARR,136-ARR_v2_94@2,136-ARR_v1_72@3,The results for both full and filtered test sets are summarized in Table 7.,"However, the examples are noisy and such data would not be available in a zero-shot setup.",no
,136-ARR,136-ARR_v2_53@2,136-ARR_v1_16@0,"We follow previous work in which simple hand-crafted templates have been used as an efficient way of introducing domain knowledge (Kale and Rastogi, 2020a;Kasner and Dušek, 2020a) template generation engines (Laha et al., 2019;Heidari et al., 2021;Mehta et al., 2021), the approach may produce less fluent outputs, but it minimizes manual workload and makes it easier to control the quality of the input for the subsequent steps.",We first give an overview of our neural D2T generation pipeline ( §3.1).,no
,136-ARR,136-ARR_v2_35@1,136-ARR_v1_72@2,"Wikipedia is commonly used for large-scale pretraining of D2T generation models (Jin et al., 2020;Chen et al., 2020a).",Templates for WebNLG could be extracted from the training data by delexicalizing single-triple examples.,no
,136-ARR,136-ARR_v2_42@4,136-ARR_v1_16@0,"The synthesized sentences are used as input into our models, the original human-written texts are used as ground truth.",We first give an overview of our neural D2T generation pipeline ( §3.1).,no
,136-ARR,136-ARR_v2_35@1,136-ARR_v1_72@4,"Wikipedia is commonly used for large-scale pretraining of D2T generation models (Jin et al., 2020;Chen et al., 2020a).","Therefore, we handcrafted templates for all 354 predicates, including unseen predicates in the test set.",no
,136-ARR,136-ARR_v2_42@2,136-ARR_v1_70@0,We apply a split-and-rephrase model on each sentence in the paragraph and resolve coreferences in the split sentences.,D2T Datasets,no
,136-ARR,136-ARR_v2_94@0,136-ARR_v1_94@0,"Aside from the main D2T generation results, we also provide an intrinsic evaluation of our pipeline modules on the WIKIFLUENT test sets.",Statistics for the datasets described in the paper are listed in Table 7.,no
,136-ARR,136-ARR_v2_99@4,136-ARR_v1_72@1,We believe that training models for zeroshot D2T generation using large cross-domain corpora will help to build D2T generation systems with good performance across various domains.,"The dataset was extended for the WebNLG+ Challenge (Ferreira et al., 2020), but we use the version 1.4 for comparability to prior work.",no
,136-ARR,136-ARR_v2_72@0,136-ARR_v1_72@6,We train our pipeline modules on the WIKIFLU-ENT corpus as described in §5.,"We use the cleaned version of the dataset (Dušek et al., 2019).",no
,136-ARR,136-ARR_v2_96@2,136-ARR_v1_78@3,"The task of paragraph compression could be used as a task-specific pretraining (Gururangan et al., 2020) for more efficient finetuning of D2T models, e.g., with a small amount of clean data.",Laha et al. ( 2020) is (to our knowledge) the only other zero-shot D2T generation system applied on WebNLG.,no
,136-ARR,136-ARR_v2_99@0,136-ARR_v1_80@0,We presented an approach for zero-shot D2T generation.,Manual Evaluation,no
,136-ARR,136-ARR_v2_99@4,136-ARR_v1_85@1,We believe that training models for zeroshot D2T generation using large cross-domain corpora will help to build D2T generation systems with good performance across various domains.,RANDOM is the baseline of generating a random order.,no
,136-ARR,136-ARR_v2_35@1,136-ARR_v1_30@1,"Wikipedia is commonly used for large-scale pretraining of D2T generation models (Jin et al., 2020;Chen et al., 2020a).","To maximize the coherency of the resulting description, we apply an ordering model O to get an ordered sequence of facts: F o "" tf o 1 , . . . , f on u.",no
,136-ARR,136-ARR_v2_94@1,136-ARR_v1_91@0,"We evaluated the ordering, aggregation, and paragraph compression modules trained on the full WIKIFLUENT corpus.","In the current form, our pipeline can be directly applied to generating text from RDF triples (or similarly structured data) which require no extra processing.",no
,136-ARR,136-ARR_v2_23@2,136-ARR_v1_72@8,We can use this to our advantage and apply a sentence ordering model to maximize the coherency of the paragraph resulting from their concatenation.,We created a template for each of the 8 attributes manually.,no
,136-ARR,136-ARR_v2_17@0,136-ARR_v1_26@1,"In this section, we provide the formal description of our proposed approach.",We need at least one template for each predicate.,no
,136-ARR,136-ARR_v2_47@2,136-ARR_v1_72@0,Note that this procedure replaces the referring expressions only in the synthesized sentences (which are used as input) and keeps them in the original paragraphs (which are used as ground truth).,"WebNLG The WebNLG dataset contains RDF triples from DBPedia (Auer et al., 2007) and their crowdsourced descriptions.",no
,136-ARR,136-ARR_v2_23@4,136-ARR_v1_36@0,The ordering module allows downstream modules to only focus on operations over neighboring sentences.,A key to our approach is building a large-scale synthetic corpus providing training data for the text operations in our pipeline.,no
,136-ARR,136-ARR_v2_96@2,136-ARR_v1_26@2,"The task of paragraph compression could be used as a task-specific pretraining (Gururangan et al., 2020) for more efficient finetuning of D2T models, e.g., with a small amount of clean data.","Typically, the template will include placeholders which are filled with s i and o i .",no
,136-ARR,136-ARR_v2_93@0,136-ARR_v1_28@0,Intrinsic Evaluation,"We acknowledge that this step may be a bottleneck on datasets with an unconstrained (or very large) set of predicates, which is why we also discuss possibilities for automating this step in §7.",no
,136-ARR,136-ARR_v2_72@0,136-ARR_v1_17@0,We train our pipeline modules on the WIKIFLU-ENT corpus as described in §5.,Method Overview,no
,136-ARR,136-ARR_v2_83@2,136-ARR_v1_28@0,"Nevertheless, our systems still underperform the state-of-the-art supervised systems.","We acknowledge that this step may be a bottleneck on datasets with an unconstrained (or very large) set of predicates, which is why we also discuss possibilities for automating this step in §7.",no
,136-ARR,136-ARR_v2_24@0,136-ARR_v1_30@1,"Formally, we apply the ordering model OpF q to get an ordered sequence of facts: F o "" tf o 1 , . . . , f on u, where o 1:n is a permutation of indices.","To maximize the coherency of the resulting description, we apply an ordering model O to get an ordered sequence of facts: F o "" tf o 1 , . . . , f on u.",no
,136-ARR,136-ARR_v2_47@3,136-ARR_v1_67@0,"As a consequence, the paragraph compression module is implicitly trained to generate referring expressions in the final description.",Ablation Study,no
1349,166-ARR,166-ARR_v2_20@1,166-ARR_v1_20@1,"While this ranker architecture approach is simple (and can also be used for the retrieval step via an approximate nearest neighbor search such as FAISS (Johnson et al., 2017), ScaNN (Guo et al., 2020) or the Pinecone managed service 2 ), the overall ranking quality is generally lower compared to methods that employ a query-document crossattention interaction.","While this ranker architecture approach is simple (and can also be used for the retrieval step via an approximate nearest neighbor search such as FAISS (Johnson et al., 2017) or ScaNN (Guo et al., 2020)), the overall ranking quality is generally lower compared to methods that employ a query-document cross-attention interaction.",yes
1350,166-ARR,166-ARR_v2_38@0,166-ARR_v1_40@0,"To remedy this issue, we follow an approach similar to EDEN quantization (Vargaftik et al., 2022), which uses a randomized Hadamard transform prior to quantization.","To remediate this issue, we follow the DRIVE approach (Vargaftik et al., 2021a), which uses a randomized Hadamard transform prior to quantization.",yes
1355,166-ARR,166-ARR_v2_48@2,166-ARR_v1_51@2,"For MSMARCO, we initialized the model from reduced width pre-trained weights 4 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.","For MS-MARCO, we initialized the model from reduced width pre-trained weights 2 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.",yes
1356,166-ARR,166-ARR_v2_51@0,166-ARR_v1_54@0,"In the following sections, we denote the SDR variants as ""AESI-{c}-{B}b"" where {c} is replaced with the width of the encoded vector and {B} is replaced with the number of bits in the quantization scheme.","It the following sections, we denote the SDR variants as ""AESI-{c}-{B}b"" where {c} is replaced with the width of the encoded vector and {B} is replaced with the number of bits in the quantization scheme.",yes
1357,166-ARR,166-ARR_v2_54@0,166-ARR_v1_57@0,Evaluation Results,Evaluation,yes
1358,166-ARR,166-ARR_v2_57@1,166-ARR_v1_62@0,The Distilbert model (full interaction architecture) has the highest quality and smallest index size (since it is only executed online).,The Distilbert model has the highest quality and smallest index size (since it is only executed online).,yes
1359,166-ARR,166-ARR_v2_59@2,166-ARR_v1_63@5,"The late interaction variants we consider have the same configuration as in the MS-MARCO case, where the baseline uses 24 features (with float16 quantization) and SDR uses 16 features (with 6 bits EDEN quantization).","The late interaction variants we consider have the same configuration as in the MS-MARCO case, where the baseline uses 24 features (with float16 quantization) and SDR uses 16 features (with 6 bits DRIVE quantization).",yes
1360,166-ARR,166-ARR_v2_71@2,166-ARR_v1_73@2,"Using a quantization technique fitted to the Gaussian distribution of post randomized Hadamard transform data further improve quality, making the EDEN quantization superior to other quantization techniques in our case.","Using a quantization technique fitted to the Gaussian distribution of post randomized Hadamard transform data further improve quality, making the DRIVE quantization superior to other quantization techniques in our case.",yes
1361,166-ARR,166-ARR_v2_73@0,166-ARR_v1_75@0,"To better understand the impact of side information, we measure the error rate between an input vector and its reconstructed vector (i.e., after encoding and decoding).","To better understand the impact of side information, we measure the error rate between an input vector and a reconstructed vector (i.e., after encoding and decoding) for different input vectors.",yes
1362,166-ARR,166-ARR_v2_74@0,166-ARR_v1_76@0,"In IR, the document frequency of a token is known to be negatively correlated with the token's importance.","In the information retrieval field, the document frequency of a token is known to negatively correlated with the token's importance.",yes
1363,166-ARR,166-ARR_v2_74@2,166-ARR_v1_76@2,This shows that the AESI scheme can better focus on tokens that are important for ranking.,This shows that the AESI scheme has a better focus on tokens that are important for ranking.,yes
1364,166-ARR,166-ARR_v2_74@3,166-ARR_v1_76@3,"A possible explanation for this phenomena is that the static embeddings for infrequent tokens are more informative (i.e., more helpful as side information) compared to static embeddings for frequent tokens (e.g., 'the').","A possible explanation for this phenomena is that the static embeddings for infrequent tokens is more informative (i.e., more helpful as side information) compared to static embeddings for frequent tokens (e.g., 'the').",yes
1365,166-ARR,166-ARR_v2_74@4,166-ARR_v1_76@4,"We also found AESI excels more in compressing nouns, verbs, and adjectives, while AE-2L excels more in compressing punctuation, determiners, and adpositions.","We also found AESI excels more in compressing nouns, verbs, and adjectives, while AE-2L excels more in compressing punctuations, determiners, and adpositions.",yes
1366,166-ARR,166-ARR_v2_74@6,166-ARR_v1_76@6,The details of this evaluation are provided in Appendix C.,The details of this evaluation appear in Appendix C.,yes
1367,166-ARR,166-ARR_v2_76@0,166-ARR_v1_78@0,"In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction transformer based models for passage re-ranking.","In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction models.",yes
1368,166-ARR,166-ARR_v2_76@2,166-ARR_v1_78@2,"In addition, we explored different quantization techniques and showed that the recently proposed EDEN performs well in our use case and presented extensive experimentation.","In addition, we explored different quantization techniques and showed that the recently proposed DRIVE performs well in our use case and presented extensive experimentation.",yes
1369,166-ARR,166-ARR_v2_87@2,166-ARR_v1_89@2,"In addition to EDEN (Appendix A, Algorithm 1), we consider the following quantization strategies: Deterministic Rounding (DR) (Gersho and Gray, 1992).","In addition to DRIVE (Appendix A, Algorithm 1), we consider the following quantization strategies: Deterministic Rounding (DR) (Gersho and Gray, 1992).",yes
1370,166-ARR,166-ARR_v2_87@12,166-ARR_v1_89@12,"EDEN with Bias Correction (EDEN-BC) (Vargaftik et al., 2022, Section 2.3).","DRIVE with Bias Correction (DRIVE-BC) (Vargaftik et al., 2021b, Appendix C.3).",yes
1371,166-ARR,166-ARR_v2_87@13,166-ARR_v1_89@13,"This variant of EDEN optimizes for lower bias over the mean squared error (MSE) by multiplying the dequantization result in Algorithm 1 by a bias correction scalar: x 2 2 / H(x), ŷ .",This variant of DRIVE optimizes for lower bias over the mean squared error (MSE) by multiplying the dequantization result in Algorithm 1 by a bias correction scalar: x 2 2 / ŷ 2 2 .,yes
1372,166-ARR,166-ARR_v2_88@2,166-ARR_v1_90@2,"Second, we see that EDEN performs better than all other schemes.","Second, we see that DRIVE performs better than all other schemes.",yes
1373,166-ARR,166-ARR_v2_88@5,166-ARR_v1_90@5,"This conclusion follows by observing that EDEN and the deterministic rounding methods (DR, H-DR) are respectively better than EDEN-BC and the stochastic rounding methods (SR, H-SR).","This conclusion follows by observing that DRIVE and the deterministic rounding methods (DR, H-DR) are respectively better than DRIVE-BC and the stochastic rounding methods (SR, H-SR).",yes
1374,166-ARR,166-ARR_v2_100@1,166-ARR_v1_102@1,"We compare the baseline -an autoencoder with 2 layers and float16 quantization -to SDR scheme, with the same number of features and EDEN 6bits quantization.","We compare the baseline -an autoencoder with 2 layers and float16 quantization -to SDR scheme, with the same number of features and DRIVE 6bits quantization.",yes
1377,166-ARR,166-ARR_v2_100@6,166-ARR_v1_102@5,"Finally, with the largest size of features we tested, 64, the baseline reached a MAP@1K score of 0.313, similar to the score achieved by SDR-20 (0.314), demonstrating the effectiveness of the static embeddings as side information.","Finally, with the largest size of features we tested, 64, the baseline reached a MAP@1K score of 0.313, similar to the score achieved by SDR-20 (0.315), demonstrating the effectiveness of the static embeddings as side information.",yes
1378,166-ARR,166-ARR_v2_13@2,166-ARR_v1_13@2,"To improve the compression-reliability tradeoff, we leverage static token embeddings, which are available since the ranker has access to the document text (as it needs to render it to the user), and are computationally cheap to obtain.","To improve the compression-reliability tradeoff, we leverage static token embeddings, which are available given that the ranker has access to the document text (as it needs to render it to the user), and are computationally cheap to obtain.",yes
1379,166-ARR,166-ARR_v2_13@4,166-ARR_v1_13@4,Ablation tests verify that adding the static vectors significantly improves the compression rates for the same ranking accuracy.,Ablation tests verify that adding the static vectors significantly improves the compression rates for the same ranker accuracy.,yes
1380,166-ARR,166-ARR_v2_15@2,166-ARR_v1_15@2,"For the MSMARCO dataset, we used a distilled model with a reduced vector width (Hofstätter et al., 2020a) as the initial pre-trained weights for the late-interaction model.","For the MSMARCO dataset, we used a distilled model with a reduced vector width (Hofstätter et al., 2020a) as the initialized pre-trained weights for the late-interaction model.",yes
1381,166-ARR,166-ARR_v2_16@0,166-ARR_v1_16@0,"To summarize, here are the contribution of this work 1 :",We make the following contributions:,yes
,166-ARR,166-ARR_v2_20@2,166-ARR_v1_37@1,"For that reason these methods are used mainly for first-stage retrieval, followed by a reranking step.","For our usage, the input is the contextual token embedding (the L-th layer's output), and the side information is the static token embedding (the output of BERT's initial embedding layer).",no
,166-ARR,166-ARR_v2_20@2,166-ARR_v1_37@2,"For that reason these methods are used mainly for first-stage retrieval, followed by a reranking step.","The resulting cdimensional encoded vector can be thought of as the distilled context of the input token. (Hendrycks and Gimpel, 2016).",no
,166-ARR,166-ARR_v2_20@2,166-ARR_v1_59@0,"For that reason these methods are used mainly for first-stage retrieval, followed by a reranking step.",3 The checkpoints will be released with the published paper.,no
,166-ARR,166-ARR_v2_45@5,166-ARR_v1_37@1,It maps from article and section titles to relevant paragraphs.,"For our usage, the input is the contextual token embedding (the L-th layer's output), and the side information is the static token embedding (the output of BERT's initial embedding layer).",no
,166-ARR,166-ARR_v2_45@5,166-ARR_v1_37@2,It maps from article and section titles to relevant paragraphs.,"The resulting cdimensional encoded vector can be thought of as the distilled context of the input token. (Hendrycks and Gimpel, 2016).",no
,166-ARR,166-ARR_v2_45@5,166-ARR_v1_59@0,It maps from article and section titles to relevant paragraphs.,3 The checkpoints will be released with the published paper.,no
,166-ARR,166-ARR_v2_45@6,166-ARR_v1_37@1,"Following , we use the automatic by-article annotations variant, which considers all paragraphs within the same article as relevant.","For our usage, the input is the contextual token embedding (the L-th layer's output), and the side information is the static token embedding (the output of BERT's initial embedding layer).",no
,166-ARR,166-ARR_v2_45@6,166-ARR_v1_37@2,"Following , we use the automatic by-article annotations variant, which considers all paragraphs within the same article as relevant.","The resulting cdimensional encoded vector can be thought of as the distilled context of the input token. (Hendrycks and Gimpel, 2016).",no
,166-ARR,166-ARR_v2_45@6,166-ARR_v1_59@0,"Following , we use the automatic by-article annotations variant, which considers all paragraphs within the same article as relevant.",3 The checkpoints will be released with the published paper.,no
,166-ARR,166-ARR_v2_76@0,166-ARR_v1_89@2,"In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction transformer based models for passage re-ranking.","In addition to DRIVE (Appendix A, Algorithm 1), we consider the following quantization strategies: Deterministic Rounding (DR) (Gersho and Gray, 1992).",no
,166-ARR,166-ARR_v2_100@1,166-ARR_v1_76@2,"We compare the baseline -an autoencoder with 2 layers and float16 quantization -to SDR scheme, with the same number of features and EDEN 6bits quantization.",This shows that the AESI scheme has a better focus on tokens that are important for ranking.,no
,166-ARR,166-ARR_v2_51@0,166-ARR_v1_90@2,"In the following sections, we denote the SDR variants as ""AESI-{c}-{B}b"" where {c} is replaced with the width of the encoded vector and {B} is replaced with the number of bits in the quantization scheme.","Second, we see that DRIVE performs better than all other schemes.",no
,166-ARR,166-ARR_v2_87@12,166-ARR_v1_76@0,"EDEN with Bias Correction (EDEN-BC) (Vargaftik et al., 2022, Section 2.3).","In the information retrieval field, the document frequency of a token is known to negatively correlated with the token's importance.",no
,166-ARR,166-ARR_v2_87@12,166-ARR_v1_51@2,"EDEN with Bias Correction (EDEN-BC) (Vargaftik et al., 2022, Section 2.3).","For MS-MARCO, we initialized the model from reduced width pre-trained weights 2 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.",no
,166-ARR,166-ARR_v2_74@4,166-ARR_v1_76@3,"We also found AESI excels more in compressing nouns, verbs, and adjectives, while AE-2L excels more in compressing punctuation, determiners, and adpositions.","A possible explanation for this phenomena is that the static embeddings for infrequent tokens is more informative (i.e., more helpful as side information) compared to static embeddings for frequent tokens (e.g., 'the').",no
,166-ARR,166-ARR_v2_100@1,166-ARR_v1_13@4,"We compare the baseline -an autoencoder with 2 layers and float16 quantization -to SDR scheme, with the same number of features and EDEN 6bits quantization.",Ablation tests verify that adding the static vectors significantly improves the compression rates for the same ranker accuracy.,no
,166-ARR,166-ARR_v2_88@2,166-ARR_v1_76@6,"Second, we see that EDEN performs better than all other schemes.",The details of this evaluation appear in Appendix C.,no
,166-ARR,166-ARR_v2_100@6,166-ARR_v1_90@5,"Finally, with the largest size of features we tested, 64, the baseline reached a MAP@1K score of 0.313, similar to the score achieved by SDR-20 (0.314), demonstrating the effectiveness of the static embeddings as side information.","This conclusion follows by observing that DRIVE and the deterministic rounding methods (DR, H-DR) are respectively better than DRIVE-BC and the stochastic rounding methods (SR, H-SR).",no
,166-ARR,166-ARR_v2_74@4,166-ARR_v1_76@6,"We also found AESI excels more in compressing nouns, verbs, and adjectives, while AE-2L excels more in compressing punctuation, determiners, and adpositions.",The details of this evaluation appear in Appendix C.,no
,166-ARR,166-ARR_v2_48@2,166-ARR_v1_78@0,"For MSMARCO, we initialized the model from reduced width pre-trained weights 4 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.","In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction models.",no
,166-ARR,166-ARR_v2_88@5,166-ARR_v1_54@0,"This conclusion follows by observing that EDEN and the deterministic rounding methods (DR, H-DR) are respectively better than EDEN-BC and the stochastic rounding methods (SR, H-SR).","It the following sections, we denote the SDR variants as ""AESI-{c}-{B}b"" where {c} is replaced with the width of the encoded vector and {B} is replaced with the number of bits in the quantization scheme.",no
,166-ARR,166-ARR_v2_38@0,166-ARR_v1_16@0,"To remedy this issue, we follow an approach similar to EDEN quantization (Vargaftik et al., 2022), which uses a randomized Hadamard transform prior to quantization.",We make the following contributions:,no
,166-ARR,166-ARR_v2_13@4,166-ARR_v1_51@2,Ablation tests verify that adding the static vectors significantly improves the compression rates for the same ranking accuracy.,"For MS-MARCO, we initialized the model from reduced width pre-trained weights 2 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.",no
,166-ARR,166-ARR_v2_20@1,166-ARR_v1_76@3,"While this ranker architecture approach is simple (and can also be used for the retrieval step via an approximate nearest neighbor search such as FAISS (Johnson et al., 2017), ScaNN (Guo et al., 2020) or the Pinecone managed service 2 ), the overall ranking quality is generally lower compared to methods that employ a query-document crossattention interaction.","A possible explanation for this phenomena is that the static embeddings for infrequent tokens is more informative (i.e., more helpful as side information) compared to static embeddings for frequent tokens (e.g., 'the').",no
,166-ARR,166-ARR_v2_13@2,166-ARR_v1_78@0,"To improve the compression-reliability tradeoff, we leverage static token embeddings, which are available since the ranker has access to the document text (as it needs to render it to the user), and are computationally cheap to obtain.","In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction models.",no
,166-ARR,166-ARR_v2_71@2,166-ARR_v1_89@13,"Using a quantization technique fitted to the Gaussian distribution of post randomized Hadamard transform data further improve quality, making the EDEN quantization superior to other quantization techniques in our case.",This variant of DRIVE optimizes for lower bias over the mean squared error (MSE) by multiplying the dequantization result in Algorithm 1 by a bias correction scalar: x 2 2 / ŷ 2 2 .,no
,166-ARR,166-ARR_v2_16@0,166-ARR_v1_78@0,"To summarize, here are the contribution of this work 1 :","In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction models.",no
1500,172-ARR,172-ARR_v2_36@1,172-ARR_v1_40@1,"We tested our adaptation strategy on three different language encoders coupled with CLIP-T, including BERT-base, ELECTRA-base, and ELECTRA-large.","We tested our pretraining strategy on three different language encoders coupled with CLIP-T, including BERT-base, ELECTRA-base, and ELECTRA-large.",yes
1501,172-ARR,172-ARR_v2_41@3,172-ARR_v1_45@3,"Analyzing the separated entries as a whole, we discovered that the betterperforming entries have a larger visually grounded ratio (Figure 4), as the quartile, median and mean values are generally higher for improved samples.",We found out that the better-performing entries have a shorter sentence length and their tokens have a larger visually grounded ratio (Figure 3).,yes
1503,172-ARR,172-ARR_v2_49@6,172-ARR_v1_55@6,"The issue is also present in the finetuning phase, and the maximum sequence length of GLUE and SWAG is 128; therefore we used 2 blocks of CLIP sub-sequences to model it.","The issue is also present in the finetuning phase, and the maxmum sequence length of GLUE and SWAG is 128; therefore we used 2 blocks of CLIP sub-sequences to model it.",yes
1505,172-ARR,172-ARR_v2_43@2,172-ARR_v1_62@2,"Other experiments include changing the number of layers in the crossmodal encoder, training for longer, and swapping to a much larger wiki (14G).","Other attempts include changing the number of layers in the cross-modal encoder, training for longer, and swapping to a much larger wiki (14G).",yes
1506,172-ARR,172-ARR_v2_44@0,172-ARR_v1_63@0,"Besides experimental evidence, we also justify the CLIPTC loss via further analysis, as the CLIPTC objective can theoretically be trivially solved by identity mapping.","Besides experimental evidence, we also justify the VC loss via further analysis because VC loss can theoretically be trivially solved by identity mapping.",yes
1507,172-ARR,172-ARR_v2_44@1,172-ARR_v1_63@1,"Despite this possibility, we find that the loss is crucial to cross attention learning.",The importance of this loss is to balances out the cross-modal matching loss.,yes
1508,172-ARR,172-ARR_v2_44@2,172-ARR_v1_63@2,"Since we do not impose negative hard samples from sampled sentences, the MATCH objective can be solved sufficiently simply by guiding the cross attention to focus on common trivial words.","Without VC loss, the cross-modal matching is too easy, because relying on only a few common words is sufficient, even when the word is trivial to the meaning of the sentence.",yes
1509,172-ARR,172-ARR_v2_44@4,172-ARR_v1_63@5,We show comparisons of the attention maps generated from the cross-modal encoders with a random sequence from RTE in Table 9 in the Appendix to verify this claim.,We show the attention of the cross-modal matching with a random sequence from RTE in Table 9.,yes
1510,172-ARR,172-ARR_v2_0@0,172-ARR_v1_0@0,XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding,XDBERT: Distilling Visual Information to BERT via Cross-Modal Encoders to Improve Language Understanding,yes
1511,172-ARR,172-ARR_v2_7@0,172-ARR_v1_7@0,"In this work, we establish the link between pretrained multimodal transformers and visuallygrounded language learning.","In this work, we established the link between pretrained multimodal transformers and visuallygrounded language learning.",yes
1512,172-ARR,172-ARR_v2_7@1,172-ARR_v1_7@1,"We devise a way to distill visual information from components of a pretrained multimodal transformer (CLIP texttransfomer, abbreviated as CLIP-T) to pretrained language transformers (BERT/ELECTRA), to incorporate versatile perception of words into the model (Figure 1).","We devised a way to distill visual information from components of a pretrained multimodal transformer (CLIP texttransfomer, briefed by CLIP-T) to pretrained language transformers (BERT/ELECTRA).",yes
1513,172-ARR,172-ARR_v2_8@0,172-ARR_v1_8@0,"Methodologically, we use the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot fewer than the original pretraining steps).","Methodologically, we used the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot smaller than the original pretraining steps).",yes
1514,172-ARR,172-ARR_v2_8@3,172-ARR_v1_10@1,We do ablation studies to show that each of the task provides improvement (Section 5).,We do ablation studies to show that each of the task provides improvement (Appendix D).,yes
1515,172-ARR,172-ARR_v2_24@1,172-ARR_v1_30@1,"In these tasks, BERT and CLIP-T takes sentences A and B respectively as input, and losses are calculated from both BERT output and CLIP-T output.","In these tasks, BERT and CLIP-T takes sentences A and B respectively as input as we condition the output.",yes
1516,172-ARR,172-ARR_v2_32@0,172-ARR_v1_36@0,"This is the MLM objective done on the CLIP-T side of the full model, omitting the masking part because CLIP has no mask token.","This is the MLM objective done on the CLIP-T side of the full model, but the CLIP-T sentence input tokens is neither being masked nor replaced.",yes
,172-ARR,172-ARR_v2_43@3,172-ARR_v1_64@2,"Swapping to wiki reduces potential overfitting from the 20 Epochs setting trained on wiki103, as training for the same amount of steps on wiki is less than 1 epoch.","In the following examples, Bold words are visuallygrounded, while normal words are non-visuallygrounded.",no
,172-ARR,172-ARR_v2_43@3,172-ARR_v1_62@0,"Swapping to wiki reduces potential overfitting from the 20 Epochs setting trained on wiki103, as training for the same amount of steps on wiki is less than 1 epoch.",We report the performance of small tasks while using different loss functions.,no
,172-ARR,172-ARR_v2_43@3,172-ARR_v1_48@0,"Swapping to wiki reduces potential overfitting from the 20 Epochs setting trained on wiki103, as training for the same amount of steps on wiki is less than 1 epoch.",We briefly describe the ablation studies on the adaptation process.,no
,172-ARR,172-ARR_v2_44@3,172-ARR_v1_36@1,"With the CLIPTC objective, the diversity of the input embeddings corresponding to different tokens must be retained in the cross-modal encoder, leading to more robust cross-modal attention.",This loss prevents the cross-modal matching to only focus on common trivial words.,no
,172-ARR,172-ARR_v2_28@1,172-ARR_v1_64@3,We modify this objective to same sentence prediction as both streams of our model takes text as input.,Words in brackets are stopwords and does not count towards either category.,no
,172-ARR,172-ARR_v2_43@3,172-ARR_v1_63@4,"Swapping to wiki reduces potential overfitting from the 20 Epochs setting trained on wiki103, as training for the same amount of steps on wiki is less than 1 epoch.","By adding VC loss, each VC token must keep its representation, which makes the cross-modal attention more robust.",no
,172-ARR,172-ARR_v2_41@2,172-ARR_v1_64@3,Entries where both models obtain the same performance are set aside.,Words in brackets are stopwords and does not count towards either category.,no
,172-ARR,172-ARR_v2_32@1,172-ARR_v1_63@3,"Same as MLM, 15% of the tokens are randomly selected for reconstruction.","Driven by the loss function of cross-modal matching, the model sacrifices rare word representations to better match common words.",no
,172-ARR,172-ARR_v2_22@1,172-ARR_v1_45@2,"The text-image input (X i , Y i ) is paired, and every (X j , Y k ) (j ̸ = k) is a non-pair.","XDBERT-b gains performance on 52 entries, while performing on par or worse on 225 entries.",no
,172-ARR,172-ARR_v2_22@1,172-ARR_v1_36@1,"The text-image input (X i , Y i ) is paired, and every (X j , Y k ) (j ̸ = k) is a non-pair.",This loss prevents the cross-modal matching to only focus on common trivial words.,no
,172-ARR,172-ARR_v2_41@2,172-ARR_v1_52@0,Entries where both models obtain the same performance are set aside.,The results show that multi-modal training for solving vision-language tasks does not improve the performance of the models on natural language understanding tasks.,no
,172-ARR,172-ARR_v2_28@1,172-ARR_v1_63@3,We modify this objective to same sentence prediction as both streams of our model takes text as input.,"Driven by the loss function of cross-modal matching, the model sacrifices rare word representations to better match common words.",no
,172-ARR,172-ARR_v2_28@0,172-ARR_v1_63@3,"The Image-Text Matching (ITM) objective is widely used in multimodal learning (Tan and Bansal, 2020;Radford et al., 2021).","Driven by the loss function of cross-modal matching, the model sacrifices rare word representations to better match common words.",no
,172-ARR,172-ARR_v2_22@1,172-ARR_v1_52@0,"The text-image input (X i , Y i ) is paired, and every (X j , Y k ) (j ̸ = k) is a non-pair.",The results show that multi-modal training for solving vision-language tasks does not improve the performance of the models on natural language understanding tasks.,no
,172-ARR,172-ARR_v2_28@1,172-ARR_v1_63@4,We modify this objective to same sentence prediction as both streams of our model takes text as input.,"By adding VC loss, each VC token must keep its representation, which makes the cross-modal attention more robust.",no
1681,178-ARR,178-ARR_v2_17@2,178-ARR_v1_17@2,"Then, we will introduce our model including triaffine attention and triaffine scoring based on the proposed triaffine transformations.","Then, we will introduce our model based on the proposed triaffine transformations.",yes
1682,178-ARR,178-ARR_v2_19@0,178-ARR_v1_19@0,"We define the deep triaffine transformation with vectors u, v, w ∈ R d and a tensor W ∈ R d+1 × R d × R d+1 which outputs a scalar by applying distinct MLP (multi-layer perceptron) transformations on input vectors and calculating tensor vector multiplications.","We define the deep triaffine transformation with vectors u, v, w ∈ R d and a tensor W ∈ R d+1 × R d × R d+1 which outputs a scalar by applying distinct MLP transformations on input vectors and calculating tensor vector multiplications.",yes
1683,178-ARR,178-ARR_v2_42@0,178-ARR_v1_44@0,"Since h c i,j,r are composed by h ig,jg,r , we can decompose Equation 11 into following if and only if the layer of MLP transformation on h c i,j,r is 0:","Since h c i,j,r are composed by h ig,jg,r , we can decompose Equation 11 into:",yes
1684,178-ARR,178-ARR_v2_57@0,178-ARR_v1_59@1,"To fairly compare with previous works, we follow the same dataset split with Lu and Roth (2015) for ACE2004 and ACE2005 and use the split from for GENIA and KBP2017 datasets.","To fairly compare with previous works, we follow the same dataset split with Lu and Roth (2015) for ACE2004 and ACE2005 datasets and use the split from for GENIA and KBP2017 datasets.",yes
1685,178-ARR,178-ARR_v2_6@0,178-ARR_v1_6@0,"Although some of the factors may have been explored in previous works, to the best of our knowledge, it is the first work to fuse all these heterogeneous factors into a unified network.","Although some of the factors may be explored in previous works, to the best of our knowledge, it is the first work to fuse all these heterogeneous factors into a unified network.",yes
1688,178-ARR,178-ARR_v2_2@4,178-ARR_v1_2@4,Triaffine attention uses boundaries and labels as queries and uses inside tokens and related spans as keys and values for span representations.,"Triaffine attention uses boundaries and labels as queries, and uses inside tokens and related spans as keys and values for span representations.",yes
1689,178-ARR,178-ARR_v2_15@3,178-ARR_v1_15@3,"Except for the similar formula of vectors' interactions, the motivation and the use of triaffine are different in our paper.",There are two key differences between our triaffine transformation and theirs.,yes
1690,178-ARR,178-ARR_v2_15@4,178-ARR_v1_15@4,"Firstly, they only model the homogeneous features such as three tokens, but our triaffine transformation can model heterogeneous factors including labels, boundaries, and related spans.","Firstly, they only model the homogeneous features such as three tokens, but our triaffine transformation can model heterogeneous factors.",yes
,178-ARR,178-ARR_v2_88@0,178-ARR_v1_7@3,"We set seeds of torch, torch.cuda, numpy, and random in Python to ensure reproducibility.",We will release our codes and models for further research.,no
,178-ARR,178-ARR_v2_88@1,178-ARR_v1_7@3,We use a grid search to find the best hyperparameters depending on development set performances.,We will release our codes and models for further research.,no
,178-ARR,178-ARR_v2_88@3,178-ARR_v1_7@3,"If the contextual embedding learning rate is 3e-5, we use static embedding learning rate and task learning rate as 5e-4 and 3e-5.",We will release our codes and models for further research.,no
,178-ARR,178-ARR_v2_89@0,178-ARR_v1_7@3,"We introduce the decomposition of triaffine scoring in calculating p i,j,r and p c i,j,r .",We will release our codes and models for further research.,no
,178-ARR,178-ARR_v2_57@0,178-ARR_v1_17@2,"To fairly compare with previous works, we follow the same dataset split with Lu and Roth (2015) for ACE2004 and ACE2005 and use the split from for GENIA and KBP2017 datasets.","Then, we will introduce our model based on the proposed triaffine transformations.",no
,178-ARR,178-ARR_v2_15@4,178-ARR_v1_44@0,"Firstly, they only model the homogeneous features such as three tokens, but our triaffine transformation can model heterogeneous factors including labels, boundaries, and related spans.","Since h c i,j,r are composed by h ig,jg,r , we can decompose Equation 11 into:",no
,178-ARR,178-ARR_v2_17@2,178-ARR_v1_44@0,"Then, we will introduce our model including triaffine attention and triaffine scoring based on the proposed triaffine transformations.","Since h c i,j,r are composed by h ig,jg,r , we can decompose Equation 11 into:",no
,178-ARR,178-ARR_v2_17@2,178-ARR_v1_15@4,"Then, we will introduce our model including triaffine attention and triaffine scoring based on the proposed triaffine transformations.","Firstly, they only model the homogeneous features such as three tokens, but our triaffine transformation can model heterogeneous factors.",no
1964,192-ARR,192-ARR_v2_2@5,192-ARR_v1_2@5,"Specifically, an entity recognizer and a similarity evaluator are first trained in parallel as two teachers from the source domain.","Specifically, an entity recognizer and a similarity evaluator teachers are first trained in parallel from the source domain.",yes
1965,192-ARR,192-ARR_v2_21@2,192-ARR_v1_23@2,"During the learning process, the samples from the target language are fed into the teacher model and the outputs are taken as the supervisory signal for two tasks in the student model.","During the learning process, the samples from target language are fed into the teacher model and the outputs are taken as the supervisory signal for two tasks in the student model.",yes
1966,192-ARR,192-ARR_v2_2@6,192-ARR_v1_2@6,"Then, two tasks in the student model are supervised by these teachers simultaneously.","Then, two tasks in the student model are supervised by the two teachers simultaneously.",yes
1967,192-ARR,192-ARR_v2_25@2,192-ARR_v1_27@2,We aim to find entity similarity to help the crosslingual NER model in the target language.,We aim to find entity similarity to help the cross-lingual NER model in target language.,yes
1968,192-ARR,192-ARR_v2_25@4,192-ARR_v1_27@4,"To address this challenge, we propose a binary classifier called similarity evaluator to leverage the labeled source language data for similarity prediction.","To address this challenge, we propose a binary classifier called similarity teacher to leverage the labeled source language data for similarity prediction.",yes
1969,192-ARR,192-ARR_v2_2@7,192-ARR_v1_2@7,Empirical studies on the three datasets across 7 different languages confirm the effectiveness of the proposed model.,Empirical studies on the datasets across 7 different languages confirm the effectiveness of the proposed model.,yes
1970,192-ARR,192-ARR_v2_25@5,192-ARR_v1_27@5,"Our similarity evaluator model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.","Our similarity teacher model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.",yes
1971,192-ARR,192-ARR_v2_26@0,192-ARR_v1_28@0,Entity Recognizer,Entity Recognizer Teacher,yes
1972,192-ARR,192-ARR_v2_33@0,192-ARR_v1_35@0,"To leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.","In order to leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.",yes
1973,192-ARR,192-ARR_v2_36@0,192-ARR_v1_38@0,"The inter-entities similarity is measured on the hidden representations h i and h j of the tokens queried by the entity indices < i, j > on the sequences representations.","The inter-entities similarity is measured on the tokens hidden representations h i and h j , queried by the entity indices < i, j > on the sequences representations.",yes
1974,192-ARR,192-ARR_v2_4@1,192-ARR_v1_4@1,"The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) makes this task achieve significant performances.","The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) make this task achieves significant performances.",yes
1975,192-ARR,192-ARR_v2_42@0,192-ARR_v1_44@0,Teacher-student Distillation Learning,Teacher Student Distillation Learning,yes
1976,192-ARR,192-ARR_v2_43@0,192-ARR_v1_45@0,"In this section, we consider transferring the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task.","In this section, we consider to transfer the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task.",yes
1977,192-ARR,192-ARR_v2_44@0,192-ARR_v1_46@0,"The mBERT is also used as an encoder for the sentence siamese pair, and the entity token feature is queried from the latent sequence encoding representation.","The multi-lingual BERT is also used as encoder for the sentence siamese pair, and the entity token feature queried from the latent sequence encoding representation.",yes
1978,192-ARR,192-ARR_v2_4@2,192-ARR_v1_4@2,"However, since deep neural networks highly rely on a large amount of labelled training data, the annotation acquiring process is expensive and time consuming.","However, since deep neural networks highly relies on a large amount of labelled training data, the annotation acquiring process is expensive and time consuming.",yes
1979,192-ARR,192-ARR_v2_4@3,192-ARR_v1_4@3,This situation is more severe for zero-resource languages.,This situation is more severe for low-resource languages.,yes
1980,192-ARR,192-ARR_v2_2@0,192-ARR_v1_2@0,Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages.,Cross-lingual named entity recognition task is one of the critical problem for evaluating the potential transfer learning techniques on low resource languages.,yes
1981,192-ARR,192-ARR_v2_56@2,192-ARR_v1_57@3,"We set the batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014).","We set batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014).",yes
1982,192-ARR,192-ARR_v2_62@1,192-ARR_v1_63@1,"Specifically, compared with the remarkable RIKD, AdvPicker, and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].","Specifically, compared with the remarkable RIKD, AdvPicker and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].",yes
1983,192-ARR,192-ARR_v2_63@0,192-ARR_v1_64@0,Note that BERT-f performs better than our model on the Chinese dataset due to their re-tokenization of the dataset.,Note that Bert-f performs better than our model on Chinese dataset due to their re-tokenization of the dataset.,yes
1984,192-ARR,192-ARR_v2_66@1,192-ARR_v1_67@1,"That is, the teacher model has the same as the neural network structure of the student model.","That is, both of the teacher and student have the same neural network structure.",yes
1985,192-ARR,192-ARR_v2_66@2,192-ARR_v1_67@2,This causes a performance drop across all languages due to two single teachers cannot make a difference with the combination.,This causes a performance drop across all languages due to two single teachers cannot make a difference with combination.,yes
1986,192-ARR,192-ARR_v2_67@0,192-ARR_v1_68@0,"(2) MTMT w/o weighting, which set the α (•) , β and γ all to be 1 in the loss of student learning.","(2) MTMT w/o weighting, which set the α 1 ,α 2 , β and γ all to be 1 in the loss of student model learning.",yes
1987,192-ARR,192-ARR_v2_67@1,192-ARR_v1_68@1,"It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es), which validates that weighting loss can bring more confident knowledge to the student model.","It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es), which validates that weighting loss can bring more confident knowledge to student model.",yes
1988,192-ARR,192-ARR_v2_68@1,192-ARR_v1_69@1,"In this case, our approach degrades into the single teacherstudent learning model as in TSL (Wu et al., 2020a).","In this case, our approach degrades into the Single Teacher-Student learning model as in TSL (Wu et al., 2020a).",yes
1989,192-ARR,192-ARR_v2_71@1,192-ARR_v1_72@1,"Specifically, if there is a set of tokens in which every two of them have a high Entity Similarity score, and one of the tokens is predicted to have a distinct label while other tokens have identical labels, then the one with the distinct label is predicted wrongly and is corrected by the student model to have the label of all other tokens.","Specifically, if there is a set of tokens in which every two of them have high Entity Similarity score, and one of the tokens is predicted to have a distinct label while other tokens have identical labels, then the one with the distinct label is predicted wrongly and is corrected by the student model to have the label of all other tokens.",yes
1990,192-ARR,192-ARR_v2_72@0,192-ARR_v1_73@0,Embedding Distribution,Embeddings Distribution,yes
1991,192-ARR,192-ARR_v2_73@1,192-ARR_v1_74@1,"It can be seen that the embedding distribution of the student model is close to similarity evaluator teacher, as illustrated in Figure 5.","It can be seen that the embeddings distribution of student model is close to similarity evaluator teacher, as illustrated in Figure 5.",yes
1992,192-ARR,192-ARR_v2_5@0,192-ARR_v1_6@0,Many studies have been done to solve this crosslingual NER problem.,Many studies have been done to solve this crosslanguage NER problem.,yes
1993,192-ARR,192-ARR_v2_73@2,192-ARR_v1_74@2,"We conjecture that the student model captures similarity knowledge from the similarity evaluator teacher, i.e. the same class of examples tend to cluster and the different class of examples tend to segregate in the embedding distribution.","We conjecture that the student model captures similarity knowledge from the similarity evaluator teacher, i.e. the same class of examples tend to cluster and the different class of examples tend to segregate in the embeddings distribution.",yes
1994,192-ARR,192-ARR_v2_75@0,192-ARR_v1_76@0,"In this section, we evaluate the effectiveness of weight loss in student learning from a quantitative perspective.","In the section, we evaluate the effectiveness of the weighting loss in student learning from quantitative perspective.",yes
1995,192-ARR,192-ARR_v2_76@1,192-ARR_v1_77@1,"Therefore, the student model is better suited to the target language with learning fewer low-confidence misrecognitions for the target language.","Therefore, the student model is better suited to target language with learning less low-confidence misrecognitions for the target language.",yes
1996,192-ARR,192-ARR_v2_77@1,192-ARR_v1_78@1,The encoder of the student model obtains the clustering information of the target language with the help of β.,The encoder of student model obtains the clustering information of the target language with the help of β.,yes
1997,192-ARR,192-ARR_v2_78@2,192-ARR_v1_79@2,"The student model learns less from unreasonable results, and it can make more accurate entity recognition for the target language.","The student model learns less from unreasonable results, and it can make more accuracy entity recognition for the target language.",yes
1998,192-ARR,192-ARR_v2_80@2,192-ARR_v1_81@2,"Moreover, to guarantee the student learning performance, we also propose a weighting strategy to take into consideration the reliability of the teachers.","Moreover, in order to guarantee the student learning performance, we also propose a weighting strategy to take consideration of the reliability of the teachers.",yes
1999,192-ARR,192-ARR_v2_5@2,192-ARR_v1_6@2,"Shared feature space based models exploit language-independent features, which lacks the domain-specific features for the target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).","Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).",yes
2000,192-ARR,192-ARR_v2_6@0,192-ARR_v1_7@0,"Although the above-mentioned models solve the cross-lingual NER problem to some extent, the auxiliary tasks, as in multi-task learning, have not been studied in this problem.","Although above mentioned models solve the cross-lingual NER problem in some extent, the auxiliary tasks, as in the multi-task learning, have not been studied in this problem.",yes
2001,192-ARR,192-ARR_v2_2@1,192-ARR_v1_2@1,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer.,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority.,yes
2002,192-ARR,192-ARR_v2_6@1,192-ARR_v1_7@1,"Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target languages.","Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target language.",yes
2003,192-ARR,192-ARR_v2_7@1,192-ARR_v1_8@1,"Given a Spanish sentence ""Arévalo (Avila), 23 may (EFE)."", the token ""Arévalo"" is recognized as ORG type using the learned model from the English domain.","Given a Spanish sentence ""Arévalo (Avila), 23 may (EFE)."", the token ""Arévalo"" is recognized as ORG type using the learned model from English domain.",yes
2004,192-ARR,192-ARR_v2_2@2,192-ARR_v1_2@2,"However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains.","However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domain.",yes
2005,192-ARR,192-ARR_v2_9@0,192-ARR_v1_10@0,We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results show the superiority of our presented MTMT model.,We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results shows the superiority our presented MTMT model.,yes
2006,192-ARR,192-ARR_v2_13@0,192-ARR_v1_14@0,"Our approach is closely related to the existing works on cross-lingual NER, knowledge distillation, and siamese network.","Our approach is closely related to the existing works on cross-lingual NER, knowledge distillation and siamese network.",yes
2007,192-ARR,192-ARR_v2_15@1,192-ARR_v1_16@1,"Recently, the pre-trained multilingual language model is effective to address the challenge (Devlin et al., 2019).","Recently, the pre-trained multilingual language models mBERT is effective to address the challenge (Devlin et al., 2019).",yes
2008,192-ARR,192-ARR_v2_15@2,192-ARR_v1_16@2,"Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from the labeled source language to that of target languages (Keung et al., 2019).","Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019).",yes
2009,192-ARR,192-ARR_v2_16@0,192-ARR_v1_17@0,Translation based models generally generate pseudo-labeled target data to alleviate target data scarcity.,Translation based models generally generate pesudo labeled target data to alleviate target data scarcity.,yes
2010,192-ARR,192-ARR_v2_2@4,192-ARR_v1_2@4,"In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on the target domain.","In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on target domain.",yes
2011,192-ARR,192-ARR_v2_16@1,192-ARR_v1_17@1,"For example, (Wu et al., 2020b;Zhang et al., 2021) gain an improvement by translating the labeled source language to the target language word-by-word.","For example, (Wu et al., 2020b;Zhang et al., 2021) gain a improvement by translating the labeled source language to the target language word-by-word.",yes
2012,192-ARR,192-ARR_v2_17@0,192-ARR_v1_18@0,"Knowledge distillation based models include a teacher model and a student model (Wu et al., 2020c).","Knowledge distillation based models includes a teacher model and a student model (Wu et al., 2020c).",yes
2013,192-ARR,192-ARR_v2_17@1,192-ARR_v1_18@1,The teacher model is trained on the labeled source language.,The teacher model is trained on labeled source language.,yes
2014,192-ARR,192-ARR_v2_17@2,192-ARR_v1_18@2,The student model learns from the soft label predicted by the teacher model on unlabeled target language data.,The student model learns from the soft label predicted by teacher model on unlabeled target language data.,yes
2015,192-ARR,192-ARR_v2_18@2,192-ARR_v1_19@2,"However, there is a dilemma to adapt the siamese network to tokenlevel recognition tasks such as NER.","However, there is a dilemma to adapt siamese network to token-level recognition tasks such as NER.",yes
2016,192-ARR,192-ARR_v2_20@1,192-ARR_v1_21@1,Our framework is consist of two models: teacher training model learned from the source language and teacher-student distillation learning model learned from the target language.,Our framework is consist of two models: teacher training model learned from source language and teacher-student distillation learning model learned from target language.,yes
,192-ARR,192-ARR_v2_17@2,192-ARR_v1_67@2,The student model learns from the soft label predicted by the teacher model on unlabeled target language data.,This causes a performance drop across all languages due to two single teachers cannot make a difference with combination.,no
,192-ARR,192-ARR_v2_56@2,192-ARR_v1_2@5,"We set the batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014).","Specifically, an entity recognizer and a similarity evaluator teachers are first trained in parallel from the source domain.",no
,192-ARR,192-ARR_v2_15@1,192-ARR_v1_10@0,"Recently, the pre-trained multilingual language model is effective to address the challenge (Devlin et al., 2019).",We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results shows the superiority our presented MTMT model.,no
,192-ARR,192-ARR_v2_2@6,192-ARR_v1_77@1,"Then, two tasks in the student model are supervised by these teachers simultaneously.","Therefore, the student model is better suited to target language with learning less low-confidence misrecognitions for the target language.",no
,192-ARR,192-ARR_v2_36@0,192-ARR_v1_16@2,"The inter-entities similarity is measured on the hidden representations h i and h j of the tokens queried by the entity indices < i, j > on the sequences representations.","Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019).",no
,192-ARR,192-ARR_v2_6@1,192-ARR_v1_81@2,"Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target languages.","Moreover, in order to guarantee the student learning performance, we also propose a weighting strategy to take consideration of the reliability of the teachers.",no
,192-ARR,192-ARR_v2_36@0,192-ARR_v1_21@1,"The inter-entities similarity is measured on the hidden representations h i and h j of the tokens queried by the entity indices < i, j > on the sequences representations.",Our framework is consist of two models: teacher training model learned from source language and teacher-student distillation learning model learned from target language.,no
,192-ARR,192-ARR_v2_36@0,192-ARR_v1_35@0,"The inter-entities similarity is measured on the hidden representations h i and h j of the tokens queried by the entity indices < i, j > on the sequences representations.","In order to leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.",no
,192-ARR,192-ARR_v2_17@0,192-ARR_v1_16@2,"Knowledge distillation based models include a teacher model and a student model (Wu et al., 2020c).","Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019).",no
,192-ARR,192-ARR_v2_5@0,192-ARR_v1_7@0,Many studies have been done to solve this crosslingual NER problem.,"Although above mentioned models solve the cross-lingual NER problem in some extent, the auxiliary tasks, as in the multi-task learning, have not been studied in this problem.",no
,192-ARR,192-ARR_v2_2@4,192-ARR_v1_67@1,"In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on the target domain.","That is, both of the teacher and student have the same neural network structure.",no
,192-ARR,192-ARR_v2_75@0,192-ARR_v1_23@2,"In this section, we evaluate the effectiveness of weight loss in student learning from a quantitative perspective.","During the learning process, the samples from target language are fed into the teacher model and the outputs are taken as the supervisory signal for two tasks in the student model.",no
,192-ARR,192-ARR_v2_17@0,192-ARR_v1_81@2,"Knowledge distillation based models include a teacher model and a student model (Wu et al., 2020c).","Moreover, in order to guarantee the student learning performance, we also propose a weighting strategy to take consideration of the reliability of the teachers.",no
,192-ARR,192-ARR_v2_17@0,192-ARR_v1_16@1,"Knowledge distillation based models include a teacher model and a student model (Wu et al., 2020c).","Recently, the pre-trained multilingual language models mBERT is effective to address the challenge (Devlin et al., 2019).",no
,192-ARR,192-ARR_v2_18@2,192-ARR_v1_2@1,"However, there is a dilemma to adapt the siamese network to tokenlevel recognition tasks such as NER.",Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority.,no
,192-ARR,192-ARR_v2_25@5,192-ARR_v1_28@0,"Our similarity evaluator model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.",Entity Recognizer Teacher,no
,192-ARR,192-ARR_v2_63@0,192-ARR_v1_57@3,Note that BERT-f performs better than our model on the Chinese dataset due to their re-tokenization of the dataset.,"We set batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014).",no
,192-ARR,192-ARR_v2_2@1,192-ARR_v1_77@1,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer.,"Therefore, the student model is better suited to target language with learning less low-confidence misrecognitions for the target language.",no
,192-ARR,192-ARR_v2_63@0,192-ARR_v1_19@2,Note that BERT-f performs better than our model on the Chinese dataset due to their re-tokenization of the dataset.,"However, there is a dilemma to adapt siamese network to token-level recognition tasks such as NER.",no
,192-ARR,192-ARR_v2_75@0,192-ARR_v1_18@0,"In this section, we evaluate the effectiveness of weight loss in student learning from a quantitative perspective.","Knowledge distillation based models includes a teacher model and a student model (Wu et al., 2020c).",no
,192-ARR,192-ARR_v2_2@7,192-ARR_v1_35@0,Empirical studies on the three datasets across 7 different languages confirm the effectiveness of the proposed model.,"In order to leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.",no
,192-ARR,192-ARR_v2_16@1,192-ARR_v1_63@1,"For example, (Wu et al., 2020b;Zhang et al., 2021) gain an improvement by translating the labeled source language to the target language word-by-word.","Specifically, compared with the remarkable RIKD, AdvPicker and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].",no
,192-ARR,192-ARR_v2_7@1,192-ARR_v1_27@2,"Given a Spanish sentence ""Arévalo (Avila), 23 may (EFE)."", the token ""Arévalo"" is recognized as ORG type using the learned model from the English domain.",We aim to find entity similarity to help the cross-lingual NER model in target language.,no
,192-ARR,192-ARR_v2_16@0,192-ARR_v1_46@0,Translation based models generally generate pseudo-labeled target data to alleviate target data scarcity.,"The multi-lingual BERT is also used as encoder for the sentence siamese pair, and the entity token feature queried from the latent sequence encoding representation.",no
,192-ARR,192-ARR_v2_62@1,192-ARR_v1_76@0,"Specifically, compared with the remarkable RIKD, AdvPicker, and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].","In the section, we evaluate the effectiveness of the weighting loss in student learning from quantitative perspective.",no
,192-ARR,192-ARR_v2_6@0,192-ARR_v1_79@2,"Although the above-mentioned models solve the cross-lingual NER problem to some extent, the auxiliary tasks, as in multi-task learning, have not been studied in this problem.","The student model learns less from unreasonable results, and it can make more accuracy entity recognition for the target language.",no
,192-ARR,192-ARR_v2_71@1,192-ARR_v1_79@2,"Specifically, if there is a set of tokens in which every two of them have a high Entity Similarity score, and one of the tokens is predicted to have a distinct label while other tokens have identical labels, then the one with the distinct label is predicted wrongly and is corrected by the student model to have the label of all other tokens.","The student model learns less from unreasonable results, and it can make more accuracy entity recognition for the target language.",no
,192-ARR,192-ARR_v2_73@2,192-ARR_v1_10@0,"We conjecture that the student model captures similarity knowledge from the similarity evaluator teacher, i.e. the same class of examples tend to cluster and the different class of examples tend to segregate in the embedding distribution.",We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results shows the superiority our presented MTMT model.,no
,192-ARR,192-ARR_v2_66@1,192-ARR_v1_2@1,"That is, the teacher model has the same as the neural network structure of the student model.",Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority.,no
,192-ARR,192-ARR_v2_16@0,192-ARR_v1_18@1,Translation based models generally generate pseudo-labeled target data to alleviate target data scarcity.,The teacher model is trained on labeled source language.,no
,192-ARR,192-ARR_v2_2@0,192-ARR_v1_6@2,Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages.,"Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).",no
,192-ARR,192-ARR_v2_25@2,192-ARR_v1_74@1,We aim to find entity similarity to help the crosslingual NER model in the target language.,"It can be seen that the embeddings distribution of student model is close to similarity evaluator teacher, as illustrated in Figure 5.",no
,192-ARR,192-ARR_v2_6@1,192-ARR_v1_4@1,"Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target languages.","The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) make this task achieves significant performances.",no
,192-ARR,192-ARR_v2_26@0,192-ARR_v1_6@2,Entity Recognizer,"Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).",no
,192-ARR,192-ARR_v2_9@0,192-ARR_v1_19@2,We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results show the superiority of our presented MTMT model.,"However, there is a dilemma to adapt siamese network to token-level recognition tasks such as NER.",no
,192-ARR,192-ARR_v2_76@1,192-ARR_v1_6@2,"Therefore, the student model is better suited to the target language with learning fewer low-confidence misrecognitions for the target language.","Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).",no
,192-ARR,192-ARR_v2_77@1,192-ARR_v1_10@0,The encoder of the student model obtains the clustering information of the target language with the help of β.,We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results shows the superiority our presented MTMT model.,no
,192-ARR,192-ARR_v2_80@2,192-ARR_v1_16@2,"Moreover, to guarantee the student learning performance, we also propose a weighting strategy to take into consideration the reliability of the teachers.","Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019).",no
,192-ARR,192-ARR_v2_36@0,192-ARR_v1_7@1,"The inter-entities similarity is measured on the hidden representations h i and h j of the tokens queried by the entity indices < i, j > on the sequences representations.","Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target language.",no
,192-ARR,192-ARR_v2_67@1,192-ARR_v1_77@1,"It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es), which validates that weighting loss can bring more confident knowledge to the student model.","Therefore, the student model is better suited to target language with learning less low-confidence misrecognitions for the target language.",no
,192-ARR,192-ARR_v2_44@0,192-ARR_v1_16@2,"The mBERT is also used as an encoder for the sentence siamese pair, and the entity token feature is queried from the latent sequence encoding representation.","Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019).",no
,192-ARR,192-ARR_v2_67@1,192-ARR_v1_45@0,"It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es), which validates that weighting loss can bring more confident knowledge to the student model.","In this section, we consider to transfer the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task.",no
,192-ARR,192-ARR_v2_36@0,192-ARR_v1_64@0,"The inter-entities similarity is measured on the hidden representations h i and h j of the tokens queried by the entity indices < i, j > on the sequences representations.",Note that Bert-f performs better than our model on Chinese dataset due to their re-tokenization of the dataset.,no
,192-ARR,192-ARR_v2_17@2,192-ARR_v1_45@0,The student model learns from the soft label predicted by the teacher model on unlabeled target language data.,"In this section, we consider to transfer the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task.",no
,192-ARR,192-ARR_v2_33@0,192-ARR_v1_16@2,"To leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.","Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019).",no
,192-ARR,192-ARR_v2_63@0,192-ARR_v1_63@1,Note that BERT-f performs better than our model on the Chinese dataset due to their re-tokenization of the dataset.,"Specifically, compared with the remarkable RIKD, AdvPicker and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].",no
,192-ARR,192-ARR_v2_18@2,192-ARR_v1_67@1,"However, there is a dilemma to adapt the siamese network to tokenlevel recognition tasks such as NER.","That is, both of the teacher and student have the same neural network structure.",no
,192-ARR,192-ARR_v2_25@2,192-ARR_v1_27@5,We aim to find entity similarity to help the crosslingual NER model in the target language.,"Our similarity teacher model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.",no
,192-ARR,192-ARR_v2_4@2,192-ARR_v1_2@5,"However, since deep neural networks highly rely on a large amount of labelled training data, the annotation acquiring process is expensive and time consuming.","Specifically, an entity recognizer and a similarity evaluator teachers are first trained in parallel from the source domain.",no
,192-ARR,192-ARR_v2_33@0,192-ARR_v1_69@1,"To leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.","In this case, our approach degrades into the Single Teacher-Student learning model as in TSL (Wu et al., 2020a).",no
,192-ARR,192-ARR_v2_2@1,192-ARR_v1_35@0,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer.,"In order to leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.",no
,192-ARR,192-ARR_v2_7@1,192-ARR_v1_2@4,"Given a Spanish sentence ""Arévalo (Avila), 23 may (EFE)."", the token ""Arévalo"" is recognized as ORG type using the learned model from the English domain.","In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on target domain.",no
,192-ARR,192-ARR_v2_63@0,192-ARR_v1_2@7,Note that BERT-f performs better than our model on the Chinese dataset due to their re-tokenization of the dataset.,Empirical studies on the datasets across 7 different languages confirm the effectiveness of the proposed model.,no
2033,193-ARR,193-ARR_v2_2@4,193-ARR_v1_2@4,"Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not appear to enhance its performance on such tasks.","Overall, our results suggest that language modeling objectives incentivize the model to implicitly learn some notion of spelling, and that explicitly teaching the model how to spell does not enhance its performance on such tasks.",yes
2034,193-ARR,193-ARR_v2_23@2,193-ARR_v1_25@2,"One possibility is that Arabic's rich morphology incentivizes the model to store more information about each token's character composition; however, it is also possible that AraBERT's different vocabulary, which allocates shorter character sequences to each token type, might explain this difference (we discuss the link between sequence length and accuracy in Appendix C).","One possibility is that Arabic's rich morphology incentivizes the model to store more information about each token's character composition, however it is also possible that AraBERT's different vocabulary, which allocates shorter character sequences to each token type, might explain this difference (we discuss the link between sequence length and accuracy later in this section).",yes
2035,193-ARR,193-ARR_v2_30@4,193-ARR_v1_29@1,"8 We train RoBERTa-Large on English Wikipedia using the hyperparameter configuration of 24hBERT (Izsak et al., 2021), and cease training after 24 hours (approximately 16,000 steps).","6 We train RoBERTa-Large on English Wikipedia using the hyperparameter configuration of 24hBERT (Izsak et al., 2021), and cease training after 24 hours (∼16000 steps).",yes
2036,193-ARR,193-ARR_v2_31@3,193-ARR_v1_30@3,"This result indicates that in this scenario, the model does not utilize the character information injected into the tokens' embeddings.",This result indicates that the model does not utilize the character information injected into the tokens' embeddings.,yes
2037,193-ARR,193-ARR_v2_32@1,193-ARR_v1_30@4,"Along with the results from Section 4, we hypothesize that the implicit notion of spelling that the model learns during pretraining might be sufficient for masked language modeling.","Along with the results from Section 4, we conjecture that the model learns an implicit notion of spelling during pretraining, which is sufficient for masked language modeling, and does not benefit from explicitly adding orthographic information.",yes
2038,193-ARR,193-ARR_v2_7@1,193-ARR_v1_7@1,"We compare the pretraining process of the character-infused model to that of an identical model whose embedding layer is randomly initialized (and not pretrained), and find that both learning curves converge to virtually identical values within the first 1,000 gradient updates, a fraction of the total optimization process.","We compare the pretraining process of the character-infused model to that of an identical model whose embedding layer is randomly initialized (and not pretrained), and find that both learning curves converge to virtually identical values within the first 1000 gradient updates, a fraction of the total optimization process.",yes
2039,193-ARR,193-ARR_v2_7@2,193-ARR_v1_7@2,"This experiment suggests that while language models may need to learn some notion of spelling to optimize their objectives, they might also be able to quickly acquire most of the character-level information they need from plain token sequences without directly observing the composition of each token.","This experiment suggests that while language models may need to learn some notion of spelling to optimize their objectives, they can quickly acquire all the character-level information they need without directly observing the composition of each token.",yes
2040,193-ARR,193-ARR_v2_2@2,193-ARR_v1_2@2,Our results show that the embedding layers of RoBERTa and GPT2 each hold enough information to accurately spell up to a third of the vocabulary and reach high character ngram overlap across all token types.,Our results show that the embedding layer of RoBERTa holds enough information to accurately spell up to a third of the vocabulary and reach high average character ngram overlap on all token types.,yes
2041,193-ARR,193-ARR_v2_2@3,193-ARR_v1_2@3,"We further test whether enriching subword models with character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment.","We further test whether enriching subword models with additional character information can improve language modeling, and observe that this method has a near-identical learning curve as training without spelling-based enrichment.",yes
2042,193-ARR,193-ARR_v2_21@4,193-ARR_v1_23@3,"These results are persistent across different models and filters, strongly indicating that the embedding layer of pretrained models contains significant amounts of information about each token's character composition.","These results are persistent across different models and filters, strongly indicating that the embedding layer of pretrained models contain significant amounts of information about each token's character composition.",yes
,193-ARR,193-ARR_v2_25@0,193-ARR_v1_28@3,Character-Aware Models Some models are provided with the raw character sequence of each token.,5 We provide additional analysis on spelling accuracy by subword frequency and length in Appendices B and C.,no
,193-ARR,193-ARR_v2_25@0,193-ARR_v1_39@1,Character-Aware Models Some models are provided with the raw character sequence of each token.,These are the default hyperparameters for training a transformer language model in Fairseq .,no
,193-ARR,193-ARR_v2_28@1,193-ARR_v1_28@3,"We thus use GloVe (Pennington et al., 2014) to estimate a lower bound on character information that can be obtained by simple context-oblivious models.",5 We provide additional analysis on spelling accuracy by subword frequency and length in Appendices B and C.,no
,193-ARR,193-ARR_v2_28@2,193-ARR_v1_38@0,We probe the first 50K words in GloVe's vocabulary with SpellingBee.,"We implement SpellingBee with a 6-layer encoderdecoder model, with 512 model dimensions.",no
,193-ARR,193-ARR_v2_26@0,193-ARR_v1_36@0,"Although CharacterBERT's embedding layer is better at reconstructing original words (when similarity filters are applied), this does not mean that character-aware models are necessarily better downstream.",We manually analyze 100 random tokens that SpellingBee spelled incorrectly with the lemma filter to understand the nature of the spelling mistakes.,no
,193-ARR,193-ARR_v2_32@0,193-ARR_v1_28@3,"Although there are many possible ways to explicitly add orthographic information to tokens embeddings, our method is relatively straightforward as it gives the model a chance to utilize pre-stored character information.",5 We provide additional analysis on spelling accuracy by subword frequency and length in Appendices B and C.,no
,193-ARR,193-ARR_v2_28@0,193-ARR_v1_38@0,"The first generation of neural word representations (Mikolov et al., 2013a,b) contained only embedding layers, without any contextualization mechanism.","We implement SpellingBee with a 6-layer encoderdecoder model, with 512 model dimensions.",no
,193-ARR,193-ARR_v2_28@3,193-ARR_v1_28@3,"Table 2 shows that GloVe embeddings do contain a weak orthographic signal, better than random embeddings, but substantially weaker than the information stored in the embedding layer of large transformer-based language models.",5 We provide additional analysis on spelling accuracy by subword frequency and length in Appendices B and C.,no
,193-ARR,193-ARR_v2_26@0,193-ARR_v1_28@3,"Although CharacterBERT's embedding layer is better at reconstructing original words (when similarity filters are applied), this does not mean that character-aware models are necessarily better downstream.",5 We provide additional analysis on spelling accuracy by subword frequency and length in Appendices B and C.,no
,193-ARR,193-ARR_v2_32@0,193-ARR_v1_38@0,"Although there are many possible ways to explicitly add orthographic information to tokens embeddings, our method is relatively straightforward as it gives the model a chance to utilize pre-stored character information.","We implement SpellingBee with a 6-layer encoderdecoder model, with 512 model dimensions.",no
2066,195-ARR,195-ARR_v2_18@0,195-ARR_v1_12@4,MuST-SHE thus allows the identification and pinpointed evaluation of numerous and qualitatively different grammatical gender instances under authentic conditions.,"Thus, MuST-SHE allows the identification of numerous and qualitatively different grammatical gender instances under authentic conditions.",yes
2067,195-ARR,195-ARR_v2_18@2,195-ARR_v1_12@6,"As a matter of fact, as Gygax et al. (2019) suggest, accounting for gender in languages with similar typological features allows for proper comparisons.","In fact, as Gygax et al. (2019) suggest, accounting for gender in languages with similar typological features allows for proper comparison.",yes
2068,195-ARR,195-ARR_v2_2@5,195-ARR_v1_2@5,"By shedding light on model behaviours, gender bias, and its detection at several levels of granularity, our findings emphasize the value of dedicated analyses beyond aggregated overall results.","Our work sheds light on model behaviours, gender bias, and its detection at several levels of granularity for English-French/Italian/Spanish.",yes
2069,195-ARR,195-ARR_v2_20@2,195-ARR_v1_14@2,"As shown in Table 1 (a-c), we differentiate among six POS categories: 7 i) articles, ii) pronouns, iii) nouns, and iv) verbs.","As shown in Table 1 (a-c), we differentiate among six POS categories: 6 i) articles, ii) pronouns, iii) nouns, iv) verbs.",yes
2070,195-ARR,195-ARR_v2_21@5,195-ARR_v1_15@5,"9 This annotation lets us verify whether a model consistently picks the same gender paradigm for all words in the chain, enabling the assessment of its syntagmatic behaviour.","8 This annotation let us verify whether a model consistently picks the same gender paradigm for all words in the chain, enabling the assessment of its syntagmatic behaviour.",yes
2071,195-ARR,195-ARR_v2_23@1,195-ARR_v1_17@1,"For each language pair, they annotated the whole corpus independently, based on detailed guidelines (see Appendix A).","For each language pair, they annotated the whole corpus independently, based on detailed guidelines (see Appendix §A).",yes
2072,195-ARR,195-ARR_v2_26@0,195-ARR_v1_21@0,Speech Translation models,Speech Translation models.,yes
2073,195-ARR,195-ARR_v2_27@2,195-ARR_v1_21@3,"Working on WinoMT/ST, Kocmi et al. (2020) correlate higher BLEU scores and gender stereotyping, whereas Costa-jussà et al. (2020) show that systems with lower performance tend to produce fewer feminine translations for occupations, but rely less on stereotypical cues.","Working on WinoMT/ST, (Kocmi et al., 2020) correlates higher BLEU scores and gender stereotyping, whereas (Costa-jussà et al., 2020a) shows that systems with lower performance tend to produce fewer feminine translations for occupations, but rely less on stereotypical cues.",yes
2074,195-ARR,195-ARR_v2_29@4,195-ARR_v1_22@7,"Thus, for the sake of comparison with , we train these systems in the same (controlled) data conditions i.e. on the MuST-C corpus only.","Thus, for the sake of comparison with , we train these systems in the same controlled data environment, i.e. on the MuST-C corpus only.",yes
2075,195-ARR,195-ARR_v2_30@0,195-ARR_v1_23@0,Evaluation method,Evaluation method.,yes
2078,195-ARR,195-ARR_v2_32@2,195-ARR_v1_24@1,"14 Finally, since we aim at gaining qualitative insights into systems' behaviour, and at ensuring a sound and thorough multifaceted evaluation, we overcome the described coverage limitation of the automatic evaluation by complementing it with a manual analysis of all the gender-marked words and agreement chains that remained out of coverage.","13 Finally, since we aim at gaining exhaustive, qualitative insights into systems' behaviour, and at ensuring a sound and thorough multifaceted evaluation, we overcome the described coverage limitation of the automatic evaluation by complementing it with a manual analysis of all the words and agreement chains that remained out of coverage.",yes
2079,195-ARR,195-ARR_v2_32@3,195-ARR_v1_24@2,"This extensive manual evaluation was carried out via a systematic annotation of systems' outputs, performed by the same linguists that enriched MuST-SHE, who provided the appropriate knowledge of both the resource and the evaluation task.","This extensive manual evaluation was carried out via a systematic annotation of systems' outputs, performed by the same linguists that enriched MuST-SHE, who provided the appropriate knowledge of both resource and evaluation tasks.",yes
2080,195-ARR,195-ARR_v2_36@0,195-ARR_v1_31@0,"At a finer level of granularity, we use our extension of MuST-SHE to inspect gender bias across open and closed class words.","At a finer level of granularity, we use MuST-SHE extension to inspect gender bias across open and closed class words.",yes
2081,195-ARR,195-ARR_v2_37@4,195-ARR_v1_33@5,"As discussed in Section 8, we believe that our breakdown per POS is informative inasmuch it prompts qualitative considerations on how to pursue gender bias mitigation in models and corpora (Czarnowska et al., 2021;Doughman et al., 2021).","As discussed in §8, we believe that our breakdown per POS is informative inasmuch it prompts qualitative considerations on how to pursue gender bias mitigation in models and corpora (Czarnowska et al., 2021;Doughman et al., 2021).",yes
2082,195-ARR,195-ARR_v2_49@1,195-ARR_v1_42@1,"Similarly to the approach employed for single words (Section 5.3), we discern between OOC chains due to: i) translation errors (Err), and ii) alternative translations preserving the source meaning.","Similarly to the approach employed for single words ( §5.3), we discern between OOC chains due to: i) translation errors (Err), and ii) alternative translations preserving the source meaning.",yes
2083,195-ARR,195-ARR_v2_49@4,195-ARR_v1_42@4,"Instead, when the generated alternative chain exhibits gender markings, we distinguish if the chosen gender is correct (C), wrong (W), or if the system produces a chain that does not respect gender agreement because it combines both feminine and masculine gender inflections (NO).","Instead, when the generated alternative chain exhibits gender markings, we distinguish if the chosen gender is correct (C), wrong (W), or if the system produces a chain that does not respect gender agreement, because it combines both feminine and masculine gender inflections (NO).",yes
2084,195-ARR,195-ARR_v2_50@1,195-ARR_v1_43@1,"Interestingly, such results are only partially corroborating previous analyses.","Interestingly, such results are only partially corroborating analyses.",yes
2085,195-ARR,195-ARR_v2_51@0,195-ARR_v1_43@2,"On the one hand, unlike the OOC words' results discussed in Section 5.3, we attest that CHAR models produce the highest proportion of translation errors.","On the one hand, unlike the OOC words' results discussed in §5.3, we attest that CHAR models produce the highest proportion of translation errors.",yes
2086,195-ARR,195-ARR_v2_51@3,195-ARR_v1_44@0,"Finally, again in line with our automatic evaluation (Table 6), we confirm that respecting agreement is not an issue for our ST models: we identify only 3 cases (2 for en-fr BPE, 1 for en-fr CHAR) where concord is broken (NO).","Finally, in line with our automatic evaluation (Table 6), we confirm that respecting agreement is not an issue for our ST models: we identify only 3 cases (2 for en-fr BPE, 1 for en-fr CHAR) where concord is broken (NO).",yes
2087,195-ARR,195-ARR_v2_51@7,195-ARR_v1_44@4,"However, the most common type among these outliers are constructions with semi-copula verbs (e.g. en: She... [became a vet]; it: ...E' [diventata F un M veterinatrio M ]), which -as discussed in Section 3.1 -exhibit a weaker agreement constraint.","However, the most common type among these outliers are constructions with semi-copula verbs (e.g. en: She... [became a vet]; it: ...E' [diventata F un M veterinatrio M ]), which -as discussed in §3.1 -exhibit a weaker agreement constraint.",yes
2088,195-ARR,195-ARR_v2_53@3,195-ARR_v1_46@3,"On three language pairs (English-French/Italian/Spanish), our study shows that, while all POS are subject to masculine skews, they are not impacted to the same extent.","On 3 language pairs (en-es/fr/it) our study shows that, while all POS are subject to masculine skews, they are not impacted to the same extent.",yes
2089,195-ARR,195-ARR_v2_54@1,195-ARR_v1_46@7,"Accordingly, our results are in line with previous studies showing that, in spite of lower generic performance, character-based segmentation exhibits a better capability at handling feminine translation at different levels of granularity.","Accordingly, our results are in line with previous studies showing that, in spite of lower generic performance, character-based segmentation favours feminine translation at different levels of granularity.",yes
2090,195-ARR,195-ARR_v2_56@3,195-ARR_v1_48@3,"Specifically, our investigation on the relation between data size/segmentation technique and gender bias provides initial cues on which models and components to audit and implement toward the goal of reducing gender bias.","Specifically, our investigation on the relation between model size/segmentation technique and gender bias provides initial cues on which models and components to audit and implement toward the goal of reducing gender bias.",yes
2091,195-ARR,195-ARR_v2_56@6,195-ARR_v1_48@6,"In fact, while it is known that the MuST-C corpus (Cattoni et al., 2021) used for training comprises a majority of masculine speakers, 21 the fact that certain lexical categories are more biased than others suggests that, on top of more coarse-grained quantitative attempts at gender balancing (Costa-jussà and de Jorge, 2020), data curation ought to account for more sensitive, nuanced, and qualitative asymmetries.","In fact, while it is known that the MuST-C corpus used for training comprises a majority of masculine speakers, 20 the fact that certain lexical categories are more biased than others suggests that, on top of more coarse-grained quantitative attempts at gender balancing (Costa-jussà and de Jorge, 2020), data curation ought to account for more sensitive, nuanced, and qualitative asymmetries.",yes
2092,195-ARR,195-ARR_v2_7@1,195-ARR_v1_6@1,"2 (2) In light of recent studies exploring how model design and overall perfomance interplay with gender bias (Roberts et al., 2020;, we rely on our manually curated resource to compare three ST models, which are trained on varying amounts of data, and built with different segmentation techniques: character and byte-pairencoding (BPE) (Sennrich et al., 2016).","2 (2) In light of recent studies exploring how model design and overall perfomance interplay with gender bias (Roberts et al., 2020;, we rely on our manually curated resource to compare three ST models, which are trained on varying amounts of data, and built with different segmentation techniques: character and bytepair-encoding (BPE) (Sennrich et al., 2016).",yes
2093,195-ARR,195-ARR_v2_58@0,195-ARR_v1_50@0,"While our experiments are limited to the binary linguistic forms represented in the used data, to the best of our knowledge, ST natural language corpora going beyond binarism do not yet exist.","While our experiments are largely limited to the binary linguistic forms represented in the used data, to the best of our knowledge, ST natural language corpora going beyond binarism do not yet exist.",yes
2094,195-ARR,195-ARR_v2_2@1,195-ARR_v1_2@1,"However, most of current evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions.","However, most evaluation practices adopt a word-level focus on a narrow set of occupational nouns under synthetic conditions.",yes
2095,195-ARR,195-ARR_v2_60@1,195-ARR_v1_53@1,"Successively, such guidelines have been refined and improved by means of discussions with the annotators, who had carried out a pilot annotation round to get acquainted with MuST-SHE language data.","Successively, such guidelines have been refined and improved by means of discussions with the annotators, who had carried out a trail annotation round to get acquainted with MuST-SHE language data.",yes
2096,195-ARR,195-ARR_v2_61@1,195-ARR_v1_54@1,"We ensured that at least one annotator per language was a native speaker, whereas the second one had at least a C1 proficiency level of the assigned language.",We ensured that at least one annotator per language was a native speaker.,yes
2097,195-ARR,195-ARR_v2_62@0,195-ARR_v1_55@0,"In this section we describe in detail the ST models created for our study, whose source code is publicly released at: https://github.com/ mgaido91/FBK-fairseq-ST/tree/acl_2021.","In this section we describe in detail the ST models created for our study, whose source code will be publicly released upon acceptance of the paper.",yes
2098,195-ARR,195-ARR_v2_64@0,195-ARR_v1_57@0,"Since the amount of ST data is limited (i.e. MuST-C - Cattoni et al. 2021-and Europarl-ST -Iranzo-Sánchez et al. 2020, knowledge transfer from the ASR and MT tasks is leveraged by respectively initializing the ST encoder with ASR pretrained weights (Weiss et al., 2017b;Bansal et al., 2019) and by distilling knowledge from a strong MT teacher (Liu et al., 2019).","Since the amount of ST data is limited (i.e. MuST-C -Cattoni et al. 2020-and Europarl-ST -Iranzo-Sánchez et al. 2020, knowledge transfer from the ASR and MT tasks is leveraged by respectively initializing the ST encoder with ASR pretrained weights (Weiss et al., 2017b;Bansal et al., 2019) and by distilling knowledge from a strong MT teacher (Liu et al., 2019).",yes
2099,195-ARR,195-ARR_v2_8@1,195-ARR_v1_6@3,"Finally, in line with concurring studies, we find that iv) character-based systems have an edge on translating gender phenomena, by favouring morphological and lexical diversity.","Finally, in line with concurring studies, we find that iv) character-based systems favour morphological and lexical diversity when translating gender phenomena.",yes
2100,195-ARR,195-ARR_v2_10@1,195-ARR_v1_8@1,"Along this line, focus has been given to bias analysis in models' innards and outputs (Vig et al., 2020;Costajussà et al., 2022), and to ascertain the validity of bias measurement practices (Blodgett et al., 2021;Antoniak and Mimno, 2021;Goldfarb-Tarrant et al., 2021).","Along this line, focus has been given to bias analysis in models' innards and ouputs (Vig et al., 2020;Costajussà et al., 2020b), and to ascertain the validity of bias measurament practices (Blodgett et al., 2021;Antoniak and Mimno, 2021;Goldfarb-Tarrant et al., 2021).",yes
2101,195-ARR,195-ARR_v2_10@2,195-ARR_v1_8@2,"Complementary mounting evidence suggests that -rather than striving for generalizations -gender bias detection ought to incorporate contextual and linguistic specificity (González et al., 2020;Ciora et al., 2021;Matthews et al., 2021;Malik et al., 2021;Kurpicz-Briki and Leoni, 2021), which however receives little attention due to a heavy focus on English NLP (Bender and Friedman, 2018).","Complementary evidence suggests thatrather than striving for generalizations -gender bias detection ought to incorporate contextual and linguistic specificity (González et al., 2020;Ciora et al., 2021;Matthews et al., 2021;Malik et al., 2021;Kurpicz-Briki and Leoni, 2021), which however receives little attention due to a heavy focus on English NLP.",yes
2102,195-ARR,195-ARR_v2_10@3,195-ARR_v1_8@3,"Purported agnostic approaches and evaluations (Bender, 2009) can prevent from drawing reliable conclusions and mitigating recommendations, as attested by monolingual studies on grammatical gender languages (Zhou et al., 2019;Gonen et al., 2019;Zmigrod et al., 2019) and in automatic translation scenarios (Vanmassenhove et al., 2018;Moryossef et al., 2019).","Purported agnostic approaches and evaluations (Bender, 2009) can prevent from drawing reliable conclusions and mitigating recommendations, as attested by monolingual studies on grammatical gender languages (Zhou et al., 2019;Gonen et al., 2019) and in automatic translation scenarios (Vanmassenhove et al., 2018;Moryossef et al., 2019).",yes
2103,195-ARR,195-ARR_v2_11@0,195-ARR_v1_8@4,"Unlike English, grammatical gender languages exhibit an elaborate morphological and syntactic system, where gender is overtly marked on numerous POS (e.g., verbs, determiners, nouns), and related words have to agree on the same gender features (see Figure 1 for an example).","Unlike English, grammatical gender languages exhibit an elaborate morphological and syntactic system, where gender is overtly marked on numerous POS (e.g., verbs, determiners, nouns), and related words have to agree on the same gender features.",yes
2104,195-ARR,195-ARR_v2_12@1,195-ARR_v1_9@1,"It has been progressively enriched with new features Kocmi et al., 2020), and adapted for ST (Costa-jussà et al., 2020).","It has been progressively enriched with new features Kocmi et al., 2020), and adapted for ST (Costa-jussà et al., 2020a).",yes
2105,195-ARR,195-ARR_v2_12@5,195-ARR_v1_9@5,"However, gender-marking involves also several other, so far less accounted POS categories, but if they are just as problematic is not clear yet.","However, gender-marking involves also several other, so far less accounted POS categories, but whether they are just as problematic is not clear yet.",yes
2106,195-ARR,195-ARR_v2_13@1,195-ARR_v1_10@1,"As a result, they maximize lexical and contextual variability to inspect whether translation models yield feminine under-representation in real-world-like scenarios .","As a result, they maximize variability to inspect whether translation models yield feminine under-representation in real-world-like scenarios .",yes
2107,195-ARR,195-ARR_v2_2@4,195-ARR_v1_2@4,"Focusing on speech translation, we conduct a multifaceted evaluation on three language directions (English-French/Italian/Spanish), with models trained on varying amounts of data and different word segmentation techniques.","On this basis, we conduct multifaceted automatic and manual evaluations for three speech translation models, trained on varying amounts of data and different word segmentation techniques.",yes
2108,195-ARR,195-ARR_v2_14@3,195-ARR_v1_10@7,"Closer to our intent, analyze the output of different ST systems and note that their models seem to wrongly pick divergent gender inflections for unrelated words in the same sentence (e.g. en: As a researcher, professor; fr: En tant que chercheuse F , professeur M ) but not for dependency-related ones (e.g. en: The classic Asian student; it: [La classica studentessa asiatica] F ).","Closer to our intent, analyze the output of different ST systems and note that their models seem to wrongly pick divergent gender inflections for unrelated words in the same sentence (e.g. fr: En tant que chercheuse F , professeur M ) but not for dependency-related ones (e.g. it: [la classica studentessa asiatica] F ).",yes
2109,195-ARR,195-ARR_v2_16@1,195-ARR_v1_12@1,"Rather than building it from scratch, we add two annotation layers to the existing MuST-SHE bench- mark , which is built on spoken language data retrieved from TED talks.","Rather than building it from scratch, we add two annotation layers to the existing TED-based MuST-SHE benchmark .",yes
2110,195-ARR,195-ARR_v2_16@2,195-ARR_v1_12@2,"Available for en-es/fr/it, it represents the only multilingual MT and ST GBET 5 exhibiting a natural variety of gender phenomena, which are balanced across feminine and masculine forms.","4 Available for en-es/fr/it, it represents the only multilingual MT/ST GBET 5 exhibiting a natural variety of gender phenomena.",yes
2111,195-ARR,195-ARR_v2_17@0,195-ARR_v1_12@3,"In the reference translations of the corpus, each target gender-marked word -corresponding to a neutral expression in the English source -is annotated with its alternative wrong gender form (e.g. en: the girl left; it: la<il> ragazza è an-data<andato> via).","In the reference translations, each gender-marked word -corresponding to a neutral expression in the source -is annotated with its alternative wrong gender form (e.g. en: the girl left; it: la<il> ragazza è andata<andato> via).",yes
,195-ARR,195-ARR_v2_32@4,195-ARR_v1_52@1,"Accordingly, we manage to make our study completely exhaustive by covering every gendermarked instance of MuST-SHE.","To ensure precision, the two layers of linguistic information have been added i) in the course of two separate annotation processes; ii) following strict and comprehensive guidelines.",no
,195-ARR,195-ARR_v2_6@1,195-ARR_v1_64@4,"In fact, to be grammatically correct, each word in the chain has to be inflected with the same (masculine or feminine) gender form.","The following (long) sentence is an example of such an attraction error, where the complement desperate is inflected in the same masculine and plural form as its just preceding noun rather than the chain functioning as subject (the nurse).",no
,195-ARR,195-ARR_v2_40@4,195-ARR_v1_64@0,"Also, note that the outcome of the manual analyses reiterates the results obtained with the automatic evaluation based on accuracy at the word-level, thus confirming its reliability.","We found that, even in the case of dependencies within a longer range, systems largely respect agreement in translation and consistently pick the same gender form for all co-related words.",no
,195-ARR,195-ARR_v2_68@8,195-ARR_v1_37@7,Figure 6 shows coverage of fully generated agreement chains split into feminine (F) and masculine (M) forms.,18 We noticed that CHAR's lower translation quality may have to do more with fluency rather than lexical issues.,no
,195-ARR,195-ARR_v2_32@5,195-ARR_v1_64@2,"Also, such additional manual evaluation serves as a proof-ofconcept to ensure the validity of the employed automatic evaluation metrics. .","Among these 4 cases, we found the above discussed weaker gender-enforcing structures (see the description of (semi-)copula verbs and their predicative complements in §6.2), and we also detected what resembles agreement attraction errors (Linzen et al., 2016).",no
,195-ARR,195-ARR_v2_32@5,195-ARR_v1_65@1,"Also, such additional manual evaluation serves as a proof-ofconcept to ensure the validity of the employed automatic evaluation metrics. .","We can thus conclude that, even taking into account longer dependencies, agreement still does not emerge as an issue entrenched with gender bias.",no
,195-ARR,195-ARR_v2_32@4,195-ARR_v1_63@0,"Accordingly, we manage to make our study completely exhaustive by covering every gendermarked instance of MuST-SHE.","To account for such longer spans, we considered all MuST-SHE sentences where both i) a word (or chain) functioning as a subject, and ii) its referring verb or predicative complement are annotated as gender-marked words in the references.",no
,195-ARR,195-ARR_v2_17@1,195-ARR_v1_26@0,"As further discussed in in Section 4.2, such a feature enables fine-grained analyses of gender realization, which can also disentangle systems' tendency to (over)generate masculine -over feminine forms -in translation.",Overall quality and gender translation,no
,195-ARR,195-ARR_v2_60@2,195-ARR_v1_64@0,The final version of the annotation guidelines is included in the resource release (ict.fbk.eu/must-she) and is also retrievable at: https://bit.ly/3CdU50s.,"We found that, even in the case of dependencies within a longer range, systems largely respect agreement in translation and consistently pick the same gender form for all co-related words.",no
,195-ARR,195-ARR_v2_60@2,195-ARR_v1_26@0,The final version of the annotation guidelines is included in the resource release (ict.fbk.eu/must-she) and is also retrievable at: https://bit.ly/3CdU50s.,Overall quality and gender translation,no
,195-ARR,195-ARR_v2_6@1,195-ARR_v1_27@3,"In fact, to be grammatically correct, each word in the chain has to be inflected with the same (masculine or feminine) gender form.","The higher overall translation quality of LARGE-BPE models is also reflected by the coverage scores (All-Cov), where 13 The scripts will be released upon paper acceptance.",no
,195-ARR,195-ARR_v2_32@5,195-ARR_v1_52@0,"Also, such additional manual evaluation serves as a proof-ofconcept to ensure the validity of the employed automatic evaluation metrics. .",A MuST-SHE Manual Annotation POS and agreement chains annotations were carried out on MuST-SHE reference translations.,no
,195-ARR,195-ARR_v2_17@1,195-ARR_v1_52@0,"As further discussed in in Section 4.2, such a feature enables fine-grained analyses of gender realization, which can also disentangle systems' tendency to (over)generate masculine -over feminine forms -in translation.",A MuST-SHE Manual Annotation POS and agreement chains annotations were carried out on MuST-SHE reference translations.,no
,195-ARR,195-ARR_v2_68@8,195-ARR_v1_64@4,Figure 6 shows coverage of fully generated agreement chains split into feminine (F) and masculine (M) forms.,"The following (long) sentence is an example of such an attraction error, where the complement desperate is inflected in the same masculine and plural form as its just preceding noun rather than the chain functioning as subject (the nurse).",no
,195-ARR,195-ARR_v2_17@1,195-ARR_v1_27@2,"As further discussed in in Section 4.2, such a feature enables fine-grained analyses of gender realization, which can also disentangle systems' tendency to (over)generate masculine -over feminine forms -in translation.","Also, in line with previous analyses , SMALL-BPE models outperform the CHAR ones by ∼1 BLEU point.",no
,195-ARR,195-ARR_v2_60@2,195-ARR_v1_37@7,The final version of the annotation guidelines is included in the resource release (ict.fbk.eu/must-she) and is also retrievable at: https://bit.ly/3CdU50s.,18 We noticed that CHAR's lower translation quality may have to do more with fluency rather than lexical issues.,no
,195-ARR,195-ARR_v2_68@8,195-ARR_v1_63@0,Figure 6 shows coverage of fully generated agreement chains split into feminine (F) and masculine (M) forms.,"To account for such longer spans, we considered all MuST-SHE sentences where both i) a word (or chain) functioning as a subject, and ii) its referring verb or predicative complement are annotated as gender-marked words in the references.",no
,195-ARR,195-ARR_v2_60@2,195-ARR_v1_64@4,The final version of the annotation guidelines is included in the resource release (ict.fbk.eu/must-she) and is also retrievable at: https://bit.ly/3CdU50s.,"The following (long) sentence is an example of such an attraction error, where the complement desperate is inflected in the same masculine and plural form as its just preceding noun rather than the chain functioning as subject (the nurse).",no
,195-ARR,195-ARR_v2_40@4,195-ARR_v1_52@1,"Also, note that the outcome of the manual analyses reiterates the results obtained with the automatic evaluation based on accuracy at the word-level, thus confirming its reliability.","To ensure precision, the two layers of linguistic information have been added i) in the course of two separate annotation processes; ii) following strict and comprehensive guidelines.",no
,195-ARR,195-ARR_v2_32@5,195-ARR_v1_27@3,"Also, such additional manual evaluation serves as a proof-ofconcept to ensure the validity of the employed automatic evaluation metrics. .","The higher overall translation quality of LARGE-BPE models is also reflected by the coverage scores (All-Cov), where 13 The scripts will be released upon paper acceptance.",no
,195-ARR,195-ARR_v2_17@1,195-ARR_v1_62@0,"As further discussed in in Section 4.2, such a feature enables fine-grained analyses of gender realization, which can also disentangle systems' tendency to (over)generate masculine -over feminine forms -in translation.","Considering long-range dependencies that go beyond the phrase level, a gender relevant covariation is also that of subject-verb agreement, as the one shown in Table 9 (see also footnote 1).",no
,195-ARR,195-ARR_v2_32@5,195-ARR_v1_26@0,"Also, such additional manual evaluation serves as a proof-ofconcept to ensure the validity of the employed automatic evaluation metrics. .",Overall quality and gender translation,no
,195-ARR,195-ARR_v2_32@4,195-ARR_v1_64@4,"Accordingly, we manage to make our study completely exhaustive by covering every gendermarked instance of MuST-SHE.","The following (long) sentence is an example of such an attraction error, where the complement desperate is inflected in the same masculine and plural form as its just preceding noun rather than the chain functioning as subject (the nurse).",no
,195-ARR,195-ARR_v2_68@8,195-ARR_v1_27@0,Figure 6 shows coverage of fully generated agreement chains split into feminine (F) and masculine (M) forms.,"Table 3 presents SacreBLEU (Post, 2018), 14 coverage, and gender accuracy scores on the MuST-SHE test sets.",no
,195-ARR,195-ARR_v2_6@1,195-ARR_v1_64@2,"In fact, to be grammatically correct, each word in the chain has to be inflected with the same (masculine or feminine) gender form.","Among these 4 cases, we found the above discussed weaker gender-enforcing structures (see the description of (semi-)copula verbs and their predicative complements in §6.2), and we also detected what resembles agreement attraction errors (Linzen et al., 2016).",no
,195-ARR,195-ARR_v2_6@1,195-ARR_v1_52@0,"In fact, to be grammatically correct, each word in the chain has to be inflected with the same (masculine or feminine) gender form.",A MuST-SHE Manual Annotation POS and agreement chains annotations were carried out on MuST-SHE reference translations.,no
,195-ARR,195-ARR_v2_60@2,195-ARR_v1_63@1,The final version of the annotation guidelines is included in the resource release (ict.fbk.eu/must-she) and is also retrievable at: https://bit.ly/3CdU50s.,"We identified 55 sentences for en-es, 54 for en-fr and 41 for en-it, and we manually analyzed all the corresponding systems' outputs.",no
,195-ARR,195-ARR_v2_32@5,195-ARR_v1_64@0,"Also, such additional manual evaluation serves as a proof-ofconcept to ensure the validity of the employed automatic evaluation metrics. .","We found that, even in the case of dependencies within a longer range, systems largely respect agreement in translation and consistently pick the same gender form for all co-related words.",no
,195-ARR,195-ARR_v2_60@2,195-ARR_v1_64@2,The final version of the annotation guidelines is included in the resource release (ict.fbk.eu/must-she) and is also retrievable at: https://bit.ly/3CdU50s.,"Among these 4 cases, we found the above discussed weaker gender-enforcing structures (see the description of (semi-)copula verbs and their predicative complements in §6.2), and we also detected what resembles agreement attraction errors (Linzen et al., 2016).",no
,195-ARR,195-ARR_v2_32@4,195-ARR_v1_63@1,"Accordingly, we manage to make our study completely exhaustive by covering every gendermarked instance of MuST-SHE.","We identified 55 sentences for en-es, 54 for en-fr and 41 for en-it, and we manually analyzed all the corresponding systems' outputs.",no
,195-ARR,195-ARR_v2_40@4,195-ARR_v1_63@1,"Also, note that the outcome of the manual analyses reiterates the results obtained with the automatic evaluation based on accuracy at the word-level, thus confirming its reliability.","We identified 55 sentences for en-es, 54 for en-fr and 41 for en-it, and we manually analyzed all the corresponding systems' outputs.",no
,195-ARR,195-ARR_v2_6@1,195-ARR_v1_27@2,"In fact, to be grammatically correct, each word in the chain has to be inflected with the same (masculine or feminine) gender form.","Also, in line with previous analyses , SMALL-BPE models outperform the CHAR ones by ∼1 BLEU point.",no
,195-ARR,195-ARR_v2_6@1,195-ARR_v1_26@0,"In fact, to be grammatically correct, each word in the chain has to be inflected with the same (masculine or feminine) gender form.",Overall quality and gender translation,no
,195-ARR,195-ARR_v2_40@4,195-ARR_v1_37@7,"Also, note that the outcome of the manual analyses reiterates the results obtained with the automatic evaluation based on accuracy at the word-level, thus confirming its reliability.",18 We noticed that CHAR's lower translation quality may have to do more with fluency rather than lexical issues.,no
,195-ARR,195-ARR_v2_60@2,195-ARR_v1_52@0,The final version of the annotation guidelines is included in the resource release (ict.fbk.eu/must-she) and is also retrievable at: https://bit.ly/3CdU50s.,A MuST-SHE Manual Annotation POS and agreement chains annotations were carried out on MuST-SHE reference translations.,no
,195-ARR,195-ARR_v2_17@1,195-ARR_v1_27@3,"As further discussed in in Section 4.2, such a feature enables fine-grained analyses of gender realization, which can also disentangle systems' tendency to (over)generate masculine -over feminine forms -in translation.","The higher overall translation quality of LARGE-BPE models is also reflected by the coverage scores (All-Cov), where 13 The scripts will be released upon paper acceptance.",no
,195-ARR,195-ARR_v2_32@4,195-ARR_v1_65@1,"Accordingly, we manage to make our study completely exhaustive by covering every gendermarked instance of MuST-SHE.","We can thus conclude that, even taking into account longer dependencies, agreement still does not emerge as an issue entrenched with gender bias.",no
,195-ARR,195-ARR_v2_6@1,195-ARR_v1_62@0,"In fact, to be grammatically correct, each word in the chain has to be inflected with the same (masculine or feminine) gender form.","Considering long-range dependencies that go beyond the phrase level, a gender relevant covariation is also that of subject-verb agreement, as the one shown in Table 9 (see also footnote 1).",no
,195-ARR,195-ARR_v2_68@8,195-ARR_v1_27@2,Figure 6 shows coverage of fully generated agreement chains split into feminine (F) and masculine (M) forms.,"Also, in line with previous analyses , SMALL-BPE models outperform the CHAR ones by ∼1 BLEU point.",no
,195-ARR,195-ARR_v2_32@5,195-ARR_v1_27@2,"Also, such additional manual evaluation serves as a proof-ofconcept to ensure the validity of the employed automatic evaluation metrics. .","Also, in line with previous analyses , SMALL-BPE models outperform the CHAR ones by ∼1 BLEU point.",no
,195-ARR,195-ARR_v2_6@1,195-ARR_v1_52@1,"In fact, to be grammatically correct, each word in the chain has to be inflected with the same (masculine or feminine) gender form.","To ensure precision, the two layers of linguistic information have been added i) in the course of two separate annotation processes; ii) following strict and comprehensive guidelines.",no
,195-ARR,195-ARR_v2_60@2,195-ARR_v1_62@0,The final version of the annotation guidelines is included in the resource release (ict.fbk.eu/must-she) and is also retrievable at: https://bit.ly/3CdU50s.,"Considering long-range dependencies that go beyond the phrase level, a gender relevant covariation is also that of subject-verb agreement, as the one shown in Table 9 (see also footnote 1).",no
,195-ARR,195-ARR_v2_17@1,195-ARR_v1_65@1,"As further discussed in in Section 4.2, such a feature enables fine-grained analyses of gender realization, which can also disentangle systems' tendency to (over)generate masculine -over feminine forms -in translation.","We can thus conclude that, even taking into account longer dependencies, agreement still does not emerge as an issue entrenched with gender bias.",no
,195-ARR,195-ARR_v2_68@8,195-ARR_v1_65@1,Figure 6 shows coverage of fully generated agreement chains split into feminine (F) and masculine (M) forms.,"We can thus conclude that, even taking into account longer dependencies, agreement still does not emerge as an issue entrenched with gender bias.",no
2296,207-ARR,207-ARR_v2_2@5,207-ARR_v1_2@5,"We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracy on downstream tasks (e.g. GLUE).","We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracies on downstream tasks (e.g. GLUE).",yes
2297,207-ARR,207-ARR_v2_30@3,207-ARR_v1_30@3,Hoory et al. (2021a) improved on these bounds by using Gaussian noise.,Hoory et al. (2021b) improved on these bounds by using Gaussian noise.,yes
2298,207-ARR,207-ARR_v2_46@3,207-ARR_v1_44@2,"For DP-SP we use the unigram Sentence-Piece algorithm (Kudo and Richardson, 2018).","For DP-SP we use the popular unigram SentencePiece algorithm (Kudo and Richardson, 2018).",yes
2299,207-ARR,207-ARR_v2_58@2,207-ARR_v1_57@2,"Even for the most stringent guarantees of DP-Train ( of 6.06) which result in approx 20% of pretrain accuracy drop, on average GLUE fine-tuned performance is not significantly different from the baseline.","Even for the most stringent guarantees of DP-Train ( of 6.06) which result in approx 20% of pretrain accuracy drop, on average glue fine-tuned performance is not significantly different from the baseline.",yes
2300,207-ARR,207-ARR_v2_58@4,207-ARR_v1_57@4,"On average, we also see that full-DP is not significantly better on subsequent fine-tuning tasks (e.g. mnli_m, mnli_msm, qnli etc), however for some tasks (e.g. cola) fine-tuning performance is significantly better than that of a (non-private) baseline.","On average, We also see that full-DP is not significantly better on subsequent fine-tuning tasks (e.g. mnli_m, mnli_msm, qnli etc), however for some tasks (e.g. cola) fine-tuning performance is significantly better than that of a (non-private) baseline.",yes
2301,207-ARR,207-ARR_v2_58@5,207-ARR_v1_57@5,"On average, even DP-train models have approximately the same GLUE performance (difference insignificant).","On average, even DP-train models have approximately the same GLUE-fine tuning performance (the difference is insignificant).",yes
2302,207-ARR,207-ARR_v2_62@0,207-ARR_v1_60@0,"One important caveat here is that the span corruption training objective was splitting a piece of text into input/target randomly, so it is possible a different definition of memorization would be more suitable.","One important caveat here is that the span corruption training objective was splitting a piece of text into input/target randomly, so it is possible a different definition of memorization would be more suitable here.",yes
2303,207-ARR,207-ARR_v2_63@8,207-ARR_v1_62@4,For a large this protective effect is almost non-existent.,For a large this protective effect is almost non-existant.,yes
2304,207-ARR,207-ARR_v2_68@0,207-ARR_v1_67@0,"Theorem 1 (Hoory et al., 2021b) uses the notion of k and m (maximum L2 and infinity norms) of sentence-level (1-D vector) histogram.","Theorem 1 (Hoory et al., 2021a) uses the notion of k and m (maximum L2 and infinity norms) of sentence-level (1-D vector) histogram.",yes
2305,207-ARR,207-ARR_v2_68@7,207-ARR_v1_67@7,"Just as in (Hoory et al., 2021b), we assume that sentence length is 256 words, which is a very pessimistic estimate-this translates into approximately 2.45 tokens per word.","Just as in (Hoory et al., 2021a), we assume that sentence length is 256 words, which is a very pessimistic estimate-this translates into approximately 2.45 tokens per word.",yes
2306,207-ARR,207-ARR_v2_68@9,207-ARR_v1_67@9,"Additionally, (Hoory et al., 2021b) authors provide a Corollary that allows to obtain slightly better bounds while using approximately the same clipping norm and the same level of noise.","Additionally, (Hoory et al., 2021a) authors provide a Corollary that allows to obtain slightly better bounds while using approximately the same clipping norm and the same level of noise.",yes
2307,207-ARR,207-ARR_v2_13@0,207-ARR_v1_13@0,"In §5.2 we verify how our models fare with respect to an empirical ""privacy"" definition, namely robustness to training data extraction attacks.","In §5.2 we verify how our models fair with respect to an empirical ""privacy"" definition, namely robustness to training data extraction attacks.",yes
2308,207-ARR,207-ARR_v2_19@5,207-ARR_v1_19@5,"Hoory et al. (2021a) looked into DP fine-tuning of a (publicly) pre-trained BERT model (Devlin et al., 2019) in the medical domain and explored how DP fine tuning affects the performance and privacy of the models.","Hoory et al. (2021c) looked into DP fine-tuning of a (publicly) pre-trained BERT model (Devlin et al., 2019) in the medical domain and explored how DP fine tuning affects the performance and privacy of the models.",yes
,207-ARR,207-ARR_v2_13@0,207-ARR_v1_67@9,"In §5.2 we verify how our models fare with respect to an empirical ""privacy"" definition, namely robustness to training data extraction attacks.","Additionally, (Hoory et al., 2021a) authors provide a Corollary that allows to obtain slightly better bounds while using approximately the same clipping norm and the same level of noise.",no
,207-ARR,207-ARR_v2_30@3,207-ARR_v1_44@2,Hoory et al. (2021a) improved on these bounds by using Gaussian noise.,"For DP-SP we use the popular unigram SentencePiece algorithm (Kudo and Richardson, 2018).",no
,207-ARR,207-ARR_v2_58@2,207-ARR_v1_19@5,"Even for the most stringent guarantees of DP-Train ( of 6.06) which result in approx 20% of pretrain accuracy drop, on average GLUE fine-tuned performance is not significantly different from the baseline.","Hoory et al. (2021c) looked into DP fine-tuning of a (publicly) pre-trained BERT model (Devlin et al., 2019) in the medical domain and explored how DP fine tuning affects the performance and privacy of the models.",no
,207-ARR,207-ARR_v2_68@0,207-ARR_v1_62@4,"Theorem 1 (Hoory et al., 2021b) uses the notion of k and m (maximum L2 and infinity norms) of sentence-level (1-D vector) histogram.",For a large this protective effect is almost non-existant.,no
,207-ARR,207-ARR_v2_62@0,207-ARR_v1_44@2,"One important caveat here is that the span corruption training objective was splitting a piece of text into input/target randomly, so it is possible a different definition of memorization would be more suitable.","For DP-SP we use the popular unigram SentencePiece algorithm (Kudo and Richardson, 2018).",no
,207-ARR,207-ARR_v2_68@9,207-ARR_v1_57@5,"Additionally, (Hoory et al., 2021b) authors provide a Corollary that allows to obtain slightly better bounds while using approximately the same clipping norm and the same level of noise.","On average, even DP-train models have approximately the same GLUE-fine tuning performance (the difference is insignificant).",no
,207-ARR,207-ARR_v2_46@3,207-ARR_v1_57@4,"For DP-SP we use the unigram Sentence-Piece algorithm (Kudo and Richardson, 2018).","On average, We also see that full-DP is not significantly better on subsequent fine-tuning tasks (e.g. mnli_m, mnli_msm, qnli etc), however for some tasks (e.g. cola) fine-tuning performance is significantly better than that of a (non-private) baseline.",no
,207-ARR,207-ARR_v2_58@2,207-ARR_v1_30@3,"Even for the most stringent guarantees of DP-Train ( of 6.06) which result in approx 20% of pretrain accuracy drop, on average GLUE fine-tuned performance is not significantly different from the baseline.",Hoory et al. (2021b) improved on these bounds by using Gaussian noise.,no
,207-ARR,207-ARR_v2_63@8,207-ARR_v1_57@4,For a large this protective effect is almost non-existent.,"On average, We also see that full-DP is not significantly better on subsequent fine-tuning tasks (e.g. mnli_m, mnli_msm, qnli etc), however for some tasks (e.g. cola) fine-tuning performance is significantly better than that of a (non-private) baseline.",no
,207-ARR,207-ARR_v2_58@2,207-ARR_v1_57@4,"Even for the most stringent guarantees of DP-Train ( of 6.06) which result in approx 20% of pretrain accuracy drop, on average GLUE fine-tuned performance is not significantly different from the baseline.","On average, We also see that full-DP is not significantly better on subsequent fine-tuning tasks (e.g. mnli_m, mnli_msm, qnli etc), however for some tasks (e.g. cola) fine-tuning performance is significantly better than that of a (non-private) baseline.",no
,207-ARR,207-ARR_v2_30@3,207-ARR_v1_60@0,Hoory et al. (2021a) improved on these bounds by using Gaussian noise.,"One important caveat here is that the span corruption training objective was splitting a piece of text into input/target randomly, so it is possible a different definition of memorization would be more suitable here.",no
,207-ARR,207-ARR_v2_2@5,207-ARR_v1_30@3,"We focus on T5 and show that by using recent advances in JAX and XLA we can train models with DP that do not suffer a large drop in pre-training utility, nor in training speed, and can still be fine-tuned to high accuracy on downstream tasks (e.g. GLUE).",Hoory et al. (2021b) improved on these bounds by using Gaussian noise.,no
,207-ARR,207-ARR_v2_58@4,207-ARR_v1_67@9,"On average, we also see that full-DP is not significantly better on subsequent fine-tuning tasks (e.g. mnli_m, mnli_msm, qnli etc), however for some tasks (e.g. cola) fine-tuning performance is significantly better than that of a (non-private) baseline.","Additionally, (Hoory et al., 2021a) authors provide a Corollary that allows to obtain slightly better bounds while using approximately the same clipping norm and the same level of noise.",no
2395,21-ARR,21-ARR_v2_11@0,21-ARR_v1_11@0,"Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., , 2021bXu & McAuley, 2022).","Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks .",yes
2396,21-ARR,21-ARR_v2_11@2,21-ARR_v1_11@2,"Hinton et al. (2015b) first introduced the idea of knowledge distillation to exploit the ""dark knowledge"" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model.","Hinton et al. (2015) first introduced the idea of knowledge distillation to exploit the ""dark knowledge"" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model.",yes
2397,21-ARR,21-ARR_v2_4@1,21-ARR_v1_4@1,"Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015b) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015b;Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Ahn et al., 2019;Park et al., 2019;Passalis & Tefas, 2018;Heo et al., 2019;Kim et al., 2018;Shi et al., 2021;Sanh et al., 2019;Jiao et al., 2019;Wang et al., 2020b).","Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015;Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Ahn et al., 2019;Park et al., 2019;Passalis & Tefas, 2018;Heo et al., 2019;Kim et al., 2018;Shi et al., 2021;Sanh et al., 2019;Jiao et al., 2019;Wang et al., 2020b).",yes
2398,21-ARR,21-ARR_v2_12@6,21-ARR_v1_12@6,"For example, meta pseudo labels (Pham et al., 2020) use meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model.","For example, meta pseudo labels (Pham et al., 2020) uses meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model.",yes
2399,21-ARR,21-ARR_v2_12@10,21-ARR_v1_12@10,"In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problems, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.","In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problem, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.",yes
2400,21-ARR,21-ARR_v2_13@3,21-ARR_v1_13@3,"Liu et al. (2020) proposed a self-distillation network which utilizes meta-learning to train a label-generator as a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers.","Liu et al. (2020) proposed a self-distillation network and utilizes meta-learning to train a label-generator, which is a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers.",yes
2401,21-ARR,21-ARR_v2_20@2,21-ARR_v1_20@2,"Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error (MSE) between student and teacher logits, the similarity between the student and the teacher's attention distribution, etc.","Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error between student and teacher logits, the similarity between the student and the teacher's attention distribution, etc.",yes
2402,21-ARR,21-ARR_v2_34@2,21-ARR_v1_29@2,"This ""learning to teach"" paradigm naturally fits the bi-level optimization framework in meta learning literature.","This ""learning to teach"" paradigm naturally fits the bi-level optimization framework in the meta learning literature.",yes
2405,21-ARR,21-ARR_v2_9@5,21-ARR_v1_9@5,"Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably because of its ability to adjust the parameters of the teacher model.","Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably because of its ability to adjust its parameters.",yes
,21-ARR,21-ARR_v2_72@2,21-ARR_v1_58@1,We find that there is no significant difference between between PKD and MetaDistil.,"For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.",no
,21-ARR,21-ARR_v2_68@5,21-ARR_v1_58@1,"As shown in Table 4, the cross-taught students underperform the static students taught by their own teachers by 0.27 and 0.12 for ResNet-32 and ResNet-20, respectively.","For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.",no
,21-ARR,21-ARR_v2_71@1,21-ARR_v1_58@1,"There are two possibilities: (1) the student better mimics the teacher, and (2) the changes of teacher helps student perform better on hard examples that would be incorrectly classified by the student with vanilla KD.","For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.",no
,21-ARR,21-ARR_v2_70@0,21-ARR_v1_58@1,"We also investigate why MetaDistil works by conducting experiments on the development sets of MNLI, SST, and MRPC, which are important tasks in GLUE that have a large, medium, and small training set, respectively.","For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.",no
,21-ARR,21-ARR_v2_67@1,21-ARR_v1_58@1,"Nevertheless, we would like to point out that as suggested by Hooker et al. (2020), model compression may lead to biases.","For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.",no
,21-ARR,21-ARR_v2_67@2,21-ARR_v1_58@1,"However, this is not an outstanding problem of our method but a common risk in model compression, which needs to be addressed in the future.","For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.",no
,21-ARR,21-ARR_v2_72@3,21-ARR_v1_58@1,This suggests that the improvement does not come from student better mimicking the teacher.,"For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.",no
,21-ARR,21-ARR_v2_71@2,21-ARR_v1_58@1,We conduct a series of analysis on the MRPC dataset.,"For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.",no
,21-ARR,21-ARR_v2_68@2,21-ARR_v1_58@1,"As shown in Table 4, on both experiments, dynamic MetaDistil outperforms conventional KD and static distillation with the teacher at the end of MetaDistil training.","For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.",no
2693,246-ARR,246-ARR_v2_35@0,246-ARR_v1_34@0,"As M approaches M test , the automatic metrics' τ values approach 1, which is much higher than the respective values at M jud , typically around 0.6-0.8.","As M approaches M test , the automatic metrics' τ values approach 1, which is significantly higher than the respective values at M jud , typically around 0.6-0.8.",yes
2694,246-ARR,246-ARR_v2_36@1,246-ARR_v1_35@1,"We see that on both datasets the judgments' rankings are still quite variable, reaching a maximum τ of around 0.8-0.85.","We see that on both datasets the judgments' rankings are still quite variable, reaching a maximum of around 0.8-0.85 τ .",yes
2695,246-ARR,246-ARR_v2_41@2,246-ARR_v1_40@2,"The largest decrease in width is in the ROUGE family of metrics on SummEval, potentially because that metric and dataset combination saw the biggest improvement in ranking stability (see Fig. 3).","The largest decrease in width is in the ROUGE family of metrics on SummEval, likely because that metric and dataset combination saw the biggest improvement in ranking stability (see Fig. 3).",yes
2696,246-ARR,246-ARR_v2_53@2,246-ARR_v1_52@2,"Based on a survey of recent summarization papers in *ACL conferences (see Appendix C), we found that the average improvement over baseline/state-of-the-art models that was reported on the CNN/Dailymail dataset was on average 0.5 ROUGE-1.","Based on a survey of summarization papers in *ACL conferences over the past few years, we found that the average improvement over baseline/state-of-the-art models that was reported on the CNN/Dailymail dataset was on average 0.5 ROUGE-1.",yes
2697,246-ARR,246-ARR_v2_54@0,246-ARR_v1_53@0,"To that end, we redefine a high quality evaluation metric to be one for which a small difference in scores reliably indicates a difference in quality.","To that extent, we redefine a high quality evaluation metric to be one for which a small difference in scores reliably indicates a difference in quality.",yes
2698,246-ARR,246-ARR_v2_6@1,246-ARR_v1_6@1,"First, the metrics' scores which are used in practice are not the ones which are evaluated in system-level correlations; researchers compare systems based on metric scores calculated on the entire test set but calculate scores for system-level correlations when evaluating metrics on a much smaller subset of judged summaries.","First, the metrics' scores which are used in practice are not the ones which are evaluated in system-level correlations: Researchers compare systems based on metric scores calculated on the entire test set but calculate scores for system-level correlations when evaluating metrics on a much smaller subset of judged summaries.",yes
2699,246-ARR,246-ARR_v2_65@1,246-ARR_v1_64@1,"We view our work as continuing this direction of research, described next.",We view our work as continuing this direction of research.,yes
2700,246-ARR,246-ARR_v2_9@2,246-ARR_v1_9@2,"This allows us to show, for example, that a ROUGE-1 score difference of less than 0.5 between systems has almost no correlation to how humans would rank the same two systems according to our best estimates ( §4.2).",This allows us to show that a ROUGE-1 score difference of less than 0.5 between systems has almost no correlation to how humans would rank the same two systems according to our best estimates ( §4.2).,yes
2701,246-ARR,246-ARR_v2_2@4,246-ARR_v1_2@4,"Second, we propose to calculate correlations only on pairs of systems that are separated by small differences in automatic scores which are commonly observed in practice.","Second, we propose to calculate correlations only on pairs of systems which are separated by differences in automatic scores that are commonly used to argue one system is of higher quality.",yes
,246-ARR,246-ARR_v2_77@4,246-ARR_v1_71@5,The results are shown in Table 1.,"Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).",no
,246-ARR,246-ARR_v2_31@2,246-ARR_v1_71@5,"When all M test test instances are used, the automatic metrics' rankings become near constant.","Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).",no
,246-ARR,246-ARR_v2_77@1,246-ARR_v1_71@5,"We selected papers from 2020 and 2021 that were published in a *ACL conference (including Findings), had ""summary"" or ""summarization"" in the title, proposed a new system, and compared systems on the CNN/DailyMail dataset with ROUGE.","Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).",no
,246-ARR,246-ARR_v2_31@1,246-ARR_v1_71@5,The τ value quantifies how similar two system rankings would be if they were computed with two random sets of M input documents.,"Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).",no
,246-ARR,246-ARR_v2_77@3,246-ARR_v1_71@5,We did not include ablation experiments for which the differences are likely smaller than the differences between the top two performing systems.,"Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).",no
,246-ARR,246-ARR_v2_31@3,246-ARR_v1_71@5,The error regions represent ±1 standard deviation.,"Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).",no
,246-ARR,246-ARR_v2_77@5,246-ARR_v1_71@5,The average reported difference was found to be 0.49 ROUGE-1.,"Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).",no
,246-ARR,246-ARR_v2_77@0,246-ARR_v1_71@5,"To estimate the difference in ROUGE-1 score that is commonly reported in papers, we performed a survey of recently published summarization papers.","Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).",no
,246-ARR,246-ARR_v2_76@0,246-ARR_v1_71@5,"Figs. 10 and 11 contain the r SYS ∆( , u) correlations for ROUGE, BERTScore, and QAEval for various combinations of and u on both the Summ-Eval and REALSumm datasets.","Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).",no
2709,249-ARR,249-ARR_v2_24@1,249-ARR_v1_25@1,"For machine learning as a service (MLaaS), A can be applied to perform a man-in-the-middle attack on the application programming interfaces.","For Machine Learning as a Service (MLaaS), A can be directly applied to perform a man-in-the-middle attack on the application programming interfaces.",yes
2710,249-ARR,249-ARR_v2_24@2,249-ARR_v1_25@2,"Moreover, even if the raw data are protected and the transmission channel is secure, a curious service provider can train its attacker A to collect personas of service users.","Moreover, even if the raw data are protected and the transmission channel is secure, a curious service provider can train its attacker model A to collect personas of service users.",yes
2711,249-ARR,249-ARR_v2_26@0,249-ARR_v1_27@0,The LM training objective in Equation 1 only considers the utility of chatbots.,The simple LM training objective only considers the utility of chatbots.,yes
2712,249-ARR,249-ARR_v2_27@0,249-ARR_v1_28@0,"Following the intuition that the adversary cannot obtain better results than a random guess, in Section 4.1, we propose KL loss that aims to flatten the persona predictor's estimated distribution.","Following the intuition that the adversary cannot obtain better results than a random guess, we propose KL loss that aims to flatten the persona predictor's estimated distribution.",yes
2713,249-ARR,249-ARR_v2_27@1,249-ARR_v1_28@1,"Based on minimizing the mutual information between hidden states f (u) of chatbots and private persona attributes s, we propose MI loss in Section 4.2.","Based on minimizing the mutual information between hidden states f (u) of chatbots and private persona attributes s, we propose MI loss.",yes
2714,249-ARR,249-ARR_v2_31@1,249-ARR_v1_30@2,"For optimization, we can leave out constant terms and the logarithm (Mireshghallah et al., 2021) to obtain the following loss function:","For optimization, we can leave out constant terms (Mireshghallah et al., 2021) and obtain the following loss function:",yes
2715,249-ARR,249-ARR_v2_42@2,249-ARR_v1_44@2,"Instead, we use p Ψ (s|f (u)) to approximate q(s|f (u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2019):","Instead, we use p Ψ (s|f (u)) to approximate q(s|f (u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2018):",yes
2716,249-ARR,249-ARR_v2_53@0,249-ARR_v1_55@0,Overall Objective,Overall Loss,yes
2717,249-ARR,249-ARR_v2_57@0,249-ARR_v1_59@0,Experiments,Experiment,yes
2718,249-ARR,249-ARR_v2_59@0,249-ARR_v1_61@0,Experimental Settings,Experimental Setting,yes
2721,249-ARR,249-ARR_v2_4@3,249-ARR_v1_4@3,"Unfortunately, large language models tend to memorize training data and some private data can be recovered from models (Pan et al., 2020;Carlini et al., 2021).","Unfortunately, large language models tend to memorize training data and some private data can be recovered from models via black-box training data extraction attacks (Carlini et al., 2021).",yes
2722,249-ARR,249-ARR_v2_68@1,249-ARR_v1_68@10,"Suppose the adversary knows the persona with Max-Ratio, then it can improve its guess by not predicting this persona, which is a threat for fewer persona labels (for example, binary classification).","Suppose the adversary knows the persona with Max-Ratio, then he can improve his guess by not predicting this persona, which is a threat for fewer persona labels (for example, binary classification).",yes
2723,249-ARR,249-ARR_v2_7@8,249-ARR_v1_7@8,"We combine proposed KL divergence loss (KL loss) with mutual information loss (MI loss) (Song et al., 2019) as additional defense objectives to train the GPT-2 and decrease the attacker's persona inference accuracy to 0.53%.","We combine proposed KL divergence loss (KL loss) with mutual information loss (MI loss) (Song et al., 2018) as additional defense objectives to train the GPT-2 and decrease the attacker's persona inference accuracy to 0.53%.",yes
2724,249-ARR,249-ARR_v2_12@7,249-ARR_v1_13@4,"Though the setup of this work is similar to ours, they merely consider simple cases of data recovery with given rules and suffer great utility degradation to obtain optimal defense performance.","Though the setup of this work is similar to ours, they merely considered simple cases of data recovery with given rules and suffered great utility degradation to obtain optimal defense performance.",yes
2725,249-ARR,249-ARR_v2_13@0,249-ARR_v1_14@0,Attacking on Language Models,Attacking on Casual Language Model,yes
,249-ARR,249-ARR_v2_27@2,249-ARR_v1_27@2,"Lastly, we show the overall training objective in Section 4.3.","To avoid black-box persona inference attacks, more constraints on privacy should be added for training LM-based chatbots.",no
,249-ARR,249-ARR_v2_27@2,249-ARR_v1_12@2,"Lastly, we show the overall training objective in Section 4.3.",Carlini et al. (2021) performed black-box model inversion attack on GPT-2 through descriptive prompts with beam search.,no
,249-ARR,249-ARR_v2_27@2,249-ARR_v1_12@3,"Lastly, we show the overall training objective in Section 4.3.",Lehman et al. (2021) examined BERT pretrained on Electronic Health Records via blank filling and model probing to recover Personal Health Information.,no
,249-ARR,249-ARR_v2_27@2,249-ARR_v1_12@4,"Lastly, we show the overall training objective in Section 4.3.","Furthermore, given black-box access to a language model's pre-train and finetune stages, Zanella-Béguelin et al. (2020) showed that sensitive sequences of the fine-tuning dataset can be extracted.",no
,249-ARR,249-ARR_v2_27@2,249-ARR_v1_12@5,"Lastly, we show the overall training objective in Section 4.3.","For the distributed client-server setup, Malekzadeh et al. (2021) considered the sensitive attribute leakage from the server side with honest-but-curious (HBC) classifiers.",no
,249-ARR,249-ARR_v2_27@2,249-ARR_v1_15@3,"Lastly, we show the overall training objective in Section 4.3.","Lastly, we comprehensively explain our proposed defense strategies in Section 3.3.",no
,249-ARR,249-ARR_v2_26@0,249-ARR_v1_25@2,The LM training objective in Equation 1 only considers the utility of chatbots.,"Moreover, even if the raw data are protected and the transmission channel is secure, a curious service provider can train its attacker model A to collect personas of service users.",no
,249-ARR,249-ARR_v2_59@0,249-ARR_v1_28@0,Experimental Settings,"Following the intuition that the adversary cannot obtain better results than a random guess, we propose KL loss that aims to flatten the persona predictor's estimated distribution.",no
,249-ARR,249-ARR_v2_24@2,249-ARR_v1_59@0,"Moreover, even if the raw data are protected and the transmission channel is secure, a curious service provider can train its attacker A to collect personas of service users.",Experiment,no
,249-ARR,249-ARR_v2_12@7,249-ARR_v1_44@2,"Though the setup of this work is similar to ours, they merely consider simple cases of data recovery with given rules and suffer great utility degradation to obtain optimal defense performance.","Instead, we use p Ψ (s|f (u)) to approximate q(s|f (u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2018):",no
,249-ARR,249-ARR_v2_53@0,249-ARR_v1_28@1,Overall Objective,"Based on minimizing the mutual information between hidden states f (u) of chatbots and private persona attributes s, we propose MI loss.",no
,249-ARR,249-ARR_v2_27@1,249-ARR_v1_44@2,"Based on minimizing the mutual information between hidden states f (u) of chatbots and private persona attributes s, we propose MI loss in Section 4.2.","Instead, we use p Ψ (s|f (u)) to approximate q(s|f (u)) via minimizing their KL divergence and then we can obtain the following lower bound (Song et al., 2018):",no
,249-ARR,249-ARR_v2_26@0,249-ARR_v1_7@8,The LM training objective in Equation 1 only considers the utility of chatbots.,"We combine proposed KL divergence loss (KL loss) with mutual information loss (MI loss) (Song et al., 2018) as additional defense objectives to train the GPT-2 and decrease the attacker's persona inference accuracy to 0.53%.",no
,249-ARR,249-ARR_v2_31@1,249-ARR_v1_13@4,"For optimization, we can leave out constant terms and the logarithm (Mireshghallah et al., 2021) to obtain the following loss function:","Though the setup of this work is similar to ours, they merely considered simple cases of data recovery with given rules and suffered great utility degradation to obtain optimal defense performance.",no
,249-ARR,249-ARR_v2_12@7,249-ARR_v1_61@0,"Though the setup of this work is similar to ours, they merely consider simple cases of data recovery with given rules and suffer great utility degradation to obtain optimal defense performance.",Experimental Setting,no
2762,250-ARR,250-ARR_v2_15@4,250-ARR_v1_14@4,"3 In other words, we don't compare with works like TinyBERT (Jiao et al., 2019) or MiniLM (Wang et al., 2020b), which use extra losses like self-attention distribution matching.","Otherwise said, we don't compare with works like TinyBERT (Jiao et al., 2019) or MiniLM (Wang et al., 2020b), which use extra losses like self-attention distribution matching.",yes
2763,250-ARR,250-ARR_v2_22@2,250-ARR_v1_21@2,"Since in Sun et al. (2020a), the mean-pooling representation shows better results, we adopt it to compute the sentence representation of each layer.","Since in (Sun et al., 2020a), the mean-pooling representation shows better results, we adopt it to compute the sentence representation of each layer.",yes
2764,250-ARR,250-ARR_v2_53@8,250-ARR_v1_47@3,"CoDIR results are adopted from their paper, and we followed their experimental protocol by not reporting scores on STS-B. On average, RAIL-KD performs on par with CoDIR (+0.2%) and outperforms it on 5 out of 8 datasets, while being almost two-times faster as shown in the next section.","CoDIR results are adopted from their paper, and we followed their experimental protocol by not reporting scores on STS-B. On average, RAIL-KD performs on par with CoDIR (+0.2%) and outperforms it on 5 out of 8 datasets, while being almost twice faster as shown in the next section.",yes
2765,250-ARR,250-ARR_v2_55@2,250-ARR_v1_49@2,We used this configuration because CoDIR pretrained student models are not available and we can only run CoDIR code out-of-the-box.,We used this configuration because CODIR pretrained student models are not available and we can only run CODIR code out-of-the-box.,yes
2766,250-ARR,250-ARR_v2_68@1,250-ARR_v1_60@4,"Moreover, we observe that ALP-KD results have less similarity scores in the upper intermediate layers.","Moreover, we observe that ALP-KD results in less similarity scores in the upper intermediate layers.",yes
2767,250-ARR,250-ARR_v2_12@0,250-ARR_v1_11@0,"In recent years, a wide range of methods have tried to expand knowledge transfer of transformerbased (Vaswani et al., 2017) NLU models beyond logits matching.","Recent years, have seen a wide range of methods have witnessed to expand knowledge transfer of transformer-based (Vaswani et al., 2017) NLU models beyond logits matching.",yes
2768,250-ARR,250-ARR_v2_12@2,250-ARR_v1_11@2,"TinyBERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020b), and MiniLM (Wang et al., 2020b) matched the intermediate layers representations and self-attention distributions of the teacher and the student.","Tiny-BERT (Jiao et al., 2019), MobileBERT (Sun et al., 2020b), and MiniLM (Wang et al., 2020b) matched the intermediate layers representations and selfattention distributions of the teacher and the student.",yes
2769,250-ARR,250-ARR_v2_13@13,250-ARR_v1_12@13,"Alternatively, CoDIR (Sun et al., 2020a) exploited contrastive learning (Tian et al., 2019) to perform intermediate layers matching between the teacher and the student models with no deterministic mapping.","Alternatively, CODIR (Sun et al., 2020a) exploited contrastive learning (Tian et al., 2019) to perform intermediate layers matching between the teacher and the student models with no deterministic mapping.",yes
2770,250-ARR,250-ARR_v2_14@0,250-ARR_v1_13@0,"Table 1 summarizes the main characteristics of the existing state-of-the-art intermediate layer distillation techniques (PKD, CKD, and CoDIR) used for pre-trained language models compared with our proposed RAIL-KD.","Table 1 summarizes the main characteristics of the existing state-of-the-art intermediate layer distillation techniques (PKD (Sun et al., 2019), CKD (Wu et al., 2020a), and CoDIR) used for pre-trained language models compared with our proposed RAIL-KD.",yes
2771,250-ARR,250-ARR_v2_15@1,250-ARR_v1_14@1,"For instance, RAIL-KD is roughly two-times faster than CoDIR in a 24 to 6 layer compression.","For instance, RAIL-KD is roughly twice faster than CoDIR in a 24 to 6 layer compression.",yes
,250-ARR,250-ARR_v2_48@5,250-ARR_v1_43@3,"We ran all the experiments on a single NVIDIA V100 GPU using mixed-precision training (Micikevicius et al., 2018) and PyTorch (Paszke et al., 2019) framework.",Hyperparameters selection and other implementation details can be found in Appendix.,no
,250-ARR,250-ARR_v2_61@2,250-ARR_v1_43@3,"This indicates that the gains of RAIL-KD are significant, and are not due to chance in our random selection of layers to distill.",Hyperparameters selection and other implementation details can be found in Appendix.,no
,250-ARR,250-ARR_v2_46@4,250-ARR_v1_43@3,"We re-implement PKD (Sun et al., 2019) and ALP-KD approaches using the default settings proposed in the respective papers.",Hyperparameters selection and other implementation details can be found in Appendix.,no
,250-ARR,250-ARR_v2_69@2,250-ARR_v1_43@3,The figure clearly shows (light colors) that most of ALP weights are concentrated on top layers of the teacher.,Hyperparameters selection and other implementation details can be found in Appendix.,no
,250-ARR,250-ARR_v2_47@1,250-ARR_v1_43@3,"Since, the hidden dimensions of the RoBERTa 24 and DistilRoBERTa 6 are different, we linearly transform them into same lowerdimensional space.",Hyperparameters selection and other implementation details can be found in Appendix.,no
,250-ARR,250-ARR_v2_48@3,250-ARR_v1_43@3,"We search learning rate from {1e-5, 2e-5, 5e-5, 4e-6}, batch size from {8, 16, 32}, and fixed the epoch number to 40 for all the experiments.",Hyperparameters selection and other implementation details can be found in Appendix.,no
,250-ARR,250-ARR_v2_46@6,250-ARR_v1_43@3,"More precisely, the best layer setting for PKD teacher BERT 12 is {2, 4, 6, 8, 10} to distill into DistilBERT 6 .",Hyperparameters selection and other implementation details can be found in Appendix.,no
,250-ARR,250-ARR_v2_46@5,250-ARR_v1_43@3,"We used early stopping based on performance on the development set, while making sure that the figures are in line with the ones reported in the papers.",Hyperparameters selection and other implementation details can be found in Appendix.,no
,250-ARR,250-ARR_v2_69@3,250-ARR_v1_43@3,"For instance, layers 1,2,5 of the three students mostly attend to the last layer of BERT 12 .",Hyperparameters selection and other implementation details can be found in Appendix.,no
,250-ARR,250-ARR_v2_47@0,250-ARR_v1_43@3,"Using ALP-KD, we compute attention weights for the intermediate layers of the teacher (i.e., 1 to 11 for BERT 12 and 1 to 23 for RoBERTa 24 models) to calculate the weighted intermediate representations of the teacher for each intermediate layer of the student model (i.e., 1 to 5 layers of the student models).",Hyperparameters selection and other implementation details can be found in Appendix.,no
2848,258-ARR,258-ARR_v2_4@2,258-ARR_v1_4@1,"However, designing high-quality cloze tests for language learning is a laborious process that involves finding an optimal distribution of gaps based on aspects such as function, distance, number of answers, etc. (ALTE, 2005;2011).","Designing high-quality cloze tests for language learning is a laborious process that involves finding an optimal distribution of gaps based on aspects such as function, distance, number of answers, etc. (ALTE, 2005;2011).",yes
2849,258-ARR,258-ARR_v2_28@5,258-ARR_v1_25@5,A combination of gaps that yields lower KL divergence is assumed to be a better solution.,A combination of gaps that yields lower KL divergence is assumed to be a better selection.,yes
2850,258-ARR,258-ARR_v2_32@0,258-ARR_v1_28@2,"For this reason, we use a collection of expertly created open cloze tests at the B2 CEFR level that was kindly provided by Cambridge University Press & Assessment (CUP&A) for research purposes.","For this reason, we use a collection of expertly created open cloze tests at the B2 CEFR level that was kindly provided by <anonymised> for research purposes.",yes
2851,258-ARR,258-ARR_v2_34@4,258-ARR_v1_33@3,"6 On top of the encoding layers, we have two branches that are being learned simultaneously (Figure 2).","6 On top of the encoding layers, we have two branches that are being learned simultaneously.",yes
2852,258-ARR,258-ARR_v2_34@6,258-ARR_v1_34@1,"For the second branch, we add ELECTRA's generation layer plus a linear layer which aims to predict the best word from the whole vocabulary as an auxiliary task.","For the second branch, we add ELECTRA's generation layer plus a linear layer which aims to predict the best word from the whole vocabulary.",yes
2853,258-ARR,258-ARR_v2_5@2,258-ARR_v1_5@2,"Our main objective is standard token classification, where we aim to minimise the error of classifying a token as a gap or not.","One objective is standard token classification, where we aim to minimise the error of classifying a token as a gap or not.",yes
2854,258-ARR,258-ARR_v2_36@0,258-ARR_v1_36@0,"We compare our multi-objective ELECTRA model to other systems, namely:","We compare our ELECTRA-based model to other systems, namely:",yes
2855,258-ARR,258-ARR_v2_39@0,258-ARR_v1_40@0,We use the pre-trained base model with standard parameters and fine-tune the weights of the whole architecture.,We use the base pre-trained model with standard parameters and fine-tune the weights of the whole architecture.,yes
2856,258-ARR,258-ARR_v2_40@2,258-ARR_v1_40@1,"Both random and Exercise Maker attempt to generate the same number of gaps per task as defined in the gold standard, although this is not always possible since the required conditions (such as specific words or PoS) are not always met.","Both random and Exercise Maker attempt to generate the same number of gaps per task as defined in the gold standard, although this is not always possible since the required conditions are not always met.",yes
2857,258-ARR,258-ARR_v2_5@3,258-ARR_v1_5@3,The second and auxiliary objective is a language-model-based objective whereby we attempt to minimise the language model error when predicting the right answer for each gap.,The other objective is a language-modelbased objective whereby we attempt to minimise the language model error when predicting the right answer for each gap.,yes
2858,258-ARR,258-ARR_v2_44@0,258-ARR_v1_43@0,We also report human evaluation by three test experts from CUP&A who volunteered for the task.,"We also performed human evaluation, which was carried out by three test experts from <anonymised> who volunteered for the task.",yes
2859,258-ARR,258-ARR_v2_47@1,258-ARR_v1_46@1,"Table 3 reports the results of our multi-objective ELECTRA model (enhanced with dependency information) as well as the random baseline, Exercise Maker, BERT, and the standard single-task ELECTRA.","Table 3 reports the results of our multi-objective ELECTRA model (enhanced with dependency information) as well as the random baseline, Exercise Maker and BERT.",yes
2860,258-ARR,258-ARR_v2_2@0,258-ARR_v1_2@0,This paper presents the first multi-objective transformer model for constructing open cloze tests that exploits generation and discrimination capabilities to improve performance.,This paper presents the first multi-objective transformer model for generating open cloze tests that exploits generation and discrimination capabilities to improve performance.,yes
2861,258-ARR,258-ARR_v2_5@4,258-ARR_v1_5@4,"Our solution is based on a pre-trained ELECTRA (Clark et al., 2020) model that is finetuned on the two described objectives in a multitask scenario.","Our solution is based on a pre-trained ELECTRA (Clark et al., 2020) model that is fine-tuned on the two described objectives in a multi-task scenario.",yes
2862,258-ARR,258-ARR_v2_48@4,258-ARR_v1_46@8,"Although the improvement of our multi-objective ELECTRA model over BERT does not seem to be very significant based simply on P, R and F 1 , a closer look at the results reveals that BERT produces a much higher number of repeated gaps (25 compared to 9 by multi-objective ELECTRA) as well as more cases of gaps in close proximity, as shown in Figure 3.","Although the improvement of our multi-objective ELECTRA model over BERT does not seem to be very significant based simply on P, R and F 1 , a closer look at the results reveals that BERT produces a much higher number of repeated gaps (25 compared to 9 by multi-objective ELECTRA) as well as more cases of gaps in close proximity as shown in Figure 3.",yes
2863,258-ARR,258-ARR_v2_52@1,258-ARR_v1_50@1,"It also creates a better spread of gaps, as shown by a lower KL-divergence between the average PoS distribution of the output and that of the gold standard (0.55 with post-processing as opposed to 0.59 without it).","It also creates a better spread of gaps, as shown by a lower KL-divergence between the average POS distribution of the output and that of the gold standard (0.55 with post-processing as opposed to 0.59 without it).",yes
2864,258-ARR,258-ARR_v2_53@1,258-ARR_v1_51@1,"Table 5 shows that they significantly improve R, which results in higher overall F 1 .","Table 5 shows that significantly improve R, which results in higher overall F 1 .",yes
2865,258-ARR,258-ARR_v2_69@2,258-ARR_v1_66@2,"The worst performing classes are PRON (77%), NUM (77%) and VERB (75%) as opposed to the previous AUX and PART counterparts (now 79% and 83% respectively).","The worst performing classes are PRON (77%), NUM (77%) and VERB (77%) as opposed to the previous AUX and PART counterparts (now 79% and 83% respectively).",yes
2866,258-ARR,258-ARR_v2_17@8,258-ARR_v1_14@6,The first objective is typical of any standard token classification model and constitutes our key task.,The first objective is typical of any standard token classification model.,yes
2867,258-ARR,258-ARR_v2_18@0,258-ARR_v1_15@0,The second and auxiliary objective attempts to model our preference for gaps with a restricted number of answers while also ensuring that the original word can be guessed from the context.,The second objective attempts to model our preference for gaps with a restricted number of answers while also ensuring that the original word can be guessed from the context.,yes
2868,258-ARR,258-ARR_v2_4@0,258-ARR_v1_4@0,"Open cloze (Taylor, 1953) tests are a common type of exercise where words are removed from a piece of text and must then be filled in by the students without any options to choose from.",Open cloze tests are a common type of learning exercise where a student must complete a sentence by filling in a gap without any options to choose from.,yes
,258-ARR,258-ARR_v2_4@1,258-ARR_v1_31@0,"They are often used in language learning environments as a quick and effective way to test vocabulary, grammar and reading comprehension (Tremblay, 2011;Trace, 2020).",Experiments,no
,258-ARR,258-ARR_v2_4@1,258-ARR_v1_32@0,"They are often used in language learning environments as a quick and effective way to test vocabulary, grammar and reading comprehension (Tremblay, 2011;Trace, 2020).",Setup,no
,258-ARR,258-ARR_v2_4@1,258-ARR_v1_33@0,"They are often used in language learning environments as a quick and effective way to test vocabulary, grammar and reading comprehension (Tremblay, 2011;Trace, 2020).",We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers.,no
,258-ARR,258-ARR_v2_17@4,258-ARR_v1_31@0,We believe that this discrimination objective makes it more suitable for our token classification task.,Experiments,no
,258-ARR,258-ARR_v2_17@4,258-ARR_v1_32@0,We believe that this discrimination objective makes it more suitable for our token classification task.,Setup,no
,258-ARR,258-ARR_v2_17@4,258-ARR_v1_33@0,We believe that this discrimination objective makes it more suitable for our token classification task.,We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers.,no
,258-ARR,258-ARR_v2_17@5,258-ARR_v1_31@0,"Moreover, we also exploit ELECTRA's generation capabilities as a language model for estimating the answers to the proposed gaps as an auxiliary task.",Experiments,no
,258-ARR,258-ARR_v2_17@5,258-ARR_v1_32@0,"Moreover, we also exploit ELECTRA's generation capabilities as a language model for estimating the answers to the proposed gaps as an auxiliary task.",Setup,no
,258-ARR,258-ARR_v2_17@5,258-ARR_v1_33@0,"Moreover, we also exploit ELECTRA's generation capabilities as a language model for estimating the answers to the proposed gaps as an auxiliary task.",We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers.,no
,258-ARR,258-ARR_v2_40@0,258-ARR_v1_31@0,"Standard ELECTRA Similar to BERT, it predicts potentially good gaps using a standard pre-trained ELECTRA-base model.",Experiments,no
,258-ARR,258-ARR_v2_40@0,258-ARR_v1_32@0,"Standard ELECTRA Similar to BERT, it predicts potentially good gaps using a standard pre-trained ELECTRA-base model.",Setup,no
,258-ARR,258-ARR_v2_40@0,258-ARR_v1_33@0,"Standard ELECTRA Similar to BERT, it predicts potentially good gaps using a standard pre-trained ELECTRA-base model.",We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers.,no
,258-ARR,258-ARR_v2_40@1,258-ARR_v1_31@0,This is a single-objective model that is fine-tuned on token classification only.,Experiments,no
,258-ARR,258-ARR_v2_40@1,258-ARR_v1_32@0,This is a single-objective model that is fine-tuned on token classification only.,Setup,no
,258-ARR,258-ARR_v2_40@1,258-ARR_v1_33@0,This is a single-objective model that is fine-tuned on token classification only.,We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers.,no
,258-ARR,258-ARR_v2_48@5,258-ARR_v1_31@0,We also perform an ablation study in Table 3 where we compare our multi-objective ELECTRA model to a standard one that does not include our auxiliary language model objective.,Experiments,no
,258-ARR,258-ARR_v2_48@5,258-ARR_v1_32@0,We also perform an ablation study in Table 3 where we compare our multi-objective ELECTRA model to a standard one that does not include our auxiliary language model objective.,Setup,no
,258-ARR,258-ARR_v2_48@5,258-ARR_v1_33@0,We also perform an ablation study in Table 3 where we compare our multi-objective ELECTRA model to a standard one that does not include our auxiliary language model objective.,We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers.,no
,258-ARR,258-ARR_v2_48@6,258-ARR_v1_31@0,"Results show that the former outperforms the latter on all metrics, confirming that the addition of the LM objective is clearly beneficial.",Experiments,no
,258-ARR,258-ARR_v2_48@6,258-ARR_v1_32@0,"Results show that the former outperforms the latter on all metrics, confirming that the addition of the LM objective is clearly beneficial.",Setup,no
,258-ARR,258-ARR_v2_48@6,258-ARR_v1_33@0,"Results show that the former outperforms the latter on all metrics, confirming that the addition of the LM objective is clearly beneficial.",We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers.,no
2879,276-ARR,276-ARR_v2_43@3,276-ARR_v1_78@3,"However, F −1 as used in ( 9) is increasing only on interval [0, 0.5] (Fig. 1).","However, F −1 as used in ( 9) is increasing only on interval [0, 0.5].",yes
2880,276-ARR,276-ARR_v2_43@4,276-ARR_v1_78@4,"For v ≥ 0.5, we get negative argument to ln which yields a complex function, whose real part is even decreasing.","For v ≥ 0.5, we get negative argument to ln which yields a complex function, whose real parts is even decreasing.",yes
2881,276-ARR,276-ARR_v2_46@1,276-ARR_v1_80@1,"We rely on the standard proof of the Laplace mechanism as shown, e.g, by Habernal (2021).","We rely on the standard proof of the Laplace mechanism as shown, e.g, by (Dwork and Roth, 2013;Habernal, 2021).",yes
2882,276-ARR,276-ARR_v2_0@0,276-ARR_v1_0@0,How reparametrization trick broke differentially-private text representation learning,How reparametrization trick broke differentially-private text representation leaning,yes
2883,276-ARR,276-ARR_v2_33@0,276-ARR_v1_35@0,"While both (4) and ( 7) generate samples from Lap(µ; b), note the substantial difference between u and v, since each is drawn from a different uniform distribution (see visualizations in Fig. 1).","While both (4) and ( 7) generate samples from Lap(µ; b), note the substantial difference between u and v, since each is drawn from a different uniform distribution.",yes
,276-ARR,276-ARR_v2_64@0,276-ARR_v1_55@3,Ethics Statement,"7 For example, we repeated the full experiment on ADePT (n = 2, ε = 0.1) 100 times which results in standard deviation 0.0008 from the mean value 0.195.",no
,276-ARR,276-ARR_v2_65@0,276-ARR_v1_55@5,"We declare no conflict of interests with the authors of DPText, we do not even know them personally.",All source codes are attached/will go on Github.,no
,276-ARR,276-ARR_v2_44@5,276-ARR_v1_45@0,See Fig. 3 in the Appendix for various Laplace-based distributions sampled with different techniques including possible distributions sampled in DPText.,See Appendix B for proof.,no
,276-ARR,276-ARR_v2_65@1,276-ARR_v1_45@0,The purpose of this paper is strictly scientific.,See Appendix B for proof.,no
,276-ARR,276-ARR_v2_64@0,276-ARR_v1_84@0,Ethics Statement,See Figure 2.,no
3097,30-ARR,30-ARR_v2_21@2,30-ARR_v1_24@2,"Only adjectives, adverbs, nouns, and verbs were considered as rationales.","Only adjectives, adverbs, nouns, and verbs were considered.",yes
3098,30-ARR,30-ARR_v2_21@5,30-ARR_v1_24@5,"We also limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams).","We limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams).",yes
3099,30-ARR,30-ARR_v2_4@0,30-ARR_v1_4@0,"Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020;Srivastava et al., 2020) in datasets can cause sub-optimal model performance for neural networks.","Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020;Srivastava et al., 2020) in datasets cause sub-optimal model performance for neural networks.",yes
3100,30-ARR,30-ARR_v2_22@4,30-ARR_v1_25@4,All re-annotated examples were approved only if all authors were happy with the quality of the annotations.,All re-annotated examples were approved only if all authors were happy with the quality of annotations.,yes
3101,30-ARR,30-ARR_v2_23@0,30-ARR_v1_26@0,"Our annotation procedure generated 5,073 rationales in 855 movie reviews involved in Section 3.1 and 3.3 (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).","Our annotation procedure generated 5,073 rationales in 855 movie reviews (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).",yes
3102,30-ARR,30-ARR_v2_23@3,30-ARR_v1_26@3,"Note that our approach using 100 labelled examples can outperform manual CAD (Kaushik et al., 2020) using the entire training set of 1,707 examples (see Section 5.3), making our approach 300×1707 183.68×100 ≈ 27.88 times more efficient than manually generated CAD.","Note that our approach using 100 labelled examples can outperform the manual CAD (Kaushik et al., 2020) using the entire training set with 1,707 examples (see Section 5.3), making our approach 300×1707 183.68×100 ≈ 27.88 times more efficient than manually generated CAD.",yes
3103,30-ARR,30-ARR_v2_4@1,30-ARR_v1_4@1,"As shown in Figure 1, the bold phrases-""100% bad"" and ""brain cell killing""-are underlying causes for a negative sentiment prediction that most human readers would recognise.","As shown in Figure 1, the phrases in bold-""100% bad"" and ""brain cell killing""-are underlying causes for a negative prediction most human readers would recognise.",yes
3104,30-ARR,30-ARR_v2_26@1,30-ARR_v1_29@1,"To generate a semi-factual example, x ′ i , we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms.","To generate a semi-factual example, x i ', we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms.",yes
3105,30-ARR,30-ARR_v2_27@0,30-ARR_v1_30@0,"In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x ′ i , with some replaced non-rationales and all the other tokens identical to x i .","In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x i ', with some replaced non-rationales and all the other tokens identical to x i .",yes
3106,30-ARR,30-ARR_v2_2@0,30-ARR_v1_2@0,We present a novel rationale-centric framework with human-in-the-loop -Rationales-centric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios.,We present a novel rational-centric framework with human-in-the-loop -Rationales-centric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios.,yes
3107,30-ARR,30-ARR_v2_4@3,30-ARR_v1_4@3,"The underlined phrase-""acting and plot""has been incorrectly recognised as a causal term by the model used fort this example, and is referred to as a spurious pattern.","The underlined phrase-""acting and plot""-are incorrectly recognised as causal terms by the model, and are referred to as spurious patterns.",yes
3108,30-ARR,30-ARR_v2_5@0,30-ARR_v1_5@0,"Spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021), and are usually useless, or even harmful, at test time.","These spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021) and are usually useless, or even harmful, at test time.",yes
3109,30-ARR,30-ARR_v2_32@0,30-ARR_v1_35@0,Note that the two correction methods in dynamic human-intervened correction can operate in parallel and the generated examples are added to the small training set to re-train the model.,Note that the two correction methods in dynamic human-intervened correction can be operated in parallel and the generated examples are added to the small training set to re-train the model.,yes
3110,30-ARR,30-ARR_v2_37@0,30-ARR_v1_37@0,"Broadly speaking, our RDL framework takes advantage of invariance that makes a model less sensitive to non-rationale words or spurious patterns (Tu et al., 2020; in favour of focusing on useful mappings of rationales to labels.","Broadly speaking, our RDL framework acts like a sensible inductive bias (Wu et al., 2021b;Warstadt and Bowman, 2020) that restricts a model from superficially focusing on whole texts or spurious patterns (Tu et al., 2020; in favour of focusing on useful mappings of rationales to labels.",yes
3111,30-ARR,30-ARR_v2_49@1,30-ARR_v1_53@1,"Following Kaushik et al. (2020), we fine-tune RoBERTa for up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).","Following Kaushik et al. (2020), we fine-tune RoBERTa up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).",yes
3112,30-ARR,30-ARR_v2_49@4,30-ARR_v1_54@2,"We found that setting the learning rate to {5e-5, 5e-6 and 5e-6} could optimise Static, Static+n, and Full, respectively.","We found that setting the learning rate to 5e-5, 5e-6 and 5e-6 could optimise Static, Static+n, and Full, respectively.",yes
3113,30-ARR,30-ARR_v2_51@1,30-ARR_v1_56@1,"Among all Static+n methods, Static+350 seems the best-performing method and exceeds Static with a 1.56% in-distribution improvement in average accuracy.","Among all Static+n methods, Static+350 seems the best-performing method that exceeds Static with a 1.56% in-distribution improvement in average accuracy.",yes
3114,30-ARR,30-ARR_v2_51@2,30-ARR_v1_56@2,"Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, and 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets respectively.","Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets, respectively.",yes
3115,30-ARR,30-ARR_v2_52@0,30-ARR_v1_57@0,"The Static+n methods can even outperform Full (i.e. normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset.","The Static+n methods can even outperform Full (i,e, the normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset.",yes
3116,30-ARR,30-ARR_v2_52@4,30-ARR_v1_57@4,"It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp dataset (93.66±0.84) than on the in-distribution dataset (93.23±0.46), which could also be explained by the high similarity between the underlying distributions of these two datasets.","It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp (93.66±0.84) than on the in-distribution dataset (93.23±0.46), which could also be explained by the high similarity between the underlying distributions of these two test sets.",yes
3117,30-ARR,30-ARR_v2_52@6,30-ARR_v1_57@6,"We believe that the duplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.","We believe that the multiplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.",yes
3118,30-ARR,30-ARR_v2_5@3,30-ARR_v1_7@1,"In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu and MacNamee, 2020;Lu et al., 2021).","In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu et al., 2021).",yes
3119,30-ARR,30-ARR_v2_53@0,30-ARR_v1_58@0,"As shown in Table 3, RR is slightly better than the baseline Static approach.","As shown in Table 3, RR is slightly better than the baseline Static.",yes
3120,30-ARR,30-ARR_v2_53@1,30-ARR_v1_58@1,"This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to the original data, introducing noise that helps prevent overfitting to some extent.","This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to original data, introducing noise that helps prevent overfitting to some extent.",yes
3121,30-ARR,30-ARR_v2_53@2,30-ARR_v1_58@2,"However, the magnitude of improvement of the Static+n method is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.","However, the magnitude of improvement of Static+n methods is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.",yes
3122,30-ARR,30-ARR_v2_59@1,30-ARR_v1_63@1,"To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter).","To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter), respectively.",yes
3123,30-ARR,30-ARR_v2_61@3,30-ARR_v1_65@3,"Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, but no improvement for the SemEval dataset.","Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, except for the SemEval dataset.",yes
3124,30-ARR,30-ARR_v2_61@4,30-ARR_v1_65@4,"Finally, the performance of our methods is better that the state-of-the-art manual CAD method in few-shot learning scenarios on all OOD datasets.","Finally, the performance of our methods outperforms another state-of-the-art manual CAD method in fewshot learning scenarios on all OOD datasets.",yes
3125,30-ARR,30-ARR_v2_63@2,30-ARR_v1_68@2,"As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increase in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.","As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increasing in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.",yes
3126,30-ARR,30-ARR_v2_2@1,30-ARR_v1_2@1,"By using static semi-factual generation and dynamic humanintervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.","By using static semi-factual generation and dynamic human-intervened correction, RDL, acting like a sensible ""inductive bias"", exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.",yes
3127,30-ARR,30-ARR_v2_12@4,30-ARR_v1_13@4,"In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002;Kenny and Keane, 2021), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same).","In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same).",yes
3128,30-ARR,30-ARR_v2_12@6,30-ARR_v1_13@6,"We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power, both for in-distribution and OOD predictions.","We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power both for in-distribution and OOD predictions.",yes
3129,30-ARR,30-ARR_v2_2@2,30-ARR_v1_2@2,Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarks-especially for few-shot learning scenarios.,"Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests, especially for few-shot learning scenarios, compared to many state-of-the-art benchmarks.",yes
3130,30-ARR,30-ARR_v2_13@7,30-ARR_v1_17@7,"Human-the-loop Machine Learning (Wu et al., 2021) has received increasing research attention.","Human-the-loop Machine Learning (Wu et al., 2021a) has received increasing research attention.",yes
3131,30-ARR,30-ARR_v2_14@1,30-ARR_v1_17@12,"Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights.","Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of some model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights.",yes
3132,30-ARR,30-ARR_v2_14@2,30-ARR_v1_17@13,"However, previous work focuses on increasing the explainability of AI systems for high-stakes domains such as health and finance (Li et al., 2020;Yang et al., 2020b), instead of improving model robustness or generalisation ability.","However, previous work focuses on increasing the explainability of AI systems for high-stake domains such as health and finance, instead of improving the model robustness or generalisation ability.",yes
,30-ARR,30-ARR_v2_18@2,30-ARR_v1_14@1,"Then, all model-identified rationales (underlined texts in Figure 2) are examined by human annotators to identify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model).","In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.",no
,30-ARR,30-ARR_v2_18@2,30-ARR_v1_17@0,"Then, all model-identified rationales (underlined texts in Figure 2) are examined by human annotators to identify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model).","Data augmentation has been used for resolving artefacts in training datasets before (Gururangan et al., 2018;Srivastava et al., 2020;Kaushik et al., 2021).",no
,30-ARR,30-ARR_v2_66@0,30-ARR_v1_14@1,Ethical Statement,"In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.",no
,30-ARR,30-ARR_v2_67@0,30-ARR_v1_15@1,We honor the ACL Code of Ethics.,All resources will be released on Github.,no
,30-ARR,30-ARR_v2_18@4,30-ARR_v1_14@2,"Finally, newly generated examples are added into the training set to re-train the deep learning model.","The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD using the full augmented training set of 3,414 examples.",no
,30-ARR,30-ARR_v2_67@2,30-ARR_v1_14@1,All annotators have received labor fees corresponding to the amount of their annotated instances.,"In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.",no
,30-ARR,30-ARR_v2_65@2,30-ARR_v1_15@1,"In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.",All resources will be released on Github.,no
,30-ARR,30-ARR_v2_67@1,30-ARR_v1_14@1,No private data or non-public information was used in this work.,"In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.",no
,30-ARR,30-ARR_v2_18@4,30-ARR_v1_14@1,"Finally, newly generated examples are added into the training set to re-train the deep learning model.","In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.",no
,30-ARR,30-ARR_v2_67@2,30-ARR_v1_15@1,All annotators have received labor fees corresponding to the amount of their annotated instances.,All resources will be released on Github.,no
,30-ARR,30-ARR_v2_67@0,30-ARR_v1_16@0,We honor the ACL Code of Ethics.,Related Work,no
,30-ARR,30-ARR_v2_67@1,30-ARR_v1_15@0,No private data or non-public information was used in this work.,"To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and humanintervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.",no
,30-ARR,30-ARR_v2_65@2,30-ARR_v1_14@2,"In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.","The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD using the full augmented training set of 3,414 examples.",no
,30-ARR,30-ARR_v2_18@2,30-ARR_v1_15@1,"Then, all model-identified rationales (underlined texts in Figure 2) are examined by human annotators to identify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model).",All resources will be released on Github.,no
,30-ARR,30-ARR_v2_67@1,30-ARR_v1_16@0,No private data or non-public information was used in this work.,Related Work,no
,30-ARR,30-ARR_v2_18@4,30-ARR_v1_17@0,"Finally, newly generated examples are added into the training set to re-train the deep learning model.","Data augmentation has been used for resolving artefacts in training datasets before (Gururangan et al., 2018;Srivastava et al., 2020;Kaushik et al., 2021).",no
,30-ARR,30-ARR_v2_18@3,30-ARR_v1_15@0,Both false rationales and missing rationales are corrected to produce augmented examples.,"To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and humanintervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.",no
,30-ARR,30-ARR_v2_18@3,30-ARR_v1_14@1,Both false rationales and missing rationales are corrected to produce augmented examples.,"In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.",no
,30-ARR,30-ARR_v2_66@0,30-ARR_v1_16@0,Ethical Statement,Related Work,no
,30-ARR,30-ARR_v2_18@2,30-ARR_v1_16@0,"Then, all model-identified rationales (underlined texts in Figure 2) are examined by human annotators to identify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model).",Related Work,no
,30-ARR,30-ARR_v2_67@0,30-ARR_v1_14@2,We honor the ACL Code of Ethics.,"The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD using the full augmented training set of 3,414 examples.",no
,30-ARR,30-ARR_v2_46@0,30-ARR_v1_14@2,"To address RQ1, we compare the performance of models trained by the static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static.","The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD using the full augmented training set of 3,414 examples.",no
,30-ARR,30-ARR_v2_65@2,30-ARR_v1_16@0,"In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.",Related Work,no
,30-ARR,30-ARR_v2_67@2,30-ARR_v1_16@0,All annotators have received labor fees corresponding to the amount of their annotated instances.,Related Work,no
,30-ARR,30-ARR_v2_46@1,30-ARR_v1_14@2,"We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full.","The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD using the full augmented training set of 3,414 examples.",no
,30-ARR,30-ARR_v2_67@2,30-ARR_v1_15@0,All annotators have received labor fees corresponding to the amount of their annotated instances.,"To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and humanintervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.",no
,30-ARR,30-ARR_v2_18@4,30-ARR_v1_15@1,"Finally, newly generated examples are added into the training set to re-train the deep learning model.",All resources will be released on Github.,no
,30-ARR,30-ARR_v2_46@1,30-ARR_v1_15@1,"We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full.",All resources will be released on Github.,no
,30-ARR,30-ARR_v2_18@3,30-ARR_v1_15@1,Both false rationales and missing rationales are corrected to produce augmented examples.,All resources will be released on Github.,no
,30-ARR,30-ARR_v2_65@2,30-ARR_v1_14@1,"In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.","In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.",no
,30-ARR,30-ARR_v2_46@1,30-ARR_v1_15@0,"We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full.","To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and humanintervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.",no
,30-ARR,30-ARR_v2_18@4,30-ARR_v1_16@0,"Finally, newly generated examples are added into the training set to re-train the deep learning model.",Related Work,no
,30-ARR,30-ARR_v2_46@1,30-ARR_v1_14@1,"We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full.","In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.",no
,30-ARR,30-ARR_v2_67@2,30-ARR_v1_14@2,All annotators have received labor fees corresponding to the amount of their annotated instances.,"The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD using the full augmented training set of 3,414 examples.",no
,30-ARR,30-ARR_v2_18@3,30-ARR_v1_17@0,Both false rationales and missing rationales are corrected to produce augmented examples.,"Data augmentation has been used for resolving artefacts in training datasets before (Gururangan et al., 2018;Srivastava et al., 2020;Kaushik et al., 2021).",no
,30-ARR,30-ARR_v2_65@2,30-ARR_v1_15@0,"In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.","To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and humanintervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.",no
3299,315-ARR,315-ARR_v2_31@2,315-ARR_v1_32@7,"However, there are two noticeable performance drops around relative frequency ranges of 10%-30% (bucket 2) and 90%-100% (bucket 6), denoted as Drop#1 (-0.3 BLEU) and Drop#2 (-0.6 BLEU).","However, there are two noticeable performance drops around relative frequency ranges of 10%-30% and 90%-100%, denoted as Drop#1 (-0.3 BLEU) and Drop#2 (-0.6 BLEU).",yes
3300,315-ARR,315-ARR_v2_38@1,315-ARR_v1_39@1,We first build pseudo terms from the target by sampling 0-3 words (more tokens after tokenization) from reference as the pre-defined constraints for training.,We first build pseudo terms from the target by sampling 0-3 words from reference as the pre-defined constraints for training.,yes
3301,315-ARR,315-ARR_v2_40@4,315-ARR_v1_42@2,"As illustrated in Figure 2, we propose to directly align the target-side constraints with the source words and prompt the alignment information to the model during both training and inference.",We propose to directly align the target-side constraints with the source words and prompt the alignment information to the model during both training and inference.,yes
3302,315-ARR,315-ARR_v2_54@2,315-ARR_v1_59@2,"When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do.","When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference.",yes
3303,315-ARR,315-ARR_v2_21@0,315-ARR_v1_22@0,Iterative refinement by editing is an NAT paradigm that suits constrained translations due to its flexibility.,"For NATs, iterative refinement by editing is an NAT paradigm that suits constrained translations due to its flexibility.",yes
,315-ARR,315-ARR_v2_76@1,315-ARR_v1_32@9,"Clearly, Drop#1 disappears in the given setting.",Such words are not as universal as ones at the leftmost that can fit in most contexts and do not have to appear in the target due to multiple modes in translation.,no
,315-ARR,315-ARR_v2_75@2,315-ARR_v1_32@8,"The third finding is because, as the BPE training was only done on the training set of the datasets, there will be <UNK> on the target side of the test set.",Drop#1 is probably because the constraint words within this range are mostly functional or less important.,no
,315-ARR,315-ARR_v2_75@2,315-ARR_v1_32@9,"The third finding is because, as the BPE training was only done on the training set of the datasets, there will be <UNK> on the target side of the test set.",Such words are not as universal as ones at the leftmost that can fit in most contexts and do not have to appear in the target due to multiple modes in translation.,no
,315-ARR,315-ARR_v2_76@1,315-ARR_v1_32@8,"Clearly, Drop#1 disappears in the given setting.",Drop#1 is probably because the constraint words within this range are mostly functional or less important.,no
,315-ARR,315-ARR_v2_76@0,315-ARR_v1_73@2,"To give a clearer view about how is UNK causing Drop#1, we exclude samples with UNK as constraints, and obtain a revised self-constrained translation results, as in Figure 5.","The purpose of this work is to enhance existing methods for constrained NAT, i.e., editing-based iterative NAT methods, under rare lexical constraints.",no
3352,32-ARR,32-ARR_v2_42@2,32-ARR_v1_25@2,The algorithm is implemented in PyTorch-1.10 and experiments are conducted on Nvidia RTX-6000 and RTX-A6000 GPU.,The algorithm is implemented in PyTorch-1.10 and experiments are conducted on Nvidia RTX-A6000 GPU with 48GB of memory.,yes
3353,32-ARR,32-ARR_v2_4@2,32-ARR_v1_4@2,"Data augmentation (Wei and Zou, 2019), regularization and re-initialization further improve the results.","Data augmentation (Wei and Zou, 2019), regularization and re-initialization (Zhang et al., 2021) further improve the results.",yes
3354,32-ARR,32-ARR_v2_7@0,32-ARR_v1_6@0,Prior work have proposed regularization methods to overcome this problem .,"Prior work have proposed regularization methods to overcome this problem Zhang et al., 2021).",yes
3355,32-ARR,32-ARR_v2_0@0,32-ARR_v1_0@0,Embedding Hallucination for Few-Shot Language Fine-tuning,Embedding Hallucination for Few-shot Language Fine-tuning,yes
3356,32-ARR,32-ARR_v2_8@0,32-ARR_v1_7@0,"Current common text data augmentation methods, such as EDA (Wei and Zou, 2019) (which have been used in recent few-shot learning papers (Wei et al., 2021;Basu et al., 2021)) and AEDA (Karimi et al., 2021) operate at the lexical level, which while resulting in human readable texts, lead to limited diversity due to the discrete nature of the lexical space.","Current common text data augmentation methods, such as EDA (Wei and Zou, 2019) (which have been used in recent few-shot learning papers (Wei et al., 2021;Basu et al., 2021)) and AEDA (Karimi et al., 2021a) operate at the lexical level, which while resulting in human readable texts, lead to limited diversity due to the discrete nature of the lexical space.",yes
3357,32-ARR,32-ARR_v2_8@8,32-ARR_v1_7@8,"We further experimentally show the overall superiority of EmbedHalluc when comparing to regularization methods proposed to address the problem of over-fitting during fine-tuning of LMs, such as Mixout and Re-Init .","We further experimentally show the overall superiority of EmbedHalluc when comparing to regularization methods proposed to address the problem of over-fitting during fine-tuning of LMs, such as Mixout and Re-Init (Zhang et al., 2021).",yes
3358,32-ARR,32-ARR_v2_10@2,32-ARR_v1_9@2,Other tricks include bias correction in optimizer and re-initialization of top layers in Transformer .,"Other tricks include bias correction in optimizer and re-initialization of top layers in Transformer (Zhang et al., 2021).",yes
3359,32-ARR,32-ARR_v2_10@8,32-ARR_v1_9@8,"In NLP, few-shot learning has been successfully applied to machine translation (Arthaud et al., 2021), abstract summarizing (Fabbri et al., 2021), question and answering (Hua et al., 2020;Ram et al., 2021), and entity recognition (de Lichy et al., 2021;Tong et al., 2021;Ding et al., 2021), by meta learning (Li and Zhang, 2021;Bansal et al., 2020;Sharaf et al., 2020), data augmentation (Wei et al., 2021;Wei and Zou, 2019;Karimi et al., 2021;, and prompts Tam et al., 2021).","Few-shot learning has been successfully applied to machine translation (Arthaud et al., 2021), abstract summarizing (Fabbri et al., 2021), question and answering (Hua et al., 2020;Ram et al., 2021), and entity recognition (de Lichy et al., 2021;Tong et al., 2021;Ding et al., 2021), by meta learning (Li and Zhang, 2021;Bansal et al., 2020;Sharaf et al., 2020), data augmentation (Wei et al., 2021;Wei and Zou, 2019;Karimi et al., 2021b), and prompts (Gao et al., 2021;Tam et al., 2021).",yes
3360,32-ARR,32-ARR_v2_14@0,32-ARR_v1_13@0,"GAN (Goodfellow et al., 2014) has led the revolution of generative models to achieve impressive results in synthesizing images (Zhu et al., 2017) and higher dimensional data .","GAN (Goodfellow et al., 2014) has led the revolution of generative models to achieve impressive results in synthesizing images (Zhu et al., 2017) and higher dimensional data (Wang et al., 2020).",yes
3361,32-ARR,32-ARR_v2_18@3,32-ARR_v1_17@3,"We propose Label Calibration (LabelCalib) by pseudolabeling from a teacher model F GEN0 (LM 1 in Algorithm 1), where F GEN0 is first fine-tuned on the original training set (without augmentation).","We propose Label Calibration (LabelCalib) by pseudo-labeling from a teacher model F GEN0 , where F GEN0 is first fine-tuned on the original training set (without augmentation).",yes
,32-ARR,32-ARR_v2_8@8,32-ARR_v1_7@0,"We further experimentally show the overall superiority of EmbedHalluc when comparing to regularization methods proposed to address the problem of over-fitting during fine-tuning of LMs, such as Mixout and Re-Init .","Current common text data augmentation methods, such as EDA (Wei and Zou, 2019) (which have been used in recent few-shot learning papers (Wei et al., 2021;Basu et al., 2021)) and AEDA (Karimi et al., 2021a) operate at the lexical level, which while resulting in human readable texts, lead to limited diversity due to the discrete nature of the lexical space.",no
,32-ARR,32-ARR_v2_4@2,32-ARR_v1_13@0,"Data augmentation (Wei and Zou, 2019), regularization and re-initialization further improve the results.","GAN (Goodfellow et al., 2014) has led the revolution of generative models to achieve impressive results in synthesizing images (Zhu et al., 2017) and higher dimensional data (Wang et al., 2020).",no
,32-ARR,32-ARR_v2_4@2,32-ARR_v1_7@0,"Data augmentation (Wei and Zou, 2019), regularization and re-initialization further improve the results.","Current common text data augmentation methods, such as EDA (Wei and Zou, 2019) (which have been used in recent few-shot learning papers (Wei et al., 2021;Basu et al., 2021)) and AEDA (Karimi et al., 2021a) operate at the lexical level, which while resulting in human readable texts, lead to limited diversity due to the discrete nature of the lexical space.",no
,32-ARR,32-ARR_v2_10@2,32-ARR_v1_9@8,Other tricks include bias correction in optimizer and re-initialization of top layers in Transformer .,"Few-shot learning has been successfully applied to machine translation (Arthaud et al., 2021), abstract summarizing (Fabbri et al., 2021), question and answering (Hua et al., 2020;Ram et al., 2021), and entity recognition (de Lichy et al., 2021;Tong et al., 2021;Ding et al., 2021), by meta learning (Li and Zhang, 2021;Bansal et al., 2020;Sharaf et al., 2020), data augmentation (Wei et al., 2021;Wei and Zou, 2019;Karimi et al., 2021b), and prompts (Gao et al., 2021;Tam et al., 2021).",no
,32-ARR,32-ARR_v2_14@0,32-ARR_v1_0@0,"GAN (Goodfellow et al., 2014) has led the revolution of generative models to achieve impressive results in synthesizing images (Zhu et al., 2017) and higher dimensional data .",Embedding Hallucination for Few-shot Language Fine-tuning,no
,32-ARR,32-ARR_v2_42@2,32-ARR_v1_7@0,The algorithm is implemented in PyTorch-1.10 and experiments are conducted on Nvidia RTX-6000 and RTX-A6000 GPU.,"Current common text data augmentation methods, such as EDA (Wei and Zou, 2019) (which have been used in recent few-shot learning papers (Wei et al., 2021;Basu et al., 2021)) and AEDA (Karimi et al., 2021a) operate at the lexical level, which while resulting in human readable texts, lead to limited diversity due to the discrete nature of the lexical space.",no
,32-ARR,32-ARR_v2_8@0,32-ARR_v1_13@0,"Current common text data augmentation methods, such as EDA (Wei and Zou, 2019) (which have been used in recent few-shot learning papers (Wei et al., 2021;Basu et al., 2021)) and AEDA (Karimi et al., 2021) operate at the lexical level, which while resulting in human readable texts, lead to limited diversity due to the discrete nature of the lexical space.","GAN (Goodfellow et al., 2014) has led the revolution of generative models to achieve impressive results in synthesizing images (Zhu et al., 2017) and higher dimensional data (Wang et al., 2020).",no
,32-ARR,32-ARR_v2_8@8,32-ARR_v1_9@8,"We further experimentally show the overall superiority of EmbedHalluc when comparing to regularization methods proposed to address the problem of over-fitting during fine-tuning of LMs, such as Mixout and Re-Init .","Few-shot learning has been successfully applied to machine translation (Arthaud et al., 2021), abstract summarizing (Fabbri et al., 2021), question and answering (Hua et al., 2020;Ram et al., 2021), and entity recognition (de Lichy et al., 2021;Tong et al., 2021;Ding et al., 2021), by meta learning (Li and Zhang, 2021;Bansal et al., 2020;Sharaf et al., 2020), data augmentation (Wei et al., 2021;Wei and Zou, 2019;Karimi et al., 2021b), and prompts (Gao et al., 2021;Tam et al., 2021).",no
,32-ARR,32-ARR_v2_0@0,32-ARR_v1_17@3,Embedding Hallucination for Few-Shot Language Fine-tuning,"We propose Label Calibration (LabelCalib) by pseudo-labeling from a teacher model F GEN0 , where F GEN0 is first fine-tuned on the original training set (without augmentation).",no
,32-ARR,32-ARR_v2_8@8,32-ARR_v1_9@2,"We further experimentally show the overall superiority of EmbedHalluc when comparing to regularization methods proposed to address the problem of over-fitting during fine-tuning of LMs, such as Mixout and Re-Init .","Other tricks include bias correction in optimizer and re-initialization of top layers in Transformer (Zhang et al., 2021).",no
3394,326-ARR,326-ARR_v2_56@1,326-ARR_v1_51@1,"As seen, our model can give good aligned information, where preposition phrase ""around 50 years ago"" is arranged at the end of sentence in English, while its aligned phrase is at front in Chinese.","As seen, our model can give good aligned information, where preposition phrase ""around 50 years ago"" is arranged at the end of sentence in English, while its aligned phrase is at the front in Chinese.",yes
3395,326-ARR,326-ARR_v2_60@4,326-ARR_v1_53@4,"Although we have shown the superiority of E-ATT, considering the whole TRANSFORMER block 2 , the use of E-ATT brings 17% energy reduction.","Although we have shown the superiority of E-ATT, considering the whole Transformer block 2 , the use of E-ATT brings 17% energy reduction.",yes
3396,326-ARR,326-ARR_v2_60@6,326-ARR_v1_53@6,It is worth to further design techniques that reduce the energy cost of other modules in TRANSFORMER.,It is worth to further design techniques that reduce the energy cost of other modules in Transformer.,yes
,326-ARR,326-ARR_v2_57@0,326-ARR_v1_68@3,Binarization Statistics,"Those of TRANSFORMER block are 83.17% and 83.10%, respectively.",no
,326-ARR,326-ARR_v2_57@0,326-ARR_v1_2@5,Binarization Statistics,Our code will be released upon the acceptance.,no
,326-ARR,326-ARR_v2_57@0,326-ARR_v1_54@3,Binarization Statistics,All datasets are modified into truecase format with mosesdecoder by training truecase models upon train set.,no
3400,327-ARR,327-ARR_v2_40@1,327-ARR_v1_41@1,"This baseline helps reduce variance in REINFORCE (Williams, 1992).",This baseline helps reduce variance in REINFORCE.,yes
3401,327-ARR,327-ARR_v2_2@6,327-ARR_v1_2@7,"Additionally, we find that this model enjoys stable training relative to a non-RL setting.","In addition, we find that this approach enjoys stable training compared to a non-RL setting.",yes
3402,327-ARR,327-ARR_v2_57@1,327-ARR_v1_58@1,It scores higher than XENT by 1.28 BLEU and 0.71 BLEURT points.,It scores higher than the XENT baseline by 1.28 BLEU and 0.71 BLEURT points.,yes
3403,327-ARR,327-ARR_v2_57@2,327-ARR_v1_58@2,"Although BL-R was specially trained to optimize BLEU, F DISC still outperforms it by over half a point on that metric.","Although BL-R was specially trained to optimize BLEU, F DISC still outperforms it by over half a point that metric.",yes
3404,327-ARR,327-ARR_v2_65@0,327-ARR_v1_65@0,This paper proposes new F1 score metrics based on optimized rather than greedy alignments between predicted and reference tokens.,This paper proposed new F1 score metrics based on optimized rather than greedy alignments between predicted and reference tokens.,yes
3405,327-ARR,327-ARR_v2_65@1,327-ARR_v1_65@1,"Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extract alignments as a constrained optimization problem.","Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extracted alignments as a constrained optimization problem.",yes
3406,327-ARR,327-ARR_v2_65@3,327-ARR_v1_65@3,"In the continuous case, we find alignments that minimize earth mover's distance between the two token embedding distributions.","In the continuous case, we found alignments that minimized earth mover's distance between the two token embedding distributions.",yes
3407,327-ARR,327-ARR_v2_66@0,327-ARR_v1_66@0,"We apply new metrics as rewards during RLbased training for AMR-to-text generation, with F DISC outperforming both a cross-entropy baseline and one optimizing BLEU rewards.","We applied new metrics as rewards during RLbased training for AMR-to-text generation, with F DISC outperforming both a cross-entropy baseline and one optimizing BLEU rewards.",yes
3408,327-ARR,327-ARR_v2_66@1,327-ARR_v1_66@1,"Despite being computed on a downstream model's token embeddings, the metrics still provide informative rewards during training without signs of overfitting.","Despite being computed on a downstream model's token embeddings, the metrics still provided informative rewards during training without signs of overfitting.",yes
3409,327-ARR,327-ARR_v2_2@1,327-ARR_v1_2@1,Metrics leveraging contextualized embeddings appear more flexible than those that match n-grams and thus ideal as training rewards.,Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards.,yes
3410,327-ARR,327-ARR_v2_2@2,327-ARR_v1_2@2,"Yet metrics such as BERTSCORE greedily align candidate and reference tokens, which can give system outputs excess credit relative to a reference.","However, metrics such as BERTSCORE greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference.",yes
3411,327-ARR,327-ARR_v2_2@3,327-ARR_v1_2@3,Past systems using such semantic similarity rewards further suffer from repetitive outputs and overfitting.,"Furthermore, past approaches featuring semantic similarity rewards suffer from repetitive outputs and overfitting.",yes
3412,327-ARR,327-ARR_v2_16@0,327-ARR_v1_17@0,"Aligning tokens between hypothesis and reference can be seen as an assignment problem, where a token pair (ŷ i , y j ) is highly weighted if it incurs low cost (i.e., distance).","Aligning tokens between hypothesis and reference can be seen as an assignment problem, where a token pair (ŷ i , y j ) is highly weighted if it incurs low cost (i.e. distance).",yes
3413,327-ARR,327-ARR_v2_17@1,327-ARR_v1_18@1,"For the latter, we extract alignments from the earth mover's distance (EMD, Villani, 2009;Peyré and Cuturi, 2019) transport matrix.","For the latter, we extract alignments from the earth mover's distance (EMD, Villani, 2009;Peyré et al., 2019) transport matrix.",yes
3414,327-ARR,327-ARR_v2_2@4,327-ARR_v1_2@4,"To address these issues, we propose metrics that replace the greedy alignments in BERTSCORE with optimized ones.",We address these issues by proposing metrics that replace the greedy alignments in BERTSCORE with optimized ones.,yes
,327-ARR,327-ARR_v2_65@3,327-ARR_v1_2@4,"In the continuous case, we find alignments that minimize earth mover's distance between the two token embedding distributions.",We address these issues by proposing metrics that replace the greedy alignments in BERTSCORE with optimized ones.,no
,327-ARR,327-ARR_v2_2@2,327-ARR_v1_65@3,"Yet metrics such as BERTSCORE greedily align candidate and reference tokens, which can give system outputs excess credit relative to a reference.","In the continuous case, we found alignments that minimized earth mover's distance between the two token embedding distributions.",no
,327-ARR,327-ARR_v2_65@0,327-ARR_v1_66@1,This paper proposes new F1 score metrics based on optimized rather than greedy alignments between predicted and reference tokens.,"Despite being computed on a downstream model's token embeddings, the metrics still provided informative rewards during training without signs of overfitting.",no
,327-ARR,327-ARR_v2_65@3,327-ARR_v1_41@1,"In the continuous case, we find alignments that minimize earth mover's distance between the two token embedding distributions.",This baseline helps reduce variance in REINFORCE.,no
,327-ARR,327-ARR_v2_65@1,327-ARR_v1_18@1,"Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extract alignments as a constrained optimization problem.","For the latter, we extract alignments from the earth mover's distance (EMD, Villani, 2009;Peyré et al., 2019) transport matrix.",no
,327-ARR,327-ARR_v2_57@2,327-ARR_v1_2@1,"Although BL-R was specially trained to optimize BLEU, F DISC still outperforms it by over half a point on that metric.",Metrics leveraging contextualized embeddings appear more flexible than their n-gram matching counterparts and thus ideal as training rewards.,no
,327-ARR,327-ARR_v2_2@1,327-ARR_v1_65@3,Metrics leveraging contextualized embeddings appear more flexible than those that match n-grams and thus ideal as training rewards.,"In the continuous case, we found alignments that minimized earth mover's distance between the two token embedding distributions.",no
,327-ARR,327-ARR_v2_2@1,327-ARR_v1_65@1,Metrics leveraging contextualized embeddings appear more flexible than those that match n-grams and thus ideal as training rewards.,"Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extracted alignments as a constrained optimization problem.",no
,327-ARR,327-ARR_v2_2@2,327-ARR_v1_65@0,"Yet metrics such as BERTSCORE greedily align candidate and reference tokens, which can give system outputs excess credit relative to a reference.",This paper proposed new F1 score metrics based on optimized rather than greedy alignments between predicted and reference tokens.,no
,327-ARR,327-ARR_v2_65@1,327-ARR_v1_58@1,"Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extract alignments as a constrained optimization problem.",It scores higher than the XENT baseline by 1.28 BLEU and 0.71 BLEURT points.,no
,327-ARR,327-ARR_v2_16@0,327-ARR_v1_2@2,"Aligning tokens between hypothesis and reference can be seen as an assignment problem, where a token pair (ŷ i , y j ) is highly weighted if it incurs low cost (i.e., distance).","However, metrics such as BERTSCORE greedily align candidate and reference tokens, which can allow system outputs to receive excess credit relative to a reference.",no
,327-ARR,327-ARR_v2_66@1,327-ARR_v1_65@1,"Despite being computed on a downstream model's token embeddings, the metrics still provide informative rewards during training without signs of overfitting.","Instead of letting hypotheses align to reference tokens without regard to their frequencies (and vice versa), we extracted alignments as a constrained optimization problem.",no
,327-ARR,327-ARR_v2_65@3,327-ARR_v1_58@1,"In the continuous case, we find alignments that minimize earth mover's distance between the two token embedding distributions.",It scores higher than the XENT baseline by 1.28 BLEU and 0.71 BLEURT points.,no
,327-ARR,327-ARR_v2_2@6,327-ARR_v1_66@0,"Additionally, we find that this model enjoys stable training relative to a non-RL setting.","We applied new metrics as rewards during RLbased training for AMR-to-text generation, with F DISC outperforming both a cross-entropy baseline and one optimizing BLEU rewards.",no
,327-ARR,327-ARR_v2_66@1,327-ARR_v1_58@1,"Despite being computed on a downstream model's token embeddings, the metrics still provide informative rewards during training without signs of overfitting.",It scores higher than the XENT baseline by 1.28 BLEU and 0.71 BLEURT points.,no
3575,344-ARR,344-ARR_v2_16@1,344-ARR_v1_16@1,"4) Current NLU datasets do not combine a large set of fine-grained intents (again, with multiintent examples) and a large set of fine-grained slots, which prevents proper and more insightful evaluations of joint NLU models (Chen et al., 2019;Gangadharaiah and Narayanaswamy, 2019).","4) Current NLU datasets do not combine a large set of fine-grained intents (again, with multiintent examples) and a large set of fine-grained slots, which prevents proper and more insightful evaluations of joint NLU models Gangadharaiah and Narayanaswamy, 2019).",yes
3576,344-ARR,344-ARR_v2_22@0,344-ARR_v1_22@0,"One of the main contributions of this work is the novel design of the intent space, defined in a highly modular manner that natively supports intent re-combinations and multi-intent annotations 7 .","One of the main contributions of this work is the design of the intent space, defined in a highly modular manner that natively supports intent recombinations and multi-intent annotations.",yes
3577,344-ARR,344-ARR_v2_2@5,344-ARR_v1_3@1,"Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, the validity of 'intent modularisation', and call for further research on ToD NLU.","Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, and call for further research on ToD NLU.",yes
3578,344-ARR,344-ARR_v2_23@1,344-ARR_v1_22@4,"1) The modular nature of the ontology allows for expressing a much more complex set of ideas through different combinations of intent modules (see Table 3), while reducing the overall size of the intent set compared to previous ID datasets 8 (see Table 1 and Table 5).","1) The modular nature of the ontology allows for expressing a much more complex set of ideas through different combinations of intent modules (see Table 2), while reducing the overall size of the intent set compared to previous ID datasets 7 (see Table 4).",yes
3579,344-ARR,344-ARR_v2_23@2,344-ARR_v1_23@0,"2) It allows for the definition of partial intents (e.g., ""The savings one"").","2) It allows for the definition of partial intents (e.g. ""The savings one"").",yes
3580,344-ARR,344-ARR_v2_23@3,344-ARR_v1_23@1,"This is crucial in multi-turn interactions, where the user often has to answer disambiguation questions (e.g., ""Which account would you like to close?"").","This is crucial in multi-turn interactions, where the user often has to answer disambiguation questions (e.g. ""Which account would you like to close?"").",yes
3581,344-ARR,344-ARR_v2_27@0,344-ARR_v1_28@0,"Similarly to intent modules, slots can also be divided into the generic ones (e.g. time, date) and the domain-specific ones (e.g company_name, rooms, kids), see Table 10.","Similarly to intent modules, slots can also be divided into the generic ones (e.g. time, date) and the domain-specific ones (e.g company_name, rooms, kids) (see Table 9).",yes
3582,344-ARR,344-ARR_v2_29@0,344-ARR_v1_30@0,"Previous NLU datasets have usually relied on crowdworkers, aiming to collect a large number of examples, and typically optimising for quantity over quality.","Previous NLU datasets have usually relied on crowdworkers, aiming to collect a large numbers of examples, and typically optimising for quantity over quality.",yes
3583,344-ARR,344-ARR_v2_43@1,344-ARR_v1_44@1,"Another group of SotA ID baselines reformulates the ID task into the (extractive) question-answering (QA) problem (Namazifar et al., 2021;Fuisz et al., 2022).","Another group of SotA ID baselines reformulates the ID task into the (extractive) question-answering (QA) problem (Namazifar et al., 2021).",yes
3584,344-ARR,344-ARR_v2_58@4,344-ARR_v1_58@4,"For instance, we observe particularly high scores for highly generic and reusable intent modules such as change, how, how_much, thank, when, and affirm, all with per-intent F 1 scores of ≥ 90.","For instance, we observe particularly high scores for highly generic and reusable intent modules such as change, how, how_much, thank, when, and affirm, all with per-intent F 1 scores ≥ 90.",yes
3585,344-ARR,344-ARR_v2_62@3,344-ARR_v1_61@4,Future work includes rethinking the SL task for these slots.,Future work should also look into alternatives to fine-grained slot annotations for such slots.,yes
3586,344-ARR,344-ARR_v2_65@2,344-ARR_v1_62@2,"Upon collection, the dataset has undergone an additional check by the internal Ethics committee of the company.","Upon collection, the dataset has undergone an additional check by the internal Ethics committee.",yes
3587,344-ARR,344-ARR_v2_65@3,344-ARR_v1_62@3,NLU++ is licensed under CC-BY-4.0.,It is licensed under CC BY 4.0.,yes
3588,344-ARR,344-ARR_v2_69@1,344-ARR_v1_65@1,"Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) using a dual-encoder framework (Yang et al., 2019) with larger embedding capacity (i.e., a shared multilingual vocabulary of 500k subwords).","Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) using a dual-encoder framework with larger embedding capacity (i.e., a shared multilingual vocabulary of 500k subwords).",yes
3589,344-ARR,344-ARR_v2_6@9,344-ARR_v1_5@18,"Unlike previous ID datasets, examples are annotated with multiple labels, named intent modules 3 (see Table 1), with some examples naturally obtaining even up to 6-7 labels.","Unlike previous ID datasets, examples are annotated with multiple labels, with some examples naturally obtaining even up to 6-7 labels.",yes
3590,344-ARR,344-ARR_v2_6@11,344-ARR_v1_5@19,"In addition, NLU++ defines a rich set of slots which are combined with the multi-intent sentences.",NLU++ defines a rich set of slots which are combined with multi-intent sentences.,yes
3591,344-ARR,344-ARR_v2_7@1,344-ARR_v1_6@1,"Our benchmark comparisons also demonstrate strong performance and shed new light on the (ability of) recently emerging QA-based NLU models (Namazifar et al., 2021;Fuisz et al., 2022), and warrant further research on ToD NLU.","Our benchmark comparisons also demonstrate strong performance and shed new light on the (ability of) recently emerging QA-based NLU models, and warrant further research on ToD NLU.",yes
3592,344-ARR,344-ARR_v2_2@2,344-ARR_v1_2@2,"1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences, introducing and validating the idea of intent modules that can be combined into complex intents that convey complex user goals, combined with finer-grained and thus more challenging slot sets.",1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences combined with finer-grained and thus more challenging slot sets.,yes
3593,344-ARR,344-ARR_v2_10@0,344-ARR_v1_9@0,"In order to adapt to the increasing data requirements of deep learning models, increasingly larger dialogue datasets have been released in recent years (Budzianowski et al., 2018;Wei et al., 2018;Rastogi et al., 2019;Peskov et al., 2019).","To adapt to the increasing data requirements of deep learning models, increasingly larger dialogue datasets have been released in recent years (Budzianowski et al., 2018;Wei et al., 2018;Rastogi et al., 2019;Peskov et al., 2019).",yes
3594,344-ARR,344-ARR_v2_11@0,344-ARR_v1_10@0,"2) The domain-specific ontologies required a lot of expertise for annotation, therefore many annotation mistakes were made (Eric et al., 2019;Zang et al., 2020).","2) The domain-specific ontologies required a lot of expertise for annotation, therefore many annotation mistakes were made (Eric et al., 2019;",yes
3595,344-ARR,344-ARR_v2_2@3,344-ARR_v1_2@3,"2) The ontology is divided into domain-specific and generic (i.e., domainuniversal) intent modules that overlap across domains, promoting cross-domain reusability of annotated examples.","2) The ontology is divided into domain-specific and generic (i.e., domainuniversal) intents that overlap across domains, promoting cross-domain reusability of annotated examples.",yes
,344-ARR,344-ARR_v2_63@2,344-ARR_v1_37@7,"However, future work should start looking into NLU datasets composed by system + user turns.",Domain Setups.,no
,344-ARR,344-ARR_v2_62@0,344-ARR_v1_37@7,"Further, we believe that span-based annotation might be sub-optimal for canonical values such as times and dates, where small differences in the span would lead to evaluation errors but would not suppose a problem for the value to be parsed.",Domain Setups.,no
,344-ARR,344-ARR_v2_63@0,344-ARR_v1_49@3,"Finally, while single-turn NLU is more dataefficient and easier to model, some user utterances only make sense in the presence of context from the previous system utterance.","We repeated the similar hyper-parameter search procedure for QA-based models, using ALB-QA.",no
,344-ARR,344-ARR_v2_62@0,344-ARR_v1_49@3,"Further, we believe that span-based annotation might be sub-optimal for canonical values such as times and dates, where small differences in the span would lead to evaluation errors but would not suppose a problem for the value to be parsed.","We repeated the similar hyper-parameter search procedure for QA-based models, using ALB-QA.",no
,344-ARR,344-ARR_v2_23@7,344-ARR_v1_49@3,"For example, the module overdraft is clearly related to BANKING, but the module change is much more generic, likely to occur in several different domains.","We repeated the similar hyper-parameter search procedure for QA-based models, using ALB-QA.",no
,344-ARR,344-ARR_v2_61@3,344-ARR_v1_37@7,"Further, the boundaries of some generic intents can sometimes be unclear and difficult to annotate, even for expert annotators.",Domain Setups.,no
,344-ARR,344-ARR_v2_63@2,344-ARR_v1_49@2,"However, future work should start looking into NLU datasets composed by system + user turns.",17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups.,no
,344-ARR,344-ARR_v2_63@1,344-ARR_v1_37@7,"While some previous datasets (Coope et al., 2020) deal with this issue with the help of extra annotations indicating if a slot has been requested, in this work we opt for using non-contextualised slots such as number and time and let the policy handle the contextualisation.",Domain Setups.,no
,344-ARR,344-ARR_v2_61@4,344-ARR_v1_37@7,18 Future work should try to ground the set of generic intents.,Domain Setups.,no
,344-ARR,344-ARR_v2_6@10,344-ARR_v1_37@7,"These labels can be seen as sub-intent annotations, where their combinations yield full intents equivalent to ""traditional"" intents (Table 1).",Domain Setups.,no
,344-ARR,344-ARR_v2_23@5,344-ARR_v1_49@2,"For instance, if (i) examples with the intents change and booking, and (ii) examples with the intents cancel and account exist in the training data, (iii) an unseen example with the intents cancel and booking could be properly predicted, as all the single intents/modules have already been seen by the ID model 9 .",17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups.,no
,344-ARR,344-ARR_v2_62@0,344-ARR_v1_49@2,"Further, we believe that span-based annotation might be sub-optimal for canonical values such as times and dates, where small differences in the span would lead to evaluation errors but would not suppose a problem for the value to be parsed.",17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups.,no
,344-ARR,344-ARR_v2_23@5,344-ARR_v1_49@3,"For instance, if (i) examples with the intents change and booking, and (ii) examples with the intents cancel and account exist in the training data, (iii) an unseen example with the intents cancel and booking could be properly predicted, as all the single intents/modules have already been seen by the ID model 9 .","We repeated the similar hyper-parameter search procedure for QA-based models, using ALB-QA.",no
,344-ARR,344-ARR_v2_61@4,344-ARR_v1_49@2,18 Future work should try to ground the set of generic intents.,17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups.,no
,344-ARR,344-ARR_v2_61@3,344-ARR_v1_49@3,"Further, the boundaries of some generic intents can sometimes be unclear and difficult to annotate, even for expert annotators.","We repeated the similar hyper-parameter search procedure for QA-based models, using ALB-QA.",no
,344-ARR,344-ARR_v2_63@0,344-ARR_v1_49@2,"Finally, while single-turn NLU is more dataefficient and easier to model, some user utterances only make sense in the presence of context from the previous system utterance.",17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups.,no
,344-ARR,344-ARR_v2_6@10,344-ARR_v1_49@2,"These labels can be seen as sub-intent annotations, where their combinations yield full intents equivalent to ""traditional"" intents (Table 1).",17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups.,no
,344-ARR,344-ARR_v2_6@10,344-ARR_v1_49@3,"These labels can be seen as sub-intent annotations, where their combinations yield full intents equivalent to ""traditional"" intents (Table 1).","We repeated the similar hyper-parameter search procedure for QA-based models, using ALB-QA.",no
,344-ARR,344-ARR_v2_63@1,344-ARR_v1_49@2,"While some previous datasets (Coope et al., 2020) deal with this issue with the help of extra annotations indicating if a slot has been requested, in this work we opt for using non-contextualised slots such as number and time and let the policy handle the contextualisation.",17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups.,no
,344-ARR,344-ARR_v2_23@5,344-ARR_v1_37@7,"For instance, if (i) examples with the intents change and booking, and (ii) examples with the intents cancel and account exist in the training data, (iii) an unseen example with the intents cancel and booking could be properly predicted, as all the single intents/modules have already been seen by the ID model 9 .",Domain Setups.,no
,344-ARR,344-ARR_v2_63@0,344-ARR_v1_37@7,"Finally, while single-turn NLU is more dataefficient and easier to model, some user utterances only make sense in the presence of context from the previous system utterance.",Domain Setups.,no
3728,350-ARR,350-ARR_v2_37@5,350-ARR_v1_37@5,The test set is used only for measuring phoneme error rate and is not involved in any tuning.,The test set was used only for measuring phoneme error rate and was not involved in any tuning.,yes
3729,350-ARR,350-ARR_v2_4@0,350-ARR_v1_4@0,Recent progress in automatic speech recognition (ASR) was made by training neural networks on increasingly large amounts of annotated data.,Recent progress in automatic speech recognition (ASR) has been obtained by training neural networks on increasingly large amounts of annotated data.,yes
3730,350-ARR,350-ARR_v2_49@1,350-ARR_v1_49@1,"Note that the nru33 subset is used here rather than the full nru, to make it more comparable with other languages.","Note that the nru33 subset is used here rather than the full nru, to make it more comparable.",yes
3731,350-ARR,350-ARR_v2_50@0,350-ARR_v1_49@3,Phoneme error rates (PER) reported are obtained using the speaker turn segmentation from the transcript.,Phoneme error rates reported are obtained using the speaker turn segmentation from the transcript.,yes
3732,350-ARR,350-ARR_v2_52@1,350-ARR_v1_52@0,"Here, phoneme error rate (PER) columns in Table 3 show that pretrained XLSR-53 outperforms other models for all languages in public datasets.","Looking at the phoneme error rate (PER) columns in Table 3, XLSR-53 is seen to outperform the other models for all languages in the public dataset, with an average of 13.6% PER.",yes
3733,350-ARR,350-ARR_v2_52@2,350-ARR_v1_52@1,"In one case (mlv), it obtains 8.6% PER with only 20 minutes of training.","In one case, it obtains 8.6% PER with only 20 minutes of training.",yes
3734,350-ARR,350-ARR_v2_52@3,350-ARR_v1_52@2,"Similarly in the private dataset, Table 4 shows XLSR-53 outperforming the other models for all languages.","Similarly in the private dataset, Table 4 shows XLSR-53 outperforming the other models for all languages, except for Tsuut'ina (srs).",yes
3735,350-ARR,350-ARR_v2_52@4,350-ARR_v1_52@4,"Note that the HMM-GMM result for Cree (crl) is 13.0% PER, slightly better than for the HMM-BLSTM model without LM result from (Gupta and Boulianne, 2020).","Note that the HMM-GMM result for Cree (crl) is 14.2% PER, slightly better than previously reported for an HMM-BLSTM deep recursive model (Gupta and Boulianne, 2020).",yes
3736,350-ARR,350-ARR_v2_54@1,350-ARR_v1_53@1,"This can be seen in Table 5, where we summarized results from Tables 3 and 4 Table 5 shows that with over 99 minutes, HMM-GMM, XLSR and k2-conf have a PER of 13.8% or less.","This can be seen in Table 5, where we summarized results from Tables 3 and 4 Table 5 shows that with over 99 minutes, HMM-GMM, XLSR and k2-conf have a PER of 13.9% or less (excluding the srs result for XLSR).",yes
3737,350-ARR,350-ARR_v2_54@5,350-ARR_v1_53@5,The result for the full nru set from Wisniewski et al. (2020) is included for completeness.,The result for nru from Wisniewski et al. (2020) is included for completeness.,yes
3738,350-ARR,350-ARR_v2_54@7,350-ARR_v1_53@7,"Here a PER below 9% was obtained for all the languages in Tables 3 and 4 which had more than 99 minutes for training, so it looks like useful error rates are feasible with 1.7 hours of transcribed data.","Here a PER below 10% was obtained for all the languages in Tables 3 and 4 which had more than 99 minutes for training, so it looks like useful error rates are feasible with 1.5 hours of transcribed data.",yes
3739,350-ARR,350-ARR_v2_55@2,350-ARR_v1_54@2,"So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware, and makes it possible to test many different hypothesis in a short time, for example about the phoneme inventory.","So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware in a short time.",yes
3740,350-ARR,350-ARR_v2_58@0,350-ARR_v1_57@0,Fine-tuning a large pretrained multilingual model clearly outperformed the other approaches.,Fine-tuning a large pretrained multilingual model outperformed the other approaches (although failing in one case).,yes
3741,350-ARR,350-ARR_v2_6@1,350-ARR_v1_6@1,"In addition, none has yet evaluated fine-tuning recent large models pretrained on many languages, for example XLSR (Conneau et al., 2020) 3 , which are particularly well suited for low-resource languages.","In addition, none has yet evaluated the recent large models pretrained on many languages, for example XLSR (Conneau et al., 2020) 3 , which are particularly well suited for low-resource languages.",yes
3742,350-ARR,350-ARR_v2_7@3,350-ARR_v1_7@3,"We more firmly establish feasibility of accurate phonemic transcription with 3 hours or less of transcribed data by reporting on 4 new languages, including Cree and highly polysynthetic Inuktitut, in addition to 7 other previously studied in the literature.","We more firmly establish the feasibility of accurate automatic phonemic transcription with 3 hours or less of transcribed data by reporting on 4 new languages, including Cree and highly polysynthetic Inuktitut, in addition to 7 other previously studied in the literature.",yes
3743,350-ARR,350-ARR_v2_12@0,350-ARR_v1_12@0,Table 1 gives amounts of training and testing audio in minutes for each language in this dataset.,Table 1 gives the amount of training and testing audio in minutes for each language in this dataset.,yes
3744,350-ARR,350-ARR_v2_12@1,350-ARR_v1_12@1,"The language code is ISO-639-3 (International Organization for Standardization, 2018).","The language code is the 3-letter ISO-639-3 code (International Organization for Standardization, 2018).",yes
3745,350-ARR,350-ARR_v2_12@2,350-ARR_v1_12@2,The number of phonemes depends on the particular rules for grapheme-to-phoneme conversion (more details in section 3.2).,The number of phonemes depends on the particular rules for grapheme-to-phoneme conversion (as described in more details in section 3.2).,yes
3746,350-ARR,350-ARR_v2_12@3,350-ARR_v1_12@3,"The IPA column says yes when the recording was transcribed in IPA phonemes, otherwise it was in orthographic text.","The IPA column contains yes if recording was transcribed in IPA phonemes, or no if transcribed in orthographic text.",yes
3747,350-ARR,350-ARR_v2_14@2,350-ARR_v1_14@2,Transcribed recordings from a single speaker of Kurmanji Kurdish were kindly shared with us by Translators without Borders.,Transcribed recordings from a single speaker of Kurmanji Kurdish transcribed were kindly shared with us by Translators without Borders.,yes
,350-ARR,350-ARR_v2_6@2,350-ARR_v1_52@3,"We think it fair to include such models, as we aim at a practical solution for the transcription problem at hand, regardless of the underlying approach.","For this language XLSR-53 training converges unusually fast and floors to a high PER; in contrast, HMM-GMM and k2-conf results on this language are their best in the private dataset.",no
,350-ARR,350-ARR_v2_6@2,350-ARR_v1_52@5,"We think it fair to include such models, as we aim at a practical solution for the transcription problem at hand, regardless of the underlying approach.","The HMM-GMM system obtains the second-lowest average PER (41.1%) for public languages but is third for the private dataset, with 13.2% PER.",no
,350-ARR,350-ARR_v2_53@0,350-ARR_v1_52@3,It was feasible to train HMM-GMM with 10 different random train/test partitions 13 and compute the Student's t 95% uncertainty intervals shown in the PER column.,"For this language XLSR-53 training converges unusually fast and floors to a high PER; in contrast, HMM-GMM and k2-conf results on this language are their best in the private dataset.",no
,350-ARR,350-ARR_v2_53@0,350-ARR_v1_52@5,It was feasible to train HMM-GMM with 10 different random train/test partitions 13 and compute the Student's t 95% uncertainty intervals shown in the PER column.,"The HMM-GMM system obtains the second-lowest average PER (41.1%) for public languages but is third for the private dataset, with 13.2% PER.",no
,350-ARR,350-ARR_v2_53@1,350-ARR_v1_52@3,The uncertainty remains relatively small even for the smallest datasets which contain only a few minutes of test speech.,"For this language XLSR-53 training converges unusually fast and floors to a high PER; in contrast, HMM-GMM and k2-conf results on this language are their best in the private dataset.",no
,350-ARR,350-ARR_v2_53@1,350-ARR_v1_52@5,The uncertainty remains relatively small even for the smallest datasets which contain only a few minutes of test speech.,"The HMM-GMM system obtains the second-lowest average PER (41.1%) for public languages but is third for the private dataset, with 13.2% PER.",no
,350-ARR,350-ARR_v2_54@5,350-ARR_v1_53@1,The result for the full nru set from Wisniewski et al. (2020) is included for completeness.,"This can be seen in Table 5, where we summarized results from Tables 3 and 4 Table 5 shows that with over 99 minutes, HMM-GMM, XLSR and k2-conf have a PER of 13.9% or less (excluding the srs result for XLSR).",no
,350-ARR,350-ARR_v2_14@2,350-ARR_v1_53@7,Transcribed recordings from a single speaker of Kurmanji Kurdish were kindly shared with us by Translators without Borders.,"Here a PER below 10% was obtained for all the languages in Tables 3 and 4 which had more than 99 minutes for training, so it looks like useful error rates are feasible with 1.5 hours of transcribed data.",no
,350-ARR,350-ARR_v2_55@2,350-ARR_v1_52@0,"So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware, and makes it possible to test many different hypothesis in a short time, for example about the phoneme inventory.","Looking at the phoneme error rate (PER) columns in Table 3, XLSR-53 is seen to outperform the other models for all languages in the public dataset, with an average of 13.6% PER.",no
,350-ARR,350-ARR_v2_52@3,350-ARR_v1_4@0,"Similarly in the private dataset, Table 4 shows XLSR-53 outperforming the other models for all languages.",Recent progress in automatic speech recognition (ASR) has been obtained by training neural networks on increasingly large amounts of annotated data.,no
,350-ARR,350-ARR_v2_54@7,350-ARR_v1_54@2,"Here a PER below 9% was obtained for all the languages in Tables 3 and 4 which had more than 99 minutes for training, so it looks like useful error rates are feasible with 1.7 hours of transcribed data.","So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware in a short time.",no
,350-ARR,350-ARR_v2_54@1,350-ARR_v1_49@1,"This can be seen in Table 5, where we summarized results from Tables 3 and 4 Table 5 shows that with over 99 minutes, HMM-GMM, XLSR and k2-conf have a PER of 13.8% or less.","Note that the nru33 subset is used here rather than the full nru, to make it more comparable.",no
,350-ARR,350-ARR_v2_12@0,350-ARR_v1_12@3,Table 1 gives amounts of training and testing audio in minutes for each language in this dataset.,"The IPA column contains yes if recording was transcribed in IPA phonemes, or no if transcribed in orthographic text.",no
,350-ARR,350-ARR_v2_37@5,350-ARR_v1_54@2,The test set is used only for measuring phoneme error rate and is not involved in any tuning.,"So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware in a short time.",no
,350-ARR,350-ARR_v2_52@1,350-ARR_v1_12@0,"Here, phoneme error rate (PER) columns in Table 3 show that pretrained XLSR-53 outperforms other models for all languages in public datasets.",Table 1 gives the amount of training and testing audio in minutes for each language in this dataset.,no
,350-ARR,350-ARR_v2_54@5,350-ARR_v1_6@1,The result for the full nru set from Wisniewski et al. (2020) is included for completeness.,"In addition, none has yet evaluated the recent large models pretrained on many languages, for example XLSR (Conneau et al., 2020) 3 , which are particularly well suited for low-resource languages.",no
,350-ARR,350-ARR_v2_12@1,350-ARR_v1_37@5,"The language code is ISO-639-3 (International Organization for Standardization, 2018).",The test set was used only for measuring phoneme error rate and was not involved in any tuning.,no
,350-ARR,350-ARR_v2_55@2,350-ARR_v1_4@0,"So although it does not yield the best PER, it could still be a useful model for field work, since it can run on limited hardware, and makes it possible to test many different hypothesis in a short time, for example about the phoneme inventory.",Recent progress in automatic speech recognition (ASR) has been obtained by training neural networks on increasingly large amounts of annotated data.,no
,350-ARR,350-ARR_v2_7@3,350-ARR_v1_6@1,"We more firmly establish feasibility of accurate phonemic transcription with 3 hours or less of transcribed data by reporting on 4 new languages, including Cree and highly polysynthetic Inuktitut, in addition to 7 other previously studied in the literature.","In addition, none has yet evaluated the recent large models pretrained on many languages, for example XLSR (Conneau et al., 2020) 3 , which are particularly well suited for low-resource languages.",no
,350-ARR,350-ARR_v2_49@1,350-ARR_v1_53@1,"Note that the nru33 subset is used here rather than the full nru, to make it more comparable with other languages.","This can be seen in Table 5, where we summarized results from Tables 3 and 4 Table 5 shows that with over 99 minutes, HMM-GMM, XLSR and k2-conf have a PER of 13.9% or less (excluding the srs result for XLSR).",no
4085,40-ARR,40-ARR_v2_21@2,40-ARR_v1_21@2,We provide further details on the optimization and model task performance in Appendix A.,We give optimization details and model task performance in Appendix A.,yes
4086,40-ARR,40-ARR_v2_4@0,40-ARR_v1_4@0,"The usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Zhang and Zhang, 2019;Klerke and Plank, 2019).","The usefulness of learned self-attention functions often correlates with how well it aligns with human attention (Das et al., 2016;Klerke et al., 2016;Barrett et al., 2018;Klerke and Plank, 2019).",yes
4087,40-ARR,40-ARR_v2_25@3,40-ARR_v1_25@3,"For sentiment reading, the E-Z Reader and BNC show the highest correlations followed by the Transformer attention flow values (the ranking between E-Z/BNC and Transformer flows is significant at p < 0.05 ).","For sentiment reading, the E-Z Reader and BNC show the highest correlations followed by the Transformer attention flow values (the ranking between E-Z/BNC and Transformer flows is significant at p < .05 ).",yes
4088,40-ARR,40-ARR_v2_25@4,40-ARR_v1_25@4,"For relation extraction, we see the highest correlation for BERTbase attention flows (with and without fine-tuning) and BERT-large followed by the E-Z Reader (ranking is significant at p < 0.05).","For relation extraction, we see the highest correlation for BERT-base attention flows (with and without fine-tuning) and BERT-large followed by the E-Z Reader (ranking is significant at p < .05).",yes
4089,40-ARR,40-ARR_v2_25@8,40-ARR_v1_25@8,Correlations grouped by sentence length shows stable values around 0.6 (SST) and 0.4 − 0.6 (Wikipedia) except for shorter sentences where correlations fluctuate.,Correlations grouped by sentence length shows stable values around .6 (SST) and .4-.6 (Wikipedia) except for shorter sentences where correlations fluctuate.,yes
4090,40-ARR,40-ARR_v2_4@2,40-ARR_v1_4@2,"We compare the learned attention functions and the heuristic model across two taskspecific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available dataset with eye-tracking recordings of native speakers of English .","We compare the learned attention functions and the heuristic model across two task-specific English reading tasks, namely sentiment analysis (SST movie reviews) and relation extraction (Wikipedia), as well as natural reading, using a publicly available data set with eye-tracking recordings of native speakers of English .",yes
4091,40-ARR,40-ARR_v2_32@2,40-ARR_v1_31@2,"We can see that the Transformer models correlate better for more predictable words on both datasets whereas the E-Z Reader is less influenced by word predictability and already shows medium correlation on the most hard-to-predict words (0.3 − 0.4 for both, SST and Wikipedia).","We can see that the Transformer models correlate better for more predictable words on both datasets whereas the E-Z Reader is less influenced by word predictability and already shows medium correlation on the most hard-to-predict words (.3-.4 for both, SST and Wikipedia).",yes
4092,40-ARR,40-ARR_v2_32@3,40-ARR_v1_31@3,"In fact, on SST, Transformers only pass the E-Z Reader on the most predictable tokens (word predictability > 0.03).","In fact, on SST, Transformers only pass the E-Z Reader on the most predictable tokens (word predictability >.03).",yes
4093,40-ARR,40-ARR_v2_33@12,40-ARR_v1_33@12,The highest correlation can be observed when comparing human attention for task-specific and natural reading (0.72).,The highest correlation can be observed when comparing human attention for task-specific and natural reading (.72).,yes
4094,40-ARR,40-ARR_v2_43@1,40-ARR_v1_42@1,"It remains an open problem how best to interpret self-attention modules (Jain and Wallace, 2019;Wiegreffe and Pinter, 2019), and whether they provide meaningful explanations for model predictions.","It remains an open problem how best to interpret self-attention modules (Jain and Wallace, 2019;Wiegreffe and Pinter, 2019), and whether they provide explanations for model decisions.",yes
4095,40-ARR,40-ARR_v2_43@4,40-ARR_v1_42@3,Faithfulness and practicality is often evaluated using automated procedures such as input reduction experiments or measuring time and model complexity.,Faithfulness and practicality can be evaluated using automated procedures such as input reduction experiments or measuring time and model complexity.,yes
4096,40-ARR,40-ARR_v2_48@5,40-ARR_v1_47@5,"Our input reduction suggest that in a sense, trained language models and humans have suboptimal, i.e., less sparse, task-solving strategies, and are heavily regularized by what is optimal in natural reading contexts.","Our input reduction experiments suggest that in a sense, both pretrained language models and humans have suboptimal, i.e., less sparse, task-solving strategies, and are heavily regularized by what is optimal in natural reading contexts.",yes
4097,40-ARR,40-ARR_v2_5@7,40-ARR_v1_5@7,"In addition, we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns.",In addition we investigate a subset of the ZuCo corpus for which aligned task-specific and natural reading data is available and find that Transformers correlate stronger to natural reading patterns.,yes
4101,40-ARR,40-ARR_v2_5@10,40-ARR_v1_5@10,Our code is available at github.com/ oeberle/task_gaze_transformers.,Our code is available at github.com/anon.,yes
4102,40-ARR,40-ARR_v2_2@3,40-ARR_v1_2@3,"We find the predictiveness of large-scale pretrained self-attention for human attention depends on 'what is in the tail', e.g., the syntactic nature of rare contexts.","We find the predictiveness of large-scale pretrained self-attention for human attention depends on 'what is in the tail,' e.g., the syntactic nature of rare contexts.",yes
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_32@1,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).","On SST, correlations with E-Z Reader are very consistent across POS tags whereas attention flow shows weak correlations on proper nouns (.12), nouns (.16) and verbs (.16) as presented in Figure 2.",no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_32@2,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",The BNC frequency baseline correlates well with human fixations on adpositions (ADP) which both assign comparably low values.,no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_32@3,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",Proper nouns (PROPN) are overestimated in BNC as a result of their infrequent occurrence.,no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_52@2,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",Resulting perplexity on the held-out test set was ppl = 81.9.,no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_52@3,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).","Then, word-based total fixation times are computed from the E-Z Readers trace files and averaged over all subjects.",no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_53@0,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).","In addition to Spearman correlation over all tokens, we also report Pearson correlation coefficients on a sentence and token-level.",no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_53@1,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",Results are displayed in Table 4.,no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_53@2,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).","Compared to Spearman correlation on all tokens, the ranking does hardly change for Pearson or sentence-level correlations.",no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_53@3,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",Absolute correlation coefficients are higher for Spearman compared to Pearson and also are slightly higher on the sentence-level as compared to the tokenlevel analysis.,no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_53@4,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",Biggest changes occur in a drop for BNC when Spearman correlation is calculated on all tokens for relation extraction and an increase for self-attention (LRP) in sentiment reading.,no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_53@5,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",We hypothesize that both effects can be traced back to the level of sparsity and the corresponding ranking for Spearman correlations.,no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_53@6,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).","In our entropy analysis we found that, i.e. self-attention shows a sparser representation which was likely caused by the overconfidence of the model, and which could explain the higher rank-based correlation.",no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_54@0,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",Figure 5 shows the full distribution of POS tags of the first tokens flipped.,no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_54@1,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",This extends Figure 4 where we only show the first 3 POS tags.,no
,40-ARR,40-ARR_v2_43@2,40-ARR_v1_55@0,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",We compute entropy values for different attention and relevance scores in both task settings.,no
4117,402-ARR,402-ARR_v2_15@5,402-ARR_v1_16@5,"Finally, the premise URLs are used to retrieve the premise articles.","Finally, the premise article URLs are used to retrieve the premise articles.",yes
4118,402-ARR,402-ARR_v2_15@6,402-ARR_v1_16@6,"We try to directly retrieve the article where possible, but also use archive.org's API in case a premise article is no longer available online.","We try to directly retrieve the article where possible, but also use archive.org's APIs in case the premise article URL is no longer available online.",yes
4119,402-ARR,402-ARR_v2_15@7,402-ARR_v1_16@7,We follow the same general procedure for data collection from Politifact and Snopes except that we directly crawl the respective websites instead of using Google's fact check tool APIs for collecting claims and associated metadata.,"We follow the same general procedure for data collection from Politifact and Snopes except that instead of using Google's fact check tool APIs for collecting claims and associated metadata, we directly crawl the respective websites to collect the data.",yes
4120,402-ARR,402-ARR_v2_4@0,402-ARR_v1_4@0,"The rise of social media has led to a democratization of news, but it has also amplified issues related to fake news and misinformation.","The rise of social media has lead to a democratization of news, but it has also amplified issues related to fake news and misinformation.",yes
4121,402-ARR,402-ARR_v2_16@0,402-ARR_v1_17@0,We perform some basic cleanup to the collected data before inclusion in the dataset.,We only perform some basic cleanup to the collected data before inclusion in the dataset.,yes
4122,402-ARR,402-ARR_v2_16@4,402-ARR_v1_17@4,"Admittedly, this does not eliminate auxiliary text such as navigation links, footer text, recommended links, etc.","Admittedly, this still does not eliminate the auxiliary text usually present in the web pages such as navigation links, footer text, recommended links, etc.",yes
4123,402-ARR,402-ARR_v2_16@6,402-ARR_v1_17@7,"We map the numerous claim veracity labels used by the fact checking websites into three broad labels: True, Partially True/False, and False.","We also map the numerous claim veracity labels used by the fact checking websites into three broad labels: True, Partially True/False, and False.",yes
4124,402-ARR,402-ARR_v2_18@0,402-ARR_v1_19@0,"The contributed dataset contains a total of 33,721 claims.","The contributed dataset contains a total of 33,697 claims.",yes
4125,402-ARR,402-ARR_v2_18@1,402-ARR_v1_19@1,"We split those claims into the following three sets: training set containing 26,976 claims, validation set containing 3,372 claims, and test set containing 3,373 claims.","We split those claims into the following three sets: training set containing 26,957 claims, validation set containing 3369 claims, and test set containing 3371 claims.",yes
4126,402-ARR,402-ARR_v2_18@3,402-ARR_v1_19@3,We provide the extracted text files for the review and premise articles.,We provide both the HTML and extracted text files for the review and premise articles.,yes
4127,402-ARR,402-ARR_v2_19@0,402-ARR_v1_20@0,Fig. 1 shows the number of claims per fact checking services.,Figure 1 shows the number of claims collected from each of the fact checking services.,yes
4128,402-ARR,402-ARR_v2_19@1,402-ARR_v1_20@1,Fig. 2 shows the claim rating distribution.,Figure 2 shows the claim rating distribution.,yes
4129,402-ARR,402-ARR_v2_19@2,402-ARR_v1_20@2,Claims in the Partially True/False and False categories significantly outnumber the claims in the True category.,We see that the claims in the Partially True/False and False categories significantly outnumber the claims in the True category.,yes
4130,402-ARR,402-ARR_v2_19@3,402-ARR_v1_20@3,"In reality, the number of true claims is much larger than the number of partially true/false and false claims, but fact checking services focus on debunking controversial claims and therefore the majority of the claims they investigate are false or partially true/false.","In reality, the number of true claims is much larger then the number of partially true/false and false claims, but fact checking services focus on debunking controversial claims and therefore the majority of the claims they investigate are false or partially true/false.",yes
4131,402-ARR,402-ARR_v2_19@4,402-ARR_v1_20@4,This imbalance poses an important challenge.,This imbalance poses an important challenge for the models.,yes
4132,402-ARR,402-ARR_v2_22@2,402-ARR_v1_22@4,This reflects the real world task of automated veracity detection more truthfully due to the availability of the premise articles cited by the professional fact checkers in claim review articles.,We argue that our dataset reflects the real world task of automated veracity detection more truthfully due to the availability of the premise articles cited by the professional fact checkers in claim review articles.,yes
4133,402-ARR,402-ARR_v2_22@4,402-ARR_v1_22@6,Using a web search to retrieve evidence documents after a fact checking service has verified a claim is problematic since multiple news agencies often publish articles referencing the original fact checking review article.,"Using a web search to retrieve evidence documents for a claim is problematic due to the fact that once a fact checking service has fact checked a claim, we observe that multiple other news agency also publish articles referencing the original fact checking review article.",yes
4136,402-ARR,402-ARR_v2_24@2,402-ARR_v1_24@2,"For the first stage, we evaluate two different approaches.","For the first stage, we use and evaluate two different approaches.",yes
4137,402-ARR,402-ARR_v2_24@3,402-ARR_v1_24@3,"The first approach is term frequency inverse document frequency (TF-IDF), which is typically used by fact checking methods for sentence based retrieval (Aly et al., 2021).",The first approach is the well-known and commonly used basic text retrieval technique called term frequency inverse document frequency (TF-IDF).,yes
4138,402-ARR,402-ARR_v2_30@0,402-ARR_v1_31@0,A key step performed by professional fact checkers is examining the premise articles associated with a claim and extracting useful evidence from them to establish claim veracity.,One of the key steps in the fact checking process performed by human professional fact checkers is examining the premise articles associated with a claim and extracting useful evidence from them to establish claim veracity.,yes
4139,402-ARR,402-ARR_v2_32@0,402-ARR_v1_33@1,We measure TF-IDF similarity between the claim text and the premise article sentences to rank the sentence level evidence.,We use TF-IDF based similarity measure between the claim text and the premise article sentences to rank the sentence level evidence.,yes
4140,402-ARR,402-ARR_v2_34@0,402-ARR_v1_35@0,We propose a novel way of adapting the dense passage retrieval method proposed by Karpukhin et al. (2020) task of retrieving evidence sentences from premise articles.,We propose a novel way of adapting dense passage retrieval methods proposed by Karpukhin et al. (2020) for open domain question answering to the task of retrieving evidence sentences from premise articles.,yes
4141,402-ARR,402-ARR_v2_34@1,402-ARR_v1_35@1,Karpukhin et al.'s method uses a dual encoder architecture.,The dense passage retrieval method proposed by Karpukhin et al. (2020) uses a dual encoder architecture.,yes
4142,402-ARR,402-ARR_v2_34@3,402-ARR_v1_35@3,The question encoder E Q and the passage encoder E P embed question q and passage p into d-dimensional vectors.,The question encoder E Q and the passage encoder E P embed a given question q and passage p into d-dimensional real-valued vectors.,yes
4143,402-ARR,402-ARR_v2_36@0,402-ARR_v1_37@0,The model is then trained to learn embeddings such that the similarity score between relevant questionpassage pairs will be higher than irrelevant ones.,The model is then trained to learn embedding functions such that the similarity score between relevant pairs of questions and passages will be higher than irrelevant ones.,yes
4144,402-ARR,402-ARR_v2_36@1,402-ARR_v1_38@0,We adapt this method for our first stage by taking advantage of the fact that the review article published by fact checking websites (along with a claim) typically contains key evidence taken from the premise articles.,We adapt this method for our first stage by taking advantage of the fact that the review article published by fact checking websites along with a claim typically contains key evidence taken from the premise articles.,yes
4145,402-ARR,402-ARR_v2_36@2,402-ARR_v1_38@1,The evidence is usually paraphrased in order to form a coherent argument in support of the claim veracity verdict.,The evidence taken from the premise articles is usually paraphrased in order to form a coherent argument in support of the claim veracity verdict.,yes
4146,402-ARR,402-ARR_v2_39@1,402-ARR_v1_41@1,"Each instance is made up of a claim C i with one positive sentence from the associated review article s R+ i,j and n − 1 randomly chosen negative sentences s R− i,k .","Each instance is made up of a claim C i with one positive sentence from the associated review article s R+ i,j and n randomly chosen negative sentences s R− i,k .",yes
4147,402-ARR,402-ARR_v2_42@0,402-ARR_v1_44@0,"After training, we use the encoders to encode the claim text and the sentences of the associated premise articles.","After training the model, we use the encoders to encode the claim text and the sentences of the associated premise articles.",yes
4148,402-ARR,402-ARR_v2_42@2,402-ARR_v1_44@2,We use the top scoring sentences as evidence sentences in the next stage to perform claim veracity inference.,We use the top scoring sentences as evidence sentences in the next stage to perform the claim veracity inference.,yes
4149,402-ARR,402-ARR_v2_48@2,402-ARR_v1_50@2,Each sentence (claimant with claim text or each evidence sentence of the premise article) is embedded as a sequence of hidden vectors (one per word) by a bi-directional recurrent network (Bi-LSTM or Bi-GRU).,"At the bottom of a HAN, each sentence (claimant with claim text or each evidence sentence of the premise article) is embedded as a sequence of hidden vectors (one per word) by a bi-directional recurrent network (Bi-LSTM or Bi-GRU).",yes
4150,402-ARR,402-ARR_v2_50@2,402-ARR_v1_52@2,The input sequence is encoded using the RoBERTa-base model and passed through a dense linear layer followed by a softmax to obtain the predicted claim veracity label distribution.,The input sequence is encoded using the RoBERTa-base model and passed through a dense linear layer followed by a softmax layer to obtain the predicted claim veracity label distribution.,yes
4151,402-ARR,402-ARR_v2_56@3,402-ARR_v1_57@3,The results clearly show that the DPR (dense passage retrieval) method outperforms the method based on TF-IDF.,The results clearly show that the DPR (dense passage retrieval) method outperforms the TF-IDF similarity based method.,yes
4152,402-ARR,402-ARR_v2_61@4,402-ARR_v1_60@1,The evidence sentences are then concatenated to the claim text and truncated to the maximum sequence length capability of the transformer model being used to perform claim veracity inference.,The sentences with top scores are then used to perform claim veracity inference.,yes
4153,402-ARR,402-ARR_v2_64@0,402-ARR_v1_64@0,Fig. 3 shows the number of claims investigated in each 6-month period in our dataset.,Figure 3 shows the number of claims investigated in each 6-month period in our dataset.,yes
4154,402-ARR,402-ARR_v2_64@4,402-ARR_v1_64@4,Fig. 4 shows the macro F1 results achieved by the top 4 algorithms with DPR evidence in each 6-month period.,Figure 4 shows the macro F1 results achieved by the top 4 algorithms with DPR evidence in each 6-month period.,yes
4155,402-ARR,402-ARR_v2_66@1,402-ARR_v1_66@1,WatClaimCheck includes premise articles used by professional fact checkers and therefore corresponds more closely to the task of claim veracity inference in automated fact checking.,It is the first dataset that includes premise articles used by professional fact checkers and therefore corresponds more closely to the task of claim veracity inference in automated fact checking.,yes
4158,402-ARR,402-ARR_v2_7@1,402-ARR_v1_7@1,"When the first stage fails to retrieve key passages, then the inferred verdict will be negatively affected regardless of how good the second stage is.","When the first stage fails to retrieve some key passages, then the inferred verdict will be negatively affected regardless of how good the second stage is.",yes
4159,402-ARR,402-ARR_v2_9@10,402-ARR_v1_10@6,"Finally, Sect. 6 concludes and discusses possible future work.","Finally, Section 6 concludes and discusses possible future work.",yes
4160,402-ARR,402-ARR_v2_11@0,402-ARR_v1_12@0,"There is an important line of work that focuses on claim verification (Kotonya and Toni, 2020a;Guo et al., 2022).",There is an important line of work that focuses on claim verification.,yes
4161,402-ARR,402-ARR_v2_11@1,402-ARR_v1_12@1,"This includes techniques that predict the veracity of a claim based on the text of the claim only (Rashkin et al., 2017), linguistic features (Popat et al., 2017), meta information about the claimant (e.g., name, job, party affiliation, veracity history) (Wang, 2017), review articles (Augenstein et al., 2019;Shu et al., 2018;Nakov et al., 2021), relevant articles returned by a search engine (Popat et al., 2018;Augenstein et al., 2019;Mishra and Setty, 2019) as well as premise articles (Aly et al., 2021;Kotonya and Toni, 2020b).","This includes techniques that predict the veracity of a claim based on the text of the claim only (Rashkin et al., 2017), linguistic features (Popat et al., 2017), meta information about the claimant (e.g., name, job, party affiliation, veracity history) (Wang, 2017b), review articles (Augenstein et al., 2019;Shu et al., 2018;Nakov et al., 2021), as well as relevant articles returned by a search engine (Popat et al., 2018;Augenstein et al., 2019;Mishra and Setty, 2019).",yes
4162,402-ARR,402-ARR_v2_11@2,402-ARR_v1_12@3,There is an important distinction between articles returned by a search engine and premise articles.,There is an important distinction between articles returned by a search engine in previous work and the premise articles that we consider.,yes
4163,402-ARR,402-ARR_v2_12@4,402-ARR_v1_13@4,"An important task that can help the detection of fake news is stance detection (Borges et al., 2019;Jwa et al., 2019), i.e., does the content of an article agree or disagree with the title of the article?","An important task that can help the detection of fake news is the task of stance detection (Borges et al., 2019;Jwa et al., 2019), i.e., does the content of an article agree or disagree with the title of the article?",yes
4164,402-ARR,402-ARR_v2_15@1,402-ARR_v1_16@1,We utilize Google's fact check tool APIs 2 to collect the claims' metadata for all fact checking services except Politifact and Snopes.,We utilize Google's fact check tool APIs 1 to collect the claims' metadata for all previously listed fact checking services except Politifact and Snopes.,yes
4165,402-ARR,402-ARR_v2_15@4,402-ARR_v1_16@4,"We parse the article body, retrieving the premise article URLs used in the review article to justify the claim veracity.","We carefully parse the article body, retrieving the premise article URLs used in the claim review article to justify the claim veracity.",yes
,402-ARR,402-ARR_v2_6@4,402-ARR_v1_17@5,"In contrast, WatClaim-Check includes claims investigated by 8 fact checking organizations on any topic.",We include both the HTML and text version of the premise articles.,no
,402-ARR,402-ARR_v2_6@4,402-ARR_v1_17@6,"In contrast, WatClaim-Check includes claims investigated by 8 fact checking organizations on any topic.",We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,no
,402-ARR,402-ARR_v2_6@4,402-ARR_v1_33@0,"In contrast, WatClaim-Check includes claims investigated by 8 fact checking organizations on any topic.",TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,no
,402-ARR,402-ARR_v2_6@4,402-ARR_v1_43@0,"In contrast, WatClaim-Check includes claims investigated by 8 fact checking organizations on any topic.",We evaluate the model using the claims and the associated review articles in the validation and test set.,no
,402-ARR,402-ARR_v2_6@4,402-ARR_v1_12@2,"In contrast, WatClaim-Check includes claims investigated by 8 fact checking organizations on any topic.","To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.",no
,402-ARR,402-ARR_v2_16@5,402-ARR_v1_17@5,The premise articles include the source document of the claim when available as well as evidence articles used by fact checkers.,We include both the HTML and text version of the premise articles.,no
,402-ARR,402-ARR_v2_16@5,402-ARR_v1_17@6,The premise articles include the source document of the claim when available as well as evidence articles used by fact checkers.,We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,no
,402-ARR,402-ARR_v2_16@5,402-ARR_v1_33@0,The premise articles include the source document of the claim when available as well as evidence articles used by fact checkers.,TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,no
,402-ARR,402-ARR_v2_16@5,402-ARR_v1_43@0,The premise articles include the source document of the claim when available as well as evidence articles used by fact checkers.,We evaluate the model using the claims and the associated review articles in the validation and test set.,no
,402-ARR,402-ARR_v2_16@5,402-ARR_v1_12@2,The premise articles include the source document of the claim when available as well as evidence articles used by fact checkers.,"To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.",no
,402-ARR,402-ARR_v2_37@3,402-ARR_v1_17@5,"This corresponds to the ""gold"" negative sampling technique in (Karpukhin et al., 2020).",We include both the HTML and text version of the premise articles.,no
,402-ARR,402-ARR_v2_37@3,402-ARR_v1_17@6,"This corresponds to the ""gold"" negative sampling technique in (Karpukhin et al., 2020).",We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,no
,402-ARR,402-ARR_v2_37@3,402-ARR_v1_33@0,"This corresponds to the ""gold"" negative sampling technique in (Karpukhin et al., 2020).",TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,no
,402-ARR,402-ARR_v2_37@3,402-ARR_v1_43@0,"This corresponds to the ""gold"" negative sampling technique in (Karpukhin et al., 2020).",We evaluate the model using the claims and the associated review articles in the validation and test set.,no
,402-ARR,402-ARR_v2_37@3,402-ARR_v1_12@2,"This corresponds to the ""gold"" negative sampling technique in (Karpukhin et al., 2020).","To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.",no
,402-ARR,402-ARR_v2_39@2,402-ARR_v1_17@5,These negative sentences are positive sentences for other claims within the same batch.,We include both the HTML and text version of the premise articles.,no
,402-ARR,402-ARR_v2_39@2,402-ARR_v1_17@6,These negative sentences are positive sentences for other claims within the same batch.,We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,no
,402-ARR,402-ARR_v2_39@2,402-ARR_v1_33@0,These negative sentences are positive sentences for other claims within the same batch.,TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,no
,402-ARR,402-ARR_v2_39@2,402-ARR_v1_43@0,These negative sentences are positive sentences for other claims within the same batch.,We evaluate the model using the claims and the associated review articles in the validation and test set.,no
,402-ARR,402-ARR_v2_39@2,402-ARR_v1_12@2,These negative sentences are positive sentences for other claims within the same batch.,"To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.",no
,402-ARR,402-ARR_v2_56@1,402-ARR_v1_17@5,"The test contains a total of 114, 290 sentences and 3, 373 claims.",We include both the HTML and text version of the premise articles.,no
,402-ARR,402-ARR_v2_56@1,402-ARR_v1_17@6,"The test contains a total of 114, 290 sentences and 3, 373 claims.",We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,no
,402-ARR,402-ARR_v2_56@1,402-ARR_v1_33@0,"The test contains a total of 114, 290 sentences and 3, 373 claims.",TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,no
,402-ARR,402-ARR_v2_56@1,402-ARR_v1_43@0,"The test contains a total of 114, 290 sentences and 3, 373 claims.",We evaluate the model using the claims and the associated review articles in the validation and test set.,no
,402-ARR,402-ARR_v2_56@1,402-ARR_v1_12@2,"The test contains a total of 114, 290 sentences and 3, 373 claims.","To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.",no
,402-ARR,402-ARR_v2_60@0,402-ARR_v1_17@5,The evidence sentences are concatenated in the descending order of their similarity score.,We include both the HTML and text version of the premise articles.,no
,402-ARR,402-ARR_v2_60@0,402-ARR_v1_17@6,The evidence sentences are concatenated in the descending order of their similarity score.,We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,no
,402-ARR,402-ARR_v2_60@0,402-ARR_v1_33@0,The evidence sentences are concatenated in the descending order of their similarity score.,TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,no
,402-ARR,402-ARR_v2_60@0,402-ARR_v1_43@0,The evidence sentences are concatenated in the descending order of their similarity score.,We evaluate the model using the claims and the associated review articles in the validation and test set.,no
,402-ARR,402-ARR_v2_60@0,402-ARR_v1_12@2,The evidence sentences are concatenated in the descending order of their similarity score.,"To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.",no
,402-ARR,402-ARR_v2_60@1,402-ARR_v1_17@5,"Afterwards, the claim text and evidence sentences are concatenated.",We include both the HTML and text version of the premise articles.,no
,402-ARR,402-ARR_v2_60@1,402-ARR_v1_17@6,"Afterwards, the claim text and evidence sentences are concatenated.",We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,no
,402-ARR,402-ARR_v2_60@1,402-ARR_v1_33@0,"Afterwards, the claim text and evidence sentences are concatenated.",TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,no
,402-ARR,402-ARR_v2_60@1,402-ARR_v1_43@0,"Afterwards, the claim text and evidence sentences are concatenated.",We evaluate the model using the claims and the associated review articles in the validation and test set.,no
,402-ARR,402-ARR_v2_60@1,402-ARR_v1_12@2,"Afterwards, the claim text and evidence sentences are concatenated.","To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.",no
,402-ARR,402-ARR_v2_60@2,402-ARR_v1_17@5,The resulting text is then truncated to the maximum sequence length capability of the transformer model being used to perform claim veracity inference.,We include both the HTML and text version of the premise articles.,no
,402-ARR,402-ARR_v2_60@2,402-ARR_v1_17@6,The resulting text is then truncated to the maximum sequence length capability of the transformer model being used to perform claim veracity inference.,We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,no
,402-ARR,402-ARR_v2_60@2,402-ARR_v1_33@0,The resulting text is then truncated to the maximum sequence length capability of the transformer model being used to perform claim veracity inference.,TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,no
,402-ARR,402-ARR_v2_60@2,402-ARR_v1_43@0,The resulting text is then truncated to the maximum sequence length capability of the transformer model being used to perform claim veracity inference.,We evaluate the model using the claims and the associated review articles in the validation and test set.,no
,402-ARR,402-ARR_v2_60@2,402-ARR_v1_12@2,The resulting text is then truncated to the maximum sequence length capability of the transformer model being used to perform claim veracity inference.,"To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.",no
,402-ARR,402-ARR_v2_61@3,402-ARR_v1_17@5,We concatenate the evidence sentences in the descending order of their similarity score.,We include both the HTML and text version of the premise articles.,no
,402-ARR,402-ARR_v2_61@3,402-ARR_v1_17@6,We concatenate the evidence sentences in the descending order of their similarity score.,We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,no
,402-ARR,402-ARR_v2_61@3,402-ARR_v1_33@0,We concatenate the evidence sentences in the descending order of their similarity score.,TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,no
,402-ARR,402-ARR_v2_61@3,402-ARR_v1_43@0,We concatenate the evidence sentences in the descending order of their similarity score.,We evaluate the model using the claims and the associated review articles in the validation and test set.,no
,402-ARR,402-ARR_v2_61@3,402-ARR_v1_12@2,We concatenate the evidence sentences in the descending order of their similarity score.,"To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.",no
4232,412-ARR,412-ARR_v2_28@2,412-ARR_v1_25@2,"A gradient reversal layer (Ganin and Lempitsky, 2015) is added between the feature concatenation and the discriminator to reverse the gradients through the backpropagation phase and support extracting general features.",A gradient reversal layer is added between the feature concatenation and the discriminator to reverse the gradients through the backpropagation phase and support extracting general features.,yes
4233,412-ARR,412-ARR_v2_2@6,412-ARR_v1_2@6,"Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset.",Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques when considering the CompLex from the Lexical Complexity Prediction 2021 dataset.,yes
4234,412-ARR,412-ARR_v2_31@0,412-ARR_v1_28@0,The following setups also include the Basic Domain Adaptation training setting.,The following setups also include the Basic Domain Adaptation training setup.,yes
4235,412-ARR,412-ARR_v2_32@1,412-ARR_v1_29@1,"The concatenation layer now contains the BiLSTM and Transformer features, along with the VAE encoder features (F v ), namely F t +F c +F v .","The concatenation layer now contains the BiLSTM and Transformer features, plus the VAE encoder features (F v ).",yes
4236,412-ARR,412-ARR_v2_44@0,412-ARR_v1_41@0,"We rely on a Transformer-based model as the main feature extractor for the context of the target word (i.e., the full sentence), considering their superior performance on most natural language processing tasks.","We rely on a Transformer-based model as the main feature extractor for the context of the target word (i.e., the full sentence), considering their superior performance on most NLP tasks.",yes
4237,412-ARR,412-ARR_v2_44@1,412-ARR_v1_41@1,"The selected model for the first dataset is RoBERTa (Liu et al., 2019), as it yields better results when compared to its counterpart, BERT.","The selected model for the first dataset is RoBERTa (Liu et al., 2019), inasmuch as it yields better results when compared to its counterpart, BERT.",yes
4238,412-ARR,412-ARR_v2_44@2,412-ARR_v1_41@2,"RoBERTa is trained with higher learning rates and larger mini-batches, and it modifies the key hyper-parameters of BERT.","RoBERTa is trained with higher learning rates and larger minibatches, as well as it modifies the key hyperparameters of BERT.",yes
4239,412-ARR,412-ARR_v2_46@0,412-ARR_v1_44@0,"We aim to further improve performance by adding extra features via Variational AutoEncoders (VAEs) (Kingma and Welling, 2014) to the context representation for a target word.","We aim to further improve performance by adding extra features via Variational AutoEncoders (VAEs) (Kingma and Welling, 2013) to the context representation for a target word.",yes
4240,412-ARR,412-ARR_v2_52@1,412-ARR_v1_50@1,"Consequently, we introduced a generalization technique to extract only cross-domain features that do not present a bias towards a certain domain.","Consequently, we were introduced a generalization technique to extract only cross-domain features that do not present a bias towards a certain domain.",yes
4241,412-ARR,412-ARR_v2_58@3,412-ARR_v1_56@3,"The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016a) 1 .","The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016) 1 .",yes
4242,412-ARR,412-ARR_v2_4@2,412-ARR_v1_4@1,"However, complex word identification is a highly contextualized task, far from being trivial.","However, complex word identification (CWI) is a highly contextualized task, far from being trivial.",yes
4243,412-ARR,412-ARR_v2_58@5,412-ARR_v1_56@5,"The corresponding flow is described in Algorithm 1, while the loss function is presented in Equation 7:","The corresponding flow is described in algorithm 1, while the loss function is presented in Equation 7.",yes
4244,412-ARR,412-ARR_v2_68@1,412-ARR_v1_68@1,"The learning rate is set to 2e-5, while the loss functions used for the complexity task are the L1 loss (Janocha and Czarnecki, 2016) for the CompLex LCP dataset and the Mean Squared Error (MSE) loss (Kline and Berardi, 2005) for the CWI dataset.","The learning rate is set to 2e-5, while the loss functions used for the complexity task are the L1 loss (Janocha and Czarnecki, 2017) for the first dataset and the Mean Squared Error (MSE) loss (Kline and Berardi, 2005) for the second one.",yes
4245,412-ARR,412-ARR_v2_68@3,412-ARR_v1_68@3,The λ parameter used for domain adaptation was updated according to Equation 11:,The λ parameter used for domain adaptation was updated according to Equation 11.,yes
4246,412-ARR,412-ARR_v2_12@2,412-ARR_v1_5@0,"Nevertheless, certain training techniques and auxiliary tasks help the model improve its generalization abilities, forcing it to focus only on the most relevant, general features (Schrom et al., 2021).","Nevertheless, certain training techniques and auxiliary tasks help the model improve its generalization abilities (Schrom et al., 2021), forcing it to focus only on the most relevant, general features.",yes
4247,412-ARR,412-ARR_v2_73@2,412-ARR_v1_74@2,"Zaharia et al. (2021) created models that are based on target and context feature extractors, alongside features resulted from Graph Convolutional Networks, Capsule Networks, and pre-trained word embeddings.","Zaharia et al. (2021) created models that are based on target and context feature extractors, alongside features resulted from Graph Convolutional Networks, Capsule Networks, or pre-trained word embeddings.",yes
4248,412-ARR,412-ARR_v2_12@3,412-ARR_v1_5@1,"Techniques like domain adaptation (Ganin et al., 2016) can be used for various tasks, with the purpose of selecting relevant features for follow-up processes.","Techniques like domain adaptation can be used for various tasks, with the purpose of selecting relevant features for follow-up processes.",yes
4249,412-ARR,412-ARR_v2_76@2,412-ARR_v1_78@2,"The performance is evaluated in terms of MAE; however, we also report the Pearson Correlation Coefficient.","The performance is evaluated in terms of MAE; However, we also report the Pearson Correlation Coefficient.",yes
4250,412-ARR,412-ARR_v2_79@0,412-ARR_v1_80@0,"The domain adaptation technique supports our model to learn general cross-domain or crosslanguage features, while achieving higher performance.","The domain adaptation technique supports our model to learn general cross-domain (or cross-language) features, while achieving higher performance.",yes
4251,412-ARR,412-ARR_v2_80@0,412-ARR_v1_84@0,Conclusions and Future Work,Conclusion and Future Work,yes
4252,412-ARR,412-ARR_v2_81@0,412-ARR_v1_85@0,"This work proposes a series of training techniques, including domain adaptation, as well as multi-task adversarial learning, that can be used for improving the overall performance of the models for CWI.","This work proposes a series of training techniques, including adversarial domain adaptation, as well as multi-task learning, that can be used for improving the overall performance of the models for CWI.",yes
4253,412-ARR,412-ARR_v2_81@2,412-ARR_v1_86@0,"Moreover, by jointly training the model on the CWI tasks and an auxiliary similar task (i.e., text simplification), the overall performance is improved.","Moreover, by jointly training the model on the CWI taks and an auxiliary similar task (i.e., text simplification), the overall performance is improved.",yes
4254,412-ARR,412-ARR_v2_81@3,412-ARR_v1_86@1,"The task discriminator also ensures the extraction of general features, thus making the model more robust on the CWI dataset.","The task discriminator also ensures the extraction of general features, thus making the model more robust on the CWI task.",yes
4255,412-ARR,412-ARR_v2_13@0,412-ARR_v1_7@0,"We propose several solutions to improve the performance of a model for CWI in a cross-domain or a cross-lingual setting, by adding auxiliary components (i.e., Transformer (Vaswani et al., 2017) decoders, Variational Auto Encoders -VAEs (Kingma and Welling, 2014)), as well as a domain adaptation training technique (Farahani et al., 2021).","We propose several solutions to improve the performance of a model for CWI in a cross-domain or a cross-lingual setting, by adding auxiliary components (i.e., Transformer (Vaswani et al., 2017) decoders, Variational Auto Encoders -VAEs (Kingma and Welling, 2013)), as well as a domain adaptation training technique (Farahani et al., 2020).",yes
4256,412-ARR,412-ARR_v2_16@1,412-ARR_v1_11@1,Several works employed domain adaptation to improve performance.,Several works employ domain adaptation to improve performance.,yes
4257,412-ARR,412-ARR_v2_16@2,412-ARR_v1_11@2,"For example, Du et al. (2020) approached the sentiment analysis task by using a BERT-based (Devlin et al., 2019) (Klimaszewski and Andruszkiewicz, 2019), mixup synthesis training (Tang et al., 2020), and effective regularization (Vernikos et al., 2020).","For example, Du et al. (2020) approach the sentiment analysis task by using a BERT-based (Devlin et al., 2019) (Klimaszewski and Andruszkiewicz, 2019), mixup synthesis training (Tang et al., 2020), and effective regularization (Vernikos et al., 2020).",yes
4258,412-ARR,412-ARR_v2_17@2,412-ARR_v1_12@2,"The latter had the purpose of supporting the adversarial training setup, thus covering the scenario where the model was unable to detect whether the input language was from the source dataset or the target one.","The latter has the purpose of supporting the adversarial training setup, thus covering the scenario where the model is unable the detect whether the input language is from the source dataset or the target one.",yes
4259,412-ARR,412-ARR_v2_17@3,412-ARR_v1_12@3,"A similar cross-lingual approach was adopted by Zhang et al. (2020), who developed a system to classify entries from the target language, while only labels from the source language were provided.","A similar cross-lingual approach is adopted by Zhang et al. (2020), who developed a system to classify entries from the target language, while only labels from the source language are provided.",yes
4260,412-ARR,412-ARR_v2_18@0,412-ARR_v1_13@0,"Under a Named Entity Recognition training scenario, Kim et al. (2017) used features on two levels (i.e., word and characters), together with Recurrent Neural Networks and a language discriminator used for the domain-adversarial setup.","Under a Named Entity Recognition (NER) training scenario, Kim et al. (2017) use features on two levels (i.e., word and characters), together with Recurrent Neural Networks and a language discriminator used for the domain-adversarial setup.",yes
4261,412-ARR,412-ARR_v2_2@3,412-ARR_v1_2@3,"In this paper, we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations.",In this paper we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations.,yes
4262,412-ARR,412-ARR_v2_18@1,412-ARR_v1_13@1,"Similarly, Huang et al. (2019) used target language discriminators during the process of training models for low-resource name tagging.","Similarly, Huang et al. (2019) use target language discriminators during the process of training models for low-resource name tagging.",yes
4263,412-ARR,412-ARR_v2_19@1,412-ARR_v1_14@1,"Gooding and Kochmar (2019) based their implementation for CWI as a sequence labeling task on Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, inasmuch as the context helps towards proper identification of complex tokens.","Gooding and Kochmar (2019) base their implementation for CWI as a sequence labeling task on Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks, inasmuch as the context helps towards proper identification of complex tokens.",yes
4264,412-ARR,412-ARR_v2_19@3,412-ARR_v1_14@3,"Also adopting a sequence labeling approach, Finnimore et al. (2019) considered handcrafted features, including punctuation or syllables, that can properly identify complex structures.","Also adopting a sequence labeling approach, Finnimore et al. ( 2019) consider handcrafted features, including punctuation or syllables, that can properly identify complex structures.",yes
4265,412-ARR,412-ARR_v2_20@2,412-ARR_v1_16@0,"At the same time, Zaharia et al. (2020) explored the power of Transformer-based models (Vaswani et al., 2017) in cross-lingual environments by using different training scenarios, depending on the scarcity of the resources: zero-shot, one-shot, as well as few-shot learning.","At the same time, Zaharia et al. (2020) explore the power of Transformer-based models (Vaswani et al., 2017) in cross-lingual environments by using different training scenarios, depending on the scarcity of the resources: zero-shot, one-shot, as well as few-shot learning.",yes
4266,412-ARR,412-ARR_v2_20@4,412-ARR_v1_17@1,"For example, De Hertog and Tack (2018) introduced a series of architectures that combine deep learning features, as well as handcrafted features to address CWI as a regression problem.","For example, De Hertog and Tack (2018) introduce a series of architectures that combine deep learning features, as well as handcrafted features to solve a regression problem.",yes
4267,412-ARR,412-ARR_v2_23@1,412-ARR_v1_20@1,"The entries of Com-pLex consist of a sentence in English and a target token, alongside the complexity of the token, given its context.","The entries of Com-pLex consist of a sentence and a target token, alongside the complexity of the token, given its context.",yes
4268,412-ARR,412-ARR_v2_25@0,412-ARR_v1_22@0,"The CWI dataset was introduced in the CWI Shared Task 2018 (Yimam et al., 2018).","The Complex Word Identification (CWI) Shared Dataset was introduced in the CWI Shared Task 2018 (Yimam et al., 2018).",yes
,412-ARR,412-ARR_v2_4@0,412-ARR_v1_74@7,"The overarching goal of the complex word identification (CWI) task is to find words that can be simplified in a given text (Paetzold and Specia, 2016b).","However, the remaining models improve upon the starting architecture, with the largest improvements being observed for domain adaptation and the text simplification auxiliary task (Base + Text simplification + DA), with a Pearson correlation coefficient on the test dataset of .7744, 2.42% better than the base model.",no
,412-ARR,412-ARR_v2_4@0,412-ARR_v1_74@8,"The overarching goal of the complex word identification (CWI) task is to find words that can be simplified in a given text (Paetzold and Specia, 2016b).",The improved performance can be also seen for the Mean Absolute Error score (MAE = .0652).,no
,412-ARR,412-ARR_v2_4@0,412-ARR_v1_75@0,"The overarching goal of the complex word identification (CWI) task is to find words that can be simplified in a given text (Paetzold and Specia, 2016b).","While the Transformer decoder auxiliary task does not offer the best performance for the single word dataset, the same architecture offers the second-best performance for the multiple word dataset, with a Pearson score of .8252 compared to the best one, .8285.",no
,412-ARR,412-ARR_v2_4@0,412-ARR_v1_76@0,"The overarching goal of the complex word identification (CWI) task is to find words that can be simplified in a given text (Paetzold and Specia, 2016b).","The domain adaptation and VAE configuration provide improvements upon the base model (.7554 versus .7502 Pearson), but the VAE does not have an important contribution, considering that the Base + domain adaptation model has a slightly higher Pearson score of .7569.",no
,412-ARR,412-ARR_v2_76@5,412-ARR_v1_74@7,"Third, Gooding and Kochmar (2018) approach the English section of the dataset by employing linear regressions.","However, the remaining models improve upon the starting architecture, with the largest improvements being observed for domain adaptation and the text simplification auxiliary task (Base + Text simplification + DA), with a Pearson correlation coefficient on the test dataset of .7744, 2.42% better than the base model.",no
,412-ARR,412-ARR_v2_76@5,412-ARR_v1_74@8,"Third, Gooding and Kochmar (2018) approach the English section of the dataset by employing linear regressions.",The improved performance can be also seen for the Mean Absolute Error score (MAE = .0652).,no
,412-ARR,412-ARR_v2_76@5,412-ARR_v1_75@0,"Third, Gooding and Kochmar (2018) approach the English section of the dataset by employing linear regressions.","While the Transformer decoder auxiliary task does not offer the best performance for the single word dataset, the same architecture offers the second-best performance for the multiple word dataset, with a Pearson score of .8252 compared to the best one, .8285.",no
,412-ARR,412-ARR_v2_76@5,412-ARR_v1_76@0,"Third, Gooding and Kochmar (2018) approach the English section of the dataset by employing linear regressions.","The domain adaptation and VAE configuration provide improvements upon the base model (.7554 versus .7502 Pearson), but the VAE does not have an important contribution, considering that the Base + domain adaptation model has a slightly higher Pearson score of .7569.",no
,412-ARR,412-ARR_v2_76@6,412-ARR_v1_74@7,"The authors used several types of handcrafted features, including word n-grams, POS tags, dependency parse relations, and psycholinguistic features.","However, the remaining models improve upon the starting architecture, with the largest improvements being observed for domain adaptation and the text simplification auxiliary task (Base + Text simplification + DA), with a Pearson correlation coefficient on the test dataset of .7744, 2.42% better than the base model.",no
,412-ARR,412-ARR_v2_76@6,412-ARR_v1_74@8,"The authors used several types of handcrafted features, including word n-grams, POS tags, dependency parse relations, and psycholinguistic features.",The improved performance can be also seen for the Mean Absolute Error score (MAE = .0652).,no
,412-ARR,412-ARR_v2_76@6,412-ARR_v1_75@0,"The authors used several types of handcrafted features, including word n-grams, POS tags, dependency parse relations, and psycholinguistic features.","While the Transformer decoder auxiliary task does not offer the best performance for the single word dataset, the same architecture offers the second-best performance for the multiple word dataset, with a Pearson score of .8252 compared to the best one, .8285.",no
,412-ARR,412-ARR_v2_76@6,412-ARR_v1_76@0,"The authors used several types of handcrafted features, including word n-grams, POS tags, dependency parse relations, and psycholinguistic features.","The domain adaptation and VAE configuration provide improvements upon the base model (.7554 versus .7502 Pearson), but the VAE does not have an important contribution, considering that the Base + domain adaptation model has a slightly higher Pearson score of .7569.",no
,412-ARR,412-ARR_v2_77@0,412-ARR_v1_74@7,Table 3 presents the results obtained on the multilingual validation dataset and compares the performance of different configurations.,"However, the remaining models improve upon the starting architecture, with the largest improvements being observed for domain adaptation and the text simplification auxiliary task (Base + Text simplification + DA), with a Pearson correlation coefficient on the test dataset of .7744, 2.42% better than the base model.",no
,412-ARR,412-ARR_v2_77@0,412-ARR_v1_74@8,Table 3 presents the results obtained on the multilingual validation dataset and compares the performance of different configurations.,The improved performance can be also seen for the Mean Absolute Error score (MAE = .0652).,no
,412-ARR,412-ARR_v2_77@0,412-ARR_v1_75@0,Table 3 presents the results obtained on the multilingual validation dataset and compares the performance of different configurations.,"While the Transformer decoder auxiliary task does not offer the best performance for the single word dataset, the same architecture offers the second-best performance for the multiple word dataset, with a Pearson score of .8252 compared to the best one, .8285.",no
,412-ARR,412-ARR_v2_77@0,412-ARR_v1_76@0,Table 3 presents the results obtained on the multilingual validation dataset and compares the performance of different configurations.,"The domain adaptation and VAE configuration provide improvements upon the base model (.7554 versus .7502 Pearson), but the VAE does not have an important contribution, considering that the Base + domain adaptation model has a slightly higher Pearson score of .7569.",no
,412-ARR,412-ARR_v2_13@0,412-ARR_v1_80@0,"We propose several solutions to improve the performance of a model for CWI in a cross-domain or a cross-lingual setting, by adding auxiliary components (i.e., Transformer (Vaswani et al., 2017) decoders, Variational Auto Encoders -VAEs (Kingma and Welling, 2014)), as well as a domain adaptation training technique (Farahani et al., 2021).","The domain adaptation technique supports our model to learn general cross-domain (or cross-language) features, while achieving higher performance.",no
,412-ARR,412-ARR_v2_31@0,412-ARR_v1_86@0,The following setups also include the Basic Domain Adaptation training setting.,"Moreover, by jointly training the model on the CWI taks and an auxiliary similar task (i.e., text simplification), the overall performance is improved.",no
,412-ARR,412-ARR_v2_13@0,412-ARR_v1_85@0,"We propose several solutions to improve the performance of a model for CWI in a cross-domain or a cross-lingual setting, by adding auxiliary components (i.e., Transformer (Vaswani et al., 2017) decoders, Variational Auto Encoders -VAEs (Kingma and Welling, 2014)), as well as a domain adaptation training technique (Farahani et al., 2021).","This work proposes a series of training techniques, including adversarial domain adaptation, as well as multi-task learning, that can be used for improving the overall performance of the models for CWI.",no
,412-ARR,412-ARR_v2_4@2,412-ARR_v1_2@3,"However, complex word identification is a highly contextualized task, far from being trivial.",In this paper we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations.,no
,412-ARR,412-ARR_v2_76@2,412-ARR_v1_2@3,"The performance is evaluated in terms of MAE; however, we also report the Pearson Correlation Coefficient.",In this paper we propose a novel training technique for the CWI task based on domain adaptation to improve the target character and context representations.,no
,412-ARR,412-ARR_v2_18@1,412-ARR_v1_12@2,"Similarly, Huang et al. (2019) used target language discriminators during the process of training models for low-resource name tagging.","The latter has the purpose of supporting the adversarial training setup, thus covering the scenario where the model is unable the detect whether the input language is from the source dataset or the target one.",no
,412-ARR,412-ARR_v2_17@3,412-ARR_v1_56@3,"A similar cross-lingual approach was adopted by Zhang et al. (2020), who developed a system to classify entries from the target language, while only labels from the source language were provided.","The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016) 1 .",no
,412-ARR,412-ARR_v2_58@3,412-ARR_v1_11@1,"The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016a) 1 .",Several works employ domain adaptation to improve performance.,no
,412-ARR,412-ARR_v2_80@0,412-ARR_v1_78@2,Conclusions and Future Work,"The performance is evaluated in terms of MAE; However, we also report the Pearson Correlation Coefficient.",no
,412-ARR,412-ARR_v2_2@6,412-ARR_v1_29@1,"Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset.","The concatenation layer now contains the BiLSTM and Transformer features, plus the VAE encoder features (F v ).",no
,412-ARR,412-ARR_v2_44@1,412-ARR_v1_14@3,"The selected model for the first dataset is RoBERTa (Liu et al., 2019), as it yields better results when compared to its counterpart, BERT.","Also adopting a sequence labeling approach, Finnimore et al. ( 2019) consider handcrafted features, including punctuation or syllables, that can properly identify complex structures.",no
,412-ARR,412-ARR_v2_23@1,412-ARR_v1_22@0,"The entries of Com-pLex consist of a sentence in English and a target token, alongside the complexity of the token, given its context.","The Complex Word Identification (CWI) Shared Dataset was introduced in the CWI Shared Task 2018 (Yimam et al., 2018).",no
,412-ARR,412-ARR_v2_2@6,412-ARR_v1_50@1,"Our model obtains a boost of up to 2.42% in terms of Pearson Correlation Coefficients in contrast to vanilla training techniques, when considering the CompLex from the Lexical Complexity Prediction 2021 dataset.","Consequently, we were introduced a generalization technique to extract only cross-domain features that do not present a bias towards a certain domain.",no
,412-ARR,412-ARR_v2_31@0,412-ARR_v1_41@0,The following setups also include the Basic Domain Adaptation training setting.,"We rely on a Transformer-based model as the main feature extractor for the context of the target word (i.e., the full sentence), considering their superior performance on most NLP tasks.",no
,412-ARR,412-ARR_v2_23@1,412-ARR_v1_25@2,"The entries of Com-pLex consist of a sentence in English and a target token, alongside the complexity of the token, given its context.",A gradient reversal layer is added between the feature concatenation and the discriminator to reverse the gradients through the backpropagation phase and support extracting general features.,no
,412-ARR,412-ARR_v2_18@1,412-ARR_v1_16@0,"Similarly, Huang et al. (2019) used target language discriminators during the process of training models for low-resource name tagging.","At the same time, Zaharia et al. (2020) explore the power of Transformer-based models (Vaswani et al., 2017) in cross-lingual environments by using different training scenarios, depending on the scarcity of the resources: zero-shot, one-shot, as well as few-shot learning.",no
,412-ARR,412-ARR_v2_31@0,412-ARR_v1_56@3,The following setups also include the Basic Domain Adaptation training setting.,"The dataset used for text simplification is represented by BenchLS (Paetzold and Specia, 2016) 1 .",no
,412-ARR,412-ARR_v2_68@3,412-ARR_v1_5@0,The λ parameter used for domain adaptation was updated according to Equation 11:,"Nevertheless, certain training techniques and auxiliary tasks help the model improve its generalization abilities (Schrom et al., 2021), forcing it to focus only on the most relevant, general features.",no
,412-ARR,412-ARR_v2_58@5,412-ARR_v1_4@1,"The corresponding flow is described in Algorithm 1, while the loss function is presented in Equation 7:","However, complex word identification (CWI) is a highly contextualized task, far from being trivial.",no
,412-ARR,412-ARR_v2_68@3,412-ARR_v1_29@1,The λ parameter used for domain adaptation was updated according to Equation 11:,"The concatenation layer now contains the BiLSTM and Transformer features, plus the VAE encoder features (F v ).",no
,412-ARR,412-ARR_v2_32@1,412-ARR_v1_20@1,"The concatenation layer now contains the BiLSTM and Transformer features, along with the VAE encoder features (F v ), namely F t +F c +F v .","The entries of Com-pLex consist of a sentence and a target token, alongside the complexity of the token, given its context.",no
4281,413-ARR,413-ARR_v2_25@0,413-ARR_v1_23@0,"Following the experimental settings in NegBERT, we use the three standard benchmarks for negation cue detection and scope resolution tasks, i.e. BioScope (Vincze et al., 2008) (separated into two subsets, sourced from abstracts and full-text papers, resp.), the SFU product reviews dataset (Konstantinova et al., 2012), and the Sherlock dataset (Morante and Blanco, 2012).","Following the experiment settings in NegBERT, we use the three standard benchmarks for negation cue detection and scope resolution tasks, i.e. BioScope (Vincze et al., 2008) (separated into two subsets, sourced from abstracts and full-text papers, resp.), the SFU product reviews dataset (Konstantinova et al., 2012), and the Sherlock dataset (Morante and Blanco, 2012).",yes
4282,413-ARR,413-ARR_v2_25@3,413-ARR_v1_23@3,"To investigate cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets.","To investigate the cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets.",yes
4283,413-ARR,413-ARR_v2_28@1,413-ARR_v1_26@4,"Regarding the in-dataset setting, AugNB outperforms the baseline NegBERT on all datasets except for Sherlock.","Regarding the in-dataset setting (training and evaluating on the same dataset), AugNB outperforms the baseline NegBERT on all datasets except for Sherlock.",yes
4284,413-ARR,413-ARR_v2_28@2,413-ARR_v1_26@5,"Gains are more noticeable over the biomedical datasets (BioScope, VetCompass).","Gains are more noticeable in the biomedical datasets (BioScope, VetCompass).",yes
4285,413-ARR,413-ARR_v2_29@2,413-ARR_v1_27@2,"CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue helps the model learn better representations for negation cues and thus, better distinguish between cues and normal words.","CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue will help the model learn better representations for negation cues and thus, better distinguish between cues and normal words.",yes
4286,413-ARR,413-ARR_v2_4@4,413-ARR_v1_4@4,"A recent study on English texts found that negation detection models do not transfer well across domains, due to variations in expression of negation (Khandelwal and Sawant, 2020).","A recent work on negation detection in English texts found that negation detection models do not transfer well across domains, due to variations in expression of negation (Khandelwal and Sawant, 2020).",yes
4289,413-ARR,413-ARR_v2_2@1,413-ARR_v1_2@1,"Recent work has shown that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.","Recent works show that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.",yes
4290,413-ARR,413-ARR_v2_11@1,413-ARR_v1_10@1,"Most work follows a common scheme in extracting various features from the sentence, and using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).","Most works follow a common scheme in extracting various features from the sentence, in using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).",yes
4291,413-ARR,413-ARR_v2_15@1,413-ARR_v1_14@1,"To obtain sentences with negations, we extend the NegEx lexicon with additional negation cues obtained from biomedical texts (Morante, 2010), and apply it to sentences extracted from a corpus using the SpaCy English sentence tokenizer, keeping only those sentences with at least one identified negation cue.","To obtain sentences with negations, we extend the NegEx lexicon with additional negation cues obtained from biomedical texts (Morante, 2010), and apply it to sentences extracted from a corpus using the SpaCy English sentence tokenizer, keeping only those sentences with at least one identified negation.",yes
4292,413-ARR,413-ARR_v2_16@2,413-ARR_v1_15@2,"Descriptions of clinical trials can be quite long, but a core aspect of the trial description is the patient inclusion/exclusion criteria, specifying what types of characteristics or conditions a patient must have/not have in order to be suitable for the trial.","Description of clinical trials can be quite long, but a core aspect of the trial description is the patient inclusion/exclusion criteria, specifying what types of characteristics or conditions a patient must have/not have in order to be suitable for the trial.",yes
4293,413-ARR,413-ARR_v2_19@1,413-ARR_v1_18@1,"We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representations for sentences containing negation.","We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representation for sentences containing negation.",yes
4294,413-ARR,413-ARR_v2_20@1,413-ARR_v1_19@1,"Inspired by work on entity and span masking (Joshi et al., 2020;Yamada et al., 2020), we explore explicitly incorporating information about negation cues into the model by masking these cues, and targeting prediction of the masked cue in the pretraining stage.","Inspired by various works on entity and span masking (Joshi et al., 2020;Yamada et al., 2020), we explore explicitly incorporating information about negation cues into the model by masking these cues, and targeting prediction of the masked cue in the pre-training stage.",yes
,413-ARR,413-ARR_v2_25@0,413-ARR_v1_10@1,"Following the experimental settings in NegBERT, we use the three standard benchmarks for negation cue detection and scope resolution tasks, i.e. BioScope (Vincze et al., 2008) (separated into two subsets, sourced from abstracts and full-text papers, resp.), the SFU product reviews dataset (Konstantinova et al., 2012), and the Sherlock dataset (Morante and Blanco, 2012).","Most works follow a common scheme in extracting various features from the sentence, in using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).",no
,413-ARR,413-ARR_v2_25@3,413-ARR_v1_2@1,"To investigate cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets.","Recent works show that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.",no
,413-ARR,413-ARR_v2_29@2,413-ARR_v1_14@1,"CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue helps the model learn better representations for negation cues and thus, better distinguish between cues and normal words.","To obtain sentences with negations, we extend the NegEx lexicon with additional negation cues obtained from biomedical texts (Morante, 2010), and apply it to sentences extracted from a corpus using the SpaCy English sentence tokenizer, keeping only those sentences with at least one identified negation.",no
,413-ARR,413-ARR_v2_19@1,413-ARR_v1_2@1,"We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representations for sentences containing negation.","Recent works show that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.",no
,413-ARR,413-ARR_v2_19@1,413-ARR_v1_23@3,"We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representations for sentences containing negation.","To investigate the cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets.",no
,413-ARR,413-ARR_v2_2@1,413-ARR_v1_26@5,"Recent work has shown that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.","Gains are more noticeable in the biomedical datasets (BioScope, VetCompass).",no
,413-ARR,413-ARR_v2_28@1,413-ARR_v1_15@2,"Regarding the in-dataset setting, AugNB outperforms the baseline NegBERT on all datasets except for Sherlock.","Description of clinical trials can be quite long, but a core aspect of the trial description is the patient inclusion/exclusion criteria, specifying what types of characteristics or conditions a patient must have/not have in order to be suitable for the trial.",no
,413-ARR,413-ARR_v2_25@3,413-ARR_v1_10@1,"To investigate cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets.","Most works follow a common scheme in extracting various features from the sentence, in using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).",no
,413-ARR,413-ARR_v2_29@2,413-ARR_v1_10@1,"CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue helps the model learn better representations for negation cues and thus, better distinguish between cues and normal words.","Most works follow a common scheme in extracting various features from the sentence, in using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).",no
,413-ARR,413-ARR_v2_20@1,413-ARR_v1_27@2,"Inspired by work on entity and span masking (Joshi et al., 2020;Yamada et al., 2020), we explore explicitly incorporating information about negation cues into the model by masking these cues, and targeting prediction of the masked cue in the pretraining stage.","CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue will help the model learn better representations for negation cues and thus, better distinguish between cues and normal words.",no
,413-ARR,413-ARR_v2_4@4,413-ARR_v1_2@1,"A recent study on English texts found that negation detection models do not transfer well across domains, due to variations in expression of negation (Khandelwal and Sawant, 2020).","Recent works show that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.",no
,413-ARR,413-ARR_v2_28@2,413-ARR_v1_10@1,"Gains are more noticeable over the biomedical datasets (BioScope, VetCompass).","Most works follow a common scheme in extracting various features from the sentence, in using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).",no
4432,426-ARR,426-ARR_v2_4@0,426-ARR_v1_4@0,"The dominant approach in the design of current NLP solutions is (pre-)training a large neural language model, usually applying a Transformer architecture, such as GPT-2, RoBERTa or T5, and fine-tuning the model for specific tasks (Devlin et al., 2019;Raffel et al., 2019).","The dominant approach in the design of current NLP solutions consists in (pre-)training a large neural language model, usually applying a Transformer architecture, such as GPT-2, RoBERTa or T5, and fine-tuning the model for specific tasks (Devlin et al., 2018;Raffel et al., 2019).",yes
4433,426-ARR,426-ARR_v2_34@1,426-ARR_v1_31@1,"Secondly, the PDF files were downloaded and the English texts were processed into DjVu files (as this is the target format for the pipeline) using the pdf2djvu tool 5 .","Secondly, the PDF files were downloaded and the English texts were processed into DjVu files (as this is the target format for the pipeline) using the pdf2dvju tool 4 .",yes
4434,426-ARR,426-ARR_v2_38@1,426-ARR_v1_34@1,"At present, the most popular architectures for language models are Transformer (Devlin et al., 2019) models (earlier, e.g. Word2vec (Mikolov et al., 2013) or LSTM models (Peters et al., 2017)).","At present, the most popular architectures for language models are Transformer (Devlin et al., 2018) models (earlier, e.g. Word2vec (Mikolov et al., 2013) or LSTM models (Peters et al., 2017)).",yes
4435,426-ARR,426-ARR_v2_4@1,426-ARR_v1_4@1,"The solutions are evaluated on benchmarks such as GLUE (Wang et al., 2019b) or SuperGLUE (Wang et al., 2019a), which allow comparing the performance of various methods designed for the same purpose.","The solutions are evaluated on benchmarks such as GLUE ((Wang et al., 2018)) or SuperGLUE ((Wang et al., 2019)), which allow comparing the performance of various methods designed for the same purpose.",yes
4436,426-ARR,426-ARR_v2_41@1,426-ARR_v1_37@1,The ChallAm models have the same number of parameters as the original RoBERTa Base (125M).,The ChallAm models have the same numbers of parameters as the original RoBERTa Base (125M).,yes
4437,426-ARR,426-ARR_v2_4@2,426-ARR_v1_4@2,An important feature of a good NLP benchmark is the clear separation between train and test sets.,A main feature of a good NLP benchmark is the clear separation between train and test sets.,yes
4438,426-ARR,426-ARR_v2_4@3,426-ARR_v1_4@3,"This requirement prevents data contamination, when the model (pre-)trained on huge data might have ""seen"" the test set in some form.","This requirement prevents data contamination, when the model (pre-)trained on huge data might have ""seen"" the test set.",yes
4439,426-ARR,426-ARR_v2_46@10,426-ARR_v1_42@10,These data are used by the Gonito evaluation platform during submission evaluation.,These data are used by the evaluation platform during submission evaluation.,yes
4440,426-ARR,426-ARR_v2_52@1,426-ARR_v1_48@1,"They are released on the Gonito evaluation platform, which enables the calculation of metrics both offline and online, as well as the submission of solutions.","They are released on an evaluation platform, which enables the calculation of metrics both offline and online, as well as the submission of solutions.",yes
4441,426-ARR,426-ARR_v2_58@1,426-ARR_v1_55@1,The expected format is the latitude and longitude.,The expected format is a latitude and longitude.,yes
4442,426-ARR,426-ARR_v2_61@3,426-ARR_v1_57@3,"The metric is perplexity; PerplexityHashed, to be precise, as implemented in the GEval evaluation tool (Graliński et al., 2019), the modification is analogous to LogLossHashed in (Graliński, 2017), its goal is to ensure proper evaluation in the competitive (shared-task) setup (i.e. avoid self-reported probabilities and ensure objective comparison of all reported solutions, including out-of-vocabulary words).","The metric is perplexity; PerplexityHashed, to be precise, as implemented in the GEval evaluation tool , the modification is analogous to LogLossHashed in (Graliński, 2017), its goal is to ensure proper evaluation in the competitive (shared-task) setup (i.e. avoid self-reported probabilities and ensure objective comparison of all reported solutions, including out-of-vocabulary words).",yes
4443,426-ARR,426-ARR_v2_64@0,426-ARR_v1_61@0,Results,Baselines,yes
4444,426-ARR,426-ARR_v2_65@0,426-ARR_v1_62@0,Strong baselines for all three tasks are available at the Gonito evaluation platform.,Baselines for all three tasks are available at the evaluation platform.,yes
4445,426-ARR,426-ARR_v2_65@1,426-ARR_v1_62@1,"The baselines (see Tables 2 and 3) include, for each model, its score in the appropriate metric as well as the Git SHA1 reference code in the Gonito benchmark (in curly brackets).","5 The baselines (see Tables 2 and 3) include, for each model, its score in the appropriate metric as well as the Git SHA1 reference code (in curly brackets).",yes
4446,426-ARR,426-ARR_v2_69@1,426-ARR_v1_64@1,RetroGeo requires two values (latitude and longitude) -we treat them separately and train two separate regression models for them.,RetroGeo requires two values (latitude and longitude) -we treat them separately and train two separate models for them.,yes
4447,426-ARR,426-ARR_v2_76@1,426-ARR_v1_71@1,"Since standard RoBERTa training does not incorporate any data, but text, we did not include temporal metadata during inference.","Since standard RoBERTa training does not incorporate any data, but text, we didn't include temporal metadata during inference.",yes
4448,426-ARR,426-ARR_v2_78@1,426-ARR_v1_73@1,This means the incorporation of temporal metadata has a positive impact on the MLM task.,This means the incorporation of temporal metadata has a positive impact on MLM task.,yes
4449,426-ARR,426-ARR_v2_80@5,426-ARR_v1_75@5,"We use one of them, DeHateBERT (Aluru et al., 2020), to detect the abusive texts in the ChallAm dataset.","We use one of them, DeHateBERT (Aluru et al., 2020), to filter out the abusive texts in the ChallAm dataset.",yes
4450,426-ARR,426-ARR_v2_80@6,426-ARR_v1_75@6,We tagged items that either (1) are marked as abusive speech by DeHateBERT with the probability greater than 0.75 or (2) contain words from a list of blocked words.,We filtered out items that either (1) are marked as abusive speech by DeHateBERT with the probability greater than 0.75 or (2) contain words from a list of blocked words.,yes
4451,426-ARR,426-ARR_v2_80@7,426-ARR_v1_75@7,The fraction of detected texts was 2.04-2.40 % (depending on the challenge and set).,The fraction of filtered out texts was 2.04-2.40% (depending on the challenge and set).,yes
4452,426-ARR,426-ARR_v2_90@0,426-ARR_v1_89@0,"See Table 4 and 5 for the list of top 30 features correlating most with, respectively, the worst and bad results in ChallAm challenges (as returned by the GEval tool with the option -worst-features -numerical-features (Graliński et al., 2019)).","See Table 4 and 5 for the list of top 30 features correlating most with, respectively, the worst and bad results in ChallAm challenges (as returned by the GEval tool with the option -worst-features -numerical-features ).",yes
4453,426-ARR,426-ARR_v2_31@1,426-ARR_v1_23@1,We provide and make freely available training data from Chronicling America for three ML tasks.,We provide and make available training data from Chronicling America for three ML tasks.,yes
4454,426-ARR,426-ARR_v2_20@0,426-ARR_v1_24@0,Related Machine Learning datasets and challenges,Similar Machine Learning datasets and challenges,yes
,426-ARR,426-ARR_v2_78@3,426-ARR_v1_25@7,"Since we did not train RoBERTa ChallAm large, we cannot confirm this holds true, when it comes to large RoBERTa models.","The authors provide several baseline solutions for the two tasks, which apply up-to-date methods, pointing out that there is still room for improvement in the KIE research area.",no
,426-ARR,426-ARR_v2_80@8,426-ARR_v1_25@5,The tags along with the probabilities are available in the hate-speech-info.tsv files for each test directory.,"The two datasets described there contain, respectively, NDA documents and financial reports from charity organizations.",no
,426-ARR,426-ARR_v2_65@2,426-ARR_v1_25@4,Reference codes can be used to access any of the baseline solutions at http: //gonito.net/q.,"Some Key Information Extraction tasks are presented in (Stanisławek et al., 2021).",no
,426-ARR,426-ARR_v2_81@0,426-ARR_v1_25@7,Note that temporal and geospatial metadata might constitute useful features in future work on better detection of hate speech in historical texts.,"The authors provide several baseline solutions for the two tasks, which apply up-to-date methods, pointing out that there is still room for improvement in the KIE research area.",no
,426-ARR,426-ARR_v2_78@4,426-ARR_v1_25@5,"The standard RoBERTa large is the best performing model, so in this case, a larger model is better even if not trained on the data from different domain.","The two datasets described there contain, respectively, NDA documents and financial reports from charity organizations.",no
,426-ARR,426-ARR_v2_78@3,426-ARR_v1_25@8,"Since we did not train RoBERTa ChallAm large, we cannot confirm this holds true, when it comes to large RoBERTa models.","A challenge that comprises both layout recognition and KIE is presented in (Huang et al., 2019) -the challenge is opened for the recognition of OCR-scanned receipts.",no
,426-ARR,426-ARR_v2_78@4,426-ARR_v1_25@7,"The standard RoBERTa large is the best performing model, so in this case, a larger model is better even if not trained on the data from different domain.","The authors provide several baseline solutions for the two tasks, which apply up-to-date methods, pointing out that there is still room for improvement in the KIE research area.",no
,426-ARR,426-ARR_v2_65@2,426-ARR_v1_25@5,Reference codes can be used to access any of the baseline solutions at http: //gonito.net/q.,"The two datasets described there contain, respectively, NDA documents and financial reports from charity organizations.",no
,426-ARR,426-ARR_v2_81@0,426-ARR_v1_25@6,Note that temporal and geospatial metadata might constitute useful features in future work on better detection of hate speech in historical texts.,"The tasks for the datasets consist in detecting data points, such as effective dates, interested parties, charity address, income, spending.",no
,426-ARR,426-ARR_v2_78@3,426-ARR_v1_25@4,"Since we did not train RoBERTa ChallAm large, we cannot confirm this holds true, when it comes to large RoBERTa models.","Some Key Information Extraction tasks are presented in (Stanisławek et al., 2021).",no
,426-ARR,426-ARR_v2_65@2,426-ARR_v1_25@6,Reference codes can be used to access any of the baseline solutions at http: //gonito.net/q.,"The tasks for the datasets consist in detecting data points, such as effective dates, interested parties, charity address, income, spending.",no
,426-ARR,426-ARR_v2_78@4,426-ARR_v1_25@6,"The standard RoBERTa large is the best performing model, so in this case, a larger model is better even if not trained on the data from different domain.","The tasks for the datasets consist in detecting data points, such as effective dates, interested parties, charity address, income, spending.",no
,426-ARR,426-ARR_v2_80@8,426-ARR_v1_25@4,The tags along with the probabilities are available in the hate-speech-info.tsv files for each test directory.,"Some Key Information Extraction tasks are presented in (Stanisławek et al., 2021).",no
,426-ARR,426-ARR_v2_65@2,426-ARR_v1_25@7,Reference codes can be used to access any of the baseline solutions at http: //gonito.net/q.,"The authors provide several baseline solutions for the two tasks, which apply up-to-date methods, pointing out that there is still room for improvement in the KIE research area.",no
,426-ARR,426-ARR_v2_65@2,426-ARR_v1_25@8,Reference codes can be used to access any of the baseline solutions at http: //gonito.net/q.,"A challenge that comprises both layout recognition and KIE is presented in (Huang et al., 2019) -the challenge is opened for the recognition of OCR-scanned receipts.",no
,426-ARR,426-ARR_v2_81@0,426-ARR_v1_25@4,Note that temporal and geospatial metadata might constitute useful features in future work on better detection of hate speech in historical texts.,"Some Key Information Extraction tasks are presented in (Stanisławek et al., 2021).",no
,426-ARR,426-ARR_v2_80@8,426-ARR_v1_25@6,The tags along with the probabilities are available in the hate-speech-info.tsv files for each test directory.,"The tasks for the datasets consist in detecting data points, such as effective dates, interested parties, charity address, income, spending.",no
,426-ARR,426-ARR_v2_81@0,426-ARR_v1_25@5,Note that temporal and geospatial metadata might constitute useful features in future work on better detection of hate speech in historical texts.,"The two datasets described there contain, respectively, NDA documents and financial reports from charity organizations.",no
,426-ARR,426-ARR_v2_80@8,426-ARR_v1_25@7,The tags along with the probabilities are available in the hate-speech-info.tsv files for each test directory.,"The authors provide several baseline solutions for the two tasks, which apply up-to-date methods, pointing out that there is still room for improvement in the KIE research area.",no
,426-ARR,426-ARR_v2_81@0,426-ARR_v1_25@8,Note that temporal and geospatial metadata might constitute useful features in future work on better detection of hate speech in historical texts.,"A challenge that comprises both layout recognition and KIE is presented in (Huang et al., 2019) -the challenge is opened for the recognition of OCR-scanned receipts.",no
,426-ARR,426-ARR_v2_78@3,426-ARR_v1_25@6,"Since we did not train RoBERTa ChallAm large, we cannot confirm this holds true, when it comes to large RoBERTa models.","The tasks for the datasets consist in detecting data points, such as effective dates, interested parties, charity address, income, spending.",no
,426-ARR,426-ARR_v2_78@4,426-ARR_v1_25@4,"The standard RoBERTa large is the best performing model, so in this case, a larger model is better even if not trained on the data from different domain.","Some Key Information Extraction tasks are presented in (Stanisławek et al., 2021).",no
,426-ARR,426-ARR_v2_78@4,426-ARR_v1_25@8,"The standard RoBERTa large is the best performing model, so in this case, a larger model is better even if not trained on the data from different domain.","A challenge that comprises both layout recognition and KIE is presented in (Huang et al., 2019) -the challenge is opened for the recognition of OCR-scanned receipts.",no
4746,463-ARR,463-ARR_v2_19@1,463-ARR_v1_21@1,"When the dimension of the token's representation is r with m input tokens, the function can be defined as follows:","When the dimension of the token's representation is r with with m input tokens, the function can be defined as follows:",yes
4747,463-ARR,463-ARR_v2_21@2,463-ARR_v1_24@2,"The dimension of r 1 , r 2 and r 3 is the dimension of the 2 nd feedforward layer, while the input dimension of the linear layer (L α ) is the output dimension of the first feed-forward layer with the token representation (r 1 , r 2 , r 3 ) as its inputs.","The dimension of r 1 , r 2 and r 3 is the dimension of the 2nd feedforward layer, while the dimension of the linear layer (L α ) is the output dimension of the first feed-forward layer with the token representation (r 1 , r 2 , r 3 ) as its inputs.",yes
4748,463-ARR,463-ARR_v2_23@0,463-ARR_v1_26@0,"In this section, we experiment on two different methods to make AdapterBias more parameter efficient.","In this section, we experiment on two ways to make AdapterBias more parameter efficient.",yes
4749,463-ARR,463-ARR_v2_25@0,463-ARR_v1_28@0,"Redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important (Houlsby et al., 2019).","Redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important.",yes
4750,463-ARR,463-ARR_v2_38@0,463-ARR_v1_38@0,"In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters (Houlsby et al., 2019), Diff-pruning (Guo et al., 2020), BitFit (Ben Zaken et al., 2021), andLoRA (Hu et al., 2021).","In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters (Houlsby et al., 2019), Diff-pruning (Guo et al., 2020), andBitFit (Ben Zaken et al., 2021) 2019).",yes
4751,463-ARR,463-ARR_v2_39@1,463-ARR_v1_38@1,"Although Diff-pruning (Guo et al., 2020) achieves the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM.","Although Diff-pruning (Guo et al., 2020) has the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM.",yes
4752,463-ARR,463-ARR_v2_39@2,463-ARR_v1_38@2,"Thus, Diff-pruning requires 340M trainable parameters of BERT-large during the training stage, while AdapterBias only trains 0.17M parameters.","Thus, Diff-pruning requires 340M trainable parameters of BERT-large during the training stage, while AdapterBias only trains 0.23M parameters.",yes
4753,463-ARR,463-ARR_v2_39@3,463-ARR_v1_38@3,"Furthermore, AdapterBias achieves comparable performance with BitFit and LoRA with fewer parameters needed per task.","Furthermore, AdapterBias achieves comparable performance with BitFit with fewer parameters needed per task.",yes
4754,463-ARR,463-ARR_v2_5@0,463-ARR_v1_5@0,"To tackle these shortcomings, Adapters (Houlsby et al., 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al., 2017) have been proposed.","To tackle these shortcomings, Adapters (Houlsby et al., 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al., 2017) has been proposed.",yes
4755,463-ARR,463-ARR_v2_48@0,463-ARR_v1_47@0,Experiments are conducted to examine whether AdapterBias can be more parameter-efficient by sharing its components across all layers.,Experiments are conducted to see whether Adapter-Bias can be more parameter-efficient by sharing its components across all layers.,yes
4756,463-ARR,463-ARR_v2_5@1,463-ARR_v1_5@1,"Instead of full fine-tuning the whole model, Adapters introduce extra tunable weights and freeze the original parameters of PLM.","Instead of full fine-tuning the whole model, Adapters introduces extra tunable weights and freezes the original parameters of PLM.",yes
4757,463-ARR,463-ARR_v2_54@0,463-ARR_v1_53@0,"In Table 5, we use BERT-base (BB) and BERTlarge (BL) as the PLMs.","In Table 5, we use BERT-base (BB) and BERTlarge (BL) for the PLM.",yes
4758,463-ARR,463-ARR_v2_56@1,463-ARR_v1_55@1,"Compared to the similar work Bit-Fit (Ben Zaken et al., 2021), where the shifts are identical for all tokens, AdapterBias adds tokendependent shifts to the output representation.","Compared to our similar work Bit-Fit (Ben Zaken et al., 2021), where the shifts are identical for all tokens, AdapterBias adds tokendependent shifts to the output representation.",yes
4759,463-ARR,463-ARR_v2_58@4,463-ARR_v1_56@4,"They observe on a syntactic task with BShift (Conneau et al., 2018) that the ninth layer of BERT embeds a rich hierarchy of syntactic information.","Jawahar et al. (2019) also observe on a syntactic task with BShift (Conneau et al., 2018) that the ninth layer of BERT embeds a rich hierarchy of syntactic information.",yes
4760,463-ARR,463-ARR_v2_58@6,463-ARR_v1_56@6,"For instance, RTE (Giampiccolo et al., 2007;Bentivogli et al., 2009) and MNLI (Williams et al., 2017), where both recognize textual entailment, have higher values in the upper layers than the lower ones.","For instance, RTE (Giampiccolo et al., 2007;Bentivogli et al., 2009) and MNLI (Williams et al., 2017), where both tasks recognize textual entailment, have higher values in the upper layers than those in the lower ones.",yes
4761,463-ARR,463-ARR_v2_61@0,463-ARR_v1_59@0,"Since α i represents the weight of the representation shift for i th token in a transformer layer, we can observe the significance of i th token from the summation of α i in all the transformer layers.","Since α i represents the weight of the representation shift for ith token in a transformer layer, we can observe the significance of ith token from the summation of α i in all the transformer layers.",yes
4762,463-ARR,463-ARR_v2_65@2,463-ARR_v1_63@2,"Through extensive experiments, not only does AdapterBias reach competitive results on the GLUE benchmark, but also obtain good performance on small-to-medium datasets.","Through extensive experiments, not only does AdapterBias reaches competitive results on the GLUE benchmark, but it also obtains good performance on small-to-medium datasets.",yes
4763,463-ARR,463-ARR_v2_5@9,463-ARR_v1_6@1,"The results in Houlsby et al. (2019) have shown that the performance on GLUE benchmark (Wang et al., 2018) is almost the same when removing the Adapters in the lower layers, which indicates that not every adapter is useful.","The results in Houlsby et al. (2019) have shown that the performance on GLUE benchmark (Wang et al., 2018) is almost the same when removing the Adapters in the first layers, which indicates that not every adapter is useful.",yes
4764,463-ARR,463-ARR_v2_5@10,463-ARR_v1_6@2,It raises the question of whether adapters can be even more parameter-efficient.,It leaves the question of whether adapters can be even more parameter-efficient.,yes
4765,463-ARR,463-ARR_v2_6@0,463-ARR_v1_7@0,"To develop practical and memory-efficient methods of utilizing PLMs, Diff pruning (Guo et al., 2020) enables parameter-efficient transfer learning that scales well with new tasks.","To develop practical and memory-efficient adapters, Diff pruning (Guo et al., 2020) enables parameter-efficient transfer learning that scales well with new tasks.",yes
4766,463-ARR,463-ARR_v2_6@1,463-ARR_v1_7@1,"The approach learns a task-specific ""diff"" vector that extends the original pre-trained parameters and encourages the sparsity of the vector through L 0 -norm regularization.","The approach learns a taskspecific ""diff"" vector that extends the original pretrained parameters and encourages the sparsity of the vector through L 0 -norm regularization.",yes
4767,463-ARR,463-ARR_v2_7@4,463-ARR_v1_8@4,"Bit-Fit assigns identical shifts to all the tokens, while AdapterBias adds more significant shifts to the representations that are related to the task.","Bit-Fit assigns identical shifts to all the tokens, while AdapterBias adds more significant shifts to the tokens related to the task.",yes
4768,463-ARR,463-ARR_v2_8@0,463-ARR_v1_9@0,"With fewer trainable parameters required, AdapterBias achieves comparable performance on the GLUE benchmark with Houlsby et al. (2019); Pfeiffer et al. (2020a); Guo et al. (2020);Ben Zaken et al. (2021);Hu et al. (2021).","With fewer trainable parameters required, AdapterBias achieves comparable performance on the GLUE benchmark with Houlsby et al. (2019); Pfeiffer et al. (2020a); Guo et al. (2020); Ben Zaken et al. (2021).",yes
4769,463-ARR,463-ARR_v2_13@0,463-ARR_v1_14@0,"Recently, studies start to focus on improving the parameter-efficiency of adaptation to a new task.","Recently, studies start to focus on improving the parameter-efficiency of adapters.",yes
4772,463-ARR,463-ARR_v2_16@2,463-ARR_v1_17@2,AdapterBias produces a suitable weight for the bias based on the input token.,AdapterBias produces a suitable weight of the bias based on the input tokens.,yes
4773,463-ARR,463-ARR_v2_18@3,463-ARR_v1_20@3,The tokens which are more related to the task should be assigned larger representation shifts than other tokens.,"Since some tokens are more important to some tasks, these tokens should be assigned larger representation shifts than other tokens.",yes
,463-ARR,463-ARR_v2_16@5,463-ARR_v1_28@1,"During the training stage, we freeze θ and tune θ only.","In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important.",no
,463-ARR,463-ARR_v2_35@0,463-ARR_v1_28@1,Experimental settings,"In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important.",no
,463-ARR,463-ARR_v2_38@3,463-ARR_v1_40@1,"AdapterBias reaches 81.2 average score in GLUE benchmark, with the smallest amount of parameters (0.17M) added per task.",All results are scored by the GLUE evaluate server.,no
,463-ARR,463-ARR_v2_38@6,463-ARR_v1_28@1,The Full-FT corresponds to fine-tuning the whole PLM without adding adapters.,"In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important.",no
,463-ARR,463-ARR_v2_36@0,463-ARR_v1_63@4,"We base our experiments on HuggingFace PyTorch implementation (Wolf et al., 2019) of BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019c) 2.","Finally, we provide analysis on what AdapterBias learns by comparing α, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information.",no
,463-ARR,463-ARR_v2_53@4,463-ARR_v1_63@5,The settings are the same as in Table 1.,Our study overturns previous architectures of adapters by proposing a simple adapter that can produce suitable representation shifts for different tokens.,no
,463-ARR,463-ARR_v2_38@2,463-ARR_v1_63@4,Here we use BERTlarge as the PLM.,"Finally, we provide analysis on what AdapterBias learns by comparing α, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information.",no
,463-ARR,463-ARR_v2_38@6,463-ARR_v1_63@4,The Full-FT corresponds to fine-tuning the whole PLM without adding adapters.,"Finally, we provide analysis on what AdapterBias learns by comparing α, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information.",no
,463-ARR,463-ARR_v2_36@0,463-ARR_v1_63@3,"We base our experiments on HuggingFace PyTorch implementation (Wolf et al., 2019) of BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019c) 2.","In addition, we demonstrate the robustness of AdapterBias to different PLMs.",no
,463-ARR,463-ARR_v2_53@5,463-ARR_v1_63@3,The Full-FT represents finetuning the whole PLM without adding adapters.,"In addition, we demonstrate the robustness of AdapterBias to different PLMs.",no
,463-ARR,463-ARR_v2_38@6,463-ARR_v1_40@1,The Full-FT corresponds to fine-tuning the whole PLM without adding adapters.,All results are scored by the GLUE evaluate server.,no
,463-ARR,463-ARR_v2_53@4,463-ARR_v1_28@1,The settings are the same as in Table 1.,"In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important.",no
,463-ARR,463-ARR_v2_38@2,463-ARR_v1_40@1,Here we use BERTlarge as the PLM.,All results are scored by the GLUE evaluate server.,no
,463-ARR,463-ARR_v2_16@5,463-ARR_v1_63@4,"During the training stage, we freeze θ and tune θ only.","Finally, we provide analysis on what AdapterBias learns by comparing α, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information.",no
,463-ARR,463-ARR_v2_53@4,463-ARR_v1_63@3,The settings are the same as in Table 1.,"In addition, we demonstrate the robustness of AdapterBias to different PLMs.",no
,463-ARR,463-ARR_v2_36@1,463-ARR_v1_63@3,We experiment with 3 random seeds and choose the seed with the best performance on the validation set to evaluate on the GLUE server.,"In addition, we demonstrate the robustness of AdapterBias to different PLMs.",no
,463-ARR,463-ARR_v2_35@0,463-ARR_v1_63@4,Experimental settings,"Finally, we provide analysis on what AdapterBias learns by comparing α, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information.",no
,463-ARR,463-ARR_v2_35@0,463-ARR_v1_40@1,Experimental settings,All results are scored by the GLUE evaluate server.,no
,463-ARR,463-ARR_v2_38@5,463-ARR_v1_28@1,The settings are the same as in Table 1.,"In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important.",no
,463-ARR,463-ARR_v2_16@5,463-ARR_v1_63@5,"During the training stage, we freeze θ and tune θ only.",Our study overturns previous architectures of adapters by proposing a simple adapter that can produce suitable representation shifts for different tokens.,no
,463-ARR,463-ARR_v2_38@5,463-ARR_v1_63@4,The settings are the same as in Table 1.,"Finally, we provide analysis on what AdapterBias learns by comparing α, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information.",no
,463-ARR,463-ARR_v2_36@1,463-ARR_v1_63@5,We experiment with 3 random seeds and choose the seed with the best performance on the validation set to evaluate on the GLUE server.,Our study overturns previous architectures of adapters by proposing a simple adapter that can produce suitable representation shifts for different tokens.,no
,463-ARR,463-ARR_v2_38@2,463-ARR_v1_63@5,Here we use BERTlarge as the PLM.,Our study overturns previous architectures of adapters by proposing a simple adapter that can produce suitable representation shifts for different tokens.,no
,463-ARR,463-ARR_v2_53@5,463-ARR_v1_63@4,The Full-FT represents finetuning the whole PLM without adding adapters.,"Finally, we provide analysis on what AdapterBias learns by comparing α, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information.",no
,463-ARR,463-ARR_v2_38@1,463-ARR_v1_28@1,"In Table 1, we report the test scores on the GLUE benchmark and the required new parameters per task.","In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important.",no
,463-ARR,463-ARR_v2_53@5,463-ARR_v1_40@1,The Full-FT represents finetuning the whole PLM without adding adapters.,All results are scored by the GLUE evaluate server.,no
4788,465-ARR,465-ARR_v2_31@1,465-ARR_v1_29@1,"In the graph structure, we formulate each node as a MeSH label, and edges represent relationships in the MeSH hierarchy.","In the graph structure, we formulate each node as a MeSH label, and edges are implement MeSH hierarchies.",yes
4789,465-ARR,465-ARR_v2_38@1,465-ARR_v1_36@1,We consider only a subset of the full MeSH list by employing a masked labelwise attention computes the element-wise multiplication of a mask matrix and an attention matrix for two reasons.,We only consider a subset of the full MeSH list and employ a masked label-wise attention that computes the element-wise multiplication of a mask matrix and an attention matrix for the following two reasons.,yes
4790,465-ARR,465-ARR_v2_39@0,465-ARR_v1_37@0,"For each article, selecting a subset of MeSH labels, namely a MeSH mask, downsamples the negative examples, which forces the classifier to concentrate on the candidate labels.","For each article, selecting a subset of MeSH labels, namely a MeSH mask, achieves down-sampling of the negative examples, which forces the classifier to concentrate on the candidate labels.",yes
4791,465-ARR,465-ARR_v2_40@4,465-ARR_v1_38@4,"We build a journal-MeSH label cooccurrence matrix using conditional probabilities, i.e., P (L i | J j ), which denote the probabilities of occurrence of label L i when journal J j appears.","We build a co-occurrence matrix between journals and MeSH labels using conditional probabilities, i.e., P (L i | J j ), which denotes the probability of occurrence of label L i when journal J j appears.",yes
4792,465-ARR,465-ARR_v2_42@1,465-ARR_v1_40@1,"To avoid the noise of rare co-occurrences, a threshold τ filters noisy correlations.","To avoid the noise of rare co-occurrences, we set a threshold τ to filter noisy correlations.",yes
4793,465-ARR,465-ARR_v2_46@1,465-ARR_v1_45@1,"Next, we use KNN based on cosine similarity between abstracts to find the K nearest neighbours for each article in the training set.","Next, we calculate the cosine similarity between abstracts and use KNN to find k nearest neighbours for each article.",yes
4794,465-ARR,465-ARR_v2_46@2,465-ARR_v1_45@2,"To form the unique MeSH mask for article a, we collect MeSH terms M a from the neighbours of a:","After that, we collect MeSH terms from neighbours and form as M n .",yes
4795,465-ARR,465-ARR_v2_4@3,465-ARR_v1_4@3,"Currently, there are 29,369 main MeSH headings, and each MEDLINE citation has 13 MeSH indices, on average.","Currently, there are 29,369 main MeSH headings, and each MEDLINE citation html has 13 MeSH indices, on average.",yes
4796,465-ARR,465-ARR_v2_68@1,465-ARR_v1_64@1,"For pre-processing, we removed nonalphanumeric characters, stop words, punctuation, and single character words, and we converted all words to lowercase.","For pre-processing, we removed nonalphanumeric characters, stop words, punctuation, and single character words, and we converted all words are lowercased.",yes
4799,465-ARR,465-ARR_v2_78@7,465-ARR_v1_75@7,"Although our model is trained only on the abstract and title (which may suggest that it captures less complex semantics), it performs very well against more complex systems.",Although our model is trained only on the abstract and title (which may suggest that it can capture less complex semantics) it performs very well against more complex systems.,yes
4800,465-ARR,465-ARR_v2_8@2,465-ARR_v1_7@2,"This module combines a hybrid of information, at the levels of words and the latent representations of the semantic units, to capture local correlations and longterm dependencies from text.","This module combines a hybrid of information, at the levels of words and latent semantics, to capture local correlations and long-term dependencies from text.",yes
4801,465-ARR,465-ARR_v2_8@3,465-ARR_v1_7@3,2. Our proposed method appears to be the first to employ graph convolutional neural networks that integrate information from the complete MeSH hierarchy to map label representations.,2. Our proposed method appears to be the first to employ graph convolutional neural networks that integrate MeSH hierarchical information to map label representations.,yes
4802,465-ARR,465-ARR_v2_11@8,465-ARR_v1_10@8,The main difference between AttentionMeSH and MeSHProbeNet is that the former uses a label-wise attention mechanism while the latter develops selfattentive MeSH probes to extract comprehensive aspects of information from the input articles.,The main difference between AttentionMeSH and MeSHProbeNet is that the former uses a label-wise attention mechanism while the latter develops selfattentive MeSH probes to extract comprehensive aspects of biomedical information from the input articles.,yes
4803,465-ARR,465-ARR_v2_13@0,465-ARR_v1_12@0,Graph Convolutional Networks in Natural Language Processing,Graph Convolutional Network in Text Classification,yes
4804,465-ARR,465-ARR_v2_14@0,465-ARR_v1_13@0,"Graph convolutional neural networks (GCN)s (Kipf and Welling, 2017) have received considerable attention and achieved remarkable success in natural language processing recently.","Graph convolutional neural networks (GCN)s (Kipf and Welling, 2017) have received considerable attention recently.",yes
4805,465-ARR,465-ARR_v2_14@4,465-ARR_v1_13@4,"Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multi-label text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels, and Rios and Kavuluru (2018) built a multilabel classifier that was learned from a 2-layer GCN over the label hierarchy.","Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multilabel text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels.",yes
,465-ARR,465-ARR_v2_15@0,465-ARR_v1_25@0,GCN also provides a powerful toolkit for embedding the taxonomies into low dimension representations that could be utilized for specific tasks.,"In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_15@1,465-ARR_v1_25@0,"For instance, Pujary et al. (2020) used GCN to learn an undirected graph derived from disease names in the MeSH taxonomy in order to detect and normalize disease mentions in biomedical texts.","In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_29@0,465-ARR_v1_25@0,"MeSH taxonomies are organized in 16 categories, and each is further divided into subcategories.","In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_29@1,465-ARR_v1_25@0,"Within each subcategory, MeSH terms are ordered hierarchically from most general to most specific, up to 13 hierarchical levels.","In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_52@0,465-ARR_v1_25@0,"If the label appears in M , we assign 1, 0 otherwise.","In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_66@1,465-ARR_v1_25@0,"We also download the entire MEDLINE collection based on the PubMed Annual Baseline Repository (as of Dec. 2020) and obtain 31,850,051 citations with titles and abstracts.","In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_66@2,465-ARR_v1_25@0,"In order to reduce bias, we only focus on articles that are annotated by human curators (not annotated by a 'curated' or 'auto' modes in MEDLINE).","In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_66@3,465-ARR_v1_25@0,"We then match PMC articles with the citations in PubMed to PMID and obtain a set of 1,284,308 citations.","In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_66@4,465-ARR_v1_25@0,"Out of these PMC articles, we use the latest 20,000 articles as the test set, the next latest 200,000 articles as the validation data set, and the remaining 1.24M articles as the training set.","In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_66@5,465-ARR_v1_25@0,"In total, 28,415 distinct MeSH terms are covered in the training dataset.","In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_68@7,465-ARR_v1_25@0,"We use FAISS (Johnson et al., 2019) to find similar documents for each citation among the training set, and the whole process takes 10 hours.","In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_68@13,465-ARR_v1_25@0,The detailed hyper-parameter settings are shown in Table 3.,"In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_68@14,465-ARR_v1_25@0,The code for our method is available at https://github.com/xdwang0726/KenMeSH.,"In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.",no
,465-ARR,465-ARR_v2_38@1,465-ARR_v1_13@4,We consider only a subset of the full MeSH list by employing a masked labelwise attention computes the element-wise multiplication of a mask matrix and an attention matrix for two reasons.,"Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multilabel text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels.",no
,465-ARR,465-ARR_v2_40@4,465-ARR_v1_13@4,"We build a journal-MeSH label cooccurrence matrix using conditional probabilities, i.e., P (L i | J j ), which denote the probabilities of occurrence of label L i when journal J j appears.","Other research focused on learning the relationships between nodes in a graph, such as the label co-occurrences for multilabel text classifications; e.g., MAGNET (Pal et al., 2020) built a label graph to capture dependency structures among labels.",no
,465-ARR,465-ARR_v2_78@7,465-ARR_v1_40@1,"Although our model is trained only on the abstract and title (which may suggest that it captures less complex semantics), it performs very well against more complex systems.","To avoid the noise of rare co-occurrences, we set a threshold τ to filter noisy correlations.",no
4814,468-ARR,468-ARR_v2_18@4,468-ARR_v1_18@4,"In this scenario, the bias model's predictions are used to weight the main model's cross-entropy loss.","In this scenario, the bias model's predictions are used to weigh the main model's cross-entropy loss.",yes
4815,468-ARR,468-ARR_v2_22@0,468-ARR_v1_22@0,"We work with the MLQE-PE dataset (Fomicheva et al., 2020) which was specifically designed for the training of MT QE models.",We work with the MLQE-PE dataset which was specifically designed for the training of MT QE models.,yes
4816,468-ARR,468-ARR_v2_22@3,468-ARR_v1_22@3,"A seventh dataset, Russian-English, was curated from Reddit posts and WikiQuotes.","A seventh dataset, Russian-English, was collected based on Reddit posts and WikiQuotes.",yes
4817,468-ARR,468-ARR_v2_27@8,468-ARR_v1_27@8,"The Russian-English dataset is an exception where the source sentence is a good predictor for the translation quality, most likely due to the distinct nature of Reddit data and WikiQuotes (both user-generated).","Perhaps due to the distinct nature of Reddit data and WikiQuotes (both user-generated), the Russian-English dataset is an exception where the source sentence is a good predictor for the translation quality.",yes
4818,468-ARR,468-ARR_v2_28@2,468-ARR_v1_28@2,"To achieve this, one of the authors, a German native speaker, manually annotated translations in the test set that are grammatically correct but do not preserve the meaning of the source.","To achieve this, we manually annotated translations in the test set that are grammatically correct but do not preserve the meaning of the source.",yes
4819,468-ARR,468-ARR_v2_28@4,468-ARR_v1_29@1,"A key takeaway from the labelling process was that it is not only the models that have a partial input bias -human annotators clearly seem to over-rely on the target fluency, too.",A key takeaway from the labelling process was that it is not only the models that have a partial input bias -human annotators clearly seem to over-rely on the target fluency.,yes
4820,468-ARR,468-ARR_v2_28@5,468-ARR_v1_29@2,"Even if the instructions clearly specify that a DA score below 70 should be assigned to inadequate translations, 4 annotators tended to give higher scores if the sentence was fluent and appeared logical.","Despite the instructions clarly specify that a DA score below 70 should be assigned to inadequate translations, 3 annotators tended to give higher scores if the sentence was fluent and appeared logical.",yes
4821,468-ARR,468-ARR_v2_33@8,468-ARR_v1_34@8,"6 In both cases, the main task optimises the MSE loss, and the auxiliary task is a binary classification problem using the binary cross-entropy loss.","5 In both cases, the main task optimises the MSE loss, however, the auxiliary task is a binary classification problem using the binary cross-entropy loss.",yes
4822,468-ARR,468-ARR_v2_35@3,468-ARR_v1_36@3,"The shared layers, on the other hand, are penalised for learning a mapping between target sentence and scores.","The shared layers, however, are penalised for learning a mapping between target sentence and scores.",yes
4823,468-ARR,468-ARR_v2_41@2,468-ARR_v1_42@2,"Since our QE task is formulated as a regression problem, we attempt to find an equivalent strategy to weigh down biased examples when working with MSE loss.","Since our QE task is formulated as a regression problem, we attempt to find an equivalent strategy to down-weigh biased examples when working with MSE loss.",yes
4826,468-ARR,468-ARR_v2_5@3,468-ARR_v1_5@3,"We work with the recently published multilingual QE dataset MLQE-PE (Fomicheva et al., 2020), allowing us to test the generalisability of our approaches across different languages and quality scores.","We work with the recently published multilingual QE dataset MLQE-PE , allowing us to test the generalisability of our approaches across different languages and quality scores.",yes
4829,468-ARR,468-ARR_v2_55@1,468-ARR_v1_56@1,"Considering the experimental design, the multitask architecture provides additional degrees of freedom that were not explored extensively, yet.","Firstly, the multitask architecture provides additional degrees of freedom that were not explored extensively, yet.",yes
4830,468-ARR,468-ARR_v2_55@2,468-ARR_v1_56@2,"For example, one could vary the amount of training per task or learn the training schedule as a parameter which adapts dynamically during the training process (Kiperwasser and Ballesteros, 2018;Zaremoodi et al., 2018).","For example, one could vary the amount of training per task or even learn the training schedule as a parameter which adapts dynamically during the training process (Kiperwasser and Ballesteros, 2018;Zaremoodi et al., 2018).",yes
4831,468-ARR,468-ARR_v2_56@0,468-ARR_v1_56@4,"Going beyond the field of Machine Translation Quality Estimation, it would be interesting to see the methods applied in adjacent areas of NLP.","Furthermore, it would be interesting to apply our proposed methods and architectures in adjacent fields of NLP.",yes
4834,468-ARR,468-ARR_v2_12@3,468-ARR_v1_12@3,"Sentence-level QE has evolved from the first feature-heavy prediction models (Blatz et al., 2004) to neural architectures such as RNNs and Transformers (Vaswani et al., 2017), which accelerated the developments in the field by reducing the work of manual feature engineering and improving contextual representations (Kim et al., 2017;Wang et al., 2018;Fan et al., 2019).","Sentencelevel QE has evolved from the first feature-heavy prediction models in Blatz et al. (2004) to neural architectures such as RNNs and Transformers (Vaswani et al., 2017), which accelerated the developments in the field by reducing the work of manual feature engineering and improving contextual representations (Kim et al., 2017;Wang et al., 2018;Fan et al., 2019).",yes
4835,468-ARR,468-ARR_v2_13@6,468-ARR_v1_13@6,"Following their work, in an attempt to reduce statistical artifacts, MLQE-PE (Fomicheva et al., 2020) -a new QE dataset diversifying the topics and languages covered -was created, which forms the basis of this work and will be described in more detail in Section 3.1.","Following their work, in an attempt to reduce statistical artifacts, MLQE-PE -a new QE dataset diversifying the topics and languages covered -was created, which forms the basis of this work and will be described in more detail in Section 3.1.",yes
4836,468-ARR,468-ARR_v2_18@1,468-ARR_v1_18@1,The notion of focal loss was first introduced by Lin et al. (2017) as a means to improve classification results on imbalanced classes by weighing down the impact of samples that the model had already learned to classify well.,The notion of focal loss was first introduced by Lin et al. (2017) as a means to improve classification results on imbalanced classes by downweighting the impact of samples that the model had already learned to classify well.,yes
,468-ARR,468-ARR_v2_27@9,468-ARR_v1_56@5,This source sentence bias could best be mitigated by curating a new dataset which is why we chose not to focus our efforts on the Russian-English dataset.,We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model.,no
,468-ARR,468-ARR_v2_42@8,468-ARR_v1_56@5,"Column r shows the mean Pearson correlation of labels and predictions and the standard deviation over 5 runs, each training for 3 epochs = 15 minutes.",We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model.,no
,468-ARR,468-ARR_v2_42@9,468-ARR_v1_56@5,Column MSE is the average mean squared error.,We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model.,no
,468-ARR,468-ARR_v2_42@10,468-ARR_v1_56@5,"Column r target measures the performance when testing on the target sentence only and thus approximates the bias mitigation effect, where a smaller correlation is better.",We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model.,no
,468-ARR,468-ARR_v2_55@4,468-ARR_v1_56@5,"To further evaluate the generalisability of the proposed methods, experiments with additional datasets, low-resource language pairs as well as alternative QE architectures and language models could be conducted, too.",We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model.,no
,468-ARR,468-ARR_v2_56@2,468-ARR_v1_56@5,"Other observable biases could also be considered as candidates for the use of targeted bias reduction techniques, provided that it is possible to design a counterbalancing auxiliary task or isolate the bias well enough to deploy adversarial approaches.",We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model.,no
,468-ARR,468-ARR_v2_56@3,468-ARR_v1_56@5,"We think that if the latter scenario applies, the adapted debiased focal loss technique for regression could be worth further exploration, too.",We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model.,no
,468-ARR,468-ARR_v2_22@3,468-ARR_v1_56@4,"A seventh dataset, Russian-English, was curated from Reddit posts and WikiQuotes.","Furthermore, it would be interesting to apply our proposed methods and architectures in adjacent fields of NLP.",no
,468-ARR,468-ARR_v2_55@2,468-ARR_v1_29@1,"For example, one could vary the amount of training per task or learn the training schedule as a parameter which adapts dynamically during the training process (Kiperwasser and Ballesteros, 2018;Zaremoodi et al., 2018).",A key takeaway from the labelling process was that it is not only the models that have a partial input bias -human annotators clearly seem to over-rely on the target fluency.,no
,468-ARR,468-ARR_v2_35@3,468-ARR_v1_18@4,"The shared layers, on the other hand, are penalised for learning a mapping between target sentence and scores.","In this scenario, the bias model's predictions are used to weigh the main model's cross-entropy loss.",no
,468-ARR,468-ARR_v2_18@1,468-ARR_v1_29@1,The notion of focal loss was first introduced by Lin et al. (2017) as a means to improve classification results on imbalanced classes by weighing down the impact of samples that the model had already learned to classify well.,A key takeaway from the labelling process was that it is not only the models that have a partial input bias -human annotators clearly seem to over-rely on the target fluency.,no
,468-ARR,468-ARR_v2_28@5,468-ARR_v1_22@0,"Even if the instructions clearly specify that a DA score below 70 should be assigned to inadequate translations, 4 annotators tended to give higher scores if the sentence was fluent and appeared logical.",We work with the MLQE-PE dataset which was specifically designed for the training of MT QE models.,no
,468-ARR,468-ARR_v2_28@2,468-ARR_v1_42@2,"To achieve this, one of the authors, a German native speaker, manually annotated translations in the test set that are grammatically correct but do not preserve the meaning of the source.","Since our QE task is formulated as a regression problem, we attempt to find an equivalent strategy to down-weigh biased examples when working with MSE loss.",no
,468-ARR,468-ARR_v2_55@1,468-ARR_v1_22@3,"Considering the experimental design, the multitask architecture provides additional degrees of freedom that were not explored extensively, yet.","A seventh dataset, Russian-English, was collected based on Reddit posts and WikiQuotes.",no
,468-ARR,468-ARR_v2_41@2,468-ARR_v1_5@3,"Since our QE task is formulated as a regression problem, we attempt to find an equivalent strategy to weigh down biased examples when working with MSE loss.","We work with the recently published multilingual QE dataset MLQE-PE , allowing us to test the generalisability of our approaches across different languages and quality scores.",no
,468-ARR,468-ARR_v2_18@1,468-ARR_v1_56@1,The notion of focal loss was first introduced by Lin et al. (2017) as a means to improve classification results on imbalanced classes by weighing down the impact of samples that the model had already learned to classify well.,"Firstly, the multitask architecture provides additional degrees of freedom that were not explored extensively, yet.",no
,468-ARR,468-ARR_v2_22@0,468-ARR_v1_13@6,"We work with the MLQE-PE dataset (Fomicheva et al., 2020) which was specifically designed for the training of MT QE models.","Following their work, in an attempt to reduce statistical artifacts, MLQE-PE -a new QE dataset diversifying the topics and languages covered -was created, which forms the basis of this work and will be described in more detail in Section 3.1.",no
4967,473-ARR,473-ARR_v2_26@0,473-ARR_v1_27@0,"The SDG task is to generate a simple definition d sim for a given word and context (w * , c), where c = [w 1 , . . . , w * , . . . , w n ] is a sentence containing w * .","The SDG task consists in generating a simple definition d sim for a given word and context (w * , c), where c = [w 1 , . . . , w * , . . . , w n ] is a sentence containing w * .",yes
4968,473-ARR,473-ARR_v2_26@1,473-ARR_v1_27@1,"This task is challenging because there is no corpus like {(w * i , c i , d sim i )} N i=1 and hence it is fully unsupervised.","This task is challenging because there is no corpus like {(w * i , c i , d sim i )} N i=1 and hence fully unsupervised.",yes
4969,473-ARR,473-ARR_v2_2@8,473-ARR_v1_2@8,"Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87% 1 .","Our method outperforms the baseline model by a 1.6 SARI score on the English dataset, and the low level (HSK level 1-3) words in Chinese definitions raised by 5.03%.",yes
4972,473-ARR,473-ARR_v2_33@2,473-ARR_v1_34@2,"Instead, we introduce the language modeling task (Section 3.2.3) to enhance the reconstruction decoder and make it more focused on simple text generation.","Instead, we introduce the language modeling task (Section 3.2.3) to enhance the reconstruction decoder and make it more focusd on simple text generation.",yes
4973,473-ARR,473-ARR_v2_37@1,473-ARR_v1_38@1,The model is optimized using the following loss function.,The model is optimized using the following loss function:,yes
4974,473-ARR,473-ARR_v2_54@0,473-ARR_v1_56@0,"For parameters in the decoders, we divided them into two parts, which are complexity-independent and complexity-dependent parameters.","For parameters in the decoders, we dived them into two parts, which are complexity-independent and complexity-dependent parameters.",yes
4975,473-ARR,473-ARR_v2_4@1,473-ARR_v1_4@1,"In recent years, researchers attempted to automatically generate definitions for words rather than formulating predefined worddefinition inventories (Ishiwatari et al., 2019;Huang et al., 2021).","In recent years, researchers attempted to automatically generate definitions for words rather than formulating predefined worddefinition inventories (Ishiwatari et al., 2019;Yang et al., 2020;Huang et al., 2021).",yes
4976,473-ARR,473-ARR_v2_59@0,473-ARR_v1_61@0,We believe that the required information may vary for different complexity.,We believe that the required information may be various for different complexity.,yes
4977,473-ARR,473-ARR_v2_67@2,473-ARR_v1_69@2,We list more statistics in Table 2.,We list more detailed statistics in Table 2.,yes
4978,473-ARR,473-ARR_v2_75@5,473-ARR_v1_78@5,"Note that we set the learning rate to 3e-4, warmup steps to 500 when fine-tuning both MASS and MASS-ZH.","Note that we set the learning rate to 3e-4, warmup steps to 500, and random seed to 1111 when fine-tuning both MASS and MASS-ZH.",yes
4979,473-ARR,473-ARR_v2_76@3,473-ARR_v1_79@3,We tune the λ parameters in Eq. 7 on the validation set and adopt the same hyper-parameters as the baseline for comparison.,We adopt the same hyper-parameters as the baseline for comparison.,yes
4980,473-ARR,473-ARR_v2_87@2,473-ARR_v1_87@4,We also observe that complex definition generation also improves by 0.17 on BLEU and 1.09 on SemSim.,We also observe that complex definition generation also slightly improves by 0.31 on BLEU and 0.82 on SemSim.,yes
4981,473-ARR,473-ARR_v2_87@3,473-ARR_v1_87@5,This shows that SimpDefiner improves the ability to generate both complex and simple definitions.,This indicates that SimpDefiner improves the ability to generate both complex and simple definitions.,yes
4982,473-ARR,473-ARR_v2_5@3,473-ARR_v1_6@0,"Different from previous work (Noraset et al., 2017;Gadetsky et al., 2018;Mickus et al., 2019; that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).","Different from previous work (Noraset et al., 2017;Gadetsky et al., 2018;Mickus et al., 2019;Kong et al., 2020) that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).",yes
4983,473-ARR,473-ARR_v2_88@1,473-ARR_v1_88@1,"We can see that the proportion of low-level (HSK level 1-3) words increases by 3.87%, and that of high-level (HSK level 7+) words decreases by 0.46%.","We can see that the proportion of low-level (HSK level 1-3) words increases by 5.03%, and that of high-level (HSK level 7+) words decreases by 1.61%.",yes
4984,473-ARR,473-ARR_v2_91@2,473-ARR_v1_91@2,"For the layer normalization (LN) and query projection (QP) as parameter-shared layers, we ablate them by sharing their parameters between models.","For the layer normalization (LN) and query projection (QP) as parameter-shared layers, we ablate them by share their parameters between models.",yes
4985,473-ARR,473-ARR_v2_5@4,473-ARR_v1_6@1,"Making the definitions easier to read and understand could benefit the language learners, low literacy readers, as well as helping people with aphasia or dyslexia.","Make the definitions easier to read and understand could benefit the language learners, low literacy readers, as well as helping people with aphasia or dyslexia.",yes
4986,473-ARR,473-ARR_v2_5@5,473-ARR_v1_6@2,"For example, compared with the Oxford Dictionary (OD), the Oxford Advanced Learner's Dictionary (OALD) has simpler definitions, which are specifically designed for language learners.","For example, compared with the Oxford Dictionary (OD), the Oxford Advanced Learner's Dictionary (OALD) has simpler definitions, which is specifically designed for language learners.",yes
4987,473-ARR,473-ARR_v2_2@2,473-ARR_v1_2@2,We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers.,We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers better.,yes
4988,473-ARR,473-ARR_v2_14@1,473-ARR_v1_15@1,"Gadetsky et al. (2018) tackled this problem by computing the AdaGram vectors (Bartunov et al., 2016) of input words, which are capable of learning different representations at desired semantic resolutions.","Gadetsky et al. (2018) tackled this problem by computing the AdaGram vectors (Bartunov et al., 2016) of input words, which is capable to learn different representations at desired semantic resolution.",yes
4989,473-ARR,473-ARR_v2_14@4,473-ARR_v1_15@4,"Reid et al. (2020) initialized encoders with BERT (Devlin et al., 2019) and employed variational inference for estimation and leveraged contextualized word embeddings for improved performance.","Reid et al. (2020) initialized encoders with BERT (Devlin et al., 2019) and employed variational inference for estimation and leverage contextualized word embeddings for improved performance.",yes
4990,473-ARR,473-ARR_v2_14@5,473-ARR_v1_15@5,Bevilacqua et al. (2020) employed a novel spanbased encoding scheme to fine-tune a pre-trained English encoder-decoder system to generate definitions.,Bevilacqua et al. (2020) employed a novel span-based encoding scheme to fine-tune a pre-trained English encoderdecoder system to generate definitions.,yes
4991,473-ARR,473-ARR_v2_15@2,473-ARR_v1_16@2,"Besides, our model is based on MASS (Song et al., 2019), which is a pre-trained encoder-decoder model and is suitable for generation tasks.","Besides, our model is based on MASS (Song et al., 2019), which is a pre-trained encoder-decoder model and is more suitable for generation tasks.",yes
4992,473-ARR,473-ARR_v2_2@5,473-ARR_v1_2@5,We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders.,We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between the components.,yes
4993,473-ARR,473-ARR_v2_18@3,473-ARR_v1_19@3,"Martin et al. (2019) proposed to prepend additional prompt tokens to source sentences at train time, which enables the end-users to condition the simplifications returned by the model on attributes like length, lexical complexity, and syntactic complexity.","Martin et al. (2019) proposed to prepend additional prompt tokens to source sentences at train time, which enable the end-users to condition the simplifications returned by the model on attributes like length, lexical complexity, and syntactic complexity.",yes
4994,473-ARR,473-ARR_v2_2@6,473-ARR_v1_2@6,"By jointly training these components, the framework can generate both complex and simple definitions simultaneously.","By joint training these components, the framework can generate both complex and simple definitions simultaneously.",yes
4995,473-ARR,473-ARR_v2_19@1,473-ARR_v1_20@1,"Unlike the baseline, the SimpDefiner can generate simple definitions directly, alleviating the accumulated errors.","Unlike the baseline, the SimpDefiner can simultaneously generate complex and simple definitions without the need for aligned complex-simple sentence pairs.",yes
,473-ARR,473-ARR_v2_33@3,473-ARR_v1_72@3,The experiment results in Section 6 confirm our assumption.,"The training set contains 6,574 words and 67,861 entries.",no
,473-ARR,473-ARR_v2_33@3,473-ARR_v1_72@4,The experiment results in Section 6 confirm our assumption.,Statistics are listed in Table 1.,no
,473-ARR,473-ARR_v2_33@3,473-ARR_v1_81@2,The experiment results in Section 6 confirm our assumption.,"Specifically, we use the BLEU (Papineni et al., 2002) and Semantic Similarity metrics to evaluate the accuracy, and use the SARI (Xu et al., 2016), and HSK Level metrics to evaluate the simplicity.",no
,473-ARR,473-ARR_v2_76@4,473-ARR_v1_72@3,We set 5 different random seeds as and report the average result of multiple runs.,"The training set contains 6,574 words and 67,861 entries.",no
,473-ARR,473-ARR_v2_76@4,473-ARR_v1_72@4,We set 5 different random seeds as and report the average result of multiple runs.,Statistics are listed in Table 1.,no
,473-ARR,473-ARR_v2_76@4,473-ARR_v1_81@2,We set 5 different random seeds as and report the average result of multiple runs.,"Specifically, we use the BLEU (Papineni et al., 2002) and Semantic Similarity metrics to evaluate the accuracy, and use the SARI (Xu et al., 2016), and HSK Level metrics to evaluate the simplicity.",no
,473-ARR,473-ARR_v2_76@5,473-ARR_v1_72@3,Each run takes 7.68 GPU hours on 4 GeFource RTX 2080 Ti GPUs.,"The training set contains 6,574 words and 67,861 entries.",no
,473-ARR,473-ARR_v2_76@5,473-ARR_v1_72@4,Each run takes 7.68 GPU hours on 4 GeFource RTX 2080 Ti GPUs.,Statistics are listed in Table 1.,no
,473-ARR,473-ARR_v2_76@5,473-ARR_v1_81@2,Each run takes 7.68 GPU hours on 4 GeFource RTX 2080 Ti GPUs.,"Specifically, we use the BLEU (Papineni et al., 2002) and Semantic Similarity metrics to evaluate the accuracy, and use the SARI (Xu et al., 2016), and HSK Level metrics to evaluate the simplicity.",no
,473-ARR,473-ARR_v2_97@0,473-ARR_v1_72@3,Table 8 shows two generation cases from English and Chinese test set respectively.,"The training set contains 6,574 words and 67,861 entries.",no
,473-ARR,473-ARR_v2_97@0,473-ARR_v1_72@4,Table 8 shows two generation cases from English and Chinese test set respectively.,Statistics are listed in Table 1.,no
,473-ARR,473-ARR_v2_97@0,473-ARR_v1_81@2,Table 8 shows two generation cases from English and Chinese test set respectively.,"Specifically, we use the BLEU (Papineni et al., 2002) and Semantic Similarity metrics to evaluate the accuracy, and use the SARI (Xu et al., 2016), and HSK Level metrics to evaluate the simplicity.",no
,473-ARR,473-ARR_v2_97@1,473-ARR_v1_72@3,"In both cases, the golden definition is a long sentence with quite complicated syntax.","The training set contains 6,574 words and 67,861 entries.",no
,473-ARR,473-ARR_v2_97@1,473-ARR_v1_72@4,"In both cases, the golden definition is a long sentence with quite complicated syntax.",Statistics are listed in Table 1.,no
,473-ARR,473-ARR_v2_97@1,473-ARR_v1_81@2,"In both cases, the golden definition is a long sentence with quite complicated syntax.","Specifically, we use the BLEU (Papineni et al., 2002) and Semantic Similarity metrics to evaluate the accuracy, and use the SARI (Xu et al., 2016), and HSK Level metrics to evaluate the simplicity.",no
,473-ARR,473-ARR_v2_2@5,473-ARR_v1_78@5,We disentangle the complexity factors from the text by carefully designing a parameter sharing scheme between two decoders.,"Note that we set the learning rate to 3e-4, warmup steps to 500, and random seed to 1111 when fine-tuning both MASS and MASS-ZH.",no
,473-ARR,473-ARR_v2_87@3,473-ARR_v1_88@1,This shows that SimpDefiner improves the ability to generate both complex and simple definitions.,"We can see that the proportion of low-level (HSK level 1-3) words increases by 5.03%, and that of high-level (HSK level 7+) words decreases by 1.61%.",no
,473-ARR,473-ARR_v2_87@2,473-ARR_v1_6@0,We also observe that complex definition generation also improves by 0.17 on BLEU and 1.09 on SemSim.,"Different from previous work (Noraset et al., 2017;Gadetsky et al., 2018;Mickus et al., 2019;Kong et al., 2020) that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).",no
,473-ARR,473-ARR_v2_75@5,473-ARR_v1_61@0,"Note that we set the learning rate to 3e-4, warmup steps to 500 when fine-tuning both MASS and MASS-ZH.",We believe that the required information may be various for different complexity.,no
,473-ARR,473-ARR_v2_91@2,473-ARR_v1_15@4,"For the layer normalization (LN) and query projection (QP) as parameter-shared layers, we ablate them by sharing their parameters between models.","Reid et al. (2020) initialized encoders with BERT (Devlin et al., 2019) and employed variational inference for estimation and leverage contextualized word embeddings for improved performance.",no
,473-ARR,473-ARR_v2_75@5,473-ARR_v1_16@2,"Note that we set the learning rate to 3e-4, warmup steps to 500 when fine-tuning both MASS and MASS-ZH.","Besides, our model is based on MASS (Song et al., 2019), which is a pre-trained encoder-decoder model and is more suitable for generation tasks.",no
,473-ARR,473-ARR_v2_54@0,473-ARR_v1_2@8,"For parameters in the decoders, we divided them into two parts, which are complexity-independent and complexity-dependent parameters.","Our method outperforms the baseline model by a 1.6 SARI score on the English dataset, and the low level (HSK level 1-3) words in Chinese definitions raised by 5.03%.",no
,473-ARR,473-ARR_v2_2@2,473-ARR_v1_15@1,We propose a novel task of Simple Definition Generation (SDG) to help language learners and low literacy readers.,"Gadetsky et al. (2018) tackled this problem by computing the AdaGram vectors (Bartunov et al., 2016) of input words, which is capable to learn different representations at desired semantic resolution.",no
,473-ARR,473-ARR_v2_2@8,473-ARR_v1_78@5,"Our method outperforms the baseline model by a 1.77 SARI score on the English dataset, and raises the proportion of the low level (HSK level 1-3) words in Chinese definitions by 3.87% 1 .","Note that we set the learning rate to 3e-4, warmup steps to 500, and random seed to 1111 when fine-tuning both MASS and MASS-ZH.",no
,473-ARR,473-ARR_v2_91@2,473-ARR_v1_20@1,"For the layer normalization (LN) and query projection (QP) as parameter-shared layers, we ablate them by sharing their parameters between models.","Unlike the baseline, the SimpDefiner can simultaneously generate complex and simple definitions without the need for aligned complex-simple sentence pairs.",no
,473-ARR,473-ARR_v2_15@2,473-ARR_v1_38@1,"Besides, our model is based on MASS (Song et al., 2019), which is a pre-trained encoder-decoder model and is suitable for generation tasks.",The model is optimized using the following loss function:,no
,473-ARR,473-ARR_v2_59@0,473-ARR_v1_6@0,We believe that the required information may vary for different complexity.,"Different from previous work (Noraset et al., 2017;Gadetsky et al., 2018;Mickus et al., 2019;Kong et al., 2020) that focused only on how to generate definitions, we further propose a novel task of Simple Definition Generation (SDG).",no
5502,61-ARR,61-ARR_v2_26@4,61-ARR_v1_22@4,"In summary, relative slot accuracy enables relative comparison according to the distribution of the domain in a dialogue.","In summary, relative slot accuracy enables relative comparison according to the distribution of the domain in a dialog.",yes
5503,61-ARR,61-ARR_v2_26@5,61-ARR_v1_23@0,"Dependency on Predefined Slots As discussed in Section 2.2, slot accuracy requiring total predefined slots is not a scalable method for evaluating the current dialogue dataset that contains a few domains in each dialogue.","Dependency on predefined slots As discussed in Section 2.2, slot accuracy requiring total predefined slots is not a scalable method for evaluating the current dialog dataset that contain a few domains in each dialog.",yes
5504,61-ARR,61-ARR_v2_26@6,61-ARR_v1_23@1,"For example, when evaluating a dialogue sample that solely deals with the restaurant domain, even domains that never appear at all (i.e., hotel, train, attraction, and taxi) are involved in measuring performance, making deviations among different models trivial.","For example, when evaluating a dialog sample that solely deals with the restaurant domain, even domains that never appear at all (i.e., hotel, train, attraction, and taxi) are involved in measuring performance, making deviations among different models trivial.",yes
5505,61-ARR,61-ARR_v2_26@7,61-ARR_v1_23@2,"However, relative slot accuracy can evaluate the model's predictive score without being affected by slots never seen in the current dialogue, which is a more realistic way, considering that each dialogue contains its own turn and slot composition.","However, relative slot accuracy can evaluate the model's predictive score without being affected by slots never seen in the current dialog, which is a more realistic way, considering that each dialog contains its own turn and slot composition.",yes
5506,61-ARR,61-ARR_v2_29@2,61-ARR_v1_26@2,"This metric is not affected by unseen slots in the current dialogue situation, and compensates for the model's correct predic-tion.","This metric is not affected by unseen slots in the current dialog situation, and compensates for the model's correct prediction.",yes
5507,61-ARR,61-ARR_v2_30@0,61-ARR_v1_27@0,"Our findings show that if the model makes an incorrect prediction, the error accumulates until the end of the dialogue, and the joint goal accuracy remains at zero.","Our findings show that if the model makes an incorrect prediction, the error accumulates until the end of the dialog, and the joint goal accuracy remains at zero.",yes
5508,61-ARR,61-ARR_v2_30@1,61-ARR_v1_27@1,"In this section, we discuss a few cases of 59 dialogues that do not show the trend among 642 dialogues selected in Section 2.1; however, it is important to note that these few cases have negligible effect on the trend in Figure 1, solely changing the position where the joint goal accuracy first becomes zero.","In this section, we discuss a few cases of 59 dialogs that do not show the trend among 642 dialogs selected in Section 2.1; however, it is important to note that these few cases have negligible effect on the trend in Figure 1, solely changing the position where the joint goal accuracy first becomes zero.",yes
5509,61-ARR,61-ARR_v2_31@0,61-ARR_v1_27@2,"We sampled dialogues of the MultiWOZ 2.1 test set in Table A1 and Table A2, and marked values appearing in the dialogue in bold.","We sampled dialogs of the MultiWOZ 2.1 test set in Table A1 and Table A2, and marked values appearing in the dialog in bold.",yes
5510,61-ARR,61-ARR_v2_31@2,61-ARR_v1_27@4,"In the first dialogue presented in Table A1, the joint goal accuracy is measured as 1 at turn 2.","In the first dialog presented in Table A1, the joint goal accuracy is measured as 1 at turn 2.",yes
5511,61-ARR,61-ARR_v2_32@0,61-ARR_v1_28@0,"The second dialogue presented in Table A2, reports the incorrect prediction according to the interpretation of annotations at turn 4.","The second dialog presented in Table A2, reports the incorrect prediction according to the interpretation of annotations at turn 4.",yes
5512,61-ARR,61-ARR_v2_32@1,61-ARR_v1_28@1,"In other words, because the dialogue about the hotel-internet slot appears over turns 4 and 5, it is solely an error depending on the prediction timing of the model.","In other words, because the dialog about the hotel-internet slot appears over turns 4 and 5, it is solely an error depending on the prediction timing of the model.",yes
5513,61-ARR,61-ARR_v2_33@1,61-ARR_v1_29@1,"Furthermore, the fact that the starting point of making the joint goal accuracy of subsequent turns to 0 mainly occurs at the beginning of the dialogue does not change.","Furthermore, the fact that the starting point of making the joint goal accuracy of subsequent turns to 0 mainly occurs at the beginning of the dialog does not change.",yes
5514,61-ARR,61-ARR_v2_2@0,61-ARR_v1_2@0,Dialogue state tracking (DST) aims to extract essential information from multi-turn dialogue situations and take appropriate actions.,Dialogue state tracking (DST) aims to extract essential information from multi-turn dialog situations and take appropriate actions.,yes
5515,61-ARR,61-ARR_v2_6@1,61-ARR_v1_6@1,"To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ dataset.","To address the above challenge, we propose reporting the relative slot accuracy along with the existing metrics in MultiWOZ.",yes
5516,61-ARR,61-ARR_v2_6@2,61-ARR_v1_6@2,"While slot accuracy has the challenge of overestimation by always considering all predefined slots in every turn, relative slot accuracy does not depend on predefined slots, and calculates a score that is affected solely by slots that appear in the current dialogue.","Because slot accuracy has the challenge of overestimation by always considering all predefined slots in every turn, relative slot accuracy does not depend on predefined slots, and calculates a score that is affected solely by slots that appear in the current dialog.",yes
5517,61-ARR,61-ARR_v2_9@3,61-ARR_v1_9@3,"As illustrated in Figure 1, we measured the relative position of the turn causing this phenomenon for the dialogue.","As illustrated in Figure 1, we measured the relative position of the turn causing this phenomenon for the dialog.",yes
5518,61-ARR,61-ARR_v2_9@6,61-ARR_v1_9@6,"Accordingly, the relative position where joint goal accuracy first became zero was mainly at the beginning of the dialogue 1 .","Accordingly, the relative position where joint goal accuracy first became zero was mainly at the beginning of the dialog 1 .",yes
5519,61-ARR,61-ARR_v2_9@7,61-ARR_v1_9@7,"This means that the joint goal accuracy after the beginning of the dialogue is unconditionally measured as zero because of the initial misprediction, although the model may correctly predict new belief states at later turns.","This means that the joint goal accuracy after the beginning of the dialog is unconditionally measured as zero because of the initial misprediction, although the model may correctly predict new belief states at later turns.",yes
5520,61-ARR,61-ARR_v2_9@8,61-ARR_v1_9@8,"Failure to measure the performance of the latter part means that it cannot consider various dialogue situations provided in the dataset, which is a critical issue in building a realistic DST model.","Failure to measure the performance of the latter part means that it cannot consider various dialog situations provided in the dataset, which is a critical issue in building a realistic DST model.",yes
5521,61-ARR,61-ARR_v2_11@0,61-ARR_v1_11@0,Slot accuracy can compensate for situations where joint goal accuracy does not fully evaluate the dialogue situation.,Slot accuracy can compensate for situations where joint goal accuracy does not fully evaluate the dialog situation.,yes
5522,61-ARR,61-ARR_v2_13@1,61-ARR_v1_13@1,"Each value of x-axis in Figure 2 indicates the ""maximum"" number of slots that appear in a single dialogue, and we confirmed that approximately 85% of the test set utilized solely less than 12 of the 30 predefined slots in the experiment.","Each value of x-axis in Figure 2 indicates the ""maximum"" number of slots that appear in a single dialog, and we confirmed that approximately 85% of the test set utilized solely less than 12 of the 30 predefined slots in the experiment.",yes
5523,61-ARR,61-ARR_v2_13@2,61-ARR_v1_13@2,"Because the number of belief states appearing in the early and middle turns of the dialogue are smaller, and even fewer states make false predictions, calculating slot accuracy using Equation 2 reduces the influence of M and W , and the final score is dominated by the total slot number T .","Because the number of belief states appearing in the early and middle turns of the dialog are smaller, and even fewer states make false predictions, calculating slot accuracy using Equation 2 reduces the influence of M and W , and the final score is dominated by the total slot number T .",yes
5524,61-ARR,61-ARR_v2_18@1,61-ARR_v1_16@1,"The deviation among DST models will be even more minor when constructing datasets with various dialogue situations, because the number of predefined slots will continually in-crease.","The deviation among DST models will be even more minor when constructing datasets with various dialog situations, because the number of predefined slots will continually increase.",yes
5525,61-ARR,61-ARR_v2_23@0,61-ARR_v1_20@0,"We measured MultiWOZ 2.1, an improved version of MultiWOZ 2.0 , which has been adopted in several studies, according to Table A5.","We measured MultiWOZ 2.1, an improved version of MultiWOZ 2.0 (Budzianowski et al., 2018), which has been adopted in several studies, according to Table A5.",yes
5526,61-ARR,61-ARR_v2_2@4,61-ARR_v1_2@4,"Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialogue.","Relative slot accuracy does not depend on the number of predefined slots, and allows intuitive evaluation by assigning relative scores according to the turn of each dialog.",yes
,61-ARR,61-ARR_v2_25@3,61-ARR_v1_24@4,"Meanwhile, relative slot accuracy can explicitly highlight the deviation among models by showing a 5.47% difference between the largest and smallest values.","Therefore, relative slot accuracy can provide an intuitive evaluation reflecting the current belief state recording method, in which the number of slots accumulates incrementally as the conversation progresses.",no
,61-ARR,61-ARR_v2_16@2,61-ARR_v1_24@2,"Since average goal accuracy ignores the predicted states, it cannot properly distinguish a better model from a worse model in some specific situations.",The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3.,no
,61-ARR,61-ARR_v2_25@1,61-ARR_v1_24@3,"Regarding slot accuracy, the difference between the largest and smallest values is solely 1.09%.","In addition, regarding slot accuracy in turns 4, 5, and 6, there is no score improvement for the additional wellpredicted state by the model, whereas the score increases when the newly added state is matched in the case of relative slot accuracy.",no
,61-ARR,61-ARR_v2_25@2,61-ARR_v1_24@3,It can be one of the reasons that several researchers do not report it.,"In addition, regarding slot accuracy in turns 4, 5, and 6, there is no score improvement for the additional wellpredicted state by the model, whereas the score increases when the newly added state is matched in the case of relative slot accuracy.",no
,61-ARR,61-ARR_v2_16@3,61-ARR_v1_24@1,We will discuss it in more detail in Section 4.1.,Table A6 compares the slot and relative slot accuracies.,no
,61-ARR,61-ARR_v2_25@2,61-ARR_v1_24@4,It can be one of the reasons that several researchers do not report it.,"Therefore, relative slot accuracy can provide an intuitive evaluation reflecting the current belief state recording method, in which the number of slots accumulates incrementally as the conversation progresses.",no
,61-ARR,61-ARR_v2_25@0,61-ARR_v1_24@2,Table 1 presents the overall results.,The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3.,no
,61-ARR,61-ARR_v2_16@2,61-ARR_v1_24@1,"Since average goal accuracy ignores the predicted states, it cannot properly distinguish a better model from a worse model in some specific situations.",Table A6 compares the slot and relative slot accuracies.,no
,61-ARR,61-ARR_v2_16@1,61-ARR_v1_24@1,"The main difference between the average goal accuracy and the proposed relative slot accuracy is that the average goal accuracy only considers the slots with non-empty values in the gold states of each turn, whereas the proposed relative slot accuracy considers those in both gold and predicted states.",Table A6 compares the slot and relative slot accuracies.,no
,61-ARR,61-ARR_v2_16@3,61-ARR_v1_24@2,We will discuss it in more detail in Section 4.1.,The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3.,no
,61-ARR,61-ARR_v2_16@1,61-ARR_v1_24@2,"The main difference between the average goal accuracy and the proposed relative slot accuracy is that the average goal accuracy only considers the slots with non-empty values in the gold states of each turn, whereas the proposed relative slot accuracy considers those in both gold and predicted states.",The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3.,no
,61-ARR,61-ARR_v2_25@3,61-ARR_v1_24@3,"Meanwhile, relative slot accuracy can explicitly highlight the deviation among models by showing a 5.47% difference between the largest and smallest values.","In addition, regarding slot accuracy in turns 4, 5, and 6, there is no score improvement for the additional wellpredicted state by the model, whereas the score increases when the newly added state is matched in the case of relative slot accuracy.",no
,61-ARR,61-ARR_v2_15@0,61-ARR_v1_24@1,Other Metric,Table A6 compares the slot and relative slot accuracies.,no
,61-ARR,61-ARR_v2_16@0,61-ARR_v1_24@1,"Recently, Rastogi et al. (2020b) proposed a metric called average goal accuracy.",Table A6 compares the slot and relative slot accuracies.,no
,61-ARR,61-ARR_v2_25@2,61-ARR_v1_24@2,It can be one of the reasons that several researchers do not report it.,The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3.,no
,61-ARR,61-ARR_v2_16@2,61-ARR_v1_24@4,"Since average goal accuracy ignores the predicted states, it cannot properly distinguish a better model from a worse model in some specific situations.","Therefore, relative slot accuracy can provide an intuitive evaluation reflecting the current belief state recording method, in which the number of slots accumulates incrementally as the conversation progresses.",no
,61-ARR,61-ARR_v2_16@3,61-ARR_v1_24@4,We will discuss it in more detail in Section 4.1.,"Therefore, relative slot accuracy can provide an intuitive evaluation reflecting the current belief state recording method, in which the number of slots accumulates incrementally as the conversation progresses.",no
,61-ARR,61-ARR_v2_16@0,61-ARR_v1_24@2,"Recently, Rastogi et al. (2020b) proposed a metric called average goal accuracy.",The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3.,no
,61-ARR,61-ARR_v2_16@1,61-ARR_v1_24@4,"The main difference between the average goal accuracy and the proposed relative slot accuracy is that the average goal accuracy only considers the slots with non-empty values in the gold states of each turn, whereas the proposed relative slot accuracy considers those in both gold and predicted states.","Therefore, relative slot accuracy can provide an intuitive evaluation reflecting the current belief state recording method, in which the number of slots accumulates incrementally as the conversation progresses.",no
,61-ARR,61-ARR_v2_15@0,61-ARR_v1_24@2,Other Metric,The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3.,no
,61-ARR,61-ARR_v2_25@1,61-ARR_v1_24@1,"Regarding slot accuracy, the difference between the largest and smallest values is solely 1.09%.",Table A6 compares the slot and relative slot accuracies.,no
,61-ARR,61-ARR_v2_15@0,61-ARR_v1_24@3,Other Metric,"In addition, regarding slot accuracy in turns 4, 5, and 6, there is no score improvement for the additional wellpredicted state by the model, whereas the score increases when the newly added state is matched in the case of relative slot accuracy.",no
,61-ARR,61-ARR_v2_25@3,61-ARR_v1_24@2,"Meanwhile, relative slot accuracy can explicitly highlight the deviation among models by showing a 5.47% difference between the largest and smallest values.",The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3.,no
,61-ARR,61-ARR_v2_15@0,61-ARR_v1_24@4,Other Metric,"Therefore, relative slot accuracy can provide an intuitive evaluation reflecting the current belief state recording method, in which the number of slots accumulates incrementally as the conversation progresses.",no
,61-ARR,61-ARR_v2_25@3,61-ARR_v1_24@1,"Meanwhile, relative slot accuracy can explicitly highlight the deviation among models by showing a 5.47% difference between the largest and smallest values.",Table A6 compares the slot and relative slot accuracies.,no
5623,68-ARR,68-ARR_v2_20@2,68-ARR_v1_20@2,"Once S 2 DM is trained, only the output of source language MLP network is fed into the linear output layer for MRC.","Once S 2 DM is trained, only the output of source language MLP network input to linear output layer for MRC.",yes
5624,68-ARR,68-ARR_v2_25@0,68-ARR_v1_25@0,"The variational inference process of VG-VAE uses a factorized approximated posterior q ϕ (y|x)q ϕ (z|x) = q ϕ (y, z|x) with the objective function that maximizes a lower bound of the marginal log-likelihood:","The variational inference process of VG-VAE uses a factorized approximated posterior q φ (y|x)q φ (z|x) = q φ (y, z|x) with the objective function that maximizes a lower bound of the marginal log-likelihood:",yes
5625,68-ARR,68-ARR_v2_30@3,68-ARR_v1_28@4,"As pre-trained representations are contextually-encoded token vectors, latent variable vectors obtained by sampling from the distributions need to be averaged so as to output sentence-level semantic and syntactic vector.","The pretrained representations of sentence are contextuallyencoded token vectors, so the latent variable vectors obtained by sampling from the distributions need to be averaged, then output sentence-level semantic vector and syntactic vector.",yes
5626,68-ARR,68-ARR_v2_4@0,68-ARR_v1_4@0,"Multilingual pre-trained language models (PLMs) (Devlin et al., 2019;Conneau and Lample, 2019;Conneau et al., 2020) have been widely explored in cross-lingual understanding tasks.","Multilingual pre-trained language models (PLMs) (Devlin et al., 2019;Conneau and Lample, 2019;Conneau et al., 2020) have been widely utilized in cross-lingual understanding tasks.",yes
5627,68-ARR,68-ARR_v2_34@0,68-ARR_v1_35@0,"In order to guide S 2 DM to disassociate syntactic information into the syntactic latent variable z, we also define three losses tailored for capturing different types of syntactic information.","In order to guide S 2 DM to disassociate syntactic information into the syntactic latent variable z, we also define three types of losses tailored for capturing different types of syntactic information.",yes
5628,68-ARR,68-ARR_v2_34@1,68-ARR_v1_35@1,"First, we employ Word Position Loss (WPL) , defined as follows:","First, we employ Word Position Loss (WPL) , as follow:",yes
5629,68-ARR,68-ARR_v2_4@1,68-ARR_v1_4@1,"However, zero-shot transfer method based on multilingual PLMs does not work well for low-resource language MRC.","However, the zero-shot transfer method based on multilingual PLM has limited effectiveness for low-resource languages MRC tasks.",yes
5630,68-ARR,68-ARR_v2_4@2,68-ARR_v1_4@2,"Such multilingual MRC models could roughly detect answer spans but may fail to predict the precise boundaries of answers (Yuan et al., 2020).","This type of multilingual MRC model could roughly detect answer spans but fail to predict the precise boundaries of answers (Yuan et al., 2020).",yes
5631,68-ARR,68-ARR_v2_49@0,68-ARR_v1_50@0,Generalization Analysis,Analysis,yes
5632,68-ARR,68-ARR_v2_55@1,68-ARR_v1_52@4,"In the same way, both p θ (x s , y s ) and p θ (x t , y t ) also fit to the same distribution, no matter what the target language is.","In the same way, p θ (x s , y s ) and p θ (x t , y t ) also fit to the same distribution, no matter what the target language is.",yes
5633,68-ARR,68-ARR_v2_56@0,68-ARR_v1_54@0,"Furthermore, the semantic discrimination loss in Eq.( 5) guarantees that the semantic vectors of the source language and the target language are similar to each other.","Furthermore, the semantic discrimination loss Eq.( 5) guarantees that the semantic vectors of the source language and the target language are similar.",yes
5634,68-ARR,68-ARR_v2_56@1,68-ARR_v1_54@1,"Minimizing Eq.( 5) can be equivalent to: sim(y s , y t ) > sim(y s , n t ) + δ sim(y s , y t ) > sim(n s , y t ) + δ which is to maximize sim(y s , y t ) to encourages the target semantic vector to approach parallel source semantic vector.","Minimizing Eq.( 5) can be equivalent to: sim(ys, yt) > sim(ys, nt) + δ sim(ys, yt) > sim(ns, yt) + δ which is to maximize sim(y s , y t ) to encourages the target semantic vector to approach parallel source semantic vector.",yes
5635,68-ARR,68-ARR_v2_56@3,68-ARR_v1_55@1,"Therefore, our multilingual MRC model is suitable even for low-resource languages without training data for the decoupling model.","Therefore, our multilingual MRC model is suitable even for low-resource languages without training data of decoupling model.",yes
5636,68-ARR,68-ARR_v2_59@0,68-ARR_v1_58@0,"XQuAD (Artetxe et al., 2020) consists of 11 datasets of different languages translated from the SQuAD v1.1 (Rajpurkar et al., 2016) development set, including Spanish (es), German (de), Greek (el), Russian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), and Romanian (ro).","XQuAD (Artetxe et al., 2020) consists of 11 languages corpus translated from the SQuAD v1.1 (Rajpurkar et al., 2016) development set, include : Spanish (es), German (de), Greek (el), Russian (ru), Turkish (tr), Arabic (ar), Vietnamese (vi), Thai (th), Chinese (zh), Hindi (hi), and Romanian (ro).",yes
5637,68-ARR,68-ARR_v2_61@0,68-ARR_v1_60@0,"TyDi QA-GoldP is the gold passage task in TyDi QA (Clark et al., 2020) covering 9 typologically diverse languages: Arabic (ar), Bengali (bg), English (en), Finnish (fi), Indonesian (id), Korean (ko), Russian (ru), Swahili (sw), Telugu (te).","TyDi QA-GoldP is the gold passage task in Ty-Di QA (Clark et al., 2020) covers 9 typologically diverse languages: Arabic (ar), Bengali (bg), English (en), Finnish (fi), Indonesian (id), Korean (ko), Russian (ru), Swahili (sw), Telugu (te).",yes
5638,68-ARR,68-ARR_v2_69@0,68-ARR_v1_66@0,"For S 2 DM, we collected 26k labelled parallel sentence pairs from the Universal Dependencies (UD 2.7) Corpus (Zeman et al., 2020) as the training set.","For S 2 DM, we collected approximately 26,000+ labelled parallel sentence pairs from the Universal Dependencies (UD 2.7) Corpus (Zeman et al., 2020) as the training set.",yes
5639,68-ARR,68-ARR_v2_69@1,68-ARR_v1_66@1,The training set covers 20 languages and overlap with 13 languages of three MRC datasets.,The training set covers 20 languages and overlap with 13 languages of three MRC evaluation datasets.,yes
5640,68-ARR,68-ARR_v2_70@0,68-ARR_v1_67@0,"For our multilingual MRC models and two baseline models, we fine-tuned them on the SQuAD v1.1 (Rajpurkar et al., 2016) and evaluated them on the test data of the three multilingual MRC datasets.","For our multilingual MRC models and two baseline models, we fine-tuned them on the SQuAD v1.1 (Rajpurkar et al., 2016) and evaluated the test data of the three multilingual MRC datasets.",yes
5641,68-ARR,68-ARR_v2_72@4,68-ARR_v1_69@4,"Especially, compared with baselines on the TyDi QA-Gold dataset, S 2 DM_SP based on XLM-100 and mBERT gains 4.1%, 4.2% EM improvements on average across 9 languages, respectively.","Especially, compared with baselines on TyDi QA-Gold datasets, S 2 DM_SP based on XLM-100 and mBERT gains 4.1%, 4.2% EM respectively improvements on average across 9 languages.",yes
5642,68-ARR,68-ARR_v2_73@1,68-ARR_v1_70@1,"For cross-lingual transfer performance, our models are better than the two baselines in terms of either EM or F1 on all 11 low-resource target languages.","For cross-lingual transfer performance, our models are better than the two baselines on EM or F1 of all 11 low-resource target languages.",yes
5643,68-ARR,68-ARR_v2_5@3,68-ARR_v1_4@5,"(b) A case from BiPaR (Jing et al., 2019) where the answer predicted by a model transferred from English to Chinese violates syntactic constituent boundaries in the target language.","(b) A case from BiPaR (Jing et al., 2019) where the answer predicted by the model transferred from English to Chinese violates syntactic constituent boundaries.",yes
5644,68-ARR,68-ARR_v2_74@1,68-ARR_v1_70@3,"The results of TyDi QA-GoldP are shown in Table 4, and our models are superior to the baselines in terms of either EM or F1 for all 8 low-resource target languages.","The results of TyDi QA-GoldP are shown in Table 4, and our models are superior to the baselines on EM or F1 of all 8 low-resource target languages.",yes
5645,68-ARR,68-ARR_v2_75@1,68-ARR_v1_71@1,The results on the three datasets show the effectiveness on five languages not included in the training target languages for S 2 DM.,The results in three datasets show the effectiveness of five languages not included in the training target languages for S 2 DM.,yes
5646,68-ARR,68-ARR_v2_78@1,68-ARR_v1_74@1,The results are shown in Figure 3.,The results are reported in Table 3.,yes
5647,68-ARR,68-ARR_v2_79@3,68-ARR_v1_75@3,"All ablation models do not exceed our best model, illustrating the importance of all proposed losses.","All ablation models do not exceed our best model, illustrating the importance of all our losses.",yes
5648,68-ARR,68-ARR_v2_80@0,68-ARR_v1_78@0,Why the S 2 DM Works?,Why S 2 DM Works?,yes
5649,68-ARR,68-ARR_v2_6@1,68-ARR_v1_5@2,"Liang et al. (2021) present a boundary calibration model stacked over a base sequence labeling module, introducing a phrase boundary recovery task to pretrain the calibration module on large-scale multilingual datasets synthesized from Wikipedia documents.","Liang et al. (2021) propose a separate model for boundary calibration based on the output of a base zero-shot transfer model, introducing a phrase boundary recovery task to pretrain calibration module on large-scale multilingual datasets synthesized from Wikipedia documents.",yes
5650,68-ARR,68-ARR_v2_86@2,68-ARR_v1_83@2,"For low-resource languages without training data for the decoupling model, our theoretical analysis and experiments verify the generalization of our multilingual MRC model.","For low-resource languages without training data of decoupling model, we theoretically analyze and experiment verify the generalization of our multilingual MRC model.",yes
5651,68-ARR,68-ARR_v2_6@3,68-ARR_v1_6@1,"On four multilingual MRC datasets, we use Stanford CoreNLP 1 to collect syntax parse trees and calculate the percentages of ground-truth answers that respect syntactic constituent boundaries.","On four multilingual MRC evaluation datasets, we use Stanford CoreNLP 1 to collect syntax parse trees and calculate the percentages of ground-truth answers that respect syntactic constituent boundaries.",yes
5652,68-ARR,68-ARR_v2_6@4,68-ARR_v1_6@2,"As shown in Table 1, over 87% of answer spans respect the syntactic constraint.","As shown in Table 1, over 87% of answer spans respect syntactic constraint.",yes
5653,68-ARR,68-ARR_v2_7@1,68-ARR_v1_7@1,"For questions where the monolingual model correctly predicts the answer and respect syntactic constraint, 23.15% of them are incorrectly predicted by the transfer model, and the predicted answers violate the syntactic constraint, illustrated by the case in Figure 1 (b).","For answers that the monolingual model correctly predicts and respect syntactic constraint, 23.15% of them, which are incorrectly predicted by the transfer model, violate the syntactic constraint, illustrated by the case in Figure 1(b).",yes
5654,68-ARR,68-ARR_v2_7@2,68-ARR_v1_7@2,"This suggests that the source language syntax may have a negative impact on the answer boundary detection in the target language during zero-shot transfer, due to the linguistic discrepancies between the two languages.","It suggests that the source language syntax may have a negative impact on the answer boundary detection on the target language during zero-shot transfer, due to the linguistic discrepancies between the two languages.",yes
5655,68-ARR,68-ARR_v2_8@0,68-ARR_v1_8@0,"However, linguistic discrepancies are diverse and it is difficult to learn them.","However, the linguistic discrepancies are diverse and impossible to learn.",yes
5656,68-ARR,68-ARR_v2_8@1,68-ARR_v1_8@1,"We hence propose to decouple semantics from syntax in pretrained models for multilingual MRC, transforming the learning of linguistic discrepancies into universal semantic information.","We propose to decouple semantics from syntax in pre-trained models for multilingual MRC, transforming the learning of linguistic discrepancies into universal semantic information.",yes
5657,68-ARR,68-ARR_v2_9@0,68-ARR_v1_9@2,"To disassociate semantic and syntactic information in PLMs well, we introduce objective functions of learning cross-lingual reconstruction and semantic discrimination together with losses of incorporating word order information and syntax structure information (Part-of-Speech tags and syntax parse trees).","To disassociate semantic and syntactic information well, we introduce objective functions of learning cross-lingual reconstruction and semantic discrimination and losses of incorporating word order information and syntax structure information (Part-of-Speech tags and syntax parse trees), respectively.",yes
,68-ARR,68-ARR_v2_5@0,68-ARR_v1_24@1,"In order to address this issue, existing methods mainly resort to external resources.","In our model, we use a bag-of-words decoder as the generator.",no
,68-ARR,68-ARR_v2_5@0,68-ARR_v1_9@0,"In order to address this issue, existing methods mainly resort to external resources.","Besides, the multilingual pre-trained models have learned an amount of linguistic structure of multiple languages (Chi et al., 2020).",no
,68-ARR,68-ARR_v2_67@1,68-ARR_v1_9@1,"The external corpus contain 363.5k passages and 534k knowledge phrases in four languages: English (en), French (fr), German (de), and Spanish (es).",The semantic can not directly disentangle from complex syntactic information in PLMs.,no
,68-ARR,68-ARR_v2_73@2,68-ARR_v1_9@0,"On the MLQA dataset, LAKM uses a larger extra corpus to train a better backbone language model, while our method with less external data can still achieve similar performance in German (de) and Spanish (es).","Besides, the multilingual pre-trained models have learned an amount of linguistic structure of multiple languages (Chi et al., 2020).",no
,68-ARR,68-ARR_v2_66@0,68-ARR_v1_76@0,"Furthermore, we compared with a strong baseline that uses external knowledge to enhance crosslingual MRC:",5.2 Why Use a Siamese Network in S 2 DM?,no
,68-ARR,68-ARR_v2_83@2,68-ARR_v1_77@0,"Combined with the negative/positive results of syntactic/semantic vectors in the cross-lingual STS task in SemEval-2017, the visualization demonstrates that S 2 DM can efficiently disassociate semantics from syntax.","In order to separate semantic information from PLMs, an alternative way is to train a single net- Corresponding to S 2 DM, there are also two single-network variants: S 2 DM_single_POS and S 2 DM_single_SP.",no
,68-ARR,68-ARR_v2_67@1,68-ARR_v1_76@0,"The external corpus contain 363.5k passages and 534k knowledge phrases in four languages: English (en), French (fr), German (de), and Spanish (es).",5.2 Why Use a Siamese Network in S 2 DM?,no
,68-ARR,68-ARR_v2_73@2,68-ARR_v1_14@2,"On the MLQA dataset, LAKM uses a larger extra corpus to train a better backbone language model, while our method with less external data can still achieve similar performance in German (de) and Spanish (es).",Cui et al. (2019) propose a method that combines multilingual BERT and back-translation for cross-lingual MRC.,no
,68-ARR,68-ARR_v2_66@0,68-ARR_v1_5@0,"Furthermore, we compared with a strong baseline that uses external knowledge to enhance crosslingual MRC:","For the inaccuracy of boundary detection, existing methods mainly solve this issue by introducing external knowledge.",no
,68-ARR,68-ARR_v2_83@2,68-ARR_v1_14@1,"Combined with the negative/positive results of syntactic/semantic vectors in the cross-lingual STS task in SemEval-2017, the visualization demonstrates that S 2 DM can efficiently disassociate semantics from syntax.",Various approaches have been proposed on top of multilingual MRC based on PLMs.,no
,68-ARR,68-ARR_v2_73@2,68-ARR_v1_9@1,"On the MLQA dataset, LAKM uses a larger extra corpus to train a better backbone language model, while our method with less external data can still achieve similar performance in German (de) and Spanish (es).",The semantic can not directly disentangle from complex syntactic information in PLMs.,no
,68-ARR,68-ARR_v2_83@2,68-ARR_v1_24@1,"Combined with the negative/positive results of syntactic/semantic vectors in the cross-lingual STS task in SemEval-2017, the visualization demonstrates that S 2 DM can efficiently disassociate semantics from syntax.","In our model, we use a bag-of-words decoder as the generator.",no
,68-ARR,68-ARR_v2_67@1,68-ARR_v1_70@0,"The external corpus contain 363.5k passages and 534k knowledge phrases in four languages: English (en), French (fr), German (de), and Spanish (es).",The results of 12 languages in XQuAD and M-LQA are shown in Table 3.,no
,68-ARR,68-ARR_v2_5@0,68-ARR_v1_76@0,"In order to address this issue, existing methods mainly resort to external resources.",5.2 Why Use a Siamese Network in S 2 DM?,no
,68-ARR,68-ARR_v2_83@2,68-ARR_v1_5@0,"Combined with the negative/positive results of syntactic/semantic vectors in the cross-lingual STS task in SemEval-2017, the visualization demonstrates that S 2 DM can efficiently disassociate semantics from syntax.","For the inaccuracy of boundary detection, existing methods mainly solve this issue by introducing external knowledge.",no
,68-ARR,68-ARR_v2_66@0,68-ARR_v1_14@1,"Furthermore, we compared with a strong baseline that uses external knowledge to enhance crosslingual MRC:",Various approaches have been proposed on top of multilingual MRC based on PLMs.,no
,68-ARR,68-ARR_v2_5@0,68-ARR_v1_14@2,"In order to address this issue, existing methods mainly resort to external resources.",Cui et al. (2019) propose a method that combines multilingual BERT and back-translation for cross-lingual MRC.,no
,68-ARR,68-ARR_v2_5@0,68-ARR_v1_9@1,"In order to address this issue, existing methods mainly resort to external resources.",The semantic can not directly disentangle from complex syntactic information in PLMs.,no
,68-ARR,68-ARR_v2_73@2,68-ARR_v1_24@1,"On the MLQA dataset, LAKM uses a larger extra corpus to train a better backbone language model, while our method with less external data can still achieve similar performance in German (de) and Spanish (es).","In our model, we use a bag-of-words decoder as the generator.",no
,68-ARR,68-ARR_v2_5@0,68-ARR_v1_77@0,"In order to address this issue, existing methods mainly resort to external resources.","In order to separate semantic information from PLMs, an alternative way is to train a single net- Corresponding to S 2 DM, there are also two single-network variants: S 2 DM_single_POS and S 2 DM_single_SP.",no
,68-ARR,68-ARR_v2_73@2,68-ARR_v1_76@0,"On the MLQA dataset, LAKM uses a larger extra corpus to train a better backbone language model, while our method with less external data can still achieve similar performance in German (de) and Spanish (es).",5.2 Why Use a Siamese Network in S 2 DM?,no
,68-ARR,68-ARR_v2_66@0,68-ARR_v1_2@4,"Furthermore, we compared with a strong baseline that uses external knowledge to enhance crosslingual MRC:","Moreover, we theoretically analyze the generalization of our decoupling model, even for low-resource languages without training corpus.",no
,68-ARR,68-ARR_v2_67@1,68-ARR_v1_77@0,"The external corpus contain 363.5k passages and 534k knowledge phrases in four languages: English (en), French (fr), German (de), and Spanish (es).","In order to separate semantic information from PLMs, an alternative way is to train a single net- Corresponding to S 2 DM, there are also two single-network variants: S 2 DM_single_POS and S 2 DM_single_SP.",no
,68-ARR,68-ARR_v2_83@2,68-ARR_v1_14@2,"Combined with the negative/positive results of syntactic/semantic vectors in the cross-lingual STS task in SemEval-2017, the visualization demonstrates that S 2 DM can efficiently disassociate semantics from syntax.",Cui et al. (2019) propose a method that combines multilingual BERT and back-translation for cross-lingual MRC.,no
,68-ARR,68-ARR_v2_67@0,68-ARR_v1_9@0,"LAKM is a pre-trained task proposed in (Yuan et al., 2020) by introducing external sources for phrase-level masked language modeling task.","Besides, the multilingual pre-trained models have learned an amount of linguistic structure of multiple languages (Chi et al., 2020).",no
,68-ARR,68-ARR_v2_83@2,68-ARR_v1_9@1,"Combined with the negative/positive results of syntactic/semantic vectors in the cross-lingual STS task in SemEval-2017, the visualization demonstrates that S 2 DM can efficiently disassociate semantics from syntax.",The semantic can not directly disentangle from complex syntactic information in PLMs.,no
,68-ARR,68-ARR_v2_66@0,68-ARR_v1_9@0,"Furthermore, we compared with a strong baseline that uses external knowledge to enhance crosslingual MRC:","Besides, the multilingual pre-trained models have learned an amount of linguistic structure of multiple languages (Chi et al., 2020).",no
,68-ARR,68-ARR_v2_67@0,68-ARR_v1_14@1,"LAKM is a pre-trained task proposed in (Yuan et al., 2020) by introducing external sources for phrase-level masked language modeling task.",Various approaches have been proposed on top of multilingual MRC based on PLMs.,no
,68-ARR,68-ARR_v2_5@0,68-ARR_v1_2@4,"In order to address this issue, existing methods mainly resort to external resources.","Moreover, we theoretically analyze the generalization of our decoupling model, even for low-resource languages without training corpus.",no
,68-ARR,68-ARR_v2_67@1,68-ARR_v1_24@1,"The external corpus contain 363.5k passages and 534k knowledge phrases in four languages: English (en), French (fr), German (de), and Spanish (es).","In our model, we use a bag-of-words decoder as the generator.",no
,68-ARR,68-ARR_v2_67@0,68-ARR_v1_9@1,"LAKM is a pre-trained task proposed in (Yuan et al., 2020) by introducing external sources for phrase-level masked language modeling task.",The semantic can not directly disentangle from complex syntactic information in PLMs.,no
,68-ARR,68-ARR_v2_66@0,68-ARR_v1_9@1,"Furthermore, we compared with a strong baseline that uses external knowledge to enhance crosslingual MRC:",The semantic can not directly disentangle from complex syntactic information in PLMs.,no
,68-ARR,68-ARR_v2_67@0,68-ARR_v1_77@0,"LAKM is a pre-trained task proposed in (Yuan et al., 2020) by introducing external sources for phrase-level masked language modeling task.","In order to separate semantic information from PLMs, an alternative way is to train a single net- Corresponding to S 2 DM, there are also two single-network variants: S 2 DM_single_POS and S 2 DM_single_SP.",no
,68-ARR,68-ARR_v2_83@2,68-ARR_v1_9@0,"Combined with the negative/positive results of syntactic/semantic vectors in the cross-lingual STS task in SemEval-2017, the visualization demonstrates that S 2 DM can efficiently disassociate semantics from syntax.","Besides, the multilingual pre-trained models have learned an amount of linguistic structure of multiple languages (Chi et al., 2020).",no
,68-ARR,68-ARR_v2_66@0,68-ARR_v1_70@0,"Furthermore, we compared with a strong baseline that uses external knowledge to enhance crosslingual MRC:",The results of 12 languages in XQuAD and M-LQA are shown in Table 3.,no
5708,80-ARR,80-ARR_v2_2@0,80-ARR_v1_2@0,"We are interested in a novel task, singing voice beautification (SVB).","We are interested in a novel task, singing voice beautifying (SVB).",yes
5709,80-ARR,80-ARR_v2_5@0,80-ARR_v1_5@0,"The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019;Blaauw and Bonada, 2020;Ren et al., 2020;Liu et al., 2021a) and Singing Voice Conversion (SVC) (Sisman and Li, 2020;Li et al., 2021;Wang et al., 2021a).","The major successes of the artificial intelligent singing voice research are primarily in Singing Voice Synthesis (SVS) (Lee et al., 2019;Blaauw and Bonada, 2020;Ren et al., 2020;Zhang et al., 2021) and Singing Voice Conversion (SVC) (Sisman and Li, 2020;Li et al., 2021;Wang et al., 2021a).",yes
5710,80-ARR,80-ARR_v2_5@1,80-ARR_v1_5@1,"However, the Singing Voice Beautification (SVB) remains an important and challenging endeavor for researchers.","However, the Singing Voice Beautifying (SVB) remains an important and challenging endeavor for researchers.",yes
5711,80-ARR,80-ARR_v2_54@3,80-ARR_v1_53@3,The parallel setting could make sure that the personal vocal timbre will keep still during the beautification process.,The parallel setting could make sure that the personal vocal timbre will keep still during the beautifying process.,yes
5712,80-ARR,80-ARR_v2_54@7,80-ARR_v1_53@7,We randomly choose 6 songs in Chinese and 18 songs in English (from unseen speakers) for validation and test.,We randomly choose 617 pieces in English and 274 pieces in Chinese for validation and test.,yes
5713,80-ARR,80-ARR_v2_54@8,80-ARR_v1_53@8,"For subjective evaluations, we choose 60 samples in the test set from different singers, half in Chinese and English.","For subjective evaluations, we choose 60 samples in the test set from different singers, half in English and Chinese.",yes
5714,80-ARR,80-ARR_v2_55@1,80-ARR_v1_54@1,"Besides PopBuTFy, we pre-train the ASR model (used for PPG extraction) leveraging the extra speech datasets: AISHELL-3 (Yao Shi et al., 2020) for Chinese and LibriTTS (Zen et al., 2019) for English.","Besides PopBuTFy, we pre-train the ASR model (used for PPG extraction) leveraging the extra speech datasets: AISHELL-3 (Yao Shi, 2020) for Chinese and LibriTTS (Zen et al., 2019) for English.",yes
5715,80-ARR,80-ARR_v2_68@6,80-ARR_v1_70@6,We split evaluations for main experiments and ablation studies into several groups for them.,We split evaluations for experiments and ablation studies into several groups for them.,yes
5716,80-ARR,80-ARR_v2_9@0,80-ARR_v1_10@0,"• We propose the first generative model NSVB to solve the SVB task. NSVB not only corrects the pitch of amateur recordings, but also generates the audio with high audio quality and improved vocal tone, to which previous works typically pay little attention. • We propose Shape-Aware Dynamic Time Warping (SADTW) algorithm to synchronize the amateur recording with the template pitch curve, which ameliorates the robustness of the previous time-warping algorithm. • We propose a latent-mapping algorithm to convert the latent variable of the amateur vocal tone to the professional one's, and contribute a new dataset PopBuTFyto train the latent-mapping function. • We design NSVB as a CVAE model, which supports the semi-supervised learning to leverage unpaired, unlabeled singing data for better performance.","• We propose the first generative model NSVB to solve the SVB task. NSVB not only corrects the pitch of amateur recordings, but also generates the audio with high audio quality and improved vocal tone, to which previous works typically pay little attention. • We propose Shape-Aware Dynamic Time Warping (SADTW) algorithm to synchronize the amateur recording with the template pitch curve, which meliorates the robustness of the previous time-warping algorithm. • We propose a latent-mapping algorithm to convert the latent variable of the amateur vocal tone to the professional one's, and contribute a new dataset PopBuTFyto train the latent-mapping function, which will be released upon the paper is published. • We design NSVB as a CVAE model, which supports the semi-supervised learning to leverage unpaired, unlabeled singing data for better performance.",yes
5717,80-ARR,80-ARR_v2_12@0,80-ARR_v1_13@0,"Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015;Serrà et al., 2019;Popov et al., 2021;, which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al., 2021).","Singing Voice Conversion (SVC) is a sub-task of Voice Conversion (VC) (Berg-Kirkpatrick and Klein, 2015;Serrà et al., 2019;Popov et al., 2021), which transforms the vocal timbre (or singer identity) of one singer to that of another singer, while preserving the linguistic content and pitch/melody information (Li et al., 2021).",yes
,80-ARR,80-ARR_v2_70@2,80-ARR_v1_68@1,"Thus, we choose the dataset license: CC by-nc-sa 4.0.",The kernel size of the convolutional layer for prenet is 5.,no
,80-ARR,80-ARR_v2_3@6,80-ARR_v1_67@4,Codes: https://github. com/MoonInTheRiver/NeuralSVB.,"Lastly, two layers of 1D convolution and a ReLU process the summed skip-out to produce output.",no
,80-ARR,80-ARR_v2_70@0,80-ARR_v1_67@3,"This work develops a possible automatic way for singing voice beautification, which may cause unemployment for people with related occupations.","Finally, they produce a residual output for the next sub-layer and a skip-out.",no
,80-ARR,80-ARR_v2_70@1,80-ARR_v1_67@2,"In addition, there is the potential for harm from piracy and abuse of our released recordings.","After that, they got fused by being added up, then processed by tanh and sigmoid separately and then multiplied together.",no
,80-ARR,80-ARR_v2_69@1,80-ARR_v1_74@0,"In the latter case, we recommend people to use Singing Voice Synthesis (synthesizing waveform from PPG and MIDI) + Singing Voice Conversion (converting the vocal timbre of the synthesized waveform into the user's), or some Speech to Singing (STS) methods.",D.2 Does the paper describe how participants' privacy rights were respected in the data collection process?,no
,80-ARR,80-ARR_v2_3@4,80-ARR_v1_78@0,Audio samples are available at https://neuralsvb.,F Does the paper indicate that the data collection process was subjected to any necessary review by an appropriate review board?,no
,80-ARR,80-ARR_v2_71@0,80-ARR_v1_68@0,F.1 For every submission F.1.1 Did you discuss the limitations of your work?,"As shown in Figure 8, the content encoder is the combination of several conformer encoder layers in pink rectangle along with a 3-layer prenet.",no
,80-ARR,80-ARR_v2_69@2,80-ARR_v1_67@2,"In addition, SADTW provides a score representing the similarity of two pitch curves, which could be used to determine what kind of SVB solution should be chosen.","After that, they got fused by being added up, then processed by tanh and sigmoid separately and then multiplied together.",no
,80-ARR,80-ARR_v2_69@2,80-ARR_v1_67@3,"In addition, SADTW provides a score representing the similarity of two pitch curves, which could be used to determine what kind of SVB solution should be chosen.","Finally, they produce a residual output for the next sub-layer and a skip-out.",no
,80-ARR,80-ARR_v2_3@6,80-ARR_v1_67@3,Codes: https://github. com/MoonInTheRiver/NeuralSVB.,"Finally, they produce a residual output for the next sub-layer and a skip-out.",no
5828,9-ARR,9-ARR_v2_22@3,9-ARR_v1_24@1,"The results on GLUE are shown in Fig. 2, and the results on XTREME show similar patterns.",The results on GLUE are shown in Fig. 1.,yes
5829,9-ARR,9-ARR_v2_22@4,9-ARR_v1_24@2,We find that adding global noise with the same distribution to all the PLM parameters will harm the model performance.,We find that adding global noise with same distributions to the PLM parameters will harm the model perfor- mance.,yes
5832,9-ARR,9-ARR_v2_22@8,9-ARR_v1_24@4,"In addition, we find an interesting phenomenon that adding uniform noise is better than Gaussian noise.","In addition, we find an interesting phenomenon that adding uniform noise is better than using Gaussian noise.",yes
5833,9-ARR,9-ARR_v2_22@9,9-ARR_v1_24@5,This may be because Gaussian noise has wider ranges and some extreme values may affect the model performance.,This may be because Gaussian noise has wider ranges and some outliers may affect model performance.,yes
5834,9-ARR,9-ARR_v2_22@10,9-ARR_v1_24@6,"Thus, we use matrix-wise uniform noise in NoisyTune.","Thus, we prefer using matrix-wise uniform noise in our NoisyTune method.",yes
5835,9-ARR,9-ARR_v2_25@0,9-ARR_v1_25@0,Empirical Analysis of NoisyTune,Analysis on NoiseTune,yes
5836,9-ARR,9-ARR_v2_26@0,9-ARR_v1_26@0,"Next, we empirically analyze why NoisyTune can help PLM finetuning.",We then analyze the influence of NoisyTune on finetuning.,yes
5837,9-ARR,9-ARR_v2_26@1,9-ARR_v1_26@1,We compare the accuracy of BERT with and without NoisyTune finetuned with different percentage of samples on the MRPC dataset.,We show the accuracy of the BERT model with or without NoisyTune on the MRPC dataset in Fig. 2.,yes
5838,9-ARR,9-ARR_v2_2@6,9-ARR_v1_2@5,Extensive experiments on both GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can consistently empower the finetuning of different PLMs on different downstream tasks.,Extensive experiments on the GLUE English benchmark and the XTREME multilingual benchmark show that NoisyTune can consistently improve the performance of different PLMs in many downstream tasks.,yes
5839,9-ARR,9-ARR_v2_26@3,9-ARR_v1_26@3,"We find NoisyTune can consistently improve PLMs under different amounts of data, especially when less training data is used.",We find NoisyTune can consistently improve PLMs at different finetuning steps.,yes
5840,9-ARR,9-ARR_v2_27@0,9-ARR_v1_27@0,"To further study the impact of NoisyTune on PLM finetuning, we show the relative changes of the L 1 -norms of different kinds of parameters in the BERT model during finetuning on the MRPC dataset in Fig. 5.","To further study the impact of NoisyTune on model finetuning, we show the relative changes of the L1-norms of different kinds of paramters in the BERT model during training on MRPC and STS-B in Fig. 3.",yes
5841,9-ARR,9-ARR_v2_27@1,9-ARR_v1_27@1,"3 Since the noise we added to PLMs in NoisyTune is zero-mean uniform noise, the absolute parameter L 1 -norm will not change too much.","3 Since the noise we added is zeromean, the absolute parameter L1-norms will not be changed too much.",yes
5842,9-ARR,9-ARR_v2_27@2,9-ARR_v1_27@2,"However, we can see that the relative change of L 1 -norms becomes smaller when Noisy-Tune is applied, which indicates that the PLMs can find the (sub)optimal parameters for downstream tasks more easily.","However, we can see that the relative change of L1-norms becomes smaller when NoisyTune is applied, which indicates that the model uses smaller paces towards convergence.",yes
5845,9-ARR,9-ARR_v2_23@0,9-ARR_v1_28@0,Combination with Existing PLM Finetuning Methods,Empower Other Finetuning Methods,yes
5846,9-ARR,9-ARR_v2_24@2,9-ARR_v1_29@0,"In this section, we explore whether NoisyTune has the potential to empower the existing PLM finetuning techniques to achieve better performance.",Our NoisyTune method also has the potential to empower other PLM finetuning techniques.,yes
5847,9-ARR,9-ARR_v2_24@3,9-ARR_v1_29@1,"Here we select two well-known PLM finetuning for experiments, i.e., RecAdam and Mixout (Lee et al., 2020).","We compare the performance of the original RecAdam and Mixout (Lee et al., 2020) method and their variants combined with NoisyTune.",yes
5848,9-ARR,9-ARR_v2_24@4,9-ARR_v1_29@2,The experimental results are summarized in Fig. 3.,The results are shown in Fig. 4.,yes
5849,9-ARR,9-ARR_v2_24@5,9-ARR_v1_29@3,We find that combining NoisyTune with existing PLM finetuning techniques can further improve their performance.,We find that combining NoisyTune with existing PLM finetuning techniques can further improve the performance.,yes
5852,9-ARR,9-ARR_v2_31@0,9-ARR_v1_31@0,"In this paper, we propose a very simple but effective method named NoisyTune, which can help better finetune PLMs on downstream tasks by adding a little noise to them before finetuning.","In this paper, we propose a very simple but effective method named NoisyTune, which adds a little noise to PLMs before finetuning for better transferability from pretraining tasks to downstream tasks.",yes
5853,9-ARR,9-ARR_v2_31@1,9-ARR_v1_31@1,"In NoisyTune, we propose a matrix-wise perturbing method that adds noise with different intensities to different kinds of parameter matrices in PLMs according to their variances.","In NoisyTune, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of different parameter matrices in PLMs, which can consider the varied characteristics of different types of parameters.",yes
5854,9-ARR,9-ARR_v2_31@3,9-ARR_v1_31@2,Extensive experiments on both monolingual GLUE benchmark and multilingual XTREME benchmark demonstrate NoisyTune can consistently empower the finetuning of different PLMs on various downstream tasks to achieve better performance.,Extensive experiments on the monolingual GLUE benchmark and the multilingual XTREME benchmark demonstrate the NoisyTune can consistently improve the performance of different PLMs in various downstream tasks.,yes
5855,9-ARR,9-ARR_v2_4@1,9-ARR_v1_4@1,"Many PLMs such as BERT (Devlin et al., 2019), RoBERTa and UniLM (Dong et al., 2019) which are pretrained from large-scale unlabeled corpus in a selfsupervised way, have significantly improve various downstream tasks such as reading comprehension , machine translation (Brown et al., 2020), text classification (Bao et al., 2020), dialog (Wu et al., 2020) and recommendation by finetuning on these tasks.","Many PLMs such as BERT (Devlin et al., 2019) and RoBERTa have played critical roles in various applications, such as reading comprehension, machine translation and text classification (Dong et al., 2019).",yes
5856,9-ARR,9-ARR_v2_5@0,9-ARR_v1_5@0,"How to effectively finetune PLMs to better empower downstream tasks is an important research problem (Zheng et al., 2021).","How to effectively finetune PLMs to better empower downstream tasks is an important research problem (Zheng et al., 2021a).",yes
5857,9-ARR,9-ARR_v2_5@3,9-ARR_v1_5@1,"Only a few works explore more effective and robust PLM finetuning methods Lee et al., 2020;Aghajanyan et al., 2021;Xu et al., 2021).","Besides naively finetuning PLMs with labeled data in downstream tasks, many works explore more effective and robust PLM finetuning methods Jiang et al., 2020;Lee et al., 2020;Aghajanyan et al., 2021;Xu et al., 2021).",yes
5858,9-ARR,9-ARR_v2_5@5,9-ARR_v1_5@3,Lee et al. (2020) proposed Mixout which randomly replaces part of the parameters in the finetuned model with their original weights in the PLMs.,Lee et al. (2020) proposed a Mixout method to randomly replace parts of model parameters with their original pretrained weights.,yes
5859,9-ARR,9-ARR_v2_5@6,9-ARR_v1_5@4,These PLM finetuning methods mainly focus on preventing PLMs from overfitting the limited labeled data in downstream tasks.,These finetuning methods mainly focus on preventing PLMs from overfitting limited labeled data in downstream tasks.,yes
5860,9-ARR,9-ARR_v2_5@8,9-ARR_v1_5@5,"It is not easy for existing PLM finetuning methods to overcome such gap (Roberts et al., 2020), which may lead to suboptimal performance especially when labeled data in downstream tasks is insufficient.","However, PLMs have been well trained in the self-supervised pretraining tasks, and it can be difficult for them to overcome the barrier between pretraining and downstream tasks as well as the gaps of their domains during finetuning (Roberts et al., 2020), which may lead to a suboptimal performance especially when labeled data in downstream tasks is insufficient.",yes
5861,9-ARR,9-ARR_v2_2@1,9-ARR_v1_2@1,"However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks.","However, PLMs may have risks in overfitting pretraining signals, and there are some gaps between downstream tasks and the pretraining tasks.",yes
5862,9-ARR,9-ARR_v2_6@0,9-ARR_v1_6@0,"In order to handle this problem, in this paper we propose a very simple yet effective method named NoisyTune, which can help better finetune PLMs for downstream tasks.","In this paper, we propose a very simple yet effective method named NoisyTune, which can help better finetune PLMs for downstream tasks.",yes
5863,9-ARR,9-ARR_v2_6@2,9-ARR_v1_6@1,"It can help prevent PLMs from overfitting the tasks and data in the pretraining stage, and reduce the gap between pretraining and downstream tasks.","The key idea of NoisyTune is to add a little noise to perturb PLMs parameters before finetuning, which can help prevent them from overfitting the signals in the pretraining tasks, and reduce the gap between pretraining and downstreaming tasks.",yes
5864,9-ARR,9-ARR_v2_6@3,9-ARR_v1_6@2,"Since PLMs have different types of parameters which usually own different characteristics, in NoisyTune we use a matrix-wise perturbing method that adds uniform noise with different intensities to different parameter matrices according to their standard deviations for better adaptation.","Since different types of parameters in PLMs may have different characteristics, we propose a matrix-wise perturbing method that adds uniform noise with different intensities according to the standard deviations of different parameter matrices for better adaptation.",yes
5865,9-ARR,9-ARR_v2_6@4,9-ARR_v1_6@3,"We conduct extensive experiments on two widely used NLP benchmarks, namely, GLUE (Wang et al., 2018) for English language understanding and XTREME (Hu et al., 2020) for multilingual language understanding.","We conduct experiments on two widely used NLP benchmarks, i.e., GLUE (Wang et al., 2018) for English language understanding and XTREME (Hu et al., 2020) for multilingual language understanding.",yes
5866,9-ARR,9-ARR_v2_6@5,9-ARR_v1_6@4,The results show NoisyTune can empower the finetuning of different PLMs on many different downstream NLP tasks to consistently achieve better performance.,The results show that NoisyTune can consistently boost the performance of different PLMs in many downstream NLP tasks.,yes
5867,9-ARR,9-ARR_v2_8@2,9-ARR_v1_8@1,"It may be difficult for PLMs to effectively adapt to downstream tasks especially when labeled data in these tasks are limited, which is usually the case.","Since the parameters of PLMs are well tuned in the pretraining tasks and may overfit self-supervision signals, it may be difficult for them to adapt to downstream tasks especially when labeled data in downstream tasks are rather limited.",yes
5868,9-ARR,9-ARR_v2_2@2,9-ARR_v1_2@2,Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance.,"It can be difficult for vanilla finetuning methods to overcome the barrier between pretraining and downstream tasks, which leads to suboptimal performance.",yes
5869,9-ARR,9-ARR_v2_8@3,9-ARR_v1_8@2,"Motivated by the dueling bandits mechanism (Yue and Joachims, 2009) that adds randomness to the model for exploration, as shown in Fig. 1, we propose to add some noise to the parameters of PLMs before finetuning them on downstream tasks to do some ""exploration"" in parameter space and reduce the risk of overfitting the pretraining tasks and data.","Motivated by the dueling bandits mechanism (Yue and Joachims, 2009) that adds randomness to the model for exploration, we explore adding noise to PLMs before finetuning to ""explore"" other parameter spaces to reduce the problem of overfitting pretraining tasks.",yes
5870,9-ARR,9-ARR_v2_9@1,9-ARR_v1_10@2,Different parameter matrices in the PLMs usually have different characteristics and scales.,"However, different parameter matrices in the PLM have very different characteristics.",yes
5871,9-ARR,9-ARR_v2_9@2,9-ARR_v1_10@3,"For example, some researchers found that the self-attention parameters and the feed-forward network parameters in Transformers have very different properties, such as rank and density .","For example, the self-attention parameters and the feed-forward network parameters usually have very different properties .",yes
5872,9-ARR,9-ARR_v2_9@3,9-ARR_v1_10@4,"Thus, adding unified noise to all parameter matrices in PLMs may not be optimal for keeping their good model utility.","Thus, adding global noise may not be optimal for keeping good model utility.",yes
5873,9-ARR,9-ARR_v2_9@4,9-ARR_v1_10@5,"To handle this challenge, we propose a matrix-wise perturbing method that adds noise with different intensities to different parameter matrices according to their variances.","To solve this challenge, we propose a matrix-wise perturbing method that adds noise with different intensities according to the variance of parameter matrices.",yes
5874,9-ARR,9-ARR_v2_11@1,9-ARR_v1_10@6,"Denote the perturbed version of the parameter matrix W i as Wi , which is computed as follows:","We denote the perturbed version of the parameter matrix W i as Wi , which is computed as follows:",yes
5875,9-ARR,9-ARR_v2_2@3,9-ARR_v1_2@3,"In this paper, we propose a very simple yet effective method named NoisyTune to help better finetune PLMs on downstream tasks by adding some noise to the parameters of PLMs before finetuning.","In this paper, we propose a very simple yet effective method named NoisyTune which can help better finetune PLMs in downstream tasks by adding some noise to the parameters of PLMs before finetuning.",yes
5876,9-ARR,9-ARR_v2_14@0,9-ARR_v1_12@2,"In addition, in some PLMs there are some constant matrices, such as token type embeddings in RoBERTa .","In addition, in some PLMs there exist constant matrices, such as token type embeddings in RoBERTa .",yes
5877,9-ARR,9-ARR_v2_14@2,9-ARR_v1_12@4,It can ensure that these constant matrices will not be accidentally activated by additional noise.,This will ensure that these constant matrices will not be accidentally activated by additional noise.,yes
5878,9-ARR,9-ARR_v2_17@1,9-ARR_v1_15@1,"The first one is GLUE (Wang et al., 2018), which is a benchmark for English language understanding that contains different tasks like natural language inference, sentiment analysis and sentence similarity evaluation.","The first one is GLUE (Wang et al., 2018), which is a benchmark for English language understanding that contains tasks like natural language inference, sentiment analysis and sentence similarity evaluation.",yes
5879,9-ARR,9-ARR_v2_17@6,9-ARR_v1_17@2,"In order not to harm the alignment of token embeddings across different languages, we do not add noise to the token embeddings in multilingual PLMs.","In order not to harm the alignment of token embeddings across different languages, We do not add noise to the token embeddings in multilingual PLMs.",yes
5882,9-ARR,9-ARR_v2_19@0,9-ARR_v1_19@0,"On the GLUE benchmark, we compare the performance of directly finetuning the base version of BERT (Devlin et al., 2019), XLNET , RoBERTa and ELEC-TRA (Clark et al., 2020) with that of finetuning them after applying NoisyTune.","On the GLUE benchmark, we compare directly finetuning the base version of BERT (Devlin et al., 2019), XLNET , RoBERTa and ELECTRA (Clark et al., 2020) as well as finetuning them after applying NoisyTune.",yes
5883,9-ARR,9-ARR_v2_19@1,9-ARR_v1_20@0,"On the XTREME benchmark, we compare the performance of directly finetuning both base and large versions of XLM-R (Conneau et al., 2020) with that of their variants obtained by applying NoisyTune.","On the XTREME benchmark, we compare both base and large versions of XLM-R (Conneau et al., 2020) and their variants processed by NoisyTune.",yes
5886,9-ARR,9-ARR_v2_20@0,9-ARR_v1_21@2,"According to these results, NoisyTune can consistently improve the performance of different PLMs on different tasks in both English and multilingual settings.","From the results, we can see that NoisyTune can consistently improve the performance of different PLMs on different tasks.",yes
5887,9-ARR,9-ARR_v2_20@1,9-ARR_v1_21@3,"In addition, the performance improvement brought by NoisyTune is usually larger on relatively small datasets (e.g., RTE, CoLA and WNLI).","In addition, the performance improvement on relatively small datasets is usually larger (e.g., RTE, CoLA and WNLI).",yes
5888,9-ARR,9-ARR_v2_20@2,9-ARR_v1_21@4,"These results indicate that when labeled data in downstream tasks is insufficient, it is quite difficult to effectively finetune PLMs starting from the original parameters which usually overfit the pretraining tasks and data.","This indicates that when labeled training data in downstream tasks is not redundant, it may be more difficult to well adapt PLMs to downstream tasks from the parameter space well tuned in pretraining tasks.",yes
5889,9-ARR,9-ARR_v2_20@3,9-ARR_v1_21@5,"The experimental results validate that NoisyTune can properly perturb PLMs with a little noise to explore different parameter spaces and reduce the overfitting problem, making PLMs easier to be adapted to downstream tasks.","Thus, properly perturbing PLMs with noise can explore different parameter spaces and meanwhile keep useful knowledge encoded in pretraining tasks.",yes
5890,9-ARR,9-ARR_v2_21@0,9-ARR_v1_22@0,Which Noise to Use and How?,Influence of Noise Type,yes
5891,9-ARR,9-ARR_v2_22@0,9-ARR_v1_23@0,In this section we study which kind of noise is more suitable for NoisyTune.,"Next, we study the influence of using different kinds of noise on NoisyTune.",yes
,9-ARR,9-ARR_v2_29@1,9-ARR_v1_8@0,The average GLUE scores w.r.t. different λ values are shown in Fig. 6.,"In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.",no
,9-ARR,9-ARR_v2_5@7,9-ARR_v1_16@4,"Besides the overfitting of downstream task data, a rarely studied problem is that the PLMs usually overfit the pretraining tasks and data (Qi et al., 2020), which may have significant gap with the downstream task and data.",The XTREME results are evaluated on the test set.,no
,9-ARR,9-ARR_v2_22@1,9-ARR_v1_16@4,"In addition, we explore whether our proposed matrix-wise perturbing method is better than using a unified global noise for all model parameters in PLMs.",The XTREME results are evaluated on the test set.,no
,9-ARR,9-ARR_v2_28@0,9-ARR_v1_10@1,Hyperparameter Analysis,"In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).",no
,9-ARR,9-ARR_v2_6@1,9-ARR_v1_16@4,"Different from the standard finetuning paradigm (Fig. 1 (a)) which directly finetunes PLMs on the downstream task data, the key idea of NoisyTune is to add a small amount of noise to perturb PLMs parameters before finetuning (Fig. 1 (b)).",The XTREME results are evaluated on the test set.,no
,9-ARR,9-ARR_v2_22@7,9-ARR_v1_10@1,"The results show that matrix-wise noise is a much better choice, since the different characteristics of different parameter matrices can be taken into consideration.","In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).",no
,9-ARR,9-ARR_v2_28@0,9-ARR_v1_16@4,Hyperparameter Analysis,The XTREME results are evaluated on the test set.,no
,9-ARR,9-ARR_v2_28@0,9-ARR_v1_26@2,Hyperparameter Analysis,2 The interval between two adjacent checkpoints is 50 iterations.,no
,9-ARR,9-ARR_v2_22@7,9-ARR_v1_16@5,"The results show that matrix-wise noise is a much better choice, since the different characteristics of different parameter matrices can be taken into consideration.",The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME.,no
,9-ARR,9-ARR_v2_6@1,9-ARR_v1_16@5,"Different from the standard finetuning paradigm (Fig. 1 (a)) which directly finetunes PLMs on the downstream task data, the key idea of NoisyTune is to add a small amount of noise to perturb PLMs parameters before finetuning (Fig. 1 (b)).",The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME.,no
,9-ARR,9-ARR_v2_29@4,9-ARR_v1_8@0,"While when λ is too large, the useful pretrained knowledge in PLMs may be overwhelmed by random noise.","In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.",no
,9-ARR,9-ARR_v2_22@1,9-ARR_v1_26@2,"In addition, we explore whether our proposed matrix-wise perturbing method is better than using a unified global noise for all model parameters in PLMs.",2 The interval between two adjacent checkpoints is 50 iterations.,no
,9-ARR,9-ARR_v2_29@1,9-ARR_v1_16@5,The average GLUE scores w.r.t. different λ values are shown in Fig. 6.,The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME.,no
,9-ARR,9-ARR_v2_31@2,9-ARR_v1_16@4,"NoisyTune is a very general method, and is PLM model agnostic, downstream task agnostic, and finetuning method agnostic.",The XTREME results are evaluated on the test set.,no
,9-ARR,9-ARR_v2_8@0,9-ARR_v1_26@2,The goal of NoisyTune is for more effective finetuning of PLMs on downstream tasks.,2 The interval between two adjacent checkpoints is 50 iterations.,no
,9-ARR,9-ARR_v2_5@7,9-ARR_v1_26@2,"Besides the overfitting of downstream task data, a rarely studied problem is that the PLMs usually overfit the pretraining tasks and data (Qi et al., 2020), which may have significant gap with the downstream task and data.",2 The interval between two adjacent checkpoints is 50 iterations.,no
,9-ARR,9-ARR_v2_29@0,9-ARR_v1_26@2,"We study the influence of the most important hyperparameter in NoisyTune, i.e., λ, which controls the relative noise intensity.",2 The interval between two adjacent checkpoints is 50 iterations.,no
,9-ARR,9-ARR_v2_8@1,9-ARR_v1_8@0,"The motivation of NoisyTune is that PLMs are well pretrained on some unlabeled corpus with some self-supervision tasks, and they may overfit these pretraining data and tasks (Qi et al., 2020), which usually have gap with the downstream task and data.","In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.",no
,9-ARR,9-ARR_v2_29@1,9-ARR_v1_10@1,The average GLUE scores w.r.t. different λ values are shown in Fig. 6.,"In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).",no
,9-ARR,9-ARR_v2_24@0,9-ARR_v1_16@5,"From Fig. 1, it is very clear that NoisyTune is independent of the specific PLM finetuning method, since it is applied at the stage before finetuning PLM on the task-specific data.",The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME.,no
,9-ARR,9-ARR_v2_29@5,9-ARR_v1_4@2,Values between 0.1 and 0.15 are more suitable for NoisyTune on the GLUE datasets.,"In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",no
,9-ARR,9-ARR_v2_29@3,9-ARR_v1_4@2,"This is because when λ is too small, it is difficult for PLMs to do parameter space exploration and overcome the overfitting problem.","In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",no
,9-ARR,9-ARR_v2_6@6,9-ARR_v1_26@2,"In addition, the results show NoisyTune can be easily combined with many existing PLM finetuning methods and further improve their performance.",2 The interval between two adjacent checkpoints is 50 iterations.,no
,9-ARR,9-ARR_v2_9@0,9-ARR_v1_4@2,"PLMs usually have different kinds of parameter matrices, such as query, key, value, and feedforward network matrices (Devlin et al., 2019).","In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",no
,9-ARR,9-ARR_v2_28@0,9-ARR_v1_4@2,Hyperparameter Analysis,"In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",no
,9-ARR,9-ARR_v2_31@2,9-ARR_v1_10@1,"NoisyTune is a very general method, and is PLM model agnostic, downstream task agnostic, and finetuning method agnostic.","In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).",no
,9-ARR,9-ARR_v2_31@2,9-ARR_v1_26@2,"NoisyTune is a very general method, and is PLM model agnostic, downstream task agnostic, and finetuning method agnostic.",2 The interval between two adjacent checkpoints is 50 iterations.,no
,9-ARR,9-ARR_v2_8@0,9-ARR_v1_16@4,The goal of NoisyTune is for more effective finetuning of PLMs on downstream tasks.,The XTREME results are evaluated on the test set.,no
,9-ARR,9-ARR_v2_22@7,9-ARR_v1_8@0,"The results show that matrix-wise noise is a much better choice, since the different characteristics of different parameter matrices can be taken into consideration.","In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.",no
,9-ARR,9-ARR_v2_29@5,9-ARR_v1_16@5,Values between 0.1 and 0.15 are more suitable for NoisyTune on the GLUE datasets.,The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME.,no
,9-ARR,9-ARR_v2_29@1,9-ARR_v1_26@2,The average GLUE scores w.r.t. different λ values are shown in Fig. 6.,2 The interval between two adjacent checkpoints is 50 iterations.,no
,9-ARR,9-ARR_v2_6@6,9-ARR_v1_10@1,"In addition, the results show NoisyTune can be easily combined with many existing PLM finetuning methods and further improve their performance.","In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).",no
,9-ARR,9-ARR_v2_9@0,9-ARR_v1_26@2,"PLMs usually have different kinds of parameter matrices, such as query, key, value, and feedforward network matrices (Devlin et al., 2019).",2 The interval between two adjacent checkpoints is 50 iterations.,no
,9-ARR,9-ARR_v2_29@5,9-ARR_v1_8@0,Values between 0.1 and 0.15 are more suitable for NoisyTune on the GLUE datasets.,"In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.",no
,9-ARR,9-ARR_v2_6@1,9-ARR_v1_10@1,"Different from the standard finetuning paradigm (Fig. 1 (a)) which directly finetunes PLMs on the downstream task data, the key idea of NoisyTune is to add a small amount of noise to perturb PLMs parameters before finetuning (Fig. 1 (b)).","In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).",no
,9-ARR,9-ARR_v2_9@0,9-ARR_v1_10@1,"PLMs usually have different kinds of parameter matrices, such as query, key, value, and feedforward network matrices (Devlin et al., 2019).","In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).",no
,9-ARR,9-ARR_v2_26@2,9-ARR_v1_8@0,2 The results are shown in Fig. 4.,"In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.",no
,9-ARR,9-ARR_v2_26@2,9-ARR_v1_16@5,2 The results are shown in Fig. 4.,The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME.,no
,9-ARR,9-ARR_v2_8@1,9-ARR_v1_16@4,"The motivation of NoisyTune is that PLMs are well pretrained on some unlabeled corpus with some self-supervision tasks, and they may overfit these pretraining data and tasks (Qi et al., 2020), which usually have gap with the downstream task and data.",The XTREME results are evaluated on the test set.,no
,9-ARR,9-ARR_v2_22@7,9-ARR_v1_26@2,"The results show that matrix-wise noise is a much better choice, since the different characteristics of different parameter matrices can be taken into consideration.",2 The interval between two adjacent checkpoints is 50 iterations.,no
,9-ARR,9-ARR_v2_5@7,9-ARR_v1_16@5,"Besides the overfitting of downstream task data, a rarely studied problem is that the PLMs usually overfit the pretraining tasks and data (Qi et al., 2020), which may have significant gap with the downstream task and data.",The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME.,no
,9-ARR,9-ARR_v2_24@1,9-ARR_v1_4@2,"Thus, it is very easy to combine NoisyTune with any kind of existing PLM finetuning method.","In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",no
,9-ARR,9-ARR_v2_26@2,9-ARR_v1_16@4,2 The results are shown in Fig. 4.,The XTREME results are evaluated on the test set.,no
,9-ARR,9-ARR_v2_22@1,9-ARR_v1_10@1,"In addition, we explore whether our proposed matrix-wise perturbing method is better than using a unified global noise for all model parameters in PLMs.","In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).",no
,9-ARR,9-ARR_v2_9@0,9-ARR_v1_16@5,"PLMs usually have different kinds of parameter matrices, such as query, key, value, and feedforward network matrices (Devlin et al., 2019).",The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME.,no
,9-ARR,9-ARR_v2_6@6,9-ARR_v1_16@4,"In addition, the results show NoisyTune can be easily combined with many existing PLM finetuning methods and further improve their performance.",The XTREME results are evaluated on the test set.,no
,9-ARR,9-ARR_v2_29@4,9-ARR_v1_16@5,"While when λ is too large, the useful pretrained knowledge in PLMs may be overwhelmed by random noise.",The hyperparameter λ is 0.15 on GLUE and is 0.1 on XTREME.,no
,9-ARR,9-ARR_v2_9@0,9-ARR_v1_16@4,"PLMs usually have different kinds of parameter matrices, such as query, key, value, and feedforward network matrices (Devlin et al., 2019).",The XTREME results are evaluated on the test set.,no
,9-ARR,9-ARR_v2_24@1,9-ARR_v1_8@0,"Thus, it is very easy to combine NoisyTune with any kind of existing PLM finetuning method.","In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.",no
,9-ARR,9-ARR_v2_6@1,9-ARR_v1_4@2,"Different from the standard finetuning paradigm (Fig. 1 (a)) which directly finetunes PLMs on the downstream task data, the key idea of NoisyTune is to add a small amount of noise to perturb PLMs parameters before finetuning (Fig. 1 (b)).","In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",no
,9-ARR,9-ARR_v2_22@7,9-ARR_v1_4@2,"The results show that matrix-wise noise is a much better choice, since the different characteristics of different parameter matrices can be taken into consideration.","In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",no
,9-ARR,9-ARR_v2_28@0,9-ARR_v1_8@0,Hyperparameter Analysis,"In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.",no
,9-ARR,9-ARR_v2_29@2,9-ARR_v1_4@2,"We find that when λ is too small or too large, the performance is not optimal.","In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.",no
,9-ARR,9-ARR_v2_8@1,9-ARR_v1_10@1,"The motivation of NoisyTune is that PLMs are well pretrained on some unlabeled corpus with some self-supervision tasks, and they may overfit these pretraining data and tasks (Qi et al., 2020), which usually have gap with the downstream task and data.","In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).",no
5920,90-ARR,90-ARR_v2_4@1,90-ARR_v1_4@1,"However, dialogue can also be viewed as a sequential decision making process, which is well-suited to planning and reinforcement learning (RL) algorithms.","However, dialogue can also be viewed as a sequential decision making process which is well-suited to planning and reinforcement learning (RL) algorithms.",yes
5921,90-ARR,90-ARR_v2_30@1,90-ARR_v1_33@1,"In the case of the information retrieval task that we consider in this paper, these high-level actions correspond to deciding which database entity to retrieve for the user (e.g., suggesting a flight to the customer that meets all of their needs).","In the case of the information retrieval task that we consider in this paper, these high-level actions correspond to deciding which database entity to retrieve for the user (e.g. suggesting a flight to the customer that meets all of their needs).",yes
5925,90-ARR,90-ARR_v2_59@1,90-ARR_v1_62@1,"In terms of task success, CALM outperforms the prior SOTA (AirConcierge) by approximately 7%, achieving 88% task success when using greedy decoding from the language model (see Table 1).","In terms of task success, CALM outperforms the prior SOTA (AirConcierge) by approximately 7%, achieving 88% task success when using greedy decoding from the language model.",yes
5926,90-ARR,90-ARR_v2_59@2,90-ARR_v1_62@2,"Compared with AirConcierge, where all reasoning about the task context is done outside of the language model, CALM does all of the filtering, selecting, and responding with relevant flight table entries within the language model, in a fully end-to-end manner.","Compared with AirConcierge, where all reasoning about the task context is done outside of the language model, CALM does all of the filtering, selecting, and responding with relevant flight table entries within the language model, in a fully endto-end manner.",yes
5927,90-ARR,90-ARR_v2_76@3,90-ARR_v1_77@5,We release the code and model weights for our customer bot at https://sea-snell.github.io/CALM_LM_site/.,"To promote fair evaluations in future works that use the AirDialogue dataset, we will release the code and model weights for our customer bot upon acceptance.",yes
5928,90-ARR,90-ARR_v2_79@0,90-ARR_v1_80@0,"2. We then execute this predicted information against the agent's flight flag, to produce a set of valid final actions.","2. We then execute this predicted information against the agent's flight table and reservation flag, to produce a set of valid final actions.",yes
5929,90-ARR,90-ARR_v2_6@7,90-ARR_v1_7@1,"CALM unifies the traditional language modeling objective with task-specific supervision, where a language model is interpreted as a joint representation of dynamics and policies in an MDP, and the finetuning process utilizes a conditional imitation learning objective with a novel task relabeling strategy that teaches the model how to generate high-utility dialogues (see Figures 1 and 2).","CALM unifies the traditional language modeling objective with task-specific supervision, where a language model is interpreted as a joint representation of dynamics and policies in an MDP, and the finetuning process utilizes a conditional imitation learning objective with a novel task relabeling strategy that teaches the model how to generate high-utility dialogues.",yes
5930,90-ARR,90-ARR_v2_9@3,90-ARR_v1_12@5,"When fine-tuning on this relabeled dataset, we then apply a Task Specific Auxiliary Loss on top of the standard language modeling objective; this helps the model learn to use the task context.","When fine-tuning on this relabeled dataset, we then apply a Task Specific Auxilary Loss on top of the standard language modeling objective; this helps the model learn to use the task context.",yes
5931,90-ARR,90-ARR_v2_9@4,90-ARR_v1_12@6,"Once trained, CALM can consistently solve goal-directed dialogue tasks.","Once trained, CALM can consistently solve complex tasks in dialogue.",yes
,90-ARR,90-ARR_v2_65@3,90-ARR_v1_67@0,"A practical deployable dialogue system would likely require additional measures to account for such issues, analogously to how learning-based methods for self-driving vehicles might require some additional safety mechanisms to ensure constraints, and indeed further research on reward specification, ensuring truthful outputs, and other constraint strategies for dialogue systems that combine language models and reward maximization is a promising and important direction.",Ethical Statement,no
,90-ARR,90-ARR_v2_69@1,90-ARR_v1_69@0,"To generate our response, we sample entire dialogues from the language model and then re-rank the predicted dialogues with a reward function.","In this appendix, we provide all the details in our implementation for CALM.",no
,90-ARR,90-ARR_v2_86@1,90-ARR_v1_30@2,All models are evaluated with greedy decoding.,"Additionally, the structure of our specific task allows us to sample from the true posterior q(c o |τ, c h ), by using the rejection sampling procedure described above, rather than using approximations as in prior work (Eysenbach et al., 2020).",no
,90-ARR,90-ARR_v2_65@0,90-ARR_v1_69@0,"CALM optimizes for task-specific measures of success, and while such measures might be comparatively simple for domains such as AirDialogue, in general specifying the right success measure or reward function may present challenges.","In this appendix, we provide all the details in our implementation for CALM.",no
,90-ARR,90-ARR_v2_75@4,90-ARR_v1_30@2,We execute these flight requirements against the table and compare the output to the flight that was actually booked; this determines the reward (i.e. if the correct flight was booked or not).,"Additionally, the structure of our specific task allows us to sample from the true posterior q(c o |τ, c h ), by using the rejection sampling procedure described above, rather than using approximations as in prior work (Eysenbach et al., 2020).",no
,90-ARR,90-ARR_v2_86@2,90-ARR_v1_30@2,"In addition to the full task success rate, we report success rate for each sub-component of the full task (status / flight / name).","Additionally, the structure of our specific task allows us to sample from the true posterior q(c o |τ, c h ), by using the rejection sampling procedure described above, rather than using approximations as in prior work (Eysenbach et al., 2020).",no
,90-ARR,90-ARR_v2_65@1,90-ARR_v1_30@2,"Furthermore, as with all methods based on end-to-end language models, CALM is susceptible to internal biases and inconsistencies in the language model itself.","Additionally, the structure of our specific task allows us to sample from the true posterior q(c o |τ, c h ), by using the rejection sampling procedure described above, rather than using approximations as in prior work (Eysenbach et al., 2020).",no
,90-ARR,90-ARR_v2_65@0,90-ARR_v1_67@0,"CALM optimizes for task-specific measures of success, and while such measures might be comparatively simple for domains such as AirDialogue, in general specifying the right success measure or reward function may present challenges.",Ethical Statement,no
,90-ARR,90-ARR_v2_86@2,90-ARR_v1_68@0,"In addition to the full task success rate, we report success rate for each sub-component of the full task (status / flight / name).",We note that CALM is imperfect and in generating free-form dialogue can potentially produce harmful outputs.,no
6194,2-131,2-131_v2_35@1,2-131_v1_30@1,The thickness measurements before and after correction were not statistically significant (P<.05) for any of the subfields and also when stratified by diagnosis.,The thickness measurements before and after correction were not statistically significant (p<0.05) for any of the subfields and also when stratified by diagnosis.,yes
6195,2-131,2-131_v2_9@0,2-131_v1_4@0,Results: Spectralis had the highest number of images needing manual correction.,Results: The Spectralis device had the highest number of images needing manual correction.,yes
6196,2-131,2-131_v2_37@3,2-131_v1_32@3,77% of the differences were less than 48μm and 50% were less than 10μm.,77% of the differences were less than 48 μm and 50% were less than 10 μm.,yes
6197,2-131,2-131_v2_41@0,2-131_v1_36@0,"The mean (±SD) of the macular thickness of all of the subfields, including the central 1mm subfield (FTH) for Stratus™, Cirrus™, and Spectralis™ before and after manual correction of scans, stratified by diagnosis of NV-AMD and NNV-AMD, is shown in Table 1 .","The mean (+SD) of the macular thickness of all of the subfields, including the central 1 mm subfield (foveal thickness; FTH) for Stratus™, Cirrus™, and Spectralis™ devices before and after manual correction of scans, stratified by diagnosis of NV-AMD and NNV-AMD, is shown in Table 1 .",yes
6198,2-131,2-131_v2_41@1,2-131_v1_36@1,"For NV-AMD, the FTH values for central 1mm were 375µm (±129µm), 253µm (±74µm), 312µm (±110µm) for Spectralis™, Stratus™, and Cirrus™ respectively.","For NV-AMD, the FTH values for the central 1 mm were 375 µm (+129 µm), 253 µm (+74 µm), 312 µm (+110 µm) for Spectralis™, Stratus™, and Cirrus™ respectively.",yes
6199,2-131,2-131_v2_41@2,2-131_v1_36@2,"After correction, the values were 335µm (±106µm) for Spectralis™ and 318µm (±110µm) for Cirrus™.","After correction, the values were 335 µm (+106 µm) for Spectralis™ and 318 µm (+110 µm) for Cirrus™.",yes
6200,2-131,2-131_v2_41@3,2-131_v1_36@3,"On the other hand, the FTH values for NNV-AND in the central 1mm before correction were 298µm (87µm), 193µm (±32µm), and 229µm (±30µm) for Spectralis™, Stratus™, and Cirrus™ respectively.","On the other hand, the FTH values for NNV-AND in the central 1 mm before correction were 298 µm (+87 µm), 193 µm (+32 µm), and 229 µm (+30 µm) for Spectralis™, Stratus™, and Cirrus™ respectively.",yes
6201,2-131,2-131_v2_41@4,2-131_v1_36@4,Spectralis™ was the only device to have a different FTH value of 248µm (±56µm) after correction.,Spectralis™ was the only device to have a different FTH value (248 µm +56 µm) after correction.,yes
6202,2-131,2-131_v2_41@6,2-131_v1_36@6,"The retinal thickness measurements obtained via the Cirrus™ were slightly less (range: 230 to 320µm), while Stratus™ had the lowest values, ranging from 190 to 270µm.","The retinal thickness measurements obtained via Cirrus™ were slightly less (range: 230 to 320 µm), while Stratus™ had the lowest values, ranging from 190 to 270 µm.",yes
6203,2-131,2-131_v2_41@7,2-131_v1_36@7,There were no significant (p<.05) differences between the mean FTH of the first and second scans for each of the three devices.,There were no significant (p<0.05) differences between the mean FTH of the first and second scans for each of the three devices.,yes
6204,2-131,2-131_v2_9@2,2-131_v1_4@3,The CORs were lowest for Spectralis TM and similar and higher for Cirrus TM and Stratus TM .,The CORs were lowest for SpectralisTM and similar with each other and had higher values for CirrusTM and StratusTM.,yes
6205,2-131,2-131_v2_44@7,2-131_v1_39@7,"In the central subfield, Spectralis™ had a COR of 20µm NV-AMD which increased to 23µm; both Cirrus™ and Stratus™ had relatively larger CORs of 64µm (reduced to 49µm after correction) and 35µm, respectively.","In the central subfield, Spectralis™ had a COR of 20 µm NV-AMD which increased to 23 µm; both Cirrus™ and Stratus™ had relatively larger CORs of 64 µm (reduced to 49 µm after correction) and 35 µm, respectively.",yes
6206,2-131,2-131_v2_44@8,2-131_v1_39@8,"For NNV-AMD, the COR for the central subfield was 15µm for both Cirrus™ and Spectralis™, and was 24µm for Stratus™.","For NNV-AMD, the COR for the central subfield was 15 µm for both Cirrus™ and Spectralis™, and was 24 µm for Stratus™.",yes
6207,2-131,2-131_v2_44@9,2-131_v1_39@9,"After correction, the value decreased for Spectralis™ to 12µm and increased to 36µm for Cirrus™.","After correction, the value decreased for Spectralis™ to 12 µm and increased to 36 µm for Cirrus™.",yes
6208,2-131,2-131_v2_12@2,2-131_v1_7@2,"One example is Age-related Macular Degeneration (AMD), a progressive, blinding disease that is mostly non-neovascular (NNV-AMD) but can be associated with choroidal neovascularization (NV-AMD).","One example is Age-related Macular Degeneration (AMD), a progressive, blinding disease that is mostly Non-Neovascular (NNV-AMD) but can be associated with choroidal Neovascularization (NV-AMD).",yes
6209,2-131,2-131_v2_12@3,2-131_v1_7@3,"Currently, OCT is also being employed as an outcome measure in many multicenter clinical trials of AMD with Time Domain OCT (TD-OCT) device being the most common <REF-1> , <REF-2> .","Currently, OCT is also being employed as an outcome measure in many multicenter clinical trials of AMD with Time Domain OCT (TD-OCT) devices being the most common <REF-1> , <REF-2> .",yes
6210,2-131,2-131_v2_2@0,2-131_v1_2@0,"Purpose: To evaluate the reproducibility and reliability of optical coherence tomography scans obtained using the time domain (TD-OCT) Stratus TM OCT, and the Spectral Domain (SD-OCT) Spectralis TM and Cirrus TM OCT devices before and after manual correction in eyes with either Neovascular (NV-AMD) or Non-Neovascular (NNV-AMD) age-related macular degeneration.","Purpose: To evaluate the reproducibility and reliability of Optical Coherence Tomography scans (OCT) obtained using the Time Domain (TD-OCT) StratusTM OCT, and the Spectral Domain (SD-OCT) SpectralisTM and CirrusTM OCT devices before and after manual correction in eyes with either Neovascular (NV-AMD) or Non-Neovascular (NNV-AMD) Age-related Macular Degeneration.",yes
6211,2-131,2-131_v2_13@0,2-131_v1_8@0,"As this technology is increasingly being utilized by many ophthalmologists to evaluate and monitor patients and guide treatment decisions <REF-2> , it is important to understand the reliability and accuracy of thickness measurements obtained with various devices currently available.","As this technology is increasingly being utilized by many ophthalmologists to evaluate and monitor patients and guide treatment decisions <REF-2> , it is important to understand the reliability and accuracy of thickness measurements obtained with the various devices currently available.",yes
6212,2-131,2-131_v2_49@2,2-131_v1_44@2,"The COR for Cirrus™ increased by 15–40µm after correction for NNV-AMD. Also, Cirrus™ COR values were 10–30µm higher than Stratus™ values for both NV-AMD and NNV-AMD.","The COR for Cirrus™ increased by 15–40 µm after correction for NNV-AMD. Also, Cirrus™ COR values were 10–30 µm higher than Stratus™ values for both NV-AMD and NNV-AMD.",yes
6213,2-131,2-131_v2_13@1,2-131_v1_8@1,"Recently, studies have shown that in patients with AMD, there is a high frequency of errors in automated retinal thickness measurements due to incorrect segmentation of the retina in the TD-OCT machine specifically in NV-AMD <REF-2> , <REF-3> .","Recently, studies have shown that in patients with AMD, there is a high frequency of errors in automated retinal thickness measurements due to incorrect segmentation of the retina in the TD-OCT machine, specifically in NV-AMD <REF-2> , <REF-3> .",yes
6214,2-131,2-131_v2_13@2,2-131_v1_8@2,"Using an Spectral Domain OCT (SD-OCT) device Menke et al. found that NNV-AMD had fewer errors than NV-AMD, mostly due to the pathology of the disease resulting in retinal pigment epithelial (RPE) layer changes <REF-4> .","Using a Spectral Domain OCT (SD-OCT) device Menke et al. found that retinal thickness measurements in NNV-AMD cases had fewer errors than in NV-AMD cases, mostly due to the pathology of the former disease resulting in retinal pigment epithelial (RPE) layer changes <REF-4> .",yes
6215,2-131,2-131_v2_14@0,2-131_v1_9@0,"Manual correction of the algorithm is an option in newer generations of the review software and as more OCT devices are coming to the market, it is important to understand the clinical importance of manual correction of OCT algorithms and the agreement of thickness measurements from different machines before and after correction.","Manual correction of the OCT algorithm is an option in newer generations of the OCT review software and as more devices are coming to the market, it is important to understand the clinical importance of manual correction of OCT algorithms and the agreement of thickness measurements from different machines before and after correction.",yes
6216,2-131,2-131_v2_54@0,2-131_v1_49@0,Figure 2a–f show Bland-Altman plots with 95% confidence intervals for the FTH comparison of the machines before and after correction.,Figure 2A–F show Bland-Altman plots with 95% confidence intervals for the FTH comparison of the machines before and after correction.,yes
6217,2-131,2-131_v2_54@1,2-131_v1_49@1,"Before correction, the mean difference between the machines was 32µm for Spectralis™ vs. Cirrus™, 52µm for Cirrus™ vs. Stratus™, and 84µm for Spectralis™ vs. Stratus™.","Before correction, the mean difference between the machines was 32 µm for Spectralis™ vs. Cirrus™, 52 µm for Cirrus™ vs. Stratus™, and 84 µm for Spectralis™ vs. Stratus™.",yes
6218,2-131,2-131_v2_54@2,2-131_v1_49@2,"Manual correction reduced the differences, with it being 15µm for Spectralis™ vs. Cirrus™, 51µm for Cirrus™ vs. Stratus™, and 67µm for Spectralis™ vs. Stratus™.","Manual correction reduced the differences, with it being 15 µm for Spectralis™ vs. Cirrus™, 51 µm for Cirrus™ vs. Stratus™, and 67 µm for Spectralis™ vs. Stratus™.",yes
6219,2-131,2-131_v2_54@3,2-131_v1_49@3,"When stratified by diagnoses, the values were 34µm and 29µm for Spectralis™ vs. Cirrus™, 53µm and 47µm for Cirrus™ vs. Stratus™, and 88µm and 79µm for Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD before correction, respectively.","When stratified by diagnoses, the values were 34 µm and 29 µm for Spectralis™ vs. Cirrus™, 53 µm and 47 µm for Cirrus™ vs. Stratus™, and 88 µm and 79 µm for Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD respectively, before correction.",yes
6220,2-131,2-131_v2_54@4,2-131_v1_49@4,"After manual correction, the values reduced to 17µm and 14µm Spectralis™ vs. Cirrus™ and 70µm and 61µm Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD, respectively.","After manual correction, the values reduced to 17 µm and 14 µm Spectralis™ vs. Cirrus™ and 70 µm and 61 µm Spectralis™ vs. Stratus™ for NV-AMD and NNV-AMD, respectively.",yes
6221,2-131,2-131_v2_54@5,2-131_v1_49@5,"The confidence interval widths, on average, were 5–10µm smaller than between an SD-OCT and TD-OCT machine.","The confidence interval widths, on average, were 5–10 µm smaller than when comparing between an SD-OCT and a TD-OCT machine.",yes
6222,2-131,2-131_v2_58@2,2-131_v1_53@2,One such common and clinically relevant issue is the presence of a random error in the identification of the inner and outer boundaries of the retina by the algorithm.,One such common and clinically relevant issue is the presence of random errors in the identification of the inner and outer boundaries of the retina by the OCT algorithm.,yes
6223,2-131,2-131_v2_59@0,2-131_v1_54@0,Reasons for differences in our error rates compared to previous include a lack of standard definition of an algorithm error.,Reasons for differences in our error rates compared to previous studies include the lack of standard definition of an algorithm error.,yes
6224,2-131,2-131_v2_59@3,2-131_v1_54@3,"These differences may be due to the fact that our study was prospective and while acquiring scans, the operators tried their best to ensure no errors occurred during scan acquisition.","These differences may be due to the fact that our study was prospective and, while acquiring scans, the operators tried their best to ensure no errors occurred during scan acquisition.",yes
6225,2-131,2-131_v2_60@2,2-131_v1_55@2,"For example, more than 50% of the Spectralis™ scans resulted in a 10µm or less change in the central subfield thickness.","For example, most of the Spectralis™ scans resulted in a 10 µm or less change in the central subfield thickness.",yes
6226,2-131,2-131_v2_62@1,2-131_v1_55@4,"Correction reduced the difference of the thickness measurements between the two SD-OCT devices to less than 20um; in some cases as noted above, the difference was no longer statistically significant.","Correction reduced the difference of the thickness measurements between the two SD-OCT devices to less than 20 µm; in some cases as noted above, the difference was no longer statistically significant.",yes
6227,2-131,2-131_v2_62@2,2-131_v1_55@5,"Other studies in normal and pathologic eyes including DME and macular degeneration have also demonstrated that the difference in retinal thickness between the SD machines can be attributed to the differences in segmentation of the automated algorithms <REF-7> , <REF-10> , <REF-12> .","Other studies in normal and pathologic eyes including Diabetic Macular Edema (DME) and macular degeneration have also demonstrated that the difference in retinal thickness between the SD machines can be attributed to the differences in segmentation of the automated algorithms <REF-7> , <REF-10> , <REF-11> .",yes
6228,2-131,2-131_v2_16@1,2-131_v1_11@1,Informed consent was obtained from study subjects.,Written Informed consent was obtained from study subjects.,yes
6229,2-131,2-131_v2_63@3,2-131_v1_56@3,"The disease difference can be attributed to the pathology of NV-AMD disrupting the outer border, which makes it difficult for the automated algorithm to accurately segment the retinal layers <REF-13> , <REF-14> .","This difference between diseases can be attributed to the pathology of NV-AMD disrupting the outer border, which makes it difficult for the automated algorithm to accurately segment the retinal layers <REF-12> , <REF-13> .",yes
6230,2-131,2-131_v2_63@7,2-131_v1_56@5,"Previous studies on normal eyes have reported a high repeatability of measurements with Spectralis™, with differences between repeated measurements being within 1µm <REF-12> , <REF-16> .","Previous studies on normal eyes have reported a high repeatability of measurements with Spectralis™, with differences between repeated measurements being within 1 µm <REF-11> , <REF-14> .",yes
6231,2-131,2-131_v2_63@8,2-131_v1_56@6,"For Stratus™ OCT images, other studies have found central subfield repeatability values in patients with NV-AMD to be 50µm and 32–35µm for NNV-AMD patients after correction/exclusion of scans with errors <REF-8> , <REF-17> ; our study confirms this finding.","For Stratus™ OCT images, other studies have found central subfield repeatability values in patients with NV-AMD to be 50 µm and 32–35 µm for NNV-AMD patients after correction/exclusion of scans with errors <REF-8> , <REF-15> ; our study confirms this finding.",yes
6232,2-131,2-131_v2_63@9,2-131_v1_56@7,"There has been one other published study looking at the repeatability of Cirrus™ OCT in NV-AMD, which found a central subfield repeatability value of 42um before correction and 26µm after exclusion of scans with significant segmentation errors <REF-18> .","There has been one other published study looking at the repeatability of Cirrus™ OCT in NV-AMD, which found a central subfield repeatability value of 42 µm before correction and 27 µm after exclusion <REF-16> .",yes
6233,2-131,2-131_v2_18@0,2-131_v1_13@0,Patients with confirmed diagnosis of AMD were enrolled in the study.,Patients with a confirmed diagnosis of AMD were enrolled in the study.,yes
6234,2-131,2-131_v2_64@1,2-131_v1_57@1,"For NV-AMD, Cirrus had higher coefficients after correction, and for NNV-AMD, Cirrus™ had lower coefficients as compared to Stratus™.","For NV-AMD, Cirrus™ had higher coefficients after correction, and for NNV-AMD, Cirrus™ had lower coefficients compared to Stratus™.",yes
6235,2-131,2-131_v2_65@2,2-131_v1_58@2,We found that 95% confidence intervals were narrower as compared to an SD-OCT and TD-OCT and correcting the algorithm errors further narrowed the intervals.,We found that 95% confidence intervals were narrower compared to an SD-OCT and TD-OCT and correcting the algorithm errors further narrowed the intervals.,yes
6236,2-131,2-131_v2_18@2,2-131_v1_13@2,Patients under treatment with intravitreal injections of anti-vascular endothelial growth factor (VEGF) agents were also allowed to participate in the study.,Patients under treatment with intravitreal injections of anti-Vascular Endothelial Growth Factor (VEGF) agents were also allowed to participate in the study.,yes
6237,2-131,2-131_v2_65@4,2-131_v1_58@4,This is mostly likely due to the effects of manually correcting the Spectralis™ images and that both machines have similar scanning technologies.,This is mostly likely due to the effects of manually correcting the Spectralis™ images and the fact that both machines have similar scanning technologies.,yes
6238,2-131,2-131_v2_65@6,2-131_v1_58@6,"Jaffe et al. reported similar results looking at NV-AMD, with limits of agreements being approximately 225um between a SD-OCT and TD-OCT <REF-7> .","Jaffe et al. reported similar results looking at NV-AMD, with limits of agreements being approximately 225 µm between a SD-OCT and TD-OCT <REF-7> .",yes
6239,2-131,2-131_v2_65@7,2-131_v1_58@7,The poor agreement warrants caution for clinicians when trying to use the data from different machines interchangeably especially in the central 1mm of retina since most clinicians.,The poor agreement suggests that clinicians should exercise caution when trying to use the data from different machines interchangeably.,yes
6240,2-131,2-131_v2_66@2,2-131_v1_59@1,The version of software used for the Stratus™ images did not allow correction of segmentation errors and therefore these images had to be excluded from the analysis.,"First, the software version for the Stratus™ images would not allow correction of images.",yes
6241,2-131,2-131_v2_66@3,2-131_v1_59@3,Two independent graders manually corrected all the images; this may have resulted in some inaccuracies in segmentation line correction.,"In addition, two people independently manually corrected the images, resulting in inaccuracies in segmentation line correction.",yes
6242,2-131,2-131_v2_66@1,2-131_v1_59@4,All images were taken at a single imaging center; this might have introduced some bias.,"Finally, the images were only taken at one imaging center, which could have resulted in bias.",yes
6243,2-131,2-131_v2_67@1,2-131_v1_60@1,Spectralis™ had the lowest COR values.,Spectralis™ had the lowest coefficient of repeatability values.,yes
6244,2-131,2-131_v2_19@0,2-131_v1_14@0,"Patients were scanned twice by certified OCT operators on a TD-OCT device (Stratus™ OCT) and two SD-OCT devices (Spectralis™, and Cirrus™ OCT) machines in random order and with 5–10 minutes between each device.",Patients were scanned twice by certified OCT operators on a TD-OCT device (Stratus™ OCT) and two SD-OCT devices (Spectralis™ and Cirrus™ OCT) machines in random order and with 5–10 minutes between each device.,yes
6245,2-131,2-131_v2_20@0,2-131_v1_15@0,Optical Coherence Tomography,Optical coherence tomography,yes
6246,2-131,2-131_v2_21@0,2-131_v1_16@0,"One TD-OCT machine, Stratus™ (software version 4), and two SD-OCT machines, Spectralis™ (software version 5.0 I and Cirrus™ (software version 5.0.0.326) were used.","One TD-OCT machine, Stratus™ (software version 4), and two SD-OCT machines, Spectralis™ (software version 5.01) and Cirrus™ (software version 5.0.0.326) were used.",yes
6247,2-131,2-131_v2_21@3,2-131_v1_16@3,"Using the Stratus™, two fast macular thickness maps (FMTP) were acquired from each eye.","Using the Stratus™, two Fast Macular Thickness Maps (FMTM) were acquired from each eye.",yes
6252,2-131,2-131_v2_24@0,2-131_v1_19@0,Scans from each of the three devices were reviewed at the Ocular Imaging Research and Reading Center at the Stanley M. Truhlsen Eye Institute by two independent graders.,Scans from each of the three devices were reviewed at the Retinal Imaging Research and Reading Center at the Wilmer Eye Institute by independent graders.,yes
6253,2-131,2-131_v2_24@1,2-131_v1_19@1,Segmentation errors due to incorrect identification of inner and outer retinal boundaries by automated algorithms in the Spectralis™ and Cirrus™ devices were identified and manually corrected by these graders.,Incorrect identification of inner and outer retinal boundaries by automated algorithms in Spectralis™ and Cirrus™ devices was manually corrected.,yes
6254,2-131,2-131_v2_24@2,2-131_v1_19@2,Stratus™ images could not be corrected due to the lack of editing capabilities in the operating system provided with the machine at the time of conducting the study.,Stratus™ images could not be corrected due to the lack of editing capabilities in the operating system provided with the machine at the time the study was conducted.,yes
6255,2-131,2-131_v2_24@3,2-131_v1_19@3,Only 5 patients required corrections and were excluded from the analysis.,Only five patients required corrections and were excluded from the analysis.,yes
6256,2-131,2-131_v2_24@5,2-131_v1_19@5,"Meanwhile each device identifies the inner limiting membrane (ILM) as the inner boundary of retina, identification of the outer boundary is different for each device.","Whereas each device identifies the inner limiting membrane (ILM) as the inner boundary of retina, identification of the outer boundary is different for each device.",yes
6257,2-131,2-131_v2_7@0,2-131_v1_3@1,Procedure : OCT scans were taken simultaneously using one TD-OCT and two SD-OCT devices.,OCT scans were taken simultaneously using one TD-OCT and two SD-OCT devices.,yes
6258,2-131,2-131_v2_25@0,2-131_v1_20@0,"Whenever the foveal center could be identified, grids were repositioned for scans with off-center positioning of the ETDRS grid.","Whenever the foveal center could be identified, grids were repositioned for scans with off-center positioning of the Early Treatment Diabetic Retinopathy Study (ETDRS) grid.",yes
6259,2-131,2-131_v2_8@0,2-131_v1_3@2,Main Outcome Measures : Macular thickness measurements were assessed before and after correction of the algorithm by constructing Bland-Altman plots for agreement and calculating intraclass correlation coefficients (ICCs) and coefficients of repeatability (COR) to evaluate intraclass repeatability.,Macular thickness measurements were assessed before and after correction of the OCT algorithm by constructing Bland-Altman plots for agreement and calculating intraclass correlation coefficients (ICCs) and coefficients of repeatability (COR) to evaluate intraclass repeatability.,yes
6260,2-131,2-131_v2_26@0,2-131_v1_21@0,The retinal thickness measurements of the nine standard ETDRS subfields ( Appendix A illustrates the nine-subfield abbreviations) were recorded from each device before and after correcting the errors in the scans algorithm.,The retinal thickness measurements of the nine standard ETDRS subfields ( Figure S1 illustrates the nine-subfield abbreviations) were recorded from each device before and after correcting the errors in the scans’ algorithm.,yes
6261,2-131,2-131_v2_28@4,2-131_v1_23@3,Statistical significance of difference in thickness before and after correction of images across devices was determined via student’s t-test with α = 0.05 with Bonferroni correction for multiple comparisons.,The statistical significance of difference in thickness before and after correction of images across devices was determined via the student’s t-test with α = 0.05 with Bonferroni correction for multiple comparisons.,yes
,2-131,2-131_v2_60@3,2-131_v1_25@2,Krebs et al. have also previously reported no significant differences in retinal thickness measurements before and after correction of segmentation errors of scans taken using Cirrus™ <REF-11> .,Males had a mean of 76.3 with a range of 61 to 90 years.,no
,2-131,2-131_v2_60@3,2-131_v1_25@3,Krebs et al. have also previously reported no significant differences in retinal thickness measurements before and after correction of segmentation errors of scans taken using Cirrus™ <REF-11> .,Females had a mean of 76.83 with a range of 53 to 90 years.,no
,2-131,2-131_v2_60@3,2-131_v1_7@4,Krebs et al. have also previously reported no significant differences in retinal thickness measurements before and after correction of segmentation errors of scans taken using Cirrus™ <REF-11> .,Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,no
,2-131,2-131_v2_60@3,2-131_v1_44@0,Krebs et al. have also previously reported no significant differences in retinal thickness measurements before and after correction of segmentation errors of scans taken using Cirrus™ <REF-11> .,"Overall Spectralis™ had the lowest COR, with values ranging from 5–30 µm.",no
,2-131,2-131_v2_60@3,2-131_v1_9@1,Krebs et al. have also previously reported no significant differences in retinal thickness measurements before and after correction of segmentation errors of scans taken using Cirrus™ <REF-11> .,"To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.",no
,2-131,2-131_v2_60@3,2-131_v1_56@4,Krebs et al. have also previously reported no significant differences in retinal thickness measurements before and after correction of segmentation errors of scans taken using Cirrus™ <REF-11> .,"At this point, we are not aware of any previous study looking at the repeatability of Spectralis™ images in AMD.",no
,2-131,2-131_v2_60@3,2-131_v1_59@2,Krebs et al. have also previously reported no significant differences in retinal thickness measurements before and after correction of segmentation errors of scans taken using Cirrus™ <REF-11> .,"However, very few images needed to be corrected.",no
,2-131,2-131_v2_61@0,2-131_v1_25@2,"The differences in the mean thickness values before and after correction in scans taken using Spectralis™ were most obvious in the central subfields of the retina (C1, N1, S1, T1, and I1) with the peripheral subfields being spared (N2, S2, T2 and I2).",Males had a mean of 76.3 with a range of 61 to 90 years.,no
,2-131,2-131_v2_61@0,2-131_v1_25@3,"The differences in the mean thickness values before and after correction in scans taken using Spectralis™ were most obvious in the central subfields of the retina (C1, N1, S1, T1, and I1) with the peripheral subfields being spared (N2, S2, T2 and I2).",Females had a mean of 76.83 with a range of 53 to 90 years.,no
,2-131,2-131_v2_61@0,2-131_v1_7@4,"The differences in the mean thickness values before and after correction in scans taken using Spectralis™ were most obvious in the central subfields of the retina (C1, N1, S1, T1, and I1) with the peripheral subfields being spared (N2, S2, T2 and I2).",Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,no
,2-131,2-131_v2_61@0,2-131_v1_44@0,"The differences in the mean thickness values before and after correction in scans taken using Spectralis™ were most obvious in the central subfields of the retina (C1, N1, S1, T1, and I1) with the peripheral subfields being spared (N2, S2, T2 and I2).","Overall Spectralis™ had the lowest COR, with values ranging from 5–30 µm.",no
,2-131,2-131_v2_61@0,2-131_v1_9@1,"The differences in the mean thickness values before and after correction in scans taken using Spectralis™ were most obvious in the central subfields of the retina (C1, N1, S1, T1, and I1) with the peripheral subfields being spared (N2, S2, T2 and I2).","To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.",no
,2-131,2-131_v2_61@0,2-131_v1_56@4,"The differences in the mean thickness values before and after correction in scans taken using Spectralis™ were most obvious in the central subfields of the retina (C1, N1, S1, T1, and I1) with the peripheral subfields being spared (N2, S2, T2 and I2).","At this point, we are not aware of any previous study looking at the repeatability of Spectralis™ images in AMD.",no
,2-131,2-131_v2_61@0,2-131_v1_59@2,"The differences in the mean thickness values before and after correction in scans taken using Spectralis™ were most obvious in the central subfields of the retina (C1, N1, S1, T1, and I1) with the peripheral subfields being spared (N2, S2, T2 and I2).","However, very few images needed to be corrected.",no
,2-131,2-131_v2_61@1,2-131_v1_25@2,This may be attributed to the fact that the pathology of AMD is located centrally and therefore pathology related inaccuracies in segmentation are more likely to occur in these subfields.,Males had a mean of 76.3 with a range of 61 to 90 years.,no
,2-131,2-131_v2_61@1,2-131_v1_25@3,This may be attributed to the fact that the pathology of AMD is located centrally and therefore pathology related inaccuracies in segmentation are more likely to occur in these subfields.,Females had a mean of 76.83 with a range of 53 to 90 years.,no
,2-131,2-131_v2_61@1,2-131_v1_7@4,This may be attributed to the fact that the pathology of AMD is located centrally and therefore pathology related inaccuracies in segmentation are more likely to occur in these subfields.,Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,no
,2-131,2-131_v2_61@1,2-131_v1_44@0,This may be attributed to the fact that the pathology of AMD is located centrally and therefore pathology related inaccuracies in segmentation are more likely to occur in these subfields.,"Overall Spectralis™ had the lowest COR, with values ranging from 5–30 µm.",no
,2-131,2-131_v2_61@1,2-131_v1_9@1,This may be attributed to the fact that the pathology of AMD is located centrally and therefore pathology related inaccuracies in segmentation are more likely to occur in these subfields.,"To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.",no
,2-131,2-131_v2_61@1,2-131_v1_56@4,This may be attributed to the fact that the pathology of AMD is located centrally and therefore pathology related inaccuracies in segmentation are more likely to occur in these subfields.,"At this point, we are not aware of any previous study looking at the repeatability of Spectralis™ images in AMD.",no
,2-131,2-131_v2_61@1,2-131_v1_59@2,This may be attributed to the fact that the pathology of AMD is located centrally and therefore pathology related inaccuracies in segmentation are more likely to occur in these subfields.,"However, very few images needed to be corrected.",no
,2-131,2-131_v2_63@4,2-131_v1_25@2,Krebs et al. evaluated the repeatability of retinal thickness measurements using Spectralis™ and Cirrus™ in patients with AMD.,Males had a mean of 76.3 with a range of 61 to 90 years.,no
,2-131,2-131_v2_63@4,2-131_v1_25@3,Krebs et al. evaluated the repeatability of retinal thickness measurements using Spectralis™ and Cirrus™ in patients with AMD.,Females had a mean of 76.83 with a range of 53 to 90 years.,no
,2-131,2-131_v2_63@4,2-131_v1_7@4,Krebs et al. evaluated the repeatability of retinal thickness measurements using Spectralis™ and Cirrus™ in patients with AMD.,Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,no
,2-131,2-131_v2_63@4,2-131_v1_44@0,Krebs et al. evaluated the repeatability of retinal thickness measurements using Spectralis™ and Cirrus™ in patients with AMD.,"Overall Spectralis™ had the lowest COR, with values ranging from 5–30 µm.",no
,2-131,2-131_v2_63@4,2-131_v1_9@1,Krebs et al. evaluated the repeatability of retinal thickness measurements using Spectralis™ and Cirrus™ in patients with AMD.,"To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.",no
,2-131,2-131_v2_63@4,2-131_v1_56@4,Krebs et al. evaluated the repeatability of retinal thickness measurements using Spectralis™ and Cirrus™ in patients with AMD.,"At this point, we are not aware of any previous study looking at the repeatability of Spectralis™ images in AMD.",no
,2-131,2-131_v2_63@4,2-131_v1_59@2,Krebs et al. evaluated the repeatability of retinal thickness measurements using Spectralis™ and Cirrus™ in patients with AMD.,"However, very few images needed to be corrected.",no
,2-131,2-131_v2_63@5,2-131_v1_25@2,For images taken using Spectralis™ the mean difference between repeated measurements was found to be within 11µm before correction and within 1µm after correction.,Males had a mean of 76.3 with a range of 61 to 90 years.,no
,2-131,2-131_v2_63@5,2-131_v1_25@3,For images taken using Spectralis™ the mean difference between repeated measurements was found to be within 11µm before correction and within 1µm after correction.,Females had a mean of 76.83 with a range of 53 to 90 years.,no
,2-131,2-131_v2_63@5,2-131_v1_7@4,For images taken using Spectralis™ the mean difference between repeated measurements was found to be within 11µm before correction and within 1µm after correction.,Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,no
,2-131,2-131_v2_63@5,2-131_v1_44@0,For images taken using Spectralis™ the mean difference between repeated measurements was found to be within 11µm before correction and within 1µm after correction.,"Overall Spectralis™ had the lowest COR, with values ranging from 5–30 µm.",no
,2-131,2-131_v2_63@5,2-131_v1_9@1,For images taken using Spectralis™ the mean difference between repeated measurements was found to be within 11µm before correction and within 1µm after correction.,"To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.",no
,2-131,2-131_v2_63@5,2-131_v1_56@4,For images taken using Spectralis™ the mean difference between repeated measurements was found to be within 11µm before correction and within 1µm after correction.,"At this point, we are not aware of any previous study looking at the repeatability of Spectralis™ images in AMD.",no
,2-131,2-131_v2_63@5,2-131_v1_59@2,For images taken using Spectralis™ the mean difference between repeated measurements was found to be within 11µm before correction and within 1µm after correction.,"However, very few images needed to be corrected.",no
,2-131,2-131_v2_63@6,2-131_v1_25@2,For images taken using Cirrus™ the mean difference between repeated measurements was found to be within 6µm before correction and within 4µm after correction <REF-15> .,Males had a mean of 76.3 with a range of 61 to 90 years.,no
,2-131,2-131_v2_63@6,2-131_v1_25@3,For images taken using Cirrus™ the mean difference between repeated measurements was found to be within 6µm before correction and within 4µm after correction <REF-15> .,Females had a mean of 76.83 with a range of 53 to 90 years.,no
,2-131,2-131_v2_63@6,2-131_v1_7@4,For images taken using Cirrus™ the mean difference between repeated measurements was found to be within 6µm before correction and within 4µm after correction <REF-15> .,Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,no
,2-131,2-131_v2_63@6,2-131_v1_44@0,For images taken using Cirrus™ the mean difference between repeated measurements was found to be within 6µm before correction and within 4µm after correction <REF-15> .,"Overall Spectralis™ had the lowest COR, with values ranging from 5–30 µm.",no
,2-131,2-131_v2_63@6,2-131_v1_9@1,For images taken using Cirrus™ the mean difference between repeated measurements was found to be within 6µm before correction and within 4µm after correction <REF-15> .,"To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.",no
,2-131,2-131_v2_63@6,2-131_v1_56@4,For images taken using Cirrus™ the mean difference between repeated measurements was found to be within 6µm before correction and within 4µm after correction <REF-15> .,"At this point, we are not aware of any previous study looking at the repeatability of Spectralis™ images in AMD.",no
,2-131,2-131_v2_63@6,2-131_v1_59@2,For images taken using Cirrus™ the mean difference between repeated measurements was found to be within 6µm before correction and within 4µm after correction <REF-15> .,"However, very few images needed to be corrected.",no
,2-131,2-131_v2_66@4,2-131_v1_25@2,"In addition, in a subset of patients that had a difference in the severity of disease, both eyes were included in the analysis; this may also have resulted in possible bias.",Males had a mean of 76.3 with a range of 61 to 90 years.,no
,2-131,2-131_v2_66@4,2-131_v1_25@3,"In addition, in a subset of patients that had a difference in the severity of disease, both eyes were included in the analysis; this may also have resulted in possible bias.",Females had a mean of 76.83 with a range of 53 to 90 years.,no
,2-131,2-131_v2_66@4,2-131_v1_7@4,"In addition, in a subset of patients that had a difference in the severity of disease, both eyes were included in the analysis; this may also have resulted in possible bias.",Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,no
,2-131,2-131_v2_66@4,2-131_v1_44@0,"In addition, in a subset of patients that had a difference in the severity of disease, both eyes were included in the analysis; this may also have resulted in possible bias.","Overall Spectralis™ had the lowest COR, with values ranging from 5–30 µm.",no
,2-131,2-131_v2_66@4,2-131_v1_9@1,"In addition, in a subset of patients that had a difference in the severity of disease, both eyes were included in the analysis; this may also have resulted in possible bias.","To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.",no
,2-131,2-131_v2_66@4,2-131_v1_56@4,"In addition, in a subset of patients that had a difference in the severity of disease, both eyes were included in the analysis; this may also have resulted in possible bias.","At this point, we are not aware of any previous study looking at the repeatability of Spectralis™ images in AMD.",no
,2-131,2-131_v2_66@4,2-131_v1_59@2,"In addition, in a subset of patients that had a difference in the severity of disease, both eyes were included in the analysis; this may also have resulted in possible bias.","However, very few images needed to be corrected.",no
,2-131,2-131_v2_66@5,2-131_v1_25@2,The Cirrus device that was used to capture the images did not have eye tracking and may have led to the slightly larger COR values when compared to Spectralis.,Males had a mean of 76.3 with a range of 61 to 90 years.,no
,2-131,2-131_v2_66@5,2-131_v1_25@3,The Cirrus device that was used to capture the images did not have eye tracking and may have led to the slightly larger COR values when compared to Spectralis.,Females had a mean of 76.83 with a range of 53 to 90 years.,no
,2-131,2-131_v2_66@5,2-131_v1_7@4,The Cirrus device that was used to capture the images did not have eye tracking and may have led to the slightly larger COR values when compared to Spectralis.,Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,no
,2-131,2-131_v2_66@5,2-131_v1_44@0,The Cirrus device that was used to capture the images did not have eye tracking and may have led to the slightly larger COR values when compared to Spectralis.,"Overall Spectralis™ had the lowest COR, with values ranging from 5–30 µm.",no
,2-131,2-131_v2_66@5,2-131_v1_9@1,The Cirrus device that was used to capture the images did not have eye tracking and may have led to the slightly larger COR values when compared to Spectralis.,"To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.",no
,2-131,2-131_v2_66@5,2-131_v1_56@4,The Cirrus device that was used to capture the images did not have eye tracking and may have led to the slightly larger COR values when compared to Spectralis.,"At this point, we are not aware of any previous study looking at the repeatability of Spectralis™ images in AMD.",no
,2-131,2-131_v2_66@5,2-131_v1_59@2,The Cirrus device that was used to capture the images did not have eye tracking and may have led to the slightly larger COR values when compared to Spectralis.,"However, very few images needed to be corrected.",no
,2-131,2-131_v2_28@0,2-131_v1_25@2,No formal sample size calculation was performed before the conduct of the study.,Males had a mean of 76.3 with a range of 61 to 90 years.,no
,2-131,2-131_v2_28@0,2-131_v1_25@3,No formal sample size calculation was performed before the conduct of the study.,Females had a mean of 76.83 with a range of 53 to 90 years.,no
,2-131,2-131_v2_28@0,2-131_v1_7@4,No formal sample size calculation was performed before the conduct of the study.,Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,no
,2-131,2-131_v2_28@0,2-131_v1_44@0,No formal sample size calculation was performed before the conduct of the study.,"Overall Spectralis™ had the lowest COR, with values ranging from 5–30 µm.",no
,2-131,2-131_v2_28@0,2-131_v1_9@1,No formal sample size calculation was performed before the conduct of the study.,"To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.",no
,2-131,2-131_v2_28@0,2-131_v1_56@4,No formal sample size calculation was performed before the conduct of the study.,"At this point, we are not aware of any previous study looking at the repeatability of Spectralis™ images in AMD.",no
,2-131,2-131_v2_28@0,2-131_v1_59@2,No formal sample size calculation was performed before the conduct of the study.,"However, very few images needed to be corrected.",no
,2-131,2-131_v2_41@7,2-131_v1_32@3,There were no significant (p<.05) differences between the mean FTH of the first and second scans for each of the three devices.,77% of the differences were less than 48 μm and 50% were less than 10 μm.,no
6307,2-180,2-180_v2_17@5,2-180_v1_16@4,"Secondly, even though traditional rTMS is considered to be focal, magnetic fields of lower intensity are delivered outside of the focal area <REF-33> , raising the possibility that low intensity stimulation may be contributing to therapeutic effects by acting on interconnected brain regions.","Secondly, even though traditional rTMS is considered to be focal, magnetic fields of lower intensity are delivered outside of the focal area, raising the possibility that low intensity stimulation may be contributing to therapeutic effects by acting on interconnected brain regions.",yes
6308,2-180,2-180_v2_18@0,2-180_v1_17@0,"We chose a complex pattern of stimulation that is based on biomimetic principles (described in detail <REF-23> 59.9-ms trains of 20 pulses at 3 different frequencies as follows: 1 min warm-up at 6.71 Hz, 8 min treatment at 10.1 Hz, and 1 min cool down at 6.26 Hz) and has been shown to induce structural changes in mice <REF-23> .","We chose a complex pattern of stimulation because it has been shown to induce structural changes in mice <REF-19> (59.9-ms trains of 20 pulses at 3 different frequencies as follows: 1 min warm-up at 6.71 Hz, 8 min treatment at 10.1 Hz, and 1 min cool down at 6.26 Hz.",yes
6309,2-180,2-180_v2_18@1,2-180_v1_17@1,The pulse was monophasic with a 300µs rise time and 100µs fall time.,"The pulse duration was 200 µs, which is within the range used in human rTMS.",yes
6312,2-180,2-180_v2_4@1,2-180_v1_4@1,"Importantly, rTMS induces long term potentiation (LTP) in rodent hippocampus in vitro <REF-2> and several sessions of high-frequency rTMS increases the capacity to induce LTP compared to untreated controls, suggesting it may also regulate metaplasticity <REF-3> , <REF-4> .","Importantly, rTMS induces long term potentiation (LTP) in rodent hippocampus in vitro <REF-2> and several sessions of high-frequency rTMS increases the capacity to induce LTP (metaplasticity) compared to untreated controls <REF-3> , <REF-4> .",yes
6313,2-180,2-180_v2_38@3,2-180_v1_37@3,"Although it is difficult to draw conclusions from the null results presented here, the absence of observed behavioural and structural change is consistent with previously reported rTMS specificity for abnormal systems <REF-8> , <REF-23> .","The absence of observed behavioural and structural change is consistent with previously reported rTMS specificity for abnormal systems 8 , <REF-19> .",yes
6314,2-180,2-180_v2_38@4,2-180_v1_37@4,The lack of adverse effects in our long-term study suggests that up to 5 weeks of daily sessions of low intensity pulsed magnetic field stimulation at the parameters used in this study appears safe to use in healthy participants.,"Furthermore, the lack of adverse effects in our long term study contributes evidence that rTMS is safe to use in healthy control participants <REF-20> .",yes
6315,2-180,2-180_v2_40@3,2-180_v1_39@3,"This is consistent with previous reports that long-term rTMS effects are specific to abnormal brain circuitry: two weeks of rTMS improved visual tracking, visual electrophysiological function and topographical accuracy in a different strain of mice (ephrin-A2A5 -/- double knockouts) with abnormal circuitry but produced no lasting effects in wildtype mice <REF-23> .","This is consistent with previous reports that long-term rTMS effects are specific to abnormal brain circuitry: two weeks of rTMS improved visual tracking, visual electrophysiological function and topographical accuracy in mice with abnormal circuitry but produced no lasting effects in wildtype mice <REF-19> .",yes
6316,2-180,2-180_v2_0@0,2-180_v1_0@0,Long term delivery of pulsed magnetic fields does not alter visual discrimination learning or dendritic spine density in the mouse CA1 pyramidal or dentate gyrus neurons,Long term delivery of pulsed magnetic fields does not improve learning or alter dendritic spine density in the mouse hippocampus,yes
6317,2-180,2-180_v2_40@6,2-180_v1_39@6,"Furthermore, there is a lack of studies assessing cognitive effects of long-term rTMS in patients together with healthy controls, which presents a large gap in knowledge <REF-8> .","To our knowledge, there have been no studies assessing cognitive effects of long-term rTMS in patients and healthy controls, which presents a large gap in knowledge.",yes
6318,2-180,2-180_v2_42@1,2-180_v1_41@1,"Importantly, we found similar spine densities in sham wildtype and ephrin-A2 -/- mice, suggesting that spine density is not solely dependent on ephrin-A2, in agreement with the literature <REF-19> – <REF-21> .","Importantly, we found similar spine densities in sham wildtype and ephrin-A2 -/- mice, suggesting that if present, deficits in spines are subtle in ephrin-A2 -/- mice.",yes
6319,2-180,2-180_v2_8@1,2-180_v1_41@2,"In addition, although ephrin-A2 is expressed in the mouse hippocampus throughout life and has been implicated in its topographic organisation <REF-19> , <REF-20> , there is no evidence that it is involved in synaptic plasticity or spine dynamics <REF-21> .","Although ephrin-A2 is expressed in the mouse hippocampus throughout life and has been implicated in its topographic organisation <REF-35> , <REF-36> , to our knowledge, there is no evidence that ephrin-A2 is involved in synaptic plasticity or spine dynamics.",yes
6320,2-180,2-180_v2_42@2,2-180_v1_41@4,"As such, the null effect of rTMS on dendritic spine density may be attributed to the absence of both a specific spine and learning deficit in both wildtype and ephrin-A2 -/- mice.","As such, the null effect of rTMS on dendritic spine density is in line with our behavioural results and may be attributed to the absence of a specific spine deficit for rTMS to correct in both wildtype and ephrin-A2 -/- mice.",yes
6321,2-180,2-180_v2_42@5,2-180_v1_42@1,"However, it is surprising that dendritic spine density remains unaffected after long-term stimulation, given our previous results using the same stimulation parameters, demonstrating structural reorganisation in abnormal axon terminals following multiple stimulations, but not a single rTMS session <REF-23> .","However, it is surprising that dendritic spine density remains unaffected after long-term stimulation, given our previous results using the same stimulation parameters, demonstrating structural reorganisation in abnormal axon terminals following multiple, but not single rTMS stimulation sessions <REF-19> .",yes
6322,2-180,2-180_v2_6@2,2-180_v1_6@2,"Given the significant structural changes induced in the mouse visual system following repeated stimulation sessions, and evidence for structural changes in the human brain <REF-17> , we hypothesised that a similar long-term rTMS regime in combination with a hippocampus-dependent learning task, would rescue impaired learning strategies previously found in ephrin-A2 -/- mice <REF-18> and alter spine density in the hippocampus.","Given the significant structural changes induced in the mouse visual system following repeated stimulation sessions, we hypothesised that a similar long-term rTMS regime in combination with a hippocampal learning task, would rescue impaired learning strategies previously found in ephrin-A2 -/- mice <REF-17> and alter spine density in the hippocampus.",yes
6323,2-180,2-180_v2_2@2,2-180_v1_2@2,"We delivered 5 weeks of daily pulsed rTMS stimulation to adult ephrin-A2 -/- and wildtype (C57BI/6j) mice (n=10 per genotype) undergoing a visual learning task and analysed learning performance, as well as spine density, in the dentate gyrus molecular and CA1 pyramidal cell layers in Golgi-stained brain sections.","We delivered 5 weeks of daily pulsed rTMS stimulation to ephrin-A2 -/- and wildtype mice (n=10 per genotype) undergoing a visual learning task and analysed learning performance, as well as spine density, in the dentate gyrus molecular and CA1 pyramidal cell layers in Golgi-stained brain sections.",yes
6324,2-180,2-180_v2_8@3,2-180_v1_7@1,"Although mice of both genotypes learned the task, their performance remained suboptimal due to lack of motivation to obtain food rewards through insufficient food restriction <REF-22> and neither learning behaviour, nor hippocampal spine density were affected by long term rTMS.","Although the mice learned the task, their performance remained suboptimal due to lack of motivation to obtain food rewards through insufficient food restriction <REF-18> and neither learning behaviour, nor hippocampal spine density were affected by long term rTMS.",yes
6325,2-180,2-180_v2_11@6,2-180_v1_10@5,"Mice were age matched, aged 8–10 weeks old (equivalent to young sexually mature adult in humans) when commencing the experiment.","Mice were age matched, aged 8–10 weeks old when commencing the experiment.",yes
6326,2-180,2-180_v2_11@7,2-180_v1_10@6,"For the duration of the study, mice were kept in standard caging in a controlled environment (12/12 light/dark cycle; temperature 22°C±2°C, separated into cages with clear plastic walls (17 cm × 19 cm base, 16 cm high) based on sex and genotype (2–4 per cage)).","For the duration of the study, mice were kept in standard caging in a controlled environment (12/12 light/dark cycle; temperature 22°C±2°C, separated into cages with clear plastic walls (17 cm x 19 cm base, 16 cm high) based on sex and genotype (2–4 per cage)).",yes
,2-180,2-180_v2_40@7,2-180_v1_41@3,"Because of the lack of understanding of fundamental interactions between rTMS and behaviour, it would be of great interest to perform an exhaustive battery of behavioural tests in healthy wildtype mice (and eventually in animal models of disease) in conjunction with various rTMS protocols.","Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .",no
,2-180,2-180_v2_43@0,2-180_v1_41@3,"Alternatively, the timing of rTMS delivery relative to the behavioural task may have influenced the outcome of our experiments.","Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .",no
,2-180,2-180_v2_45@0,2-180_v1_41@3,"Importantly, we are conscious of the limitations of our rodent scaled rTMS delivery device which may have contributed to the lack of effect observed here.","Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .",no
,2-180,2-180_v2_45@0,2-180_v1_39@7,"Importantly, we are conscious of the limitations of our rodent scaled rTMS delivery device which may have contributed to the lack of effect observed here.","Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.",no
,2-180,2-180_v2_19@3,2-180_v1_39@7,"Unlike in cat studies, the coil was not in direct contact with the mouse head but was held as close as possible to the scalp (~1mm).","Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.",no
,2-180,2-180_v2_43@3,2-180_v1_39@7,"With such pre-treatment, an effect of rTMS might even have been detected in improved performances on a day to day basis.","Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.",no
,2-180,2-180_v2_43@1,2-180_v1_41@3,"Here we stimulated after the task, however rTMS might have been more effective if delivered before.","Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .",no
,2-180,2-180_v2_19@5,2-180_v1_39@7,"Unlike in the cat study, stereotaxic delivery was not attempted because the dimensions of the coil ensured that the field reached the entire dorsal hippocampus, which in the mouse, is relatively large in proportion to total brain size.","Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.",no
,2-180,2-180_v2_45@3,2-180_v1_39@7,Additional effort in designing appropriate small animal rTMS coils is urgently needed to improve the construct validity of animal rTMS research.,"Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.",no
,2-180,2-180_v2_19@3,2-180_v1_41@3,"Unlike in cat studies, the coil was not in direct contact with the mouse head but was held as close as possible to the scalp (~1mm).","Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .",no
,2-180,2-180_v2_8@0,2-180_v1_39@7,We used ephrin-A2 -/- mice because they have previously been shown to have a specific learning deficit <REF-18> .,"Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.",no
,2-180,2-180_v2_11@4,2-180_v1_39@7,Randomised littermates were not used because the breeding colony was structured to produce ephrin-A2/A5 double knockout mice for other studies and no WT littermates were obtained.,"Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.",no
,2-180,2-180_v2_40@8,2-180_v1_39@7,Subsequent anatomical and physiological analyses could then be carried out to elucidate the neural mechanisms of rTMS and gain insight into the treatment of human disease.,"Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.",no
,2-180,2-180_v2_40@8,2-180_v1_41@3,Subsequent anatomical and physiological analyses could then be carried out to elucidate the neural mechanisms of rTMS and gain insight into the treatment of human disease.,"Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .",no
,2-180,2-180_v2_11@4,2-180_v1_41@3,Randomised littermates were not used because the breeding colony was structured to produce ephrin-A2/A5 double knockout mice for other studies and no WT littermates were obtained.,"Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .",no
,2-180,2-180_v2_19@5,2-180_v1_41@3,"Unlike in the cat study, stereotaxic delivery was not attempted because the dimensions of the coil ensured that the field reached the entire dorsal hippocampus, which in the mouse, is relatively large in proportion to total brain size.","Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .",no
,2-180,2-180_v2_19@4,2-180_v1_41@3,The gap between the coil and the head does not attenuate the field because magnetic fields decrease with distance from the source but are not modified by air or biological tissue (e.g. skin/scalp <REF-36> ).,"Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .",no
,2-180,2-180_v2_8@0,2-180_v1_41@3,We used ephrin-A2 -/- mice because they have previously been shown to have a specific learning deficit <REF-18> .,"Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .",no
6423,2-278,2-278_v2_2@0,2-278_v1_2@0,"In an article in Science on ""Bayes' Theorem in the 21st Century"", Bradley Efron uses Bayes' theorem to calculate the probability that twins are identical given that the sonogram shows twin boys.","In a recent article in Science on ""Bayes' Theorem in the 21st Century"", Bradley Efron uses Bayes' theorem to calculate the probability that twins are identical given that the sonogram shows twin boys.",yes
6424,2-278,2-278_v2_5@2,2-278_v1_5@2,We argue that this example is relatively useless in illustrating Bayesian data analysis.,"We argue that this example is not only flawed, but useless in illustrating Bayesian data analysis because it does not rely on any data.",yes
6425,2-278,2-278_v2_5@5,2-278_v1_5@4,"Rather, Efron combines different pieces of expert knowledge from the doctor and genetics using Bayes’ theorem.","Instead, Efron combines different pieces of expert knowledge from the doctor and genetics using Bayes’ theorem.",yes
6426,2-278,2-278_v2_6@1,2-278_v1_7@0,"If we use the data point together with an uninformative uniform prior on P(A|B) (see Box 1 ) to determine the probability of identical twins given the twins are two boys, we obtain, with 95% certainty, a probability of between 0.01 and 0.84; if we use a highly informative prior based on information from the doctor and genetics, we obtain a probability of between 0.49 and 0.51.","If we use the data point together with an uninformative uniform prior on P(A|B) to determine the probability of identical twins given the twins are two boys (see Box 1 ), we obtain, with 95% certainty, a probability of between 0.01 and 0.84; if we use a highly informative prior based on information from the doctor and genetics, we obtain a probability of between 0.49 and 0.51.",yes
6427,2-278,2-278_v2_2@2,2-278_v1_2@2,"While we agree that the choice of the prior is essential, we argue that the calculations on identical twins give a biased impression of the influence of uninformative priors in Bayesian data analyses.","We argue that this conclusion is problematic because Efron's example on identical twins does not use data, hence it is not Bayesian statistics; his priors are not appropriate and are not uninformative; and using the available data point and an uninformative prior actually leads to a reasonable posterior distribution.",yes
,2-278,2-278_v2_6@0,2-278_v1_6@0,"Efron’s example can be rearranged so that it fits a more realistic situation in statistical data analysis, albeit with a very low sample size: consider the twin boys that, as Efron casually mentions, turned out to be fraternal, as a random sample from the larger population of twin boys and try to draw inference about the proportion of identical twins among the population of twin boys (note that this approach is different from the calculations provided by Efron).",We also have other more technical issues with Efron’s example.,no
,2-278,2-278_v2_6@3,2-278_v1_6@1,"We think that to illustrate the influence of non-informative priors on results of Bayesian data analyses, such an approach would be fairer than the calculations given by Efron.",Efron interprets the term P(A) on the right side of the equation (see sidebar in Efron 2013a <REF-1> ) as the prior on the probability that twins are identical.,no
,2-278,2-278_v2_6@0,2-278_v1_5@8,"Efron’s example can be rearranged so that it fits a more realistic situation in statistical data analysis, albeit with a very low sample size: consider the twin boys that, as Efron casually mentions, turned out to be fraternal, as a random sample from the larger population of twin boys and try to draw inference about the proportion of identical twins among the population of twin boys (note that this approach is different from the calculations provided by Efron).","Indeed, without data, Efron’s example is not Bayesian statistics and his conclusion about Bayesian statistics based on this example is unjustified.",no
,2-278,2-278_v2_6@3,2-278_v1_6@4,"We think that to illustrate the influence of non-informative priors on results of Bayesian data analyses, such an approach would be fairer than the calculations given by Efron.","First, however, the parameter of interest is P(A|B) rather than P(A) according to Efron’s study question (see sidebar in Efron 2013a <REF-1> ), thus the focus should be on the appropriate prior for P(A|B).",no
,2-278,2-278_v2_6@0,2-278_v1_5@3,"Efron’s example can be rearranged so that it fits a more realistic situation in statistical data analysis, albeit with a very low sample size: consider the twin boys that, as Efron casually mentions, turned out to be fraternal, as a random sample from the larger population of twin boys and try to draw inference about the proportion of identical twins among the population of twin boys (note that this approach is different from the calculations provided by Efron).","Although there is one data point (a couple is due to be parents of twin boys, and the twins are fraternal), Efron does not use it to update prior knowledge.",no
6471,2-282,2-282_v2_25@0,2-282_v1_23@0,"Cortical activation occurring during the preparation of movement is detected by the EEG module thanks to a method based on the event-related desynchronization/synchronization (ERD/ERS) phenomenon <REF-7> , <REF-9> , <REF-10> .",Cortical activation occurring during the preparation of movement is detected by the EEG module thanks to a method based on the event-related desynchronization/synchronization (ERD/ERS) phenomenon <REF-6> .,yes
6472,2-282,2-282_v2_25@1,2-282_v1_23@1,We extracted a QP for the detection of intentionality of movement <REF-11> by considering: (i) the changes in the β²/α and β/α ratio (representing bursts of β-γ frequencies) during the pre-movement period; (ii) an appropriate threshold indicating which peaks of ratios are actually followed by a movement (and therefore may be considered as a predictor of movement); (iii) the number of movements executed.,"By considering: (i) the changes in the β²/α and β/α ratio (representing bursts of β-γ frequencies) during the pre-movement period; (ii) an appropriate threshold indicating which peaks of ratios are actually followed by a movement (and therefore may be considered as a predictor of movement); (iii) the number of movements executed, we extracted a QP for the detection of intentionality of movement <REF-8> .",yes
6473,2-282,2-282_v2_26@0,2-282_v1_24@0,"Upsampled EEG data were processed with a Hamming window of 256 samples, using an overlap of 250 in the time domain.","Upsampled EEG data were processed with a Hamming window of 256 samples, an overlap of 250 in the time domain.",yes
6474,2-282,2-282_v2_29@1,2-282_v1_27@1,"The pre-movement period (lasting 2 seconds) was defined according to the acoustic order given to the patients and the detection of the beginning of movement via the gyroscopes, by considering 2 seconds back from the point of detection of the beginning of movement.",The pre-movement period (lasting 2 seconds) was defined according to the acoustic order given to the patients and the detection of the beginning of movement via the gyroscopes.,yes
6475,2-282,2-282_v2_33@4,2-282_v1_31@4,"The peaks (β/α and β²/α ratios) higher than a defined threshold were considered as indicators of a potential voluntary movement <REF-11> , given that they represent the detection of the cortical motor preparation of the movement <REF-14> , <REF-15> .","The peaks (β/α and β²/α ratios) higher than a defined threshold were considered as indicators of a potential voluntary movement <REF-8> , given that they represent the detection of the cortical motor preparation of the movement.",yes
6476,2-282,2-282_v2_33@6,2-282_v1_31@6,EEG QP is the geometric mean of the probability of movement (true positive stimulations) and the percentage of movements predicted <REF-11> .,QP is the geometric mean of the probability of movement (true positive stimulations) and the percentage of movements predicted <REF-8> .,yes
6477,2-282,2-282_v2_35@2,2-282_v1_33@2,"The Welch's averaged modified periodogram method was used to compute the magnitude squared coherence of an EEG channel and an EMG electrode along the frequency subbands <REF-7> , <REF-12> , <REF-16> .","The Welch's averaged modified periodogram method was used to compute the magnitude squared coherence of an EEG channel and an EMG electrode along the frequency band <REF-6> , <REF-9> , <REF-10> .",yes
6478,2-282,2-282_v2_45@0,2-282_v1_43@0,The rise of low frequencies was used for a mathematical modelling which considered:,The rise of low frequencies was used for a mathematical modeling which considered:,yes
6479,2-282,2-282_v2_47@0,2-282_v1_45@0,- the max PSD in the low-frequency band over time.,- the max PSD in the low frequency band over time.,yes
6480,2-282,2-282_v2_76@0,2-282_v1_75@0,E. Simulation of ERD/ERS: which thresholds would be required to obtain high EEG QPs?,E. Simulation of ERD/ERS: which thresholds would be required to obtain high QPs?,yes
6481,2-282,2-282_v2_94@1,2-282_v1_90@1,"When all the possible combinations of EEG/EMG/kinematic QPs are tested, the probability trees could yield an optimal efficiency.","When all the possible combinations of EEG/EMG/kinematic are tested, the probability tree could yield an optimal efficiency.",yes
6482,2-282,2-282_v2_94@2,2-282_v1_90@2,"An exhaustive list of the probabilities for the entire amount of data recorded is not provided, because of the huge amount of time that this analysis requires in terms of data processing.","An exhaustive list of the probabilities for the entire amount of data recorded is not provided, because of the huge amount of time that this analysis requires.",yes
6485,2-282,2-282_v2_100@4,2-282_v1_95@4,"We propose that the EEG QP can be complemented by the QPs extracted from the cortico-muscular coherence and the QPs obtained by the analysis of the changes in the kinematic signals, which occur prior to the voluntary movements.",We propose that the QP can be complemented by the cortico-muscular coherence and the analysis of the changes in the kinematic signals which occur prior to the voluntary movements.,yes
6486,2-282,2-282_v2_100@5,2-282_v1_95@5,We suggest a fusion of the QP parameters in order to increase the likelihood to detect the intentionality of movement.,We suggest a fusion of the parameters.,yes
6487,2-282,2-282_v2_100@10,2-282_v1_95@9,"This is in agreement with adaptive methods which are being developed currently with the goal of improving the classification algorithms for BCI system in order to extract EEG patterns related to a cognitive or motor status <REF-6> , <REF-27> .",This is in agreement with adaptive methods which are being developed currently with the goal of improving the classifiers <REF-5> .,yes
6488,2-282,2-282_v2_100@11,2-282_v1_95@10,"Our approach will have to be tested in a large sample of patients in the future, in order to demonstrate its real clinical usefulness in daily practice.","Also, our approach will have to be tested in a large sample of patients in the future in order to demonstrate its real clinical usefulness in daily practice.",yes
6491,2-282,2-282_v2_5@4,2-282_v1_5@2,"Why the use of a multimodal detection of the intentionality of movement? Although the potential for BCIs in neurological disorders is huge, the applicability of current BCI systems has been limited by several factors <REF-6> .","Why use a multimodal detection of the intentionality of movement? Although the potential for BCIs in neurological disorders is huge, the applicability of current BCI systems has been limited by several factors <REF-5> .",yes
6492,2-282,2-282_v2_5@5,2-282_v1_5@3,One of them is the poor performance of BCIs based on EEG analysis only (also due to: inter-individual differences in the detectability of movement-related EEG-activity; differences in the way BCI users can voluntary modify their brain activity; and the fact that brain atrophy and neuroplastic changes occurring in patients affected with movement disorders makes it difficult to generalize EEG markers).,One of them is the poor performance of BCIs based on conventional EEG analysis.,yes
6493,2-282,2-282_v2_7@1,2-282_v1_7@1,"From each module, acting during different time-windows (EEG, kinematic and corticomuscular (described in detail in sections C–F) quality parameters (QPs) for the detection of the intentionality of movement or for the early detection of movement are extracted.","From each module (described in detail in sections B–F) quality parameters (QPs) for the detection of the intentionality of movement or the early detection of movement are extracted (we extract QPs and probabilities of stimulation for the EEG and Kinematic modules, and probability of stimulation for the corticomuscular module).",yes
6494,2-282,2-282_v2_11@0,2-282_v1_11@0,"Acquisition of data was carried out on 4 neurological patients exhibiting a bilateral upper limb tremor (combinations of rest, postural and/or kinetic tremor), following approval of the Ethical Committee of ULB – Hôpital Erasme ( Table 1 ).","Acquisition of data was carried out on 4 neurological patients exhibiting a bilateral upper limb tremor (combinations of rest, postural and/or kinetic tremor), following approval of the Ethical Committee of ULB – Hôpital Erasme.",yes
6495,2-282,2-282_v2_11@4,2-282_v1_11@4,Mean age of the patients was 62±20 years.,Mean age of the patients was 62 ± 20 years.,yes
6496,2-282,2-282_v2_11@6,2-282_v1_11@6,The ADL-T24 score range was 3–20/24 <REF-7> .,The ADL-T24 score range was 4–17/24 <REF-6> .,yes
6497,2-282,2-282_v2_11@7,2-282_v1_11@7,The Schwab and England ADL score ranged from 50 to 80% <REF-8> .,The Schwab and England ADL score ranged from 50 to 100% <REF-7> .,yes
6498,2-282,2-282_v2_15@5,2-282_v1_13@3,"After hearing an acoustic signal, they prepared themselves for the execution of movement by mental imagery of the movement.","After (1) hearing an acoustic signal, the patient (2) prepared themselves mentally for the execution of movement and (3) performed the task.",yes
6499,2-282,2-282_v2_15@9,2-282_v1_13@8,"The nomenclature used for the recorded files–as reported in figures- is “pppFNnn” standing for patient’s code, task executed (""Finger-to-nose"") and run number, respectively.","The nomenclature used for the recorded files is “pppFNnn"" standing for patient’s code, task executed (""Finger-to-nose"") and run number, respectively.",yes
6500,2-282,2-282_v2_17@0,2-282_v1_15@0,"(i) IMU sensors (inertial measurement units: tri-axial gyroscopes, accelerometers, magnetometers).","(i) IMU sensors (inertial measurement units; tri-axial gyroscopes, accelerometers, magnetometers).",yes
6501,2-282,2-282_v2_21@1,2-282_v1_19@1,"In order to build a “movement window”, the signal from the magnetometer (which provides a very clean signal) is processed first.","In order to build a “movement window”, the signal from the magnetometer (which provides a very clean signal), is processed first.",yes
,2-282,2-282_v2_92@5,2-282_v1_7@2,"This would be done separately for each patient, thus taking into account the inter-individual variability.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_90@6,2-282_v1_7@2,"We would like to point out that in the study of Pfurtscheller et al. on single-trial classification of EEG and imagination <REF-20> , the frequency of the most reactive components was 11±0.4 Hz (mean±SD).",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_90@1,2-282_v1_7@2,"When a neurological patient with tremor is seated and assessed, he/she may exhibit a tremor of the head and trunk.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_94@6,2-282_v1_7@2,Our data provide a ground for the concept of multimodal approach developed for the early detection of the intentionality of movement.,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_29@2,2-282_v1_7@2,We decided to use a period of 2000 msec based on the available literature which considers that 2 seconds encompasses the preparation phase at the cortical level <REF-13> .,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_101@0,2-282_v1_7@2,Data availability,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_91@6,2-282_v1_7@2,"However, no technique has been widely accepted so far.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_94@9,2-282_v1_7@2,"In order to provide the most possible accurate BCI-driven FES system, each subject needs to be studied in order to define the best combination of QPs.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_94@11,2-282_v1_7@2,The system would take into account these features.,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_59@10,2-282_v1_7@2,This highlights the importance of our multimodal approach.,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_94@7,2-282_v1_7@2,The presented probability trees are general schemes.,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_90@5,2-282_v1_7@2,"For instance, we have seen patients with cerebellar disorders and orthostatic tremor in whom the sub-band 8–10 Hz was much less informative as compared with the sub-band 10–12 Hz.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_90@10,2-282_v1_7@2,We aim to pursue the use of narrow bands of frequency in multiple tasks.,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_90@2,2-282_v1_7@2,This tremor may be pretty stable or rather intermittent.,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_90@8,2-282_v1_7@2,"Although the desynchronized components were centered at 10.9 Hz±0.9 Hz, the synchronized components were narrow-banded, with higher frequencies at 12.0 Hz±1.0 Hz.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_95@0,2-282_v1_7@2,"Results obtained with the simulation study provide useful information about EEG QP in order to select patients more effectively for a BCI-based treatment, including rehabilitation.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_90@9,2-282_v1_7@2,We agree with the authors that the classification of single EEG trials improves when ERD and ERS patterns are combined for multiple tasks.,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_100@6,2-282_v1_7@2,The analysis of the corticomuscular coherence shows that this parameter alone cannot be used to predict voluntary motion and be implemented in a BCI.,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_95@2,2-282_v1_7@2,Future studies could take advantage of these findings to select the best neurological candidates on the basis of the ERD/ERS for BCI-based management.,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_5@3,2-282_v1_7@2,This phenomenon might be induced by a cortico-cerebellar activation during voluntary movements <REF-5> .,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_92@4,2-282_v1_7@2,"By doing so, the performance of the modules would be evaluated to find out which ones contribute most to a high detection rate.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_59@9,2-282_v1_7@2,"These patients exhibited reproducible low values for the cortico-muscular coherence, by contrast to reproducible high values for the other QPs.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_90@4,2-282_v1_7@2,"Therefore, we decided to have a close look to each of these bands.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_102@0,2-282_v1_7@2,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2014 Grimaldi G et al.,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_7@2,2-282_v1_7@2,QPs were calculated for each movement executed by the patients (one run contains several movements; see section B).,Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_94@12,2-282_v1_7@2,"By analysing a larger group of patients, we might identify subgroups of patients on the basis of the results of the probability trees.",Patients and the experimental procedure were as detailed in the next sections.,no
,2-282,2-282_v2_92@1,2-282_v1_7@2,"As such, our system would be a multimodal control unit, including an EEG-module like often used for BCIs, but also body modules to control a stimulation unit.",Patients and the experimental procedure were as detailed in the next sections.,no
7198,3-256,3-256_v2_76@3,3-256_v1_76@3,"Additionally, CR increased the plasma insulin content in GHR-KO mice [( p < 0.05), ( Figure 2F )].","Additionally, CR increased the plasma insulin content in GHR-KO mice [( p < 0.05, ( Figure 2F )].",yes
7199,3-256,3-256_v2_98@2,3-256_v1_98@2,"Our insulin sensitivity results in GHR-KO mice on 30% CR differed from those obtained in a previous study, showing that caloric restriction promotes euglycemia in GHR-KO mice ( Figure 2C ).",Our insulin sensitivity results in GHR-KO mice on 30% CR differed from those obtained in a previous study showing that caloric restriction promotes euglycemia in GHR-KO mice ( Figure 2C ).,yes
7200,3-256,3-256_v2_98@5,3-256_v1_98@5,"Moreover, data from macromolecular analysis of insulin signaling in GHR-KO mice on CR, including decreased insulin receptor (INSR) and thymoma viral proto-oncogene 1/protein kinase b (AKT1/PKB) concentrations in the skeletal musculature of GHR-KO’s on CR, and decreased phosphatidylinositol-4,5-bisphosphate 3-kinase (PI3K) subunits content in the livers of GHR-KO’s on CR, (all relative to GHR-KO’s on AL), also corroborate and portend decreased insulin sensitivity in GHR-KO mice on CR [ Bonkowski et al. , 2009 ].","Moreover, data from macromolecular analysis of insulin signaling in GHR-KO mice on CR, including decreased insulin receptor (INSR) and thymoma viral proto-oncogene 1/protein kinase b (AKT1/PKB) concentrations in the skeletal musculature of GHR-KO’s on CR, and decreased phosphatidylinositol-4,5-bisphosphate 3-kinase (PI3K) subunits content in the livers of GHR-KO’s on CR (relative to GHR-KO’s on AL), also corroborate and portend decreased insulin sensitivity in GHR-KO mice on CR [ Bonkowski et al. , 2009 ].",yes
7201,3-256,3-256_v2_98@6,3-256_v1_98@6,"Additionally, it is worth noting that a tight regulation of euglycemia would be more consistent with health and survival than a predilection for hypoglycemia [ Tan & Flanagan, 2013 ]; thus, “improving health”, as CR has been broadly documented as doing, might mean preventing the innate endocrinological/metabolic derangements that are merely coincident with the longevity of the GHR-KO mouse.","Additionally, it is worth noting that a tight regulation of euglycemia would be more consistent with health and survival than a predilection for hypoglycemia [ Tan & Flanagan, 2013 ], thus “improving health”, as CR has been broadly documented as doing, might mean preventing the innate endocrinological/metabolic derangements that are merely coincident with the longevity of the GHR-KO mouse.",yes
7202,3-256,3-256_v2_99@1,3-256_v1_99@1,"We discovered that CR did not alter the metabolism or spontaneous activity of GHR-KO mice, and also revealed that CR has no effect on the anxiety or memory function of GHR-KO mice.",We discovered that CR did not alter the metabolism or spontaneous activity of GHR-KO mice and also revealed that CR has no effect on the anxiety or memory function of GHR-KO mice.,yes
7203,3-256,3-256_v2_103@0,3-256_v1_102@0,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2015 Arum O et al.,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2014 Arum O et al.,yes
7204,3-256,3-256_v2_16@0,3-256_v1_16@0,"The resulting mice had elements of a 129/Ola, a Balb/c, two C57Bl/6J, and two C3H/HeJ stocks; therefore, although lacking the methodological benefits of “reproducible genetic heterogeneity” [ Miller et al. , 1999 ], this stock possesses considerable genetic variation, and thus the results are likely applicable to other mouse populations.","The resulting mice had elements of a 129/Ola, a Balb/c, two C57Bl/6J, and two C3H/HeJ stocks; therefore, although lacking the methodological benefits of “reproducible genetic heterogeneity” [ Miller, et al. , 1999 ], this stock possesses considerable genetic variation, and thus the results are likely applicable to other mouse populations.",yes
,3-256,3-256_v2_76@3,3-256_v1_102@0,"Additionally, CR increased the plasma insulin content in GHR-KO mice [( p < 0.05), ( Figure 2F )].",The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2014 Arum O et al.,no
,3-256,3-256_v2_98@6,3-256_v1_98@2,"Additionally, it is worth noting that a tight regulation of euglycemia would be more consistent with health and survival than a predilection for hypoglycemia [ Tan & Flanagan, 2013 ]; thus, “improving health”, as CR has been broadly documented as doing, might mean preventing the innate endocrinological/metabolic derangements that are merely coincident with the longevity of the GHR-KO mouse.",Our insulin sensitivity results in GHR-KO mice on 30% CR differed from those obtained in a previous study showing that caloric restriction promotes euglycemia in GHR-KO mice ( Figure 2C ).,no
,3-256,3-256_v2_76@3,3-256_v1_98@2,"Additionally, CR increased the plasma insulin content in GHR-KO mice [( p < 0.05), ( Figure 2F )].",Our insulin sensitivity results in GHR-KO mice on 30% CR differed from those obtained in a previous study showing that caloric restriction promotes euglycemia in GHR-KO mice ( Figure 2C ).,no
,3-256,3-256_v2_98@5,3-256_v1_102@0,"Moreover, data from macromolecular analysis of insulin signaling in GHR-KO mice on CR, including decreased insulin receptor (INSR) and thymoma viral proto-oncogene 1/protein kinase b (AKT1/PKB) concentrations in the skeletal musculature of GHR-KO’s on CR, and decreased phosphatidylinositol-4,5-bisphosphate 3-kinase (PI3K) subunits content in the livers of GHR-KO’s on CR, (all relative to GHR-KO’s on AL), also corroborate and portend decreased insulin sensitivity in GHR-KO mice on CR [ Bonkowski et al. , 2009 ].",The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2014 Arum O et al.,no
,3-256,3-256_v2_103@0,3-256_v1_16@0,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2015 Arum O et al.,"The resulting mice had elements of a 129/Ola, a Balb/c, two C57Bl/6J, and two C3H/HeJ stocks; therefore, although lacking the methodological benefits of “reproducible genetic heterogeneity” [ Miller, et al. , 1999 ], this stock possesses considerable genetic variation, and thus the results are likely applicable to other mouse populations.",no
,3-256,3-256_v2_98@6,3-256_v1_98@5,"Additionally, it is worth noting that a tight regulation of euglycemia would be more consistent with health and survival than a predilection for hypoglycemia [ Tan & Flanagan, 2013 ]; thus, “improving health”, as CR has been broadly documented as doing, might mean preventing the innate endocrinological/metabolic derangements that are merely coincident with the longevity of the GHR-KO mouse.","Moreover, data from macromolecular analysis of insulin signaling in GHR-KO mice on CR, including decreased insulin receptor (INSR) and thymoma viral proto-oncogene 1/protein kinase b (AKT1/PKB) concentrations in the skeletal musculature of GHR-KO’s on CR, and decreased phosphatidylinositol-4,5-bisphosphate 3-kinase (PI3K) subunits content in the livers of GHR-KO’s on CR (relative to GHR-KO’s on AL), also corroborate and portend decreased insulin sensitivity in GHR-KO mice on CR [ Bonkowski et al. , 2009 ].",no
,3-256,3-256_v2_98@5,3-256_v1_98@6,"Moreover, data from macromolecular analysis of insulin signaling in GHR-KO mice on CR, including decreased insulin receptor (INSR) and thymoma viral proto-oncogene 1/protein kinase b (AKT1/PKB) concentrations in the skeletal musculature of GHR-KO’s on CR, and decreased phosphatidylinositol-4,5-bisphosphate 3-kinase (PI3K) subunits content in the livers of GHR-KO’s on CR, (all relative to GHR-KO’s on AL), also corroborate and portend decreased insulin sensitivity in GHR-KO mice on CR [ Bonkowski et al. , 2009 ].","Additionally, it is worth noting that a tight regulation of euglycemia would be more consistent with health and survival than a predilection for hypoglycemia [ Tan & Flanagan, 2013 ], thus “improving health”, as CR has been broadly documented as doing, might mean preventing the innate endocrinological/metabolic derangements that are merely coincident with the longevity of the GHR-KO mouse.",no
7320,4-143,4-143_v2_23@3,4-143_v1_23@3,"According to the TMT scores, thus, one may conclude that all of the children who had a concussion 5 to 8 days prior to their participation in this study did not seem to have neurocognitive impairments.","According to the TMT scores, thus, one may conclude that all these children who had a concussion 5 to 8 days prior to their participation in this study did not seem to have neurocognitive impairments.",yes
7321,4-143,4-143_v2_24@0,4-143_v1_24@0,"The patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.","Their patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.",yes
7322,4-143,4-143_v2_2@6,4-143_v1_2@6,Results showed that only one of the three concussed children showed a score from the trail making test that was worse than the scores obtained from control subjects.,Results showed that only one of the three concussed children showed a score from the trail making test that was worse than those obtained from control subjects.,yes
7323,4-143,4-143_v2_24@1,4-143_v1_24@1,"Specifically, the first block of DE in the adaptation session that was significantly different from the last block of DE of the baseline session occurred later in the patients than in the controls; and the rate of performance change was higher (i.e., slower adaptation) in the patients as well.","Specifically, the first block of DE in the adaptation session that is significantly different from the last block of DE of the baseline session occurred later in the patients than in the controls; and the rate of performance change was higher (i.e., slower adaptation) in the patients as well.",yes
7324,4-143,4-143_v2_25@2,4-143_v1_25@2,"Second, we did not collect any information regarding our subjects’ demographic and social statuses, which could be considered as potential confounding characteristics.","Also, we did not collect from our subjects any information regarding their demographic and social status, which could be considered as potential confounding characteristics.",yes
7325,4-143,4-143_v2_26@0,4-143_v1_26@0,"In conclusion, the results from this study provide preliminary data indicating that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.","In conclusion, the results from this study indicate that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.",yes
7326,4-143,4-143_v2_2@9,4-143_v1_2@9,Our findings provide preliminary data that suggest that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.,Our findings collectively suggest that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.,yes
7327,4-143,4-143_v2_4@3,4-143_v1_4@3,"In this short observational/case study, we investigated the pattern of adaptation to a novel visuomotor condition in three children who suffered a concussion while playing sports, and compared their adaptation patterns to those obtained from three children without a concussion.","In this research, we investigated the pattern of adaptation to a novel visuomotor condition in three children who suffered a concussion while playing sports, and compared their adaptation patterns to those obtained from three children without a concussion.",yes
7328,4-143,4-143_v2_6@0,4-143_v1_6@0,"The purpose of this observational/case study was to determine qualitatively whether concussed children with neurocognitive impairments, as indicated by the TMT scores, would also demonstrate sensorimotor deficits, as indicated by the visuomotor adaptation patterns.","The purpose of this observational study was to determine whether concussed children with neurocognitive impairments, as indicated by the TMT scores, would also demonstrate sensorimotor deficits, as indicated by the visuomotor adaptation patterns.",yes
7329,4-143,4-143_v2_8@0,4-143_v1_8@0,"Three children (15 years old, one male (cc3)), who presented to the Emergency Department at the Children’s Hospital of Wisconsin within 24 hours from the time of injury and who received a diagnosis of concussion (Glasgow Coma Scale ≥ 14), participated in this study.","Three children (15 years old), who presented to the Emergency Department at the Children’s Hospital of Wisconsin within 24 hours from the time of injury and who received a diagnosis of concussion (Glasgow Coma Scale ≥ 14), participated in this study.",yes
7330,4-143,4-143_v2_8@4,4-143_v1_8@2,"Three children (12 (cc1), 14 (cc3) and 17 (cc2) years old, all males), who were recruited from the Milwaukee Metropolitan area, served as controls.","Three children (12, 14 and 17 years old), who were recruited from the Milwaukee Metropolitan area, served as controls.",yes
7331,4-143,4-143_v2_8@5,4-143_v1_8@3,"Selection criteria for subjects were the same for both patients and controls (except their concussion status), which were: subject was 10–17 years of age, regularly participated in an athletic activity, was English-speaking, was right handed, and had no neurological disease or peripheral disorder affecting movement of the right arm.","Selection criteria for subjects were the same between patients and controls (except their concussion status), which were: subject is 10–17 years of age, regularly participates in an athletic activity, is English-speaking, is right handed, and has no neurological disease or peripheral disorder affecting movement of the right arm.",yes
7332,4-143,4-143_v2_2@2,4-143_v1_2@2,"In this observational/case study, we investigated the association between neurocognitive function, assessed using a trail making test, and sensorimotor function, assessed using a visuomotor adaptation task, in three children who suffered from a concussion while playing sports.","In this observational study, we investigated the association between neurocognitive function, assessed using a trail making test, and sensorimotor function, assessed using a visuomotor adaptation task, in three children who suffered from a concussion while playing sports.",yes
7333,4-143,4-143_v2_14@0,4-143_v1_14@0,"Because of the nature of the present study (i.e., observational/qualitative/case study of concussed children), we only tried to recruit a small number of concussed children.","Because of the nature of the present study (i.e., observational/case study of concussed children), we only tried to recruit a small number of concussed children.",yes
,4-143,4-143_v2_23@3,4-143_v1_2@6,"According to the TMT scores, thus, one may conclude that all of the children who had a concussion 5 to 8 days prior to their participation in this study did not seem to have neurocognitive impairments.",Results showed that only one of the three concussed children showed a score from the trail making test that was worse than those obtained from control subjects.,no
,4-143,4-143_v2_2@2,4-143_v1_24@0,"In this observational/case study, we investigated the association between neurocognitive function, assessed using a trail making test, and sensorimotor function, assessed using a visuomotor adaptation task, in three children who suffered from a concussion while playing sports.","Their patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.",no
,4-143,4-143_v2_2@9,4-143_v1_26@0,Our findings provide preliminary data that suggest that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.,"In conclusion, the results from this study indicate that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.",no
,4-143,4-143_v2_8@5,4-143_v1_8@2,"Selection criteria for subjects were the same for both patients and controls (except their concussion status), which were: subject was 10–17 years of age, regularly participated in an athletic activity, was English-speaking, was right handed, and had no neurological disease or peripheral disorder affecting movement of the right arm.","Three children (12, 14 and 17 years old), who were recruited from the Milwaukee Metropolitan area, served as controls.",no
,4-143,4-143_v2_26@0,4-143_v1_24@0,"In conclusion, the results from this study provide preliminary data indicating that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.","Their patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.",no
,4-143,4-143_v2_2@6,4-143_v1_4@3,Results showed that only one of the three concussed children showed a score from the trail making test that was worse than the scores obtained from control subjects.,"In this research, we investigated the pattern of adaptation to a novel visuomotor condition in three children who suffered a concussion while playing sports, and compared their adaptation patterns to those obtained from three children without a concussion.",no
,4-143,4-143_v2_8@0,4-143_v1_2@6,"Three children (15 years old, one male (cc3)), who presented to the Emergency Department at the Children’s Hospital of Wisconsin within 24 hours from the time of injury and who received a diagnosis of concussion (Glasgow Coma Scale ≥ 14), participated in this study.",Results showed that only one of the three concussed children showed a score from the trail making test that was worse than those obtained from control subjects.,no
,4-143,4-143_v2_8@4,4-143_v1_25@2,"Three children (12 (cc1), 14 (cc3) and 17 (cc2) years old, all males), who were recruited from the Milwaukee Metropolitan area, served as controls.","Also, we did not collect from our subjects any information regarding their demographic and social status, which could be considered as potential confounding characteristics.",no
,4-143,4-143_v2_6@0,4-143_v1_24@0,"The purpose of this observational/case study was to determine qualitatively whether concussed children with neurocognitive impairments, as indicated by the TMT scores, would also demonstrate sensorimotor deficits, as indicated by the visuomotor adaptation patterns.","Their patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.",no
,4-143,4-143_v2_2@2,4-143_v1_8@0,"In this observational/case study, we investigated the association between neurocognitive function, assessed using a trail making test, and sensorimotor function, assessed using a visuomotor adaptation task, in three children who suffered from a concussion while playing sports.","Three children (15 years old), who presented to the Emergency Department at the Children’s Hospital of Wisconsin within 24 hours from the time of injury and who received a diagnosis of concussion (Glasgow Coma Scale ≥ 14), participated in this study.",no
,4-143,4-143_v2_14@0,4-143_v1_8@0,"Because of the nature of the present study (i.e., observational/qualitative/case study of concussed children), we only tried to recruit a small number of concussed children.","Three children (15 years old), who presented to the Emergency Department at the Children’s Hospital of Wisconsin within 24 hours from the time of injury and who received a diagnosis of concussion (Glasgow Coma Scale ≥ 14), participated in this study.",no
,4-143,4-143_v2_25@2,4-143_v1_24@0,"Second, we did not collect any information regarding our subjects’ demographic and social statuses, which could be considered as potential confounding characteristics.","Their patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.",no
,4-143,4-143_v2_24@0,4-143_v1_4@3,"The patterns of visuomotor adaptation observed in the patients, however, appear to be somewhat different from those observed in the controls.","In this research, we investigated the pattern of adaptation to a novel visuomotor condition in three children who suffered a concussion while playing sports, and compared their adaptation patterns to those obtained from three children without a concussion.",no
,4-143,4-143_v2_26@0,4-143_v1_8@2,"In conclusion, the results from this study provide preliminary data indicating that children with concussion may have sensorimotor impairments even when they do not seem to have neurocognitive impairments.","Three children (12, 14 and 17 years old), who were recruited from the Milwaukee Metropolitan area, served as controls.",no
7538,5-1356,5-1356_v2_2@5,5-1356_v1_2@5,We propose a statistical framework based on the Dirichlet-multinomial distribution that can discover changes in isoform usage between conditions and SNPs that affect relative expression of transcripts using these quantifications.,We propose a statistical framework based on the Dirichlet-multinomial distribution that can discover changes in isoform usage between conditions and SNPs that affect splicing outcome using these quantifications.,yes
7539,5-1356,5-1356_v2_17@1,5-1356_v1_15@1,"Similarly, separate modeling and testing of exon junctions ( Altrans <REF-27> ) or splicing events ( rMATS <REF-29> , GLiMMPS <REF-32> , Jia et al. <REF-33> , Montgomery et al. <REF-49> ) of a gene leads to non-independent statistical tests, although the full effect of this on calibration (e.g., controlling the rate of false discoveries) is not known.","Similarly, separate modeling and testing of exon junctions ( Altrans <REF-26> ) or splicing events ( rMATS <REF-28> , GLiMMPS <REF-30> , <REF-31> , <REF-47> ) of a gene leads to non-independent statistical tests, although the full effect of this on calibration (e.g., controlling the rate of false discoveries) is not known.",yes
7540,5-1356,5-1356_v2_24@1,5-1356_v1_26@1,"The mean and covariance matrix of random proportions Π are ( Π ) = γ / γ + = π and ( ∏ ) = { γ + diag ( γ ) − γ γ T } / { γ + 2 ( γ + + 1 ) } , respectively.","The mean and covariance matrix of random proportions Π are ( Π ) = γ/γ + = π and ( Π ) = {γ + diag( γ ) – γ γ T }/{ γ + 2 ( γ + + 1)}, respectively.",yes
7541,5-1356,5-1356_v2_26@0,5-1356_v1_29@0,"The mean of Y is unchanged at ( Y ) = {( Y | Π )} = ( m Π ) = m γ / γ + = m π , while the covariance matrix of Y is given by ( Y ) = cm {diag( π ) − ππ T }, where c = ( m + γ + )/(1+ γ + ) is an additional factor when representing the Dirichlet-multinomial covariance to the ordinary multinomial covariance.","The mean of Y is unchanged at ( Y ) = {( Y | Π )} = ( m Π ) = m γ /γ + = m π , while the covariance matrix of Y is given by ( Y ) = cm {diag( π ) – ππ T }, where c = ( m +γ + )/(1+γ + ) is an additional factor when representing the Dirichlet-multinomial covariance to the ordinary multinomial covariance.",yes
7542,5-1356,5-1356_v2_27@0,5-1356_v1_30@0,"We can represent the DM distribution using an alternative parameterization: π = γ / γ + and θ = 1/(1 + γ + ); then, the covariance of Y can be represented as ( Y ) = n {diag( π ) − ππ T } {1 + θ ( n − 1)}, where θ can be interpreted as a dispersion parameter.","We can represent the DM distribution using an alternative parameterization: π = γ /γ + and θ = 1/(1 + γ + ); then, the covariance of Y can be represented as ( Y ) = n {diag( π ) – ππ T }{1 + θ ( n – 1)}, where θ can be interpreted as a dispersion parameter.",yes
7543,5-1356,5-1356_v2_29@10,5-1356_v1_36@1,"In comparisons across c groups, the number of degrees of freedom is ( c − 1) × ( q − 1).","In comparisons across c groups, the number of degrees of freedom is ( c – 1) × ( q – 1).",yes
7544,5-1356,5-1356_v2_30@0,5-1356_v1_37@0,"In a DTU analysis, groups are defined by the design of an experiment and are the same for each gene.","In a DS analysis, groups are defined by the design of an experiment and are the same for each gene.",yes
7545,5-1356,5-1356_v2_30@1,5-1356_v1_37@1,"In tuQTL analyses, the aim is to find nearby (bi-allelic) SNPs associated with transcript usage of a gene.","In sQTL analyses, the aim is to find nearby (bi-allelic) SNPs associated with alternative splicing of a gene.",yes
7546,5-1356,5-1356_v2_30@3,5-1356_v1_37@3,"Thus, tuQTL analyses are similar to DTU analyses with the difference that multiple models are fitted and tested for each gene.","Thus, sQTL analyses are similar to DS analyses with the difference that multiple models are fitted and tested for each gene.",yes
7547,5-1356,5-1356_v2_30@4,5-1356_v1_37@4,"Additional challenges to be handled in tuQTL analyses include a large number of tests per gene with highly variable allele frequencies (models) and linkage disequilibrium, which can be accounted for in the multiple testing corrections.","Additional challenges to be handled in sQTL analyses include a large number of tests per gene with highly variable allele frequencies (models) and linkage disequilibrium, which can be accounted for in the multiple testing corrections.",yes
7548,5-1356,5-1356_v2_32@1,5-1356_v1_39@1,"Following the edgeR strategy <REF-1> , <REF-2> , <REF-53> , we propose multiple approaches for dispersion estimation, all based on the maximization and adjustment of the profile likelihood, since standard maximum likelihood (ML) is known to produce biased estimates as it tends to underestimate variance parameters by not allowing for the fact that other unknown parameters are estimated from the same data <REF-54> , <REF-55> .","Following the edgeR ideology <REF-1> , <REF-2> , <REF-51> , we propose multiple approaches for dispersion estimation, all based on the maximization and adjustment of the profile likelihood, since standard maximum likelihood (ML) is known to produce biased estimates as it tends to underestimate variance parameters by not allowing for the fact that other unknown parameters are estimated from the same data <REF-52> , <REF-53> .",yes
7549,5-1356,5-1356_v2_44@1,5-1356_v1_54@1,We performed simulations that correspond to a two-group comparison with no DTU (i.e. null model) where feature counts were generated from the DM distribution with identical parameters in both groups.,We performed simulations that correspond to a two-group comparison with no DS (i.e. null model) where feature counts were generated from the DM distribution with identical parameters in both groups.,yes
7550,5-1356,5-1356_v2_45@1,5-1356_v1_55@1,"Additionally, the median error of concentration estimates for Cox-Reid APL is always lower than for PL or maximum likelihood (ML) used in the dirmult package <REF-7> ( Figure 1C , Figure S2 ).","Additionally, the median error of concentration estimates for Cox-Reid adjusted profile likelihood is always lower than for PL or maximum likelihood (ML) used in the dirmult package <REF-7> ( Figure 1B , Figure S2 ).",yes
7551,5-1356,5-1356_v2_47@2,5-1356_v1_59@2,"Thus, sharing information about concentration (dispersion) between genes by moderating the gene-wise APL is applied.","Thus, sharing information about concentration (dispersion) between genes by moderating to the gene-wise (adjusted) profile likelihood is applied.",yes
7552,5-1356,5-1356_v2_51@0,5-1356_v1_62@0,"The aim of these analyses is to compare the performance of DRIMSeq against DEXSeq , which emerged among the top performing methods for detection of DTU from RNA-seq data <REF-23> .","The aim of these analyses is to compare the performance of DRIMSeq against DEXSeq , which emerged among the top performing methods for detection of DS from RNA-seq data <REF-22> .",yes
7553,5-1356,5-1356_v2_51@1,5-1356_v1_62@1,"For DRIMSeq , we consider different dispersion estimates: common, gene-wise with no moderation and with moderation-to-common and to-trended dispersion.","Additionally, for DRIMSeq , we consider different dispersion estimates: common, gene-wise with no moderation and with moderation-to-common and to-trended dispersion.",yes
7554,5-1356,5-1356_v2_55@0,5-1356_v1_65@0,"As noted by Soneson et al . <REF-23> , detecting DTU in human is harder than in fruit fly due to the more complex transcriptome of the first one; all methods have much smaller false discovery rate (FDR).","As noted by Soneson et al . <REF-22> , detecting DS in fruit fly is easier than in human; all methods have much smaller false discovery rate (FDR).",yes
7555,5-1356,5-1356_v2_56@4,5-1356_v1_66@4,"Additionally, we have considered how other filtering approaches affect DTU detection.","Additionally, we have considered how other filtering approaches affect DS detection.",yes
7556,5-1356,5-1356_v2_59@0,5-1356_v1_68@0,The p-value distributions highlight a better fit of the DM model to transcript counts compared to exonic counts (it is more uniform with a sharp peak close to zero).,The p-value distributions highlight a better fit of the DM model to transcript counts compared to exonic counts (it is uniform with a sharp peak close to zero).,yes
7557,5-1356,5-1356_v2_60@0,5-1356_v1_69@0,DS analyses on real datasets,DS analysis,yes
7558,5-1356,5-1356_v2_62@4,5-1356_v1_71@4,"To not diminish DEXSeq for its ability to fit more complex models, we run it using a model that does the four control versus three knock-down comparison with library layout as an additional covariate (model full 2).","To not diminish DEXSeq for its ability to fit GLMs, we run it using a model that does the four control versus three knock-down comparison with library layout as an additional covariate (model full glm).",yes
7559,5-1356,5-1356_v2_62@5,5-1356_v1_71@5,"For the adenocarcinoma data, we do a two-group comparison of six normal versus six cancer samples (model full) and for DEXSeq , we fit an extra model that takes into account patient effects (model full 2).","For the adenocarcinoma data, we do a two-group comparison of six normal versus six cancer samples (model full) and for DEXSeq , we fit an extra model that takes into account patient effects (model full glm).",yes
7560,5-1356,5-1356_v2_63@1,5-1356_v1_72@1,"Accounting for covariates in DEXSeq (model full 2) or performing the analysis on a subgroup without covariates (model full paired) results in more DS genes detected ( Figure S28 , Figure S29 and Figure S30 ).",Accounting for covariates in DEXSeq using the GLM or performing the analysis on a subgroup without covariates (model full paired) results in more DS genes detected ( Figure S26 and Figure S27 ).,yes
7561,5-1356,5-1356_v2_6@4,5-1356_v1_6@4,"Notably, the beta-binomial distribution, such as those used in differential methylation from bisulphite sequencing data <REF-13> – <REF-15> , represent a special case of the DM.","Notably, the beta-binomial distribution, such as those used in differential methylation from bisulphite sequencing data <REF-13> – <REF-15> , represents a special case of the DM.",yes
7562,5-1356,5-1356_v2_2@2,5-1356_v1_2@2,There are situations where differences (e.g. between normal and disease state) in the relative ratio of expressed isoforms may have significant phenotypic consequences or lead to prognostic capabilities.,"There are situations where the total abundance of gene expression does not change (e.g. between normal and disease state), but differences in the relative ratio of expressed isoforms may have significant phenotypic consequences or lead to prognostic capabilities.",yes
7563,5-1356,5-1356_v2_65@3,5-1356_v1_74@3,"In particular, the p-value distributions under the null indicate that DM fits better to transcript counts than exon counts ( Figure S14 , Figure S31 and Figure S32 ).",Also the distributions of p-values indicate that DM fits better to the transcript counts ( Figure S28 and Figure S29 ).,yes
7564,5-1356,5-1356_v2_66@2,5-1356_v1_75@2,"Of course, these validations represent an incomplete truth, and ideally, large-scale independent validation would be needed to comprehensively compare the DTU detection methods.","Of course, these validations represent an incomplete truth, and ideally, large-scale independent validation would be needed to comprehensively compare DS detection methods.",yes
7565,5-1356,5-1356_v2_70@0,5-1356_v1_79@0,"To demonstrate the application of DRIMSeq to tuQTL analysis, we use the data from the GEUVADIS project <REF-46> where 465 RNA-seq samples from lymphoblastoid cell lines were sequenced, 422 of which were sequenced in the 1000 Genomes Project Phase 1.","To demonstrate the application of DRIMSeq to sQTL analysis, we use the data from the GEUVADIS project <REF-44> where 465 RNA-seq samples from lymphoblastoid cell lines were sequenced, 422 of which were sequenced in the 1000 Genomes Project Phase 1.",yes
7566,5-1356,5-1356_v2_7@1,5-1356_v1_7@1,"Hence, gene expression can be viewed as a multivariate expression of transcripts or exons and such a representation allows the study of not only the overall gene expression, but also the expressed variant composition.","Hence, gene expression can be viewed as a multivariate expression of transcripts or exons, and such representation allows to study not only the overall gene expression, but how it is composed from different isoforms.",yes
7567,5-1356,5-1356_v2_71@4,5-1356_v1_80@4,"The numbers of tested and associated genes and tuQTLs are indicated in Figure 4 , Figure S38 and Figure S39 .","The numbers of tested and associated genes and sQTLs are indicated in Figure 4 , Figure S35 and Figure S36 .",yes
7570,5-1356,5-1356_v2_78@1,5-1356_v1_87@1,We have shown that this framework can be used for detecting differential isoform usage between experimental conditions as well as for identifying tuQTLs.,We have shown that this framework can be used for detecting differential isoform usage between experimental conditions as well as for identifying sQTLs.,yes
7571,5-1356,5-1356_v2_80@3,5-1356_v1_89@3,"Thus, we incorporate estimation techniques analogous to those used in negative binomial frameworks, such as Cox-Reid APL; perhaps not surprisingly, raw profile likelihood or standard maximum likelihood approaches do not perform as well in our tests of estimation performance.","Thus, we incorporate estimation techniques analogous to those used in negative binomial frameworks, such as Cox-Reid adjusted profile likelihood; perhaps not surprisingly, raw profile likelihood or standard maximum likelihood approaches do not perform as well in our tests of estimation performance.",yes
7572,5-1356,5-1356_v2_81@0,5-1356_v1_90@0,"In comparison to other available methods, DRIMSeq seems to be more conservative than both DEXSeq (using transcript counts) and sQTLseekeR , identifying fewer DTU genes and tuQTLs, respectively.","In comparison to other available methods, DRIMSeq seems to be more conservative than both DEXSeq (using transcript counts) and sQTLseekeR , identifying fewer DS genes and sQTLs, respectively.",yes
7573,5-1356,5-1356_v2_81@2,5-1356_v1_90@2,"Moreover, the sQTL associations detected by DRIMSeq have more enrichment in splicing-related features than sQTLseekeR tuQTLs, which could be due to the fact that DRIMSeq accounts for the higher uncertainty of lowly expressed genes by using transcript counts instead of transcript ratios.","Moreover, the sQTL associations detected by DRIMSeq have more enrichment in splicing-related features than sQTLseekeR sQTLs, which could be due to the fact that DRIMSeq accounts for the higher uncertainty of lowly expressed genes by using transcript counts instead of transcript ratios.",yes
7574,5-1356,5-1356_v2_83@2,5-1356_v1_92@2,"In the tuQTL analysis, it would allow studying samples from the pooled populations, with the subpopulation as a covariate, allowing larger sample sizes and increased power to detect interesting changes.","In the sQTL analysis, it would allow studying samples from the pooled populations, with the subpopulation as a covariate, allowing larger sample sizes and increased power to detect interesting changes.",yes
7575,5-1356,5-1356_v2_85@1,5-1356_v1_94@1,"In addition to the user friendly workflow for the DTU and tuQTL analyses, it provides plotting functions that generate diagnostic figures such as the dispersion versus mean gene expression figures and histograms of p-values.","In addition to the user friendly workflow for the DS and sQTL analyses, it provides plotting functions that generate diagnostic figures such as the dispersion versus mean gene expression figures and histograms of p-values.",yes
7576,5-1356,5-1356_v2_90@0,5-1356_v1_101@0,Data for the tuQTL analyses was downloaded from the GEUVADIS project website.,Data for the sQTL analyses was downloaded from the GEUVADIS project website.,yes
7577,5-1356,5-1356_v2_93@0,5-1356_v1_104@0,DRIMSeq analyses for this paper were done with version 0.3.3 available on Zenodo https://zenodo.org/record/53084 <REF-61> and Bioconductor release 3.2.,DRIMSeq analyses for this paper were done with version 0.3.3 available on Zenodo http://dx.doi.org/10.5281/zenodo.53084 <REF-59> and Bioconductor release 3.2.,yes
7578,5-1356,5-1356_v2_93@1,5-1356_v1_104@1,Source code used for the analyses in this paper is available on Zenodo https://zenodo.org/record/167305 <REF-62> .,Source code used for the analyses in this paper is available on Zenodo http://dx.doi.org/10.5281/zenodo.53059 <REF-60> .,yes
7579,5-1356,5-1356_v2_10@0,5-1356_v1_10@0,"The Dirichlet-multinomial framework, implemented as a Bioconductor R package called DRIMSeq , is applicable to both differential transcript usage (DTU) analysis between conditions and transcript usage quantitative trait loci (tuQTL) analysis.","The Dirichlet-multinomial framework, implemented as a Bioconductor R package called DRIMSeq , is oriented for both DS analysis and sQTL analysis.",yes
7580,5-1356,5-1356_v2_2@3,5-1356_v1_2@3,"Similarly, knowledge of single nucleotide polymorphisms (SNPs) that affect splicing, so-called splicing quantitative trait loci (sQTL) will help to characterize the effects of genetic variation on gene expression.","Similarly, knowledge of single nucleotide polymorphisms (SNPs) that affect splicing, so-called splicing quantitative trait loci (sQTL), will help to characterize the effects of genetic variation on gene expression.",yes
7581,5-1356,5-1356_v2_13@0,5-1356_v1_13@0,"DS can be studied in three main ways: as differential transcript usage (DTU) or, in a more local context, as differential exon or exon junction usage (DEU) or as specific splicing events (e.g., exon skipping), and all have their advantages and disadvantages.","DS can be studied in three main ways: as differential isoform usage or, in a more local context, as differential exon or exon junction usage or as specific splicing events (e.g., exon skipping), and all have their advantages and disadvantages.",yes
7582,5-1356,5-1356_v2_13@6,5-1356_v1_13@6,"This issue is captured in Altrans <REF-27> , which quantifies exon-links (exon junctions) or in MISO <REF-28> , rMATS <REF-29> , SUPPA <REF-30> and SGSeq <REF-31> , all of which calculate splicing event inclusion levels expressed as percentage spliced in (PSI).","This issue is captured in Altrans <REF-26> , which quantifies exon-links (exon junctions) or in MISO <REF-27> , rMATS <REF-28> and SUPPA <REF-29> , all of which calculate splicing event inclusion levels expressed as percentage spliced in (PSI).",yes
7583,5-1356,5-1356_v2_13@9,5-1356_v1_13@9,"However, there are (hypothetical) instances where changes in splicing pattern may not be captured by exon-level quantifications (Figure 1A in the paper by Monlog et al. <REF-35> ).","However, there are (hypothetical) instances where changes in splicing pattern may not be captured by exon-level quantifications (Figure 1A in Monlog et al. <REF-33> ).",yes
7584,5-1356,5-1356_v2_13@10,5-1356_v1_13@10,"Furthermore, detection of more complex transcript variations remains a challenge for exon junction or PSI methods (see Figure S5 in the paper by Ongen et al. <REF-27> ).","Furthermore, detection of more complex transcript variations remains a challenge for exon junction or PSI methods (see Figure S5 in Ongen et al. <REF-26> ).",yes
7585,5-1356,5-1356_v2_13@11,5-1356_v1_13@11,"Soneson et al. <REF-23> considered counting which accommodates various types of local splicing events, such as exon paths traced out by paired reads, junction counts or events that correspond to combinations of isoforms; in general, the default exon-based counting resulted in strongest performance for DS gene detection.","Soneson et al . <REF-22> considered counting to accommodate various types of local splicing events, such as exon paths traced out by paired reads, junction counts or events that correspond to combinations of isoforms; in general, the default exon-based counting resulted in strongest performance for DS gene detection.",yes
,5-1356,5-1356_v2_48@0,5-1356_v1_33@0,"In these simulations, the overall best performance of the DM model is achieved when dispersion parameters are estimated with the Cox-Reid APL and the dispersion moderation is applied.",Detecting DS and sQTLs with the DM model,no
,5-1356,5-1356_v2_48@0,5-1356_v1_7@0,"In these simulations, the overall best performance of the DM model is achieved when dispersion parameters are estimated with the Cox-Reid APL and the dispersion moderation is applied.",Expressed transcripts are generated by alternatively including exons into mature mRNAs.,no
,5-1356,5-1356_v2_48@0,5-1356_v1_8@2,"In these simulations, the overall best performance of the DM model is achieved when dispersion parameters are estimated with the Cox-Reid APL and the dispersion moderation is applied.","From an inference point of view, sQTL analyses are equivalent to DS analyses where covariates are defined by genotypes.",no
,5-1356,5-1356_v2_48@0,5-1356_v1_98@0,"In these simulations, the overall best performance of the DM model is achieved when dispersion parameters are estimated with the Cox-Reid APL and the dispersion moderation is applied.","Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,5-1356,5-1356_v2_48@1,5-1356_v1_33@0,"This strategy leads to p-value distributions that in most of the cases are closer to the uniform distribution ( Figure 1D , Figure S4 and Figure S11 ).",Detecting DS and sQTLs with the DM model,no
,5-1356,5-1356_v2_48@1,5-1356_v1_7@0,"This strategy leads to p-value distributions that in most of the cases are closer to the uniform distribution ( Figure 1D , Figure S4 and Figure S11 ).",Expressed transcripts are generated by alternatively including exons into mature mRNAs.,no
,5-1356,5-1356_v2_48@1,5-1356_v1_8@2,"This strategy leads to p-value distributions that in most of the cases are closer to the uniform distribution ( Figure 1D , Figure S4 and Figure S11 ).","From an inference point of view, sQTL analyses are equivalent to DS analyses where covariates are defined by genotypes.",no
,5-1356,5-1356_v2_48@1,5-1356_v1_98@0,"This strategy leads to p-value distributions that in most of the cases are closer to the uniform distribution ( Figure 1D , Figure S4 and Figure S11 ).","Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,5-1356,5-1356_v2_50@1,5-1356_v1_33@0,For each condition three replicates were simulated resulting in 3 versus 3 comparisons.,Detecting DS and sQTLs with the DM model,no
,5-1356,5-1356_v2_50@1,5-1356_v1_7@0,For each condition three replicates were simulated resulting in 3 versus 3 comparisons.,Expressed transcripts are generated by alternatively including exons into mature mRNAs.,no
,5-1356,5-1356_v2_50@1,5-1356_v1_8@2,For each condition three replicates were simulated resulting in 3 versus 3 comparisons.,"From an inference point of view, sQTL analyses are equivalent to DS analyses where covariates are defined by genotypes.",no
,5-1356,5-1356_v2_50@1,5-1356_v1_98@0,For each condition three replicates were simulated resulting in 3 versus 3 comparisons.,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,5-1356,5-1356_v2_74@3,5-1356_v1_33@0,"To further investigate characteristics of detected tuQTLs, we measured enrichment of splicing-related features as used in a previous comparison <REF-35> .",Detecting DS and sQTLs with the DM model,no
,5-1356,5-1356_v2_74@3,5-1356_v1_7@0,"To further investigate characteristics of detected tuQTLs, we measured enrichment of splicing-related features as used in a previous comparison <REF-35> .",Expressed transcripts are generated by alternatively including exons into mature mRNAs.,no
,5-1356,5-1356_v2_74@3,5-1356_v1_8@2,"To further investigate characteristics of detected tuQTLs, we measured enrichment of splicing-related features as used in a previous comparison <REF-35> .","From an inference point of view, sQTL analyses are equivalent to DS analyses where covariates are defined by genotypes.",no
,5-1356,5-1356_v2_74@3,5-1356_v1_98@0,"To further investigate characteristics of detected tuQTLs, we measured enrichment of splicing-related features as used in a previous comparison <REF-35> .","Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,5-1356,5-1356_v2_74@4,5-1356_v1_33@0,"This includes the location of tuQTLs within exons, within splice sites, in the surrounding of GWAS SNPs and distance to the closest exon.",Detecting DS and sQTLs with the DM model,no
,5-1356,5-1356_v2_74@4,5-1356_v1_7@0,"This includes the location of tuQTLs within exons, within splice sites, in the surrounding of GWAS SNPs and distance to the closest exon.",Expressed transcripts are generated by alternatively including exons into mature mRNAs.,no
,5-1356,5-1356_v2_74@4,5-1356_v1_8@2,"This includes the location of tuQTLs within exons, within splice sites, in the surrounding of GWAS SNPs and distance to the closest exon.","From an inference point of view, sQTL analyses are equivalent to DS analyses where covariates are defined by genotypes.",no
,5-1356,5-1356_v2_74@4,5-1356_v1_98@0,"This includes the location of tuQTLs within exons, within splice sites, in the surrounding of GWAS SNPs and distance to the closest exon.","Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,5-1356,5-1356_v2_7@0,5-1356_v1_33@0,Genes may express diverse transcript isoforms (mRNA variants) as a consequence of alternative splicing or due to the differences in transcription start sites and polyadenylation sites <REF-16> .,Detecting DS and sQTLs with the DM model,no
,5-1356,5-1356_v2_7@0,5-1356_v1_7@0,Genes may express diverse transcript isoforms (mRNA variants) as a consequence of alternative splicing or due to the differences in transcription start sites and polyadenylation sites <REF-16> .,Expressed transcripts are generated by alternatively including exons into mature mRNAs.,no
,5-1356,5-1356_v2_7@0,5-1356_v1_8@2,Genes may express diverse transcript isoforms (mRNA variants) as a consequence of alternative splicing or due to the differences in transcription start sites and polyadenylation sites <REF-16> .,"From an inference point of view, sQTL analyses are equivalent to DS analyses where covariates are defined by genotypes.",no
,5-1356,5-1356_v2_7@0,5-1356_v1_98@0,Genes may express diverse transcript isoforms (mRNA variants) as a consequence of alternative splicing or due to the differences in transcription start sites and polyadenylation sites <REF-16> .,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,5-1356,5-1356_v2_28@0,5-1356_v1_33@0,Detecting DTU and tuQTLs with the Dirichlet-multinomial model,Detecting DS and sQTLs with the DM model,no
,5-1356,5-1356_v2_28@0,5-1356_v1_7@0,Detecting DTU and tuQTLs with the Dirichlet-multinomial model,Expressed transcripts are generated by alternatively including exons into mature mRNAs.,no
,5-1356,5-1356_v2_28@0,5-1356_v1_8@2,Detecting DTU and tuQTLs with the Dirichlet-multinomial model,"From an inference point of view, sQTL analyses are equivalent to DS analyses where covariates are defined by genotypes.",no
,5-1356,5-1356_v2_28@0,5-1356_v1_98@0,Detecting DTU and tuQTLs with the Dirichlet-multinomial model,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,5-1356,5-1356_v2_90@0,5-1356_v1_62@1,Data for the tuQTL analyses was downloaded from the GEUVADIS project website.,"Additionally, for DRIMSeq , we consider different dispersion estimates: common, gene-wise with no moderation and with moderation-to-common and to-trended dispersion.",no
,5-1356,5-1356_v2_26@0,5-1356_v1_37@3,"The mean of Y is unchanged at ( Y ) = {( Y | Π )} = ( m Π ) = m γ / γ + = m π , while the covariance matrix of Y is given by ( Y ) = cm {diag( π ) − ππ T }, where c = ( m + γ + )/(1+ γ + ) is an additional factor when representing the Dirichlet-multinomial covariance to the ordinary multinomial covariance.","Thus, sQTL analyses are similar to DS analyses with the difference that multiple models are fitted and tested for each gene.",no
,5-1356,5-1356_v2_83@2,5-1356_v1_30@0,"In the tuQTL analysis, it would allow studying samples from the pooled populations, with the subpopulation as a covariate, allowing larger sample sizes and increased power to detect interesting changes.","We can represent the DM distribution using an alternative parameterization: π = γ /γ + and θ = 1/(1 + γ + ); then, the covariance of Y can be represented as ( Y ) = n {diag( π ) – ππ T }{1 + θ ( n – 1)}, where θ can be interpreted as a dispersion parameter.",no
,5-1356,5-1356_v2_47@2,5-1356_v1_7@1,"Thus, sharing information about concentration (dispersion) between genes by moderating the gene-wise APL is applied.","Hence, gene expression can be viewed as a multivariate expression of transcripts or exons, and such representation allows to study not only the overall gene expression, but how it is composed from different isoforms.",no
,5-1356,5-1356_v2_80@3,5-1356_v1_55@1,"Thus, we incorporate estimation techniques analogous to those used in negative binomial frameworks, such as Cox-Reid APL; perhaps not surprisingly, raw profile likelihood or standard maximum likelihood approaches do not perform as well in our tests of estimation performance.","Additionally, the median error of concentration estimates for Cox-Reid adjusted profile likelihood is always lower than for PL or maximum likelihood (ML) used in the dirmult package <REF-7> ( Figure 1B , Figure S2 ).",no
,5-1356,5-1356_v2_71@4,5-1356_v1_90@0,"The numbers of tested and associated genes and tuQTLs are indicated in Figure 4 , Figure S38 and Figure S39 .","In comparison to other available methods, DRIMSeq seems to be more conservative than both DEXSeq (using transcript counts) and sQTLseekeR , identifying fewer DS genes and sQTLs, respectively.",no
,5-1356,5-1356_v2_66@2,5-1356_v1_65@0,"Of course, these validations represent an incomplete truth, and ideally, large-scale independent validation would be needed to comprehensively compare the DTU detection methods.","As noted by Soneson et al . <REF-22> , detecting DS in fruit fly is easier than in human; all methods have much smaller false discovery rate (FDR).",no
,5-1356,5-1356_v2_56@4,5-1356_v1_37@3,"Additionally, we have considered how other filtering approaches affect DTU detection.","Thus, sQTL analyses are similar to DS analyses with the difference that multiple models are fitted and tested for each gene.",no
,5-1356,5-1356_v2_32@1,5-1356_v1_79@0,"Following the edgeR strategy <REF-1> , <REF-2> , <REF-53> , we propose multiple approaches for dispersion estimation, all based on the maximization and adjustment of the profile likelihood, since standard maximum likelihood (ML) is known to produce biased estimates as it tends to underestimate variance parameters by not allowing for the fact that other unknown parameters are estimated from the same data <REF-54> , <REF-55> .","To demonstrate the application of DRIMSeq to sQTL analysis, we use the data from the GEUVADIS project <REF-44> where 465 RNA-seq samples from lymphoblastoid cell lines were sequenced, 422 of which were sequenced in the 1000 Genomes Project Phase 1.",no
,5-1356,5-1356_v2_63@1,5-1356_v1_29@0,"Accounting for covariates in DEXSeq (model full 2) or performing the analysis on a subgroup without covariates (model full paired) results in more DS genes detected ( Figure S28 , Figure S29 and Figure S30 ).","The mean of Y is unchanged at ( Y ) = {( Y | Π )} = ( m Π ) = m γ /γ + = m π , while the covariance matrix of Y is given by ( Y ) = cm {diag( π ) – ππ T }, where c = ( m +γ + )/(1+γ + ) is an additional factor when representing the Dirichlet-multinomial covariance to the ordinary multinomial covariance.",no
,5-1356,5-1356_v2_13@6,5-1356_v1_74@3,"This issue is captured in Altrans <REF-27> , which quantifies exon-links (exon junctions) or in MISO <REF-28> , rMATS <REF-29> , SUPPA <REF-30> and SGSeq <REF-31> , all of which calculate splicing event inclusion levels expressed as percentage spliced in (PSI).",Also the distributions of p-values indicate that DM fits better to the transcript counts ( Figure S28 and Figure S29 ).,no
,5-1356,5-1356_v2_32@1,5-1356_v1_65@0,"Following the edgeR strategy <REF-1> , <REF-2> , <REF-53> , we propose multiple approaches for dispersion estimation, all based on the maximization and adjustment of the profile likelihood, since standard maximum likelihood (ML) is known to produce biased estimates as it tends to underestimate variance parameters by not allowing for the fact that other unknown parameters are estimated from the same data <REF-54> , <REF-55> .","As noted by Soneson et al . <REF-22> , detecting DS in fruit fly is easier than in human; all methods have much smaller false discovery rate (FDR).",no
,5-1356,5-1356_v2_78@1,5-1356_v1_29@0,We have shown that this framework can be used for detecting differential isoform usage between experimental conditions as well as for identifying tuQTLs.,"The mean of Y is unchanged at ( Y ) = {( Y | Π )} = ( m Π ) = m γ /γ + = m π , while the covariance matrix of Y is given by ( Y ) = cm {diag( π ) – ππ T }, where c = ( m +γ + )/(1+γ + ) is an additional factor when representing the Dirichlet-multinomial covariance to the ordinary multinomial covariance.",no
,5-1356,5-1356_v2_13@9,5-1356_v1_94@1,"However, there are (hypothetical) instances where changes in splicing pattern may not be captured by exon-level quantifications (Figure 1A in the paper by Monlog et al. <REF-35> ).","In addition to the user friendly workflow for the DS and sQTL analyses, it provides plotting functions that generate diagnostic figures such as the dispersion versus mean gene expression figures and histograms of p-values.",no
,5-1356,5-1356_v2_2@2,5-1356_v1_62@1,There are situations where differences (e.g. between normal and disease state) in the relative ratio of expressed isoforms may have significant phenotypic consequences or lead to prognostic capabilities.,"Additionally, for DRIMSeq , we consider different dispersion estimates: common, gene-wise with no moderation and with moderation-to-common and to-trended dispersion.",no
,5-1356,5-1356_v2_30@4,5-1356_v1_104@0,"Additional challenges to be handled in tuQTL analyses include a large number of tests per gene with highly variable allele frequencies (models) and linkage disequilibrium, which can be accounted for in the multiple testing corrections.",DRIMSeq analyses for this paper were done with version 0.3.3 available on Zenodo http://dx.doi.org/10.5281/zenodo.53084 <REF-59> and Bioconductor release 3.2.,no
,5-1356,5-1356_v2_13@9,5-1356_v1_13@11,"However, there are (hypothetical) instances where changes in splicing pattern may not be captured by exon-level quantifications (Figure 1A in the paper by Monlog et al. <REF-35> ).","Soneson et al . <REF-22> considered counting to accommodate various types of local splicing events, such as exon paths traced out by paired reads, junction counts or events that correspond to combinations of isoforms; in general, the default exon-based counting resulted in strongest performance for DS gene detection.",no
,5-1356,5-1356_v2_71@4,5-1356_v1_90@2,"The numbers of tested and associated genes and tuQTLs are indicated in Figure 4 , Figure S38 and Figure S39 .","Moreover, the sQTL associations detected by DRIMSeq have more enrichment in splicing-related features than sQTLseekeR sQTLs, which could be due to the fact that DRIMSeq accounts for the higher uncertainty of lowly expressed genes by using transcript counts instead of transcript ratios.",no
7718,5-1822,5-1822_v2_15@2,5-1822_v1_15@2,"A total of 72 rats were randomly assigned into four experimental groups; PcTx1-treated ( n =12), saline-treated ( n =16), uninjured/untreated controls ( n =6) or blood-spinal cord barrier integrity ( n =38).","A total of 34 rats were randomly assigned into three treatment groups; PcTx1-treated ( n =12), saline-treated ( n =16) or uninjured/untreated controls ( n =6).",yes
7719,5-1822,5-1822_v2_19@2,5-1822_v1_19@2,The role of the pump was to slowly and steadily release PcTx1 (1.08 μg/h) subcutaneously to compensate for estimated renal and tissue losses of PcTx1 over the first 48 h period.,The role of the pump was to slowly release PcTx1 (1.08 μg/h) subcutaneously to compensate for renal losses and maintain a stable plasma concentration of the drug over a 48 h period.,yes
7720,5-1822,5-1822_v2_21@0,5-1822_v1_21@0,An inherent feature of all spinal contusion models is inter-animal variations in the size of the initial spinal cord lesions produced.,An inherent feature of all spinal contusion models is inter-animal variance in the size of the spinal lesions produced.,yes
7721,5-1822,5-1822_v2_23@6,5-1822_v1_23@4,Animals were then video recorded during three attempts at crossing the ladder and the footage analysed by a blinded observer to count the number of hind limb foot faults (foot slipping below the ladder between rungs).,Animals were video recorded during three attempts at crossing the ladder and the footage analysed by a blinded observer to count the number of hind limb foot faults (foot slipping below the ladder between rungs).,yes
7722,5-1822,5-1822_v2_23@13,5-1822_v1_23@9,"Each animal was video recorded when swimming in a tank of water (27–31°C) and the recordings analysed to determine if the animal used their hind limbs to swim, indicative of preservation of supra-spinal connections ( Magnuson et al., 2009 ; Saunders et al. , 1998 ; Smith et al. , 2006 ), and whether there were alternating hind limb movements.","The limb pattern during swimming in a tank of water (27–31°C) was also video recorded and analysed to determine if the animals used alternating hind limb movements during swimming, indicative of supra-spinal connections.",yes
7723,5-1822,5-1822_v2_25@0,5-1822_v1_25@0,"At the end of each experimental period (24 h or 6 weeks post-injury) injured animals (together with age-matched controls) were terminally anaesthetized with an overdose of inhaled isoflurane (Lyppard Australia), the chest cavity opened and transcardially perfused with 50 ml heparinised (5 IU/ml) PBS (80 ml/min/kg) followed by 150 ml of 4% paraformaldehyde solution.",At the end of each experimental period (24 h or 6 weeks post-injury) injured animals (together with age-matched controls) were terminally anaesthetized with an overdose of inhaled isoflurane (Lyppard Australia) and transcardially perfused with 50 ml heparinised (5 IU/ml) PBS (80 ml/min/kg) followed by 150 ml of 4% paraformaldehyde solution.,yes
7724,5-1822,5-1822_v2_29@0,5-1822_v1_29@0,Specific tissue regions containing some of the white matter tracts involved in hind limb motor function were outlined and measured separately.,Specific tissue regions containing white matter tracts involved in hind limb motor function were outlined and measured separately.,yes
7725,5-1822,5-1822_v2_35@0,5-1822_v1_35@0,"The functional performance of PcTx1-treated and saline-treated animals was compared using two-way analysis of covariance (ANCOVA, Prism v6, Graphpad, San Diego, USA).",The functional performance of PcTx1-treated and saline-treated animals was compared using two-way analysis of covariance (ANCOVA).,yes
7726,5-1822,5-1822_v2_35@1,5-1822_v1_35@1,This method uses the General Linear Model approach and fits least squares linear regression lines to the raw data for individual animals then compares the slopes (correlations) of the regression lines.,This method fits least squares linear regression lines to the raw data for individual animals and then compares the slopes and intercepts of the regression lines.,yes
7727,5-1822,5-1822_v2_35@3,5-1822_v1_35@2,Differences in the initial injury severity between animals within each group introduces a covariate that makes a substantial contribution to the total observed variance in each treatment group.,Inter-animal differences in the initial injury severity (SIS scores) introduce a covariate that makes a substantial contribution to the total observed variance in each treatment group.,yes
7728,5-1822,5-1822_v2_37@0,5-1822_v1_37@0,"Blood-spinal cord barrier (BSCB) function at the lesion site was assessed between 2 h and 7 days post-SCI for different size permeability tracers ( n = 2 per tracer and time point) in a separate series of 38 untreated, injured rats.","Blood-spinal cord barrier (BSCB) function at the lesion site was assessed between 2 h and 7 days post-SCI for different size permeability tracers ( n = 2–3 per tracer) in a separate series of untreated, injured rats.",yes
7729,5-1822,5-1822_v2_50@2,5-1822_v1_50@2,"Linear regression and ANCOVA revealed that the data were best described by two separate lines having the same slope, but with the PcTx1-treated group having a significantly higher elevation compared to the saline-treated group (F 1,9 =16.1324, p=0.003).","Linear regression and ANCOVA analysis revealed that the data were best described by two separate lines with the same slope, but with the PcTx1-treated group having a significantly higher elevation compared to the saline-treated group (p=0.002, n =6–10).",yes
7730,5-1822,5-1822_v2_54@1,5-1822_v1_54@1,There was no observable differences between the two treatment groups in the pattern and style of swimming.,There was no observable difference between the two treatment groups in the swimming test (not shown).,yes
7731,5-1822,5-1822_v2_62@2,5-1822_v1_62@2,"ANCOVA revealed two separate lines with the same slope best described the data with the PcTx1-treated group having a significantly higher elevation ( Figure 5 ; F 1,8 =12.9908, p=0.0069).","ANCOVA revealed two separate lines with the same slope best described the data with the PcTx1-treated group having a significantly higher elevation ( Figure 5 ; p=0.003, n =6).",yes
7732,5-1822,5-1822_v2_83@0,5-1822_v1_83@0,"All of the gene transcripts with significant changes (i.e. FC>2 or FC<-2) for the two injury sizes (SIS 2.5 and 2.75) were separated into their biological categories ( Figure 9 ) using the ‘Panther’ gene classification system ( Mi et al. , 2013 ).","All of the gene transcripts with significant changes (i.e. FC>2 or FC<-2) for the two injury sizes (SIS 2.5 and 2.75) were separated into their biological categories ( ) using the ‘Panther’ gene classification system ( Mi et al. , 2013 ).",yes
,5-1822,5-1822_v2_62@2,5-1822_v1_15@2,"ANCOVA revealed two separate lines with the same slope best described the data with the PcTx1-treated group having a significantly higher elevation ( Figure 5 ; F 1,8 =12.9908, p=0.0069).","A total of 34 rats were randomly assigned into three treatment groups; PcTx1-treated ( n =12), saline-treated ( n =16) or uninjured/untreated controls ( n =6).",no
,5-1822,5-1822_v2_19@2,5-1822_v1_21@0,The role of the pump was to slowly and steadily release PcTx1 (1.08 μg/h) subcutaneously to compensate for estimated renal and tissue losses of PcTx1 over the first 48 h period.,An inherent feature of all spinal contusion models is inter-animal variance in the size of the spinal lesions produced.,no
,5-1822,5-1822_v2_35@3,5-1822_v1_15@2,Differences in the initial injury severity between animals within each group introduces a covariate that makes a substantial contribution to the total observed variance in each treatment group.,"A total of 34 rats were randomly assigned into three treatment groups; PcTx1-treated ( n =12), saline-treated ( n =16) or uninjured/untreated controls ( n =6).",no
,5-1822,5-1822_v2_50@2,5-1822_v1_37@0,"Linear regression and ANCOVA revealed that the data were best described by two separate lines having the same slope, but with the PcTx1-treated group having a significantly higher elevation compared to the saline-treated group (F 1,9 =16.1324, p=0.003).","Blood-spinal cord barrier (BSCB) function at the lesion site was assessed between 2 h and 7 days post-SCI for different size permeability tracers ( n = 2–3 per tracer) in a separate series of untreated, injured rats.",no
,5-1822,5-1822_v2_19@2,5-1822_v1_29@0,The role of the pump was to slowly and steadily release PcTx1 (1.08 μg/h) subcutaneously to compensate for estimated renal and tissue losses of PcTx1 over the first 48 h period.,Specific tissue regions containing white matter tracts involved in hind limb motor function were outlined and measured separately.,no
,5-1822,5-1822_v2_35@0,5-1822_v1_54@1,"The functional performance of PcTx1-treated and saline-treated animals was compared using two-way analysis of covariance (ANCOVA, Prism v6, Graphpad, San Diego, USA).",There was no observable difference between the two treatment groups in the swimming test (not shown).,no
,5-1822,5-1822_v2_62@2,5-1822_v1_21@0,"ANCOVA revealed two separate lines with the same slope best described the data with the PcTx1-treated group having a significantly higher elevation ( Figure 5 ; F 1,8 =12.9908, p=0.0069).",An inherent feature of all spinal contusion models is inter-animal variance in the size of the spinal lesions produced.,no
,5-1822,5-1822_v2_35@3,5-1822_v1_23@9,Differences in the initial injury severity between animals within each group introduces a covariate that makes a substantial contribution to the total observed variance in each treatment group.,"The limb pattern during swimming in a tank of water (27–31°C) was also video recorded and analysed to determine if the animals used alternating hind limb movements during swimming, indicative of supra-spinal connections.",no
,5-1822,5-1822_v2_35@1,5-1822_v1_23@4,This method uses the General Linear Model approach and fits least squares linear regression lines to the raw data for individual animals then compares the slopes (correlations) of the regression lines.,Animals were video recorded during three attempts at crossing the ladder and the footage analysed by a blinded observer to count the number of hind limb foot faults (foot slipping below the ladder between rungs).,no
,5-1822,5-1822_v2_35@0,5-1822_v1_35@1,"The functional performance of PcTx1-treated and saline-treated animals was compared using two-way analysis of covariance (ANCOVA, Prism v6, Graphpad, San Diego, USA).",This method fits least squares linear regression lines to the raw data for individual animals and then compares the slopes and intercepts of the regression lines.,no
,5-1822,5-1822_v2_54@1,5-1822_v1_35@2,There was no observable differences between the two treatment groups in the pattern and style of swimming.,Inter-animal differences in the initial injury severity (SIS scores) introduce a covariate that makes a substantial contribution to the total observed variance in each treatment group.,no
,5-1822,5-1822_v2_37@0,5-1822_v1_15@2,"Blood-spinal cord barrier (BSCB) function at the lesion site was assessed between 2 h and 7 days post-SCI for different size permeability tracers ( n = 2 per tracer and time point) in a separate series of 38 untreated, injured rats.","A total of 34 rats were randomly assigned into three treatment groups; PcTx1-treated ( n =12), saline-treated ( n =16) or uninjured/untreated controls ( n =6).",no
,5-1822,5-1822_v2_29@0,5-1822_v1_35@2,Specific tissue regions containing some of the white matter tracts involved in hind limb motor function were outlined and measured separately.,Inter-animal differences in the initial injury severity (SIS scores) introduce a covariate that makes a substantial contribution to the total observed variance in each treatment group.,no
,5-1822,5-1822_v2_25@0,5-1822_v1_35@2,"At the end of each experimental period (24 h or 6 weeks post-injury) injured animals (together with age-matched controls) were terminally anaesthetized with an overdose of inhaled isoflurane (Lyppard Australia), the chest cavity opened and transcardially perfused with 50 ml heparinised (5 IU/ml) PBS (80 ml/min/kg) followed by 150 ml of 4% paraformaldehyde solution.",Inter-animal differences in the initial injury severity (SIS scores) introduce a covariate that makes a substantial contribution to the total observed variance in each treatment group.,no
,5-1822,5-1822_v2_23@13,5-1822_v1_25@0,"Each animal was video recorded when swimming in a tank of water (27–31°C) and the recordings analysed to determine if the animal used their hind limbs to swim, indicative of preservation of supra-spinal connections ( Magnuson et al., 2009 ; Saunders et al. , 1998 ; Smith et al. , 2006 ), and whether there were alternating hind limb movements.",At the end of each experimental period (24 h or 6 weeks post-injury) injured animals (together with age-matched controls) were terminally anaesthetized with an overdose of inhaled isoflurane (Lyppard Australia) and transcardially perfused with 50 ml heparinised (5 IU/ml) PBS (80 ml/min/kg) followed by 150 ml of 4% paraformaldehyde solution.,no
7902,5-392,5-392_v2_16@0,5-392_v1_16@0,The mtDNA of the diplonemid Diplonema papillatum is composed of numerous free DNA circles ( Figure 1 ) with a total size of about 600 kb <REF-31> .,The mtDNA of the diplonemid Diplonema papillatum is composed of numerous free DNA circles ( Figure 1 ) with a total coding capacity of about 600 kb <REF-31> .,yes
7906,5-392,5-392_v2_16@4,5-392_v1_16@4,"The small subunit (SSU) mito-rRNA has been identified only very recently because its sequence is extremely diverged, which has made its identification challenging <REF-34> , <REF-35> .","The absence of the small subunit (SSU) mito-rRNA is explained by its being diverged beyond recognition, as the LSU gene is fragmented and extremely diverged, which made its identification challenging <REF-34> .",yes
7907,5-392,5-392_v2_19@2,5-392_v1_19@2,"In Diplonema , only a few insertions of blocks of uridines have been documented initially, but the recent comprehensive count amounts to ~200 <REF-31> , <REF-32> , <REF-35> .","In Diplonema , only a few insertions of blocks of uridines have been documented so far <REF-31> , <REF-32> .",yes
7908,5-392,5-392_v2_22@0,5-392_v1_22@0,"The post-transcriptional processing is very different in Diplonema , where fragments of genes transcribed from individual DNA circles are trans -spliced <REF-31> , <REF-33> .","The post-transcriptional processing is very different in Diplonema , where fragments of genes transcribed from individual DNA circles are trans -spliced in a highly systematic 5′ to 3′ progression <REF-31> , <REF-33> .",yes
7909,5-392,5-392_v2_25@0,5-392_v1_25@0,Why are mitochondrial genomes in Euglenozoa so diverse?,Why are mitochondrial genomes in Euglenozoa so diverse,yes
7910,5-392,5-392_v2_26@0,5-392_v1_26@0,"Soon after its discovery, RNA editing in kinetoplastids was explained as a remnant of the RNA world <REF-44> .","Soon after its discovery, RNA editing in kinetoplastids was explained as a remnant of the RNA world <REF-43> , but the absence of similar mechanisms in both sister clades <REF-33> , <REF-37> puts this scenario finally to rest.",yes
7911,5-392,5-392_v2_26@3,5-392_v1_26@3,"The recent finding of a mt genome in Euglena <REF-38> implies that the irreversible scrambling originally implied for the mtDNA of the euglenozoan last common ancestor <REF-45> did happen at a later stage in evolution, probably in the predecessor of diplonemids and kinetoplastids.","The recent finding of a standard mt genome in Euglena <REF-37> implies that the irreversible scrambling originally implied for the mtDNA of the euglenozoan last common ancestor <REF-44> did happen at a later stage in evolution, probably in the predecessor of diplonemids and kinetoplastids.",yes
7912,5-392,5-392_v2_26@4,5-392_v1_26@4,"Although despite the available sequence data the mutual relationships among the three euglenozoan lineages remain unresolved, we can predict, on the basis of their mt genomes and transcriptomes, that the mostly free-living photosynthetic euglenids constitute the earliest offshoot of the long euglenozoan branch.","Although despite the available sequence data, the mutual relationships among the three euglenozoan lineages remain unresolved, and we can predict, on the basis of their mt genomes and transcriptomes, that the mostly free-living photosynthetic euglenids constitute the earliest offshoot of the long euglenozoan branch.",yes
7913,5-392,5-392_v2_8@1,5-392_v1_8@1,"Like all mitochondria of aerobic protists, this organelle contains mitochondrial DNA (mtDNA) <REF-7> .","As all mitochondria of aerobic protists, it contains mitochondrial DNA (mtDNA) <REF-7> .",yes
7914,5-392,5-392_v2_10@0,5-392_v1_10@0,"Standard mt genomes are usually represented by a circular or linear DNA molecule encoding an average of fewer than two dozen genes ranging from 2 to 66 proteins in Chromera velia and Andalucia godoyi , respectively <REF-13> , <REF-14> .","Standard mt genomes are usually represented by a circular or linear DNA molecule encoding an average of fewer than two dozen genes ranging from 2 to 66 proteins in Chromera velia and Andalucia godoyi <REF-13> , <REF-14> .",yes
7915,5-392,5-392_v2_10@2,5-392_v1_10@2,"In euglenids and diplonemids, mtDNA seems to be evenly distributed throughout the lumen of the organelle <REF-8> , <REF-15> , whereas in kinetoplastids, the picture is more complex ( Figure 2 ).","In euglenids and diplonemids, mtDNA seems to be evenly distributed throughout the lumen of the organelle <REF-8> , <REF-15> , and the picture is more complex in kinetoplastids ( Figure 2 ).",yes
7916,5-392,5-392_v2_10@3,5-392_v1_10@3,"In the obligatory parasitic trypanosomatids mtDNA is invariably compacted into a single disk-shaped structure of concatenated DNA termed the kinetoplast DNA (kDNA), the free-living or commensalic bodonids have their kDNA distributed either evenly or in foci in the mt lumen <REF-16> .","Whereas in the obligatory parasitic trypanosomatids it is invariably compacted into a single disk-shaped structure of concatenated DNA termed the kinetoplast DNA (kDNA), the free-living or commensalic bodonids have their kDNA distributed either evenly or in foci in the mt lumen <REF-16> .",yes
7917,5-392,5-392_v2_13@4,5-392_v1_13@4,"The conserved region carries 18 protein-coding genes, mostly subunits of respiratory complexes (complex I: nad1 , nad2 , nad3 , nad4 , nad5 , nad7 , nad8 , and nad9 ; complex III: cob ; complex IV: cox1 , cox2 , and cox3 ; complex V: atp6 ), one ribosomal protein ( rps12 ), small and large mito-rRNA genes ( 12S and 9S ), and four open reading frames of unknown function ( MURF2 , MURF5 , cr3 , and cr4 ) ( Figure 3 ) <REF-24> , <REF-25> .","The conserved region carries 18 protein-coding genes, mostly subunits of respiratory complexes (complex I: nad1 , nad2 , nad3 , nad4 , nad5 , nad7 , nad8 , and nad9 ; complex III: cob ; complex IV: cox1 , cox2 , and cox3 ; complex V: atp6 ), one ribosomal protein ( rpl12 ), small and large mito-rRNA genes ( 12S and 9S ), and four open reading frames of unknown function ( MURF2 , MURF5 , cr3 , and cr4 ) ( Figure 3 ) <REF-24> , <REF-25> .",yes
,5-392,5-392_v2_22@0,5-392_v1_13@4,"The post-transcriptional processing is very different in Diplonema , where fragments of genes transcribed from individual DNA circles are trans -spliced <REF-31> , <REF-33> .","The conserved region carries 18 protein-coding genes, mostly subunits of respiratory complexes (complex I: nad1 , nad2 , nad3 , nad4 , nad5 , nad7 , nad8 , and nad9 ; complex III: cob ; complex IV: cox1 , cox2 , and cox3 ; complex V: atp6 ), one ribosomal protein ( rpl12 ), small and large mito-rRNA genes ( 12S and 9S ), and four open reading frames of unknown function ( MURF2 , MURF5 , cr3 , and cr4 ) ( Figure 3 ) <REF-24> , <REF-25> .",no
,5-392,5-392_v2_10@0,5-392_v1_26@0,"Standard mt genomes are usually represented by a circular or linear DNA molecule encoding an average of fewer than two dozen genes ranging from 2 to 66 proteins in Chromera velia and Andalucia godoyi , respectively <REF-13> , <REF-14> .","Soon after its discovery, RNA editing in kinetoplastids was explained as a remnant of the RNA world <REF-43> , but the absence of similar mechanisms in both sister clades <REF-33> , <REF-37> puts this scenario finally to rest.",no
,5-392,5-392_v2_10@3,5-392_v1_22@0,"In the obligatory parasitic trypanosomatids mtDNA is invariably compacted into a single disk-shaped structure of concatenated DNA termed the kinetoplast DNA (kDNA), the free-living or commensalic bodonids have their kDNA distributed either evenly or in foci in the mt lumen <REF-16> .","The post-transcriptional processing is very different in Diplonema , where fragments of genes transcribed from individual DNA circles are trans -spliced in a highly systematic 5′ to 3′ progression <REF-31> , <REF-33> .",no
,5-392,5-392_v2_26@3,5-392_v1_19@2,"The recent finding of a mt genome in Euglena <REF-38> implies that the irreversible scrambling originally implied for the mtDNA of the euglenozoan last common ancestor <REF-45> did happen at a later stage in evolution, probably in the predecessor of diplonemids and kinetoplastids.","In Diplonema , only a few insertions of blocks of uridines have been documented so far <REF-31> , <REF-32> .",no
,5-392,5-392_v2_10@3,5-392_v1_8@1,"In the obligatory parasitic trypanosomatids mtDNA is invariably compacted into a single disk-shaped structure of concatenated DNA termed the kinetoplast DNA (kDNA), the free-living or commensalic bodonids have their kDNA distributed either evenly or in foci in the mt lumen <REF-16> .","As all mitochondria of aerobic protists, it contains mitochondrial DNA (mtDNA) <REF-7> .",no
,5-392,5-392_v2_10@0,5-392_v1_19@2,"Standard mt genomes are usually represented by a circular or linear DNA molecule encoding an average of fewer than two dozen genes ranging from 2 to 66 proteins in Chromera velia and Andalucia godoyi , respectively <REF-13> , <REF-14> .","In Diplonema , only a few insertions of blocks of uridines have been documented so far <REF-31> , <REF-32> .",no
,5-392,5-392_v2_13@4,5-392_v1_10@0,"The conserved region carries 18 protein-coding genes, mostly subunits of respiratory complexes (complex I: nad1 , nad2 , nad3 , nad4 , nad5 , nad7 , nad8 , and nad9 ; complex III: cob ; complex IV: cox1 , cox2 , and cox3 ; complex V: atp6 ), one ribosomal protein ( rps12 ), small and large mito-rRNA genes ( 12S and 9S ), and four open reading frames of unknown function ( MURF2 , MURF5 , cr3 , and cr4 ) ( Figure 3 ) <REF-24> , <REF-25> .","Standard mt genomes are usually represented by a circular or linear DNA molecule encoding an average of fewer than two dozen genes ranging from 2 to 66 proteins in Chromera velia and Andalucia godoyi <REF-13> , <REF-14> .",no
,5-392,5-392_v2_16@4,5-392_v1_8@1,"The small subunit (SSU) mito-rRNA has been identified only very recently because its sequence is extremely diverged, which has made its identification challenging <REF-34> , <REF-35> .","As all mitochondria of aerobic protists, it contains mitochondrial DNA (mtDNA) <REF-7> .",no
,5-392,5-392_v2_10@3,5-392_v1_16@0,"In the obligatory parasitic trypanosomatids mtDNA is invariably compacted into a single disk-shaped structure of concatenated DNA termed the kinetoplast DNA (kDNA), the free-living or commensalic bodonids have their kDNA distributed either evenly or in foci in the mt lumen <REF-16> .",The mtDNA of the diplonemid Diplonema papillatum is composed of numerous free DNA circles ( Figure 1 ) with a total coding capacity of about 600 kb <REF-31> .,no
,5-392,5-392_v2_8@1,5-392_v1_19@2,"Like all mitochondria of aerobic protists, this organelle contains mitochondrial DNA (mtDNA) <REF-7> .","In Diplonema , only a few insertions of blocks of uridines have been documented so far <REF-31> , <REF-32> .",no
,5-392,5-392_v2_26@3,5-392_v1_10@3,"The recent finding of a mt genome in Euglena <REF-38> implies that the irreversible scrambling originally implied for the mtDNA of the euglenozoan last common ancestor <REF-45> did happen at a later stage in evolution, probably in the predecessor of diplonemids and kinetoplastids.","Whereas in the obligatory parasitic trypanosomatids it is invariably compacted into a single disk-shaped structure of concatenated DNA termed the kinetoplast DNA (kDNA), the free-living or commensalic bodonids have their kDNA distributed either evenly or in foci in the mt lumen <REF-16> .",no
,5-392,5-392_v2_22@0,5-392_v1_10@0,"The post-transcriptional processing is very different in Diplonema , where fragments of genes transcribed from individual DNA circles are trans -spliced <REF-31> , <REF-33> .","Standard mt genomes are usually represented by a circular or linear DNA molecule encoding an average of fewer than two dozen genes ranging from 2 to 66 proteins in Chromera velia and Andalucia godoyi <REF-13> , <REF-14> .",no
,5-392,5-392_v2_26@4,5-392_v1_22@0,"Although despite the available sequence data the mutual relationships among the three euglenozoan lineages remain unresolved, we can predict, on the basis of their mt genomes and transcriptomes, that the mostly free-living photosynthetic euglenids constitute the earliest offshoot of the long euglenozoan branch.","The post-transcriptional processing is very different in Diplonema , where fragments of genes transcribed from individual DNA circles are trans -spliced in a highly systematic 5′ to 3′ progression <REF-31> , <REF-33> .",no
8025,6-1302,6-1302_v2_20@10,6-1302_v1_20@10,"This classification resembles the one depicted in Gingeras, 2009 <REF-7> and can be summarized as follows:",This classification resembles the one depicted in <REF-7> and can be summarized as follows:,yes
8026,6-1302,6-1302_v2_37@2,6-1302_v1_37@2,All candidate fusion transcripts were validated using qPCR and Sanger sequencing except the class 3 overlap which requires systematic reconstruction of the fusion transcript for the design of primers.,All candidate fusion transcripts were validated using qPCR and Sanger sequencing except the class three overlap which requires systematic reconstruction of the fusion transcript for the design of primers.,yes
8027,6-1302,6-1302_v2_41@1,6-1302_v1_41@1,"1 µl of each cDNA sample (2ng/µl) was added to a 5 µl of reaction mix containing 3 µl of Master Mix (LightCycler ® 480 sybr green I Master, Roche Diagnostics, GERMANY) and 0.66 µM forward and reverse primers.","1 µl of each cDNA sample (2ng/µl) was added to a 5 µl of reaction mix containing 3 µl of Master Mix (LightCycler ® 480 sybr green I Master, Roche Diagnostics, GERMANY) and 0.33 µM forward and reverse primers.",yes
8028,6-1302,6-1302_v2_41@8,6-1302_v1_41@8,"Ultimately, a melting curve analysis ranging from 60°C to 95°C was performed to control primer specificity.","Ultimately, a melting curve analysis ranging from 95°C to 60°C was performed to control primer specificity.",yes
8029,6-1302,6-1302_v2_44@1,6-1302_v1_44@1,"A first step aging slide with the cytogenetic preparation was performed by immerging slides in 2xSSC solution (saline sodium citrate) for 30 minutes at 37°C, followed by dehydration in 3 baths of increasing ethanol concentration: 70%, 85% and 100%, each for 1 minute.","A first step aging slide with the cytogenetic preparation was performed by immerging slides in 2xSSC solution (saline sodium citrate) for 30 minutes at 37°C, followed by dehydration in 3 baths of increasing ethanol concentration: 70°, 85° and 100°, each for 1 minute.",yes
8030,6-1302,6-1302_v2_54@5,6-1302_v1_54@5,"The CBFB-MYH11 and PML-RARA fusion transcripts expressed in the AML-inv16 and AML-t(15;17) samples were identified using both RNA-seq and qPCR analysis, confirming the reliability of RNA-seq and the Crac suite in this type of analysis.","The CBFB-MYH11 and PML-RAR fusion transcripts expressed in the AML-inv16 and AML-t(15;17) samples were identified using both RNA-seq and qPCR analysis, confirming the reliability of RNA-seq and the Crac suite in this type of analysis.",yes
8031,6-1302,6-1302_v2_57@0,6-1302_v1_57@0,"Among the Class 1 chRNAs more frequently associated with genomic translocation, we identified 4 chRNAs associated with PML and RARA genes in patient OM110223 suffering from AML-t(15,17).","Among the Class 1 chRNAs more frequently associated with genomic translocation, we identified 4 chRNAs associated with PML and RAR genes in patient OM110223 suffering from AML-t(15,17).",yes
8032,6-1302,6-1302_v2_8@2,6-1302_v1_8@2,"In some well-documented cases, gene fusions, in addition to contributing to neoplastic transformation, produce fusion RNA and proteins used as therapeutic targets (Mertens et al. , 2015 <REF-5> , Yoshihara et al. , 2015 <REF-6> and references therein).","In some well-documented cases, gene fusions, in addition to contributing to neoplastic transformation, produce fusion RNA and proteins used as therapeutic targets ( <REF-5> , <REF-6> and references therein).",yes
8033,6-1302,6-1302_v2_65@0,6-1302_v1_65@0,New Class 1 PML-RARA variants,New Class 1 PML-RAR variants,yes
8034,6-1302,6-1302_v2_66@1,6-1302_v1_66@1,"Acute promyelocytic leukemia (APL) molecular diagnosis and minimal residual disease (MRD) monitoring are currently based on bcr1, bcr2 or bcr3 fusion transcript detection, depending on the DNA breakpoint <REF-21> , <REF-22> .","Acute promyelocytic leukemia (APL) molecular diagnosis and minimal residual disease (MDR) monitoring are currently based on bcr1, bcr2 or bcr3 fusion transcript detection, depending on the DNA breakpoint <REF-21> , <REF-22> .",yes
8035,6-1302,6-1302_v2_82@0,6-1302_v1_82@0,"In order to validate our strategy on a large cohort, we analyzed a publicly available dataset of 125 AML and 17 normal CD34+ HSC RNA-seq using a tag search approach (see Materials and methods).","To further extend our analysis and in order to validate our strategy on a large cohort, we analyzed a publicly available dataset of 125 AML and 17 normal CD34+ HSC RNA-seq using a tag search approach (see Materials and methods).",yes
8036,6-1302,6-1302_v2_83@0,6-1302_v1_83@0,"We identified four new types of tumor-specific chRNA; TRIM28-TRIM28, DHRS7B-TMEM11, PLXNB-BLRD1 and SLC16A3-METRNL, expressed in all AML groups ( Figure 5B ).","We identified four new types of tumor-specific chRNA; TRIM28-TRIM28, DHRS7B-TMEME11, PLXNB-BLRD1 and SLC16A3-METRNL, expressed in all AML groups ( Figure 5B ).",yes
8037,6-1302,6-1302_v2_83@4,6-1302_v1_83@4,"FLT3, PML-RARA and CBFB-MYH11 are strong markers in AML, and also useful for prognostic and MRD monitoring.","FLT3, PML-RARA and CBFB-MYH11 are strong markers in AML, and also useful for prognostic and MDR monitoring.",yes
8038,6-1302,6-1302_v2_88@0,6-1302_v1_88@0,"The use of RNA-seq to provide a detailed view of the transcriptome and to detect new RNA transcripts, opens up new opportunities for improving diagnosis and treatment of human diseases.","The use of RNA-seq to provide a detailed view of the transcriptome and to detect new RNA transcripts, opens up new opportunities for improving diagnosis and treatment of human disease.",yes
8039,6-1302,6-1302_v2_88@2,6-1302_v1_88@2,"ChRNAs, also known as “fusion RNA” or “canonical chimeras” <REF-5> , <REF-26> , are already used in diagnosis, but many other chimeric fusion products generated by transcriptional mechanisms such as read-throughs, cis or trans-splicing <REF-5> , <REF-7> , <REF-9> , also have the potential to be used in diagnosis if correctly categorized.","ChRNAs, also known as “fusion RNA” or “canonical chimeras”( <REF-5> , <REF-26> ), are already used in diagnosis, but many other chimeric fusion products generated by transcriptional mechanisms such as read-throughs, cis or trans-splicing <REF-5> , <REF-7> , <REF-9> , also have the potential to be used in diagnosis if correctly categorized.",yes
8040,6-1302,6-1302_v2_91@2,6-1302_v1_91@2,"We also identified novel PML-RARA isoforms, shorter than the isoforms currently used in diagnosis, which could again be used in patient follow-up.","We also identified novel PML-RAR isoforms, shorter than the isoforms currently used in diagnosis, which could again be used in patient follow-up.",yes
8041,6-1302,6-1302_v2_91@5,6-1302_v1_91@5,"MRD and patient follow-up in APL is usually performed by PML-RARA transcript QPCR, and relapse is associated with an increase of bcr1, bcr2 or bcr3 fusion transcripts <REF-32> .","MRD and patient follow-up in APL is usually performed by PML-RAR transcript QPCR, and relapse is associated with an increase of bcr1, bcr2 or bcr3 fusion transcripts <REF-32> .",yes
8042,6-1302,6-1302_v2_95@0,6-1302_v1_95@0,"Abbreviations: chRNA, chimeric RNA; AML, acute myeloid leukemia; NK, normal karyotype; UK, unknown karyotype; AK, abnormal karyotype; APL, Acute Promyelocytic Leukemia; PBMCs, peripheral blood mononuclear cells; Inv16, chromosome 16 inversion; t(15;17), translocation of chromosomes 15 and 17; qPCR, quantitative polymerase chain reaction; NONE, non-annotated region; LincRNA, Long intergenic noncoding RNAs; Bcr, break chromosomal region; FISH, Fluorescence in situ hybridization; ATRA, all-trans retinoic acid; VD, vitamin D; TGFβ, transforming growth factor beta; MRD, minimal residual disease.","Abbreviations: chRNA, chimeric RNA; AML, acute myeloid leukemia; NK, normal karyotype; UK, unknown karyotype; AK, abnormal karyotype; APL, Acute Promyelocytic Leukemia; PBMCs, peripheral blood mononuclear cells; Inv16, chromosome 16 inversion; t(15;17), translocation of chromosomes 15 and 17; qPCR, quantitative polymerase chain reaction; NONE, non-annotated region; LincRNA, Long intergenic noncoding RNAs; Bcr, break chromosomal region; FISH, Fluorescence in situ hybridization; ATRA, all-trans retinoic acid; VD, vitamin D; TGFβtransforming growth factor beta; MRD, minimal residual disease.",yes
8043,6-1302,6-1302_v2_13@8,6-1302_v1_13@8,"For the latter, peripheral blood mononuclear cells were collected by ficoll-hypaque density gradient and cultured at a concentration of 1×10 6 /ml, with or without 0.1µM ATRA, for 3 days.","For the latter, peripheral blood mononuclear cells were collected by ficoll-hypaque density gradient and cultured at a concentration of 1×10 6 /ml, with or without 0,1µM ATRA, for 3 days.",yes
8044,6-1302,6-1302_v2_13@13,6-1302_v1_13@13,"The chemical agents used for differentiation were 1µM all-trans retinoic acid (ATRA; Sigma-Aldrich, Gillingham, UK), or 0.1µM vitamin D (VD) associated with 500pg/ml transforming growth factor beta TGFβ (Promega Corporation, USA) for the NB4 cell line <REF-18> .","The chemical agents used for differentiation were 1µM all-trans retinoic acid (ATRA; Sigma-Aldrich, Gillingham, UK), or 0,1µM vitamin D (VD) associated with 500pg/ml transforming growth factor beta TGFβ (Promega Corporation, USA) for the NB4 cell line <REF-18> .",yes
8045,6-1302,6-1302_v2_13@14,6-1302_v1_13@14,"For the U937 cell line, 0.1µM TTNPB associated with 1µM Targretin (LGD1069) and 0.1µM 1 alpha, 25 dihydroxyvitamin D3 (VD) were used.","For the U937 cell line, 0,1µM TTNPB associated with 1µM Targretin (LGD1069) and 0,1µM 1 alpha, 25 dihydroxyvitamin D3 (VD) were used.",yes
8046,6-1302,6-1302_v2_13@15,6-1302_v1_13@15,"TTNPB, LGD1069 and VD were kindly provided by Dr Klaus (Hoffman-La Roche, Switzerland), JHiernaux (Glaxo-welcome Laboratories, France), and L Binderup (Leopharmaceutical products, Denmark), respectively.","TTNPB, LGD1069 and VD were kindly provided by Dr Klaus (Hoffman-La Roche,Switzerland), JHiernaux (Glaxo-welcome Laboratories, France), and L Binderup (Leopharmaceutical products,Denmark), respectively.",yes
,6-1302,6-1302_v2_20@10,6-1302_v1_83@0,"This classification resembles the one depicted in Gingeras, 2009 <REF-7> and can be summarized as follows:","We identified four new types of tumor-specific chRNA; TRIM28-TRIM28, DHRS7B-TMEME11, PLXNB-BLRD1 and SLC16A3-METRNL, expressed in all AML groups ( Figure 5B ).",no
,6-1302,6-1302_v2_13@13,6-1302_v1_37@2,"The chemical agents used for differentiation were 1µM all-trans retinoic acid (ATRA; Sigma-Aldrich, Gillingham, UK), or 0.1µM vitamin D (VD) associated with 500pg/ml transforming growth factor beta TGFβ (Promega Corporation, USA) for the NB4 cell line <REF-18> .",All candidate fusion transcripts were validated using qPCR and Sanger sequencing except the class three overlap which requires systematic reconstruction of the fusion transcript for the design of primers.,no
,6-1302,6-1302_v2_54@5,6-1302_v1_13@13,"The CBFB-MYH11 and PML-RARA fusion transcripts expressed in the AML-inv16 and AML-t(15;17) samples were identified using both RNA-seq and qPCR analysis, confirming the reliability of RNA-seq and the Crac suite in this type of analysis.","The chemical agents used for differentiation were 1µM all-trans retinoic acid (ATRA; Sigma-Aldrich, Gillingham, UK), or 0,1µM vitamin D (VD) associated with 500pg/ml transforming growth factor beta TGFβ (Promega Corporation, USA) for the NB4 cell line <REF-18> .",no
,6-1302,6-1302_v2_13@8,6-1302_v1_88@2,"For the latter, peripheral blood mononuclear cells were collected by ficoll-hypaque density gradient and cultured at a concentration of 1×10 6 /ml, with or without 0.1µM ATRA, for 3 days.","ChRNAs, also known as “fusion RNA” or “canonical chimeras”( <REF-5> , <REF-26> ), are already used in diagnosis, but many other chimeric fusion products generated by transcriptional mechanisms such as read-throughs, cis or trans-splicing <REF-5> , <REF-7> , <REF-9> , also have the potential to be used in diagnosis if correctly categorized.",no
,6-1302,6-1302_v2_65@0,6-1302_v1_83@4,New Class 1 PML-RARA variants,"FLT3, PML-RARA and CBFB-MYH11 are strong markers in AML, and also useful for prognostic and MDR monitoring.",no
,6-1302,6-1302_v2_41@1,6-1302_v1_82@0,"1 µl of each cDNA sample (2ng/µl) was added to a 5 µl of reaction mix containing 3 µl of Master Mix (LightCycler ® 480 sybr green I Master, Roche Diagnostics, GERMANY) and 0.66 µM forward and reverse primers.","To further extend our analysis and in order to validate our strategy on a large cohort, we analyzed a publicly available dataset of 125 AML and 17 normal CD34+ HSC RNA-seq using a tag search approach (see Materials and methods).",no
,6-1302,6-1302_v2_66@1,6-1302_v1_88@0,"Acute promyelocytic leukemia (APL) molecular diagnosis and minimal residual disease (MRD) monitoring are currently based on bcr1, bcr2 or bcr3 fusion transcript detection, depending on the DNA breakpoint <REF-21> , <REF-22> .","The use of RNA-seq to provide a detailed view of the transcriptome and to detect new RNA transcripts, opens up new opportunities for improving diagnosis and treatment of human disease.",no
,6-1302,6-1302_v2_66@1,6-1302_v1_91@5,"Acute promyelocytic leukemia (APL) molecular diagnosis and minimal residual disease (MRD) monitoring are currently based on bcr1, bcr2 or bcr3 fusion transcript detection, depending on the DNA breakpoint <REF-21> , <REF-22> .","MRD and patient follow-up in APL is usually performed by PML-RAR transcript QPCR, and relapse is associated with an increase of bcr1, bcr2 or bcr3 fusion transcripts <REF-32> .",no
,6-1302,6-1302_v2_83@4,6-1302_v1_8@2,"FLT3, PML-RARA and CBFB-MYH11 are strong markers in AML, and also useful for prognostic and MRD monitoring.","In some well-documented cases, gene fusions, in addition to contributing to neoplastic transformation, produce fusion RNA and proteins used as therapeutic targets ( <REF-5> , <REF-6> and references therein).",no
,6-1302,6-1302_v2_13@8,6-1302_v1_20@10,"For the latter, peripheral blood mononuclear cells were collected by ficoll-hypaque density gradient and cultured at a concentration of 1×10 6 /ml, with or without 0.1µM ATRA, for 3 days.",This classification resembles the one depicted in <REF-7> and can be summarized as follows:,no
,6-1302,6-1302_v2_20@10,6-1302_v1_82@0,"This classification resembles the one depicted in Gingeras, 2009 <REF-7> and can be summarized as follows:","To further extend our analysis and in order to validate our strategy on a large cohort, we analyzed a publicly available dataset of 125 AML and 17 normal CD34+ HSC RNA-seq using a tag search approach (see Materials and methods).",no
,6-1302,6-1302_v2_41@1,6-1302_v1_83@0,"1 µl of each cDNA sample (2ng/µl) was added to a 5 µl of reaction mix containing 3 µl of Master Mix (LightCycler ® 480 sybr green I Master, Roche Diagnostics, GERMANY) and 0.66 µM forward and reverse primers.","We identified four new types of tumor-specific chRNA; TRIM28-TRIM28, DHRS7B-TMEME11, PLXNB-BLRD1 and SLC16A3-METRNL, expressed in all AML groups ( Figure 5B ).",no
,6-1302,6-1302_v2_88@0,6-1302_v1_95@0,"The use of RNA-seq to provide a detailed view of the transcriptome and to detect new RNA transcripts, opens up new opportunities for improving diagnosis and treatment of human diseases.","Abbreviations: chRNA, chimeric RNA; AML, acute myeloid leukemia; NK, normal karyotype; UK, unknown karyotype; AK, abnormal karyotype; APL, Acute Promyelocytic Leukemia; PBMCs, peripheral blood mononuclear cells; Inv16, chromosome 16 inversion; t(15;17), translocation of chromosomes 15 and 17; qPCR, quantitative polymerase chain reaction; NONE, non-annotated region; LincRNA, Long intergenic noncoding RNAs; Bcr, break chromosomal region; FISH, Fluorescence in situ hybridization; ATRA, all-trans retinoic acid; VD, vitamin D; TGFβtransforming growth factor beta; MRD, minimal residual disease.",no
,6-1302,6-1302_v2_41@1,6-1302_v1_8@2,"1 µl of each cDNA sample (2ng/µl) was added to a 5 µl of reaction mix containing 3 µl of Master Mix (LightCycler ® 480 sybr green I Master, Roche Diagnostics, GERMANY) and 0.66 µM forward and reverse primers.","In some well-documented cases, gene fusions, in addition to contributing to neoplastic transformation, produce fusion RNA and proteins used as therapeutic targets ( <REF-5> , <REF-6> and references therein).",no
,6-1302,6-1302_v2_91@5,6-1302_v1_91@2,"MRD and patient follow-up in APL is usually performed by PML-RARA transcript QPCR, and relapse is associated with an increase of bcr1, bcr2 or bcr3 fusion transcripts <REF-32> .","We also identified novel PML-RAR isoforms, shorter than the isoforms currently used in diagnosis, which could again be used in patient follow-up.",no
,6-1302,6-1302_v2_13@13,6-1302_v1_44@1,"The chemical agents used for differentiation were 1µM all-trans retinoic acid (ATRA; Sigma-Aldrich, Gillingham, UK), or 0.1µM vitamin D (VD) associated with 500pg/ml transforming growth factor beta TGFβ (Promega Corporation, USA) for the NB4 cell line <REF-18> .","A first step aging slide with the cytogenetic preparation was performed by immerging slides in 2xSSC solution (saline sodium citrate) for 30 minutes at 37°C, followed by dehydration in 3 baths of increasing ethanol concentration: 70°, 85° and 100°, each for 1 minute.",no
,6-1302,6-1302_v2_37@2,6-1302_v1_41@1,All candidate fusion transcripts were validated using qPCR and Sanger sequencing except the class 3 overlap which requires systematic reconstruction of the fusion transcript for the design of primers.,"1 µl of each cDNA sample (2ng/µl) was added to a 5 µl of reaction mix containing 3 µl of Master Mix (LightCycler ® 480 sybr green I Master, Roche Diagnostics, GERMANY) and 0.33 µM forward and reverse primers.",no
,6-1302,6-1302_v2_88@0,6-1302_v1_13@13,"The use of RNA-seq to provide a detailed view of the transcriptome and to detect new RNA transcripts, opens up new opportunities for improving diagnosis and treatment of human diseases.","The chemical agents used for differentiation were 1µM all-trans retinoic acid (ATRA; Sigma-Aldrich, Gillingham, UK), or 0,1µM vitamin D (VD) associated with 500pg/ml transforming growth factor beta TGFβ (Promega Corporation, USA) for the NB4 cell line <REF-18> .",no
,6-1302,6-1302_v2_8@2,6-1302_v1_91@2,"In some well-documented cases, gene fusions, in addition to contributing to neoplastic transformation, produce fusion RNA and proteins used as therapeutic targets (Mertens et al. , 2015 <REF-5> , Yoshihara et al. , 2015 <REF-6> and references therein).","We also identified novel PML-RAR isoforms, shorter than the isoforms currently used in diagnosis, which could again be used in patient follow-up.",no
,6-1302,6-1302_v2_83@4,6-1302_v1_13@14,"FLT3, PML-RARA and CBFB-MYH11 are strong markers in AML, and also useful for prognostic and MRD monitoring.","For the U937 cell line, 0,1µM TTNPB associated with 1µM Targretin (LGD1069) and 0,1µM 1 alpha, 25 dihydroxyvitamin D3 (VD) were used.",no
,6-1302,6-1302_v2_13@8,6-1302_v1_54@5,"For the latter, peripheral blood mononuclear cells were collected by ficoll-hypaque density gradient and cultured at a concentration of 1×10 6 /ml, with or without 0.1µM ATRA, for 3 days.","The CBFB-MYH11 and PML-RAR fusion transcripts expressed in the AML-inv16 and AML-t(15;17) samples were identified using both RNA-seq and qPCR analysis, confirming the reliability of RNA-seq and the Crac suite in this type of analysis.",no
,6-1302,6-1302_v2_66@1,6-1302_v1_20@10,"Acute promyelocytic leukemia (APL) molecular diagnosis and minimal residual disease (MRD) monitoring are currently based on bcr1, bcr2 or bcr3 fusion transcript detection, depending on the DNA breakpoint <REF-21> , <REF-22> .",This classification resembles the one depicted in <REF-7> and can be summarized as follows:,no
8361,6-577,6-577_v2_22@2,6-577_v1_20@2,The antibody directed against Thr202/Tyr204 di-phosphorylated active ERK was from Thermo Scientific Pierce (rabbit monoclonal; MA5-15174; batch no. OC1680806); the anti-ERK1/2 antibody was from Thermo Scientific Pierce (mouse monoclonal; MA5-15605; batch no. PH1895491).,The antibody directed against Thr2020/Tyr204 di-phosphorylated active ERK was from Thermo Scientific Pierce (rabbit monoclonal; MA5-15174; batch no. OC1680806); the anti-ERK1/2 antibody was from Thermo Scientific Pierce (mouse monoclonal; MA5-15605; batch no. PH1895491).,yes
8362,6-577,6-577_v2_22@3,6-577_v1_20@3,"After extensive washing (4×30 minutes) in PBS – 0.1% Tween 20, membranes were incubated for 2 hours at room temperature in the simultaneous presence of IRDye 680RD goat anti-mouse (925-68070) and IRDye 800CW goat anti-rabbit (925-32211; Li-COR Biotechnology GmbH, Bad Homburg, Germany) secondary antibodies, or with anti-mouse and anti-rabbit HRP-conjugated antibody.","After extensive washing in PBS – 0.1% Tween 20, membranes were incubated for 2 hours at room temperature in the simultaneous presence of IRDye 680RD goat anti-mouse (925-68070) and IRDye 800CW goat anti-rabbit (925-32211; Li-COR Biotechnology GmbH, Bad Homburg, Germany) secondary antibodies, or with anti-mouse and anti-rabbit HRP-conjugated antibody.",yes
8363,6-577,6-577_v2_4@2,6-577_v1_4@2,"The ERK family is the most studied in mammals ( Boulton et al ., 1990 ; Dhillon et al ., 2007 ) because it is involved in meiosis, mitosis and post mitotic functions in differentiated cells, as well as in the oxidative stress response and wound healing ( Castellano et al. , 2014 ; Johnson & Lapadat, 2002 ; Matsubayashi et al ., 2004 ; Runchel et al ., 2011 ).","The ERK family is the most studied in mammals ( Boulton et al ., 1990 ; Dhillon et al ., 2007 ) because it is involved in meiosis, mitosis and post mitotic functions in differentiated cells, as well as in the oxidative stress response and wound healing ( Johnson & Lapadat, 2002 ; Matsubayashi et al ., 2004 ; Runchel et al ., 2011 ).",yes
8364,6-577,6-577_v2_22@4,6-577_v1_20@4,"Another set of extensive rinsing (4×30 minutes) in PBS – 0.1% Tween 20 was performed before membranes were imaged with an Odyssey device (LI-COR Biosciences, Lincoln, Nebraska) to detect fluorescence and HRP activity using Millipore ECL.","Another set of extensive rinsing in PBS – 0.1% Tween 20 was performed before membranes were imaged with an Odyssey device (LI-COR Biosciences, Lincoln, Nebraska) to detect fluorescence and HRP activity using Millipore ECL.",yes
8365,6-577,6-577_v2_25@0,6-577_v1_23@0,"In order to confirm the presence of an ERK ortholog in corals, the human protein sequence of ERK1 (NP_001035145) was compared to the transcriptome database of Stylophora pistillata using the BLAST software ( Altschul et al. , 1990 ; Karako-Lampert et al ., 2014 ).","In order to confirm the presence of an ERK ortholog in corals, the human protein sequence of ERK1 (NP_001035145) was compared to the transcriptome database of Stylophora pistillata using the BLAST software ( Altschul et al. , 1990 ; Liew et al ., 2014 ).",yes
8366,6-577,6-577_v2_25@2,6-577_v1_23@2,"This sequence (hereafter referred to as Spi-ERK for S. pistillata ERK) is the only one that shows an homology as high as 81%, 80% and 78% with the protein sequences of the cnidarians Nematostella vectensis ERK (Nv-ERK; XP_001629498.1), Hydra vulgaris ERK (Hv-ERK; XP_002154499.3) and the human MAPK3/ERK1 (Hs-ERK1), respectively ( Figure 1 ) ( Krishna et al ., 2013 ; Putnam et al ., 2007 ).","This sequence (hereafter referred to as Spi-ERK for S. pistillata ERK) is the only one that shows an homology as high as 81%, 80% and 78% with the protein sequences of the cnidarians Nematostella vectensis ERK (Nv-ERK; XP_001629498.1), Hydra vulgaris ERK (Hv-ERK; XP_002154499.3) and the human MAPK8/JNK1 (Hs-ERK1), respectively ( Figure 1 ) ( Krishna et al ., 2013 ; Putnam et al ., 2007 ).",yes
8367,6-577,6-577_v2_25@5,6-577_v1_23@5,"This result suggests that a single ortholog of ERK is present in these cnidarians, consistently with previous work where only one ERK ortholog was found ( Castellano et al. , 2014 ; Russo et al. , 2004 ) but as opposed to the two genes encoding ERKs in most mammalian genomes ( Ip & Davis, 1998 ).","This result suggests that a single ortholog of ERK is present in these cnidarians, as opposed to the two genes encoding ERKs in most mammalian genomes ( Ip & Davis, 1998 ).",yes
8368,6-577,6-577_v2_25@7,6-577_v1_23@7,"Accordingly, we detected a single immune-reactive band with the total-ERK antibody by western blot on S. pistillata extracts ( Figure 2A and Supplementary Figure S1 ).","Accordingly, we detected a single immune-reactive band with the total-ERK antibody by western blot on S. pistillata extracts ( Figure 2A and Figure S1 ).",yes
8369,6-577,6-577_v2_4@3,6-577_v1_4@3,"The ERK gene family is evolutionnarily conserved and is found in all eukaryotes, including yeasts, plants, vertebrates and invertebrates ( Chen et al ., 2001 ; Widmann et al ., 1999 ).","The ERK gene family is evolutionnarily conserved and is found in all eukaryotes, including yeasts, plants, vertebrates and anthozoans ( Chen et al ., 2001 ; Widmann et al ., 1999 ).",yes
8370,6-577,6-577_v2_31@0,6-577_v1_29@0,"RNAi interference techniques are not available in coral, and the confirmation that the immune reactive bands observed here specifically correspond to ERK could not be obtained through this method.","RNAi interference techniques are not available in coral, and the confirmation that the immune reactive bands observed here specifically correspond to ERK could not be obtained through this medium.",yes
8371,6-577,6-577_v2_31@4,6-577_v1_29@3,"When the inhibitor was added to the seawater, the intensity of the band detected by the anti-total ERK did not vary, while the intensity of the band detected with the anti-phosphorylated ERK antibody was significantly reduced ( Figure 2B and Supplementary Figure S1 ).","When the inhibitor was added to the seawater, the intensity of the band detected by the anti-total ERK did not vary, while the intensity of the band detected with the anti-phosphorylated ERK antibody was significantly reduced ( Figure 2B and Figure S1 ).",yes
8372,6-577,6-577_v2_33@0,6-577_v1_30@0,"Finally, to assess the performance of these antibodies, we compared the signal obtained on S. pistillata and human fibroblasts protein extracts ( Figure 4 and Supplementary Figure S3 ).","To assess the performance of these antibodies, we compared the signal obtained on S. pistillata and human fibroblasts protein extracts ( Figure 3 and Figure S2 ).",yes
8373,6-577,6-577_v2_46@0,6-577_v1_40@0,Supplementary Figure S1.,Figure S1.,yes
8374,6-577,6-577_v2_47@0,6-577_v1_41@0,Supplementary Figure S2.,Figure S2.,yes
8375,6-577,6-577_v2_47@1,6-577_v1_41@1,Uncropped blot images for Figure 3 and supplementary replicates.,Uncropped blot images for Figure 3 .,yes
8376,6-577,6-577_v2_5@2,6-577_v1_5@2,"According to the manufacturer’s instructions, the antibody used in this study and directed against the Thr202/Tyr204 di-phosphorylated active ERK (Thermo Scientific Pierce; MA5-15174) showed reactivity with fruit fly, human, mink, mouse, non-human primate, pig, rat and zebrafish.","According to the manufacturer’s instructions, the antibody used in this study and directed against the Thr2020/Tyr204 di-phosphorylated active ERK (Thermo Scientific Pierce; MA5-15174) showed reactivity with fruit fly, human, mink, mouse, non-human primate, pig, rat and zebrafish.",yes
8377,6-577,6-577_v2_10@1,6-577_v1_10@1,"Two small nubbins (3–5 cm long) were cut off from each mother colony, and were allowed to heal for four weeks in 15 L open system tanks before the experiments. Corals were maintained in the same conditions as the mother colonies, i.e. at 25°C, under a photosynthetic active radiation of 200 µmol photon.m -2 .s -1 provided by 400 W metal halide lamps (HPIT, Philips) and were fed twice a week with Artemia salina nauplii .","Two small nubbins were cut from each mother colony, and were allowed to heal for four weeks in 15 L open system tanks before the experiments. Corals were maintained in the same conditions as the mother colonies, i.e. at 25°C, under a photosynthetic active radiation of 200 µmol photon.m -2 .s -1 provided by 400 W metal halide lamps (HPIT, Philips) and were fed twice a week with Artemia salina nauplii .",yes
8378,6-577,6-577_v2_11@0,6-577_v1_11@0,"Immortalized skin fibroblasts (BJ-EHLT cells) were kindly provided by E. Gilson’s lab (IRCAN) and cultured in Dulbecco’s Modified Eagle’s Medium (Invitrogen, Villebon-sur-Yvette, France) supplemented with 10% heat-inactivated fetal calf serum (Dutscher, Brumath, France) at 37°C in an atmosphere of 5% CO 2 , as previously described ( Biroccio et al ., 2013 ).","Immortalized skin fibroblasts (BJ-EHLT cells) were kindly provided by E. Gilson’s lab (IRCAN) and cultured in Dulbecco's Modified Eagle's Medium (Invitrogen, Villebon-sur-Yvette, France) supplemented with 10% heat-inactivated fetal calf serum (Dutscher, Brumath, France) at 37°C in an atmosphere of 5% CO 2 , as previously described ( Biroccio et al ., 2013 ).",yes
8379,6-577,6-577_v2_17@1,6-577_v1_15@1,"Briefly, nubbins were airbrushed in 1 mL Laemmli buffer (2% SDS, 10% glycerol, 50mM Tris HCL pH7), ( Laemmli, 1970 ) using an air-pick (5 bars) to remove the totality of the tissues surrounding the skeleton was removed from coral.","Briefly, coral tissue was removed from the skeleton in 1 mL Laemmli buffer (1.5 X, Laemmli, 1970 ) using an air-pick.",yes
,6-577,6-577_v2_25@0,6-577_v1_30@0,"In order to confirm the presence of an ERK ortholog in corals, the human protein sequence of ERK1 (NP_001035145) was compared to the transcriptome database of Stylophora pistillata using the BLAST software ( Altschul et al. , 1990 ; Karako-Lampert et al ., 2014 ).","To assess the performance of these antibodies, we compared the signal obtained on S. pistillata and human fibroblasts protein extracts ( Figure 3 and Figure S2 ).",no
,6-577,6-577_v2_4@2,6-577_v1_23@2,"The ERK family is the most studied in mammals ( Boulton et al ., 1990 ; Dhillon et al ., 2007 ) because it is involved in meiosis, mitosis and post mitotic functions in differentiated cells, as well as in the oxidative stress response and wound healing ( Castellano et al. , 2014 ; Johnson & Lapadat, 2002 ; Matsubayashi et al ., 2004 ; Runchel et al ., 2011 ).","This sequence (hereafter referred to as Spi-ERK for S. pistillata ERK) is the only one that shows an homology as high as 81%, 80% and 78% with the protein sequences of the cnidarians Nematostella vectensis ERK (Nv-ERK; XP_001629498.1), Hydra vulgaris ERK (Hv-ERK; XP_002154499.3) and the human MAPK8/JNK1 (Hs-ERK1), respectively ( Figure 1 ) ( Krishna et al ., 2013 ; Putnam et al ., 2007 ).",no
,6-577,6-577_v2_17@1,6-577_v1_30@0,"Briefly, nubbins were airbrushed in 1 mL Laemmli buffer (2% SDS, 10% glycerol, 50mM Tris HCL pH7), ( Laemmli, 1970 ) using an air-pick (5 bars) to remove the totality of the tissues surrounding the skeleton was removed from coral.","To assess the performance of these antibodies, we compared the signal obtained on S. pistillata and human fibroblasts protein extracts ( Figure 3 and Figure S2 ).",no
,6-577,6-577_v2_4@2,6-577_v1_11@0,"The ERK family is the most studied in mammals ( Boulton et al ., 1990 ; Dhillon et al ., 2007 ) because it is involved in meiosis, mitosis and post mitotic functions in differentiated cells, as well as in the oxidative stress response and wound healing ( Castellano et al. , 2014 ; Johnson & Lapadat, 2002 ; Matsubayashi et al ., 2004 ; Runchel et al ., 2011 ).","Immortalized skin fibroblasts (BJ-EHLT cells) were kindly provided by E. Gilson’s lab (IRCAN) and cultured in Dulbecco's Modified Eagle's Medium (Invitrogen, Villebon-sur-Yvette, France) supplemented with 10% heat-inactivated fetal calf serum (Dutscher, Brumath, France) at 37°C in an atmosphere of 5% CO 2 , as previously described ( Biroccio et al ., 2013 ).",no
,6-577,6-577_v2_17@1,6-577_v1_4@3,"Briefly, nubbins were airbrushed in 1 mL Laemmli buffer (2% SDS, 10% glycerol, 50mM Tris HCL pH7), ( Laemmli, 1970 ) using an air-pick (5 bars) to remove the totality of the tissues surrounding the skeleton was removed from coral.","The ERK gene family is evolutionnarily conserved and is found in all eukaryotes, including yeasts, plants, vertebrates and anthozoans ( Chen et al ., 2001 ; Widmann et al ., 1999 ).",no
,6-577,6-577_v2_11@0,6-577_v1_29@3,"Immortalized skin fibroblasts (BJ-EHLT cells) were kindly provided by E. Gilson’s lab (IRCAN) and cultured in Dulbecco’s Modified Eagle’s Medium (Invitrogen, Villebon-sur-Yvette, France) supplemented with 10% heat-inactivated fetal calf serum (Dutscher, Brumath, France) at 37°C in an atmosphere of 5% CO 2 , as previously described ( Biroccio et al ., 2013 ).","When the inhibitor was added to the seawater, the intensity of the band detected by the anti-total ERK did not vary, while the intensity of the band detected with the anti-phosphorylated ERK antibody was significantly reduced ( Figure 2B and Figure S1 ).",no
,6-577,6-577_v2_4@3,6-577_v1_23@0,"The ERK gene family is evolutionnarily conserved and is found in all eukaryotes, including yeasts, plants, vertebrates and invertebrates ( Chen et al ., 2001 ; Widmann et al ., 1999 ).","In order to confirm the presence of an ERK ortholog in corals, the human protein sequence of ERK1 (NP_001035145) was compared to the transcriptome database of Stylophora pistillata using the BLAST software ( Altschul et al. , 1990 ; Liew et al ., 2014 ).",no
,6-577,6-577_v2_17@1,6-577_v1_41@1,"Briefly, nubbins were airbrushed in 1 mL Laemmli buffer (2% SDS, 10% glycerol, 50mM Tris HCL pH7), ( Laemmli, 1970 ) using an air-pick (5 bars) to remove the totality of the tissues surrounding the skeleton was removed from coral.",Uncropped blot images for Figure 3 .,no
,6-577,6-577_v2_4@3,6-577_v1_20@3,"The ERK gene family is evolutionnarily conserved and is found in all eukaryotes, including yeasts, plants, vertebrates and invertebrates ( Chen et al ., 2001 ; Widmann et al ., 1999 ).","After extensive washing in PBS – 0.1% Tween 20, membranes were incubated for 2 hours at room temperature in the simultaneous presence of IRDye 680RD goat anti-mouse (925-68070) and IRDye 800CW goat anti-rabbit (925-32211; Li-COR Biotechnology GmbH, Bad Homburg, Germany) secondary antibodies, or with anti-mouse and anti-rabbit HRP-conjugated antibody.",no
,6-577,6-577_v2_47@1,6-577_v1_4@2,Uncropped blot images for Figure 3 and supplementary replicates.,"The ERK family is the most studied in mammals ( Boulton et al ., 1990 ; Dhillon et al ., 2007 ) because it is involved in meiosis, mitosis and post mitotic functions in differentiated cells, as well as in the oxidative stress response and wound healing ( Johnson & Lapadat, 2002 ; Matsubayashi et al ., 2004 ; Runchel et al ., 2011 ).",no
,6-577,6-577_v2_33@0,6-577_v1_23@5,"Finally, to assess the performance of these antibodies, we compared the signal obtained on S. pistillata and human fibroblasts protein extracts ( Figure 4 and Supplementary Figure S3 ).","This result suggests that a single ortholog of ERK is present in these cnidarians, as opposed to the two genes encoding ERKs in most mammalian genomes ( Ip & Davis, 1998 ).",no
,6-577,6-577_v2_22@2,6-577_v1_11@0,The antibody directed against Thr202/Tyr204 di-phosphorylated active ERK was from Thermo Scientific Pierce (rabbit monoclonal; MA5-15174; batch no. OC1680806); the anti-ERK1/2 antibody was from Thermo Scientific Pierce (mouse monoclonal; MA5-15605; batch no. PH1895491).,"Immortalized skin fibroblasts (BJ-EHLT cells) were kindly provided by E. Gilson’s lab (IRCAN) and cultured in Dulbecco's Modified Eagle's Medium (Invitrogen, Villebon-sur-Yvette, France) supplemented with 10% heat-inactivated fetal calf serum (Dutscher, Brumath, France) at 37°C in an atmosphere of 5% CO 2 , as previously described ( Biroccio et al ., 2013 ).",no
,6-577,6-577_v2_5@2,6-577_v1_20@4,"According to the manufacturer’s instructions, the antibody used in this study and directed against the Thr202/Tyr204 di-phosphorylated active ERK (Thermo Scientific Pierce; MA5-15174) showed reactivity with fruit fly, human, mink, mouse, non-human primate, pig, rat and zebrafish.","Another set of extensive rinsing in PBS – 0.1% Tween 20 was performed before membranes were imaged with an Odyssey device (LI-COR Biosciences, Lincoln, Nebraska) to detect fluorescence and HRP activity using Millipore ECL.",no
,6-577,6-577_v2_33@0,6-577_v1_41@1,"Finally, to assess the performance of these antibodies, we compared the signal obtained on S. pistillata and human fibroblasts protein extracts ( Figure 4 and Supplementary Figure S3 ).",Uncropped blot images for Figure 3 .,no
,6-577,6-577_v2_22@3,6-577_v1_11@0,"After extensive washing (4×30 minutes) in PBS – 0.1% Tween 20, membranes were incubated for 2 hours at room temperature in the simultaneous presence of IRDye 680RD goat anti-mouse (925-68070) and IRDye 800CW goat anti-rabbit (925-32211; Li-COR Biotechnology GmbH, Bad Homburg, Germany) secondary antibodies, or with anti-mouse and anti-rabbit HRP-conjugated antibody.","Immortalized skin fibroblasts (BJ-EHLT cells) were kindly provided by E. Gilson’s lab (IRCAN) and cultured in Dulbecco's Modified Eagle's Medium (Invitrogen, Villebon-sur-Yvette, France) supplemented with 10% heat-inactivated fetal calf serum (Dutscher, Brumath, France) at 37°C in an atmosphere of 5% CO 2 , as previously described ( Biroccio et al ., 2013 ).",no
,6-577,6-577_v2_46@0,6-577_v1_5@2,Supplementary Figure S1.,"According to the manufacturer’s instructions, the antibody used in this study and directed against the Thr2020/Tyr204 di-phosphorylated active ERK (Thermo Scientific Pierce; MA5-15174) showed reactivity with fruit fly, human, mink, mouse, non-human primate, pig, rat and zebrafish.",no
,6-577,6-577_v2_22@3,6-577_v1_41@1,"After extensive washing (4×30 minutes) in PBS – 0.1% Tween 20, membranes were incubated for 2 hours at room temperature in the simultaneous presence of IRDye 680RD goat anti-mouse (925-68070) and IRDye 800CW goat anti-rabbit (925-32211; Li-COR Biotechnology GmbH, Bad Homburg, Germany) secondary antibodies, or with anti-mouse and anti-rabbit HRP-conjugated antibody.",Uncropped blot images for Figure 3 .,no
,6-577,6-577_v2_25@7,6-577_v1_4@3,"Accordingly, we detected a single immune-reactive band with the total-ERK antibody by western blot on S. pistillata extracts ( Figure 2A and Supplementary Figure S1 ).","The ERK gene family is evolutionnarily conserved and is found in all eukaryotes, including yeasts, plants, vertebrates and anthozoans ( Chen et al ., 2001 ; Widmann et al ., 1999 ).",no
,6-577,6-577_v2_47@0,6-577_v1_41@1,Supplementary Figure S2.,Uncropped blot images for Figure 3 .,no
8434,7-1235,7-1235_v2_2@7,7-1235_v1_2@7,The code and processed data are open sourced and available on github and contains a tutorial built into the application for assisting users.,The code and processed data is open sourced and available on github and with a tutorial built into the application for assisting users.,yes
8435,7-1235,7-1235_v2_14@1,7-1235_v1_14@1,"The interactive plots are made using shiny (v1.1.0) and ggplot2 (v3.0.0). Plots can be downloaded as .png, .pdf, or .svg files. Data used to generate the individual plots can be downloaded as .csv files.","The interactive plots are made using shiny (v1.1.0) and ggplots2 (3.0.0). Plots can be downloaded as .png, .pdf, or .svg files. Data used to generate the individual plots can be downloaded as .csv files.",yes
8436,7-1235,7-1235_v2_16@1,7-1235_v1_16@1,Kaplan-Meier curves are generated using the survival (v2.41-3) and the survminer (v0.4-1) R packages.,Kaplan-Meier curves are generated using the survival (v2.41-3) and the survminer (0.4-1) R packages.,yes
8437,7-1235,7-1235_v2_16@3,7-1235_v1_16@3,"Hazard ratios for two-group comparisons, either median or optimal cut-off, utilize the Cox proportional hazards regression model in the survival R package; with the reported hazard ratio comparing high versus low protein groups.","Hazard ratio for two-group comparisons, either median or optimal cut-off, utilize the Cox proportional hazards regression model in the survival R package; with the reported hazard ratio comparing high versus low protein groups.",yes
8438,7-1235,7-1235_v2_4@0,7-1235_v1_4@0,Improving prognostic prediction and the identification of potential therapeutic targets is of particular interest to clinicians.,Improving prognostic predictions and the identification of potential therapeutic targets is of particular interest to clinicians.,yes
8439,7-1235,7-1235_v2_16@7,7-1235_v1_16@7,Clinical variables dependent on the cancer type selected can be used to filter patients into user-defined groupings.,"Clinical variables dependent on the cancer type selected, can be used to filter patients into user-defined groupings.",yes
8440,7-1235,7-1235_v2_19@1,7-1235_v1_19@1,Hazard ratios and p-values are based on the Cox regression model.,Hazard ratios and P values are based on the Cox regression model.,yes
8441,7-1235,7-1235_v2_4@1,7-1235_v1_4@1,"Quantification of messenger RNA at a genome-wide level has proven valuable in the discovery of gene expression profiles, which can serve as biomarkers for clinical outcomes in cancer <REF-1> .","Quantification of messenger RNA levels at a genome-wide level has proven valuable in the discovery of gene expression profiles, which can serve as biomarkers for clinical outcomes in cancer <REF-1> .",yes
8442,7-1235,7-1235_v2_21@0,7-1235_v1_21@0,"In order to demonstrate the functionality of TRGAted, we present a basic survival analysis examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.","In order to demonstrate the functionality of TRGAted, we present a basic survival analysis of examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.",yes
8443,7-1235,7-1235_v2_22@3,7-1235_v1_22@3,"Samples can be divided into quartiles, tertiles, median or optimally for p-values based on the protein of interest ( Figure 2C ).","The division of samples is available into quartiles, tertiles, median or optimum based on the protein of interest ( Figure 2C ).",yes
8444,7-1235,7-1235_v2_2@0,7-1235_v1_2@0,Reverse-phase protein arrays (RPPAs) are a highthroughput approach to protein quantification utilizing antibody-based micro-to-nano scale dot blot.,Reverse-phase protein arrays (RPPAs) are a highthroughput approach to protein quantification utilizing an antibody-based micro-to-nano scale dot blot.,yes
8445,7-1235,7-1235_v2_23@2,7-1235_v1_23@2,"Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low protein groups ( Figure 3B ).","Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low proportion groups ( Figure 3B ).",yes
8446,7-1235,7-1235_v2_23@3,7-1235_v1_23@3,Protein labeling is adaptive for both the volcano plot and bar chart and will only label significant proteins (p-value ≤ 0.05).,Protein labeling is adaptive for both the volcano plot and bar chart and will only label significant proteins.,yes
8447,7-1235,7-1235_v2_26@1,7-1235_v1_26@1,"Here, RAD50 predicts poor survival in only five cancer types, prostate, adrenocortical, breast cancer, low-grade glioma, and head and neck cancers ( Figure 4A ).","Here, RAD50 predicts poor survival in only five cancer types, prostate, adrenocortical, breast cancer, low-grade glioma and head and neck cancers ( Figure 4A ).",yes
8448,7-1235,7-1235_v2_4@3,7-1235_v1_4@3,The availability of protein-level quantifications for the TCGA cohort allows for more relevant clinical outcome predictions compared to mRNA levels.,The availability of protein-level quantification for the TCGA cohorts allow for more relevant clinical outcome predictions compared to mRNA levels.,yes
8449,7-1235,7-1235_v2_30@3,7-1235_v1_30@3,"Built on the R shiny framework, a literate code architecture, the code for TRGAted is annotated and easily modified from our GitHub repository.","Built on the R Shiny framework, a literate code architecture, the code for TRGAted is annotated and easily modified from our GitHub repository.",yes
8450,7-1235,7-1235_v2_4@4,7-1235_v1_4@4,"Currently, TCGA-based applications provide entry-level analysis in correlational, differential, and survival modalities for the RPPA information.","Currently available applications provide entry-level analysis in correlational, differential, and survival modalities for the RPPA information.",yes
8451,7-1235,7-1235_v2_2@1,7-1235_v1_2@1,"Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor and metastatic samples.","Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor or metastatic samples.",yes
8452,7-1235,7-1235_v2_10@4,7-1235_v1_10@4,"Clinical and survival information for each cancer data set was downloaded from recent work by Liu, et al. <REF-5> .",Clinical and survival information for each cancer data set were downloaded from recently updated TCGA clinical data <REF-5> .,yes
8453,7-1235,7-1235_v2_2@2,7-1235_v1_2@2,Protein-level data has particular advantages in assessing putative prognostic or therapeutic targets in tumors.,This protein-level data has particular advantages in assessing putative prognostic or therapeutic targets in tumors.,yes
8454,7-1235,7-1235_v2_10@6,7-1235_v1_10@6,"Unlike other cancer types, metastatic samples were retained for skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.","Unlike other cancer types, metastatic samples were kept in the skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.",yes
8455,7-1235,7-1235_v2_2@4,7-1235_v1_2@4,"We developed a cloud-based application, TRGAted to enable researchers to better examine patient survival based on single or multiple proteins across 31 cancer types in the TCGA.","We developed a cloud-based application, TRGAted to enable researchers to better examine survival based on single or multiple proteins across 31 cancer types in the TCGA.",yes
,7-1235,7-1235_v2_4@4,7-1235_v1_26@1,"Currently, TCGA-based applications provide entry-level analysis in correlational, differential, and survival modalities for the RPPA information.","Here, RAD50 predicts poor survival in only five cancer types, prostate, adrenocortical, breast cancer, low-grade glioma and head and neck cancers ( Figure 4A ).",no
,7-1235,7-1235_v2_10@6,7-1235_v1_4@0,"Unlike other cancer types, metastatic samples were retained for skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.",Improving prognostic predictions and the identification of potential therapeutic targets is of particular interest to clinicians.,no
,7-1235,7-1235_v2_23@3,7-1235_v1_2@4,Protein labeling is adaptive for both the volcano plot and bar chart and will only label significant proteins (p-value ≤ 0.05).,"We developed a cloud-based application, TRGAted to enable researchers to better examine survival based on single or multiple proteins across 31 cancer types in the TCGA.",no
,7-1235,7-1235_v2_14@1,7-1235_v1_2@7,"The interactive plots are made using shiny (v1.1.0) and ggplot2 (v3.0.0). Plots can be downloaded as .png, .pdf, or .svg files. Data used to generate the individual plots can be downloaded as .csv files.",The code and processed data is open sourced and available on github and with a tutorial built into the application for assisting users.,no
,7-1235,7-1235_v2_26@1,7-1235_v1_19@1,"Here, RAD50 predicts poor survival in only five cancer types, prostate, adrenocortical, breast cancer, low-grade glioma, and head and neck cancers ( Figure 4A ).",Hazard ratios and P values are based on the Cox regression model.,no
,7-1235,7-1235_v2_21@0,7-1235_v1_4@3,"In order to demonstrate the functionality of TRGAted, we present a basic survival analysis examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.",The availability of protein-level quantification for the TCGA cohorts allow for more relevant clinical outcome predictions compared to mRNA levels.,no
,7-1235,7-1235_v2_10@6,7-1235_v1_21@0,"Unlike other cancer types, metastatic samples were retained for skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.","In order to demonstrate the functionality of TRGAted, we present a basic survival analysis of examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.",no
,7-1235,7-1235_v2_4@4,7-1235_v1_2@2,"Currently, TCGA-based applications provide entry-level analysis in correlational, differential, and survival modalities for the RPPA information.",This protein-level data has particular advantages in assessing putative prognostic or therapeutic targets in tumors.,no
,7-1235,7-1235_v2_4@3,7-1235_v1_10@4,The availability of protein-level quantifications for the TCGA cohort allows for more relevant clinical outcome predictions compared to mRNA levels.,Clinical and survival information for each cancer data set were downloaded from recently updated TCGA clinical data <REF-5> .,no
,7-1235,7-1235_v2_22@3,7-1235_v1_21@0,"Samples can be divided into quartiles, tertiles, median or optimally for p-values based on the protein of interest ( Figure 2C ).","In order to demonstrate the functionality of TRGAted, we present a basic survival analysis of examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.",no
,7-1235,7-1235_v2_10@6,7-1235_v1_30@3,"Unlike other cancer types, metastatic samples were retained for skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.","Built on the R Shiny framework, a literate code architecture, the code for TRGAted is annotated and easily modified from our GitHub repository.",no
,7-1235,7-1235_v2_23@2,7-1235_v1_21@0,"Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low protein groups ( Figure 3B ).","In order to demonstrate the functionality of TRGAted, we present a basic survival analysis of examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.",no
,7-1235,7-1235_v2_2@1,7-1235_v1_21@0,"Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor and metastatic samples.","In order to demonstrate the functionality of TRGAted, we present a basic survival analysis of examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.",no
,7-1235,7-1235_v2_19@1,7-1235_v1_2@2,Hazard ratios and p-values are based on the Cox regression model.,This protein-level data has particular advantages in assessing putative prognostic or therapeutic targets in tumors.,no
,7-1235,7-1235_v2_30@3,7-1235_v1_10@6,"Built on the R shiny framework, a literate code architecture, the code for TRGAted is annotated and easily modified from our GitHub repository.","Unlike other cancer types, metastatic samples were kept in the skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.",no
,7-1235,7-1235_v2_16@7,7-1235_v1_2@1,Clinical variables dependent on the cancer type selected can be used to filter patients into user-defined groupings.,"Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor or metastatic samples.",no
,7-1235,7-1235_v2_2@1,7-1235_v1_23@2,"Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor and metastatic samples.","Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low proportion groups ( Figure 3B ).",no
,7-1235,7-1235_v2_23@2,7-1235_v1_2@0,"Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low protein groups ( Figure 3B ).",Reverse-phase protein arrays (RPPAs) are a highthroughput approach to protein quantification utilizing an antibody-based micro-to-nano scale dot blot.,no
,7-1235,7-1235_v2_4@0,7-1235_v1_2@1,Improving prognostic prediction and the identification of potential therapeutic targets is of particular interest to clinicians.,"Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor or metastatic samples.",no
,7-1235,7-1235_v2_10@6,7-1235_v1_10@4,"Unlike other cancer types, metastatic samples were retained for skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.",Clinical and survival information for each cancer data set were downloaded from recently updated TCGA clinical data <REF-5> .,no
,7-1235,7-1235_v2_2@0,7-1235_v1_22@3,Reverse-phase protein arrays (RPPAs) are a highthroughput approach to protein quantification utilizing antibody-based micro-to-nano scale dot blot.,"The division of samples is available into quartiles, tertiles, median or optimum based on the protein of interest ( Figure 2C ).",no
,7-1235,7-1235_v2_14@1,7-1235_v1_22@3,"The interactive plots are made using shiny (v1.1.0) and ggplot2 (v3.0.0). Plots can be downloaded as .png, .pdf, or .svg files. Data used to generate the individual plots can be downloaded as .csv files.","The division of samples is available into quartiles, tertiles, median or optimum based on the protein of interest ( Figure 2C ).",no
8618,7-1299,7-1299_v2_16@2,7-1299_v1_16@2,"Plants also have homologues of SMG5/6/7, known as SMG7 and SMG7-like <REF-50> , and SMG1 homologues <REF-45> , <REF-51> .","Plants also have homologues of SMG5-7, known as SMG7 and SMG7-like <REF-45> , and SMG1 homologues <REF-43> , <REF-44> .",yes
8619,7-1299,7-1299_v2_16@10,7-1299_v1_16@10,"One possibility is that an alteriave kinase has replaced SMG1 and might even be ancestral and operational in many species, allowing for the loss of SMG1 <REF-51> .","Alternatively, different mechanisms have replaced SMG1 in each independent loss of SMG1, which might explain why some organisms have retained S/TQ richness within their UPF1 protein sequences while others have not <REF-43> , <REF-44> .",yes
8620,7-1299,7-1299_v2_17@0,7-1299_v1_17@0,"The SMG5/6/7 family split and diversified in the animal lineage, with the acquisition of the PIN domain in SMG5 and SMG6 <REF-27> , <REF-57> , <REF-58> .","The SMG5-7 family split and diversified in the animal lineage, with the acquisition of the PIN domain in SMG5 and SMG6 <REF-26> , <REF-55> , <REF-56> .",yes
8621,7-1299,7-1299_v2_17@2,7-1299_v1_17@2,The SMG5/6/7 family also have a role in regulating telomere length <REF-59> .,The SMG5-7 family also have a role in regulating telomere length <REF-57> .,yes
8622,7-1299,7-1299_v2_17@3,7-1299_v1_17@3,"SMG5/6/7 homologues in plants are known as SMG7, given they lack the PIN domain of SMG5 and SMG6 <REF-50> .","SMG5-7 homologues in plants are known as SMG7, given they lack the PIN domain of SMG5 and SMG6 <REF-45> .",yes
8623,7-1299,7-1299_v2_17@4,7-1299_v1_17@4,"SMG5/6/7 family members of baker’s yeast, EBS1 and EST1, also lack the PIN domain <REF-60> .","SMG5-7 family members of baker’s yeast, EBS1 and ETS1, also lack the PIN domain <REF-48> .",yes
8624,7-1299,7-1299_v2_17@5,7-1299_v1_17@5,"In baker’s yeast, EST1 is implicated in telomere regulation but not NMD, while a knockout of EBS1 reveals a mild NMD phenotype <REF-60> , <REF-61> .","In baker’s yeast, ETS1 is implicated in telomere regulation but not NMD, while a knockout of EBS1 reveals a mild NMD phenotype <REF-48> .",yes
8625,7-1299,7-1299_v2_17@7,7-1299_v1_17@7,"The UPF1 of baker’s yeast is depleted of S/TQ dipeptides <REF-45> , which once phosphorylated by SMG1, normally act as binding site for SMG5/6/7 <REF-24> .","The UPF1 of baker’s yeast is depleted of S/TQ dipeptides <REF-43> , which once phosphorylated by SMG1, normally act as binding site for SMG5-7 <REF-23> .",yes
8626,7-1299,7-1299_v2_17@9,7-1299_v1_17@9,"Tyrosine phosphorylation of UPF1 in baker’s yeast has been observed and appears to regulate the RNA helicase activity of UPF1 <REF-62> , although the role in NMD, if any, and kinase responsible is still unknown.",Tyrosine phosphorylation of UPF1 in baker’s yeast has been observed and appears to regulate the RNA helicase activity of UPF1 <REF-58> .,yes
8627,7-1299,7-1299_v2_17@27,7-1299_v1_17@18,"However, it is unclear if SMG6L directly interacts with UPF1 or if it is via phosporylation, but there is no SMG1 and classical phosphorylation sites on UPF1 <REF-9> .","However, it is unclear how it is recruited to UPF1 given the lack of SMG1 and classical phosphorylation sites on UPF1.",yes
8628,7-1299,7-1299_v2_19@1,7-1299_v1_19@1,"Generally speaking, these can be split into four major types and a spread across many unrelated eukaryotic lineages ( Figure 2 and Figure 3 ):","Generally speaking, these can be split into four major types ( Figure 2 and Figure 3 ):",yes
8629,7-1299,7-1299_v2_20@0,7-1299_v1_20@0,"- 1) Classical SMG1-dependent NMD (As exemplified by humans, worms, and moss)","- 1) Classical SMG1-dependent NMD (As exemplified by C. elegans , humans, and moss)",yes
8630,7-1299,7-1299_v2_4@1,7-1299_v1_4@1,"Analysis of mutant screens and genetic diseases identified mutations that introduced nonsense mutations, but surprisingly, these premature termination codons (PTCs) lead to a reduction in mRNA stability <REF-1> , <REF-2> .","Early mutant screens identified mutations that introduced nonsense mutations, but surprisingly, these premature termination codons (PTCs) lead to a reduction in mRNA stability <REF-1> , <REF-2> .",yes
8631,7-1299,7-1299_v2_21@2,7-1299_v1_21@2,It is possible that the NMD pathways of some species with a type 1 NMD pathway in appearance might better resemble type 2 NMD (recent SMG1-independent NMD).,It is possible that the NMD pathways of some species with a type 1 NMD pathway in appearance might better resemble type 2 NMD (Recent SMG1-independent NMD).,yes
8632,7-1299,7-1299_v2_22@1,7-1299_v1_22@1,"However, UPF1 still maintains the relatively high level of phosphorylatable S/TQ motifs <REF-45> , and phospho-UPF1 binding protein SMG7 <REF-8> , <REF-50> .","However, UPF1 still maintains the relatively high level of S/TQ dipeptide phosphorylation sites <REF-43> , and phospho-UPF1 binding protein SMG7 <REF-8> , <REF-45> .",yes
8633,7-1299,7-1299_v2_23@1,7-1299_v1_23@1,"These ancient losses of SMG1 lead to an NMD pathway without SMG1, without SMG8 and SMG9 <REF-45> , with a UPF1 depleted in S/TQ dipeptides <REF-45> , but a potential role for SMG5/6/7 proteins <REF-9> , <REF-60> , <REF-61> .","These ancient losses of SMG1 lead to an NMD pathway without SMG1, without SMG8 and SMG9 <REF-43> , with UPF1 depleted in S/TQ dipeptides <REF-43> , and an unclear role for SMG5-7 proteins <REF-48> , <REF-59> .",yes
8634,7-1299,7-1299_v2_23@2,7-1299_v1_23@2,"Future work (see below) will be needed to better understand the exact molecular role of SMG5/6/7 proteins in type 3 NMD pathways, and to understand how the NMD pathway functions without the SMG1 activating UPF1.","Future work (see below) will be needed to better understand the exact molecular role of SMG5-7 proteins in type 3 NMD pathways, and to understand how the NMD pathway functions without the SMG1 activating UPF1.",yes
8635,7-1299,7-1299_v2_4@3,7-1299_v1_4@3,"This pathway was termed nonsense-mediated mRNA decay (NMD) and is now known to regulate hundreds to thousands of transcripts in plants, animals, fungi and ciliates <REF-4> – <REF-10> .","This pathway was termed nonsense-mediated mRNA decay (NMD) and is now known to regulate hundreds to thousands of transcripts in plants, animals and fungi <REF-4> – <REF-8> .",yes
8636,7-1299,7-1299_v2_24@6,7-1299_v1_24@12,It is certainly possible that the presence of these factors do not represent a fully functional form of an NMD pathway and instead reflect the molecular reminance of a former NMD pathway whose factors have now been co-opted for other functions.,It is certainly possible that the presence of these factors do not represent a fully functional form of an NMD pathway and instead reflect the molecular reminance of a former NMD pathway whose factors have not been co-opted for other functions.,yes
8637,7-1299,7-1299_v2_27@6,7-1299_v1_27@6,"The EJC has been lost from baker’s yeast and so cannot have a role in its NMD pathway, but the EJC is involved in the fungi Neurospora crassa’s NMD pathway <REF-84> .","The EJC has been lost from baker’s yeast and so cannot have a role in its NMD pathway, but the EJC is involved in the fungi Neurospora crassa ’s NMD pathway <REF-71> .",yes
8638,7-1299,7-1299_v2_28@5,7-1299_v1_28@4,"An alternative, but not mutually exclusive model posits that longer 3’ UTRs are able to recruit more UPF1 directly bound to the 3’ UTR <REF-95> .",An alternative model posits that longer 3’ UTRs are able to recruit more UPF1 directly bound to the 3’ UTR <REF-81> .,yes
8639,7-1299,7-1299_v2_28@6,7-1299_v1_28@5,"It has been found that UPF1 coats transcripts but translation displaces UPF1 from all regions, except the 3’ UTRs <REF-96> .","It has been found that UPF1 coats transcripts, but translation displaces UPF1 from all regions, except the 3’ UTRs <REF-82> .",yes
8640,7-1299,7-1299_v2_28@7,7-1299_v1_28@6,This model suggests that a higher level of UPF1 binding increases the chances of NMD being triggered during the termination of translation; naturally long 3’ UTRs that are resistant to NMD have been observed to bind less UPF1 than susceptible long 3’ UTR transcripts <REF-95> .,This model suggests that a higher level of UPF1 binding increases the chances of NMD being triggered during the termination of translation; Naturally long 3’ UTRs that are resistant to NMD have been observed to bind less UPF1 than susceptible long 3’ UTR transcripts <REF-81> .,yes
8641,7-1299,7-1299_v2_28@8,7-1299_v1_28@7,In fact some naturally long 3’ UTR transcripts in mammals appear to be protected from NMD by various features such as a recently identified cis-sequence element in the TRAM1 gene <REF-97> or the many genes found to bind PTBP1 near the stop codon to prevent NMD <REF-46> .,"In fact, some naturally long 3’ UTR transcripts appear to be protected from NMD by various features such as a recently identified cis -sequence element in the TRAM1 gene <REF-83> or the many genes found to bind PTBP1 near the stop codon to prevent NMD <REF-84> .",yes
8642,7-1299,7-1299_v2_29@1,7-1299_v1_30@1,"While the EJC mode has been identified in plants, fungi, and animals, suggesting an ancient origin, there are many eukaryotic lineages where it has not been characterized, or does not function <REF-9> , <REF-99> .","While the EJC mode has been identified in both plants and animals, suggesting an ancient origin, there are many eukaryotic lineages where it has not been characterized, or does not function <REF-59> , <REF-87> .",yes
8643,7-1299,7-1299_v2_31@1,7-1299_v1_32@1,"It appears that a rather complex NMD pathway, belonging to the type 1 group, existed in the last eukaryotic common ancestor (see above).","It is clear that a rather complex NMD pathway, belonging to the type 1 group, existed in the last eukaryotic common ancestor (see above).",yes
8644,7-1299,7-1299_v2_31@8,7-1299_v1_32@8,"Expansion of group II introns has been proposed to have driven the evolution of the spliceosome to enhance the splicing of these selfish elements <REF-105> , the nucleus evolved to physically separate the processes of transcription and translation and allow for intron removal before translation <REF-106> , and NMD evolved to degrade intron-retaining transcripts that escaped the nucleus <REF-106> , <REF-107> .","Expansion of group II introns has been proposed to have driven the evolution of the spliceosome to enhance the splicing of these selfish elements <REF-93> , the nucleus evolved to physically separate the processes of transcription and translation and allow for intron removal before translation <REF-94> , and NMD evolved to degrade intron-retaining transcripts that escaped the nucleus <REF-94> .",yes
8645,7-1299,7-1299_v2_31@9,7-1299_v1_32@9,These adaptations ensure that transcripts with retained introns do not undergo multiple rounds of translation.,These adaptations ensure that retention of these efficiently-spliced introns would not be repeatedly translation.,yes
8646,7-1299,7-1299_v2_2@1,7-1299_v1_2@1,"In most eukaryotes, thousands of transcripts are degraded by NMD, including many important regulators of developmental and stress response pathways.","In most eukaryotes, thousands of transcripts are degraded by NMD, including many important regulators of development and stress response pathways.",yes
8647,7-1299,7-1299_v2_31@14,7-1299_v1_32@13,"The presence of NMD may act as a buffer for novel introns with weak splice sites <REF-107> , <REF-110> .",The presence of NMD may act as a buffer for novel introns with weak splice sites <REF-96> .,yes
8648,7-1299,7-1299_v2_34@0,7-1299_v1_35@0,- 1) Why is SMG1 repeatedly lost in different lineages? Is there a backup mechanism to activate UPF1 and is this conserved between the lineages that have recently lost (eg A. thaliana ) and more anciently lost (eg baker’s yeast and T . thermophila ) SMG1? Or are there multiple SMG1 replacement mechanisms?,- 1) Why is SMG1 repeatedly lost in different lineages? Is there a backup mechanism to activate UPF1 and is this conserved between the lineages that have recently lost (eg A. thaliana ) and more anciently (eg baker’s yeast and T . thermophila ) SMG1? Or are there multiple SMG1 replacement mechanisms?,yes
8649,7-1299,7-1299_v2_34@1,7-1299_v1_35@1,- 2) What recruits the RNA degradation machinery to UPF1 when SMG1 is lost and S/TQ dipeptides are depleted? Does it rely on the direct interactions of SMG5/6/7 family proteins with UPF1 or another mechanism?,- 2) What recruits the RNA degradation machinery to UPF1 when SMG1 is lost and S/TQ dipeptides are depleted? Does this still depend on the SMG5-7 family and UPF1 phosphorylation?,yes
8650,7-1299,7-1299_v2_34@3,7-1299_v1_35@3,"- 4) Can the EJC mode of PTC recognition exist without the involvement of the EJC, potentially in T. thermophila ? If so, what is the molecular basis for this and does it exist in other species?","- 4) What is the molecular basis of an EJC mode of PTC recognition when the EJC is not involved, such as in T . thermophila ?",yes
8651,7-1299,7-1299_v2_34@4,7-1299_v1_35@4,- 5) What is the precise mechanistic roles of UPF2/UPF3 in relation to EJC mode and non-EJC mode NMD pathways? How do UPF2/UPF3 get recruited to NMD targets independently of the EJC?,- 5) What is the precise mechanistic roles of UPF2/UPF3 in relation to EJC mode and non-EJC mode NMD pathways?,yes
8652,7-1299,7-1299_v2_37@1,7-1299_v1_38@1,"I propose that the classical (type 1) NMD involves UPF1-3, the UPF1-kinase SMG1 and the SMG5/6/7 family.","I propose that the classical (type 1) NMD involves UPF1-3, the UPF1-kinase SMG1 and the SMG5-7 family.",yes
8653,7-1299,7-1299_v2_6@1,7-1299_v1_6@1,"These factors were named UP-frameshift (UPF) 1, 2 and 3 in baker's yeast and Suppressors with Morphological defects on Genitalia (SMG) 2, 3 and 4 in C. elegans .","These factors were named UP-frameshift (UPF) 1, 2 and 3 in baker's yeast and suppressors with morphological defects on genitalia (SMG) 2, 3 and 4 in C. elegans .",yes
8654,7-1299,7-1299_v2_6@6,7-1299_v1_6@6,"From these early studies in C. elegans , the different NMD factors were defined by their role in the phosphorylation of UPF1.","Initially, NMD factors were defined by their role in the phosphorylation of UPF1.",yes
8655,7-1299,7-1299_v2_6@8,7-1299_v1_6@8,SMG5/6/7 bind to phosphorylated UPF1 <REF-24> and are active in the dephosphorylation of UPF1 by recruiting the PP2A phosphatase <REF-25> – <REF-27> .,SMG5-7 bind to phosphorylated UPF1 <REF-23> and are active in the dephosphorylation of UPF1 by recruiting the PP2A phosphatase <REF-24> – <REF-26> .,yes
8656,7-1299,7-1299_v2_6@10,7-1299_v1_6@10,SMG5/6/7 have a central role in recruiting the degradation machinery to degrade the NMD target <REF-28> – <REF-31> ( Figure 1 ).,SMG5-7 have a central role in recruiting the degradation machinery to degrade the NMD target ( Figure 1 ).,yes
8657,7-1299,7-1299_v2_6@16,7-1299_v1_6@16,Many NMD targets are degraded by specific “branches” of the NMD pathway that do not require UPF2 <REF-37> or UPF3b <REF-38> in mammals.,Many NMD targets use “branches” of the NMD pathway that do not require UPF2 <REF-35> or UPF3b <REF-36> .,yes
8658,7-1299,7-1299_v2_6@17,7-1299_v1_6@17,"However, all branches do involve UPF1, highlighting its central importance to the NMD pathway.","However, all branches do involve UPF1, highlightings its central importance to the NMD pathway.",yes
8659,7-1299,7-1299_v2_9@0,7-1299_v1_9@0,"Together these studies, mostly using animal systems, paint a picture where multiple factors (UPF2, UPF3, SMG1, SMG8, and SMG9) assist in the activation of UPF1, while other factors (SMG5/6/7) act to degrade an NMD target and dephosphorylate UPF1.","Together these studies, mostly using animal systems, paint a picture where multiple factors (UPF2, UPF3, SMG1, SMG8, and SMG9) assist in the activation of UPF1, while other factors (SMG5-7) act to degrade an NMD target and dephosphorylate UPF1.",yes
8660,7-1299,7-1299_v2_2@5,7-1299_v1_2@5,"Here, I detail the factors involved in NMD, our current understanding of their interactions and how they have evolved.","Here, I outline the factors involved in NMD, our current understanding of their interactions and how they have evolved.",yes
8661,7-1299,7-1299_v2_11@9,7-1299_v1_11@9,"However, it is worth noting that parasites are known to have reduced genomes relative to free-living relatives <REF-44> , and that the non-parasitic excavata Naegleria gruberi does harbor the additional NMD factors of SMG1 and SMG9 <REF-45> .","However, it is worth noting that parasites are known to have reduced genomes relative to free-living relatives <REF-42> , and that the excavata Naegleria gruberi does harbor the additional NMD factors of SMG1 and SMG9 <REF-43> .",yes
,7-1299,7-1299_v2_17@16,7-1299_v1_29@3,"Interestingly, the importance of EBS1 and NMD4 became more pronounced when yeast cells expressed a truncated UPF1 <REF-61> ; when the truncated UPF1 was expressed alone, NMD efficiency was about 30% of wild-type, in contrast, when either EBS1 and NMD4 were deleted in the truncated UPF1 lines, NMD efficiency was close to zero <REF-61> .","The existence of DSEs in other species are possible, but to date, none have been identified.",no
,7-1299,7-1299_v2_31@11,7-1299_v1_35@2,NMD has been proposed as a general protection mechanism against RNA viruses and TE expansion <REF-109> .,- 3) What precisely are the roles of SMG5-7 family members in lineages that have lost SMG1? Do their roles differ between species with type 2 (recent loss of SMG1) and type 3 (ancient loss of SMG1) NMD pathways?,no
,7-1299,7-1299_v2_28@4,7-1299_v1_16@0,Although some transcripts appear to be targeted due to their length independent of the polyA tail in yeast <REF-94> .,Further support for this comes from the examination of plant NMD pathways.,no
,7-1299,7-1299_v2_16@11,7-1299_v1_16@0,"However, this does not explain why the putative phosphorylation sites are lost in many species <REF-45> .",Further support for this comes from the examination of plant NMD pathways.,no
,7-1299,7-1299_v2_17@23,7-1299_v1_25@1,"However, it is not clear why a phosphorylation checkpoint is needed for NMD in some organisms like mammals <REF-20> , <REF-52> and plants <REF-51> , <REF-53> , but likely not others such as yeast, but direct interaction seems likely to be the mechanism.",PNRC2 is a vertebrate-specific NMD factor <REF-33> .,no
,7-1299,7-1299_v2_17@21,7-1299_v1_25@1,"This supports the notion that NMD can be activated without phosphorylation and that phosphorylation simply enhances decay under limiting conditions <REF-28> , <REF-65> .",PNRC2 is a vertebrate-specific NMD factor <REF-33> .,no
,7-1299,7-1299_v2_28@9,7-1299_v1_29@5,"In yeast, the RNA binding protein Pub1 binds to sequence elements and protects some uORF-containing transcripts from NMD <REF-98> .",Whether such a mechanism exists in other eukaryotes and what factors determine this remain to be seen.,no
,7-1299,7-1299_v2_17@19,7-1299_v1_29@0,"SMG1 mutants in fruit flies have been found to have a lesser effect on NMD than the mutation of other NMD factors <REF-54> , <REF-55> .","In baker’s yeast, a downstream sequence element (DSE) was identified <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_34@2,7-1299_v1_29@3,"- 3) If phosphorylation of UPF1 represents a checkpoint in the activation of NMD, what explains the variability of the presence of this checkpoint between species?","The existence of DSEs in other species are possible, but to date, none have been identified.",no
,7-1299,7-1299_v2_16@11,7-1299_v1_29@3,"However, this does not explain why the putative phosphorylation sites are lost in many species <REF-45> .","The existence of DSEs in other species are possible, but to date, none have been identified.",no
,7-1299,7-1299_v2_34@6,7-1299_v1_35@2,"- 7) To identify what precisely determines the accumulation of UPF1 on some transcripts, and why this appears to be dependent UPF1 ATPase activity <REF-111> .",- 3) What precisely are the roles of SMG5-7 family members in lineages that have lost SMG1? Do their roles differ between species with type 2 (recent loss of SMG1) and type 3 (ancient loss of SMG1) NMD pathways?,no
,7-1299,7-1299_v2_17@15,7-1299_v1_25@1,"Transcripts responsive to the deletion of UPF1 also increased in deletions of EBS1 and NMD4, however, to a lesser extent <REF-61> .",PNRC2 is a vertebrate-specific NMD factor <REF-33> .,no
,7-1299,7-1299_v2_34@7,7-1299_v1_25@3,"- 8) What is the mechanism leading to NMD of uORF transcripts? Is it EJC mode, long 3’ UTR mode, both or neither? This will need to be done for each uORF transcript of interest.",More forward screens and biochemical studies are likely to yield more species-specific factors.,no
,7-1299,7-1299_v2_17@12,7-1299_v1_29@0,"It could be that RNA decay enzymes are recruited directly to UPF1, alternative mechanism to the phosphorylation-mediated recruitment <REF-61> , <REF-63> , <REF-64> .","In baker’s yeast, a downstream sequence element (DSE) was identified <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_31@11,7-1299_v1_29@0,NMD has been proposed as a general protection mechanism against RNA viruses and TE expansion <REF-109> .,"In baker’s yeast, a downstream sequence element (DSE) was identified <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_24@7,7-1299_v1_29@0,"NMD factors do function in other pathways, for example, UPF1 is known to be involved with mammalian DNA replication <REF-75> .","In baker’s yeast, a downstream sequence element (DSE) was identified <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_16@12,7-1299_v1_25@1,One exciting possibility is that direct interactions between SMG5/6/7 family proteins is sufficient for NMD to be activated in some species (see below).,PNRC2 is a vertebrate-specific NMD factor <REF-33> .,no
,7-1299,7-1299_v2_17@21,7-1299_v1_29@1,"This supports the notion that NMD can be activated without phosphorylation and that phosphorylation simply enhances decay under limiting conditions <REF-28> , <REF-65> .","When this sequence is downstream of a stop codon, NMD is elicited, likely through the recruitment of an RNA binding protein <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_34@7,7-1299_v1_16@0,"- 8) What is the mechanism leading to NMD of uORF transcripts? Is it EJC mode, long 3’ UTR mode, both or neither? This will need to be done for each uORF transcript of interest.",Further support for this comes from the examination of plant NMD pathways.,no
,7-1299,7-1299_v2_16@12,7-1299_v1_29@3,One exciting possibility is that direct interactions between SMG5/6/7 family proteins is sufficient for NMD to be activated in some species (see below).,"The existence of DSEs in other species are possible, but to date, none have been identified.",no
,7-1299,7-1299_v2_34@2,7-1299_v1_25@1,"- 3) If phosphorylation of UPF1 represents a checkpoint in the activation of NMD, what explains the variability of the presence of this checkpoint between species?",PNRC2 is a vertebrate-specific NMD factor <REF-33> .,no
,7-1299,7-1299_v2_16@11,7-1299_v1_25@1,"However, this does not explain why the putative phosphorylation sites are lost in many species <REF-45> .",PNRC2 is a vertebrate-specific NMD factor <REF-33> .,no
,7-1299,7-1299_v2_17@19,7-1299_v1_29@2,"SMG1 mutants in fruit flies have been found to have a lesser effect on NMD than the mutation of other NMD factors <REF-54> , <REF-55> .",This mechanism is very similar to the way in which the EJC mode works in animals and plants.,no
,7-1299,7-1299_v2_24@7,7-1299_v1_16@0,"NMD factors do function in other pathways, for example, UPF1 is known to be involved with mammalian DNA replication <REF-75> .",Further support for this comes from the examination of plant NMD pathways.,no
,7-1299,7-1299_v2_34@6,7-1299_v1_29@1,"- 7) To identify what precisely determines the accumulation of UPF1 on some transcripts, and why this appears to be dependent UPF1 ATPase activity <REF-111> .","When this sequence is downstream of a stop codon, NMD is elicited, likely through the recruitment of an RNA binding protein <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_16@12,7-1299_v1_29@5,One exciting possibility is that direct interactions between SMG5/6/7 family proteins is sufficient for NMD to be activated in some species (see below).,Whether such a mechanism exists in other eukaryotes and what factors determine this remain to be seen.,no
,7-1299,7-1299_v2_17@18,7-1299_v1_29@2,"This raises the possibility that in species lacking SMG1, the phosphorylation checkpoint of NMD is not required and SMG5/6/7 family proteins directly interact with UPF1 when at a PTC.",This mechanism is very similar to the way in which the EJC mode works in animals and plants.,no
,7-1299,7-1299_v2_28@4,7-1299_v1_29@0,Although some transcripts appear to be targeted due to their length independent of the polyA tail in yeast <REF-94> .,"In baker’s yeast, a downstream sequence element (DSE) was identified <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_17@18,7-1299_v1_29@0,"This raises the possibility that in species lacking SMG1, the phosphorylation checkpoint of NMD is not required and SMG5/6/7 family proteins directly interact with UPF1 when at a PTC.","In baker’s yeast, a downstream sequence element (DSE) was identified <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_16@12,7-1299_v1_16@0,One exciting possibility is that direct interactions between SMG5/6/7 family proteins is sufficient for NMD to be activated in some species (see below).,Further support for this comes from the examination of plant NMD pathways.,no
,7-1299,7-1299_v2_24@7,7-1299_v1_25@3,"NMD factors do function in other pathways, for example, UPF1 is known to be involved with mammalian DNA replication <REF-75> .",More forward screens and biochemical studies are likely to yield more species-specific factors.,no
,7-1299,7-1299_v2_17@16,7-1299_v1_29@0,"Interestingly, the importance of EBS1 and NMD4 became more pronounced when yeast cells expressed a truncated UPF1 <REF-61> ; when the truncated UPF1 was expressed alone, NMD efficiency was about 30% of wild-type, in contrast, when either EBS1 and NMD4 were deleted in the truncated UPF1 lines, NMD efficiency was close to zero <REF-61> .","In baker’s yeast, a downstream sequence element (DSE) was identified <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_17@14,7-1299_v1_16@0,"NMD4, like SMG6, contains a PIN domain <REF-61> .",Further support for this comes from the examination of plant NMD pathways.,no
,7-1299,7-1299_v2_22@5,7-1299_v1_29@1,"TOR is the only other related kinase in A. thaliana , and is involved with the regulation of translation, although the phenotype of TOR knockdown lines do not appear to match those of NMD factors in A. thaliana <REF-71> .","When this sequence is downstream of a stop codon, NMD is elicited, likely through the recruitment of an RNA binding protein <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_17@22,7-1299_v1_35@2,"Interestingly, mammalian SMG6 has also been found to bind UPF1 independent of phosphorylation <REF-66> , <REF-67> , suggesting some level of conservation of phosphorylation-independent recruitment of decay factors in NMD.",- 3) What precisely are the roles of SMG5-7 family members in lineages that have lost SMG1? Do their roles differ between species with type 2 (recent loss of SMG1) and type 3 (ancient loss of SMG1) NMD pathways?,no
,7-1299,7-1299_v2_25@2,7-1299_v1_35@2,"Protein-protein interaction studies in yeast have revealed the species specific factor NMD4 <REF-61> , <REF-77> .",- 3) What precisely are the roles of SMG5-7 family members in lineages that have lost SMG1? Do their roles differ between species with type 2 (recent loss of SMG1) and type 3 (ancient loss of SMG1) NMD pathways?,no
,7-1299,7-1299_v2_25@2,7-1299_v1_25@3,"Protein-protein interaction studies in yeast have revealed the species specific factor NMD4 <REF-61> , <REF-77> .",More forward screens and biochemical studies are likely to yield more species-specific factors.,no
,7-1299,7-1299_v2_16@11,7-1299_v1_29@5,"However, this does not explain why the putative phosphorylation sites are lost in many species <REF-45> .",Whether such a mechanism exists in other eukaryotes and what factors determine this remain to be seen.,no
,7-1299,7-1299_v2_24@8,7-1299_v1_29@0,"Although in mammals, some NMD transcripts only require a subset of NMD factors <REF-37> , <REF-38> , <REF-76> , these branches of the NMD pathway support the notion that a more reduced NMD pathway may exist.","In baker’s yeast, a downstream sequence element (DSE) was identified <REF-85> , <REF-86> .",no
,7-1299,7-1299_v2_25@3,7-1299_v1_29@3,Performing similar work in other species is likely to reveal more species/lineage specific factors.,"The existence of DSEs in other species are possible, but to date, none have been identified.",no
,7-1299,7-1299_v2_24@8,7-1299_v1_35@2,"Although in mammals, some NMD transcripts only require a subset of NMD factors <REF-37> , <REF-38> , <REF-76> , these branches of the NMD pathway support the notion that a more reduced NMD pathway may exist.",- 3) What precisely are the roles of SMG5-7 family members in lineages that have lost SMG1? Do their roles differ between species with type 2 (recent loss of SMG1) and type 3 (ancient loss of SMG1) NMD pathways?,no
,7-1299,7-1299_v2_31@11,7-1299_v1_29@2,NMD has been proposed as a general protection mechanism against RNA viruses and TE expansion <REF-109> .,This mechanism is very similar to the way in which the EJC mode works in animals and plants.,no
,7-1299,7-1299_v2_17@22,7-1299_v1_29@5,"Interestingly, mammalian SMG6 has also been found to bind UPF1 independent of phosphorylation <REF-66> , <REF-67> , suggesting some level of conservation of phosphorylation-independent recruitment of decay factors in NMD.",Whether such a mechanism exists in other eukaryotes and what factors determine this remain to be seen.,no
,7-1299,7-1299_v2_17@15,7-1299_v1_29@3,"Transcripts responsive to the deletion of UPF1 also increased in deletions of EBS1 and NMD4, however, to a lesser extent <REF-61> .","The existence of DSEs in other species are possible, but to date, none have been identified.",no
8677,7-1306,7-1306_v2_22@6,7-1306_v1_22@6,"Additional statistical information about each gene is provided in interactive plots, such as the log-fold change comparing the average expression of a gene in one cluster versus the average expression in all other cells, the percentage of cells within the cluster that express the gene, and the percentage of cells in other clusters that express the gene.","Additional statistical information about each gene is provided as an interactive table, such as the log-fold change comparing the average expression of a gene in one cluster versus the average expression in all other cells, the percentage of cells within the cluster that express the gene, and the percentage of cells in other clusters that express the gene.",yes
8678,7-1306,7-1306_v2_26@1,7-1306_v1_28@1,First the data can be exported as a set of CSV (comma separated values) files suited for further independent analysis and data sharing.,"First, as a set of CSV (comma separated values) files suited for further independent analysis and data sharing.",yes
8679,7-1306,7-1306_v2_26@3,7-1306_v1_28@3,"The data can also be exported as an H5AD file that can be re-imported into this notebook’s workflow, retaining the generated results.","Second, as an H5AD file that can be re-imported into this notebook’s workflow, retaining the generated results.",yes
8680,7-1306,7-1306_v2_28@0,7-1306_v1_30@0,"To run this notebook, the user needs a GenePattern account or can create one on the GenePattern Notebook site .","To run the notebook, the user is required to have a GenePattern account that can be created on the GenePattern Notebook site .",yes
8681,7-1306,7-1306_v2_36@4,7-1306_v1_35@2,"As the GenePattern Notebook user interface gains more features, the notebook will also be able to take advantage of these features.","As the GenePattern Notebook user interface gains more features, the notebook will also grow to take advantage of these features.",yes
8682,7-1306,7-1306_v2_9@2,7-1306_v1_9@2,"Text files from read count quantification tools like HTSeq ( Anders et al ., 2015 ) and Kallisto ( Bray et al ., 2016 ) are supported as input.","Gene by cell matrices generated by the 10X Genomics Cell Ranger pipeline and flat text files from read count quantification tools like HTSeq ( Anders et al. , 2015 ) and kallisto ( Bray et al. , 2016 ) are supported as input.",yes
8685,7-1306,7-1306_v2_14@7,7-1306_v1_14@7,We also give users the option to remove sources of technical variation by performing linear regression on the total number of molecules detected and the percentage of reads mapped to mitochondrial genes.,"To remove sources of technical variation, linear regression is used to diminish the effects of the number of detected molecules and the percentage of counts mapped to mitochondrial genes.",yes
8686,7-1306,7-1306_v2_14@11,7-1306_v1_14@10,A plot showing the percent variance explained of each principal component is then displayed so the user may choose a reasonable number of principal components for use in clustering ( Figure 2B ).,A plot showing the standard deviation of each principal component is then displayed so the user may choose a reasonable number of principal components for use in clustering ( Figure 2B ).,yes
,7-1306,7-1306_v2_14@8,7-1306_v1_22@7,"As there is debate in the field concerning the correctness of using regression on covariates such as percent mitochondrial reads ( Batson, 2018 ) we have made this step optional.",The percent expression metric shows whether a reasonable fraction of cells expresses the gene.,no
,7-1306,7-1306_v2_35@2,7-1306_v1_22@7,"This resource is freely available to the community and the analysis described in this notebook falls well within the per-account memory allocations (see the Scanpy authors’ benchmarking in Wolf et al ., 2018 ; Eulenberg et al ., 2017a ; Eulenberg et al. , 2017b ).",The percent expression metric shows whether a reasonable fraction of cells expresses the gene.,no
,7-1306,7-1306_v2_9@1,7-1306_v1_22@7,Users may upload a single expression file and specify whether the rows represent genes and the columns represent cells or vice-versa.,The percent expression metric shows whether a reasonable fraction of cells expresses the gene.,no
,7-1306,7-1306_v2_36@3,7-1306_v1_22@7,"We also encourage advanced users to copy the notebook, add new approaches or features, and publish them as a community notebook in the GenePattern Notebook repository.",The percent expression metric shows whether a reasonable fraction of cells expresses the gene.,no
,7-1306,7-1306_v2_14@12,7-1306_v1_9@1,"We note that this notebook is a living, open source document and can be modified as the single cell community’s perspectives on best practices evolves.",Each row of the matrix should represent a gene and each column represents a cell.,no
,7-1306,7-1306_v2_14@8,7-1306_v1_9@1,"As there is debate in the field concerning the correctness of using regression on covariates such as percent mitochondrial reads ( Batson, 2018 ) we have made this step optional.",Each row of the matrix should represent a gene and each column represents a cell.,no
,7-1306,7-1306_v2_35@0,7-1306_v1_9@1,We encourage users to perform analyses on their own data using this notebook.,Each row of the matrix should represent a gene and each column represents a cell.,no
,7-1306,7-1306_v2_9@1,7-1306_v1_9@1,Users may upload a single expression file and specify whether the rows represent genes and the columns represent cells or vice-versa.,Each row of the matrix should represent a gene and each column represents a cell.,no
8755,7-1891,7-1891_v2_2@5,7-1891_v1_2@5,The proportion of active users of LARCs in Pameungpeuk is very low (10.66%).,The proportion of active users of LARCs in Pameungpeuk is also very low (10.66%).,yes
8756,7-1891,7-1891_v2_28@3,7-1891_v1_27@3,Privacy and confidentiality of the clients’ information was observed through the use of data collection with coded identification numbers.,Privacy and confidentiality of the clients' information was observed through the use of data collection with coded identification numbers.,yes
8757,7-1891,7-1891_v2_2@6,7-1891_v1_2@6,This study aimed to analyze factors associated with the utilization of LARCs among family planning clients at the Pameungpeuk Rural Hospital.,This study therefore aimed to analyze factors associated with the utilization of LARCs among family planning clients at the Pameungpeuk Rural Hospital.,yes
8758,7-1891,7-1891_v2_3@2,7-1891_v1_3@2,We performed statistical analyses using chi-square test.,We performed statistical analyses using a chi-square test.,yes
8759,7-1891,7-1891_v2_9@1,7-1891_v1_9@1,"One of the Indonesian government’s efforts to reduce population growth and its associated problems, including high mortality rates, are family planning programs promoting contraceptive use, which educate patients and clients regarding family planning and contraception itself <REF-4> .","One of the Indonesian government's efforts to reduce population growth and its associated problems, including high mortality rates, are family planning programs promoting contraceptive use, which educate patients and clients regarding family planning and contraception itself <REF-4> .",yes
8760,7-1891,7-1891_v2_2@2,7-1891_v1_2@2,"However, the rate of termination of the use of short-acting contraceptives by family planning clients was higher than other methods, therefore the use of short-acting contraceptives is less efficient than long acting reversible contraceptives (LARCs) for longer term spacing because it is easy to skip a treatment for economic or other reasons, which can result in unintended pregnancy.","However, the rate of termination of the use of short-acting contraceptives by family planning clients was higher than other methods, therefore the use of short-acting contraceptive is not effective enough for use.",yes
8761,7-1891,7-1891_v2_13@3,7-1891_v1_13@3,"Some people have negative beliefs and misunderstanding about LARCs, additionally the lack of knowledge and skills of health workers can also influence access to the use of LARCs methods <REF-20> , <REF-21> .","Widespread of myths and misunderstandings about LARCs and the lack of knowledge and skills of health workers can also influence access to the use of LARCs methods <REF-19> , <REF-20> .",yes
8762,7-1891,7-1891_v2_2@3,7-1891_v1_2@3,"Therefore, the National Family Planning Program in Indonesia is encouraging the use of LARCs to control population growth.","In anticipating the decreased use of short-acting contraceptives while also seeking to control population growth, the National Family Planning Program in Indonesia is encouraging the use of long-acting reversible contraceptives (LARCs).",yes
8763,7-1891,7-1891_v2_19@0,7-1891_v1_19@0,An interviewer-administered questionnaire <REF-22> was used to collect data for this study.,An interviewer-administered questionnaire ( Supplementary File 1 ) was used to collect data for this study.,yes
8764,7-1891,7-1891_v2_22@0,7-1891_v1_21@0,The interviewer-administered questionnaire <REF-22> was conducted with consenting consecutive clients as they accessed family planning services until the desired sample size was achieved.,The interviewer-administered questionnaire ( Supplementary File 1 ) was conducted with consenting consecutive clients as they accessed family planning services until the desired sample size was achieved.,yes
,7-1891,7-1891_v2_12@2,7-1891_v1_49@0,"In BKKBN 2017 shown that the realization of modern contraceptive prevalence rate such as use of female sterilization (MOW), male sterilization (MOP), pill, IUD, injection, implant KB (Implant) and condoms on 2017 is 57.6% of the target of 60.9% or achievement of 94.58%.","Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,7-1891,7-1891_v2_12@3,7-1891_v1_49@0,"Furthermore, the realization of unmet need which is defined as the percentage of married women who do not want to have more children but do not use contraception in 2017 is 17.5%, so that the achievement is 58.63%.","Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,7-1891,7-1891_v2_12@4,7-1891_v1_49@0,"Moreover, the percentage of active contraceptive participants using LARCs was 21.5% of the target of 21.7% or achievement of 99.07%.","Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,7-1891,7-1891_v2_12@5,7-1891_v1_49@0,"Additionally, the realization of contraceptive discontinuation rate in 2017 is 22.3% of the target of 25.3% or achievement of 88.14% <REF-14> .","Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,7-1891,7-1891_v2_20@0,7-1891_v1_49@0,The instrument used in this study was a questionnaire containing questions related to factors associated with the utilization of long-acting reversible contraceptives in the Pameungpeuk Rural Hospital work area.,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,7-1891,7-1891_v2_20@1,7-1891_v1_49@0,"The questionnaire consisted of questions about the behavior of use LARCs methods, knowledge, beliefs, attitudes, exposure to information on LARCs, skills of health workers, support of partner, support of friends, support of health workers, and support of community leaders <REF-22> .","Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,7-1891,7-1891_v2_43@5,7-1891_v1_49@0,This study was done before the new National Health Insurance Scheme was rolled out and widely available.,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,7-1891,7-1891_v2_43@6,7-1891_v1_49@0,"National health insurance has in recent years, overcome many of the cost barriers to contraceptive use, including use of LARCs.","Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,7-1891,7-1891_v2_50@0,7-1891_v1_49@0,Extended data,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,7-1891,7-1891_v2_51@0,7-1891_v1_49@0,Extended data contains the questionnaire (Bahasa and English versions) and how data were measured: https://doi.org/10.6084/m9.figshare.9734561.v2 <REF-22> .,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
9715,8-1204,8-1204_v2_4@2,8-1204_v1_4@2,In two samples there were histological changes detected that might have suggested the underlying presence of a type IV collagen disorder.,In two samples there were histological changes detected that might have suggested the underlying presence of a collagen IV disorder.,yes
9716,8-1204,8-1204_v2_27@1,8-1204_v1_27@1,Two of these samples showed signs on electron microscopy that might be consistent with an underlying type IV collagen glomerular basement membrane disorder.,Two of these samples showed signs on electron microscopy that might be consistent with an underlying collagen IV glomerular basement membrane disorder.,yes
9717,8-1204,8-1204_v2_32@1,8-1204_v1_32@1,"Of those that did, two were reported to have characteristics that might be consistent with an underlying type IV collagen disorder.","Of those that did, two were reported to have characteristics that might be consistent with an underlying collagen IV disorder.",yes
9718,8-1204,8-1204_v2_32@3,8-1204_v1_32@3,"The first sample, in which the patient had the tip variant of FSGS, was suggested to be consistent with TBMN whereas there was no pathological comment made about the second from a patient with FSGS NOS.",The first sample was suggested to be consistent with TBMN whereas there was no pathological comment made about the second.,yes
9719,8-1204,8-1204_v2_32@4,8-1204_v1_32@4,"FSGS (NOS) was the most common lesion described in this study, consistent with prior reports <REF-2> .","FSGS (NOS) was the most common lesion described in this study, which is consistent with prior reports <REF-2> .",yes
9720,8-1204,8-1204_v2_32@5,8-1204_v1_32@5,"Notably, close to one in three cases of primary FSGS were not proceeding to electron microscopy despite an indication to do so and 1 in 20 cases within our cohort had structural changes that were consistent with an underlying type IV collagen variant.","Notably, close to one in three cases of primary FSGS were not proceeding to electron microscopy despite an indication to do so and 1 in 20 cases within our cohort had structural changes that were consistent with an underlying collagen IV variant.",yes
9721,8-1204,8-1204_v2_33@0,8-1204_v1_33@0,"Curiously, neither of the two patients in whom electron microscopy was suggestive of an underlying type IV collagen disorder was noted to have haematuria on their urinalysis at the time of presentation.","Curiously, neither of the two patients in whom electron microscopy was suggestive of an underlying collagen IV disorder was noted to have haematuria on their urinalysis at the time of presentation.",yes
9722,8-1204,8-1204_v2_33@3,8-1204_v1_33@2,Unfortunately due to the retrospective nature of this study we were unable to send any samples for immunostaining of type IV collagen.,"Unfortunately, due to the retrospective nature of this study we were unable to send any samples for immunostaining of collagen IV.",yes
9723,8-1204,8-1204_v2_34@0,8-1204_v1_34@0,"There is an increasing body of evidence indicating that inheritable variants in COL4A may underlie a proportion of cases of FSGS, with up to 12.5% cases of autosomal dominant FSGS attributable to COL4A3 in some cohorts <REF-10> .","There is an increasing body of evidence indicating that inheritable variants in collagen IV genes may underlie a proportion of cases of FSGS, with up to 12.5% cases of autosomal dominant FSGS attributable to COL4A3 in some cohorts <REF-15> .",yes
9724,8-1204,8-1204_v2_34@1,8-1204_v1_34@1,Not subjecting these renal biopsy samples to electron microscopy represents a potential gap in the investigation and subsequent management of such patients given they are much less likely to respond to immunosuppressive therapy <REF-16> which has otherwise been classically indicated ( Figure 2 ).,Not subjecting these renal biopsy samples to electron microscopy represents a potential gap in the investigation and subsequent management of such patients given they are much less likely to respond to immunosuppressive therapy <REF-16> which has otherwise been classically indicated.,yes
9725,8-1204,8-1204_v2_37@0,8-1204_v1_35@0,"This study was designed as a retrospective cohort study looking at the number of samples sent for electron microscopy, as well as any potential changes which might be consistent with a type IV collagen glomerular basement membrane disorder.","This study was designed as a retrospective cohort study looking at the number of samples sent for electron microscopy, as well as any potential changes which might be consistent with a collagen IV glomerular basement membrane disorder.",yes
9726,8-1204,8-1204_v2_37@1,8-1204_v1_35@1,It is important to recognise that not all groups have found the characteristic changes associated with the type IV collagen disorders such as Alport’s Syndrome or TBMN on electron microscopy.,It is important to recognise that not all groups have found the characteristic changes associated with the collagen IV disorders such as Alport’s Syndrome or TBMN on electron microscopy.,yes
9727,8-1204,8-1204_v2_37@3,8-1204_v1_35@3,It is thus possible that a lack of classical findings for a type IV collagen glomerular basement membrane disorder may have accounted for the low number of those with GBM features on electron microscopy consistent with a type IV collagen disorder noted within our study.,It is thus possible that a lack of classical findings for a collagen IV glomerular basement membrane disorder may have accounted for the low number of those with GBM features on electron microscopy consistent with a collagen 4 disorder noted within our study.,yes
9728,8-1204,8-1204_v2_38@0,8-1204_v1_36@0,"The process by which variants within the COL4A genes might cause FSGS remains unclear, particularly given their clear association with Alport Syndrome and TBMN.","The process by which variants within the collagen IV genes might cause FSGS remains unclear, particularly given their clear association with Alport Syndrome and TBMN.",yes
9729,8-1204,8-1204_v2_38@1,8-1204_v1_36@1,"One proposal is that the ultrastructural changes induced by the type IV collagen variants, perhaps under the influence of modifier genes such as laminin, result in impaired podocyte attachment to the glomerular basement membrane which leads to accelerated podocyte detachment, subsequent foot process effacement as a response to the increased shear stress induced by the denuded basement membrane and at a critical level of podocyte loss collapse of the capillary network with the appearance of the classical segmental sclerotic lesion <REF-2> , <REF-4> , <REF-19> .","One proposal is that the ultrastructural changes induced by the collagen IV variants, perhaps under the influence of modifier genes such as laminin, result in impaired podocyte attachment to the glomerular basement membrane which leads to accelerated podocyte detachment, subsequent foot process effacement as a response to the increased shear stress induced by the denuded basement membrane and at a critical level of podocyte loss collapse of the capillary network with the appearance of the classical segmental sclerotic lesion <REF-2> , <REF-4> , <REF-17> .",yes
9730,8-1204,8-1204_v2_38@2,8-1204_v1_36@2,"It also remains unclear as to whether the changes of FSGS are a secondary process occurring in those with TBMN or whether the type IV collagen variants are capable of causing primary FSGS <REF-7> , <REF-20> .","It also remains unclear as to whether the changes of FSGS are a secondary process occurring in those with TBMN or whether the collagen 4 variants are capable of causing primary FSGS <REF-7> , <REF-18> .",yes
9731,8-1204,8-1204_v2_38@3,8-1204_v1_36@3,"FSGS occurring as a secondary process to other basement membrane abnormalities may explain why immunosuppressive therapy has traditionally been less effective in inherited forms of FSGS, although there are case reports of the successful use of the calcineurin inhibitor cyclosporine for some patients harbouring inheritable type IV collagen disorders <REF-6> .","FSGS occurring as a secondary process to other basement membrane abnormalities may explain why immunosuppressive therapy has traditionally been less effective in inherited forms of FSGS, although there are case reports of the successful use of the calcineurin inhibitor cyclosporine for some patients harbouring inheritable collagen 4 disorders <REF-6> .",yes
9732,8-1204,8-1204_v2_39@1,8-1204_v1_37@1,"This may have potentially led to inadvertently overlooking characteristic basement membrane abnormalities, which may suggest an underlying and heritable type IV collagen disorder.","This may have potentially led to inadvertently overlooking characteristic basement membrane abnormalities, which may suggest an underlying and heritable collagen IV disorder.",yes
9733,8-1204,8-1204_v2_0@0,8-1204_v1_0@0,The use of electron microscopy in the diagnosis of focal segmental glomerulosclerosis: are current pathological techniques missing important abnormalities in the glomerular basement membrane?,An audit of electron microscopy in the diagnosis of focal segmental glomerulosclerosis: are current pathological techniques missing important abnormalities in the glomerular basement membrane?,yes
9734,8-1204,8-1204_v2_9@7,8-1204_v1_9@7,"Variants in COL4A3, COL4A4 and COL4A5 which encode the α3, α4 and α5 chains of type IV collagen respectively, the major constituent of the GBM previously linked to both Alport syndrome and thin basement membrane nephropathy (TBMN) may also underlie cases of FSGS <REF-4> .","Variants in COL4A3, COL4A4 and COL4A5 , which encode the α3, α4 and α5 chains of collagen IV, respectively, the major constituent of the GBM, previously linked to both Alport syndrome and thin basement membrane nephropathy (TBMN), may also underlie cases of FSGS <REF-4> .",yes
9735,8-1204,8-1204_v2_10@0,8-1204_v1_10@0,"The relationship between the three renal conditions intertwined around variants in the COL4A genes, Alport syndrome, TBMN, and FSGS is complex and incompletely understood.","The relationship between the three renal conditions intertwined around variants in the collagen IV genes, Alport syndrome, TBMN, and FSGS is complex and incompletely understood.",yes
9736,8-1204,8-1204_v2_10@1,8-1204_v1_10@1,"Whilst previously it was believed that patients heterozygous for variants in COL4A3 and COL4A4 would develop TBMN with persistent microscopic haematuria and an otherwise benign prognosis, this traditional thinking has been overturned by the discovery that some pedigrees with these variants will go on to develop significant proteinuria, FSGS lesions and the potential for progressive CKD or ESKD <REF-6> , <REF-7> .","Whilst previously it was believed that patients heterozygous for variants in COL4A3 and COL4A4 would develop TBMN with persistent microscopic haematuria and an otherwise benign prognosis, this traditional thinking has been overturned by the discovery that some pedigrees with these variants will go on to develop significant proteinuria, FSGS lesions and the potential for progressive CKD or ESRD <REF-6> , <REF-7> .",yes
9737,8-1204,8-1204_v2_10@2,8-1204_v1_10@2,"More recently, targeted gene sequencing of adults thought to have primary FSGS or steroid resistant nephrotic syndrome (the paediatric equivalent) found that pathogenic or likely pathogenic variants in COL4A genes were present in up to 38% of families with familial FSGS, and 3% of those with sporadic FSGS <REF-3> , <REF-8> .","More recently, targeted gene sequencing of adults thought to have primary FSGS or steroid resistant nephrotic syndrome (the paediatric equivalent) found that pathogenic or likely pathogenic variants in collagen IV genes were present in up to 38% of families with familial FSGS, and 3% of those with sporadic FSGS <REF-3> , <REF-8> .",yes
9738,8-1204,8-1204_v2_11@4,8-1204_v1_11@4,"The other potential renal lesions that may be caused by COL4A gene variants, Alport syndrome and TBMN, cause thinning, lamellation and fraying of the GBM and may also be associated with podocyte foot process effacement, all of which require electron microscopy to visualise <REF-4> .","The other potential renal lesions that may be caused by collagen IV gene variants, Alport syndrome and TBMN, cause thinning, lamellation and fraying of the GBM and may also be associated with podocyte foot process effacement, all of which require electron microscopy to visualise <REF-4> .",yes
9739,8-1204,8-1204_v2_11@11,8-1204_v1_11@8,Given that the pathological diagnosis of FSGS does not routinely require electron microscopy it is therefore conceivable that GBM lesions potentially associated with an underlying type IV collagen variant may have been missed.,Given that the pathological diagnosis of FSGS does not routinely require electron microscopy it is therefore conceivable that GBM lesions potentially associated with an underlying collagen 4 variant may have been missed.,yes
9740,8-1204,8-1204_v2_11@12,8-1204_v1_11@9,This represents an opportunity to reflect on our prior clinical behaviour to see if our multidisciplinary diagnostic practice may require improvement.,This represents an opportunity to audit our prior clinical behaviour to see if our multidisciplinary diagnostic practice may require improvement.,yes
9741,8-1204,8-1204_v2_11@13,8-1204_v1_11@10,We conducted a retrospective cohort analysis of prior renal biopsy results in two tertiary centres to see how many of those that had been given a histological diagnosis of FSGS were sent for subsequent electron microscopy.,We conducted a retrospective audit of prior renal biopsy results in two tertiary centres to see how many of those that had been given a histological diagnosis of FSGS were sent for subsequent electron microscopy.,yes
9742,8-1204,8-1204_v2_14@1,8-1204_v1_14@1,"In addition, of those samples that were subjected to electron microscopy, we reviewed how many displayed evidence of a potential type IV collagen disorder.","In addition, of those samples that were subjected to electron microscopy, we reviewed how many displayed evidence of a potential collagen IV disorder.",yes
,8-1204,8-1204_v2_4@2,8-1204_v1_14@1,In two samples there were histological changes detected that might have suggested the underlying presence of a type IV collagen disorder.,"In addition, of those samples that were subjected to electron microscopy, we reviewed how many displayed evidence of a potential collagen IV disorder.",no
,8-1204,8-1204_v2_11@13,8-1204_v1_36@0,We conducted a retrospective cohort analysis of prior renal biopsy results in two tertiary centres to see how many of those that had been given a histological diagnosis of FSGS were sent for subsequent electron microscopy.,"The process by which variants within the collagen IV genes might cause FSGS remains unclear, particularly given their clear association with Alport Syndrome and TBMN.",no
,8-1204,8-1204_v2_9@7,8-1204_v1_36@1,"Variants in COL4A3, COL4A4 and COL4A5 which encode the α3, α4 and α5 chains of type IV collagen respectively, the major constituent of the GBM previously linked to both Alport syndrome and thin basement membrane nephropathy (TBMN) may also underlie cases of FSGS <REF-4> .","One proposal is that the ultrastructural changes induced by the collagen IV variants, perhaps under the influence of modifier genes such as laminin, result in impaired podocyte attachment to the glomerular basement membrane which leads to accelerated podocyte detachment, subsequent foot process effacement as a response to the increased shear stress induced by the denuded basement membrane and at a critical level of podocyte loss collapse of the capillary network with the appearance of the classical segmental sclerotic lesion <REF-2> , <REF-4> , <REF-17> .",no
,8-1204,8-1204_v2_39@1,8-1204_v1_33@0,"This may have potentially led to inadvertently overlooking characteristic basement membrane abnormalities, which may suggest an underlying and heritable type IV collagen disorder.","Curiously, neither of the two patients in whom electron microscopy was suggestive of an underlying collagen IV disorder was noted to have haematuria on their urinalysis at the time of presentation.",no
,8-1204,8-1204_v2_33@0,8-1204_v1_35@1,"Curiously, neither of the two patients in whom electron microscopy was suggestive of an underlying type IV collagen disorder was noted to have haematuria on their urinalysis at the time of presentation.",It is important to recognise that not all groups have found the characteristic changes associated with the collagen IV disorders such as Alport’s Syndrome or TBMN on electron microscopy.,no
,8-1204,8-1204_v2_39@1,8-1204_v1_36@0,"This may have potentially led to inadvertently overlooking characteristic basement membrane abnormalities, which may suggest an underlying and heritable type IV collagen disorder.","The process by which variants within the collagen IV genes might cause FSGS remains unclear, particularly given their clear association with Alport Syndrome and TBMN.",no
,8-1204,8-1204_v2_38@2,8-1204_v1_10@2,"It also remains unclear as to whether the changes of FSGS are a secondary process occurring in those with TBMN or whether the type IV collagen variants are capable of causing primary FSGS <REF-7> , <REF-20> .","More recently, targeted gene sequencing of adults thought to have primary FSGS or steroid resistant nephrotic syndrome (the paediatric equivalent) found that pathogenic or likely pathogenic variants in collagen IV genes were present in up to 38% of families with familial FSGS, and 3% of those with sporadic FSGS <REF-3> , <REF-8> .",no
,8-1204,8-1204_v2_0@0,8-1204_v1_35@0,The use of electron microscopy in the diagnosis of focal segmental glomerulosclerosis: are current pathological techniques missing important abnormalities in the glomerular basement membrane?,"This study was designed as a retrospective cohort study looking at the number of samples sent for electron microscopy, as well as any potential changes which might be consistent with a collagen IV glomerular basement membrane disorder.",no
,8-1204,8-1204_v2_38@3,8-1204_v1_0@0,"FSGS occurring as a secondary process to other basement membrane abnormalities may explain why immunosuppressive therapy has traditionally been less effective in inherited forms of FSGS, although there are case reports of the successful use of the calcineurin inhibitor cyclosporine for some patients harbouring inheritable type IV collagen disorders <REF-6> .",An audit of electron microscopy in the diagnosis of focal segmental glomerulosclerosis: are current pathological techniques missing important abnormalities in the glomerular basement membrane?,no
,8-1204,8-1204_v2_37@1,8-1204_v1_9@7,It is important to recognise that not all groups have found the characteristic changes associated with the type IV collagen disorders such as Alport’s Syndrome or TBMN on electron microscopy.,"Variants in COL4A3, COL4A4 and COL4A5 , which encode the α3, α4 and α5 chains of collagen IV, respectively, the major constituent of the GBM, previously linked to both Alport syndrome and thin basement membrane nephropathy (TBMN), may also underlie cases of FSGS <REF-4> .",no
,8-1204,8-1204_v2_37@3,8-1204_v1_14@1,It is thus possible that a lack of classical findings for a type IV collagen glomerular basement membrane disorder may have accounted for the low number of those with GBM features on electron microscopy consistent with a type IV collagen disorder noted within our study.,"In addition, of those samples that were subjected to electron microscopy, we reviewed how many displayed evidence of a potential collagen IV disorder.",no
,8-1204,8-1204_v2_11@4,8-1204_v1_10@0,"The other potential renal lesions that may be caused by COL4A gene variants, Alport syndrome and TBMN, cause thinning, lamellation and fraying of the GBM and may also be associated with podocyte foot process effacement, all of which require electron microscopy to visualise <REF-4> .","The relationship between the three renal conditions intertwined around variants in the collagen IV genes, Alport syndrome, TBMN, and FSGS is complex and incompletely understood.",no
,8-1204,8-1204_v2_10@0,8-1204_v1_34@0,"The relationship between the three renal conditions intertwined around variants in the COL4A genes, Alport syndrome, TBMN, and FSGS is complex and incompletely understood.","There is an increasing body of evidence indicating that inheritable variants in collagen IV genes may underlie a proportion of cases of FSGS, with up to 12.5% cases of autosomal dominant FSGS attributable to COL4A3 in some cohorts <REF-15> .",no
,8-1204,8-1204_v2_38@2,8-1204_v1_35@0,"It also remains unclear as to whether the changes of FSGS are a secondary process occurring in those with TBMN or whether the type IV collagen variants are capable of causing primary FSGS <REF-7> , <REF-20> .","This study was designed as a retrospective cohort study looking at the number of samples sent for electron microscopy, as well as any potential changes which might be consistent with a collagen IV glomerular basement membrane disorder.",no
,8-1204,8-1204_v2_10@2,8-1204_v1_11@9,"More recently, targeted gene sequencing of adults thought to have primary FSGS or steroid resistant nephrotic syndrome (the paediatric equivalent) found that pathogenic or likely pathogenic variants in COL4A genes were present in up to 38% of families with familial FSGS, and 3% of those with sporadic FSGS <REF-3> , <REF-8> .",This represents an opportunity to audit our prior clinical behaviour to see if our multidisciplinary diagnostic practice may require improvement.,no
,8-1204,8-1204_v2_27@1,8-1204_v1_9@7,Two of these samples showed signs on electron microscopy that might be consistent with an underlying type IV collagen glomerular basement membrane disorder.,"Variants in COL4A3, COL4A4 and COL4A5 , which encode the α3, α4 and α5 chains of collagen IV, respectively, the major constituent of the GBM, previously linked to both Alport syndrome and thin basement membrane nephropathy (TBMN), may also underlie cases of FSGS <REF-4> .",no
,8-1204,8-1204_v2_38@1,8-1204_v1_34@0,"One proposal is that the ultrastructural changes induced by the type IV collagen variants, perhaps under the influence of modifier genes such as laminin, result in impaired podocyte attachment to the glomerular basement membrane which leads to accelerated podocyte detachment, subsequent foot process effacement as a response to the increased shear stress induced by the denuded basement membrane and at a critical level of podocyte loss collapse of the capillary network with the appearance of the classical segmental sclerotic lesion <REF-2> , <REF-4> , <REF-19> .","There is an increasing body of evidence indicating that inheritable variants in collagen IV genes may underlie a proportion of cases of FSGS, with up to 12.5% cases of autosomal dominant FSGS attributable to COL4A3 in some cohorts <REF-15> .",no
,8-1204,8-1204_v2_38@2,8-1204_v1_36@1,"It also remains unclear as to whether the changes of FSGS are a secondary process occurring in those with TBMN or whether the type IV collagen variants are capable of causing primary FSGS <REF-7> , <REF-20> .","One proposal is that the ultrastructural changes induced by the collagen IV variants, perhaps under the influence of modifier genes such as laminin, result in impaired podocyte attachment to the glomerular basement membrane which leads to accelerated podocyte detachment, subsequent foot process effacement as a response to the increased shear stress induced by the denuded basement membrane and at a critical level of podocyte loss collapse of the capillary network with the appearance of the classical segmental sclerotic lesion <REF-2> , <REF-4> , <REF-17> .",no
,8-1204,8-1204_v2_27@1,8-1204_v1_32@1,Two of these samples showed signs on electron microscopy that might be consistent with an underlying type IV collagen glomerular basement membrane disorder.,"Of those that did, two were reported to have characteristics that might be consistent with an underlying collagen IV disorder.",no
,8-1204,8-1204_v2_38@2,8-1204_v1_10@0,"It also remains unclear as to whether the changes of FSGS are a secondary process occurring in those with TBMN or whether the type IV collagen variants are capable of causing primary FSGS <REF-7> , <REF-20> .","The relationship between the three renal conditions intertwined around variants in the collagen IV genes, Alport syndrome, TBMN, and FSGS is complex and incompletely understood.",no
,8-1204,8-1204_v2_37@0,8-1204_v1_34@0,"This study was designed as a retrospective cohort study looking at the number of samples sent for electron microscopy, as well as any potential changes which might be consistent with a type IV collagen glomerular basement membrane disorder.","There is an increasing body of evidence indicating that inheritable variants in collagen IV genes may underlie a proportion of cases of FSGS, with up to 12.5% cases of autosomal dominant FSGS attributable to COL4A3 in some cohorts <REF-15> .",no
,8-1204,8-1204_v2_10@2,8-1204_v1_9@7,"More recently, targeted gene sequencing of adults thought to have primary FSGS or steroid resistant nephrotic syndrome (the paediatric equivalent) found that pathogenic or likely pathogenic variants in COL4A genes were present in up to 38% of families with familial FSGS, and 3% of those with sporadic FSGS <REF-3> , <REF-8> .","Variants in COL4A3, COL4A4 and COL4A5 , which encode the α3, α4 and α5 chains of collagen IV, respectively, the major constituent of the GBM, previously linked to both Alport syndrome and thin basement membrane nephropathy (TBMN), may also underlie cases of FSGS <REF-4> .",no
,8-1204,8-1204_v2_38@0,8-1204_v1_11@9,"The process by which variants within the COL4A genes might cause FSGS remains unclear, particularly given their clear association with Alport Syndrome and TBMN.",This represents an opportunity to audit our prior clinical behaviour to see if our multidisciplinary diagnostic practice may require improvement.,no
,8-1204,8-1204_v2_0@0,8-1204_v1_10@2,The use of electron microscopy in the diagnosis of focal segmental glomerulosclerosis: are current pathological techniques missing important abnormalities in the glomerular basement membrane?,"More recently, targeted gene sequencing of adults thought to have primary FSGS or steroid resistant nephrotic syndrome (the paediatric equivalent) found that pathogenic or likely pathogenic variants in collagen IV genes were present in up to 38% of families with familial FSGS, and 3% of those with sporadic FSGS <REF-3> , <REF-8> .",no
,8-1204,8-1204_v2_39@1,8-1204_v1_32@5,"This may have potentially led to inadvertently overlooking characteristic basement membrane abnormalities, which may suggest an underlying and heritable type IV collagen disorder.","Notably, close to one in three cases of primary FSGS were not proceeding to electron microscopy despite an indication to do so and 1 in 20 cases within our cohort had structural changes that were consistent with an underlying collagen IV variant.",no
,8-1204,8-1204_v2_10@1,8-1204_v1_0@0,"Whilst previously it was believed that patients heterozygous for variants in COL4A3 and COL4A4 would develop TBMN with persistent microscopic haematuria and an otherwise benign prognosis, this traditional thinking has been overturned by the discovery that some pedigrees with these variants will go on to develop significant proteinuria, FSGS lesions and the potential for progressive CKD or ESKD <REF-6> , <REF-7> .",An audit of electron microscopy in the diagnosis of focal segmental glomerulosclerosis: are current pathological techniques missing important abnormalities in the glomerular basement membrane?,no
,8-1204,8-1204_v2_11@13,8-1204_v1_11@4,We conducted a retrospective cohort analysis of prior renal biopsy results in two tertiary centres to see how many of those that had been given a histological diagnosis of FSGS were sent for subsequent electron microscopy.,"The other potential renal lesions that may be caused by collagen IV gene variants, Alport syndrome and TBMN, cause thinning, lamellation and fraying of the GBM and may also be associated with podocyte foot process effacement, all of which require electron microscopy to visualise <REF-4> .",no
,8-1204,8-1204_v2_38@3,8-1204_v1_11@10,"FSGS occurring as a secondary process to other basement membrane abnormalities may explain why immunosuppressive therapy has traditionally been less effective in inherited forms of FSGS, although there are case reports of the successful use of the calcineurin inhibitor cyclosporine for some patients harbouring inheritable type IV collagen disorders <REF-6> .",We conducted a retrospective audit of prior renal biopsy results in two tertiary centres to see how many of those that had been given a histological diagnosis of FSGS were sent for subsequent electron microscopy.,no
9812,8-1681,8-1681_v2_25@6,8-1681_v1_25@6,"At the Auckland site, the scan had a TE of 30 ms, flip angle of 62°, a multiband/slice acceleration factor of 3, an in-plane/parallel imaging acceleration factor of 2 and rBW was 1680 Hz/Px.","At the Auckland site, the scan had a TE of 30 ms, flip angle of 62 o , a multiband/slice acceleration factor of 3, an in-plane/parallel imaging acceleration factor of 2 and rBW was 1680 Hz/Px.",yes
9813,8-1681,8-1681_v2_31@4,8-1681_v1_31@4,"Subsequently, the time series was convolved with a canonical hemodynamic response function (HRF) determined from previous empirical data <REF-6> and z-standardized.","Subsequently, the time series was convolved with a canonical hemodynamic response function determined from previous empirical data <REF-6> and z-standardized.",yes
9814,8-1681,8-1681_v2_33@2,8-1681_v1_33@2,"We then calculated the Pearson correlation between these split time series, r obs’ , and used the Spearman-Brown prophecy formula <REF-42> , <REF-43> to determine, r obs , the expected reliability for the average of all six runs:","We then calculated the Pearson correlation between these split time series, r obs ’, and used the Spearman-Brown prophecy formula <REF-42> , <REF-43> to determine, r obs , the expected reliability for the average of all six runs:",yes
9815,8-1681,8-1681_v2_45@5,8-1681_v1_45@5,"CMF curves were very comparable for the two sites ( Figure 3B ), except for somewhat greater CMF at 3T in V1 and V2 in the very central visual field of participants P2 and P3 <REF-46> (see supplementary figure 1A and B <REF-46> )) for individual participants’ pRF size and CMF respectively).","CMF curves were very comparable for the two sites ( Figure 3B ), except for somewhat greater CMF at 3T in V1 and V2 in the very central visual field of participants P2 and P3.",yes
9816,8-1681,8-1681_v2_48@1,8-1681_v1_48@1,This showed that in V1 and V2 goodness-of-fit was notably greater for the Auckland 3T site than the London 1.5T site.,This showed that in V1 and V2 goodness-of-fit was notably greater for the Auckland 3T site than the London 1.5T site in all 3 participants.,yes
9817,8-1681,8-1681_v2_48@3,8-1681_v1_48@3,"In V3A, the model fits (see Figure 3C last column) at both sites were similar, but generally lower than in the other regions and with greater variability.","In V3A, the model fits at both were similar, but generally lower than in the other regions and with greater variability.",yes
9818,8-1681,8-1681_v2_48@5,8-1681_v1_48@5,"When we normalized model fits relative to the noise ceiling, ρ o 2, the maximum goodness-of-fit that could theoretically be achieved given the data from each site, we found no systematic difference in goodness-of-fit between sites ( Figure 3D ).","When we normalized model fits relative to the noise ceiling, ρ o 2 , the maximum goodness-of-fit that could theoretically be achieved given the data from each site, we found no systematic difference in goodness-of-fit between sites ( Figure 3B ).",yes
9819,8-1681,8-1681_v2_48@6,8-1681_v1_48@6,The curves mostly overlapped for P1 and P2 except in V1 where the London data outperformed the Auckland data (see Figure 2B ).,The curves mostly overlapped for P1 and P2 except in V1 where the London data outperformed the Auckland data.,yes
9820,8-1681,8-1681_v2_62@1,8-1681_v1_63@1,"In London, images were projected onto a screen and this necessitated focusing and scaling the projected image to be of the exact size.","In London, images were projected onto a screen and this necessitated focussing and scaling the projected image to be of the exact size.",yes
9821,8-1681,8-1681_v2_66@2,8-1681_v1_67@2,"A slower stimulus design where each bar position is stimulated for 2–3 s, or a random instead of an ordered stimulus sequence, could probably counteract this problem but this would come at the expense of longer scanning durations.","A slower stimulus design where each bar position is stimulated for 2-3 s, or a random instead of an ordered stimulus sequence, could probably counteract this problem but this would come at the expense of longer scanning durations.",yes
9822,8-1681,8-1681_v2_84@0,8-1681_v1_74@0,Data are available under the terms of the Creative Commons Zero “No rights reserved” data waiver (CC0 1.0 Public domain dedication).,"Data are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",yes
,8-1681,8-1681_v2_48@6,8-1681_v1_67@2,The curves mostly overlapped for P1 and P2 except in V1 where the London data outperformed the Auckland data (see Figure 2B ).,"A slower stimulus design where each bar position is stimulated for 2-3 s, or a random instead of an ordered stimulus sequence, could probably counteract this problem but this would come at the expense of longer scanning durations.",no
,8-1681,8-1681_v2_45@5,8-1681_v1_48@6,"CMF curves were very comparable for the two sites ( Figure 3B ), except for somewhat greater CMF at 3T in V1 and V2 in the very central visual field of participants P2 and P3 <REF-46> (see supplementary figure 1A and B <REF-46> )) for individual participants’ pRF size and CMF respectively).",The curves mostly overlapped for P1 and P2 except in V1 where the London data outperformed the Auckland data.,no
,8-1681,8-1681_v2_62@1,8-1681_v1_33@2,"In London, images were projected onto a screen and this necessitated focusing and scaling the projected image to be of the exact size.","We then calculated the Pearson correlation between these split time series, r obs ’, and used the Spearman-Brown prophecy formula <REF-42> , <REF-43> to determine, r obs , the expected reliability for the average of all six runs:",no
,8-1681,8-1681_v2_33@2,8-1681_v1_74@0,"We then calculated the Pearson correlation between these split time series, r obs’ , and used the Spearman-Brown prophecy formula <REF-42> , <REF-43> to determine, r obs , the expected reliability for the average of all six runs:","Data are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,8-1681,8-1681_v2_25@6,8-1681_v1_67@2,"At the Auckland site, the scan had a TE of 30 ms, flip angle of 62°, a multiband/slice acceleration factor of 3, an in-plane/parallel imaging acceleration factor of 2 and rBW was 1680 Hz/Px.","A slower stimulus design where each bar position is stimulated for 2-3 s, or a random instead of an ordered stimulus sequence, could probably counteract this problem but this would come at the expense of longer scanning durations.",no
,8-1681,8-1681_v2_62@1,8-1681_v1_45@5,"In London, images were projected onto a screen and this necessitated focusing and scaling the projected image to be of the exact size.","CMF curves were very comparable for the two sites ( Figure 3B ), except for somewhat greater CMF at 3T in V1 and V2 in the very central visual field of participants P2 and P3.",no
,8-1681,8-1681_v2_25@6,8-1681_v1_48@3,"At the Auckland site, the scan had a TE of 30 ms, flip angle of 62°, a multiband/slice acceleration factor of 3, an in-plane/parallel imaging acceleration factor of 2 and rBW was 1680 Hz/Px.","In V3A, the model fits at both were similar, but generally lower than in the other regions and with greater variability.",no
,8-1681,8-1681_v2_48@5,8-1681_v1_31@4,"When we normalized model fits relative to the noise ceiling, ρ o 2, the maximum goodness-of-fit that could theoretically be achieved given the data from each site, we found no systematic difference in goodness-of-fit between sites ( Figure 3D ).","Subsequently, the time series was convolved with a canonical hemodynamic response function determined from previous empirical data <REF-6> and z-standardized.",no
,8-1681,8-1681_v2_31@4,8-1681_v1_25@6,"Subsequently, the time series was convolved with a canonical hemodynamic response function (HRF) determined from previous empirical data <REF-6> and z-standardized.","At the Auckland site, the scan had a TE of 30 ms, flip angle of 62 o , a multiband/slice acceleration factor of 3, an in-plane/parallel imaging acceleration factor of 2 and rBW was 1680 Hz/Px.",no
,8-1681,8-1681_v2_48@5,8-1681_v1_74@0,"When we normalized model fits relative to the noise ceiling, ρ o 2, the maximum goodness-of-fit that could theoretically be achieved given the data from each site, we found no systematic difference in goodness-of-fit between sites ( Figure 3D ).","Data are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",no
,8-1681,8-1681_v2_48@3,8-1681_v1_25@6,"In V3A, the model fits (see Figure 3C last column) at both sites were similar, but generally lower than in the other regions and with greater variability.","At the Auckland site, the scan had a TE of 30 ms, flip angle of 62 o , a multiband/slice acceleration factor of 3, an in-plane/parallel imaging acceleration factor of 2 and rBW was 1680 Hz/Px.",no
10048,8-1983,8-1983_v2_4@1,8-1983_v1_4@1,"Furthermore, we observed that this method yields an average range of neurospheres sizes greater than 50 μm, but less than 100 μm after 7 DIV.","Furthermore, we found this method yields different sizes of neurospheres.",yes
10049,8-1983,8-1983_v2_4@2,8-1983_v1_4@2,"Lastly, using an anti-GFAP antibody, we show that these neurospheres can be stained, confirming their use in future immunocytochemistry studies.","Lastly, using an anti-GFAP antibody, we confirm that these neurospheres can be used for immunocytochemistry studies.",yes
10050,8-1983,8-1983_v2_75@2,8-1983_v1_73@2,"Using similar analyses, we found significant differences between the size classification of neurospheres that were less than 50 μm, between 50–100 μm, and greater than 100 μm (F (2,379) = 424; p < 0.0001) ( Figure 3B ).","Using similar analyses, we found significant differences between the size classification of neurospheres that were less than 50 μm, between 50-100 μm, and greater than 100 μm (F (2,379) = 424; p < 0.0001) ( Figure 3B ).",yes
10051,8-1983,8-1983_v2_75@3,8-1983_v1_73@3,Post hoc analysis using Bonferroni’s multiple comparison revealed a significant difference between the primary neurospheres that were greater than 100 μm compared to neurospheres that were less than 50 μm (p <0.0001) or between 50–100 μm (p <0.0001) ( Figure 3B ).,Post hoc analysis using Bonferroni’s multiple comparison revealed a significant difference between the primary neurospheres that were greater than 100 μm compared to neurospheres that were less than 50 μm (p <0.0001) or between 50-100 μm (p <0.0001) ( Figure 3B ).,yes
10052,8-1983,8-1983_v2_75@4,8-1983_v1_73@4,"Similarly, we found a substantial difference between primary neurospheres that were greater than 100 μm compared to neurospheres that were between 50–100 μm (p <0.0001) ( Figure 3B ).","Similarly, we found a substantial difference between primary neurospheres that were greater than 100 μm compared to neurospheres that were between 50 -100 μm (p <0.0001) ( Figure 3B ).",yes
10053,8-1983,8-1983_v2_7@2,8-1983_v1_7@2,"Since its introduction some 25 years ago <REF-2> , neurospheres have been used to study neurogenesis <REF-3> , genes that regulate self-renewal <REF-4> , <REF-5> , and molecular mechanisms that control neuronal and glial differentiation <REF-3> , <REF-6> – <REF-9> .","Since its introduction some 25 years ago <REF-2> , neurospheres have been used to study neurogenesis <REF-3> , genes that regulate self-renewal <REF-4> , and molecular mechanisms that control neuronal and glial differentiation <REF-3> , <REF-5> – <REF-8> .",yes
10054,8-1983,8-1983_v2_79@1,8-1983_v1_77@1,Figure 4 is a picture of a small ( Figure 4A-B ; arrowhead) and a larger ( Figure 4C ; arrowhead) primary neurosphere immunostained using an anti-GFAP antibody and counterstained with DAPI (Figure B; arrowhead).,Figure 4 is a picture of a primary neurosphere immunostained using an anti-GFAP ( Figure 4A ; arrowhead) antibody and counterstained with DAPI (Figure B; arrowhead).,yes
10055,8-1983,8-1983_v2_7@3,8-1983_v1_7@3,"Although there are many neurosphere protocols, the expected number and size of neurospheres generated after a week in vitro is not entirely characterized.","Although there are many neurosphere protocols, the expected number and size of neurospheres generated after a week in vivo is not fully understood.",yes
10058,8-1983,8-1983_v2_92@0,8-1983_v1_89@0,- Spreadsheet containing numbers and sizes per field of view.xlsx (details on numbers and sizes of neurospheres produced from each mouse).,- Spreadsheet containing numbers and sizes per field of view.xlsx (details on numbers and sizes of neurospeheres produced from each mouse).,yes
10059,8-1983,8-1983_v2_8@3,8-1983_v1_8@3,The area surrounding the ventricle is then microdissected from a given slice of tissue to enrich for neural stem/progenitor cells.,The area surrounding the ventricle is then microdissected from a given slice to enrich for neural stem/progenitor cells.,yes
10060,8-1983,8-1983_v2_8@7,8-1983_v1_8@5,"In contrast, our approach requires no specialized equipment.","In our approach, no specialized equipment is needed.",yes
10061,8-1983,8-1983_v2_8@9,8-1983_v1_8@7,"This method requires only half of a single brain, and generates reproducible numbers of neurospheres in a few days.","This method requires only half of a single brain, and generates reproducible numbers of neurospheres after a week.",yes
10062,8-1983,8-1983_v2_2@2,8-1983_v1_2@2,The objective of this protocol is to provide a stepwise method from a single isolation that predicts the average number of neurospheres generated and to estimate an approximation of its sizes after several days in vitro .,This method predicts the average number of neurospheres and provides an approximation of its expected size after 7 days in vitro .,yes
10063,8-1983,8-1983_v2_2@4,8-1983_v1_2@3,"Estimates about the numbers and sizes of neurospheres will provide investigators with quantitative data to advise on how much starting dLGE tissue is required to generate the appropriate number of spheres for the implementation of downstream applications, including immunocytochemistry, self-renewal and differentiation assays.","Characterization of numbers and sizes will provide investigators with quantitative data to advise on the implementation of downstream applications, including immnocytochemistry, self-renewal and differentiation assays.",yes
10064,8-1983,8-1983_v2_19@0,8-1983_v1_19@0,Images were taken with a Canon EOS Rebel XS camera.,Images were taken with a Canon EOS Rebel XS.,yes
10065,8-1983,8-1983_v2_19@7,8-1983_v1_19@1,The field of view represents a length of 783 μm and a width of 522 μm.,The field of view represent a length of 783 μm and a width of 522 μm.,yes
10066,8-1983,8-1983_v2_19@9,8-1983_v1_19@3,"Per animal, 3–4 wells were analyzed.","Per animal, 3-4 wells were analyzed.",yes
10067,8-1983,8-1983_v2_19@10,8-1983_v1_19@4,A total of 5 individual animals were analyzed.,A total of 5 individual animal were analyzed.,yes
10068,8-1983,8-1983_v2_19@14,8-1983_v1_19@8,"If the main effect was significant (p < 0.05), Bonferroni’s multiple comparison post hoc test were used to compare the different replicates.","If the main effect was significant (p < 0.05), Bonferroni’s multiple comparision post hoc test were used to compare the different replicates.",yes
,8-1983,8-1983_v2_2@1,8-1983_v1_2@1,"In the dLGE, metrics of sizes and numbers of neurospheres generated using this assay has not been completely characterized.",The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_8@5,8-1983_v1_8@8,"Additionally, many protocols do not provide metrics on expected numbers and sizes of neurospheres generated.",Our method also provides an approximation of the expected sizes for neurospheres after a week in vitro .,no
,8-1983,8-1983_v2_46@1,8-1983_v1_8@8,"In this section you will need the 18-gauge, 21-gauge, 23-gauge needle will be needed for trituration steps.",Our method also provides an approximation of the expected sizes for neurospheres after a week in vitro .,no
,8-1983,8-1983_v2_19@2,8-1983_v1_8@8,The optimum magnification is approximately 5x with 3888 x 2592 dimensions.,Our method also provides an approximation of the expected sizes for neurospheres after a week in vitro .,no
,8-1983,8-1983_v2_19@5,8-1983_v1_8@8,The steromicroscope was used at a working distance of ~110 mm.,Our method also provides an approximation of the expected sizes for neurospheres after a week in vitro .,no
,8-1983,8-1983_v2_83@9,8-1983_v1_2@1,This leads to over crowdedness.,The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_46@3,8-1983_v1_2@1,Hemocytometer will be needed to count cells .,The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_27@3,8-1983_v1_2@1,Before starting all premade solutions should be warmed to 37°C.,The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_83@6,8-1983_v1_2@1,This can be caused by depletion of growth factors.,The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_19@2,8-1983_v1_2@1,The optimum magnification is approximately 5x with 3888 x 2592 dimensions.,The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_84@2,8-1983_v1_2@1,"After trituration, if a uniform suspension has not been achieved an alternative method used in previous protocols are strainers <REF-15> .",The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_2@3,8-1983_v1_8@8,The advantage of this protocol is that no expensive and specialized equipment is needed for tissue isolation.,Our method also provides an approximation of the expected sizes for neurospheres after a week in vitro .,no
,8-1983,8-1983_v2_46@2,8-1983_v1_2@1,Trituration should be performed gently and slowly to avoid killing cells.,The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_37@2,8-1983_v1_2@1,"Afterwards, additional petri dishes will be needed to place in each of the dissected brains (35 mm) .",The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_8@10,8-1983_v1_2@1,"Furthermore, using our method neurospheres small appears as early as 3 days.",The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_19@4,8-1983_v1_8@8,The working distance was defined as the amount of room required between the top of the neurosphere and the bottom of the objective lens in order for the image to be in focus.,Our method also provides an approximation of the expected sizes for neurospheres after a week in vitro .,no
,8-1983,8-1983_v2_8@6,8-1983_v1_2@1,"Thus, it is unclear whether researchers can generate sufficient numbers of neurospheres in a particular range of sizes.",The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_8@5,8-1983_v1_2@1,"Additionally, many protocols do not provide metrics on expected numbers and sizes of neurospheres generated.",The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,no
,8-1983,8-1983_v2_83@4,8-1983_v1_8@8,Another marker of an unhealthy culture is a large number of differentiated neurons surrounding neurospheres.,Our method also provides an approximation of the expected sizes for neurospheres after a week in vitro .,no
10175,8-42,8-42_v2_39@1,8-42_v1_37@0,"As the length of this object attribute is 1, in this example the sequence mapping identified a single target sequence.","In this example, the sequence mapping identified one target sequence.",yes
10176,8-42,8-42_v2_39@2,8-42_v1_37@1,From this object further information can be obtained as follows:,From the seq_annotation object further information can be obtained as follows:,yes
10179,8-42,8-42_v2_92@3,8-42_v1_90@3,We will also continue to update the package and the API in sync with the OMA browser to incorporate new functionalities of OMA.,"We will also continue to update the package and API to incorporate new functionalities of OMA, such as support for local synteny which is currently under development.",yes
10180,8-42,8-42_v2_14@0,8-42_v1_14@0,"Most data available through the OMA browser is now also accessible via the API, with the exception of the local synteny data.",Most data available through the OMA browser is now also accessible via the API.,yes
10181,8-42,8-42_v2_20@1,8-42_v1_20@1,"In the results section we showcase usage of the latest version of the package (v2.0), which requires R version >= 3.6 and Bioconductor version >= 3.9.","The package requires R version >= 3.6 and Bioconductor version >= 3.9, as well as a stable internet connection.",yes
10182,8-42,8-42_v2_24@0,8-42_v1_24@0,"For Python users, we provide an analogous package named omadb .","For Python users, we provide an analogous package also named omadb .",yes
,8-42,8-42_v2_92@3,8-42_v1_37@1,We will also continue to update the package and the API in sync with the OMA browser to incorporate new functionalities of OMA.,From the seq_annotation object further information can be obtained as follows:,no
,8-42,8-42_v2_39@2,8-42_v1_20@1,From this object further information can be obtained as follows:,"The package requires R version >= 3.6 and Bioconductor version >= 3.9, as well as a stable internet connection.",no
,8-42,8-42_v2_92@3,8-42_v1_14@0,We will also continue to update the package and the API in sync with the OMA browser to incorporate new functionalities of OMA.,Most data available through the OMA browser is now also accessible via the API.,no
,8-42,8-42_v2_24@0,8-42_v1_37@0,"For Python users, we provide an analogous package named omadb .","In this example, the sequence mapping identified one target sequence.",no
,8-42,8-42_v2_20@1,8-42_v1_37@1,"In the results section we showcase usage of the latest version of the package (v2.0), which requires R version >= 3.6 and Bioconductor version >= 3.9.",From the seq_annotation object further information can be obtained as follows:,no
,8-42,8-42_v2_24@0,8-42_v1_14@0,"For Python users, we provide an analogous package named omadb .",Most data available through the OMA browser is now also accessible via the API.,no
10267,8-52,8-52_v2_24@0,8-52_v1_23@0,Homology analysis between PD_CbNPR1 and its reference sequence via BLAST search showed 99% identity.,Homology analysis between PD_CbNPR1 and its reference sequence via BLAST search showed 99% homology.,yes
10268,8-52,8-52_v2_3@2,8-52_v1_3@2,"Identification of a cis -acting element was detected by PLACE, PlantCare, and PlantPAN.",Identification of a cis -acting element was detected by PLACE.,yes
10269,8-52,8-52_v2_33@3,8-52_v1_31@3,"The CAACA pattern was also found by Kagaya et al . (1999) in Arabidopsis thaliana , while the TGTTG pattern is similar to the OsNPR1 promoter ( Hwang & Hwang, 2010 ).","The CAACA pattern was also found by Kagaya et al . (1999) in Arabidopsis thaliana , while the TGTTG pattern is similar to the OsNPR1 promoter (Hwang dan Hwang, 2010).",yes
10270,8-52,8-52_v2_4@1,8-52_v1_4@1,BLASTn search analysis indicated that PD_CbNPR1 sequence is highly conserved (99% identity) showing only a single nucleotide polymorphism (SNP) (base substitution) compared with its reference sequence.,BLASTn search analysis indicated that PD_CbNPR1 sequence is highly conserved (99% homology) showing only a single nucleotide polymorphism (SNP) (base substitution) compared with its reference sequence.,yes
10271,8-52,8-52_v2_36@0,8-52_v1_34@0,"We also found a gibberellin-responsive element (GARE) motif, which has been previously reported by Ogawa et al . (2003) as the binding site with some transcription factors induced by gibberellic acid.","We also found a gibberelline-responsive element (GARE) motif, which has been previously reported by Ogawa et al . (2003) as the binding site with some transcription factors induced by gibberellic acid.",yes
10272,8-52,8-52_v2_7@0,8-52_v1_7@0,The non-expressor of pathogenesis related gene 1 ( NPR1 ) protein is the main regulator in the systemic acquired resistance response of many plants.,The non-expressor of pathogenesis related gene 1 ( NPR1 ) protein is a main regulator in the systemic acquired resistance response of many plants.,yes
10273,8-52,8-52_v2_7@1,8-52_v1_7@1,Overexpression by modifying the distal promoter in the W-box element of the OsNPR1 gene in rice could increase its resistance against Xanthomonas oryzae pv.,Over expression by modifying distal promoter in the W-box element of the OsNPR1 gene in rice could increase its resistance against Xanthomonas oryzae pv.,yes
10274,8-52,8-52_v2_11@3,8-52_v1_11@1,The plant was grown in a greenhouse and maintained for 8 weeks before being used for DNA isolation.,The chili pepper genotype Berangkai was grown in a greenhouse and maintained for 8 weeks before being used for DNA isolation.,yes
10275,8-52,8-52_v2_2@1,8-52_v1_2@1,"The cis -acting elements of its distal promoter gene are characterized by salicylic acid inducing elements such as the W-box, RAV1AAT and ASF1, accompanied by enhancer and silencer elements.","The cis -acting elements of its distal promoter gene are characterized by salicylic acid inducing elements such as the W-box, RAV1AAT and ASF1, accompanied with enhancer and silencer elements.",yes
10276,8-52,8-52_v2_13@11,8-52_v1_13@11,The final extension was maintained at 72°C for 5 minutes.,Final extension was maintained at 72°C for 5 minutes.,yes
10277,8-52,8-52_v2_17@5,8-52_v1_17@5,"The cis -acting elements were identified using PLACE , developed by Higo et al . (1999) , PlantCARE ( Lescot et al ., 2002 ) and PlantPAN .","The cis -acting elements were identified using PLACE , developed by Higo et al . (1999) .",yes
,8-52,8-52_v2_11@1,8-52_v1_23@2,"This genotype is known to produce more yield, but susceptible to geminivirus infection.","This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
,8-52,8-52_v2_38@3,8-52_v1_23@2,In our sequence only 1 single TCA element and 3 CGTA motifs could be detected.,"This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
,8-52,8-52_v2_38@1,8-52_v1_23@2,The TCA element has a function as a cis -acting element which is involved in salicylic acid responsiveness.,"This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
,8-52,8-52_v2_11@2,8-52_v1_23@2,"Resistant genotype however, is not available in our collection so far, so comparison of both two genotypes is not possible to be performed.","This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
,8-52,8-52_v2_24@4,8-52_v1_23@2,"However, the comparable nucleotide of both sequences spanned only 180 bp.","This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
,8-52,8-52_v2_23@3,8-52_v1_23@2,Validation of every single nucleotide data was confirmed by at least two overlapping validated segment.,"This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
,8-52,8-52_v2_27@2,8-52_v1_23@2,"Tree analysis showed that our NPR1-Berangkai cDNA sequence clustered to similar clade with AM900559.1 and NM_001325099.1 and other three solanaceae ( S. lycopersicum - KX198701.1 , NM_001247629.2 ; S. tuberosum -XM_006357647.2; S. pennellii -XM_015227358.2 and S. torvum -KJ995663.1).","This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
,8-52,8-52_v2_23@1,8-52_v1_23@2,This approach is considered to be the most appropriate since the sequencing read capacity used in this study is limited for about only 500 bp on average.,"This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
,8-52,8-52_v2_40@3,8-52_v1_23@2,"However, their role in the gene expression has to be confirmed empirically.","This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
,8-52,8-52_v2_38@2,8-52_v1_23@2,Moreover we also found the CGTA motif playing a role in regulating me-JA responsiveness.,"This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
,8-52,8-52_v2_29@2,8-52_v1_23@2,The PlantCare analysis successfully showed 2 other cis -acting element motifs namely 1 TCA motif and 3 CGTA motis which could not be shown by PLACE ( Figure 3 ).,"This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.",no
10537,8-80,8-80_v2_20@2,8-80_v1_20@2,"A rim weighting procedure was run against the population figures from the most recent national census to construct weight variables, with the procedure executed separately for each country.","A rim weighting procedure was run against the population figures to construct weight variables, with the procedure executed separately for each country.",yes
10538,8-80,8-80_v2_7@0,8-80_v1_7@0,"While the worldwide rate of tobacco smoking has declined substantially in recent years, the absolute number of people currently smoking has increased from approximately 720 million smokers in 1980 to an estimated 1.1 billion today, the consequence of population growth outpacing declining smoking prevalence in many low and middle income countries (LMICs) <REF-1> – <REF-3> .","While the worldwide rate of tobacco smoking has declined substantially in recent years, the absolute number of people currently smoking has increased from approximately 720 million smokers in 1980 to an estimated 1.1 billion today, the consequence of population growth outpacing declining smoking prevalence in many low and middle income countries <REF-1> – <REF-3> .",yes
10539,8-80,8-80_v2_35@0,8-80_v1_35@0,"In almost all countries, more current smokers than ex-smokers or nonsmokers were surrounded by people who also smoked, including parents, spouse/partner, close friends and colleagues ( Figure 5a–d ).","In almost all countries, more current smokers than ex-smokers or nonsmokers were surrounded by people who also smoked, including parents, spouse/partner, close friends and colleagues ( Figure 5a-d ).",yes
10540,8-80,8-80_v2_40@2,8-80_v1_40@2,"Additionally, more than 60% of smokers and ex-smokers in India, Malawi and Brazil, had bought cigarettes when they knew the money could be spent better on household essentials like food.","Additionally, between 21% (Japan) and 87% (Brazil) of smokers and ex-smokers had bought cigarettes when they knew the money could be spent better on household essentials like food.",yes
10541,8-80,8-80_v2_88@1,8-80_v1_88@1,RYO cigarettes vary in composition but have been shown to cause comparable exposure to known and suspected carcinogens <REF-15> .,"RYO cigarettes vary in composition but have been shown to have higher tar yields than boxed cigarettes, as well as comparable exposure to known and suspected carcinogens <REF-15> .",yes
10542,8-80,8-80_v2_15@2,8-80_v1_15@2,"Smokers were defined as those who responded that they currently smoked cigarettes, cigars, cigarillos or a pipe (or bidis in India) “regularly” or “occasionally;” ex-smokers were defined as those who responded that they used to smoke but stopped; and never smokers were defined as those who responded that they had never smoked.","Smokers were defined as those who responded that they currently smoked cigarettes, cigars, cigarillos or a pipe “regularly” or “occasionally;” ex-smokers were defined as those who responded that they used to smoke but stopped; and never smokers were defined as those who responded that they had never smoked.",yes
10543,8-80,8-80_v2_96@2,8-80_v1_96@2,Compared to other nicotine-non-tobacco products ENDS most closely simulate smoking regular cigarettes in how they are used.,"Compared to other nicotine, non-tobacco products, ENDS most closely simulate smoking regular cigarettes in how they are used.",yes
10544,8-80,8-80_v2_106@0,8-80_v1_106@0,Data are available under the terms of the Creative Commons Zero “No rights reserved” data waiver (CC0 1.0 Public domain dedication).,"Data are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",yes
,8-80,8-80_v2_19@4,8-80_v1_101@1,Face-to-face respondents were not compensated for their participation.,"Next steps include a more detailed look at the economic levers of tobacco preference, the perceived harms of cigarette smoking vs the objective realities of these harms, and a look at the characteristics and choices of ex-smokers.",no
,8-80,8-80_v2_19@5,8-80_v1_101@1,Online respondents were all members of an online panel company and received Reward Points.,"Next steps include a more detailed look at the economic levers of tobacco preference, the perceived harms of cigarette smoking vs the objective realities of these harms, and a look at the characteristics and choices of ex-smokers.",no
,8-80,8-80_v2_19@6,8-80_v1_101@1,"The number of points awarded for survey completion is based on survey length, complexity, and incidence rate.","Next steps include a more detailed look at the economic levers of tobacco preference, the perceived harms of cigarette smoking vs the objective realities of these harms, and a look at the characteristics and choices of ex-smokers.",no
,8-80,8-80_v2_19@7,8-80_v1_101@1,"Once a points threshold is reached, panelists may redeem their points for online gift certificates or merchandise.","Next steps include a more detailed look at the economic levers of tobacco preference, the perceived harms of cigarette smoking vs the objective realities of these harms, and a look at the characteristics and choices of ex-smokers.",no
,8-80,8-80_v2_19@8,8-80_v1_101@1,Each country has its own unique catalog.,"Next steps include a more detailed look at the economic levers of tobacco preference, the perceived harms of cigarette smoking vs the objective realities of these harms, and a look at the characteristics and choices of ex-smokers.",no
,8-80,8-80_v2_40@2,8-80_v1_96@2,"Additionally, more than 60% of smokers and ex-smokers in India, Malawi and Brazil, had bought cigarettes when they knew the money could be spent better on household essentials like food.","Compared to other nicotine, non-tobacco products, ENDS most closely simulate smoking regular cigarettes in how they are used.",no
,8-80,8-80_v2_96@2,8-80_v1_35@0,Compared to other nicotine-non-tobacco products ENDS most closely simulate smoking regular cigarettes in how they are used.,"In almost all countries, more current smokers than ex-smokers or nonsmokers were surrounded by people who also smoked, including parents, spouse/partner, close friends and colleagues ( Figure 5a-d ).",no
,8-80,8-80_v2_40@2,8-80_v1_20@2,"Additionally, more than 60% of smokers and ex-smokers in India, Malawi and Brazil, had bought cigarettes when they knew the money could be spent better on household essentials like food.","A rim weighting procedure was run against the population figures to construct weight variables, with the procedure executed separately for each country.",no
10783,9-1088,9-1088_v2_30@7,9-1088_v1_21@7,"The covariates related to diet, hygiene and culture were stable in this period and, thus, they could be safely assumed as such.","The covariates related to diet, hygiene and culture were stable in this period and, thus, they could be safely assumed.",yes
10784,9-1088,9-1088_v2_32@2,9-1088_v1_23@2,"Our work postulates that there is an inverse relation in two time series, between the timing of sunspot numbers and stroke deaths, a hypothesis posed by previous investigations ( Geronikolou & Leontitsis, 2005 ; Geronikolou & Petropoulos, 1996 ; Stoupel et al., 1999 ).","Our work postulates that there is an inverse relation in two time series, between the timing of sunspot numbers and stroke deaths, a hypothesis posed by previous investigations ( Geronikolou & Leontisis, 2005 ; Geronikolou & Petropoulos, 1996 ; Stoupel et al., 1999 ).",yes
10785,9-1088,9-1088_v2_33@2,9-1088_v1_24@2,"The molecular interactions network approach, where the inter-species functional interactome of nuclear steroid receptors (R1) constructed on orthologues was employed ( Geronikolou et al., 2018 ).","The molecular interactions network approach, where the inter-species functional interactome of nuclear steroid receptors (R1) was constructed on orthologues was employed ( Geronikolou et al., 2018 ).",yes
10786,9-1088,9-1088_v2_33@3,9-1088_v1_24@3,"R1 has interspecies dimensions and thus has evolutionary value extending from insects to humans, that is, from early life eras till now.","R1 has interspecies dimensions and thus has evolutionary and historical value extending from insects to humans, that is, from early life eras till now.",yes
10787,9-1088,9-1088_v2_33@6,9-1088_v1_24@6,"R1 includes genes and their products involved in circadian rhythms, while its major hub NCOR1 in macrophages blocks the pro-atherogenic functions of peroxisome proliferator-activated receptor gamma (PPARγ) ( Oppi et al., 2020 ), greatly implicating stroke pathophysiology.","R1 includes genes and their products involved in circadian rhythms, while its major hub NCOR1 in macrophages blocks the pro-atherogenic functions of peroxisome proliferator-activated receptor gamma (PPARγ) in atherosclerosis ( Oppi et al., 2020 ), greatly implicating stroke pathophysiology.",yes
10788,9-1088,9-1088_v2_33@8,9-1088_v1_24@8,"It is likely that as soon as R1 is disrupted, the atherogenic and/or other pathological processes progress dramatically, with potentially lethal consequences.","It is likely that as soon as R1 is disrupted, the atherogenic and/or other pathological processes progress dramatically, with lethal consequences.",yes
10789,9-1088,9-1088_v2_35@4,9-1088_v1_26@4,"Thus, future medical practice should probably take account of chronopathology ( Stienen et al., 2015 ) so as to prevent stroke mortality shifts (chronotherapy and chronoprevention plans).","Thus, future medical practice should probably take account of chronopathology so as to prevent stroke mortality shifts (chronotherapy and chronoprevention plans).",yes
10790,9-1088,9-1088_v2_37@0,9-1088_v1_28@0,"Our work clearly established that sunspot numbers and stroke mortality were inversely correlated, and that a violent fluctuation of sunspot numbers over 35% shifted monthly mortality with a phase delay of two months.","Our work established clearly that of sunspot numbers and stroke mortality were inversely correlated, and that a violent fluctuation of sunspot numbers over 35% shifted monthly mortality to a phase delay of two months.",yes
10791,9-1088,9-1088_v2_14@0,9-1088_v1_5@0,"Stroke has been previously associated with solar activity ( Halberg et al., 2001 ; Otsuka et al., 2001 ; Stoupel et al., 1995 ; Stoupel et al., 1996 ).","Stroke has been previously associated to solar activity ( Halgberg et al., 2001 ; Otsuka et al., 2001 ; Stoupel et al., 1995 ; Stoupel et al., 1996 ).",yes
10792,9-1088,9-1088_v2_15@0,9-1088_v1_6@0,"Our aim was to investigate the dynamics and trends in the selected time series, to determine sunspot numbers vs. daily and monthly stroke deaths in synchronized periodicities with gradual time delays (chronomes), and to define a sunspot number threshold beyond which stroke death events may be influenced.","Our aim was to investigate the dynamics and trends in the time series, to determine sunspot numbers vs. daily and monthly stroke deaths in synchronized periodicities with gradual time delays (chronomes), and to define a sunspot number threshold for presence of stroke mortality.",yes
10793,9-1088,9-1088_v2_17@0,9-1088_v1_8@0,"In this study we focused on monthly stroke mortality events between 1985 and 1989, based on the underlined cause of death data (ICD-9 Table 5.3: recode 430, Table 5.4: recode 200) from the archives of Piraeus Civil Registry ( Geronikolou & Zikos, 1991 ).","In this study we focused on monthly stroke mortality events between 1985 and 1989, based on the underlined cause of death data from the archives of Piraeus Civil Registry ( Geronikolou & Zikos, 1991 ).",yes
10794,9-1088,9-1088_v2_19@0,9-1088_v1_10@0,"The stroke death rate in Piraeus, was calculated using the formula (number of all deaths per year per 1000 people in June 30 th , year x).","The stroke death rate in Piraeus, was calculated over the formula (number of all deaths per year per 1000 people in June 30 th , year x).",yes
10795,9-1088,9-1088_v2_19@1,9-1088_v1_10@1,"The overall death rate was calculated with the denominators provided by the 1981 census ( Geronikolou & Zikos, 1991 ).",The overall death rate was calculated with the denominators provided by the 1981 census.,yes
10796,9-1088,9-1088_v2_23@4,9-1088_v1_14@4,"Thus, monthly sunspot numbers by squared root variation and their violent fluctuation of over 35% was correlated to monthly stroke mortality, establishing a negative correlation between the two time-series (sunspot numbers and deaths by stroke) ( Figure 1b, d ).","Thus, monthly sunspot numbers by squared root variation and their violent fluctuation of over 35% was correlated to monthly stroke mortality, establishing a negative correlation between the two time-series (sunspot numbers and deaths of strokes) ( Figure 1b, d ).",yes
10799,9-1088,9-1088_v2_29@4,9-1088_v1_20@4,"Socioeconomic and geographic disparities have been suspected, while heliomagnetic influences have been proposed as possible etiologic contributors to human pathology ( Halberg et al. , 1998 ; Stienen et al. , 2015 ; Stoupel et al. , 1996 ; Stoupel et al. , 2003 ).","Socioeconomic and geographic disparities have been suspected, while heliomagnetic influences have been proposed as possible etiologic contributors to human pathology.",yes
10800,9-1088,9-1088_v2_30@2,9-1088_v1_21@2,"Moreover, its population is representative of the urban populations in Greece ( Geronikolou & Zikos 1991 ).","Moreover, its population is representative of the urban populations in Greece ( Geronikolou, 1991 ).",yes
10801,9-1088,9-1088_v2_30@5,9-1088_v1_21@5,"The data used in this study were original and based on the reported underlined cause of death ( Geronikolou & Zikos, 1991 ).","The data used in this study were original and based on the underlined cause of death ( Geronikolou, 1991 ).",yes
,9-1088,9-1088_v2_19@0,9-1088_v1_10@1,"The stroke death rate in Piraeus, was calculated using the formula (number of all deaths per year per 1000 people in June 30 th , year x).",The overall death rate was calculated with the denominators provided by the 1981 census.,no
,9-1088,9-1088_v2_17@0,9-1088_v1_28@0,"In this study we focused on monthly stroke mortality events between 1985 and 1989, based on the underlined cause of death data (ICD-9 Table 5.3: recode 430, Table 5.4: recode 200) from the archives of Piraeus Civil Registry ( Geronikolou & Zikos, 1991 ).","Our work established clearly that of sunspot numbers and stroke mortality were inversely correlated, and that a violent fluctuation of sunspot numbers over 35% shifted monthly mortality to a phase delay of two months.",no
,9-1088,9-1088_v2_23@4,9-1088_v1_28@0,"Thus, monthly sunspot numbers by squared root variation and their violent fluctuation of over 35% was correlated to monthly stroke mortality, establishing a negative correlation between the two time-series (sunspot numbers and deaths by stroke) ( Figure 1b, d ).","Our work established clearly that of sunspot numbers and stroke mortality were inversely correlated, and that a violent fluctuation of sunspot numbers over 35% shifted monthly mortality to a phase delay of two months.",no
,9-1088,9-1088_v2_30@5,9-1088_v1_21@7,"The data used in this study were original and based on the reported underlined cause of death ( Geronikolou & Zikos, 1991 ).","The covariates related to diet, hygiene and culture were stable in this period and, thus, they could be safely assumed.",no
,9-1088,9-1088_v2_30@5,9-1088_v1_20@4,"The data used in this study were original and based on the reported underlined cause of death ( Geronikolou & Zikos, 1991 ).","Socioeconomic and geographic disparities have been suspected, while heliomagnetic influences have been proposed as possible etiologic contributors to human pathology.",no
,9-1088,9-1088_v2_30@2,9-1088_v1_23@2,"Moreover, its population is representative of the urban populations in Greece ( Geronikolou & Zikos 1991 ).","Our work postulates that there is an inverse relation in two time series, between the timing of sunspot numbers and stroke deaths, a hypothesis posed by previous investigations ( Geronikolou & Leontisis, 2005 ; Geronikolou & Petropoulos, 1996 ; Stoupel et al., 1999 ).",no
,9-1088,9-1088_v2_32@2,9-1088_v1_20@4,"Our work postulates that there is an inverse relation in two time series, between the timing of sunspot numbers and stroke deaths, a hypothesis posed by previous investigations ( Geronikolou & Leontitsis, 2005 ; Geronikolou & Petropoulos, 1996 ; Stoupel et al., 1999 ).","Socioeconomic and geographic disparities have been suspected, while heliomagnetic influences have been proposed as possible etiologic contributors to human pathology.",no
,9-1088,9-1088_v2_30@2,9-1088_v1_14@4,"Moreover, its population is representative of the urban populations in Greece ( Geronikolou & Zikos 1991 ).","Thus, monthly sunspot numbers by squared root variation and their violent fluctuation of over 35% was correlated to monthly stroke mortality, establishing a negative correlation between the two time-series (sunspot numbers and deaths of strokes) ( Figure 1b, d ).",no
,9-1088,9-1088_v2_15@0,9-1088_v1_8@0,"Our aim was to investigate the dynamics and trends in the selected time series, to determine sunspot numbers vs. daily and monthly stroke deaths in synchronized periodicities with gradual time delays (chronomes), and to define a sunspot number threshold beyond which stroke death events may be influenced.","In this study we focused on monthly stroke mortality events between 1985 and 1989, based on the underlined cause of death data from the archives of Piraeus Civil Registry ( Geronikolou & Zikos, 1991 ).",no
,9-1088,9-1088_v2_33@2,9-1088_v1_14@4,"The molecular interactions network approach, where the inter-species functional interactome of nuclear steroid receptors (R1) constructed on orthologues was employed ( Geronikolou et al., 2018 ).","Thus, monthly sunspot numbers by squared root variation and their violent fluctuation of over 35% was correlated to monthly stroke mortality, establishing a negative correlation between the two time-series (sunspot numbers and deaths of strokes) ( Figure 1b, d ).",no
,9-1088,9-1088_v2_33@6,9-1088_v1_26@4,"R1 includes genes and their products involved in circadian rhythms, while its major hub NCOR1 in macrophages blocks the pro-atherogenic functions of peroxisome proliferator-activated receptor gamma (PPARγ) ( Oppi et al., 2020 ), greatly implicating stroke pathophysiology.","Thus, future medical practice should probably take account of chronopathology so as to prevent stroke mortality shifts (chronotherapy and chronoprevention plans).",no
,9-1088,9-1088_v2_35@4,9-1088_v1_21@2,"Thus, future medical practice should probably take account of chronopathology ( Stienen et al., 2015 ) so as to prevent stroke mortality shifts (chronotherapy and chronoprevention plans).","Moreover, its population is representative of the urban populations in Greece ( Geronikolou, 1991 ).",no
,9-1088,9-1088_v2_33@2,9-1088_v1_21@5,"The molecular interactions network approach, where the inter-species functional interactome of nuclear steroid receptors (R1) constructed on orthologues was employed ( Geronikolou et al., 2018 ).","The data used in this study were original and based on the underlined cause of death ( Geronikolou, 1991 ).",no
,9-1088,9-1088_v2_33@6,9-1088_v1_8@0,"R1 includes genes and their products involved in circadian rhythms, while its major hub NCOR1 in macrophages blocks the pro-atherogenic functions of peroxisome proliferator-activated receptor gamma (PPARγ) ( Oppi et al., 2020 ), greatly implicating stroke pathophysiology.","In this study we focused on monthly stroke mortality events between 1985 and 1989, based on the underlined cause of death data from the archives of Piraeus Civil Registry ( Geronikolou & Zikos, 1991 ).",no
,9-1088,9-1088_v2_35@4,9-1088_v1_10@1,"Thus, future medical practice should probably take account of chronopathology ( Stienen et al., 2015 ) so as to prevent stroke mortality shifts (chronotherapy and chronoprevention plans).",The overall death rate was calculated with the denominators provided by the 1981 census.,no
,9-1088,9-1088_v2_23@4,9-1088_v1_21@7,"Thus, monthly sunspot numbers by squared root variation and their violent fluctuation of over 35% was correlated to monthly stroke mortality, establishing a negative correlation between the two time-series (sunspot numbers and deaths by stroke) ( Figure 1b, d ).","The covariates related to diet, hygiene and culture were stable in this period and, thus, they could be safely assumed.",no
,9-1088,9-1088_v2_32@2,9-1088_v1_24@8,"Our work postulates that there is an inverse relation in two time series, between the timing of sunspot numbers and stroke deaths, a hypothesis posed by previous investigations ( Geronikolou & Leontitsis, 2005 ; Geronikolou & Petropoulos, 1996 ; Stoupel et al., 1999 ).","It is likely that as soon as R1 is disrupted, the atherogenic and/or other pathological processes progress dramatically, with lethal consequences.",no
10918,9-1193,9-1193_v2_29@1,9-1193_v1_29@1,"Overall, 46.5% of the trials intended to use mortality as a primary (n=98) or secondary outcome (n=220; Table 1 ), and 53•4% (n=365) did not specify mortality as an outcome.","Overall, 46.4% of the trials intended to use mortality as a primary (n=98) or secondary outcome (n=369; Table 1 ).",yes
10919,9-1193,9-1193_v2_29@2,9-1193_v1_29@2,"Out of the 521 inpatient trials, 55.7% (n=290) planned on reporting mortality as an outcome.","Out of the 525 inpatient trials, 55.6% (n=292) planned on reporting mortality as an outcome.",yes
10920,9-1193,9-1193_v2_31@0,9-1193_v1_31@0,"Out of the 683 trials, 602 (88.1%) assessed treatment interventions (186,189 planned patients); drugs were more frequent (345 trials [57.3%]), encompassing a vast range of substances.","Out of the 689 trials, 607 (88.1%) assessed treatment interventions (187,209 planned patients); drugs were more frequent (349 trials [57.5%]), encompassing a vast range of substances.",yes
10921,9-1193,9-1193_v2_31@1,9-1193_v1_31@1,The two most common pharmacological classes were antiviral drugs (assessed in 141 trials; e.g. lopinavir/ritonavir [n=45]) and antimalarial drugs (111 trials; e.g. hydroxychloroquine [n=84]).,The two most common pharmacological classes were antiviral drugs (assessed in 144 trials; e.g. lopinavir/ritonavir [n=45]) and antimalarial drugs (112 trials; e.g. hydroxychloroquine [n=83]).,yes
10922,9-1193,9-1193_v2_31@2,9-1193_v1_31@2,"There were 106 trials investigating traditional medicine and 70 exploring highly diverse pharmaceuticals of various classes, e.g. bismuth potassium citrate, ebastine, pirfenidone, dipyridamole and hydrogen peroxidase.","There were 106 trials investigating traditional medicine and 70 exploring highly diverse pharmaceuticals of various classes, e.g. bismuth potassium citrate, ebastine, pirfenidone, dipyridamole and hydrogen peroxidase ( Figure 1 and see Extended data ) <REF-12> .",yes
10923,9-1193,9-1193_v2_31@5,9-1193_v1_31@3,"The comparators were predominantly standard of care or no intervention (47.2% [n=284]), placebo (17.1% [n=103]) or other interventions (17.9%; [n=108]) ( Table 1 ).","The comparators were predominantly standard of care or no intervention (47.1% [n=286]), placebo (17% [n=103]) or other interventions (18%; [n=109]) ( Table 1 ).",yes
10924,9-1193,9-1193_v2_35@0,9-1193_v1_35@0,"Overall, 77 trials (11.3%) focused on prevention (204,641 planned participants), mainly prophylactic drug use (n=41), vaccines (n=14; 9 already started recruitment; see Extended data ) <REF-12> and non-pharmaceutical interventions (n=10) (e.g. masks or the use of media and influencers in people’s compliance to hygienic practices).","Overall, 78 trials (11.3%) focused on prevention (205,841 planned participants), mainly prophylactic drug use (n=41), vaccines (n=14; 9 already started recruitment; see Extended data ) <REF-12> and non-pharmaceutical interventions (n=10) (e.g. masks or the use of media and influencers in people’s compliance to hygienic practices).",yes
10925,9-1193,9-1193_v2_37@0,9-1193_v1_37@0,"The number of trials increased rapidly; on average 0.5 trials per day were registered in January, 8.1 in February, 8.3 in March, and 17.2 in April 2020.","The number of trials increased rapidly; on average 0.5 trials per day were registered in January, 8.1 in February, 8.4 in March, and 17.6 in April 2020.",yes
10926,9-1193,9-1193_v2_38@0,9-1193_v1_38@0,Trials were conducted in 41 countries and through international collaborations ( Table 1 ; see Extended data ) <REF-12> .,Trials were conducted in 42 countries and through international collaborations ( Table 1 ; see Extended data ) <REF-12> .,yes
10927,9-1193,9-1193_v2_38@1,9-1193_v1_38@1,"Half were from China (51.4% [n=351]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.","Half were from China (51.1% [n=352]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.",yes
10928,9-1193,9-1193_v2_38@2,9-1193_v1_38@2,"Trial characteristics were similar across the five most frequent geographical locations (China, USA, France, Spain and international) contributing to 73.6% (n=503) of the global trial research ( Table 1 ).","Trial characteristics were similar across the five most frequent geographical locations (China, USA, France, Spain and international) contributing to 73.3% (n=505) of the global trial research ( Table 1 ).",yes
10929,9-1193,9-1193_v2_38@3,9-1193_v1_38@3,Traditional medicine was assessed in 30.5% of trials from China (n=107) but rarely in other countries.,Traditional medicine was assessed in 30.4% of trials from China (n=107) but rarely in other countries.,yes
10930,9-1193,9-1193_v2_41@1,9-1193_v1_41@1,"In February, 5.1% of trials included more than 500 participants in contrast to 18.6% of trials in March ( Figure 3 ).","In February, fewer than 8% of trials included more than 500 participants in contrast to 29.6% of trials in March ( Figure 3 ).",yes
10931,9-1193,9-1193_v2_41@4,9-1193_v1_41@4,"When the proportion of trials from China decreased, so did trials assessing traditional medicine (from 46.9% to 0.9%) while the proportion of trials assessing drugs rose (from 40.6% to 77.7%).","When the proportion of trials from China decreased, so did trials assessing traditional medicine (from 46.9% to 0.9%) while the proportion of trials assessing drugs rose (from 38.1% to 77.2%).",yes
10932,9-1193,9-1193_v2_45@0,9-1193_v1_45@0,"Out of the 683 trials, 6.6% (n=45) planned to enroll 1,000 to 5,000 participants.","Out of the 689 trials, 6.7% (n=46) planned to enroll 1,000 to 5,000 participants.",yes
10933,9-1193,9-1193_v2_45@1,9-1193_v1_45@1,"Most were randomized (88.9% [n=40]), assessed drugs (80%; n=36), and many were not blinded (53.3% [n=24]).","Most were randomized (89.1% [n=41]), assessed drugs (80.4%; n=37), and many were not blinded (52.2% [n=24]).",yes
10934,9-1193,9-1193_v2_45@3,9-1193_v1_45@3,"The top three regions were the United States (22.2%; n=10), France (11.1% [n=5]) and international collaborations (11.1% [n=5]) (see Extended data ) <REF-12> .","The top three regions were the United States (21.7%; n=10), France (13% [n=6]) and international collaborations (10.9% [n=5]) (see Extended data ) <REF-12> .",yes
10935,9-1193,9-1193_v2_46@0,9-1193_v1_46@0,"Eleven (1.6%) trials, registered between February and April 2020 (seven for treatment and four for prevention), planned to enroll over 5,000 participants (see Extended data ) <REF-12> .","Eleven (1.6%) trials, registered between February and April 2020, planned to enroll over 5,000 participants (see Extended data ) <REF-12> .",yes
10936,9-1193,9-1193_v2_47@0,9-1193_v1_47@0,Five drug interventions tested in these 11 larger trials were simultaneously investigated in over 20 smaller trials (see Extended data ) <REF-12> .,Six drug interventions tested in these 11 larger trials (seven for treatment and four for prevention) were simultaneously investigated in at least 10 smaller trials (see Extended data ) <REF-12> .,yes
10937,9-1193,9-1193_v2_47@1,9-1193_v1_47@1,"Overall, 167 trials (141 for treatment, 24 for prevention and two for treatment and prevention) with fewer than 5,000 participants assessed at least one intervention that was also assessed in a larger trial (median sample size 223 [IQR 80 to 540]; 132 had fewer than 1000 participants).","Overall, 169 trials (143 for treatment, 24 for prevention and two for treatment and prevention) with fewer than 5,000 participants assessed at least one intervention that was also assessed in a larger trial (median sample size 273 [IQR 90 to 700]; 134 had fewer than 1000 participants).",yes
10938,9-1193,9-1193_v2_47@2,9-1193_v1_47@2,For 103 of those (61.7%) the larger trial was registered before.,For 107 of those (63.5%) the larger trial was registered before.,yes
10939,9-1193,9-1193_v2_47@3,9-1193_v1_47@3,"For example, 104 trials with fewer than 5,000 participants tested hydroxychloroquine and 86 of them (82.7%) were registered after the first large trial testing this drug and 82 (78.8%) assessed hydroxychloroquine as treatment ( Figure 4 ; see Extended data ) <REF-12> .","For example, 106 trials with fewer than 5,000 participants tested hydroxychloroquine and 88 of them (83%) were registered after the first large trial testing this drug and 83 (77.6%) assessed hydroxychloroquine as treatment ( Figure 4 ; see Extended data ) <REF-12> .",yes
10940,9-1193,9-1193_v2_47@4,9-1193_v1_47@4,"These 104 trials had a median sample size of 334, but cumulatively, they planned to enroll as many patients as the four larger trials testing hydroxychloroquine (75,217 vs 77,000).","These 106 trials had a median sample size of 334, but cumulatively, they planned to enroll as many patients as the four larger trials testing hydroxychloroquine (76,617 vs 77,000).",yes
10941,9-1193,9-1193_v2_51@0,9-1193_v1_51@0,"By the end of 2020, 413 trials (60.5%) with a total of 159,957 planned participants were expected to be completed (i.e. last patient, last visit), including 232 drug trials (97,282 participants) and 22 over-1,000 participants trials and five over-5,000 participants trials.","By the end of 2020, 414 trials (60.1%) with a total of 160,107 planned participants were expected to be completed (i.e. last patient, last visit), including 240 drug trials (97,846 participants) and 22 over-1,000 participants trials and five over-5,000 participants trials.",yes
10942,9-1193,9-1193_v2_54@3,9-1193_v1_54@3,"Few trials focused on prevention, but some were very large and focused on healthcare workers (i.e. 24.3% of planned trial participants are healthcare workers).","Few trials focused on prevention, but some were very large and focused on healthcare workers (i.e. 24.4% of planned trial participants are healthcare workers).",yes
10943,9-1193,9-1193_v2_55@0,9-1193_v1_55@0,The emergence of 683 trials in a 100-day period is unparalleled.,The emergence of 689 trials in a 100-day period is unparalleled.,yes
10944,9-1193,9-1193_v2_56@0,9-1193_v1_56@0,"Thomas Chalmers highlighted in 1977 the need to ‘Randomize the first patient’ <REF-19> , and a reassuring 75.5% of trials are indeed randomized.","Thomas Chalmers highlighted in 1977 the need to ‘Randomize the first patient’ <REF-19> , and a reassuring 75% of trials are indeed randomized.",yes
10945,9-1193,9-1193_v2_56@3,9-1193_v1_56@3,"Blinding may not be required for mortality outcomes; however, mortality was rarely a primary outcome.","Blinding may not be required for mortality outcomes; however, it was rarely a primary outcome.",yes
10946,9-1193,9-1193_v2_56@5,9-1193_v1_56@4,Half of the trials include fewer than 118 patients and many small trials were initiated after public registration of very large trials addressing similar questions.,Half of the trials include fewer than 120 patients and many small trials were initiated after public registration of very large trials addressing similar questions.,yes
10947,9-1193,9-1193_v2_58@1,9-1193_v1_58@1,"Strongly endorsed and prioritized by authorities and medical representatives <REF-27> , it is running as a streamlined pragmatic platform trial in over 176 hospitals, randomizing over 12,000 patients in just over four months <REF-14> .","Strongly endorsed and prioritized by authorities and medical representatives <REF-26> , it is running as streamlined pragmatic platform trial in over 176 hospitals, randomizing over 12,000 patients in just over four months <REF-14> .",yes
10948,9-1193,9-1193_v2_63@6,9-1193_v1_63@5,"Fifth, we may have missed a few cases of duplicate entries across registries or of multiple national parts of an international trial, thus slightly overestimating the number of trials but not affecting the overall interpretation.","Fourth, we may have missed a few cases of duplicate entries across registries or of multiple national parts of an international trial, thus slightly overestimating the number of trials but not affecting the overall interpretation.",yes
10949,9-1193,9-1193_v2_63@7,9-1193_v1_63@6,"Sixth, we arbitrarily selected a period of the first 100 days, which is traditionally used to benchmark early outcomes of policies or presidencies.","Fifth, we arbitrarily selected a period of the first 100 days, which is traditionally used to benchmark early outcomes of policies or presidencies.",yes
10950,9-1193,9-1193_v2_24@0,9-1193_v1_24@0,"We identified 683 trials registered or published over the pandemic’s first 100 days, testing interventions to treat or prevent COVID-19 (see Underlying data ) <REF-12> with a total planned sample size of 394,146 participants.","We identified 689 trials registered or published over the pandemic’s first 100 days, testing interventions to treat or prevent COVID-19 (see Underlying data ) <REF-12> with a total planned sample size of 396,366 participants.",yes
10951,9-1193,9-1193_v2_24@2,9-1193_v1_24@2,"Twenty-nine (4.2%) were active but no longer recruiting (58,589 participants), 381 (55.8%) started recruiting (215,807 participants), 174 (25.5%) had not yet started (97,406 participants), 50 (7.3%) were discontinued (12,048 participants), and 4 (0.6%) were terminated (577 participants).","Thirty (4.4%) were active but no longer recruiting (59,259 participants), 384 (55.7%) started recruiting (217,357 participants), 174 (25.3%) had not yet started (97,406 participants), 50 (7.3%) were discontinued (12,048 participants), and 4 (0.6%) were terminated (577 participants).",yes
10952,9-1193,9-1193_v2_24@3,9-1193_v1_24@3,The status was unknown for 10 (1.5%; 168 participants).,The status was unknown for 12 (1.7%; 168 participants).,yes
10953,9-1193,9-1193_v2_26@0,9-1193_v1_26@0,"The 683 trials’ median target sample size was 118 (IQR 60 to 300; Table 1 ); 40.7% (n=280) planned to enroll fewer than 100 participants, 8.2% (n=56) over 1,000, and 1.6% (n=11) over 5,000 (see Underlying data ) <REF-12> .","The 689 trials’ median target sample size was 120 (IQR 60 to 300; Table 1 ); 40.7% (n=280) planned to enroll fewer than 100 participants, 8.3% (n=57) over 1,000, and 1.6% (n=11) over 5,000 (see Underlying data ) <REF-12> .",yes
10954,9-1193,9-1193_v2_26@1,9-1193_v1_26@1,75.5% (n=516) trials were randomized and 59.4% (n=406) did not use blinding ( Table 1 ).,75.8% (n=522) trials were randomized and 59.2% (n=408) did not use blinding ( Table 1 ).,yes
10955,9-1193,9-1193_v2_26@2,9-1193_v1_26@2,Randomized trials were on average almost three times larger than non-randomized trials (median sample size 144 vs. 50).,Randomized trials were on average three times larger than non-randomized trials (median sample size 150 vs. 50).,yes
,9-1193,9-1193_v2_63@7,9-1193_v1_38@2,"Sixth, we arbitrarily selected a period of the first 100 days, which is traditionally used to benchmark early outcomes of policies or presidencies.","Trial characteristics were similar across the five most frequent geographical locations (China, USA, France, Spain and international) contributing to 73.3% (n=505) of the global trial research ( Table 1 ).",no
,9-1193,9-1193_v2_38@0,9-1193_v1_38@1,Trials were conducted in 41 countries and through international collaborations ( Table 1 ; see Extended data ) <REF-12> .,"Half were from China (51.1% [n=352]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.",no
,9-1193,9-1193_v2_37@0,9-1193_v1_56@0,"The number of trials increased rapidly; on average 0.5 trials per day were registered in January, 8.1 in February, 8.3 in March, and 17.2 in April 2020.","Thomas Chalmers highlighted in 1977 the need to ‘Randomize the first patient’ <REF-19> , and a reassuring 75% of trials are indeed randomized.",no
,9-1193,9-1193_v2_38@1,9-1193_v1_63@5,"Half were from China (51.4% [n=351]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.","Fourth, we may have missed a few cases of duplicate entries across registries or of multiple national parts of an international trial, thus slightly overestimating the number of trials but not affecting the overall interpretation.",no
,9-1193,9-1193_v2_38@1,9-1193_v1_24@3,"Half were from China (51.4% [n=351]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.",The status was unknown for 12 (1.7%; 168 participants).,no
,9-1193,9-1193_v2_31@5,9-1193_v1_46@0,"The comparators were predominantly standard of care or no intervention (47.2% [n=284]), placebo (17.1% [n=103]) or other interventions (17.9%; [n=108]) ( Table 1 ).","Eleven (1.6%) trials, registered between February and April 2020, planned to enroll over 5,000 participants (see Extended data ) <REF-12> .",no
,9-1193,9-1193_v2_56@0,9-1193_v1_29@1,"Thomas Chalmers highlighted in 1977 the need to ‘Randomize the first patient’ <REF-19> , and a reassuring 75.5% of trials are indeed randomized.","Overall, 46.4% of the trials intended to use mortality as a primary (n=98) or secondary outcome (n=369; Table 1 ).",no
,9-1193,9-1193_v2_55@0,9-1193_v1_47@0,The emergence of 683 trials in a 100-day period is unparalleled.,Six drug interventions tested in these 11 larger trials (seven for treatment and four for prevention) were simultaneously investigated in at least 10 smaller trials (see Extended data ) <REF-12> .,no
,9-1193,9-1193_v2_45@0,9-1193_v1_29@1,"Out of the 683 trials, 6.6% (n=45) planned to enroll 1,000 to 5,000 participants.","Overall, 46.4% of the trials intended to use mortality as a primary (n=98) or secondary outcome (n=369; Table 1 ).",no
,9-1193,9-1193_v2_54@3,9-1193_v1_24@0,"Few trials focused on prevention, but some were very large and focused on healthcare workers (i.e. 24.3% of planned trial participants are healthcare workers).","We identified 689 trials registered or published over the pandemic’s first 100 days, testing interventions to treat or prevent COVID-19 (see Underlying data ) <REF-12> with a total planned sample size of 396,366 participants.",no
,9-1193,9-1193_v2_47@4,9-1193_v1_24@2,"These 104 trials had a median sample size of 334, but cumulatively, they planned to enroll as many patients as the four larger trials testing hydroxychloroquine (75,217 vs 77,000).","Thirty (4.4%) were active but no longer recruiting (59,259 participants), 384 (55.7%) started recruiting (217,357 participants), 174 (25.3%) had not yet started (97,406 participants), 50 (7.3%) were discontinued (12,048 participants), and 4 (0.6%) were terminated (577 participants).",no
,9-1193,9-1193_v2_45@0,9-1193_v1_47@3,"Out of the 683 trials, 6.6% (n=45) planned to enroll 1,000 to 5,000 participants.","For example, 106 trials with fewer than 5,000 participants tested hydroxychloroquine and 88 of them (83%) were registered after the first large trial testing this drug and 83 (77.6%) assessed hydroxychloroquine as treatment ( Figure 4 ; see Extended data ) <REF-12> .",no
,9-1193,9-1193_v2_58@1,9-1193_v1_31@2,"Strongly endorsed and prioritized by authorities and medical representatives <REF-27> , it is running as a streamlined pragmatic platform trial in over 176 hospitals, randomizing over 12,000 patients in just over four months <REF-14> .","There were 106 trials investigating traditional medicine and 70 exploring highly diverse pharmaceuticals of various classes, e.g. bismuth potassium citrate, ebastine, pirfenidone, dipyridamole and hydrogen peroxidase ( Figure 1 and see Extended data ) <REF-12> .",no
,9-1193,9-1193_v2_47@4,9-1193_v1_26@0,"These 104 trials had a median sample size of 334, but cumulatively, they planned to enroll as many patients as the four larger trials testing hydroxychloroquine (75,217 vs 77,000).","The 689 trials’ median target sample size was 120 (IQR 60 to 300; Table 1 ); 40.7% (n=280) planned to enroll fewer than 100 participants, 8.3% (n=57) over 1,000, and 1.6% (n=11) over 5,000 (see Underlying data ) <REF-12> .",no
,9-1193,9-1193_v2_38@1,9-1193_v1_56@4,"Half were from China (51.4% [n=351]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.",Half of the trials include fewer than 120 patients and many small trials were initiated after public registration of very large trials addressing similar questions.,no
,9-1193,9-1193_v2_45@1,9-1193_v1_45@3,"Most were randomized (88.9% [n=40]), assessed drugs (80%; n=36), and many were not blinded (53.3% [n=24]).","The top three regions were the United States (21.7%; n=10), France (13% [n=6]) and international collaborations (10.9% [n=5]) (see Extended data ) <REF-12> .",no
,9-1193,9-1193_v2_24@3,9-1193_v1_31@2,The status was unknown for 10 (1.5%; 168 participants).,"There were 106 trials investigating traditional medicine and 70 exploring highly diverse pharmaceuticals of various classes, e.g. bismuth potassium citrate, ebastine, pirfenidone, dipyridamole and hydrogen peroxidase ( Figure 1 and see Extended data ) <REF-12> .",no
,9-1193,9-1193_v2_38@2,9-1193_v1_45@3,"Trial characteristics were similar across the five most frequent geographical locations (China, USA, France, Spain and international) contributing to 73.6% (n=503) of the global trial research ( Table 1 ).","The top three regions were the United States (21.7%; n=10), France (13% [n=6]) and international collaborations (10.9% [n=5]) (see Extended data ) <REF-12> .",no
,9-1193,9-1193_v2_45@3,9-1193_v1_24@2,"The top three regions were the United States (22.2%; n=10), France (11.1% [n=5]) and international collaborations (11.1% [n=5]) (see Extended data ) <REF-12> .","Thirty (4.4%) were active but no longer recruiting (59,259 participants), 384 (55.7%) started recruiting (217,357 participants), 174 (25.3%) had not yet started (97,406 participants), 50 (7.3%) were discontinued (12,048 participants), and 4 (0.6%) were terminated (577 participants).",no
,9-1193,9-1193_v2_47@2,9-1193_v1_51@0,For 103 of those (61.7%) the larger trial was registered before.,"By the end of 2020, 414 trials (60.1%) with a total of 160,107 planned participants were expected to be completed (i.e. last patient, last visit), including 240 drug trials (97,846 participants) and 22 over-1,000 participants trials and five over-5,000 participants trials.",no
,9-1193,9-1193_v2_24@3,9-1193_v1_47@2,The status was unknown for 10 (1.5%; 168 participants).,For 107 of those (63.5%) the larger trial was registered before.,no
,9-1193,9-1193_v2_56@3,9-1193_v1_47@3,"Blinding may not be required for mortality outcomes; however, mortality was rarely a primary outcome.","For example, 106 trials with fewer than 5,000 participants tested hydroxychloroquine and 88 of them (83%) were registered after the first large trial testing this drug and 83 (77.6%) assessed hydroxychloroquine as treatment ( Figure 4 ; see Extended data ) <REF-12> .",no
,9-1193,9-1193_v2_51@0,9-1193_v1_41@4,"By the end of 2020, 413 trials (60.5%) with a total of 159,957 planned participants were expected to be completed (i.e. last patient, last visit), including 232 drug trials (97,282 participants) and 22 over-1,000 participants trials and five over-5,000 participants trials.","When the proportion of trials from China decreased, so did trials assessing traditional medicine (from 46.9% to 0.9%) while the proportion of trials assessing drugs rose (from 38.1% to 77.2%).",no
,9-1193,9-1193_v2_45@1,9-1193_v1_38@1,"Most were randomized (88.9% [n=40]), assessed drugs (80%; n=36), and many were not blinded (53.3% [n=24]).","Half were from China (51.1% [n=352]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.",no
,9-1193,9-1193_v2_24@2,9-1193_v1_63@6,"Twenty-nine (4.2%) were active but no longer recruiting (58,589 participants), 381 (55.8%) started recruiting (215,807 participants), 174 (25.5%) had not yet started (97,406 participants), 50 (7.3%) were discontinued (12,048 participants), and 4 (0.6%) were terminated (577 participants).","Fifth, we arbitrarily selected a period of the first 100 days, which is traditionally used to benchmark early outcomes of policies or presidencies.",no
,9-1193,9-1193_v2_45@1,9-1193_v1_56@3,"Most were randomized (88.9% [n=40]), assessed drugs (80%; n=36), and many were not blinded (53.3% [n=24]).","Blinding may not be required for mortality outcomes; however, it was rarely a primary outcome.",no
,9-1193,9-1193_v2_45@1,9-1193_v1_24@2,"Most were randomized (88.9% [n=40]), assessed drugs (80%; n=36), and many were not blinded (53.3% [n=24]).","Thirty (4.4%) were active but no longer recruiting (59,259 participants), 384 (55.7%) started recruiting (217,357 participants), 174 (25.3%) had not yet started (97,406 participants), 50 (7.3%) were discontinued (12,048 participants), and 4 (0.6%) were terminated (577 participants).",no
,9-1193,9-1193_v2_41@4,9-1193_v1_29@1,"When the proportion of trials from China decreased, so did trials assessing traditional medicine (from 46.9% to 0.9%) while the proportion of trials assessing drugs rose (from 40.6% to 77.7%).","Overall, 46.4% of the trials intended to use mortality as a primary (n=98) or secondary outcome (n=369; Table 1 ).",no
,9-1193,9-1193_v2_51@0,9-1193_v1_31@1,"By the end of 2020, 413 trials (60.5%) with a total of 159,957 planned participants were expected to be completed (i.e. last patient, last visit), including 232 drug trials (97,282 participants) and 22 over-1,000 participants trials and five over-5,000 participants trials.",The two most common pharmacological classes were antiviral drugs (assessed in 144 trials; e.g. lopinavir/ritonavir [n=45]) and antimalarial drugs (112 trials; e.g. hydroxychloroquine [n=83]).,no
,9-1193,9-1193_v2_56@5,9-1193_v1_31@3,Half of the trials include fewer than 118 patients and many small trials were initiated after public registration of very large trials addressing similar questions.,"The comparators were predominantly standard of care or no intervention (47.1% [n=286]), placebo (17% [n=103]) or other interventions (18%; [n=109]) ( Table 1 ).",no
,9-1193,9-1193_v2_47@3,9-1193_v1_54@3,"For example, 104 trials with fewer than 5,000 participants tested hydroxychloroquine and 86 of them (82.7%) were registered after the first large trial testing this drug and 82 (78.8%) assessed hydroxychloroquine as treatment ( Figure 4 ; see Extended data ) <REF-12> .","Few trials focused on prevention, but some were very large and focused on healthcare workers (i.e. 24.4% of planned trial participants are healthcare workers).",no
,9-1193,9-1193_v2_24@3,9-1193_v1_38@1,The status was unknown for 10 (1.5%; 168 participants).,"Half were from China (51.1% [n=352]), which dominated initially ( Figure 2 ); starting March 2020, more trials came from other countries.",no
,9-1193,9-1193_v2_47@2,9-1193_v1_31@0,For 103 of those (61.7%) the larger trial was registered before.,"Out of the 689 trials, 607 (88.1%) assessed treatment interventions (187,209 planned patients); drugs were more frequent (349 trials [57.5%]), encompassing a vast range of substances.",no
,9-1193,9-1193_v2_63@6,9-1193_v1_41@4,"Fifth, we may have missed a few cases of duplicate entries across registries or of multiple national parts of an international trial, thus slightly overestimating the number of trials but not affecting the overall interpretation.","When the proportion of trials from China decreased, so did trials assessing traditional medicine (from 46.9% to 0.9%) while the proportion of trials assessing drugs rose (from 38.1% to 77.2%).",no
,9-1193,9-1193_v2_55@0,9-1193_v1_47@1,The emergence of 683 trials in a 100-day period is unparalleled.,"Overall, 169 trials (143 for treatment, 24 for prevention and two for treatment and prevention) with fewer than 5,000 participants assessed at least one intervention that was also assessed in a larger trial (median sample size 273 [IQR 90 to 700]; 134 had fewer than 1000 participants).",no
,9-1193,9-1193_v2_47@1,9-1193_v1_55@0,"Overall, 167 trials (141 for treatment, 24 for prevention and two for treatment and prevention) with fewer than 5,000 participants assessed at least one intervention that was also assessed in a larger trial (median sample size 223 [IQR 80 to 540]; 132 had fewer than 1000 participants).",The emergence of 689 trials in a 100-day period is unparalleled.,no
,9-1193,9-1193_v2_56@0,9-1193_v1_26@2,"Thomas Chalmers highlighted in 1977 the need to ‘Randomize the first patient’ <REF-19> , and a reassuring 75.5% of trials are indeed randomized.",Randomized trials were on average three times larger than non-randomized trials (median sample size 150 vs. 50).,no
,9-1193,9-1193_v2_47@3,9-1193_v1_45@0,"For example, 104 trials with fewer than 5,000 participants tested hydroxychloroquine and 86 of them (82.7%) were registered after the first large trial testing this drug and 82 (78.8%) assessed hydroxychloroquine as treatment ( Figure 4 ; see Extended data ) <REF-12> .","Out of the 689 trials, 6.7% (n=46) planned to enroll 1,000 to 5,000 participants.",no
11342,9-356,9-356_v2_21@4,9-356_v1_21@4,Such improvement allows researchers to culture small volume of a large number of Agrobacterium strains in parallel and use the diluted cultures to carry out high-throughput transformation of a large number of different constructs into Arabidopsis plants.,Such improvement allows researchers to culture small volume of a large number of Agrobacterium strains in parallel and use the diluted cultures to carry out high-throughput transformation of a large number different constructs into Arabidopsis plants.,yes
11343,9-356,9-356_v2_4@1,9-356_v1_4@1,"The development of different transformation protocols in various plants has enabled advances in plant molecular biology and crop improvements ( Saifi et al ., 2020 ).",The development of different transformation protocols in various plants has enabled advances in plant molecular biology and crop improvements.,yes
11344,9-356,9-356_v2_4@3,9-356_v1_4@3,"During 1980s and early 1990s, generating transgenic plants by leaf disc-based Agrobacterium -mediated transformation required laborious plant tissue culture and regeneration steps.","During 1980s and early 1990s, generating transgenic plants by leaf disc-based Agrobacterium -mediated transformation requires laborious plant tissue culture and regeneration steps.",yes
11345,9-356,9-356_v2_4@8,9-356_v1_4@8,"Floral dip transformation may be feasible in plants such as wheat, maize, tomato, flax, Medicago truncatula and Setaria viridis ( Agarwal et al ., 2009 ; Bastaki & Cullis, 2014 ; Martins et al ., 2015 ; Mu et al ., 2012 ; Trieu et al ., 2000 ; Yasmeen et al ., 2009 ).","Floral dip transformation may be feasible in plants such as wheat and Setaria viridis ( Agarwal et al ., 2009 ).",yes
11346,9-356,9-356_v2_5@2,9-356_v1_5@2,"Here, we tested whether a low concentration of Agrobacterium inoculum affects the plant transformation rate.","Here, we tested whether low concentration of Agrobacterium inoculum affects the plant transformation rate.",yes
11347,9-356,9-356_v2_5@3,9-356_v1_5@3,"Our data showed that, contrary to our expectation, using an extremely low density of Agrobacterium inoculum (OD 600 =0.002) in the floral dip method still warrants relatively high transformation rate in Arabidopsis.","Our data showed that, in contrary to our expectation, using extremely low density of Agrobacterium inoculum (OD 600 =0.002) in floral dip method still warrants relatively high transformation rate in Arabidopsis.",yes
11348,9-356,9-356_v2_10@3,9-356_v1_10@3,Then the overnight culture was diluted into 100 ml LB media with kanamycin (50 μg/ml) and allowed to grow further for 8 h (OD 600 =1.5~1.8) in the same shaker.,Then the overnight culture was diluted into 100 ml LB media with kanamycin (50 μg/ml) and allowed to grow further for 8 h in the same shaker.,yes
11349,9-356,9-356_v2_2@4,9-356_v1_2@4,Our data revealed that the floral dip method still guarantees a relatively high transformation rate in the Arabidopsis thaliana Col-0 ecotype even with very low Agrobacterium inoculum (OD 600 =0.002).,Our data revealed that the floral dip method still guarantees relatively high transformation rate in Arabidopsis thaliana Col-0 ecotype even with very low Agrobacterium inoculum (OD 600 =0.002).,yes
,9-356,9-356_v2_10@3,9-356_v1_21@4,Then the overnight culture was diluted into 100 ml LB media with kanamycin (50 μg/ml) and allowed to grow further for 8 h (OD 600 =1.5~1.8) in the same shaker.,Such improvement allows researchers to culture small volume of a large number of Agrobacterium strains in parallel and use the diluted cultures to carry out high-throughput transformation of a large number different constructs into Arabidopsis plants.,no
,9-356,9-356_v2_2@4,9-356_v1_5@2,Our data revealed that the floral dip method still guarantees a relatively high transformation rate in the Arabidopsis thaliana Col-0 ecotype even with very low Agrobacterium inoculum (OD 600 =0.002).,"Here, we tested whether low concentration of Agrobacterium inoculum affects the plant transformation rate.",no
,9-356,9-356_v2_4@3,9-356_v1_5@3,"During 1980s and early 1990s, generating transgenic plants by leaf disc-based Agrobacterium -mediated transformation required laborious plant tissue culture and regeneration steps.","Our data showed that, in contrary to our expectation, using extremely low density of Agrobacterium inoculum (OD 600 =0.002) in floral dip method still warrants relatively high transformation rate in Arabidopsis.",no
,9-356,9-356_v2_4@3,9-356_v1_2@4,"During 1980s and early 1990s, generating transgenic plants by leaf disc-based Agrobacterium -mediated transformation required laborious plant tissue culture and regeneration steps.",Our data revealed that the floral dip method still guarantees relatively high transformation rate in Arabidopsis thaliana Col-0 ecotype even with very low Agrobacterium inoculum (OD 600 =0.002).,no
,9-356,9-356_v2_5@3,9-356_v1_2@4,"Our data showed that, contrary to our expectation, using an extremely low density of Agrobacterium inoculum (OD 600 =0.002) in the floral dip method still warrants relatively high transformation rate in Arabidopsis.",Our data revealed that the floral dip method still guarantees relatively high transformation rate in Arabidopsis thaliana Col-0 ecotype even with very low Agrobacterium inoculum (OD 600 =0.002).,no
,9-356,9-356_v2_4@3,9-356_v1_4@8,"During 1980s and early 1990s, generating transgenic plants by leaf disc-based Agrobacterium -mediated transformation required laborious plant tissue culture and regeneration steps.","Floral dip transformation may be feasible in plants such as wheat and Setaria viridis ( Agarwal et al ., 2009 ).",no
,9-356,9-356_v2_4@3,9-356_v1_10@3,"During 1980s and early 1990s, generating transgenic plants by leaf disc-based Agrobacterium -mediated transformation required laborious plant tissue culture and regeneration steps.",Then the overnight culture was diluted into 100 ml LB media with kanamycin (50 μg/ml) and allowed to grow further for 8 h in the same shaker.,no
,9-356,9-356_v2_5@2,9-356_v1_4@1,"Here, we tested whether a low concentration of Agrobacterium inoculum affects the plant transformation rate.",The development of different transformation protocols in various plants has enabled advances in plant molecular biology and crop improvements.,no
11368,9-47,9-47_v2_24@0,9-47_v1_24@0,Expression interaction,Seurat interaction,yes
11369,9-47,9-47_v2_25@2,9-47_v1_25@2,"Using the combineExpression function in scRepertoire, we can look at the clonotypic frequencies of cells that comprise the UMAP-based clusters ( Figure 4C ).","Using the combineSeurat function in scRepertoire, we can look at the clonotypic frequencies of cells that comprise the UMAP-based clusters ( Figure 4C ), with notable expansion in the C2, C3, and C6 clusters ( Figure 4D ).",yes
11370,9-47,9-47_v2_25@7,9-47_v1_25@6,"After combining both the clonotype and expression data, interaction between categories, such as cluster label and clonotype frequency can be visualized with the alluvialClonotypes function ( Figure 4E ).","After combining both the clonotype and expression data, interaction between categories, such as cluster label and clonotype frequency can be visualized with the alluvialGraph function.",yes
11371,9-47,9-47_v2_5@1,9-47_v1_5@1,"Built using R, scRepertoire is a toolkit to assist in the analysis of immune profiles for both B and T cells, while interacting with the popular Seurat pipeline <REF-4> – <REF-6> , as well as SingleCellExperiment and monocle3 class expression objects.","Built using R, scRepertoire is a toolkit to assist in the analysis of immune profiles for both B and T cells, while interacting with the popular Seurat pipeline <REF-4> – <REF-6> .",yes
11372,9-47,9-47_v2_2@4,9-47_v1_2@4,"Enabling users to easily combine mRNA and immune profiling, scRepertoire was built to process data derived from 10x Genomics Chromium Immune Profiling for both T-cell receptor (TCR) and immunoglobulin (Ig) enrichment workflows and subsequently interacts with a number of popular R packages for single-cell expression, such as Seurat.","Enabling users to easily combine mRNA and immune profiling, scRepertoire was built to process data derived from 10x Genomics Chromium Immune Profiling for both T-cell receptor (TCR) and immunoglobulin (Ig) enrichment workflows and subsequently interacts with the popular Seurat R package.",yes
,9-47,9-47_v2_17@4,9-47_v1_25@3,More advance distribution analysis is also available using the clonesizeDistribution function based on recent work using Jensen-Shannon divergence.,The C7 and C8 clusters also have a relatively high frequency.,no
,9-47,9-47_v2_25@3,9-47_v1_25@3,This function also works with the SingleCellExperiment and monocle3 class of expression objects.,The C7 and C8 clusters also have a relatively high frequency.,no
,9-47,9-47_v2_29@3,9-47_v1_25@3,"Visualization functions in scRepertoire have a parameter, exportTable, allowing users to examine the quantifications underlying the generation of the graphs.",The C7 and C8 clusters also have a relatively high frequency.,no
,9-47,9-47_v2_24@0,9-47_v1_2@4,Expression interaction,"Enabling users to easily combine mRNA and immune profiling, scRepertoire was built to process data derived from 10x Genomics Chromium Immune Profiling for both T-cell receptor (TCR) and immunoglobulin (Ig) enrichment workflows and subsequently interacts with the popular Seurat R package.",no
,9-47,9-47_v2_24@0,9-47_v1_5@1,Expression interaction,"Built using R, scRepertoire is a toolkit to assist in the analysis of immune profiles for both B and T cells, while interacting with the popular Seurat pipeline <REF-4> – <REF-6> .",no
