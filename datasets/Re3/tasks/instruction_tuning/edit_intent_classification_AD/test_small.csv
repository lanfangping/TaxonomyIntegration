edit_index,doc_name,node_ix_src,node_ix_tgt,text_src,text_tgt,gold,label
0,10-ARR,10-ARR_v2_4@2,,"With this in mind, it is natural to consider how the advancement in natural language processing can be leveraged to help counseling.",,"Add,Claim",Claim
1,10-ARR,10-ARR_v2_5@1,,"Reflective listening asks the counselor not only to listen to the client carefully, but also to actively make a guess of what the client means.",,"Add,Claim",Claim
2,10-ARR,10-ARR_v2_5@3,,"However, people do not always say what they mean, which is especially the case for patients seeking mental support.",,"Add,Claim",Claim
3,10-ARR,10-ARR_v2_5@4,,"Reflection, as the response made based on reflective listening, sometimes needs to decode the client's meaning not explicitly expressed in words.",,"Add,Claim",Claim
4,10-ARR,10-ARR_v2_5@5,,"On the other hand, pressing the client to clarify the missing part may hinder them from expressing their own experience (Miller and Rollnick, 2012).",,"Add,Fact/Evidence",Fact/Evidence
5,10-ARR,10-ARR_v2_5@10,,All these cases pose challenges to state-of-the-art language models.,,"Add,Claim",Claim
6,10-ARR,10-ARR_v2_12@0,,Previous research has addressed the task of automating response generation in health care and counseling settings.,,"Add,Fact/Evidence",Fact/Evidence
7,10-ARR,10-ARR_v2_12@1,,Greer et al. (2019) used a decision tree to deliver pre-written scripts and guide the user to learn a set of positive emotion skills.,,"Add,Fact/Evidence",Fact/Evidence
8,10-ARR,10-ARR_v2_12@2,,V et al. (2019) identified medical entities and the client's intent to fetch an answer for cancer related questions.,,"Add,Fact/Evidence",Fact/Evidence
9,10-ARR,10-ARR_v2_12@3,,Almusharraf et al. (2020) classified client's responses to choose which question to ask next for smoking cessation.,,"Add,Fact/Evidence",Fact/Evidence
10,10-ARR,10-ARR_v2_12@4,,"There are also commercial systems like Woebot (Fitzpatrick et al., 2017) that detect mental health issues mentioned by the user and direct them to relevant information.",,"Add,Fact/Evidence",Fact/Evidence
11,10-ARR,10-ARR_v2_12@5,,"However, there is a limited amount of work on free-form generation as compared to the template-based approaches described above.",,"Add,Claim",Claim
12,10-ARR,10-ARR_v2_12@6,,Shen et al. (2020) focused on generating counseling reflections with GPT-2 based on the dialogue context and responses retrieved from similar counseling sessions.,,"Add,Fact/Evidence",Fact/Evidence
13,10-ARR,10-ARR_v2_12@8,,"To the best of our knowledge, the effect of knowledge in counseling response generation is not yet well studied.",,"Add,Claim",Claim
14,10-ARR,10-ARR_v2_35@4,,We use the original implementation 6 and the pretrained weights on ConceptNet.,,"Add,Fact/Evidence",Fact/Evidence
44,103-ARR,103-ARR_v2_53@0,,"Our work paves the way for a new paradigm for IE, where the expert defines the schema using natural language and directly runs those specifications, annotating a handful of examples in the process, and allowing for quick trial-and-error iterations.",,"Add,Claim",Claim
45,103-ARR,103-ARR_v2_53@1,,Sainz et al. (2022) propose a user interface alongside this paradigm.,,"Add,Fact/Evidence",Fact/Evidence
46,103-ARR,103-ARR_v2_53@2,,"More generally, inference capability could be extended, acquired and applied from other tasks, in a research avenue where entailment and task performance improve in tandem.",,"Add,Claim",Claim
47,103-ARR,103-ARR_v2_55@0,,The fine-tuned models derived from this work will be uploaded to HuggingFace Models repository.,,"Add,Fact/Evidence",Fact/Evidence
48,103-ARR,103-ARR_v2_55@1,,Check the GitHub repository for updated information.,,"Add,Fact/Evidence",Fact/Evidence
78,110-ARR,110-ARR_v2_48@4,,"However, the representation hardly captures their semantic antonomy, e.g., gender.",,"Add,Claim",Claim
79,110-ARR,110-ARR_v2_53@1,,We multiply 100 to each value for a better readability.,,"Add,Fact/Evidence",Fact/Evidence
80,110-ARR,110-ARR_v2_53@2,,Note that the lower the values the better.,,"Add,Fact/Evidence",Fact/Evidence
81,110-ARR,110-ARR_v2_67@2,,We train the models for 10 epochs for each dataset and apply the early stopping technique where the patience number is set to 3.,,"Add,Fact/Evidence",Fact/Evidence
82,110-ARR,110-ARR_v2_67@3,,It is observed that the training is generally finished within 8 epochs for all the models.,,"Add,Fact/Evidence",Fact/Evidence
83,110-ARR,110-ARR_v2_67@4,,The batch size per GPU and learning rates used for each dataset are described in Table 8.,,"Add,Fact/Evidence",Fact/Evidence
84,110-ARR,110-ARR_v2_67@5,,"Datasets with large training set (e.g., MNLI, QNLI, and QQP) were not sensitive to the hyperparameters.",,"Add,Fact/Evidence",Fact/Evidence
125,111-ARR,111-ARR_v2_89@0,,Conclusion,,"Add,Other",Other
126,111-ARR,111-ARR_v2_116@3,,"In Table 9 we see a stronger correlation of human annotations with LaBSE compared to Sentence-BERT, especially for languages like Bengali, Kannada for which Sentence-BERT did not see parallel data.",,"Add,Fact/Evidence",Fact/Evidence
127,111-ARR,111-ARR_v2_117@1,,"Overall, LaBSE correlates more strongly than Sentence-BERT with our annotated data.",,"Add,Fact/Evidence",Fact/Evidence
128,111-ARR,111-ARR_v2_121@2,,"In As we increase the threshold L, we see this percentage substantially reduces, indicating our chosen thresholds are within the range of variation in LaBSE scores for semantically similar sentences.",,"Add,Fact/Evidence",Fact/Evidence
129,111-ARR,111-ARR_v2_125@1,,"20 A potential tool for fluency evaluation in future work is LAMBRE (Pratapa et al., 2021).",,"Add,Claim",Claim
130,111-ARR,111-ARR_v2_125@2,,"However, the original paper does not evaluate performance on Indic languages and the grammars for Indic languages would need to collected / built.",,"Add,Claim",Claim
131,111-ARR,111-ARR_v2_141@0,,"In Figure 9 we measure the lexical overlap between paraphrases used in our DIFFUR training strategy for six different languages (Hindi, Bengali, Kannada, Telugu, Swahili and Spanish).",,"Add,Fact/Evidence",Fact/Evidence
132,111-ARR,111-ARR_v2_141@1,,"The lexical overlap is measured using the unigram F1 score, using the implementation from the SQuAD evaluation script (Rajpurkar et al., 2016).",,"Add,Fact/Evidence",Fact/Evidence
133,111-ARR,111-ARR_v2_141@2,,The wide spread of the histogram and sufficient percentage of low overlap pairs confirm the lexical diversity of the paraphrases used.,,"Add,Fact/Evidence",Fact/Evidence
134,111-ARR,111-ARR_v2_141@3,,"As shown in prior work (Krishna et al., 2020), high lexical diversity of paraphrases is helpful for changing the input style.",,"Add,Fact/Evidence",Fact/Evidence
152,111-ARR,111-ARR_v2_43@4,,In Appendix K we confirm that our backtranslated paraphrases are lexically diverse from the input.,,"Add,Fact/Evidence",Fact/Evidence
153,111-ARR,111-ARR_v2_49@0,,"3. The length of s diff acts as a proxy for the amount of style transfer, which is controlled using λ during inference (Section 3).",,"Add,Fact/Evidence",Fact/Evidence
155,111-ARR,111-ARR_v2_54@3,,"We initialize the model with the UR-INDIC checkpoint, and fine-tune it on these two losses together, giving each loss equal weight.",,"Add,Fact/Evidence",Fact/Evidence
156,111-ARR,111-ARR_v2_69@1,,"Since each of our individual metrics can only take values 0 or 1 at an instance level, our aggregation acts like a Boolean AND operation.",,"Add,Fact/Evidence",Fact/Evidence
157,111-ARR,111-ARR_v2_77@1,,"DIFFUR-MLT gives best overall performance (r-AGG / a-AGG), with a good trade-off between style accuracy (ACC), semantic similarity (SIM), langID score (LANG), and low input copy rates (COPY); metrics defined in Section 5, other language results in Appendix I.",,"Add,Fact/Evidence",Fact/Evidence
158,111-ARR,111-ARR_v2_79@0,,Experimental Setup,,"Add,Other",Other
159,111-ARR,111-ARR_v2_83@6,,"For code-mixing addition, we use Hindi/English code-mixed exemplars in Devanagari (shown in Appendix D).",,"Add,Fact/Evidence",Fact/Evidence
160,111-ARR,111-ARR_v2_84@0,,Main Results,,"Add,Other",Other
161,111-ARR,111-ARR_v2_85@0,,"Each proposed method improves over prior work, DIFFUR-MLT works best.",,"Add,Fact/Evidence",Fact/Evidence
162,111-ARR,111-ARR_v2_85@1,,"We present our On Gujarati, the DIFFUR-INDIC fails to get good performance (36.0 r-AGG) since it did not see Gujarati paraphrase data, but this performance is recovered using DIFFUR-MLT (75.0).",,"Add,Fact/Evidence",Fact/Evidence
163,111-ARR,111-ARR_v2_85@2,,In Table 4 we see human evaluations support our automatic evaluation for formality transfer.,,"Add,Fact/Evidence",Fact/Evidence
164,111-ARR,111-ARR_v2_85@3,,In Figure 4: Outputs and qualitative analysis of our best performing model for several attribute transfer tasks (λ is transfer magnitude).,,"Add,Fact/Evidence",Fact/Evidence
165,111-ARR,111-ARR_v2_85@4,,We notice lower quality qualitatively for ** marked styles; see Appendix J for more outputs.,,"Add,Fact/Evidence",Fact/Evidence
166,111-ARR,111-ARR_v2_86@0,,ACC scores.,,"Add,Other",Other
167,111-ARR,111-ARR_v2_87@1,,In Appendix I we show a breakdown by individual metrics for other languages and plot variations with λ.,,"Add,Fact/Evidence",Fact/Evidence
217,114-ARR,114-ARR_v2_67@2,,"For example, we may further subdivide a CD type TRANSFER_MONEY into finergrained ones like LOAN and PURCHASE.",,"Add,Claim",Claim
218,114-ARR,114-ARR_v2_67@3,,"We provide linguistic/lexical insights by comparing the hierarchy levels of TD/CD types on WordNet (Miller, 1992).",,"Add,Fact/Evidence",Fact/Evidence
245,115-ARR,115-ARR_v2_75@4,,"2) We conduct significance tests of our CMGCN over the baseline models, the results show that our CMGCN significantly outperforms the baseline models in terms of most of the evaluation metrics (with p−value < 0.05).",,"Add,Fact/Evidence",Fact/Evidence
246,115-ARR,115-ARR_v2_88@0,,"As described in Section 3.3, the weights of edges in the cross-modal graph are computed based on both word similarities and affective clues between textual words and the attribute-object pairs of the image regions, and the dependency tree of the textmodality.",,"Add,Fact/Evidence",Fact/Evidence
247,115-ARR,115-ARR_v2_88@1,,The approach can be easily generalized to other sentiment-related multi-modal learning scenarios.,,"Add,Claim",Claim
248,115-ARR,115-ARR_v2_88@2,,"Nevertheless, the cross-graph solution might not be generalized well to other multi-modal tasks or data genres, if there is a lack of affective knowledge or a difficulty in deriving dependency trees in low-resource settings.",,"Add,Claim",Claim
249,115-ARR,115-ARR_v2_88@3,,"Therefore, future research can consider exploiting alternatively approaches to automatically learn the weights of edges in the cross-modal graph without relying on external knowledge sources.",,"Add,Claim",Claim
273,118-ARR,118-ARR_v2_8@3,,Our codes 1 are publicly available.,,"Add,Fact/Evidence",Fact/Evidence
274,118-ARR,118-ARR_v2_64@2,,The resulting Mutual Information based Named Entity Recognition model is visualized in Figure 1.,,"Add,Fact/Evidence",Fact/Evidence
275,118-ARR,118-ARR_v2_85@3,,The results are obtained by testing MINER (Bert large) on TwitterNER .,,"Add,Fact/Evidence",Fact/Evidence
276,118-ARR,118-ARR_v2_85@4,,"We fix β = 1e03, and the orange line is f1 score when γ = 0.",,"Add,Fact/Evidence",Fact/Evidence
277,118-ARR,118-ARR_v2_85@5,,The results are obtained by testing MINER (Bert large) on TwitterNER .,,"Add,Fact/Evidence",Fact/Evidence
278,118-ARR,118-ARR_v2_85@6,,"We fix γ = 1e04, and the orange line is f1 score when β = 0.",,"Add,Fact/Evidence",Fact/Evidence
380,124-ARR,124-ARR_v2_2@7,,"† Chenhui, Liying, and Ran are under the Joint PhD Program between Alibaba and their corresponding universities.",,"Add,Fact/Evidence",Fact/Evidence
381,124-ARR,124-ARR_v2_3@1,,1 Our code and data are released at https://github.,,"Add,Fact/Evidence",Fact/Evidence
382,124-ARR,124-ARR_v2_6@1,,"The more-to-less text generation tasks output a concise piece of text from some more abundant input, such as text summarization (Tan et al., 2017;Kryściński et al., 2018).",,"Add,Fact/Evidence",Fact/Evidence
383,124-ARR,124-ARR_v2_11@0,,(2) We propose a new task of controllable generation focusing on controlling the passage macro structures.,,"Add,Fact/Evidence",Fact/Evidence
384,124-ARR,124-ARR_v2_11@1,,It offers stronger generation flexibility and applicability for practical use cases.,,"Add,Claim",Claim
385,124-ARR,124-ARR_v2_16@3,,Table 2 shows the statistics of data collected from each year.,,"Add,Fact/Evidence",Fact/Evidence
386,124-ARR,124-ARR_v2_16@4,,"Initially, 7,894 submissions are collected.",,"Add,Fact/Evidence",Fact/Evidence
387,124-ARR,124-ARR_v2_16@5,,"After filtering, 7,089 meta-reviews are retained with their corresponding 23,675 reviews.",,"Add,Fact/Evidence",Fact/Evidence
389,124-ARR,124-ARR_v2_41@2,,"Due to long inputs (see Table 17), we experiment with different source truncation lengths of 1024, 2048, and 3072 tokens.",,"Add,Fact/Evidence",Fact/Evidence
390,124-ARR,124-ARR_v2_41@3,,We cannot explore truncation length of more than 3072 tokens due to the limitation of GPU space.,,"Add,Fact/Evidence",Fact/Evidence
391,124-ARR,124-ARR_v2_41@4,,"Our learning rate is 5e-5, and we use Adam optimizer with momentum β 1 = 0.9, β 2 = 0.999 without any warm-up steps or weight decay.",,"Add,Fact/Evidence",Fact/Evidence
392,124-ARR,124-ARR_v2_41@5,,"We set the seed to be 0, and train the model for 3 epochs with gradient accumulation step of 1.",,"Add,Fact/Evidence",Fact/Evidence
393,124-ARR,124-ARR_v2_41@6,,"For decoding, we use a beam size of 4 and length penalty of 2.",,"Add,Fact/Evidence",Fact/Evidence
394,124-ARR,124-ARR_v2_45@0,,Review Combination Results,,"Add,Other",Other
395,124-ARR,124-ARR_v2_46@0,,"We also show uncontrolled generation results for different review combination methods in Table 6, with source truncation of 2048.",,"Add,Fact/Evidence",Fact/Evidence
396,124-ARR,124-ARR_v2_46@2,,"Rateconcat has the best overall performance, which is the setting we used for the main results.",,"Add,Fact/Evidence",Fact/Evidence
397,124-ARR,124-ARR_v2_46@3,,"Never- theless, it is not significantly better than merge.",,"Add,Fact/Evidence",Fact/Evidence
398,124-ARR,124-ARR_v2_46@4,,"It is also interesting to see that for merge, providing additional rating information (rate-merge) slightly worsens the performance.",,"Add,Fact/Evidence",Fact/Evidence
399,124-ARR,124-ARR_v2_46@5,,We will leave the investigation of better review combination methods for future work.,,"Add,Claim",Claim
400,124-ARR,124-ARR_v2_61@1,,"For each test instance, we provide the judges with the input reviews and randomly ordered generations from different models, and ask them to individually evaluate the generations based on the following criteria: (1) Fluency: is the generation fluent, grammatical, and without unnecessary repetitions? (2) Content Relevance: does the generation reflect the review content well, or does it produce general but trivial sentences? (3) Structure Similarity: how close does the generation structure resemble the gold structure (i.e., the control sequence)? ( 4) Decision Correctness: does the generation correctly predicts the gold human decision?",,"Add,Fact/Evidence",Fact/Evidence
401,124-ARR,124-ARR_v2_66@2,,Our work is the first fully-annotated dataset in this domain for the structure-controllable generation task.,,"Add,Claim",Claim
402,124-ARR,124-ARR_v2_70@0,,Ethical Concerns,,"Add,Other",Other
403,124-ARR,124-ARR_v2_71@0,,We have obtained approval from ICLR organizers to use the data collected from ICLR 2018-2021 on OpenReview.,,"Add,Fact/Evidence",Fact/Evidence
404,124-ARR,124-ARR_v2_72@1,,"Note that due to limited GPU space, we cannot fit 2048 input tokens for T5.",,"Add,Fact/Evidence",Fact/Evidence
405,124-ARR,124-ARR_v2_72@2,,"Thus, for fair comparison, all results shown are from source truncation of 1024.",,"Add,Fact/Evidence",Fact/Evidence
406,124-ARR,124-ARR_v2_73@1,,"Specifically, we define the task as a sequence labeling problem and apply the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) networks with a conditional random field (CRF) (Lafferty et al., 2001) (i.e., LSTM-CRF (Lample et al., 2016)) model on the annotated MReD dataset.",,"Add,Fact/Evidence",Fact/Evidence
407,124-ARR,124-ARR_v2_74@0,,The same data split as the meta-review generation task is used.,,"Add,Fact/Evidence",Fact/Evidence
408,124-ARR,124-ARR_v2_74@1,,"We adopt the standard IOBES tagging scheme (Ramshaw, 1995;Ratinov and Roth, 2009), and fine-tune BERT (Devlin et al., 2019) and RoBERTa models in Hugging Face.",,"Add,Fact/Evidence",Fact/Evidence
409,124-ARR,124-ARR_v2_74@2,,"All models are trained for 30 epochs with an early stop of 20, and each epoch takes about 30 minutes.",,"Add,Fact/Evidence",Fact/Evidence
410,124-ARR,124-ARR_v2_74@3,,We select the best model parameters based on the best micro F 1 score on the development set and apply it to the test set for evaluation.,,"Add,Fact/Evidence",Fact/Evidence
411,124-ARR,124-ARR_v2_75@0,,All models are run with single V100 GPUs.,,"Add,Fact/Evidence",Fact/Evidence
412,124-ARR,124-ARR_v2_75@1,,"We use Adam (Kingma and Ba, 2014) with an initial learning rate of 2e-5.",,"Add,Fact/Evidence",Fact/Evidence
413,124-ARR,124-ARR_v2_75@2,,We report the F 1 scores for each category as well as the overall micro F 1 and macro F 1 scores in Table 14.,,"Add,Fact/Evidence",Fact/Evidence
414,124-ARR,124-ARR_v2_79@2,,"Nevertheless, the pattern is less evident in the source (reviews) baselines.",,"Add,Claim",Claim
479,125-ARR,125-ARR_v2_4@3,,"Fincke et al., 2021) usually divides EE into two subtasks: (1) event detection, which identifies event triggers and their types, and (2) event argument extraction, which extracts the arguments and their roles for given event triggers.",,"Add,Fact/Evidence",Fact/Evidence
480,125-ARR,125-ARR_v2_4@4,,"EE has been shown to benefit a wide range of applications, e.g., building knowledge graphs , question answering (Berant et al., 2014;, and other downstream studies (Han et al., 2019a;Hogenboom et al., 2016;.",,"Add,Fact/Evidence",Fact/Evidence
481,125-ARR,125-ARR_v2_12@9,,"For example, in our experiments, we take the information from the annotation guideline, which is provided along with the dataset.",,"Add,Fact/Evidence",Fact/Evidence
482,125-ARR,125-ARR_v2_16@0,,Our code and models can be found at https: //github.com/PlusLabNLP/DEGREE.,,"Add,Fact/Evidence",Fact/Evidence
483,125-ARR,125-ARR_v2_23@0,,We list three EAE templates in Table 1.,,"Add,Fact/Evidence",Fact/Evidence
484,125-ARR,125-ARR_v2_32@0,,Efficiency Considerations.,,"Add,Other",Other
485,125-ARR,125-ARR_v2_32@1,,"DEGREE requires to enumerate all event types during inference, which could cause efficiency considerations when extending to applications that contain many event types.",,"Add,Claim",Claim
486,125-ARR,125-ARR_v2_32@2,,"This issue is minor for our experiments on the two datasets (ACE 2005 and ERE-EN), which are relatively small scales in terms of the number of event types.",,"Add,Claim",Claim
487,125-ARR,125-ARR_v2_32@4,,We leave the work on benchmarking and improving the efficiency of DEGREE in the scenario considering more diverse and comprehensive types of events as future work.,,"Add,Claim",Claim
488,125-ARR,125-ARR_v2_46@1,,"We consider the following classification-based models: (1) OneIE , the current state-of-the-art (SOTA) EE model trained with designed global features.",,"Add,Fact/Evidence",Fact/Evidence
489,125-ARR,125-ARR_v2_46@2,,"( 2) BERT_QA (Du and Cardie, 2020), which views EE tasks as a sequence of extractive question answering problems.",,"Add,Fact/Evidence",Fact/Evidence
490,125-ARR,125-ARR_v2_46@3,,"Since it learns a classifier to indicate the position of the predicted span, we view it as a classification model.",,"Add,Fact/Evidence",Fact/Evidence
491,125-ARR,125-ARR_v2_46@4,,"We also consider the following generation-based models: (3) TANL (Paolini et al., 2021), which treats EE tasks as translation tasks between augmented natural languages.",,"Add,Fact/Evidence",Fact/Evidence
492,125-ARR,125-ARR_v2_52@0,,"Finally, we perform additional experiments on few-shot and zero-shot experiments.",,"Add,Fact/Evidence",Fact/Evidence
493,125-ARR,125-ARR_v2_52@1,,The results can be found in Appendix E.,,"Add,Fact/Evidence",Fact/Evidence
494,125-ARR,125-ARR_v2_65@4,,"The output sequence designs of TANL and Temp-Gen hinder the models from fully leveraging label semantics, unlike DEGREE that generates natural sentences.",,"Add,Claim",Claim
495,125-ARR,125-ARR_v2_70@1,,This assumption may holds for most situations.,,"Add,Claim",Claim
496,125-ARR,125-ARR_v2_70@2,,"We leave the automation of template construction for future work, which can further ease the needed efforts when deploying DEGREE in a large-scale corpus.",,"Add,Claim",Claim
497,125-ARR,125-ARR_v2_94@0,,"In order to further test our models' generaliability, we additionally conduct zero-shot and fewshot experiments on the ACE05-E dataset with DEGREE(ED) and DEGREE(EAE).",,"Add,Fact/Evidence",Fact/Evidence
498,125-ARR,125-ARR_v2_95@0,,Settings.,,"Add,Other",Other
499,125-ARR,125-ARR_v2_95@1,,"We first select the top n common event types as ""seen"" types and use the rest as ""unseen/rare"" types, where the top common types are listed in Table 12.",,"Add,Fact/Evidence",Fact/Evidence
500,125-ARR,125-ARR_v2_95@2,,"To simulate a zero-shot scenario, we remove all events with ""unseen/rare"" types from the training data.",,"Add,Fact/Evidence",Fact/Evidence
501,125-ARR,125-ARR_v2_95@3,,"To simulate a few-shot scenario, we keep only k event examples for each ""unseen/rare"" type (denoted as k-shot).",,"Add,Fact/Evidence",Fact/Evidence
502,125-ARR,125-ARR_v2_95@4,,"During the evaluation, we calculate micro F1-scores only for these ""unseen/rare"" types.",,"Add,Fact/Evidence",Fact/Evidence
652,13-ARR,13-ARR_v2_25@5,,"For the vision stream, since the trajectory is represented as a sequence of panoramic image regions, which is different from VLMs pretrained on image-caption pairs, we also update the visual embedding during prompt tuning.",,"Add,Fact/Evidence",Fact/Evidence
653,13-ARR,13-ARR_v2_25@6,,The visual embedding contains image embedding and location embedding.,,"Add,Fact/Evidence",Fact/Evidence
654,13-ARR,13-ARR_v2_33@1,,"Rec indicates using Recurrent VLN-Bert (Hong et al., 2021) with different backbones or parameter initialization.",,"Add,Fact/Evidence",Fact/Evidence
655,13-ARR,13-ARR_v2_38@2,,"Our model outperforms the model fine-tuned on R2R dataset by 1.1% in unseen split, indicating that ProbES improves the generalization ability of the navigation model.",,"Add,Claim",Claim
656,13-ARR,13-ARR_v2_38@4,,"Table 6 introduces comprehensive ablation experiments showing the impact of key steps in the strategy of generating instructions, and the experiments are performed in the baseline model: IL+RL from En-vDrop .",,"Add,Fact/Evidence",Fact/Evidence
657,13-ARR,13-ARR_v2_38@5,,Class indicates classes we use to feed into CLIP.,,"Add,Fact/Evidence",Fact/Evidence
658,13-ARR,13-ARR_v2_38@6,,M and P/O represent classes from Matterport and Place365/Objects365 datasets respectively.,,"Add,Fact/Evidence",Fact/Evidence
659,13-ARR,13-ARR_v2_38@7,,G T emplate denotes the strategy used to generate templates. 'ours' denote the strategy shown in Sec 3.2.,,"Add,Fact/Evidence",Fact/Evidence
660,13-ARR,13-ARR_v2_38@8,,"For S T emplate , 'random' and 'match' indicate sampling a template randomly and choosing a template with the same number of masks as the number of viewpoints.",,"Add,Fact/Evidence",Fact/Evidence
674,133-ARR,133-ARR_v2_22@0,,Models for Text Categorization,,"Add,Other",Other
675,133-ARR,133-ARR_v2_23@0,,"We formally introduce the three families of models for text categorization, namely the BoW-based, graph-based, and sequence-based models.",,"Add,Fact/Evidence",Fact/Evidence
676,133-ARR,133-ARR_v2_23@1,,"Table 1 summarizes the key properties of the approaches: whether they require a synthetic graph, whether word position is reflected in the model, whether the model can deal with arbitrary length text, and whether the model is capable of inductive learning.",,"Add,Fact/Evidence",Fact/Evidence
677,133-ARR,133-ARR_v2_24@0,,BoW-Based Text Categorization,,"Add,Other",Other
678,133-ARR,133-ARR_v2_25@0,,"Under pure BoW-based text categorization, we denote approaches that are not order-aware and operate only on the multiset of words from the input document.",,"Add,Fact/Evidence",Fact/Evidence
679,133-ARR,133-ARR_v2_72@0,,The focus of this work is text classification.,,"Add,Fact/Evidence",Fact/Evidence
680,133-ARR,133-ARR_v2_72@1,,Potential risks that apply to text classification in general also apply to this work.,,"Add,Claim",Claim
681,133-ARR,133-ARR_v2_72@2,,"Nonetheless, we present alternatives to commonly used pretrained language models, which suffer from various sources of bias due to the large and poorly manageable data used for pretraining (Bender et al., 2021).",,"Add,Fact/Evidence",Fact/Evidence
682,133-ARR,133-ARR_v2_72@3,,"In contrast, the presented alternatives render full control over the training data and, thus, contribute to circumvent the biases otherwise introduced during pretraining.",,"Add,Claim",Claim
683,133-ARR,133-ARR_v2_76@3,,"We further motivate the choice of using wide layers with results from multi-label text classification (Galke et al., 2017), which has shown that a (wide) MLP outperforms all tested classical baselines such as SVMs, k-Nearest Neighbors, and logistic regression.",,"Add,Fact/Evidence",Fact/Evidence
684,133-ARR,133-ARR_v2_76@4,,"Follow-up work (Mai et al., 2018) then found that also CNN and LSTM do not substantially improve over the wide MLP.",,"Add,Fact/Evidence",Fact/Evidence
893,14-ARR,14-ARR_v2_25@0,,"• Grammatical error (Gram): Erroneous usage of past/current tense and mistakes in misplaced modifiers. • Event mismatch (Event): Stories that are offtopic, which present events that are not relevant to the image stream. • Object mismatch (Obj): Irrelevant nouns that do not appear in the images and are not semantically related.",,"Add,Fact/Evidence",Fact/Evidence
894,14-ARR,14-ARR_v2_69@5,,"• Stretch-VST (Hsu et al., 2021b): a modification of KGStory that produces more sentences in the story while maintaining quality.",,"Add,Fact/Evidence",Fact/Evidence
895,14-ARR,14-ARR_v2_69@6,,Appropriate knowledge added to the story results in a more detailed story.,,"Add,Claim",Claim
942,141-ARR,141-ARR_v2_7@0,,"In this example, the underlined entities are used to infer the correct answer, i.e., ""country(Moonhole, Saint Vincent and the Grenadines)"", but are not explicitly annotated for relational reasoning.",,"Add,Fact/Evidence",Fact/Evidence
943,141-ARR,141-ARR_v2_26@3,,"Here r denotes a predicate, i.e., a relation between X 0 and X l+1 .",,"Add,Fact/Evidence",Fact/Evidence
944,141-ARR,141-ARR_v2_84@0,,This completes the proof.,,"Add,Claim",Claim
994,15-ARR,15-ARR_v2_4@5,,Categories that tend to conflict are placed on opposite sites.,,"Add,Fact/Evidence",Fact/Evidence
995,15-ARR,15-ARR_v2_4@6,,"Illustration adapted from (Schwartz et al., 2012).",,"Add,Fact/Evidence",Fact/Evidence
996,15-ARR,15-ARR_v2_13@1,,Our consolidated value taxonomy (Section 3) is thus based on these schemes.,,"Add,Fact/Evidence",Fact/Evidence
997,15-ARR,15-ARR_v2_15@1,,We give an overview for completeness.,,"Add,Fact/Evidence",Fact/Evidence
998,15-ARR,15-ARR_v2_21@5,,"Formally, values are connected specifically with the argument's premise.",,"Add,Claim",Claim
999,15-ARR,15-ARR_v2_21@6,,"However, automatic models might still improve when incorporating the textual conclusion as context for the textual premise.",,"Add,Claim",Claim
1000,15-ARR,15-ARR_v2_23@0,,The taxonomy levels are chosen based on usefulness in social science research.,,"Add,Fact/Evidence",Fact/Evidence
1001,15-ARR,15-ARR_v2_23@1,,"The values at Level 1 are intended to be the items in surveys (Schwartz, 1994), which is why we also suggest to use them for dataset annotation.",,"Add,Fact/Evidence",Fact/Evidence
1002,15-ARR,15-ARR_v2_23@2,,"Moreover, Level 1 values can still be classified into being either instrumental or terminal.",,"Add,Claim",Claim
1003,15-ARR,15-ARR_v2_23@3,,"One could, however, create arbitrarily coarse-and fine-grained levels.",,"Add,Claim",Claim
1004,15-ARR,15-ARR_v2_23@5,,"The grouping of values at higher levels allows for classifications at coarser levels of granularity, enabling investigations such as, whether a specific set of arguments focus on persons or society mainly, or whether they imply a rather anxietyfree or a rather anxiety-avoiding background (cf. Figure 1).",,"Add,Claim",Claim
1005,15-ARR,15-ARR_v2_23@6,,"Also, the circular organization of the taxonomy enables the analysis of major ""directions"" in a collection of arguments, which can, for example, be used to study value differences in argumentation datasets of different cultures.",,"Add,Claim",Claim
1006,15-ARR,15-ARR_v2_23@8,,These links allow comparing value distributions identified in regional datasets with survey data.,,"Add,Fact/Evidence",Fact/Evidence
1007,15-ARR,15-ARR_v2_27@4,,"Note that this data is not intended to represent the respective culture, but to train and benchmark classifiers across sources.",,"Add,Fact/Evidence",Fact/Evidence
1008,15-ARR,15-ARR_v2_50@11,,"We found most disagreement arose from the complexity of annotating 54 values at once, with annotators sometimes confusing values despite the descriptions.",,"Add,Fact/Evidence",Fact/Evidence
1009,15-ARR,15-ARR_v2_50@12,,"For follow-up datasets, one could likely reduce such problems by training annotators on the arguments of our dataset with highest disagreement.",,"Add,Claim",Claim
1010,15-ARR,15-ARR_v2_54@4,,By definition this baseline achieves at least as high-and in most cases higher-F 1 -scores than label-wise random guessing according to the label frequency.,,"Add,Fact/Evidence",Fact/Evidence
1011,15-ARR,15-ARR_v2_56@2,,The conclusions were selected so that the different sets contain roughly the specified percentage of arguments.,,"Add,Fact/Evidence",Fact/Evidence
1012,15-ARR,15-ARR_v2_56@3,,"Unfortunately, this process led to different value distributions in the different sets.",,"Add,Fact/Evidence",Fact/Evidence
1013,15-ARR,15-ARR_v2_56@4,,"However, we deemed the conclusion-wise split more important for our experiments, as we want to test whether classifiers generalize to unseen conclusions.",,"Add,Fact/Evidence",Fact/Evidence
1014,15-ARR,15-ARR_v2_57@4,,"The comparably bad performance at higher levels is somewhat surprising, as it indicates that the categories at these higher levels are harder to separate by state-of-the-art language-based approaches.",,"Add,Claim",Claim
1015,15-ARR,15-ARR_v2_57@5,,"Maybe hierarchical classification approaches (e.g., Babbar et al., 2013) can address this comparably weak performance by utilizing signals at each level of the hierarchy simultaneously.",,"Add,Claim",Claim
1016,15-ARR,15-ARR_v2_58@4,,"Moreover, Figure 3 indicates some correlation of value frequency (grey bars) with classifier performance (colored lines).",,"Add,Fact/Evidence",Fact/Evidence
1017,15-ARR,15-ARR_v2_58@5,,One reason for this correlation could be that the dataset is too small for training reliable classifiers on the infrequent values.,,"Add,Claim",Claim
1018,15-ARR,15-ARR_v2_58@6,,"Another reason might be that there is a more developed vocabulary concerning frequent values, making it easier for classifiers to identify these values.",,"Add,Claim",Claim
1019,15-ARR,15-ARR_v2_68@2,,"Clearly expressing values behind arguments could avoid misunderstandings between humans and automated argumentation systems (Kiesel et al., 2021).",,"Add,Fact/Evidence",Fact/Evidence
1020,15-ARR,15-ARR_v2_69@1,,"Combined with Internet archive data, one could even analyse references to values over time.",,"Add,Claim",Claim
1021,15-ARR,15-ARR_v2_69@2,,We thus hope that this work can serve as a first step towards a better understanding of how the public sees and saw human values in everyday (digital) life.,,"Add,Claim",Claim
1076,151-ARR,151-ARR_v2_42@3,,We prevent specified interactions in SRC+REF training via modifying the attention mask with regional properties.,,"Add,Fact/Evidence",Fact/Evidence
1077,151-ARR,151-ARR_v2_42@4,,"We show the hard (left) and soft design (right, no h → s) in this figure.",,"Add,Fact/Evidence",Fact/Evidence
1078,151-ARR,151-ARR_v2_56@1,,"Following the official report, the Pearson's correlation is used for evaluation.",,"Add,Fact/Evidence",Fact/Evidence
1079,151-ARR,151-ARR_v2_63@0,,High-resource Zero-shot Avg.,,"Add,Other",Other
1080,151-ARR,151-ARR_v2_72@0,,Ranking-based Data Labeling,,"Add,Other",Other
1081,151-ARR,151-ARR_v2_73@0,,"To verify the effectiveness of ranking-based labeling, we collect the results of models applying different pseudo labeling strategies.",,"Add,Fact/Evidence",Fact/Evidence
1082,151-ARR,151-ARR_v2_73@1,,"After deriving the original scores from the well-trained UniTE-MRA checkpoint, we use Z-score and proposed ranking-based normalization methods to label synthetic data.",,"Add,Fact/Evidence",Fact/Evidence
1083,151-ARR,151-ARR_v2_73@2,,"For both methods, we also apply an ensembling strategy to assign training examples with averaged scores deriving from 3 UniTE-MRA checkpoints.",,"Add,Fact/Evidence",Fact/Evidence
1084,151-ARR,151-ARR_v2_73@3,,"Results show that, Z-score normalization reveals a performance drop when applying score ensembling with multiple models.",,"Add,Fact/Evidence",Fact/Evidence
1085,151-ARR,151-ARR_v2_73@4,,"Our proposed ranking-based normalization can boost the UniTE-UP model training, and its ensembling approach can further improve the performance.",,"Add,Claim",Claim
1145,152-ARR,152-ARR_v2_78@2,,Our proposed methods increases corpus size by a slightly larger factor because sentences that contain rare entity types are resampled multiple times.,,"Add,Fact/Evidence",Fact/Evidence
1146,152-ARR,152-ARR_v2_78@3,,"Although increased training corpus size leads to increased training time, note that our methods are especially suitable for scenarios where the annotated corpus is small and hence the training time is still relatively short.",,"Add,Claim",Claim
1147,152-ARR,152-ARR_v2_78@4,,"Each NER model (Shallow, Bi-LSTM, BERT) is associated with a cluster of vectors sharing the same starting point in the space, which represents the performance on the original corpus.",,"Add,Fact/Evidence",Fact/Evidence
1148,152-ARR,152-ARR_v2_82@0,,Note that data augmentation and sentence-level resampling (and resampling methods in general) are complementary methods for improving NER model training.,,"Add,Claim",Claim
1149,152-ARR,152-ARR_v2_82@1,,"Data augmentation improves the semantic richness of training instances by expanding the coverage of training data in the input feature space, while sentence-level resampling refines the importance weighting of training instances by bridging the gap between the training objective and evaluation metrics.",,"Add,Claim",Claim
1150,152-ARR,152-ARR_v2_82@2,,"Therefore, they work in orthogonal directions.",,"Add,Claim",Claim
1151,152-ARR,152-ARR_v2_82@3,,This points to a promising direction for future work: to explore the two line of methods in combination rather than in competition.,,"Add,Claim",Claim
1189,156-ARR,156-ARR_v2_20@1,,The n-stream self-attention mechanism incorporates n extra self-attention predicting streams besides main stream to predict next n continuous future tokens respectively at each time step.,,"Add,Fact/Evidence",Fact/Evidence
1191,156-ARR,156-ARR_v2_68@2,,We obtain 215 million 1 training samples (42GB in total) for pre-training.,,"Add,Fact/Evidence",Fact/Evidence
1192,156-ARR,156-ARR_v2_69@0,,"To accelerate the training process and accommodate GPU memory limitations, we adopt two methods.",,"Add,Fact/Evidence",Fact/Evidence
1193,156-ARR,156-ARR_v2_69@1,,"First, we sort the samples according to the length of the context.",,"Add,Fact/Evidence",Fact/Evidence
1194,156-ARR,156-ARR_v2_69@2,,Samples with similar length (i.e. number of tokens in context) are assembled into a batch to minimize the amount of padding.,,"Add,Fact/Evidence",Fact/Evidence
1195,156-ARR,156-ARR_v2_69@3,,"Secondly, due to the uneven distribution of sample lengths, we divide the Reddit corpus into two sub-datasets: Reddit-Short and Reddit-Long according to the length of context and response.",,"Add,Fact/Evidence",Fact/Evidence
1196,156-ARR,156-ARR_v2_109@0,,This paper proposes a new pre-training framework for dialogue response generation called Di-alogVED.,,"Add,Fact/Evidence",Fact/Evidence
1197,156-ARR,156-ARR_v2_109@1,,"The latent variable is incorporated into the sequence-to-sequence framework based on Transformer, and obtains a robust and diverse response generation model through 4 training targets.",,"Add,Fact/Evidence",Fact/Evidence
1198,156-ARR,156-ARR_v2_109@3,,Extensive experiments prove the effectiveness of our model.,,"Add,Claim",Claim
1199,156-ARR,156-ARR_v2_109@4,,Additional human evaluation demonstrates the advantages of our proposed model.,,"Add,Claim",Claim
1208,157-ARR,157-ARR_v2_30@0,,We can map these columns to the aspects we get from the Aspect Detection Module and determine if a particular persona is interested in that paragraph or not.,,"Add,Fact/Evidence",Fact/Evidence
2729,25-ARR,25-ARR_v2_42@2,,Models are consistent with Table 3.,,"Add,Fact/Evidence",Fact/Evidence
2730,25-ARR,25-ARR_v2_48@0,,Evaluating with Prescribed Topics,,"Add,Other",Other
2731,25-ARR,25-ARR_v2_49@6,,Raw average scores for models in the Ice-breaker run are additionally provided in Table 11 in Appendix A.4.,,"Add,Fact/Evidence",Fact/Evidence
2732,25-ARR,25-ARR_v2_50@1,,It also uses the brevity penalty to penalize short outputs.,,"Add,Fact/Evidence",Fact/Evidence
2733,25-ARR,25-ARR_v2_51@1,,"It computes the precision and recall using longest common subsequence (LSC) instead of n-gram, and the F1 score of precision and recall is reported as the final score.",,"Add,Fact/Evidence",Fact/Evidence
2734,25-ARR,25-ARR_v2_52@1,,"It computes the unigram precision and recall, and have a different mechanism of choosing the brevity penalty.",,"Add,Fact/Evidence",Fact/Evidence
2735,25-ARR,25-ARR_v2_53@1,,The minimum of precision and recall is reported as the final GLEU score.,,"Add,Fact/Evidence",Fact/Evidence
2736,25-ARR,25-ARR_v2_56@0,,Reference-free Metrics,,"Add,Other",Other
2737,25-ARR,25-ARR_v2_57@0,,The following introduces two reference-free automatic metrics we employed: FED and USR.,,"Add,Fact/Evidence",Fact/Evidence
2738,25-ARR,25-ARR_v2_57@1,,Their scores are computed using the conversations collected in our experiment.,,"Add,Fact/Evidence",Fact/Evidence
2739,25-ARR,25-ARR_v2_59@1,,"It consists of three sub-metrics: USR-MLM is to evaluate the understandability and naturalness, USR-DR(c) and USR-DR(f) are to evaluate the interestingness and consistency.",,"Add,Fact/Evidence",Fact/Evidence
2740,25-ARR,25-ARR_v2_59@2,,The sub-metric scores then produce an overall score through a regression model.,,"Add,Fact/Evidence",Fact/Evidence
2741,25-ARR,25-ARR_v2_60@0,,Correlation between Automatic Metrics and Human Evaluation,,"Add,Other",Other
3458,334-ARR,334-ARR_v2_54@9,,We also performed quantitative analysis on the model output to better understand the performance.,,"Add,Fact/Evidence",Fact/Evidence
3459,334-ARR,334-ARR_v2_54@10,,"Our model outputs a probabilistic score in the range {0, 1}.",,"Add,Fact/Evidence",Fact/Evidence
3460,334-ARR,334-ARR_v2_55@0,,"A score closer to 0 indicates our model is confident that bail would be denied, while a score closer to 1 means bail granted.",,"Add,Fact/Evidence",Fact/Evidence
3461,334-ARR,334-ARR_v2_55@2,,"We observe the correct bail granted predictions are shifted towards 1, and the correct bail denied predictions are shifted towards 0.",,"Add,Fact/Evidence",Fact/Evidence
3462,334-ARR,334-ARR_v2_55@3,,"Additionally, the incorrect samples are concentrated near the middle (≈ 0.5), which shows that our model was able to identify these as borderline cases.",,"Add,Fact/Evidence",Fact/Evidence
5731,86-ARR,86-ARR_v2_73@1,,"The input of the model is ""[CLS] a couple standing on what looks like a peer or boardwalk [SEP] a couple hugging each other at the park"", of which the ground truth label is ""contradiction"".",,"Add,Fact/Evidence",Fact/Evidence
5732,86-ARR,86-ARR_v2_75@1,,"For the second example, our MPII and MPII with AFiRe removed still capture the entailment relation well, and explain that ""at the beach"" and ""at restaurant"" can not be done at the same time.",,"Add,Fact/Evidence",Fact/Evidence
5733,86-ARR,86-ARR_v2_75@2,,"As we can see, these explanations generated by our method are also fluent.",,"Add,Fact/Evidence",Fact/Evidence
5734,86-ARR,86-ARR_v2_76@4,,"Our MPII still explains well, but fails to explain properly with AFiRe removed, even if the explanation contains the correct answer, which reveals the importance of AFiRe for promotion of interpretation.",,"Add,Claim",Claim
5993,1-12,1-12_v2_16@3,,"Since the N-terminal hydrophobic domain of plasmepsin V is not cleaved <REF-12> , it is likely a transmembrane signal anchor.",,"Add,Claim",Claim
5994,1-12,1-12_v2_16@4,,Both TMHMM and TopPred predict it to insert into the ER membrane with the N-terminus in the lumen such that the subsequent soluble region containing the active site would be in the cytoplasm.,,"Add,Fact/Evidence",Fact/Evidence
6004,1-21,1-21_v2_13@3,,"Namely, whereas the bacterial populations differed with regards the side scatter parameter, forward scatter showed no change in its distribution.",,"Add,Fact/Evidence",Fact/Evidence
6005,1-21,1-21_v2_18@3,,An alternative explanation is that phage binding decreases resource uptake by bacterial cells.,,"Add,Claim",Claim
6006,1-21,1-21_v2_18@4,,"However, this seems unlikely in our experiment.",,"Add,Claim",Claim
6007,1-21,1-21_v2_18@5,,"Because bacteria were exposed to inactivated phages only, the total number of viral particles is predicted to stay constant (or possibly degrade) throughout the experiment.",,"Add,Claim",Claim
6008,1-21,1-21_v2_18@6,,"When bacteria divide, the number of phages bound to a daughter cell should be roughly half the number on the mother cell; thus, the number of bound phages per cell will decrease exponentially with cell divisions.",,"Add,Claim",Claim
6009,1-21,1-21_v2_18@7,,"Using the density of phages and bacteria employed in our experiment, we predict that there will be, on average, less than one phage individual per bacterial cell after 9 to 10 cell divisions, which based on the mean doubling time presented in Figure 1 , is reached in the first 48 hours of the experiment.",,"Add,Claim",Claim
6010,1-21,1-21_v2_18@8,,Our results can explain previous observations on phage-associated increases in population size in P. fluorescens <REF-32> .,,"Add,Fact/Evidence",Fact/Evidence
6011,1-21,1-21_v2_18@9,,"Specifically, we predict that a significant number of phage in the experiments of Gomez and Buckling <REF-32> did not kill their bacterial hosts before some of the latter were able to accelerate their cell cycle and produce daughter cells.",,"Add,Claim",Claim
6012,1-21,1-21_v2_18@14,,"This response is expected to result in smaller individual size, because energy allocated to growth is directed to reproduction when the stressor is present.",,"Add,Claim",Claim
6013,1-21,1-21_v2_28@9,,Observations of c. 50 cells using TEM (Zeis EM10) showed no bound phages after the centrifugation treatment.,,"Add,Fact/Evidence",Fact/Evidence
6014,1-21,1-21_v2_30@3,,"KB medium containing UV-inactivated phages was obtained through centrifugation of inactivated phage, which were further added into pure KB, so that the medium used in the treatments only differs from the control by the presence of phages.",,"Add,Fact/Evidence",Fact/Evidence
6015,1-21,1-21_v2_36@0,,Measures of OD will be affected by changes in particle size.,,"Add,Claim",Claim
6016,1-21,1-21_v2_36@1,,"At equal bacterial density, a population of smaller cells will yield a lower OD value, because fewer particles will block less of the incoming light.",,"Add,Claim",Claim
6017,1-21,1-21_v2_36@2,,"The practical conclusion is that whenever bacteria get smaller, we understimate their count, and thus their growth rate.",,"Add,Claim",Claim
6018,1-21,1-21_v2_36@3,,"Because this means that we are more conservative about the impact of phage exposure on growth rate (i.e., if there were any bias in our results, it would be an underestimation of the increase in growth rate), we did not correct for this effect.",,"Add,Claim",Claim
6044,1-23,1-23_v2_19@5,,"In the era of evidence based medicine, level I evidence, derived from either systematic reviews or randomized controlled trials (RCTs), to support the ban against FGC is not available.",,"Add,Claim",Claim
6045,1-23,1-23_v2_19@7,,"In fact, the design and implementation of a RCT to address the effects of FGC cannot be justified and seems to be unethical.",,"Add,Claim",Claim
6070,1-62,1-62_v2_2@2,,Various studies have reported alkaliphiles from different alkaline habitats other than Lonar Lake with alkaliphile specific amino acid residues in the F 1 F o ATP synthase a-subunit.,,"Add,Fact/Evidence",Fact/Evidence
6071,1-62,1-62_v2_4@14,,"Hence, the cytoplasmic pH needs to be maintained 1.5 to 2.3 pH units below the external environment, which generates an optimal condition for ATP synthesis.",,"Add,Claim",Claim
6072,1-62,1-62_v2_9@0,,Further polymerase chain reaction (PCR) and sequencing of 16S rRNA was carried out for identification of bacterium <REF-11> .,,"Add,Fact/Evidence",Fact/Evidence
6073,1-62,1-62_v2_11@4,,"The amplified PCR product of 1.7 kbp was visualized on 1% agarose gel and the results were documented by Bio-Rad gel documentation system with Quantity One software (Bio-Rad, USA).",,"Add,Fact/Evidence",Fact/Evidence
6074,1-62,1-62_v2_11@6,,"An ATP synthase comparative study was performed by multiple sequence alignment with other strains from different categories i.e. acidophiles, neutrophiles and alkaliphiles, as shown in Figure 1 .",,"Add,Fact/Evidence",Fact/Evidence
6075,1-62,1-62_v2_19@0,,Table 1 lists the bacterial species used in the comparative studies.,,"Add,Fact/Evidence",Fact/Evidence
6081,1-62,1-62_v2_26@1,,"In addition, exchange mutations at a E219-H and a H245-E showed similar ATP synthase activity in E. coli .",,"Add,Fact/Evidence",Fact/Evidence
6082,1-62,1-62_v2_26@2,,"Moreover, the aG218-K substitution effect was suppressed by a H245-G mutation in E. coli.",,"Add,Fact/Evidence",Fact/Evidence
6083,1-62,1-62_v2_26@4,,"In addition, equivalent amino acid residue studies were also reported from the facultative alkaliphile B. pseudofirmus OF4 a -subunit <REF-7> – <REF-9> .",,"Add,Fact/Evidence",Fact/Evidence
6156,10-25,10-25_v2_24@1,,"After the acclimation, juvenile silver pompano with mean body weight of 8.56 ± 0.18 g were randomly distributed into 15 similar 20-L plastic jars filled with 15-L water (10 fish per jar with three replication per treatment).",,"Add,Fact/Evidence",Fact/Evidence
6157,10-25,10-25_v2_29@3,,"Goblet cells, congestion and hemorrhage in the intestinal tissues among treatments was compared.",,"Add,Fact/Evidence",Fact/Evidence
6158,10-25,10-25_v2_55@5,,"Furthermore, this decrease might be due to the feeding habits of carnivorous fish such as silver pompano which is lower amylase activity than protease and lipase enzymes.",,"Add,Claim",Claim
6164,10-77,10-77_v2_14@2,,"At each place, a 50-minute sampling effort was made by two people in a single visit.",,"Add,Fact/Evidence",Fact/Evidence
6165,10-77,10-77_v2_20@1,,The google earth Pro 7.3.3.7786 polygon was used to determine the sampled area of each zone.,,"Add,Fact/Evidence",Fact/Evidence
6262,2-147,2-147_v2_11@4,,Approval for the use of clinical materials was obtained from the local Ethical Review Committee.,,"Add,Fact/Evidence",Fact/Evidence
6263,2-147,2-147_v2_15@7,,Curcumin was not used to pretreat the cells and explants prior to the addition of IL-1β.,,"Add,Fact/Evidence",Fact/Evidence
6264,2-147,2-147_v2_15@8,,Curcumin and IL-1β were added simultaneously to the cultures.,,"Add,Fact/Evidence",Fact/Evidence
6271,2-155,2-155_v2_9@2,,"Within each category of chicken purchased, we collected at least four samples of each brand.",,"Add,Fact/Evidence",Fact/Evidence
6272,2-155,2-155_v2_19@4,,"Over half of all strains collected exhibited resistance to one or more antibiotics: 55%, 58%, 60%, and 76% from conventional, RWA, organic, and kosher chicken samples, respectively.",,"Add,Fact/Evidence",Fact/Evidence
6273,2-155,2-155_v2_29@1,,"Based on a national survey conducted by the USDA of poultry and hog producers in the United States, use of antibiotics at sub-therapeutic levels for growth promotion is common <REF-35> , <REF-36> .",,"Add,Fact/Evidence",Fact/Evidence
6274,2-155,2-155_v2_29@2,,"One estimate places growth promotion in livestock production as the single largest sector in which antibiotics are used in the US, accounting for 70% of the total of 50 million pounds for the year 2008 <REF-37> .",,"Add,Fact/Evidence",Fact/Evidence
6275,2-155,2-155_v2_30@0,,"Our finding that brands within categories did not differ significantly in the extent of antibiotic resistant E. coli ( Table 1 ) could arise from the fact that individual brands of chicken obtain product from multiple farms whose production practices may differ, obscuring clear patterns associated with individual brands.",,"Add,Claim",Claim
6276,2-155,2-155_v2_30@1,,Our ability to detect an effect of brand might also be constrained by low statistical power.,,"Add,Claim",Claim
6282,2-173,2-173_v2_31@8,,The mechanism by which 1-ABT inhibits esterases is not known.,,"Add,Claim",Claim
6283,2-173,2-173_v2_63@2,,"Because we have not been able to evaluate this mechanism more thoroughly in primary lung cells, particularly from asthmatic subjects, the physiological and/or clinical relevance of the present study in steroid insensitive patients requires further investigation.",,"Add,Claim",Claim
6327,2-238,2-238_v2_32@6,,The assay results are reproducible in three independent experiments.,,"Add,Fact/Evidence",Fact/Evidence
6328,2-238,2-238_v2_39@0,,Statistical analysis,,"Add,Other",Other
6329,2-238,2-238_v2_40@0,,"All values are expressed as mean ± standard deviation and the graphs were generated using Graph-Pad Prism ® (Version 4) for Windows (GraphPad Software, San Diego, California, USA.",,"Add,Fact/Evidence",Fact/Evidence
6330,2-238,2-238_v2_40@1,,"Statistical analysis was performed by one-way analysis of variance (ANOVA), followed by Bonferroni multiple comparison test for all parameters.",,"Add,Fact/Evidence",Fact/Evidence
6331,2-238,2-238_v2_40@2,,Results were considered statistically significant at P < 0.05.,,"Add,Fact/Evidence",Fact/Evidence
6332,2-238,2-238_v2_68@0,,Pathogenic fungi are increasingly responsible for life threatening infections in the elderly and immunocompromised patients.,,"Add,Claim",Claim
6333,2-238,2-238_v2_68@1,,"While some species have intrinsic resistance to anti-fungals, others develop resistance during the course of treatment.",,"Add,Claim",Claim
6334,2-238,2-238_v2_68@2,,Increasing antifungal resistance and treatment failures in patients is becoming a challenge.,,"Add,Claim",Claim
6335,2-238,2-238_v2_69@0,,The Candida genome encodes at least 3 distinct classes of histone deacetylases in addition to sirtuins.,,"Add,Claim",Claim
6336,2-238,2-238_v2_69@1,,"There are 8 different histone deacetylases ( HOS1, HOS2, HOS3, HDA1, HDA2, HDA3, RPD3, RPD31 ) which all have distinct roles in the morphogenesis of C. albicans .",,"Add,Claim",Claim
6337,2-238,2-238_v2_75@1,,"The fact that, the recombinant Hos2 enzyme did not show any inhibition with the Class I inhibitor MS-275 led us to explore alternate substrates including tubulins, which are substrates for Class II histone deacetylases.",,"Add,Fact/Evidence",Fact/Evidence
6338,2-238,2-238_v2_75@4,,It has been shown that microtubules in the fungal hyphae drive nuclear dynamics and cell cycle progression to morphogenesis <REF-34> .,,"Add,Fact/Evidence",Fact/Evidence
6339,2-238,2-238_v2_75@5,,"In view of the fact that Hos2 seems to preferentially deacetylate tubulins, it would be interesting to see if Hos2 inhibitors would act as anti-fungals, either as a monotherapy or in synergy, with existing anti-tubulin agents such as benomyl, nocodazole etc.",,"Add,Claim",Claim
6340,2-238,2-238_v2_78@0,,Data availability,,"Add,Other",Other
6341,2-238,2-238_v2_79@0,,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2014 Karthikeyan G et al.,,"Add,Fact/Evidence",Fact/Evidence
6342,2-238,2-238_v2_80@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",,"Add,Fact/Evidence",Fact/Evidence
6372,2-259,2-259_v2_8@6,,"The immunological screening included antinuclear antibodies, anti-smooth muscles antibodies, anti-mitochondria antibodies, anti LKM antibodies, anti-hepatic cytosol antibodies, complement (C3, C4, CH50), rheumatoid factor, antineutrophil cytoplasmic antibody (ANCA), antiganglioside antibodies (GM1, GM2, GD1a, GD1b, GQ1b) and onconeuronal antibodies (Hu, Ri, Yo, PNMA2, CV2, Amphiphysine).",,"Add,Fact/Evidence",Fact/Evidence
6373,2-259,2-259_v2_8@8,,The prothrombin time stayed within the normal range throughout the monitoring period.,,"Add,Fact/Evidence",Fact/Evidence
6374,2-259,2-259_v2_10@3,,Ribavirin treatment was discontinued after 35 days.,,"Add,Fact/Evidence",Fact/Evidence
6375,2-259,2-259_v2_13@2,,The present case concerned genotype 3f virus that is predominant in France <REF-8> .,,"Add,Fact/Evidence",Fact/Evidence
6376,2-259,2-259_v2_13@4,,"Moreover, several authors suggest treating severe acute HEV infections in order to preclude the development of acute liver failure <REF-9> .",,"Add,Fact/Evidence",Fact/Evidence
6386,2-260,2-260_v2_3@0,,Introduction,,"Add,Other",Other
6387,2-260,2-260_v2_5@0,,"Ever since Jensen emphasized the role of promiscuity or ‘substrate ambiguity’ in evolution through ‘fortuitous error and gain of multistep pathways’, promiscuity in proteins has been the subject of intense and detailed research <REF-7> .",,"Add,Fact/Evidence",Fact/Evidence
6388,2-260,2-260_v2_5@1,,It was demonstrated in 1976 that replacing the zinc metal ion by copper in Carboxypeptidase A introduced oxidase catalysis properties <REF-11> .,,"Add,Fact/Evidence",Fact/Evidence
6389,2-260,2-260_v2_5@2,,"Dioxygenases promiscuously hydrolyse esters <REF-12> , while the enolase superfamily is also known to catalyze numerous catalytic reactions <REF-13> , <REF-14> .",,"Add,Fact/Evidence",Fact/Evidence
6390,2-260,2-260_v2_5@3,,"Alkaline phosphatases (AP), one of the key proteins in our research, are one of the most widely researched promiscuous enzymes <REF-15> .",,"Add,Fact/Evidence",Fact/Evidence
6391,2-260,2-260_v2_5@4,,"APs are known to have sulfate monoesterase, phosphate diesterase, and phosphonate monoesterase activities <REF-16> – <REF-18> .",,"Add,Fact/Evidence",Fact/Evidence
6392,2-260,2-260_v2_5@5,,"A phosphite-dependent hydrogenase activity was also found in Escherichia coli AP (ECAP), but was absent in APs from other organisms <REF-19> .",,"Add,Fact/Evidence",Fact/Evidence
6393,2-260,2-260_v2_5@6,,"Interestingly, proteins from the AP superfamily show cross activity - Pseudomonas aeruginosa arylsulfatase (PAS) which has the primary activity of hydrolyzing sulfate monoesters also catalyzes the hydrolysis of phosphate monoesters <REF-20> , <REF-21> .",,"Add,Fact/Evidence",Fact/Evidence
6394,2-260,2-260_v2_6@3,,"For example, the catalytic Ser-His-Asp triad has virtually the same geometry in the major families of serine proteases (chymotrypsin and subtilisin), which have no sequence or structural homology <REF-26> - a classical example of convergent evolution <REF-27> , <REF-28> .",,"Add,Fact/Evidence",Fact/Evidence
6395,2-260,2-260_v2_6@6,,The choice of methods for binding site comparisons and methods for binding site detection as well as function prediction has been recently reviewed in detail <REF-31> .,,"Add,Fact/Evidence",Fact/Evidence
6396,2-260,2-260_v2_6@7,,"Notably, most of these methods are based on structural properties of the binding or the active site.",,"Add,Fact/Evidence",Fact/Evidence
6397,2-260,2-260_v2_11@0,,"Another fascinating aspect of enzymes, although strictly not defined as promiscuity, is their ability to catalyze the reaction of a range of similar substrates of the same class <REF-51> .",,"Add,Fact/Evidence",Fact/Evidence
6398,2-260,2-260_v2_11@1,,"We have hypothesized that duplicate residues, each of which results in slightly modified replicas of the active site scaffold, are responsible for the broad substrate specificity of proteins <REF-52> , <REF-53> .",,"Add,Fact/Evidence",Fact/Evidence
6502,2-288,2-288_v2_59@6,,"“Inner” and “outer” loops provide two levels of nesting, when needed.",,"Add,Fact/Evidence",Fact/Evidence
6503,2-288,2-288_v2_79@6,,"Given that the model code has been verified for mathematical accuracy over the full range of parameter space used for the Monte Carlo evaluation, the result gives an informative measure of the degree of validity of the model.",,"Add,Fact/Evidence",Fact/Evidence
6504,2-288,2-288_v2_79@7,,"This is the first step of uncertainty quantification ( Smith, 2014 ).",,"Add,Fact/Evidence",Fact/Evidence
6505,2-288,2-288_v2_88@0,,Run-time performance,,"Add,Other",Other
6506,2-288,2-288_v2_89@0,,"Run-time performance of JSim is dependent on numerous factors: model complexity, mathematical formulation, numeric methods used, use of parallel processing, and the fineness of time and spatial grids used.",,"Add,Fact/Evidence",Fact/Evidence
6507,2-288,2-288_v2_89@1,,"PDEs generally run faster than ODEs providing similar spatial resolution even though they include diffusion terms, but in general can be slower than ODEs representing simplified geometries, and are slower using high resolution general purpose solvers like TOMS731.",,"Add,Claim",Claim
6508,2-288,2-288_v2_89@2,,A direct solver-to-solver comparison on a problem which can be formulated as either an ODE or PDE model (a convection-diffusion model) is provided in Table 4 .,,"Add,Fact/Evidence",Fact/Evidence
6509,2-288,2-288_v2_92@0,,"The times reported in Table 4 are extraordinarily variable, and depend upon parameter values and on the values of the variable themselves, as the solvers can become highly efficient if a variable is not changing.",,"Add,Claim",Claim
6510,2-288,2-288_v2_92@2,,"In general, linear systems of implicit equations run faster than non-linear systems.",,"Add,Claim",Claim
6511,2-288,2-288_v2_92@3,,"Runs requiring stiff ODE solvers (e.g. CVode, Radau) typically run slower than non-stiff solvers (e.g. Dopri5, RK4).",,"Add,Claim",Claim
6512,2-288,2-288_v2_92@4,,"Analyses involving JSim loops, sensitivity analysis, optimization and Monte Carlo methods run faster when JSim multiprocessing is activated.",,"Add,Claim",Claim
6513,2-288,2-288_v2_92@5,,Models requiring fine temporal or spatial grids to capture relevant detail run slower than those for which coarse grids are sufficient.,,"Add,Claim",Claim
6514,2-288,2-288_v2_93@0,,"Computation time equivalent to real time was shown on a laptop computer for a cardiorespiratory system model with about 120 variables ( Neal & Bassingthwaighte, 2007 ).",,"Add,Fact/Evidence",Fact/Evidence
6515,2-288,2-288_v2_93@1,,"Models available at physiome.org typically run somewhere between a fraction of a second to several minutes, depending upon these various complications.",,"Add,Fact/Evidence",Fact/Evidence
6516,2-288,2-288_v2_93@2,,Some example model simulation execution times (“run times”) are compared to the real time duration of the events being modeled (“model times”) in Table 5 .,,"Add,Fact/Evidence",Fact/Evidence
6517,2-288,2-288_v2_93@3,,All the runs below were single model runs performed on a mid-range workstation (Dell Precision T3500 Xeon x86-64 2.5GHz).,,"Add,Fact/Evidence",Fact/Evidence
6518,2-288,2-288_v2_93@4,,"The Beeler-Reuter action potential model (#78 at physiome.org) has only 4 ionic currents and 28 time-dependent variables; the ( Winslow et al. , 1999 ) model (#217) has 126 time-dependent variables.",,"Add,Fact/Evidence",Fact/Evidence
6519,2-288,2-288_v2_93@5,,"Timing calculations are unreliable, dependent on the model, computational methods, and the values of the variables, and the timestep length.",,"Add,Fact/Evidence",Fact/Evidence
6520,2-288,2-288_v2_93@6,,"For the Winslow et al. , 1999 model, using a 1 microsec timestep took 96 seconds, only 100 times slower, not 1000 times, compared to that with Δt = 1 ms.",,"Add,Fact/Evidence",Fact/Evidence
6521,2-288,2-288_v2_96@0,,Code verification,,"Add,Other",Other
6522,2-288,2-288_v2_97@0,,We use a variety of strategies to verify the JSim code stack.,,"Add,Fact/Evidence",Fact/Evidence
6523,2-288,2-288_v2_97@1,,Some calculations such as values for transcendental functions and algebraic expansion of symbolic derivatives have known closed form solutions that can be compared exactly.,,"Add,Claim",Claim
6524,2-288,2-288_v2_97@2,,Our general policy is to write the code for analytical solutions into the JSim model to use the comparison to verify specifiable cases.,,"Add,Fact/Evidence",Fact/Evidence
6525,2-288,2-288_v2_97@3,,"In simple cases, e.g. respiratory mechanics models, exponential equations match numerical solutions for 7 decimal points.",,"Add,Fact/Evidence",Fact/Evidence
6526,2-288,2-288_v2_97@4,,Some ODE and PDE models such as exponential washout have known closed form analytic solutions which can be numerically compared to solutions generated by JSim’s numeric integrators.,,"Add,Claim",Claim
6527,2-288,2-288_v2_97@5,,"Even when a complete analytic solution is not available, certain statistics of the solution, such as mean transit time in blood-tissue flow models, can be calculated analytically and compared with the same statistic calculated from the model output.",,"Add,Claim",Claim
6528,2-288,2-288_v2_98@0,,"Parameter changes to models causing output changes that don’t correspond to expectation from induction, need to be evaluated qualitatively by the user.",,"Add,Claim",Claim
6529,2-288,2-288_v2_98@1,,"When modelers observe unexpected behavior, checking at deeper levels is required.",,"Add,Claim",Claim
6530,2-288,2-288_v2_98@2,,"While most such anomalies are due to user coding errors, over the 15 years of JSim’s existence, some subtle computation bugs in JSim have been diagnosed in this manner.",,"Add,Fact/Evidence",Fact/Evidence
6531,2-288,2-288_v2_98@3,,JSim cannot be proven bug-free even though finding anomalies is now rare: queries regarding problems in computation are welcomed at staff@physiome.org .,,"Add,Fact/Evidence",Fact/Evidence
6532,2-288,2-288_v2_99@0,,JSim models are sometimes exported in SBML format and run in other SBML supporting simulators as a comparison check.,,"Add,Fact/Evidence",Fact/Evidence
6533,2-288,2-288_v2_99@1,,Models are also sometimes entirely recoded in a different computational environment (Matlab is often used) as a comparison check.,,"Add,Fact/Evidence",Fact/Evidence
6534,2-288,2-288_v2_99@2,,"Model translations to other languages (e.g. SBML, CellML, Antimony) are verified via round-tripping (exporting to the target language and then reimporting).",,"Add,Fact/Evidence",Fact/Evidence
6535,2-288,2-288_v2_100@0,,Tests of JSim’s optimization and Monte Carlo functionality are based on convergence to solutions of known parameterization.,,"Add,Fact/Evidence",Fact/Evidence
6536,2-288,2-288_v2_100@1,,The optimizers are all very different; we advocate that users try a variety of optimizers for any given problem.,,"Add,Claim",Claim
6537,2-288,2-288_v2_100@2,,"There is no magic in an optimizer; the key in fitting a model solution to data, when the data are reliable, is in designing a carefully weighted distance function to fit as many computed model variables to simultaneously obtained experimental data functions as possible.",,"Add,Claim",Claim
6538,2-288,2-288_v2_101@0,,Tests of JSim’s multi-processing are based on comparisons to single processor computations.,,"Add,Fact/Evidence",Fact/Evidence
6539,2-288,2-288_v2_102@0,,The JSim verification suite consists of over 1200 individual tests drawn from the above methodologies.,,"Add,Fact/Evidence",Fact/Evidence
6540,2-288,2-288_v2_102@1,,"The suite is expanded when new computational or translation facilities are added, or when a bug has been found and fixed.",,"Add,Fact/Evidence",Fact/Evidence
6541,2-288,2-288_v2_102@2,,Most tests consist of comparing jsbatch output with user-verified reference data.,,"Add,Fact/Evidence",Fact/Evidence
6542,2-288,2-288_v2_102@3,,The verification suite is run before every official JSim release to ensure consistency of operation.,,"Add,Fact/Evidence",Fact/Evidence
6543,2-288,2-288_v2_110@0,,Using archived models for analyzing one’s own data,,"Add,Other",Other
6544,2-288,2-288_v2_111@0,,All the archived models may be downloaded so that an experimentalist can import his own data into the project file and analyze it.,,"Add,Fact/Evidence",Fact/Evidence
6545,2-288,2-288_v2_111@1,,"The JSim Home Page is an operations manual for downloading, running models and analyzing data.",,"Add,Fact/Evidence",Fact/Evidence
6546,2-288,2-288_v2_111@2,,For parameter evaluation the number of trial optimizations can be set to 1 (so there is no optimization done) but the covariance matrix is calculated to provide estimates of confidence limits from the local linear combination of sensitivity functions.,,"Add,Fact/Evidence",Fact/Evidence
6547,2-288,2-288_v2_111@3,,"For greater generality one should set up the optimizer to evaluate the set of parameters desired, then run the Monte Carlo (tab at bottom) to repeat the optimization many times in the presence of added noise; this provides realistic probability density functions of parameter values.",,"Add,Fact/Evidence",Fact/Evidence
6548,2-288,2-288_v2_112@0,,Some Alternative Simulation Platforms,,"Add,Other",Other
6549,2-288,2-288_v2_113@0,,A comprehensive feature-by-feature analysis of alternative simulation platforms is beyond the scope of this paper.,,"Add,Fact/Evidence",Fact/Evidence
6550,2-288,2-288_v2_113@1,,"Listed below are brief descriptions of some simulation systems using procedural methods, as opposed to JSim declarative approach.",,"Add,Fact/Evidence",Fact/Evidence
6551,2-288,2-288_v2_113@2,,All can be used to fit model solutions to data.,,"Add,Fact/Evidence",Fact/Evidence
6552,2-288,2-288_v2_114@0,,"Virtual Cell ( Loew & Schaff, 2001 ) is a computational environment designed for the construction and simulation of cellular-based models.",,"Add,Fact/Evidence",Fact/Evidence
6553,2-288,2-288_v2_114@1,,"Models can be created iteratively in the GUI, or via VCell’s custom mathematical language VCMDL which supports ODEs & PDEs.",,"Add,Fact/Evidence",Fact/Evidence
6554,2-288,2-288_v2_114@2,,Both deterministic and stochastic simulations are supported.,,"Add,Fact/Evidence",Fact/Evidence
6555,2-288,2-288_v2_114@3,,Model computations are performed via client accounts on VCell’s computational server farm.,,"Add,Fact/Evidence",Fact/Evidence
6556,2-288,2-288_v2_115@0,,"COPASI ( Hoops et al. , 2006 ) (for COmplex PAthway SImulator) is an integrated modeling and simulation environment aimed at metabolic networks, cell-signaling pathways, regulatory networks, infectious diseases and similar systems.",,"Add,Fact/Evidence",Fact/Evidence
6557,2-288,2-288_v2_115@1,,Models are typically created via a table-driven GUI and results viewed via embedded graphs.,,"Add,Fact/Evidence",Fact/Evidence
6558,2-288,2-288_v2_115@2,,"COPASI supports SBML and currently runs on Linux, MacOS and Windows.",,"Add,Fact/Evidence",Fact/Evidence
6559,2-288,2-288_v2_117@0,,"Chaste ( Mirams et al. , 2013 ) is C++ library for computational physiology and biology.",,"Add,Fact/Evidence",Fact/Evidence
6560,2-288,2-288_v2_117@1,,"Computational modules include mesh generation, linear algebra, ODEs, PDEs and continuum mechanics.",,"Add,Fact/Evidence",Fact/Evidence
6561,2-288,2-288_v2_117@2,,"I/O modules provide support for various file formats, including HDF5 ( Folk et al. , 1999 ).",,"Add,Fact/Evidence",Fact/Evidence
6562,2-288,2-288_v2_117@3,,"Chaste is available for Windows, MacOS, Linux and Solaris.",,"Add,Fact/Evidence",Fact/Evidence
6563,2-288,2-288_v2_118@0,,"PCenv, COR, OpenCell and OpenCOR ( CellML, 2014 ) are a related set of tools supporting CellML modeling.",,"Add,Fact/Evidence",Fact/Evidence
6564,2-288,2-288_v2_118@1,,PCenv is an interactive modeling editing and simulation environment running on Windows.,,"Add,Fact/Evidence",Fact/Evidence
6565,2-288,2-288_v2_118@2,,COR an alternative CellML modeling environment for Windows.,,"Add,Fact/Evidence",Fact/Evidence
6566,2-288,2-288_v2_118@3,,"OpenCell is a merger of PCenv and COR built upon the Mozilla platform, and running on Linux, Windows and MacOS.",,"Add,Fact/Evidence",Fact/Evidence
6567,2-288,2-288_v2_118@4,,OpenCell development has been stopped in favor of its replacement OpenCOR.,,"Add,Fact/Evidence",Fact/Evidence
6568,2-288,2-288_v2_119@0,,"Continuity ( Continuity, 2014 ) is problem-solving environment for multi-scale modeling in bioengineering and physiology - especially biomechanics, transport and electrophysiology.",,"Add,Fact/Evidence",Fact/Evidence
6569,2-288,2-288_v2_119@1,,Finite element and PDEs are supported.,,"Add,Fact/Evidence",Fact/Evidence
6570,2-288,2-288_v2_119@2,,"The Continuity language integrates with Python ( VanRossum & Drake, 2003 ) scripts to create multi-scale models.",,"Add,Fact/Evidence",Fact/Evidence
6571,2-288,2-288_v2_119@3,,"Continuity runs on Windows, MacOS, Linux and Linux clusters.",,"Add,Fact/Evidence",Fact/Evidence
6572,2-288,2-288_v2_129@0,,"SED-ML Support: SED-ML ( Kohn & Le Novere, 2008 ) (for Simulation Experiment Description) is an emerging standard for to promote reproducibility by capturing all the details of an in silico experiment.",,"Add,Fact/Evidence",Fact/Evidence
6573,2-288,2-288_v2_129@1,,"Major entities described in SED-ML are models, simulation setup (i.e. time and numeric solver parameters), tasks (a model run with specified a simulation setup), data generators (methods for combining model outputs from different tasks) and outputs (plots & tables).",,"Add,Fact/Evidence",Fact/Evidence
6574,2-288,2-288_v2_129@2,,"JSim projects support these entities, but not in a scripted form.",,"Add,Fact/Evidence",Fact/Evidence
6575,2-288,2-288_v2_129@3,,"To support SED-ML, we are currently developing a feature called Second Level Analysis (SLA) that will allow JSim users to script common JSim activities (e.g. model runs with different parameter sets, data combinatorics, plotting and export), in a way that maps to SED-ML.",,"Add,Fact/Evidence",Fact/Evidence
6576,2-288,2-288_v2_129@4,,SED-ML files will be read into JSim as SLA scripts and run there.,,"Add,Fact/Evidence",Fact/Evidence
6577,2-288,2-288_v2_129@5,,"Conversely, JSim SLA scripts may be exported to SED-ML for use in other simulators that support SED-ML.",,"Add,Fact/Evidence",Fact/Evidence
6578,2-288,2-288_v2_132@0,,Data and software availability,,"Add,Other",Other
6579,2-288,2-288_v2_133@0,,"Zenodo: JSim downloads and models Version 2, doi: http://dx.doi.org/10.5281/zenodo.8652 ( Butterworth et al. , 2014 )",,"Add,Fact/Evidence",Fact/Evidence
6584,2-288,2-288_v2_7@1,,"The more realistic the model, the more accurate the prediction.",,"Add,Claim",Claim
6585,2-288,2-288_v2_31@10,,"In general, using Matlab without Simulink takes 6 to 20 times as long as JSim solutions.",,"Add,Fact/Evidence",Fact/Evidence
6586,2-288,2-288_v2_33@0,,Declarative languages such as MML describe the logic of a computer program rather than the explicit flow of control.,,"Add,Claim",Claim
6587,2-288,2-288_v2_33@1,,In traditional procedural languages such as C and Fortran the flow of control is explicit in the code.,,"Add,Claim",Claim
6588,2-288,2-288_v2_33@2,,Declarative languages have advantages and disadvantages relative to procedural languages.,,"Add,Claim",Claim
6589,2-288,2-288_v2_33@3,,"They allow for clear exposition of the intentions of computation, since only the equations (without the numerical details) are specified.",,"Add,Claim",Claim
6590,2-288,2-288_v2_33@4,,"Because they generally represent a top-down view of the mathematics, they allow automated handling of computational complexities such as parallel processing without distracting the user with complex details.",,"Add,Claim",Claim
6591,2-288,2-288_v2_33@5,,"On the other hand, it is difficult for a procedural translation of declarative code, as going from MML to compute using Java, to be as general and efficient as optimized procedural code.",,"Add,Claim",Claim
6592,2-288,2-288_v2_33@6,,Consequently MML is designed to permit use of procedural code when circumstances demand it.,,"Add,Fact/Evidence",Fact/Evidence
6625,2-29,2-29_v2_14@0,,Two reports dealt with large paraovarian cysts laparoscopically.,,"Add,Fact/Evidence",Fact/Evidence
6626,2-29,2-29_v2_14@1,,"The first was a simple paraovarian cyst associated with pregnancy which measured 20 cm while the second with acute lower abdominal pain measured 12 cm <REF-10> , <REF-11> .",,"Add,Fact/Evidence",Fact/Evidence
6627,2-29,2-29_v2_14@3,,They reported either cyst aspiration and forceps coagulation of small cysts or cyst wall trocar puncture followed by suction and extraction for large cysts.,,"Add,Fact/Evidence",Fact/Evidence
6628,2-29,2-29_v2_14@6,,It is the authors’ view that laparoscopic cyst decompression before its removal is associated with greater risk of cyst spillage than the technique we describe.,,"Add,Claim",Claim
6663,2-58,2-58_v2_4@5,,Most relevant to the current paper are the Delboeuf and Ebbinghaus illusions that demonstrate that the size of an inner circle is overestimated or underestimated depending on the surrounding context in which it is presented.,,"Add,Fact/Evidence",Fact/Evidence
6664,2-58,2-58_v2_4@6,,"Though several explanations have been proposed for these illusions, recent research demonstrates that the effect is largely determined by the relative size of the inducer(s), their distance from the target <REF-2> , and in the case of the Ebbinghaus Illusion, the completeness of the surrounding array of elements <REF-8> .",,"Add,Fact/Evidence",Fact/Evidence
6665,2-58,2-58_v2_4@7,,"Taken together, the balance of these factors determines the magnitude of the illusion and whether the inner circle is overestimated or underestimated.",,"Add,Fact/Evidence",Fact/Evidence
6666,2-58,2-58_v2_7@4,,"To our knowledge, previous research on the Ebbinghaus Illusion has focused on the effect the surrounding elements have on the explicitly defined circle.",,"Add,Claim",Claim
6667,2-58,2-58_v2_7@5,,Here we consider the possibility of mutual influence in that the inner circle may also lead to misperceived size of the surrounding array.,,"Add,Claim",Claim
6668,2-58,2-58_v2_45@2,,"Specifically, differential effects can be obtained resulting in changes to the magnitude of the illusions depending on spatial frequency filtering.",,"Add,Fact/Evidence",Fact/Evidence
6669,2-58,2-58_v2_68@4,,This stands in contrast to several recent findings using functional and structural MRI that have implicated visual areas as early as V1 as playing a key role in the representation of perceived size <REF-31> – <REF-33> .,,"Add,Fact/Evidence",Fact/Evidence
6670,2-58,2-58_v2_68@5,,"Given the classical receptive field properties of V1 neurons, it is likely that these observations arise due to feedback to V1 from higher visual areas, that in the case of the Binding Ring Illusion may contain integrated representations of spatial frequency.",,"Add,Claim",Claim
6671,2-58,2-58_v2_68@6,,This is in line with recent research on the Müller-Lyer Illusion using dynamic causal modelling <REF-34> .,,"Add,Fact/Evidence",Fact/Evidence
6672,2-58,2-58_v2_68@7,,It was demonstrated that illusion strength could be predicted by modulating bilateral connections between the lateral occipital cortex (LOC) and right superior parietal cortex (SPC).,,"Add,Fact/Evidence",Fact/Evidence
6673,2-58,2-58_v2_68@8,,The model suggests that LOC is involved in size scaling to generate size invariant object representations that are further processed in SPC and relayed back to V1 to generate conscious illusory percepts.,,"Add,Claim",Claim
6681,2-9,2-9_v2_52@1,,"However, in our model the local host adipocytes responded to the tumor implantation with the lipogenic rather than the erythrogenic autophagy.",,"Add,Fact/Evidence",Fact/Evidence
6682,2-9,2-9_v2_52@2,,The entire cells were converted into lipid droplets.,,"Add,Fact/Evidence",Fact/Evidence
6683,2-9,2-9_v2_52@3,,"If the tumor cells could be treated to re-direct their metabolism to lipogenesis instead of erythrogenesis, perhaps the metastatic potential of the capsular vaso-mimicry could be abolished and ultimately the entire tumor replaced with fat.",,"Add,Claim",Claim
6684,2-9,2-9_v2_52@4,,"A non-malignant type of undifferentiated cells, human mesenchymal stem cells, can accumulate lipid under hypoxia, although normally they would differentiate along several pathways to form bone, cartilage, tendon, muscle or adipose tissues.",,"Add,Claim",Claim
6685,2-9,2-9_v2_52@5,,In that case the potent lipogenic effect of hypoxia was independent of PPAR-γ2 maturation pathway <REF-80> .,,"Add,Fact/Evidence",Fact/Evidence
6687,2-9,2-9_v2_15@2,,The mice were on Tekand global 14% protein rodent diet (Harlan) with access to water ad libitum.,,"Add,Fact/Evidence",Fact/Evidence
6688,2-9,2-9_v2_24@1,,Analyzed tissue sections were first examined at low magnification and coordinates for each hexagonal sector of a grid covered with tissue were recorded automatically.,,"Add,Fact/Evidence",Fact/Evidence
6689,2-9,2-9_v2_24@2,,Subsequently all sectors were explored at least once at variable high magnifications.,,"Add,Fact/Evidence",Fact/Evidence
6690,2-9,2-9_v2_24@3,,"Interesting images were captured at the magnification best suited to document a particular phenomenon or identify a structure, including colloidal Au grains.",,"Add,Fact/Evidence",Fact/Evidence
6691,2-9,2-9_v2_24@5,,In some cases the final images were assembled by multiple image alignment (MIA) to increase the surface area without losing the resolution.,,"Add,Fact/Evidence",Fact/Evidence
6692,2-9,2-9_v2_38@0,,At the time of tissue harvest the histomorphological features of the transplanted tumor spheroids resembled those of spontaneously grown tumors ( Figure 5 ).,,"Add,Fact/Evidence",Fact/Evidence
6693,2-9,2-9_v2_38@1,,"The malignant cells were arranged into small nodules surrounded with fibroblasts, vessel-free erythrosomes and some undifferentiated migrating cells (mesenchymal cells).",,"Add,Fact/Evidence",Fact/Evidence
6694,2-9,2-9_v2_38@2,,The nodules were not larger than the oxygen diffusion range (100–200 µm <REF-58> ).,,"Add,Fact/Evidence",Fact/Evidence
6695,2-9,2-9_v2_38@3,,A thicker fibrotic capsule surrounded the larger clusters of the small nodules.,,"Add,Fact/Evidence",Fact/Evidence
6696,2-9,2-9_v2_38@4,,The underlying host muscle cells appeared normal whereas adipocytes were commonly replaced with lipid droplets.,,"Add,Fact/Evidence",Fact/Evidence
6784,3-101,3-101_v2_22@5,,"Making such approximations explicit would also encourage the consideration of alternatives, e.g. the use of interval arithmetic.",,"Add,Claim",Claim
6785,3-101,3-101_v2_29@0,,"It is important to distinguish the use of randomness in heuristics from the use of probabilistic models, i.e. models that predict observable quantities as averages over probability distributions.",,"Add,Claim",Claim
6786,3-101,3-101_v2_29@1,,"The latter are in the same category as the global-minimum example discussed above: the numbers they predict are well-defined and computable, even though their computation is often beyond the limits of today’s computing technology.",,"Add,Claim",Claim
6787,3-101,3-101_v2_29@2,,"By contrast, a method such as k -means clustering, whose initialization step requires an arbitrary random choice, yields a different result each time it is applied, and there is no reason to attribute any meaning to the statistical distribution of these results.",,"Add,Claim",Claim
6788,3-101,3-101_v2_29@3,,"In fact, the distribution used in the initialization step is hardly ever documented because it is considered irrelevant.",,"Add,Claim",Claim
9279,7-407,7-407_v2_56@0,,Preregistered vs No-preregistered studies,,"Add,Other",Other
9280,7-407,7-407_v2_57@0,,"This distinction is relevant for assessing the impact of the so-called Questionable Research Practices and in particular p-hacking ( Head et al ., 2015 ; John et al ., 2012 ).",,"Add,Fact/Evidence",Fact/Evidence
9281,7-407,7-407_v2_57@1,,"Preregistered studies must describe all details on how the data will be analyzed before their collection, thus reducing the degree of freedom available during and after data collection.",,"Add,Claim",Claim
9282,7-407,7-407_v2_58@0,,From our database it was possible to compare the estimate of the effect size obtained from the pre-registered studies with that obtained from the no-preregistered ones.,,"Add,Fact/Evidence",Fact/Evidence
9283,7-407,7-407_v2_58@1,,The results are presented in the following Table 4 .,,"Add,Fact/Evidence",Fact/Evidence
9284,7-407,7-407_v2_61@0,,"The effect size point estimates clearly show that the effect size of the preregistered studies is larger than that of the no-preregistered studies, however their precision estimates (see the 95% CI) reveal a considerable overlap and consequently they cannot be considered statistically different.",,"Add,Fact/Evidence",Fact/Evidence
9285,7-407,7-407_v2_63@0,,Our very comprehensive literature search is likely to have reduced the probability of a publication bias.,,"Add,Claim",Claim
9286,7-407,7-407_v2_63@1,,Nevertheless we added a statistical estimation of the publication bias.,,"Add,Fact/Evidence",Fact/Evidence
9287,7-407,7-407_v2_65@1,,"Similarly, more recent publication bias tests like the three-parameters selection model, the p-uniform* and the Vevea and Hedges’ weight-function model ( Vevea & Woods (2005) , seem not recommended for multilevel random meta-analyses with high heterogeneity like the present one.",,"Add,Claim",Claim
9288,7-407,7-407_v2_74@1,,"This phenomenon may hence be considered among the more reliable within those covered under the umbrella term “psi” (see Cardeña, 2018 for an exhaustive review of the evidence and the theoretical hypotheses of all these phenomena).",,"Add,Fact/Evidence",Fact/Evidence
9289,7-407,7-407_v2_77@0,,"In order to arrive at such an ambitious goal, it is necessary to achieve a high degree of correct classifications based on prestimulus activity at the level of each trial so that the number of false positives and false negatives is reduced to a bare minimum.",,"Add,Claim",Claim
9290,7-407,7-407_v2_77@1,,The experiments of Mossbridge (2017) ; Baumgart et al . (2017) and Jolij & Bierman (2017) are promising examples in this regard.,,"Add,Claim",Claim
9291,7-407,7-407_v2_80@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",,"Add,Fact/Evidence",Fact/Evidence
10759,9-1032,9-1032_v2_56@5,,Recent studies have also shown promising odour-blends of volatile organic compounds identified from domesticated grasses such as rice and pollens of maize and sugarcane.,,"Add,Fact/Evidence",Fact/Evidence
10760,9-1032,9-1032_v2_57@5,,"Potentially a more rigorous evaluation of the plant coverage using standard methods such as a quadrant frame which might have provided more detailed information on plant numbers, could have revealed more associations.",,"Add,Claim",Claim
10761,9-1032,9-1032_v2_57@6,,"However, given the high colonisation during the rainy season such method would be better applied during drier seasons.",,"Add,Claim",Claim
10802,9-113,9-113_v2_67@1,,"- - LiteratureCompilation: MAE=0.532, RMSE=0.785, r 2 =0.889)",,"Add,Fact/Evidence",Fact/Evidence
10803,9-113,9-113_v2_71@2,,"Since some molecules had to be omitted for prediction with OPERA due to none or multiple predicted pK a values, no consistent significance test could be performed for all comparisons.",,"Add,Claim",Claim
10804,9-113,9-113_v2_73@0,,The developed model offers the possibility to predict pK a values for monoprotic molecules with good accuracy.,,"Add,Claim",Claim
10805,9-113,9-113_v2_73@1,,"However, since the model has been trained exclusively with monoprotic molecules, only monoprotic molecules can be predicted properly.",,"Add,Claim",Claim
10806,9-113,9-113_v2_73@2,,In this respect the model is limited.,,"Add,Claim",Claim
10807,9-113,9-113_v2_73@3,,"Nevertheless, the results show that the performance for monoprotic molecules can compete with the performance of existing prediction tools.",,"Add,Claim",Claim
10808,9-113,9-113_v2_14@3,,"The Novartis data set consists of 280 unique molecules with a molecular weight between 129 and 670 daltons (mean value 348.68, standard deviation 94.17).",,"Add,Fact/Evidence",Fact/Evidence
10809,9-113,9-113_v2_14@4,,"The calculated LogP values vary between -1.54 and 6.30 (mean value 3.01, standard deviation 1.41).",,"Add,Fact/Evidence",Fact/Evidence
10810,9-113,9-113_v2_14@5,,The 280 molecules spread over 228 unique Murcko Scaffolds.,,"Add,Fact/Evidence",Fact/Evidence
10811,9-113,9-113_v2_14@6,,The ten most common murcko scaffolds cover 15% of the molecules of the total data set (42/280).,,"Add,Fact/Evidence",Fact/Evidence
10812,9-113,9-113_v2_14@7,,A histogram of the pairwise comparison between the training set and the two external test sets (Fingerprint: 4096 bit MorganFeatures radius=3) is given in Figure 2(A) and Figure 2(B),,"Add,Fact/Evidence",Fact/Evidence
10813,9-113,9-113_v2_22@3,,"To ensure that no training data was contained in the test data sets, the conical isomeric SMILES were checked for matches in both training and test data sets and corresponding hits were removed from the test data sets.",,"Add,Fact/Evidence",Fact/Evidence
10814,9-113,9-113_v2_56@0,,The compounds for which the pK a values between the different sources deviate by more than two units are as follows:,,"Add,Fact/Evidence",Fact/Evidence
10815,9-113,9-113_v2_60@0,,"Since the annotation about the experimental settings is not given in the DataWarrior file, we can only hypothesize that these differences are due to the different experimental settings.",,"Add,Claim",Claim
10816,9-113,9-113_v2_61@0,,Machine Learning,,"Add,Other",Other
15,10-ARR,,10-ARR_v1_12@1,,"For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets.","Delete,Fact/Evidence",Fact/Evidence
16,10-ARR,,10-ARR_v1_12@2,,Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation.,"Delete,Fact/Evidence",Fact/Evidence
17,10-ARR,,10-ARR_v1_12@3,,Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation.,"Delete,Fact/Evidence",Fact/Evidence
18,10-ARR,,10-ARR_v1_12@4,,Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet.,"Delete,Fact/Evidence",Fact/Evidence
41,103-ARR,,103-ARR_v1_41@1,,"Table 3 reports our NLI system, including the median F1-Score and the standard deviation across 3 different runs of our implementations NLI and EM.","Delete,Fact/Evidence",Fact/Evidence
42,103-ARR,,103-ARR_v1_41@2,,On ACE our system is best on all comparable results.,"Delete,Fact/Evidence",Fact/Evidence
43,103-ARR,,103-ARR_v1_41@3,,"Note that RCEE_ER is better on 3 data splits, but unfortunately the splits are different.","Delete,Fact/Evidence",Fact/Evidence
75,110-ARR,,110-ARR_v1_37@0,,PLMs lack knowledge of antonyms.,"Delete,Claim",Claim
76,110-ARR,,110-ARR_v1_52@4,,Training details.,"Delete,Other",Other
77,110-ARR,,110-ARR_v1_59@0,,Catastrophic forgetting.,"Delete,Other",Other
135,111-ARR,,111-ARR_v1_70@0,,We evaluate models on (1) formality transfer;,"Delete,Fact/Evidence",Fact/Evidence
136,111-ARR,,111-ARR_v1_71@0,,(2) increasing the amount of code-mixing with English.,"Delete,Fact/Evidence",Fact/Evidence
137,111-ARR,,111-ARR_v1_78@0,,"Multilingual style transfer is mostly unexplored in prior work: a 35 paper survey by Briakou et al. (2021b) found only one work in Chinese, Russian, Latvian, Estonian, French (Shang et al., 2019;Tikhonov and Yamshchikov, 2018;Korotkova et al., 2019;Niu et al., 2018).","Delete,Fact/Evidence",Fact/Evidence
138,111-ARR,,111-ARR_v1_78@1,,"Briakou et al. (2021b) further introduced XFORMAL, the first formality transfer evaluation dataset in French, Brazilian Portugese and Italian.","Delete,Fact/Evidence",Fact/Evidence
139,111-ARR,,111-ARR_v1_78@2,,"16 Hindi formality has been studied in linguistics, focusing on politeness (Kachru, 2006;Agnihotri, 2013;Kumar, 2014) and codemixing (Bali et al., 2014).","Delete,Fact/Evidence",Fact/Evidence
140,111-ARR,,111-ARR_v1_78@3,,"Due to its prevalence in India, English-Hindi code-mixing has seen work in language modeling (Pratapa et al., 2018;Samanta et al., 2019) and core NLP tasks (Khanuja et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
141,111-ARR,,111-ARR_v1_78@4,,"To the best of our knowledge, we are the first to study style transfer for Indic languages.","Delete,Claim",Claim
142,111-ARR,,111-ARR_v1_91@0,,Results: Our results on Hindi are presented in Table 6 and other languages in Table 7.,"Delete,Fact/Evidence",Fact/Evidence
143,111-ARR,,111-ARR_v1_130@0,,"In the baseline Hindi UR model, we notice high COPY rates (45.4%), resulting in lower ACC scores.","Delete,Fact/Evidence",Fact/Evidence
144,111-ARR,,111-ARR_v1_131@3,,"Also see Figure 9 for a comparison across λ values, and Section 5 for detailed metric descriptions.","Delete,Fact/Evidence",Fact/Evidence
145,111-ARR,,111-ARR_v1_131@13,,"The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5.","Delete,Fact/Evidence",Fact/Evidence
146,111-ARR,,111-ARR_v1_131@14,,"We see the DIFFUR models outperform other systems across the λ range, and get best performance with the DIFFUR-MLT variant.","Delete,Fact/Evidence",Fact/Evidence
147,111-ARR,,111-ARR_v1_131@15,,"We also see that DIFFUR models, especially with DIFFUR-MLT, lead to better style transfer control (bottom plot, closer to x = 1 is better), giving large style variation with λ without loss in semantics (X-axis).","Delete,Fact/Evidence",Fact/Evidence
148,111-ARR,,111-ARR_v1_131@17,,"The plots show overall style transfer performance, using the r-AGG (top-left) and a-AGG (top-right) metrics from Section 5.5.","Delete,Fact/Evidence",Fact/Evidence
149,111-ARR,,111-ARR_v1_131@18,,Note that Gujarati is a zero-shot language for DIFFUR models -no Gujarati paraphrase data was seen during training.,"Delete,Fact/Evidence",Fact/Evidence
150,111-ARR,,111-ARR_v1_131@19,,"We see that while the vanilla DIFFUR model performs poorly, the DIFFUR-INDIC is competitive with baselines and the DIFFUR-MLT variant significantly outperforms other systems.","Delete,Fact/Evidence",Fact/Evidence
151,111-ARR,,111-ARR_v1_131@20,,"We also see that the DIFFUR-MLT variant lead to better style transfer control (bottom plot, closer to x = 1 is better), giving style variation with λ without loss in semantics (X-axis).","Delete,Fact/Evidence",Fact/Evidence
154,111-ARR,,111-ARR_v1_19@0,,"Denoising: To learn a style extractor, the Universal Rewriter uses the idea that two non-overlapping spans of text in the same document are likely to have the same style.","Delete,Fact/Evidence",Fact/Evidence
203,114-ARR,,114-ARR_v1_46@2,,"Given that trigger-dependent types often have indicative triggers, we build a mechanism called word saliency embeddings (WSEs) in the model for T trigger to capture such regularities.","Delete,Fact/Evidence",Fact/Evidence
204,114-ARR,,114-ARR_v1_46@3,,"Specifically, we first quantify each word's saliency value 3 as 0 or 1 based on λ, i.e., the threshold we used previously for distinguishing event types, and then use a separate embedding vector to distinguish 0 and 1, similar to word embeddings.","Delete,Fact/Evidence",Fact/Evidence
205,114-ARR,,114-ARR_v1_46@4,,Such embeddings are incorporated into the model 4 to capture a regularity that words with high saliency values are more likely to be triggers.,"Delete,Fact/Evidence",Fact/Evidence
206,114-ARR,,114-ARR_v1_46@5,,"Note WSEs are also incorporated in the model for the T context , which on the other hand seeks to learn the opposite regularity that words with high saliency values may not be triggers.","Delete,Fact/Evidence",Fact/Evidence
207,114-ARR,,114-ARR_v1_54@1,,"We use Adam (Kingma and Ba, 2015) with default hyper-parameters for parameter update.","Delete,Fact/Evidence",Fact/Evidence
208,114-ARR,,114-ARR_v1_55@0,,Experimental Setups,"Delete,Other",Other
209,114-ARR,,114-ARR_v1_56@0,,Datasets.,"Delete,Other",Other
210,114-ARR,,114-ARR_v1_56@1,,"We conduct experiments on ACE 2005 (LDC, 2005) and MAVEN documents.","Delete,Fact/Evidence",Fact/Evidence
211,114-ARR,,114-ARR_v1_56@2,,"We adopt a common split for evaluation following previous works (Li et al., 2013;Wadden et al., 2019).","Delete,Fact/Evidence",Fact/Evidence
212,114-ARR,,114-ARR_v1_56@3,,MAVEN is a newly released corpus defining 168 more fine-grained event types .,"Delete,Fact/Evidence",Fact/Evidence
213,114-ARR,,114-ARR_v1_56@4,,"Because the MAVEN test set is not publicly available and our study is concerned with per-type performance, we instead use the MAVEN development set for assessment and divide the original MAVEN training set as 9:1 for training and validating.","Delete,Fact/Evidence",Fact/Evidence
214,114-ARR,,114-ARR_v1_56@5,,Table 1 displays the comprehensive data statistics for the two datasets.,"Delete,Fact/Evidence",Fact/Evidence
215,114-ARR,,114-ARR_v1_57@0,,Evaluation Metrics.,"Delete,Other",Other
216,114-ARR,,114-ARR_v1_67@5,,"(SL), which only differentiates event types for training, outperforms BERTEns by 1.6% in F1.","Delete,Fact/Evidence",Fact/Evidence
240,115-ARR,,115-ARR_v1_65@0,,Dataset,"Delete,Other",Other
241,115-ARR,,115-ARR_v1_66@0,,We conduct experiments on a publicly available multi-modal sarcasm detection benchmark dataset collected by Cai et al. (2019).,"Delete,Fact/Evidence",Fact/Evidence
242,115-ARR,,115-ARR_v1_66@1,,This dataset contains English tweets expressing sarcasm as Positive examples and those expressing non-sarcasm as Negative examples.,"Delete,Fact/Evidence",Fact/Evidence
243,115-ARR,,115-ARR_v1_66@2,,Each example in the dataset consists of a text and an associated image.,"Delete,Fact/Evidence",Fact/Evidence
244,115-ARR,,115-ARR_v1_66@3,,The statistics of the dataset are shown in Table 1.,"Delete,Fact/Evidence",Fact/Evidence
250,115-ARR,,115-ARR_v1_14@1,,"Different from text-based sarcasm detection, multimodal sarcasm detection aims to identify the sarcastic expression among different modalities (Schifanella et al., 2016;Castro et al., 2019).","Delete,Fact/Evidence",Fact/Evidence
251,115-ARR,,115-ARR_v1_14@2,,Schifanella et al. (2016) firstly tackled the multi-modal sarcasm detection task with text and image modalities by manually designed features.,"Delete,Fact/Evidence",Fact/Evidence
252,115-ARR,,115-ARR_v1_14@3,,Cai et al. (2019) created a new dataset and proposed a hierarchical fusion model for multi-modal sarcasm detection.,"Delete,Fact/Evidence",Fact/Evidence
253,115-ARR,,115-ARR_v1_14@5,,Pan et al. (2020) proposed inter-modality attention and coattention to learn the contradiction of sarcasm.,"Delete,Fact/Evidence",Fact/Evidence
267,118-ARR,,118-ARR_v1_75@0,,"4. Conll03-Typos , which is generated from Conll2003 (Sang and De Meulder, 2003).","Delete,Fact/Evidence",Fact/Evidence
268,118-ARR,,118-ARR_v1_75@1,,"The entities in the test set is replaced by typos version(character modify, insert, and delete operation).","Delete,Fact/Evidence",Fact/Evidence
269,118-ARR,,118-ARR_v1_76@0,,"5. Conll03-OOV , which is generated from Conll2003 (Sang and De Meulder, 2003).","Delete,Fact/Evidence",Fact/Evidence
270,118-ARR,,118-ARR_v1_76@1,,The entities in the test set is replaced by another out-of-vocabulary entity in test set.,"Delete,Fact/Evidence",Fact/Evidence
271,118-ARR,,118-ARR_v1_77@0,,Table 2 reports the static results of the OOV problem on the test sets of each dataset.,"Delete,Fact/Evidence",Fact/Evidence
272,118-ARR,,118-ARR_v1_77@1,,"As shown in the table, the test set of these data sets comprises a substantial amount of OOV entities.","Delete,Fact/Evidence",Fact/Evidence
357,124-ARR,,124-ARR_v1_29@0,,We also show explorations with other review combination methods in Appendix C.1.,"Delete,Fact/Evidence",Fact/Evidence
358,124-ARR,,124-ARR_v1_31@5,,"Hence, all our future usage of the word ""Transformers"" refers to bart-large-cnn in the Transformers library .","Delete,Fact/Evidence",Fact/Evidence
359,124-ARR,,124-ARR_v1_71@0,,We show category examples in Table 9.,"Delete,Fact/Evidence",Fact/Evidence
360,124-ARR,,124-ARR_v1_72@0,,"The additional rules for annotation are as follows: First, instead of only labeling the individual sentences per se, the annotators are given a complete paragraph of meta-review to label the sentences with context information.","Delete,Fact/Evidence",Fact/Evidence
361,124-ARR,,124-ARR_v1_72@1,,"For example, if the area chair writes a sentence providing some extra background knowledge in the discussion of the weakness of the submission, that sentence itself can be considered as ""misc"".","Delete,Fact/Evidence",Fact/Evidence
362,124-ARR,,124-ARR_v1_72@2,,"However, it should be labeled as ""weakness"" to be consistent in context.","Delete,Fact/Evidence",Fact/Evidence
363,124-ARR,,124-ARR_v1_72@3,,"Second, not every sentence can be strictly classified into a single category.","Delete,Fact/Evidence",Fact/Evidence
364,124-ARR,,124-ARR_v1_72@4,,"When a sentence contains information from multiple categories, the annotators should consider its main point and primary purpose.","Delete,Fact/Evidence",Fact/Evidence
365,124-ARR,,124-ARR_v1_73@0,,"Furthermore, there are still some cases where the main point of the sentence is hard to differentiate from multiple categories.","Delete,Fact/Evidence",Fact/Evidence
366,124-ARR,,124-ARR_v1_73@3,,"We use the sign "" ? ="" because there are some rare cases where a sentence contains both ""strength"" and ""weakness"" while there is no obvious emphasis on either, and it is hard to tell whether ""strength"" should have a priority over ""weakness"" or the other way round.","Delete,Fact/Evidence",Fact/Evidence
367,124-ARR,,124-ARR_v1_73@4,,"We then label this sentence based on the final decision: if this submission is accepted, we label the sentence as ""strength"", and vice versa.","Delete,Fact/Evidence",Fact/Evidence
368,124-ARR,,124-ARR_v1_74@0,,We further analyze the category distribution in borderline papers.,"Delete,Fact/Evidence",Fact/Evidence
369,124-ARR,,124-ARR_v1_74@1,,"As shown in Table 10, for submissions within the score range of [4.5,6), there are 713 accepted submissions and 2,588 rejected submissions.","Delete,Fact/Evidence",Fact/Evidence
370,124-ARR,,124-ARR_v1_74@2,,"One clear difference is the percentage of ""strength"" and ""weakness"".","Delete,Fact/Evidence",Fact/Evidence
371,124-ARR,,124-ARR_v1_74@3,,"Another difference is the percentage of ""ac disagreement"", where the accepted papers have four times the value than rejected ones.","Delete,Fact/Evidence",Fact/Evidence
372,124-ARR,,124-ARR_v1_74@4,,"This suggests that for the accepted borderline papers, the area chair tends to share different opinions with reviewers, and thus deciding to accept the borderline submissions.","Delete,Claim",Claim
373,124-ARR,,124-ARR_v1_75@0,,"We further analyze the occurrence of each category for accepted papers and rejected papers separately across different score ranges, as shown in Table 11.","Delete,Fact/Evidence",Fact/Evidence
374,124-ARR,,124-ARR_v1_75@1,,"For accepted papers, as the score increases, the percentage of meta-reviews having ""weakness"" and ""suggestion"" drops because the high-score submissions are more likely to be accepted.","Delete,Fact/Evidence",Fact/Evidence
375,124-ARR,,124-ARR_v1_75@2,,"Even the percentage of ""decision"" drops following the same trend.","Delete,Fact/Evidence",Fact/Evidence
376,124-ARR,,124-ARR_v1_76@1,,"For the concat, we simply concatenate all reviews one after another according to their reviewers' sequence.","Delete,Fact/Evidence",Fact/Evidence
377,124-ARR,,124-ARR_v1_76@2,,"For merge, we can obtain the merged content as follows: From all review inputs, we use the longest one as a backbone.","Delete,Fact/Evidence",Fact/Evidence
378,124-ARR,,124-ARR_v1_83@0,,"For preprocessing, besides filtering based on metareview length, we also remove submissions with only one or two reviews, since the majority of the submissions have more than 3 reviews.","Delete,Fact/Evidence",Fact/Evidence
379,124-ARR,,124-ARR_v1_85@2,,"For the rest of the hyperparameters, we use the pretrained model's default values.","Delete,Fact/Evidence",Fact/Evidence
388,124-ARR,,124-ARR_v1_9@0,,"We will release our full dataset, code, and detailed settings to the community.","Delete,Claim",Claim
459,125-ARR,,125-ARR_v1_25@0,,"We want to point out one advantage of using generation-based models under the low-resource scenario compared to previous classification-based event extraction models -generation-based models do not require named entity annotations (Sha et al., 2018;Lin et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
460,125-ARR,,125-ARR_v1_25@1,,"The pre-trained decoder inherently identifies reasonable entity spans, which makes generation-based models become a good choice when annotations are expensive.","Delete,Claim",Claim
461,125-ARR,,125-ARR_v1_48@1,,"In addition to the previously mentioned EE models: OneIE (Lin et al., 2020), BERT_QA (Du and Cardie, 2020), TANL (Paolini et al., 2021), and Text2Event (Lu et al., 2021), we also consider the following baselines focusing on the high-resource setting.","Delete,Fact/Evidence",Fact/Evidence
462,125-ARR,,125-ARR_v1_48@7,,"Therefore, the advantage of DEGREE over DEGREE(PIPE) becomes less obvious.","Delete,Claim",Claim
463,125-ARR,,125-ARR_v1_48@8,,This result justifies our hypothesis that DEGREE has better performance for the lowresource setting because of its ability to better capture dependencies.,"Delete,Claim",Claim
464,125-ARR,,125-ARR_v1_49@0,,Results for event argument extraction.,"Delete,Other",Other
465,125-ARR,,125-ARR_v1_49@1,,"In Table 3, we additionally study the performance for event argument extraction task, where the model makes argument predictions with the gold trigger provided.","Delete,Fact/Evidence",Fact/Evidence
466,125-ARR,,125-ARR_v1_49@2,,"Interestingly, DEGREE(EAE) achieves pretty strong performance and outperforms other baselines with a large margin.","Delete,Fact/Evidence",Fact/Evidence
467,125-ARR,,125-ARR_v1_49@3,,"Combining the results in Table 2, we hypothesize that event argument extraction is a more challenging task than event trigger detection and it requires more training examples to learn well.","Delete,Claim",Claim
468,125-ARR,,125-ARR_v1_49@4,,"Hence, our proposed model, which takes the advantage of using label semantics to better capture dependencies, achieves a new state-of-the-art for event argument extraction.","Delete,Claim",Claim
469,125-ARR,,125-ARR_v1_87@0,,Limitations.,"Delete,Other",Other
470,125-ARR,,125-ARR_v1_87@2,,We believe this assumption holds for most of common NLP tasks.,"Delete,Claim",Claim
471,125-ARR,,125-ARR_v1_87@3,,"However, for some specific domains, such as the biomedical domain, acquiring this information can be a bit difficult (e.g., needs to hire experts to write down templates), which increases the cost of training DEGREE.","Delete,Claim",Claim
472,125-ARR,,125-ARR_v1_87@4,,"In addition, our proposed model is based on pre-trained language models.","Delete,Fact/Evidence",Fact/Evidence
473,125-ARR,,125-ARR_v1_87@5,,DEGREE performs well because it is able to leverage the prompts and the pre-trained knowledge.,"Delete,Claim",Claim
474,125-ARR,,125-ARR_v1_87@6,,"However, if the downstream domain is far from the pre-trained corpus, the advantage of leveraging knowledge becomes restricted.","Delete,Claim",Claim
475,125-ARR,,125-ARR_v1_87@8,,"DE-GREE achieves a good performance on two datasets (ACE 2005 and ERE-EN), which are more related to news-styled passages.","Delete,Claim",Claim
476,125-ARR,,125-ARR_v1_87@9,,"When considering other downstream domains, it is possible that the improvement is not as significant as it is for the two datasets we use in the paper.","Delete,Claim",Claim
477,125-ARR,,125-ARR_v1_87@10,,"The reason is the gap between the downstream domain knowledge and the pre-trained knowledge, as mentioned in the previous paragraph.","Delete,Claim",Claim
478,125-ARR,,125-ARR_v1_88@0,,Potential risks.,"Delete,Other",Other
642,13-ARR,,13-ARR_v1_19@6,,We denote the O mask as the mask for an object 1.,"Delete,Fact/Evidence",Fact/Evidence
643,13-ARR,,13-ARR_v1_20@0,,"We first sample the trajectories in the Matterport (Chang et al., 2017) Environment.","Delete,Fact/Evidence",Fact/Evidence
644,13-ARR,,13-ARR_v1_20@1,,"We randomly sample the starting and ending positions, and collect tracks with lengths of less than 8 hops.","Delete,Fact/Evidence",Fact/Evidence
645,13-ARR,,13-ARR_v1_20@2,,Then we obtain the corresponding actions of each trajectory by firstperson movement.,"Delete,Fact/Evidence",Fact/Evidence
646,13-ARR,,13-ARR_v1_20@3,,"If the agent chooses the front navigable position to move, we generate a 'forward' action.","Delete,Fact/Evidence",Fact/Evidence
647,13-ARR,,13-ARR_v1_20@4,,"If the agent chooses the back navigable position to move, we generate an 'around' action.","Delete,Fact/Evidence",Fact/Evidence
648,13-ARR,,13-ARR_v1_20@5,,"Otherwise, if the agent selects the right front navigable position to move for the next step, we generate an action sequence like {'right', 'forward'}, which is used to fill actionable verbs during instruction generation.","Delete,Fact/Evidence",Fact/Evidence
649,13-ARR,,13-ARR_v1_21@1,,"ProbES introduces CLIP, a powerful vision-language alignment model learned from a large-scale image-caption dataset.","Delete,Fact/Evidence",Fact/Evidence
650,13-ARR,,13-ARR_v1_22@0,,"To generate structured augmentation data, we fullfill the templates with phrases that describe the sampled trajectory and actions.","Delete,Fact/Evidence",Fact/Evidence
651,13-ARR,,13-ARR_v1_22@1,,"A trajectory is denoted as {v 1 , v 2 , ..., v n }, where v i represents an observation viewpoint.","Delete,Fact/Evidence",Fact/Evidence
673,133-ARR,,133-ARR_v1_66@2,,We make the source code available upon acceptance of the paper.,"Delete,Fact/Evidence",Fact/Evidence
919,141-ARR,,141-ARR_v1_7@0,,The correct answer should be Saint Vincent and the Grenadines instead of United States although both entities have co-occurring contexts with Moonhole.,"Delete,Fact/Evidence",Fact/Evidence
920,141-ARR,,141-ARR_v1_73@2,,The proof can be found in the Appendix.,"Delete,Fact/Evidence",Fact/Evidence
921,141-ARR,,141-ARR_v1_73@8,,"Then µ(r m (s, a)) takes the m-th entry of F( hs , ha ).","Delete,Fact/Evidence",Fact/Evidence
922,141-ARR,,141-ARR_v1_79@6,,The number of final clauses to define the query relation is H = 5 and the number of candidate bridging contexts for each hop is set to K = 5.,"Delete,Fact/Evidence",Fact/Evidence
923,141-ARR,,141-ARR_v1_79@7,,"The dimension of predicate embeddings and biGRU layer is 100 and 200, respectively.","Delete,Fact/Evidence",Fact/Evidence
924,141-ARR,,141-ARR_v1_79@8,,"For training, we adopt Adam optimization with learning rate initialized at 0.001.","Delete,Fact/Evidence",Fact/Evidence
925,141-ARR,,141-ARR_v1_79@9,,The batch size is set to 10.,"Delete,Fact/Evidence",Fact/Evidence
926,141-ARR,,141-ARR_v1_79@10,,"For all the experiments, we use the development dataset to evaluate the results because the test data is not publicly available.","Delete,Fact/Evidence",Fact/Evidence
927,141-ARR,,141-ARR_v1_80@0,,Experimental Result,"Delete,Other",Other
928,141-ARR,,141-ARR_v1_81@0,,"Weber et al. ( 2019) only selects four different query relations from WikiHop, namely Publisher, Developer, Country and Record_label, to evaluate their model.","Delete,Fact/Evidence",Fact/Evidence
929,141-ARR,,141-ARR_v1_81@1,,"For fair comparison, we first follow their setting to compare on these specific domains.","Delete,Fact/Evidence",Fact/Evidence
930,141-ARR,,141-ARR_v1_81@2,,"Besides BIDAF (Seo et al., 2017) and FastQA (Weissenborn et al., 2017), we also consider another three representative deep learning baselines : EPAr (Yichen Jiang and Bansal, 2019), HDEG (Tu et al., 2019), DynSAN (Zhuang and Wang, 2019) 2 , and a differentiable reasoning model DrMD adapted from (Dhingra et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
931,141-ARR,,141-ARR_v1_81@3,,HDEG is a graph-based DNN.,"Delete,Fact/Evidence",Fact/Evidence
932,141-ARR,,141-ARR_v1_81@4,,EPAr and DynSAN are memory-based DNNs.,"Delete,Fact/Evidence",Fact/Evidence
933,141-ARR,,141-ARR_v1_81@5,,"DrMD is implemented following (Dhingra et al., 2020), except that we remove pre-defined entities and only consider mention interactions given our settings.","Delete,Fact/Evidence",Fact/Evidence
934,141-ARR,,141-ARR_v1_81@6,,BERT is a baseline model that concatenates query subject (or a candidate entity) with each context in the form of we feed the hidden representations corresponding to the query subject (or candidates) into an attention model to generate a single vector to be fed into a classifier.,"Delete,Fact/Evidence",Fact/Evidence
935,141-ARR,,141-ARR_v1_81@7,,"For all the baselines, we train the models on each query relation separately to test the reasoning capability, same as our setting.","Delete,Fact/Evidence",Fact/Evidence
936,141-ARR,,141-ARR_v1_81@8,,"Table 1 lists the results for MedHop and four query relations from WikiHop according to (Weber et al., 2019).","Delete,Fact/Evidence",Fact/Evidence
937,141-ARR,,141-ARR_v1_81@9,,"Clearly, DILR substantially outperforms all the baselines on MedHop, demonstrating the importance of the reasoning capabilities for interaction-intensive medical dataset.","Delete,Claim",Claim
938,141-ARR,,141-ARR_v1_81@10,,"On the four query relations from WikiHop, we still obtain the best performances.","Delete,Fact/Evidence",Fact/Evidence
939,141-ARR,,141-ARR_v1_81@13,,The results in terms of accuracy are shown in Table 2.,"Delete,Fact/Evidence",Fact/Evidence
940,141-ARR,,141-ARR_v1_81@15,,"As shown in Table 2, there are 38 relations (D1) containing less than 1,000 training examples, 7 relations (D2) with training examples ranging from 1,000 to 4,000 and 2 relations (D3) having more than 4,000 training examples.","Delete,Fact/Evidence",Fact/Evidence
941,141-ARR,,141-ARR_v1_81@21,,The Detailed comparison on each query relation can be found in Appendix.,"Delete,Fact/Evidence",Fact/Evidence
993,15-ARR,,15-ARR_v1_31@2,,The USA part contains the majority of the arguments as we could reuse an existing dataset.,"Delete,Fact/Evidence",Fact/Evidence
1058,151-ARR,,151-ARR_v1_20@1,,The input can be formalized as:,"Delete,Fact/Evidence",Fact/Evidence
1059,151-ARR,,151-ARR_v1_35@3,,"Therefore, we propose monotonic regional attention (MRA).","Delete,Claim",Claim
1060,151-ARR,,151-ARR_v1_59@0,,Experimental Settings,"Delete,Other",Other
1061,151-ARR,,151-ARR_v1_60@0,,"Benchmarks Following previous work (Rei et al., 2020;Yuan et al., 2021), we examine the effectiveness of the propose method on WMT 2019 Metrics tasks (Ma et al., 2019).","Delete,Fact/Evidence",Fact/Evidence
1062,151-ARR,,151-ARR_v1_68@5,,"Moreover, we also testify the performance of UniTE-MUP on WMT 2020 QE tasks via finetuning.","Delete,Fact/Evidence",Fact/Evidence
1063,151-ARR,,151-ARR_v1_79@0,,"Considering the English-targeted model, we select Czech (Cz), German (De), Japanese (Ja), Russian (Ru), and Chinese (Zh) as source languages, and English (En) as target.","Delete,Fact/Evidence",Fact/Evidence
1064,151-ARR,,151-ARR_v1_79@1,,"For each translation direction, we collect 1 million samples, finally yielding 5 million examples in total for unified pretraining.","Delete,Fact/Evidence",Fact/Evidence
1065,151-ARR,,151-ARR_v1_79@2,,"As to the multilingual-targeted model, we further collect 1 million synthetic data for each language direction of En-Cz, En-De, En-Ja, En-Ru, and En-Zh.","Delete,Fact/Evidence",Fact/Evidence
1066,151-ARR,,151-ARR_v1_79@3,,"Finally, we construct 10 million examples for the pretraining of the multilingual version by adding the data of the English-targeted model.","Delete,Fact/Evidence",Fact/Evidence
1067,151-ARR,,151-ARR_v1_79@4,,"Note that, for a fair comparison, we filter out all pretraining examples that are involved in benchmarks.","Delete,Fact/Evidence",Fact/Evidence
1068,151-ARR,,151-ARR_v1_80@0,,"After trying several pooling methods which derive sequence-level representations, we use the representations located at the start of sequence as the input of feedforward network (Ranasinghe et al., 2020b).","Delete,Fact/Evidence",Fact/Evidence
1069,151-ARR,,151-ARR_v1_80@1,,"The feedforward network consists of 3 linear transitions, where the dimensionalities of corresponding outputs are 3,072, 1,024, and 1, respectively.","Delete,Fact/Evidence",Fact/Evidence
1070,151-ARR,,151-ARR_v1_80@2,,"Between any two adjacent linear modules in feedforward, hyperbolic tangent function is arranged as activation.","Delete,Fact/Evidence",Fact/Evidence
1071,151-ARR,,151-ARR_v1_82@1,,"As seen, our approach can give better performance than strong QE baselines.","Delete,Claim",Claim
1072,151-ARR,,151-ARR_v1_83@0,,All the models reported in this paper were finetuned on a single Nvidia V100 (32GB) GPU.,"Delete,Fact/Evidence",Fact/Evidence
1073,151-ARR,,151-ARR_v1_83@1,,"Specifically for UniTE-UP and UniTE-MUP, the pretraining is arranged on 4 Nvidia V100 (32GB) GPUs.","Delete,Fact/Evidence",Fact/Evidence
1074,151-ARR,,151-ARR_v1_83@2,,"Our framework is built upon COMET repository (Rei et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
1075,151-ARR,,151-ARR_v1_83@3,,"For the contribution to the research community, we will release both the source code of UniTE framework and the well-trained evaluation models including UniTE-UP and UniTE-MUP checkpoints as described in this paper upon the acceptance.","Delete,Fact/Evidence",Fact/Evidence
1166,155-ARR,,155-ARR_v1_85@4,,Table 5: The defense results of different AT methods against two combinatorial optimization attacks.,"Delete,Fact/Evidence",Fact/Evidence
1167,155-ARR,,155-ARR_v1_85@5,,We remove ASR % due to the space limit.,"Delete,Fact/Evidence",Fact/Evidence
1168,155-ARR,,155-ARR_v1_98@3,,We can also conclude that DeBERTa is significantly more robust than RoBERTa.,"Delete,Claim",Claim
1179,156-ARR,,156-ARR_v1_96@0,,"As the results shown in Table 4, the combination of TurnAPE and RoleAPE achieve the best performance.","Delete,Fact/Evidence",Fact/Evidence
1180,156-ARR,,156-ARR_v1_96@1,,"Both absolute and relative position embeddings improve model performance, nevertheless, including them at the same time can be harmful.","Delete,Claim",Claim
1181,156-ARR,,156-ARR_v1_108@0,,"In this paper, different ethical restrictions deserve discussion.","Delete,Claim",Claim
1182,156-ARR,,156-ARR_v1_109@0,,All data used in our pre-training are available online and other dialog corpus in this paper are publicly available sources.,"Delete,Fact/Evidence",Fact/Evidence
1183,156-ARR,,156-ARR_v1_109@1,,We strictly followed the platform's policies and rules when crawling data from web platforms.,"Delete,Fact/Evidence",Fact/Evidence
1184,156-ARR,,156-ARR_v1_109@2,,We did not employ any author-specific information in our research.,"Delete,Fact/Evidence",Fact/Evidence
1185,156-ARR,,156-ARR_v1_110@0,,"Our corpus may includes some bias, such as political bias and social bias, and our model might have inherited some forms of these bias.","Delete,Claim",Claim
1186,156-ARR,,156-ARR_v1_110@1,,"In order to limit these bias as much as possible, we filter controversial articles and removed data with offensive information when possible.","Delete,Fact/Evidence",Fact/Evidence
1187,156-ARR,,156-ARR_v1_112@0,,"We demonstrate the responses generated from our model as well as other baseline models in Table 7, 8 and 9, respectively.","Delete,Fact/Evidence",Fact/Evidence
1188,156-ARR,,156-ARR_v1_112@1,,The results in Table 8 and 9 show that our model accurately outputs the knowledge information contained in context although we do not model knowledge explicitly.,"Delete,Fact/Evidence",Fact/Evidence
1190,156-ARR,,156-ARR_v1_10@0,,"Our pre-trained models and source code will be released, hoping to facilitate further research progress in dialogue generation.","Delete,Claim",Claim
1204,157-ARR,,157-ARR_v1_30@0,,Figure 2 shows the relevance of different content (columns) for the various stakeholders (the rows) for a 10K filing document.,"Delete,Fact/Evidence",Fact/Evidence
1205,157-ARR,,157-ARR_v1_30@1,,Groups of stakeholders are made that form the personas interested in the different parts of the document.,"Delete,Fact/Evidence",Fact/Evidence
1206,157-ARR,,157-ARR_v1_30@2,,The different columns are also grouped together as to indicate what kind of information those sections will contain.,"Delete,Fact/Evidence",Fact/Evidence
1207,157-ARR,,157-ARR_v1_53@0,,Table 5: Results from the human experiment on using the Default Reading experience with DYNAMICTOC.,"Delete,Other",Other
2726,25-ARR,,25-ARR_v1_19@2,,"This is despite evidence of adjectival scale labels being problematic in terms of bias resulting from positively and negatively worded items not being true opposites of one another, and items intended to have neutral intensity in fact proving to have specific conceptual meanings.","Delete,Claim",Claim
2727,25-ARR,,25-ARR_v1_46@0,,Comparison with Automatic Evaluation Metrics,"Delete,Other",Other
2728,25-ARR,,25-ARR_v1_48@0,,As can be seen from Tables Tables 6 unfortu-9 Raw average scores for models in the Ice-breaker run are additionally provided in Table 10 in Appendix A.4.,"Delete,Fact/Evidence",Fact/Evidence
3457,334-ARR,,334-ARR_v1_51@9,,"This provides interesting future direction, where legal knowledge is incorporated into the prediction model.","Delete,Claim",Claim
5718,86-ARR,,86-ARR_v1_70@0,,"Inference Promotion: We can achieve 11.73 and 2.06 absolute inference accuracy improvements compared to the baselines for the NLI and CQA task, respectively.","Delete,Fact/Evidence",Fact/Evidence
5719,86-ARR,,86-ARR_v1_70@1,,"For the NLI task, with our MPII framework, the Transformer baseline model can improve over 5 absolute accuracy score.","Delete,Fact/Evidence",Fact/Evidence
5720,86-ARR,,86-ARR_v1_70@2,,"The ablation study shows the contribution comes from not only the mutual interaction of inference and interpretation in the Stepwise Integration Mechanism (SIM), but also the adversarial mutual information training objective introduced in the Adversarial Fidelity Regularization (AFiRe).","Delete,Fact/Evidence",Fact/Evidence
5721,86-ARR,,86-ARR_v1_70@3,,"Moreover, with parameters initialized with the pretrained BART model, the accuracy can be further improved by a 4.53 absolute score.","Delete,Fact/Evidence",Fact/Evidence
5722,86-ARR,,86-ARR_v1_70@4,,"For the CQA task, we observe that better performance is still achieved compared with the CAGE baseline model.","Delete,Fact/Evidence",Fact/Evidence
5723,86-ARR,,86-ARR_v1_70@5,,"If we remove the AFiRe, a significant inference degradation would be witnessed.","Delete,Claim",Claim
5724,86-ARR,,86-ARR_v1_70@6,,It also indicates the effectiveness of AFiRe for utilizing interpretability to improve the inference ability.,"Delete,Claim",Claim
5725,86-ARR,,86-ARR_v1_71@0,,Interpretation Promotion: The quality of generated interpretation can also be significantly improved with our mutual promotion method on both NLI and CQA tasks.,"Delete,Fact/Evidence",Fact/Evidence
5726,86-ARR,,86-ARR_v1_71@1,,"For NLI task, combined with our MPII, the Transformer baseline model can provide more accurate, fluent and diverse interpretation with much better results in all metrics.","Delete,Fact/Evidence",Fact/Evidence
5727,86-ARR,,86-ARR_v1_71@2,,"Similar with the inference results, the ablation study shows that both SIM and AFiRe contribute to the performance improvement.","Delete,Fact/Evidence",Fact/Evidence
5728,86-ARR,,86-ARR_v1_71@3,,"With the pretrained BART model, we further improve the BLEU and Inter-Rep performance and get comparable PPL compared with the e-INFERSENT model.","Delete,Fact/Evidence",Fact/Evidence
5729,86-ARR,,86-ARR_v1_71@4,,"For CQA task, our method performs better in terms of BLEU score and the diversity of generated explanations.","Delete,Fact/Evidence",Fact/Evidence
5730,86-ARR,,86-ARR_v1_71@5,,"We notice that the BLEU scores are pretty low for CQA task, which may stem from the free form of expression for explanations in the dataset, i.e. several different explanations share the same commonsense knowledge.","Delete,Claim",Claim
5735,86-ARR,,86-ARR_v1_16@1,,Both prediction label and explanation token are generated at every decoding step.,"Delete,Fact/Evidence",Fact/Evidence
5736,86-ARR,,86-ARR_v1_16@2,,Two fusion gates are attached to enable deep interaction of their hidden representations.,"Delete,Fact/Evidence",Fact/Evidence
5759,89-ARR,,89-ARR_v1_26@6,,Our results alert investors who use text-based stock prediction models to deploy defense systems to guard against loss caused by potential adversarial attack.,"Delete,Claim",Claim
5760,89-ARR,,89-ARR_v1_29@0,,"In summary, we show that financial forecast models are vulnerable to adversarial attack even if it is subject to certain physical constraints.","Delete,Claim",Claim
5761,89-ARR,,89-ARR_v1_12@4,,"A directional financial forecast model takes domains of tweets and numerical factors as input, and yields prediction for stocks' directional movement y ∈ {−1, 1}:","Delete,Fact/Evidence",Fact/Evidence
6037,1-23,,1-23_v1_19@10,,Data about health hazards linked to FGC were not derived from any of these studies.,"Delete,Fact/Evidence",Fact/Evidence
6038,1-23,,1-23_v1_19@12,,"If a new test or a drug is to be prescribed for a patient, it should pass through a complicated series of tests and randomized comparisons before getting approved.","Delete,Claim",Claim
6039,1-23,,1-23_v1_19@13,,The same occurs with any surgical procedure.,"Delete,Claim",Claim
6040,1-23,,1-23_v1_19@14,,No procedure can be considered superior to another or blamed for complications except after randomized controlled trials comparing the new to standard surgery.,"Delete,Claim",Claim
6041,1-23,,1-23_v1_19@15,,It therefore seems unrealistic to consider data about FGC not derived from randomized or cohort studies are true and conclusive.,"Delete,Claim",Claim
6042,1-23,,1-23_v1_22@0,,"The ban against FGC seems to be gender based, especially because no similar act was taken against male circumcision.","Delete,Claim",Claim
6043,1-23,,1-23_v1_22@1,,"If male circumcision is considered safe by anti FGC groups, they should advise how to render FGC as safe as male circumcision instead of enforcing the ban against it.","Delete,Claim",Claim
6046,1-23,,1-23_v1_19@8,,Research including reported data about past experiences will always be threatened by the individual’s memory and the influence of exposure status on the recalling process <REF-21> .,"Delete,Fact/Evidence",Fact/Evidence
6047,1-23,,1-23_v1_19@9,,The strongest evidence comes from randomized controlled trials followed by cohort studies.,"Delete,Claim",Claim
6054,1-62,,1-62_v1_16@0,,The 16S rRNA PCR product sequencing was carried out by modified Sanger’s dideoxy chain termination cycle sequencing method <REF-21> .,"Delete,Fact/Evidence",Fact/Evidence
6055,1-62,,1-62_v1_16@1,,"Electropherogram was read by an automated DNA sequencer ( Applied Biosystems ABI3500 XL Genetic Analyzer, Big Dye Terminator version 3.1 Cycle sequencing kit ) for 1.5 kb amplicon of the isolate.","Delete,Fact/Evidence",Fact/Evidence
6056,1-62,,1-62_v1_16@2,,The resulting final DNA sequence of isolate was subjected to BLAST analysis on the NCBI web server.,"Delete,Fact/Evidence",Fact/Evidence
6057,1-62,,1-62_v1_16@3,,The phylogenetic tree of 16S rRNA of the isolate was constructed by using a neighbor-joining (NJ) method with 1000 replicates of bootstrap in MEGA4.1 software <REF-22> .,"Delete,Fact/Evidence",Fact/Evidence
6058,1-62,,1-62_v1_16@4,,The bootstrap consensus tree inferred from 1000 replicates was selected to represent the evolutionary history of the taxa for 16S rRNA sequence analysis.,"Delete,Fact/Evidence",Fact/Evidence
6059,1-62,,1-62_v1_17@1,,The NCBI Blast server was used for the identification of the ATP synthase a -subunit and the phylogenetic analysis of the a -subunit was carried out with the help of MEGA4.1.,"Delete,Fact/Evidence",Fact/Evidence
6060,1-62,,1-62_v1_17@2,,The blastx was used to get the amino acid sequence of the a -subunit.,"Delete,Fact/Evidence",Fact/Evidence
6061,1-62,,1-62_v1_17@3,,"The a -subunit of isolate was compared with that of established alkaliphiles, acidophiles and neutrophiles with the help of ClustalW.","Delete,Fact/Evidence",Fact/Evidence
6062,1-62,,1-62_v1_20@1,,"Out of these, some bacterial colonies were found with pink and orange pigmentation along with no pigmentation i.e. white colonies.","Delete,Fact/Evidence",Fact/Evidence
6063,1-62,,1-62_v1_20@4,,"However, the pH range of growth was pH 7.0 to 12.0 (Supplementary figure 1) .","Delete,Fact/Evidence",Fact/Evidence
6064,1-62,,1-62_v1_33@0,,"Comparison of a -subunit of Stenotrophomonas species DL18 with acidophiles, alkaliphiles and neutrophiles","Delete,Other",Other
6065,1-62,,1-62_v1_37@0,,"However, Ala 198 from the Stenotrophomonas species DL 18 and Ala 196 from T. cyclicum were found to correspond with Gly 208 in E. coli and Gly 170 in Bacillus pseudofirmus OF4 in alignment of the a -subunit as shown in Figure 2 .","Delete,Fact/Evidence",Fact/Evidence
6066,1-62,,1-62_v1_37@1,,"In a similar way, Gly 207 was observed in Stenotrophomonas species DL 18, while in this respective position alanine was found in alkaliphiles and neutrophiles.","Delete,Fact/Evidence",Fact/Evidence
6067,1-62,,1-62_v1_37@2,,But both amino acids correspond from the same amino acid family.,"Delete,Fact/Evidence",Fact/Evidence
6068,1-62,,1-62_v1_38@4,,"However, exchange mutations at Gly 120 and Lys 180 to Lys 120 and Gly 180 showed ATP synthase activity in Bacillus pseudofirmus OF4.","Delete,Fact/Evidence",Fact/Evidence
6069,1-62,,1-62_v1_39@0,,"After comparison with acidophiles, neutrophiles and alkaliphiles, Leu 197 of the Stenotrophomonas species DL 18 was found to be conserved in TMH-4 (Figure 3) .","Delete,Fact/Evidence",Fact/Evidence
6076,1-62,,1-62_v1_6@1,,"The c 11 ring from Ilyobacter tartaricus has Na + ion binding specificity, while the c 13 ring from Bacillus pseudofirmus OF4 and the c 15 ring from Spirulina platensis has H + ion binding specificity <REF-6> .","Delete,Fact/Evidence",Fact/Evidence
6077,1-62,,1-62_v1_6@2,,Ion coordination geometry and distances determine the ion specificity.,"Delete,Claim",Claim
6078,1-62,,1-62_v1_6@3,,"Translocated ions bind to conserved carboxylate of aspartate or glutamate (D/E) in the outer α-helices of c-rings <REF-5> , <REF-6> .","Delete,Fact/Evidence",Fact/Evidence
6079,1-62,,1-62_v1_6@4,,"However, ions are further coordinated by a network of residues.","Delete,Claim",Claim
6080,1-62,,1-62_v1_6@5,,The inner and outer α-helices are in a staggered position.,"Delete,Claim",Claim
6084,1-62,,1-62_v1_6@6,,"Out of these, the inner helices are hydrophobic in nature and in contact with the phospholipids, while outer helices are hydrophilic with a -subunit interaction for ion translocation <REF-2> , <REF-6> .","Delete,Fact/Evidence",Fact/Evidence
6085,1-62,,1-62_v1_6@7,,"The transmembrane electric potential controls the kinetics of rotary motion, which seems to be independent of the ionic gradient <REF-5> .","Delete,Fact/Evidence",Fact/Evidence
6086,1-62,,1-62_v1_6@9,,"For this, there are some crucial amino acid residue adaptations in the a - and c -subunit solving the problem of proton capture from an alkaline environment and subsequent translocation to the binding sites on the c-ring <REF-6> .","Delete,Fact/Evidence",Fact/Evidence
6087,1-62,,1-62_v1_7@0,,"Various studies based on the mechanism of proton binding <REF-7> , <REF-8> , through hydronium ion proton retention and transportation for ATP synthesis in the bacterial system including alkaliphiles <REF-9> , <REF-10> suggest the presence of alkaliphile specific conserved amino acid motifs in transmembrane helix-4 (TMH-4) and TMH-5 of the a -subunit and the inner and the outer helix of the c -subunit <REF-3> , <REF-11> – <REF-14> of the ATP synthase F o subunit as well as Na + /H + antiporter <REF-15> and other cation binding proton transporters including multiple drug transporters <REF-6> , <REF-16> .","Delete,Fact/Evidence",Fact/Evidence
6088,1-62,,1-62_v1_7@1,,The presence of the above mentioned alkaliphile specific sequences of ATP synthase F o subunit along with Na + /H + antiporters and other multiple drug transporters are the major strategies for pH homeostasis of extremely alkaliphilic species.,"Delete,Claim",Claim
6089,1-62,,1-62_v1_8@1,,"The arginine residue of the a -subunit, which transfers protons to the c -subunit, is conserved in almost all bacterial species <REF-3> .","Delete,Fact/Evidence",Fact/Evidence
6090,1-62,,1-62_v1_8@2,,"Recent developments in molecular studies of facultative alkaliphiles suggests the presence of a highly conserved A X A X A X A motif in the amino terminal helix and a P XX E XX P motif in the carboxy terminal helix of the ATP synthase c -subunit in Bacillus pseudofirmus OF4, an established facultative alkaliphile <REF-6> , <REF-17> , <REF-18> .","Delete,Fact/Evidence",Fact/Evidence
6091,1-62,,1-62_v1_2@3,,"Attempts were made to isolate and identify alkaliphiles from their naturally occurring original habitat, i.e. Lonar Lake, India with high alkaline conditions of pH 10.5.","Delete,Claim",Claim
6092,1-62,,1-62_v1_8@5,,"The present study deals with the isolation, identification and analysis of alkaliphile specific amino acid motifs in the a -subunit of ATP synthase.","Delete,Fact/Evidence",Fact/Evidence
6093,1-62,,1-62_v1_11@2,,"After mix culture was obtained by the spread plate method, pure culture of each type of colony was maintained for further studies.","Delete,Fact/Evidence",Fact/Evidence
6094,1-62,,1-62_v1_12@0,,DNA extraction and Polymerase Chain Reaction (PCR),"Delete,Other",Other
6095,1-62,,1-62_v1_13@1,,DNA quantization and quality control for protein contamination was carried out by spectrophotometric absorbance at A 260 and A 280 .,"Delete,Fact/Evidence",Fact/Evidence
6096,1-62,,1-62_v1_13@2,,The small subunit ribosomal RNA (16S rRNA) PCR for identification of bacterium was performed with forward primer (16S20F: 5’ATGTTGATCATGGCTCA3’) and reverse primer (16S1540R: 5’AAGGAGGTGATCCAACCGCA 3’) <REF-20> .,"Delete,Fact/Evidence",Fact/Evidence
6097,1-62,,1-62_v1_13@3,,"Briefly, master mix was prepared for 16S rRNA PCR: 10x PCR Rxn Buffer without MgCl 2 (Invitrogen, P/N Y02028B Lot no. WK1B1b, USA), 1mM MgCl 2 (Invitrogen, P/N Y02016B Lot no. WK2B1a, USA), 200 µM dNTP mix (Merck, India), 100 picomoles of each reverse and forward primers (Integrated DNA technologies, USA), 2.5U of Taq Polymerase enzyme (Invitrogen, USA, 11615-010 Lot No. VKRB1E) and nuclease free water (Merck, India) was added to make up a final volume of 100 µl.","Delete,Fact/Evidence",Fact/Evidence
6098,1-62,,1-62_v1_13@4,,"Following thermal cycling conditions were used for PCR: Initial denaturation at 94°C for 5 min, followed by 30 cycles of denaturation at 94°C for 1 min, primer annealing at 55°C for 1 min and primer extension at 72°C for 2 min.","Delete,Fact/Evidence",Fact/Evidence
6099,1-62,,1-62_v1_13@5,,Thirty cycles of PCR were followed by final extension at 72°C for 5 min followed by cooling at 4°C.,"Delete,Fact/Evidence",Fact/Evidence
6100,1-62,,1-62_v1_14@0,,ATP synthase F o amplification primers were designed based on S. maltophilia K279a as follows: forward primer Steno atp1F: 5’CCTGGCGGATCCTTAGATCTCCG 3’ and reverse primer Steno atp1R: 5’CAGTGAGGATCCTTAGATCTCCGAGGCCAGCT 3’.,"Delete,Fact/Evidence",Fact/Evidence
6101,1-62,,1-62_v1_14@4,,"Results of 16S rRNA and ATP synthase F o amplicons were visualized on 1% agarose gel with 200 ng/ml ethidium bromide (sd fine, India) and results were observed and analyzed with the help of Bio-Rad gel documentation system XR with Bio-Rad Quantity-One 4.6.5 software.","Delete,Fact/Evidence",Fact/Evidence
6102,1-62,,1-62_v1_2@5,,"Although the a-subunit of Stenotrophomonas DL18 showed significant similarity with neutrophiles, the isolated bacterium is an alkaliphile and optimally grows at pH 10.5.","Delete,Fact/Evidence",Fact/Evidence
6377,2-259,,2-259_v1_9@6,,"Antiganglioside antibodies were negative (GM1, GM2, GD1a, GD1b, GQ1b).","Delete,Fact/Evidence",Fact/Evidence
6580,2-288,,2-288_v1_32@7,,"If a particular model absolutely requires procedural code, this can be developed in C, or Fortran or Java, and invoked as part of the model computation.","Delete,Fact/Evidence",Fact/Evidence
6581,2-288,,2-288_v1_78@7,,(JSim’s confidence limit calculations support modeling step 12 above.),"Delete,Fact/Evidence",Fact/Evidence
6582,2-288,,2-288_v1_88@7,,"F1000s founder, Victor Tracz, is featured as the “Seer of Science Publishing”, prodding us to do better.","Delete,Fact/Evidence",Fact/Evidence
6583,2-288,,2-288_v1_104@1,,Software is also permanently available from: 10.5281/zenodo.7635 .,"Delete,Fact/Evidence",Fact/Evidence
6629,2-29,,2-29_v1_15@3,,In our case report we had dealt with such a huge cyst in a way not only to avoid morbidity of extending surgical incision but to guard against the risk of spillage of cyst contents as well.,"Delete,Fact/Evidence",Fact/Evidence
6630,2-29,,2-29_v1_15@5,,"However, there were two reports of large paraovarian cysts removed laparoscopically where in the first one, it was associated with acute lower abdominal pain while in the second it was associated with pregnancy <REF-11> , <REF-12> .","Delete,Fact/Evidence",Fact/Evidence
6631,2-29,,2-29_v1_15@6,,"We think that in all these laparoscopically operated cases, the implemented cyst decompression procedure before its removal had less control and precautions during it and in turn more risk of cyst spillage than our mentioned maneuver.","Delete,Claim",Claim
6686,2-9,,2-9_v1_41@1,,The ability of cells to undergo such nucleo-cytoplasmic conversion was not tumor-specific.,"Delete,Claim",Claim
9274,7-407,,7-407_v1_36@0,,"Even if with our search activity we are quite sure to have reduced to a minimum the problem of publication bias, we performed a statistical estimation by using the Copas selection model which is recommended by Jin et al . (2015) .","Delete,Fact/Evidence",Fact/Evidence
9275,7-407,,7-407_v1_55@0,,"The search method used and the small number of people interested in this research field, guarantee that from an empirical point of view, any publication bias is almost absent.","Delete,Claim",Claim
9276,7-407,,7-407_v1_66@0,,"We found very interesting evidence of presentiment distilled from the conventional post-stimulus psychological research of Jolij and Bierman, who have performed a long series of experiments using a face detection paradigm.","Delete,Claim",Claim
9277,7-407,,7-407_v1_66@1,,"Additionally, the work of Kittenis found prestimulus effects from a conventional research program and pre-registered single-trial work of Mossbridge represent an important conceptual replication in countering both the use of questionable research practices and expectancy effects arguments.","Delete,Claim",Claim
9278,7-407,,7-407_v1_67@0,,"A promising development of this line of research is the development of paradigms that use software in real-time to predict meaningful future outcomes before they occur, e.g. ( Franklin et al ., 2014 )","Delete,Claim",Claim
9292,7-407,,7-407_v1_9@8,,"Additionally, we also found increasing evidence of presentiment research piggybacking onto mainstream psychology programs, even informing aspects of the conventional research.","Delete,Fact/Evidence",Fact/Evidence
