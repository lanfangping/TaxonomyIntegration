edit_index,doc_name,node_ix_src,node_ix_tgt,text_src,text_tgt,gold,label
315,120-ARR,120-ARR_v2_27@2,,The basic definition is a set of features that are correlated but not causally related.,,"Add,Claim",Claim
316,120-ARR,120-ARR_v2_28@5,,"This definition does not address the nature of the feature (genuine or not), but does make an implicit assumption that such features are of high importance (e.g., high pointwise mutual information values with the corresponding label; Gururangan et al., 2018).",,"Add,Claim",Claim
317,120-ARR,120-ARR_v2_28@6,,"This definition is no longer subjective in terms of the genuineness of the feature, but is still subjective in the level of effect on generalizability (i.e., what is a high value of PMI?).",,"Add,Claim",Claim
318,120-ARR,120-ARR_v2_51@2,,"We argue that doing so is a desired strategy in many cases (though a preferred strategy might be to interact of abstain from making a decisive prediction, see Sec. 4.2).",,"Add,Claim",Claim
319,120-ARR,120-ARR_v2_52@0,,We also acknowledge that correlations in the real world can be misleading.,,"Add,Claim",Claim
320,120-ARR,120-ARR_v2_52@1,,"For instance, people often mistake the biggest commercial city in some countries for their capital (e.g., Istanbul in Turkey), potentially due to the high correlation between the two.",,"Add,Claim",Claim
321,120-ARR,120-ARR_v2_52@2,,"In such cases, relying on the fallback option might lead to prediction error.",,"Add,Claim",Claim
322,120-ARR,120-ARR_v2_52@3,,"However, we argue that following the human strategy of relying on a fallback option in cases of uncertainty will promote models' communication abilities.",,"Add,Claim",Claim
323,120-ARR,120-ARR_v2_62@0,,"7 Though we should continually assess the challenge negation poses on the most recent models (Bowman, 2022).",,"Add,Fact/Evidence",Fact/Evidence
324,120-ARR,120-ARR_v2_62@1,,"8 We recognize that editing pretrained corpora poses significant challenges due to their immense size, as demonstrated by recent efforts such as corpus analysis and deduplication (Lee et al., 2022).",,"Add,Fact/Evidence",Fact/Evidence
325,120-ARR,120-ARR_v2_63@0,,9 See Abend and Rappoport (2017) for a survey.,,"Add,Fact/Evidence",Fact/Evidence
326,120-ARR,120-ARR_v2_79@0,,"Finally, concurrent to this work, Eisenstein (2022) discussed several types of spurious correlations in the context of causality theory (Pearl, 2009), and used a toy example to demonstrate their different effects on models.",,"Add,Fact/Evidence",Fact/Evidence
327,120-ARR,120-ARR_v2_79@1,,"They concluded that domain knowledge is required to identify the correlations that are indeed spurious, i.e., those that might challenge the generalization ability of models.",,"Add,Fact/Evidence",Fact/Evidence
586,129-ARR,129-ARR_v2_2@6,,Our implementation is available at https://github.,,"Add,Fact/Evidence",Fact/Evidence
587,129-ARR,129-ARR_v2_70@0,,Active Learning Setups.,,"Add,Other",Other
588,129-ARR,129-ARR_v2_70@1,,"Following (Yuan et al., 2020), we set the number of rounds T = 10, the overall budget for all datasets b = 1000 and the initial size of the labeled |X l | is set to 100.",,"Add,Fact/Evidence",Fact/Evidence
589,129-ARR,129-ARR_v2_70@2,,"In each AL round, we sample a batch of 100 samples from the unlabeled set X u and query their labels.",,"Add,Fact/Evidence",Fact/Evidence
590,129-ARR,129-ARR_v2_70@4,,"For weakly-supervised text classification, since the datasets are much smaller, we keep the labeling budget and the size of development set to b = 500.",,"Add,Fact/Evidence",Fact/Evidence
591,129-ARR,129-ARR_v2_70@5,,Implementation Details.,,"Add,Other",Other
592,129-ARR,129-ARR_v2_70@6,,"We choose RoBERTabase (Liu et al., 2019) from the HuggingFace codebase (Wolf et al., 2020) as the backbone for AC-TUNE and all baselines except for Pubmed and Chemprot, where we use SciBERT (Beltagy et al., 2019), a BERT model pre-trained on scientific cor-pora.",,"Add,Fact/Evidence",Fact/Evidence
593,129-ARR,129-ARR_v2_82@0,,Case Study,,"Add,Other",Other
594,129-ARR,129-ARR_v2_85@4,,"Very recently, there are also several works attempted to query labeling functions for weakly-supervised learning (Boecking et al., 2020;Hsieh et al., 2022;Zhang et al., 2022b).",,"Add,Fact/Evidence",Fact/Evidence
595,129-ARR,129-ARR_v2_85@11,,"Self-training is one of the earliest and simplest approaches to semi-supervised learning (Lee, 2013).",,"Add,Fact/Evidence",Fact/Evidence
596,129-ARR,129-ARR_v2_88@0,,There are several directions to improve ACTUNE.,,"Add,Claim",Claim
597,129-ARR,129-ARR_v2_88@4,,"Moreover, we can explore more advanced uncertainty estimation approach (Kong et al., 2020) into ACTUNE to further improve the performance.",,"Add,Claim",Claim
754,136-ARR,136-ARR_v2_7@6,,"Moreover, the semantic accuracy metrics and our manual error analysis suggest that our approach offers a way to prevent omissions and hallucinations common in few-shot approaches.",,"Add,Claim",Claim
755,136-ARR,136-ARR_v2_11@0,,"Pipeline-based D2T Generation Until the recent surge of end-to-end approaches , using several modules connected in a pipeline was a major approach for D2T generation (Gatt and Krahmer, 2018;Reiter, 2007;Reiter and Dale, 1997).",,"Add,Fact/Evidence",Fact/Evidence
756,136-ARR,136-ARR_v2_11@1,,"Our approach is inspired by the pipeline approaches, in particular the pipelines utilizing neural modules (Ferreira et al., 2019).",,"Add,Fact/Evidence",Fact/Evidence
757,136-ARR,136-ARR_v2_11@2,,"In contrast with these approaches, our pipeline works with unstructured data in natural language and it operates in zero-shot setting, i.e. without using any training data from target D2T datasets.",,"Add,Fact/Evidence",Fact/Evidence
758,136-ARR,136-ARR_v2_11@3,,Laha et al. (2019) introduce a three-step pipeline for zero-shot D2T generation similar to ours.,,"Add,Fact/Evidence",Fact/Evidence
759,136-ARR,136-ARR_v2_17@0,,"In this section, we provide the formal description of our proposed approach.",,"Add,Fact/Evidence",Fact/Evidence
761,136-ARR,136-ARR_v2_18@0,,Our pipeline proceeds as follows.,,"Add,Fact/Evidence",Fact/Evidence
762,136-ARR,136-ARR_v2_19@0,,"(1) transform the triples into facts, which are sentences in natural language, (2) sort the facts using an ordering module, (3) insert sentence delimiters between the sorted facts using an aggregation module, (4) input the ordered sequence of facts with delimiters into a paragraph compression module, which generates the final description Y .",,"Add,Fact/Evidence",Fact/Evidence
763,136-ARR,136-ARR_v2_20@0,,Transforming Triples to Facts,,"Add,Other",Other
764,136-ARR,136-ARR_v2_21@3,,This step can be realized e.g. using a simple template for each predicate (cf. ยง5.1).,,"Add,Fact/Evidence",Fact/Evidence
765,136-ARR,136-ARR_v2_23@1,,"Note, however, that that F is a indeed set of meaningful sentences.",,"Add,Fact/Evidence",Fact/Evidence
766,136-ARR,136-ARR_v2_23@2,,We can use this to our advantage and apply a sentence ordering model to maximize the coherency of the paragraph resulting from their concatenation.,,"Add,Claim",Claim
767,136-ARR,136-ARR_v2_23@3,,"An example outcome of such operation may be grouping together facts mentioning birth date and birth place of a person, followed by their occupation (see Figure 1).",,"Add,Claim",Claim
768,136-ARR,136-ARR_v2_23@4,,The ordering module allows downstream modules to only focus on operations over neighboring sentences.,,"Add,Fact/Evidence",Fact/Evidence
769,136-ARR,136-ARR_v2_24@0,,"Formally, we apply the ordering model OpF q to get an ordered sequence of facts: F o "" tf o 1 , . . . , f on u, where o 1:n is a permutation of indices.",,"Add,Fact/Evidence",Fact/Evidence
770,136-ARR,136-ARR_v2_26@0,,Some facts will be typically mentioned together in a single sentence.,,"Add,Claim",Claim
771,136-ARR,136-ARR_v2_26@1,,"Considering the previous example, occupation is likely to be mentioned separately, while birth date and birth place are likely to be mentioned together.",,"Add,Claim",Claim
772,136-ARR,136-ARR_v2_26@2,,"Using an ordered sequence of facts as input, we can apply an aggregation model to decide which facts should be merged into a single sentence.",,"Add,Claim",Claim
773,136-ARR,136-ARR_v2_27@2,,We describe our aggregation model in ยง5.3.,,"Add,Fact/Evidence",Fact/Evidence
774,136-ARR,136-ARR_v2_29@0,,The paragraph compression (PC) model is a generative model which outputs the final text description.,,"Add,Fact/Evidence",Fact/Evidence
775,136-ARR,136-ARR_v2_32@0,,Here we descibe the process of building a largescale synthetic corpus WIKIFLUENT.,,"Add,Fact/Evidence",Fact/Evidence
776,136-ARR,136-ARR_v2_32@1,,"The corpus provides training data for the neural models which we use in our implementation of the ordering, aggregation, and paragraph compression modules (cf. ยง5).",,"Add,Fact/Evidence",Fact/Evidence
777,136-ARR,136-ARR_v2_35@1,,"Wikipedia is commonly used for large-scale pretraining of D2T generation models (Jin et al., 2020;Chen et al., 2020a).",,"Add,Fact/Evidence",Fact/Evidence
778,136-ARR,136-ARR_v2_35@2,,"Although it is not biasfree, it provides more balanced sample of natural language use than typical D2T generation datasets.",,"Add,Claim",Claim
779,136-ARR,136-ARR_v2_35@3,,"We used the first paragraphs of Wikipedia entries, which contain mostly concise, fact-based descriptions.",,"Add,Claim",Claim
780,136-ARR,136-ARR_v2_42@2,,We apply a split-and-rephrase model on each sentence in the paragraph and resolve coreferences in the split sentences.,,"Add,Fact/Evidence",Fact/Evidence
782,136-ARR,136-ARR_v2_42@3,,The result is a set of simple sentences which together convey the same meaning as the original paragraph.,,"Add,Fact/Evidence",Fact/Evidence
783,136-ARR,136-ARR_v2_42@4,,"The synthesized sentences are used as input into our models, the original human-written texts are used as ground truth.",,"Add,Fact/Evidence",Fact/Evidence
784,136-ARR,136-ARR_v2_44@2,,The process is illustrated in the upper part of Figure 2.,,"Add,Fact/Evidence",Fact/Evidence
785,136-ARR,136-ARR_v2_47@2,,Note that this procedure replaces the referring expressions only in the synthesized sentences (which are used as input) and keeps them in the original paragraphs (which are used as ground truth).,,"Add,Fact/Evidence",Fact/Evidence
786,136-ARR,136-ARR_v2_47@3,,"As a consequence, the paragraph compression module is implicitly trained to generate referring expressions in the final description.",,"Add,Fact/Evidence",Fact/Evidence
787,136-ARR,136-ARR_v2_50@0,,Implementation,,"Add,Other",Other
788,136-ARR,136-ARR_v2_51@0,,"In this section, we describe how we implement our pipeline modules ( ยง3) using simple template transformations ( ยง5.1) and neural models trained on the WIKIFLUENT dataset ( ยง5.2-5.4).",,"Add,Fact/Evidence",Fact/Evidence
789,136-ARR,136-ARR_v2_53@0,,We transform triples into facts ( ยง3.1) using a singletriple template t i for each predicate.,,"Add,Fact/Evidence",Fact/Evidence
790,136-ARR,136-ARR_v2_53@2,,"We follow previous work in which simple hand-crafted templates have been used as an efficient way of introducing domain knowledge (Kale and Rastogi, 2020a;Kasner and Duลกek, 2020a) template generation engines (Laha et al., 2019;Heidari et al., 2021;Mehta et al., 2021), the approach may produce less fluent outputs, but it minimizes manual workload and makes it easier to control the quality of the input for the subsequent steps.",,"Add,Claim",Claim
791,136-ARR,136-ARR_v2_70@0,,Here we expect that the model will learn to fuse the sentences between which there are no delimiters on the input.,,"Add,Claim",Claim
792,136-ARR,136-ARR_v2_72@0,,We train our pipeline modules on the WIKIFLU-ENT corpus as described in ยง5.,,"Add,Fact/Evidence",Fact/Evidence
793,136-ARR,136-ARR_v2_72@1,,"Next, we use these modules without finetuning for generating descriptions for RDF triples on two English D2T datasets, WebNLG and E2E.",,"Add,Fact/Evidence",Fact/Evidence
794,136-ARR,136-ARR_v2_79@0,,Our main aim is the evaluation of our pipeline on the downstream task of D2T generation.,,"Add,Claim",Claim
795,136-ARR,136-ARR_v2_79@3,,"In ยง7.4, we include an intrinsic evaluation of our modules on the WIKIFLUENT test set.",,"Add,Fact/Evidence",Fact/Evidence
796,136-ARR,136-ARR_v2_81@3,,"For WebNLG (see Table 3), we compare our systems with the results of:",,"Add,Fact/Evidence",Fact/Evidence
797,136-ARR,136-ARR_v2_83@2,,"Nevertheless, our systems still underperform the state-of-the-art supervised systems.",,"Add,Fact/Evidence",Fact/Evidence
798,136-ARR,136-ARR_v2_83@3,,"For this reason, we further focus on manual error analysis in ยง7.2 to pinpoint the current shortcomings of our approach.",,"Add,Fact/Evidence",Fact/Evidence
799,136-ARR,136-ARR_v2_85@0,,Manual Error Analysis,,"Add,Other",Other
800,136-ARR,136-ARR_v2_86@0,,"Since automatic performance metrics do not provide insights into specific weaknesses of the system (van Miltenburg et al., 2021), we manually examined 100 outputs of the models.",,"Add,Fact/Evidence",Fact/Evidence
801,136-ARR,136-ARR_v2_88@8,,This behavior is the main obstacle to ensure factual consistency of the output.,,"Add,Claim",Claim
802,136-ARR,136-ARR_v2_88@9,,"As a possible remedy, we propose explicitly controlling the semantics of sentence fusion (Ben-David et al., 2020), e.g. using a variant of constrained decoding (Balakrishnan et al., 2019;.",,"Add,Claim",Claim
803,136-ARR,136-ARR_v2_93@0,,Intrinsic Evaluation,,"Add,Other",Other
804,136-ARR,136-ARR_v2_94@0,,"Aside from the main D2T generation results, we also provide an intrinsic evaluation of our pipeline modules on the WIKIFLUENT test sets.",,"Add,Fact/Evidence",Fact/Evidence
805,136-ARR,136-ARR_v2_94@1,,"We evaluated the ordering, aggregation, and paragraph compression modules trained on the full WIKIFLUENT corpus.",,"Add,Fact/Evidence",Fact/Evidence
806,136-ARR,136-ARR_v2_94@2,,The results for both full and filtered test sets are summarized in Table 7.,,"Add,Fact/Evidence",Fact/Evidence
807,136-ARR,136-ARR_v2_94@3,,"The PC model achieves high scores, which follows from the fact that we provide it with ground truth content plans (i.e., the ordering and aggregation plan corresponding to the original paragraph).",,"Add,Fact/Evidence",Fact/Evidence
808,136-ARR,136-ARR_v2_94@4,,Accuracy of the ordering and aggregation modules is comparable to their performance on D2T datasets.,,"Add,Fact/Evidence",Fact/Evidence
809,136-ARR,136-ARR_v2_96@0,,Our experiments outline several possible future research directions.,,"Add,Claim",Claim
810,136-ARR,136-ARR_v2_96@2,,"The task of paragraph compression could be used as a task-specific pretraining (Gururangan et al., 2020) for more efficient finetuning of D2T models, e.g., with a small amount of clean data.",,"Add,Claim",Claim
811,136-ARR,136-ARR_v2_96@3,,"Consistency checks may be introduced in the pipeline to control the output from the modules, and individual modules may be improved by using more efficient model architectures.",,"Add,Claim",Claim
812,136-ARR,136-ARR_v2_98@0,,Conclusion,,"Add,Other",Other
813,136-ARR,136-ARR_v2_99@0,,We presented an approach for zero-shot D2T generation.,,"Add,Fact/Evidence",Fact/Evidence
814,136-ARR,136-ARR_v2_99@1,,The approach uses a pipeline of PLMs trained on general-domain lexical operations over natural language.,,"Add,Fact/Evidence",Fact/Evidence
815,136-ARR,136-ARR_v2_99@2,,The pipeline builds upon traditional approaches and consists of three interpretable intermediate steps.,,"Add,Fact/Evidence",Fact/Evidence
816,136-ARR,136-ARR_v2_99@3,,"By avoiding noisy human-written references from the D2T datasets, our models produce more semantically consitent output.",,"Add,Fact/Evidence",Fact/Evidence
817,136-ARR,136-ARR_v2_99@4,,We believe that training models for zeroshot D2T generation using large cross-domain corpora will help to build D2T generation systems with good performance across various domains.,,"Add,Claim",Claim
818,136-ARR,136-ARR_v2_100@0,,Limitations and Broader Impact,,"Add,Other",Other
819,136-ARR,136-ARR_v2_101@0,,We study zero-shot D2T generation with the focus on generating descriptions for RDF triples.,,"Add,Fact/Evidence",Fact/Evidence
820,136-ARR,136-ARR_v2_101@1,,"Although the task of D2T generation has numerous applications, using neural models for D2T generation (especially in the zero-shot context) is still limited to experimental settings (Dale, 2020).",,"Add,Claim",Claim
821,136-ARR,136-ARR_v2_101@2,,"Similarly to other recent approaches for D2T generation, our approach relies on PLMs, which are known to reflect the biases in their pretraining corpus (Bender et al., 2021;Rogers, 2021).",,"Add,Fact/Evidence",Fact/Evidence
822,136-ARR,136-ARR_v2_101@3,,Our system may therefore rely on spurious correlations for verbalizing e.g. gender or occupation of the entities.,,"Add,Claim",Claim
823,136-ARR,136-ARR_v2_101@4,,"Since we cannot guarantee the factual correctness of the outputs of our system, the outputs should be used with caution.",,"Add,Claim",Claim
824,136-ARR,136-ARR_v2_102@0,,"On the flip side, our approach helps to reduce the number of omissions and hallucinations stemming from noise in human-written references.",,"Add,Claim",Claim
825,136-ARR,136-ARR_v2_102@1,,Our work thus contributes to the general aim of D2T generation in conveying the data semantics accurately and without relying on implicit world knowledge.,,"Add,Claim",Claim
826,136-ARR,136-ARR_v2_104@2,,Training times were approximately 24 hours for the ordering model and 3 hours for the aggregation and paragraph compression models.,,"Add,Fact/Evidence",Fact/Evidence
1346,166-ARR,166-ARR_v2_20@2,,"For that reason these methods are used mainly for first-stage retrieval, followed by a reranking step.",,"Add,Claim",Claim
1347,166-ARR,166-ARR_v2_45@5,,It maps from article and section titles to relevant paragraphs.,,"Add,Fact/Evidence",Fact/Evidence
1348,166-ARR,166-ARR_v2_45@6,,"Following , we use the automatic by-article annotations variant, which considers all paragraphs within the same article as relevant.",,"Add,Fact/Evidence",Fact/Evidence
1490,172-ARR,172-ARR_v2_22@1,,"The text-image input (X i , Y i ) is paired, and every (X j , Y k ) (j ฬธ = k) is a non-pair.",,"Add,Fact/Evidence",Fact/Evidence
1491,172-ARR,172-ARR_v2_28@0,,"The Image-Text Matching (ITM) objective is widely used in multimodal learning (Tan and Bansal, 2020;Radford et al., 2021).",,"Add,Fact/Evidence",Fact/Evidence
1492,172-ARR,172-ARR_v2_28@1,,We modify this objective to same sentence prediction as both streams of our model takes text as input.,,"Add,Fact/Evidence",Fact/Evidence
1493,172-ARR,172-ARR_v2_32@1,,"Same as MLM, 15% of the tokens are randomly selected for reconstruction.",,"Add,Fact/Evidence",Fact/Evidence
1494,172-ARR,172-ARR_v2_32@2,,We address concerns on trivial solutions learned by the model in Section 5 and 9 in the appendix.,,"Add,Fact/Evidence",Fact/Evidence
1495,172-ARR,172-ARR_v2_41@2,,Entries where both models obtain the same performance are set aside.,,"Add,Fact/Evidence",Fact/Evidence
1496,172-ARR,172-ARR_v2_43@3,,"Swapping to wiki reduces potential overfitting from the 20 Epochs setting trained on wiki103, as training for the same amount of steps on wiki is less than 1 epoch.",,"Add,Fact/Evidence",Fact/Evidence
1497,172-ARR,172-ARR_v2_44@3,,"With the CLIPTC objective, the diversity of the input embeddings corresponding to different tokens must be retained in the cross-modal encoder, leading to more robust cross-modal attention.",,"Add,Claim",Claim
1677,178-ARR,178-ARR_v2_88@0,,"We set seeds of torch, torch.cuda, numpy, and random in Python to ensure reproducibility.",,"Add,Fact/Evidence",Fact/Evidence
1678,178-ARR,178-ARR_v2_88@1,,We use a grid search to find the best hyperparameters depending on development set performances.,,"Add,Fact/Evidence",Fact/Evidence
1679,178-ARR,178-ARR_v2_88@3,,"If the contextual embedding learning rate is 3e-5, we use static embedding learning rate and task learning rate as 5e-4 and 3e-5.",,"Add,Fact/Evidence",Fact/Evidence
1680,178-ARR,178-ARR_v2_89@0,,"We introduce the decomposition of triaffine scoring in calculating p i,j,r and p c i,j,r .",,"Add,Fact/Evidence",Fact/Evidence
2023,193-ARR,193-ARR_v2_25@0,,Character-Aware Models Some models are provided with the raw character sequence of each token.,,"Add,Fact/Evidence",Fact/Evidence
2024,193-ARR,193-ARR_v2_25@2,,"However, when no filter is applied, RoBERTa's character-oblivious but highlytuned training process produces embeddings that score higher on SpellingBee, presumably by leveraging implicit similarity functions in the embedding space.",,"Add,Claim",Claim
2025,193-ARR,193-ARR_v2_26@0,,"Although CharacterBERT's embedding layer is better at reconstructing original words (when similarity filters are applied), this does not mean that character-aware models are necessarily better downstream.",,"Add,Claim",Claim
2026,193-ARR,193-ARR_v2_26@1,,El Boukkouri et al. (2020) report performance increases only on the medical domain.,,"Add,Fact/Evidence",Fact/Evidence
2027,193-ARR,193-ARR_v2_27@0,,"In Section 5, we demonstrate that initializing a masked language model's embedding layer with character information has a negligible effect on its perplexity.",,"Add,Fact/Evidence",Fact/Evidence
2028,193-ARR,193-ARR_v2_28@0,,"The first generation of neural word representations (Mikolov et al., 2013a,b) contained only embedding layers, without any contextualization mechanism.",,"Add,Fact/Evidence",Fact/Evidence
2029,193-ARR,193-ARR_v2_28@1,,"We thus use GloVe (Pennington et al., 2014) to estimate a lower bound on character information that can be obtained by simple context-oblivious models.",,"Add,Fact/Evidence",Fact/Evidence
2030,193-ARR,193-ARR_v2_28@2,,We probe the first 50K words in GloVe's vocabulary with SpellingBee.,,"Add,Fact/Evidence",Fact/Evidence
2031,193-ARR,193-ARR_v2_28@3,,"Table 2 shows that GloVe embeddings do contain a weak orthographic signal, better than random embeddings, but substantially weaker than the information stored in the embedding layer of large transformer-based language models.",,"Add,Fact/Evidence",Fact/Evidence
2032,193-ARR,193-ARR_v2_32@0,,"Although there are many possible ways to explicitly add orthographic information to tokens embeddings, our method is relatively straightforward as it gives the model a chance to utilize pre-stored character information.",,"Add,Claim",Claim
2045,195-ARR,195-ARR_v2_60@2,,The final version of the annotation guidelines is included in the resource release (ict.fbk.eu/must-she) and is also retrievable at: https://bit.ly/3CdU50s.,,"Add,Fact/Evidence",Fact/Evidence
2046,195-ARR,195-ARR_v2_68@8,,Figure 6 shows coverage of fully generated agreement chains split into feminine (F) and masculine (M) forms.,,"Add,Fact/Evidence",Fact/Evidence
2061,195-ARR,195-ARR_v2_6@1,,"In fact, to be grammatically correct, each word in the chain has to be inflected with the same (masculine or feminine) gender form.",,"Add,Claim",Claim
2062,195-ARR,195-ARR_v2_17@1,,"As further discussed in in Section 4.2, such a feature enables fine-grained analyses of gender realization, which can also disentangle systems' tendency to (over)generate masculine -over feminine forms -in translation.",,"Add,Claim",Claim
2063,195-ARR,195-ARR_v2_32@4,,"Accordingly, we manage to make our study completely exhaustive by covering every gendermarked instance of MuST-SHE.",,"Add,Fact/Evidence",Fact/Evidence
2064,195-ARR,195-ARR_v2_32@5,,"Also, such additional manual evaluation serves as a proof-ofconcept to ensure the validity of the employed automatic evaluation metrics. .",,"Add,Claim",Claim
2065,195-ARR,195-ARR_v2_40@4,,"Also, note that the outcome of the manual analyses reiterates the results obtained with the automatic evaluation based on accuracy at the word-level, thus confirming its reliability.",,"Add,Fact/Evidence",Fact/Evidence
2365,21-ARR,21-ARR_v2_40@1,,Note that the student is never trained on the quiz set and the teacher only performs meta-update on the quiz set instead of fitting it.,,"Add,Fact/Evidence",Fact/Evidence
2366,21-ARR,21-ARR_v2_40@2,,We do not use a dynamic quiz set strategy because otherwise the student would have been trained on the quiz set and the loss would not be informative.,,"Add,Claim",Claim
2367,21-ARR,21-ARR_v2_44@3,,"For MRPC and QQP, we report both F1 and accuracy.",,"Add,Fact/Evidence",Fact/Evidence
2368,21-ARR,21-ARR_v2_44@4,,"For STS-B, we report Pearson and Spearman correlation.",,"Add,Fact/Evidence",Fact/Evidence
2369,21-ARR,21-ARR_v2_44@5,,The metric for CoLA is Matthew's correlation.,,"Add,Fact/Evidence",Fact/Evidence
2370,21-ARR,21-ARR_v2_44@6,,The other tasks use accuracy as the metric.,,"Add,Fact/Evidence",Fact/Evidence
2371,21-ARR,21-ARR_v2_63@3,,"Although our approach takes more time to achieve its own peak performance, it can match up the performance of PKD (Sun et al., 2019) with a similar time cost.",,"Add,Fact/Evidence",Fact/Evidence
2372,21-ARR,21-ARR_v2_67@0,,MetaDistil focuses on improving the performance of knowledge distillation and does not introduce extra ethical concerns compared to vanilla KD methods.,,"Add,Fact/Evidence",Fact/Evidence
2373,21-ARR,21-ARR_v2_67@1,,"Nevertheless, we would like to point out that as suggested by Hooker et al. (2020), model compression may lead to biases.",,"Add,Fact/Evidence",Fact/Evidence
2374,21-ARR,21-ARR_v2_67@2,,"However, this is not an outstanding problem of our method but a common risk in model compression, which needs to be addressed in the future.",,"Add,Claim",Claim
2375,21-ARR,21-ARR_v2_68@0,,"In MetaDistil, the student is trained in a dynamic manner.",,"Add,Fact/Evidence",Fact/Evidence
2376,21-ARR,21-ARR_v2_68@1,,"To investigate the effect of such a dynamic distillation process, we attempt to use the teacher at the end of MetaDistil training to perform a static conventional KD, to verify the effectiveness of our dynamic distillation strategy.",,"Add,Fact/Evidence",Fact/Evidence
2377,21-ARR,21-ARR_v2_68@2,,"As shown in Table 4, on both experiments, dynamic MetaDistil outperforms conventional KD and static distillation with the teacher at the end of MetaDistil training.",,"Add,Fact/Evidence",Fact/Evidence
2378,21-ARR,21-ARR_v2_68@3,,"As mentioned in Section 3.2, a meta teacher is optimized to transfer its knowledge to a specific student network.",,"Add,Fact/Evidence",Fact/Evidence
2379,21-ARR,21-ARR_v2_68@4,,"To justify this motivation, we conduct experiments using a teacher optimized for the ResNet-32 student to statically distill to the ResNet-20 student, and also in reverse.",,"Add,Fact/Evidence",Fact/Evidence
2380,21-ARR,21-ARR_v2_68@5,,"As shown in Table 4, the cross-taught students underperform the static students taught by their own teachers by 0.27 and 0.12 for ResNet-32 and ResNet-20, respectively.",,"Add,Fact/Evidence",Fact/Evidence
2381,21-ARR,21-ARR_v2_68@6,,This confirms our motivation that the meta teacher in MetaDistil can adjust itself according to its student.,,"Add,Fact/Evidence",Fact/Evidence
2382,21-ARR,21-ARR_v2_70@0,,"We also investigate why MetaDistil works by conducting experiments on the development sets of MNLI, SST, and MRPC, which are important tasks in GLUE that have a large, medium, and small training set, respectively.",,"Add,Fact/Evidence",Fact/Evidence
2383,21-ARR,21-ARR_v2_70@1,,"We illustrate the validation accuracy curves of the meta teacher and student models with training steps in Figure 5, and compare them to the student performance in conventional KD.",,"Add,Fact/Evidence",Fact/Evidence
2384,21-ARR,21-ARR_v2_70@2,,"We can see that the meta teacher maintains high accuracy in the first 5,000 steps and then begins to slowly degrade.",,"Add,Fact/Evidence",Fact/Evidence
2385,21-ARR,21-ARR_v2_70@3,,"Starting from step 8,000, the teacher model underperforms the student while the student's accuracy keeps increasing.",,"Add,Fact/Evidence",Fact/Evidence
2386,21-ARR,21-ARR_v2_70@4,,This verifies our assumption that a model with the best accuracy is not necessarily the optimal teacher.,,"Add,Fact/Evidence",Fact/Evidence
2387,21-ARR,21-ARR_v2_71@0,,"While MetaDistil achieves improved student accuracy on the GLUE benchmark, it is still not very clear where the performance improvement comes from.",,"Add,Claim",Claim
2388,21-ARR,21-ARR_v2_71@1,,"There are two possibilities: (1) the student better mimics the teacher, and (2) the changes of teacher helps student perform better on hard examples that would be incorrectly classified by the student with vanilla KD.",,"Add,Claim",Claim
2389,21-ARR,21-ARR_v2_71@2,,We conduct a series of analysis on the MRPC dataset.,,"Add,Fact/Evidence",Fact/Evidence
2390,21-ARR,21-ARR_v2_72@0,,"For the first assumption, we compute the prediction loyalty (Xu et al., 2021a) of the student model distilled with PKD and MetaDistil, respectively.",,"Add,Fact/Evidence",Fact/Evidence
2391,21-ARR,21-ARR_v2_72@1,,"For MetaDistil, we measure the loyalty with respect to both the original teacher and the final teacher.",,"Add,Fact/Evidence",Fact/Evidence
2392,21-ARR,21-ARR_v2_72@2,,We find that there is no significant difference between between PKD and MetaDistil.,,"Add,Fact/Evidence",Fact/Evidence
2393,21-ARR,21-ARR_v2_72@3,,This suggests that the improvement does not come from student better mimicking the teacher.,,"Add,Claim",Claim
2394,21-ARR,21-ARR_v2_73@0,,"For the second assumption, we first identify the examples in the quiz set for which our model gives correct predictions while the student distilled by PKD makes a wrong prediction.",,"Add,Fact/Evidence",Fact/Evidence
2683,246-ARR,246-ARR_v2_31@1,,The ฯ value quantifies how similar two system rankings would be if they were computed with two random sets of M input documents.,,"Add,Claim",Claim
2684,246-ARR,246-ARR_v2_31@2,,"When all M test test instances are used, the automatic metrics' rankings become near constant.",,"Add,Claim",Claim
2685,246-ARR,246-ARR_v2_31@3,,The error regions represent ยฑ1 standard deviation.,,"Add,Fact/Evidence",Fact/Evidence
2686,246-ARR,246-ARR_v2_76@0,,"Figs. 10 and 11 contain the r SYS โ( , u) correlations for ROUGE, BERTScore, and QAEval for various combinations of and u on both the Summ-Eval and REALSumm datasets.",,"Add,Fact/Evidence",Fact/Evidence
2687,246-ARR,246-ARR_v2_77@0,,"To estimate the difference in ROUGE-1 score that is commonly reported in papers, we performed a survey of recently published summarization papers.",,"Add,Fact/Evidence",Fact/Evidence
2688,246-ARR,246-ARR_v2_77@1,,"We selected papers from 2020 and 2021 that were published in a *ACL conference (including Findings), had ""summary"" or ""summarization"" in the title, proposed a new system, and compared systems on the CNN/DailyMail dataset with ROUGE.",,"Add,Fact/Evidence",Fact/Evidence
2689,246-ARR,246-ARR_v2_77@2,,We selected the differences between the best two models that were compared with ROUGE on the test set.,,"Add,Fact/Evidence",Fact/Evidence
2690,246-ARR,246-ARR_v2_77@3,,We did not include ablation experiments for which the differences are likely smaller than the differences between the top two performing systems.,,"Add,Claim",Claim
2691,246-ARR,246-ARR_v2_77@4,,The results are shown in Table 1.,,"Add,Fact/Evidence",Fact/Evidence
2692,246-ARR,246-ARR_v2_77@5,,The average reported difference was found to be 0.49 ROUGE-1.,,"Add,Fact/Evidence",Fact/Evidence
2703,249-ARR,249-ARR_v2_27@2,,"Lastly, we show the overall training objective in Section 4.3.",,"Add,Fact/Evidence",Fact/Evidence
2747,250-ARR,250-ARR_v2_46@4,,"We re-implement PKD (Sun et al., 2019) and ALP-KD approaches using the default settings proposed in the respective papers.",,"Add,Fact/Evidence",Fact/Evidence
2748,250-ARR,250-ARR_v2_46@5,,"We used early stopping based on performance on the development set, while making sure that the figures are in line with the ones reported in the papers.",,"Add,Fact/Evidence",Fact/Evidence
2749,250-ARR,250-ARR_v2_46@6,,"More precisely, the best layer setting for PKD teacher BERT 12 is {2, 4, 6, 8, 10} to distill into DistilBERT 6 .",,"Add,Fact/Evidence",Fact/Evidence
2750,250-ARR,250-ARR_v2_46@7,,"For DistilRoBERTa 6 , we choose the intermediate layers 4, 8, 12, 16, 20 from the teacher RoBERTa 24 model for distillation that we found to work the best on the development set.",,"Add,Fact/Evidence",Fact/Evidence
2751,250-ARR,250-ARR_v2_47@0,,"Using ALP-KD, we compute attention weights for the intermediate layers of the teacher (i.e., 1 to 11 for BERT 12 and 1 to 23 for RoBERTa 24 models) to calculate the weighted intermediate representations of the teacher for each intermediate layer of the student model (i.e., 1 to 5 layers of the student models).",,"Add,Fact/Evidence",Fact/Evidence
2752,250-ARR,250-ARR_v2_47@1,,"Since, the hidden dimensions of the RoBERTa 24 and DistilRoBERTa 6 are different, we linearly transform them into same lowerdimensional space.",,"Add,Fact/Evidence",Fact/Evidence
2753,250-ARR,250-ARR_v2_48@0,,"For RAIL-KD l , at each epoch we randomly select 5 layers from the intermediate layers of the teacher (i.e., from layers 1 to 11 for BERT 12 model and 1 to 23 for RoBERTa 24 model).",,"Add,Fact/Evidence",Fact/Evidence
2754,250-ARR,250-ARR_v2_48@2,,"We fixed ฮฑ i = 1, ฮป 1 , ฮป 2 , ฮป 3 = 1/3 for our proposed approaches 4 .",,"Add,Fact/Evidence",Fact/Evidence
2755,250-ARR,250-ARR_v2_48@3,,"We search learning rate from {1e-5, 2e-5, 5e-5, 4e-6}, batch size from {8, 16, 32}, and fixed the epoch number to 40 for all the experiments.",,"Add,Fact/Evidence",Fact/Evidence
2756,250-ARR,250-ARR_v2_48@5,,"We ran all the experiments on a single NVIDIA V100 GPU using mixed-precision training (Micikevicius et al., 2018) and PyTorch (Paszke et al., 2019) framework.",,"Add,Fact/Evidence",Fact/Evidence
2757,250-ARR,250-ARR_v2_61@1,,"As Table 6 shows, the variances of RAIL-KD is in the same range for each task, for instance, RAIL-KD variance is at the same scale compared with PKD and ALP-KD on CoLA and MRPC, and even lower on RTE.",,"Add,Fact/Evidence",Fact/Evidence
2758,250-ARR,250-ARR_v2_61@2,,"This indicates that the gains of RAIL-KD are significant, and are not due to chance in our random selection of layers to distill.",,"Add,Claim",Claim
2759,250-ARR,250-ARR_v2_67@4,,Similar trends are seen from the other layers.,,"Add,Claim",Claim
2760,250-ARR,250-ARR_v2_69@2,,The figure clearly shows (light colors) that most of ALP weights are concentrated on top layers of the teacher.,,"Add,Fact/Evidence",Fact/Evidence
2761,250-ARR,250-ARR_v2_69@3,,"For instance, layers 1,2,5 of the three students mostly attend to the last layer of BERT 12 .",,"Add,Fact/Evidence",Fact/Evidence
2841,258-ARR,258-ARR_v2_4@1,,"They are often used in language learning environments as a quick and effective way to test vocabulary, grammar and reading comprehension (Tremblay, 2011;Trace, 2020).",,"Add,Fact/Evidence",Fact/Evidence
2842,258-ARR,258-ARR_v2_17@4,,We believe that this discrimination objective makes it more suitable for our token classification task.,,"Add,Claim",Claim
2843,258-ARR,258-ARR_v2_17@5,,"Moreover, we also exploit ELECTRA's generation capabilities as a language model for estimating the answers to the proposed gaps as an auxiliary task.",,"Add,Claim",Claim
2844,258-ARR,258-ARR_v2_40@0,,"Standard ELECTRA Similar to BERT, it predicts potentially good gaps using a standard pre-trained ELECTRA-base model.",,"Add,Fact/Evidence",Fact/Evidence
2845,258-ARR,258-ARR_v2_40@1,,This is a single-objective model that is fine-tuned on token classification only.,,"Add,Fact/Evidence",Fact/Evidence
2846,258-ARR,258-ARR_v2_48@5,,We also perform an ablation study in Table 3 where we compare our multi-objective ELECTRA model to a standard one that does not include our auxiliary language model objective.,,"Add,Fact/Evidence",Fact/Evidence
2847,258-ARR,258-ARR_v2_48@6,,"Results show that the former outperforms the latter on all metrics, confirming that the addition of the LM objective is clearly beneficial.",,"Add,Fact/Evidence",Fact/Evidence
2875,276-ARR,276-ARR_v2_44@5,,See Fig. 3 in the Appendix for various Laplace-based distributions sampled with different techniques including possible distributions sampled in DPText.,,"Add,Fact/Evidence",Fact/Evidence
2876,276-ARR,276-ARR_v2_64@0,,Ethics Statement,,"Add,Other",Other
2877,276-ARR,276-ARR_v2_65@0,,"We declare no conflict of interests with the authors of DPText, we do not even know them personally.",,"Add,Claim",Claim
2878,276-ARR,276-ARR_v2_65@1,,The purpose of this paper is strictly scientific.,,"Add,Claim",Claim
3081,30-ARR,30-ARR_v2_18@2,,"Then, all model-identified rationales (underlined texts in Figure 2) are examined by human annotators to identify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model).",,"Add,Fact/Evidence",Fact/Evidence
3082,30-ARR,30-ARR_v2_18@3,,Both false rationales and missing rationales are corrected to produce augmented examples.,,"Add,Fact/Evidence",Fact/Evidence
3083,30-ARR,30-ARR_v2_18@4,,"Finally, newly generated examples are added into the training set to re-train the deep learning model.",,"Add,Fact/Evidence",Fact/Evidence
3086,30-ARR,30-ARR_v2_46@0,,"To address RQ1, we compare the performance of models trained by the static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static.",,"Add,Fact/Evidence",Fact/Evidence
3087,30-ARR,30-ARR_v2_46@1,,"We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full.",,"Add,Fact/Evidence",Fact/Evidence
3092,30-ARR,30-ARR_v2_65@2,,"In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.",,"Add,Claim",Claim
3093,30-ARR,30-ARR_v2_66@0,,Ethical Statement,,"Add,Other",Other
3094,30-ARR,30-ARR_v2_67@0,,We honor the ACL Code of Ethics.,,"Add,Claim",Claim
3095,30-ARR,30-ARR_v2_67@1,,No private data or non-public information was used in this work.,,"Add,Fact/Evidence",Fact/Evidence
3096,30-ARR,30-ARR_v2_67@2,,All annotators have received labor fees corresponding to the amount of their annotated instances.,,"Add,Fact/Evidence",Fact/Evidence
3288,315-ARR,315-ARR_v2_31@3,,"Note that Drop#1 is mainly due to the the fact that there are mostly unknown tokens (i.e., <UNK>) in the bucket 2.",,"Add,Claim",Claim
3289,315-ARR,315-ARR_v2_31@4,,We leave detailed discussions about buckets and Drop#1 to Appendix C.,,"Add,Fact/Evidence",Fact/Evidence
3290,315-ARR,315-ARR_v2_41@1,,"For each translated sentence constraints C tgt = (C 1 , C 2 , ..., C k ), we use an external alignment tool external aligner, such as GIZA++ (Brown et al., 1993;Och and Ney, 2003), to find the corresponding source words, denoted as ment embedding that comes from C src and C tgt .",,"Add,Fact/Evidence",Fact/Evidence
3291,315-ARR,315-ARR_v2_74@0,,"In this section, we dig further into the buckets in self-constrained translation ( ยง3.3, ยง6.1), especially for understanding why Drop#1 happens.",,"Add,Fact/Evidence",Fact/Evidence
3292,315-ARR,315-ARR_v2_75@0,,"As seen in Table 9, we categorize and count the constraints into five classes based on their Part-Of-Speech tagging with NLTK (Bird et al., 2009).",,"Add,Fact/Evidence",Fact/Evidence
3293,315-ARR,315-ARR_v2_75@1,,"We find that, 1) punctuation (PUNK) dominates bucket 1; 2) as the constraint frequency decreases (from bucket 1 to bucket 6), the number of constraints identified as nouns (NN*) grows; 3) bucket 2 has the most UNK constraints.",,"Add,Fact/Evidence",Fact/Evidence
3294,315-ARR,315-ARR_v2_75@2,,"The third finding is because, as the BPE training was only done on the training set of the datasets, there will be <UNK> on the target side of the test set.",,"Add,Fact/Evidence",Fact/Evidence
3295,315-ARR,315-ARR_v2_75@3,,"Thus, cases in bucket 2 have a relatively large number of UNK tokens as constraints, resulting in the Drop#1.",,"Add,Fact/Evidence",Fact/Evidence
3296,315-ARR,315-ARR_v2_76@0,,"To give a clearer view about how is UNK causing Drop#1, we exclude samples with UNK as constraints, and obtain a revised self-constrained translation results, as in Figure 5.",,"Add,Fact/Evidence",Fact/Evidence
3297,315-ARR,315-ARR_v2_76@1,,"Clearly, Drop#1 disappears in the given setting.",,"Add,Fact/Evidence",Fact/Evidence
3298,315-ARR,315-ARR_v2_76@2,,"Of course, Drop#2 still verifies our claim in the paper.",,"Add,Fact/Evidence",Fact/Evidence
3336,32-ARR,32-ARR_v2_2@7,,The code will be made available at: https://github.com/yiren-jian/EmbedHalluc.,,"Add,Fact/Evidence",Fact/Evidence
3337,32-ARR,32-ARR_v2_10@7,,"Learning from limited labeled data (few-shot learning) in Computer Vision is usually achieved by meta-learning (Ren et al., 2018a,b;Jian et al., 2020;Jian and Gao, 2021) or transfer learning (Tian et al., 2020).",,"Add,Fact/Evidence",Fact/Evidence
3338,32-ARR,32-ARR_v2_22@1,,The pseudo-code for finetuning of few-shot language learners with hallucinated embeddings is shown in Algorithm 1.,,"Add,Fact/Evidence",Fact/Evidence
3339,32-ARR,32-ARR_v2_26@1,,The evaluations are conducted by averaging results on 5 different train test splits.,,"Add,Fact/Evidence",Fact/Evidence
3340,32-ARR,32-ARR_v2_26@2,,We sample 16 examples per class to form a training set and construct a validation set with the same size as the training set.,,"Add,Fact/Evidence",Fact/Evidence
3341,32-ARR,32-ARR_v2_27@0,,Training Details for Embedding Hallucinators,,"Add,Other",Other
3342,32-ARR,32-ARR_v2_37@1,,"The discriminator is a 3blocks model, each bock having a sequence of FullyConnect-BatchNorm-LeakyReLU with the same hidden dimension of 512.",,"Add,Fact/Evidence",Fact/Evidence
3343,32-ARR,32-ARR_v2_38@0,,"We train the Embedding Hallucinators for 150 epochs using a batch size of 64, the Adam optimizer (ฮฒ = (0.5, 0.999)), and a learning rate of 0.0002.",,"Add,Fact/Evidence",Fact/Evidence
3344,32-ARR,32-ARR_v2_38@1,,The real embeddings are collected from the language few-shot training set by passing text into the embedding layer of the language model.,,"Add,Fact/Evidence",Fact/Evidence
3345,32-ARR,32-ARR_v2_38@2,,We apply gradient penalty with weight of loss 100 for training the cWGAN.,,"Add,Fact/Evidence",Fact/Evidence
3346,32-ARR,32-ARR_v2_39@0,,Training Details for Few-Shot Language Learners,,"Add,Other",Other
3347,32-ARR,32-ARR_v2_40@0,,"We draw two mini-batches during the training of our few-shot language learners, i.e., one from the real language few-shot training set, another one by sampling the hallucinators (see Algorithm 1).",,"Add,Fact/Evidence",Fact/Evidence
3348,32-ARR,32-ARR_v2_52@0,,Comparing to Adversarial Training,,"Add,Other",Other
3349,32-ARR,32-ARR_v2_53@0,,Adversarial training adds noise into the training data to increase the robustness of a model.,,"Add,Claim",Claim
3350,32-ARR,32-ARR_v2_53@1,,It has been shown that adversarial training can also improve the performance of language models.,,"Add,Claim",Claim
3351,32-ARR,32-ARR_v2_61@0,,"Here, we provide best learning rates (LR, searched from 1e โ5 , 5e โ6 , 1e โ6 as discussed in main paper) for L halluc of EmbedHalluc for each task used in RoBERTa-large prompt-based fine-tuning.",,"Add,Fact/Evidence",Fact/Evidence
3389,326-ARR,326-ARR_v2_57@0,,Binarization Statistics,,"Add,Other",Other
3565,344-ARR,344-ARR_v2_6@10,,"These labels can be seen as sub-intent annotations, where their combinations yield full intents equivalent to ""traditional"" intents (Table 1).",,"Add,Fact/Evidence",Fact/Evidence
3566,344-ARR,344-ARR_v2_23@5,,"For instance, if (i) examples with the intents change and booking, and (ii) examples with the intents cancel and account exist in the training data, (iii) an unseen example with the intents cancel and booking could be properly predicted, as all the single intents/modules have already been seen by the ID model 9 .",,"Add,Claim",Claim
3567,344-ARR,344-ARR_v2_23@7,,"For example, the module overdraft is clearly related to BANKING, but the module change is much more generic, likely to occur in several different domains.",,"Add,Claim",Claim
3568,344-ARR,344-ARR_v2_61@3,,"Further, the boundaries of some generic intents can sometimes be unclear and difficult to annotate, even for expert annotators.",,"Add,Claim",Claim
3569,344-ARR,344-ARR_v2_61@4,,18 Future work should try to ground the set of generic intents.,,"Add,Claim",Claim
3570,344-ARR,344-ARR_v2_62@0,,"Further, we believe that span-based annotation might be sub-optimal for canonical values such as times and dates, where small differences in the span would lead to evaluation errors but would not suppose a problem for the value to be parsed.",,"Add,Claim",Claim
3571,344-ARR,344-ARR_v2_62@1,,"In addition, separating time and date intervals in different slots increases the difficulty of the annotations and models need to learn a more conflicting set of slots.",,"Add,Claim",Claim
3572,344-ARR,344-ARR_v2_63@0,,"Finally, while single-turn NLU is more dataefficient and easier to model, some user utterances only make sense in the presence of context from the previous system utterance.",,"Add,Claim",Claim
3573,344-ARR,344-ARR_v2_63@1,,"While some previous datasets (Coope et al., 2020) deal with this issue with the help of extra annotations indicating if a slot has been requested, in this work we opt for using non-contextualised slots such as number and time and let the policy handle the contextualisation.",,"Add,Fact/Evidence",Fact/Evidence
3574,344-ARR,344-ARR_v2_63@2,,"However, future work should start looking into NLU datasets composed by system + user turns.",,"Add,Claim",Claim
3725,350-ARR,350-ARR_v2_6@2,,"We think it fair to include such models, as we aim at a practical solution for the transcription problem at hand, regardless of the underlying approach.",,"Add,Claim",Claim
3726,350-ARR,350-ARR_v2_53@0,,It was feasible to train HMM-GMM with 10 different random train/test partitions 13 and compute the Student's t 95% uncertainty intervals shown in the PER column.,,"Add,Fact/Evidence",Fact/Evidence
3727,350-ARR,350-ARR_v2_53@1,,The uncertainty remains relatively small even for the smallest datasets which contain only a few minutes of test speech.,,"Add,Fact/Evidence",Fact/Evidence
4084,40-ARR,40-ARR_v2_43@2,,"Including gradient information to explain Transformers has recently been considered to improve their interpretability (Chefer et al., 2021b,a;Ali et al., 2022).",,"Add,Fact/Evidence",Fact/Evidence
4107,402-ARR,402-ARR_v2_6@4,,"In contrast, WatClaim-Check includes claims investigated by 8 fact checking organizations on any topic.",,"Add,Fact/Evidence",Fact/Evidence
4108,402-ARR,402-ARR_v2_16@5,,The premise articles include the source document of the claim when available as well as evidence articles used by fact checkers.,,"Add,Fact/Evidence",Fact/Evidence
4109,402-ARR,402-ARR_v2_37@3,,"This corresponds to the ""gold"" negative sampling technique in (Karpukhin et al., 2020).",,"Add,Fact/Evidence",Fact/Evidence
4110,402-ARR,402-ARR_v2_39@2,,These negative sentences are positive sentences for other claims within the same batch.,,"Add,Fact/Evidence",Fact/Evidence
4111,402-ARR,402-ARR_v2_56@1,,"The test contains a total of 114, 290 sentences and 3, 373 claims.",,"Add,Fact/Evidence",Fact/Evidence
4113,402-ARR,402-ARR_v2_60@0,,The evidence sentences are concatenated in the descending order of their similarity score.,,"Add,Fact/Evidence",Fact/Evidence
4114,402-ARR,402-ARR_v2_60@1,,"Afterwards, the claim text and evidence sentences are concatenated.",,"Add,Fact/Evidence",Fact/Evidence
4115,402-ARR,402-ARR_v2_60@2,,The resulting text is then truncated to the maximum sequence length capability of the transformer model being used to perform claim veracity inference.,,"Add,Fact/Evidence",Fact/Evidence
4116,402-ARR,402-ARR_v2_61@3,,We concatenate the evidence sentences in the descending order of their similarity score.,,"Add,Fact/Evidence",Fact/Evidence
4228,412-ARR,412-ARR_v2_4@0,,"The overarching goal of the complex word identification (CWI) task is to find words that can be simplified in a given text (Paetzold and Specia, 2016b).",,"Add,Fact/Evidence",Fact/Evidence
4229,412-ARR,412-ARR_v2_76@5,,"Third, Gooding and Kochmar (2018) approach the English section of the dataset by employing linear regressions.",,"Add,Fact/Evidence",Fact/Evidence
4230,412-ARR,412-ARR_v2_76@6,,"The authors used several types of handcrafted features, including word n-grams, POS tags, dependency parse relations, and psycholinguistic features.",,"Add,Fact/Evidence",Fact/Evidence
4231,412-ARR,412-ARR_v2_77@0,,Table 3 presents the results obtained on the multilingual validation dataset and compares the performance of different configurations.,,"Add,Fact/Evidence",Fact/Evidence
4269,413-ARR,413-ARR_v2_25@5,,"Note that we do not experiment with the setting of training with all the combined training data from the corpora as it has been pointed out by previous work that doing so hurts the performance of the models (Jimรฉnez-Zafra et al., 2020;Barnes et al., 2021) due to differences in annotation schemes between the corpora introducing noise during training.",,"Add,Fact/Evidence",Fact/Evidence
4270,413-ARR,413-ARR_v2_25@10,,"Note that for NegBERT, we also replace the BERT encoder with RoBERTa to ensure results are comparable between the models.",,"Add,Fact/Evidence",Fact/Evidence
4271,413-ARR,413-ARR_v2_27@3,,"4 To provide a more general view, we summarize the results in Table 4.",,"Add,Fact/Evidence",Fact/Evidence
4272,413-ARR,413-ARR_v2_27@4,,"In general, we observe gains in both the same-dataset setting (training and test set belongs to one corpus) and cross-dataset setting (training one one training set and testing on all others test sets) for both of the proposed models, with CueNB achieving the largest gains.",,"Add,Fact/Evidence",Fact/Evidence
4273,413-ARR,413-ARR_v2_31@0,,We conducted an error analysis on the VetCompass validation set to see what qualitative improvement CueNB makes over NegBERT.,,"Add,Fact/Evidence",Fact/Evidence
4274,413-ARR,413-ARR_v2_31@1,,"For cue detection, there are two main types of errors that CueNB helps alleviate.",,"Add,Fact/Evidence",Fact/Evidence
4275,413-ARR,413-ARR_v2_31@2,,"First, CueNB can detect more unique cues such as negative, won't, and also multiword cues like no longer.",,"Add,Fact/Evidence",Fact/Evidence
4276,413-ARR,413-ARR_v2_31@3,,"Second, CueNB is able to recognize cases when the negations are actually just speculative.",,"Add,Fact/Evidence",Fact/Evidence
4277,413-ARR,413-ARR_v2_31@4,,"For example, in the sentence O reports has smelled for past week, not sure if anal glands . . . , the word not is part of the speculation phrase not sure, indicating that this is not truly a negation phrase but rather expresses uncertainty.",,"Add,Claim",Claim
4278,413-ARR,413-ARR_v2_31@5,,"For scope resolution, CueNB mostly helps in recognizing the correct scope boundary.",,"Add,Claim",Claim
4279,413-ARR,413-ARR_v2_31@6,,One common case is when the cue relates to multiple spans in a sentence.,,"Add,Claim",Claim
4280,413-ARR,413-ARR_v2_31@8,,It also helps in cases where there are multiple separate negations in the same sentence.,,"Add,Claim",Claim
4427,426-ARR,426-ARR_v2_65@2,,Reference codes can be used to access any of the baseline solutions at http: //gonito.net/q.,,"Add,Fact/Evidence",Fact/Evidence
4428,426-ARR,426-ARR_v2_78@3,,"Since we did not train RoBERTa ChallAm large, we cannot confirm this holds true, when it comes to large RoBERTa models.",,"Add,Fact/Evidence",Fact/Evidence
4429,426-ARR,426-ARR_v2_78@4,,"The standard RoBERTa large is the best performing model, so in this case, a larger model is better even if not trained on the data from different domain.",,"Add,Claim",Claim
4430,426-ARR,426-ARR_v2_80@8,,The tags along with the probabilities are available in the hate-speech-info.tsv files for each test directory.,,"Add,Fact/Evidence",Fact/Evidence
4431,426-ARR,426-ARR_v2_81@0,,Note that temporal and geospatial metadata might constitute useful features in future work on better detection of hate speech in historical texts.,,"Add,Claim",Claim
4734,463-ARR,463-ARR_v2_16@5,,"During the training stage, we freeze ฮธ and tune ฮธ only.",,"Add,Fact/Evidence",Fact/Evidence
4735,463-ARR,463-ARR_v2_35@0,,Experimental settings,,"Add,Other",Other
4736,463-ARR,463-ARR_v2_36@0,,"We base our experiments on HuggingFace PyTorch implementation (Wolf et al., 2019) of BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019c) 2.",,"Add,Fact/Evidence",Fact/Evidence
4737,463-ARR,463-ARR_v2_36@1,,We experiment with 3 random seeds and choose the seed with the best performance on the validation set to evaluate on the GLUE server.,,"Add,Fact/Evidence",Fact/Evidence
4738,463-ARR,463-ARR_v2_36@2,,We report the test metrics provided on the submission website 2 .,,"Add,Fact/Evidence",Fact/Evidence
4739,463-ARR,463-ARR_v2_38@1,,"In Table 1, we report the test scores on the GLUE benchmark and the required new parameters per task.",,"Add,Fact/Evidence",Fact/Evidence
4740,463-ARR,463-ARR_v2_38@2,,Here we use BERTlarge as the PLM.,,"Add,Fact/Evidence",Fact/Evidence
4741,463-ARR,463-ARR_v2_38@3,,"AdapterBias reaches 81.2 average score in GLUE benchmark, with the smallest amount of parameters (0.17M) added per task.",,"Add,Fact/Evidence",Fact/Evidence
4742,463-ARR,463-ARR_v2_38@5,,The settings are the same as in Table 1.,,"Add,Fact/Evidence",Fact/Evidence
4743,463-ARR,463-ARR_v2_38@6,,The Full-FT corresponds to fine-tuning the whole PLM without adding adapters.,,"Add,Fact/Evidence",Fact/Evidence
4744,463-ARR,463-ARR_v2_53@4,,The settings are the same as in Table 1.,,"Add,Fact/Evidence",Fact/Evidence
4745,463-ARR,463-ARR_v2_53@5,,The Full-FT represents finetuning the whole PLM without adding adapters.,,"Add,Fact/Evidence",Fact/Evidence
4775,465-ARR,465-ARR_v2_15@0,,GCN also provides a powerful toolkit for embedding the taxonomies into low dimension representations that could be utilized for specific tasks.,,"Add,Fact/Evidence",Fact/Evidence
4776,465-ARR,465-ARR_v2_15@1,,"For instance, Pujary et al. (2020) used GCN to learn an undirected graph derived from disease names in the MeSH taxonomy in order to detect and normalize disease mentions in biomedical texts.",,"Add,Fact/Evidence",Fact/Evidence
4777,465-ARR,465-ARR_v2_29@0,,"MeSH taxonomies are organized in 16 categories, and each is further divided into subcategories.",,"Add,Fact/Evidence",Fact/Evidence
4778,465-ARR,465-ARR_v2_29@1,,"Within each subcategory, MeSH terms are ordered hierarchically from most general to most specific, up to 13 hierarchical levels.",,"Add,Fact/Evidence",Fact/Evidence
4779,465-ARR,465-ARR_v2_52@0,,"If the label appears in M , we assign 1, 0 otherwise.",,"Add,Fact/Evidence",Fact/Evidence
4780,465-ARR,465-ARR_v2_66@1,,"We also download the entire MEDLINE collection based on the PubMed Annual Baseline Repository (as of Dec. 2020) and obtain 31,850,051 citations with titles and abstracts.",,"Add,Fact/Evidence",Fact/Evidence
4781,465-ARR,465-ARR_v2_66@2,,"In order to reduce bias, we only focus on articles that are annotated by human curators (not annotated by a 'curated' or 'auto' modes in MEDLINE).",,"Add,Fact/Evidence",Fact/Evidence
4782,465-ARR,465-ARR_v2_66@3,,"We then match PMC articles with the citations in PubMed to PMID and obtain a set of 1,284,308 citations.",,"Add,Fact/Evidence",Fact/Evidence
4783,465-ARR,465-ARR_v2_66@4,,"Out of these PMC articles, we use the latest 20,000 articles as the test set, the next latest 200,000 articles as the validation data set, and the remaining 1.24M articles as the training set.",,"Add,Fact/Evidence",Fact/Evidence
4784,465-ARR,465-ARR_v2_66@5,,"In total, 28,415 distinct MeSH terms are covered in the training dataset.",,"Add,Fact/Evidence",Fact/Evidence
4785,465-ARR,465-ARR_v2_68@7,,"We use FAISS (Johnson et al., 2019) to find similar documents for each citation among the training set, and the whole process takes 10 hours.",,"Add,Fact/Evidence",Fact/Evidence
4786,465-ARR,465-ARR_v2_68@13,,The detailed hyper-parameter settings are shown in Table 3.,,"Add,Fact/Evidence",Fact/Evidence
4787,465-ARR,465-ARR_v2_68@14,,The code for our method is available at https://github.com/xdwang0726/KenMeSH.,,"Add,Fact/Evidence",Fact/Evidence
4807,468-ARR,468-ARR_v2_27@9,,This source sentence bias could best be mitigated by curating a new dataset which is why we chose not to focus our efforts on the Russian-English dataset.,,"Add,Claim",Claim
4808,468-ARR,468-ARR_v2_42@8,,"Column r shows the mean Pearson correlation of labels and predictions and the standard deviation over 5 runs, each training for 3 epochs = 15 minutes.",,"Add,Fact/Evidence",Fact/Evidence
4809,468-ARR,468-ARR_v2_42@9,,Column MSE is the average mean squared error.,,"Add,Fact/Evidence",Fact/Evidence
4810,468-ARR,468-ARR_v2_42@10,,"Column r target measures the performance when testing on the target sentence only and thus approximates the bias mitigation effect, where a smaller correlation is better.",,"Add,Fact/Evidence",Fact/Evidence
4811,468-ARR,468-ARR_v2_55@4,,"To further evaluate the generalisability of the proposed methods, experiments with additional datasets, low-resource language pairs as well as alternative QE architectures and language models could be conducted, too.",,"Add,Claim",Claim
4812,468-ARR,468-ARR_v2_56@2,,"Other observable biases could also be considered as candidates for the use of targeted bias reduction techniques, provided that it is possible to design a counterbalancing auxiliary task or isolate the bias well enough to deploy adversarial approaches.",,"Add,Claim",Claim
4813,468-ARR,468-ARR_v2_56@3,,"We think that if the latter scenario applies, the adapted debiased focal loss technique for regression could be worth further exploration, too.",,"Add,Claim",Claim
4962,473-ARR,473-ARR_v2_33@3,,The experiment results in Section 6 confirm our assumption.,,"Add,Fact/Evidence",Fact/Evidence
4963,473-ARR,473-ARR_v2_76@4,,We set 5 different random seeds as and report the average result of multiple runs.,,"Add,Fact/Evidence",Fact/Evidence
4964,473-ARR,473-ARR_v2_76@5,,Each run takes 7.68 GPU hours on 4 GeFource RTX 2080 Ti GPUs.,,"Add,Fact/Evidence",Fact/Evidence
4965,473-ARR,473-ARR_v2_97@0,,Table 8 shows two generation cases from English and Chinese test set respectively.,,"Add,Fact/Evidence",Fact/Evidence
4966,473-ARR,473-ARR_v2_97@1,,"In both cases, the golden definition is a long sentence with quite complicated syntax.",,"Add,Fact/Evidence",Fact/Evidence
5493,61-ARR,61-ARR_v2_15@0,,Other Metric,,"Add,Other",Other
5494,61-ARR,61-ARR_v2_16@0,,"Recently, Rastogi et al. (2020b) proposed a metric called average goal accuracy.",,"Add,Fact/Evidence",Fact/Evidence
5495,61-ARR,61-ARR_v2_16@1,,"The main difference between the average goal accuracy and the proposed relative slot accuracy is that the average goal accuracy only considers the slots with non-empty values in the gold states of each turn, whereas the proposed relative slot accuracy considers those in both gold and predicted states.",,"Add,Fact/Evidence",Fact/Evidence
5496,61-ARR,61-ARR_v2_16@2,,"Since average goal accuracy ignores the predicted states, it cannot properly distinguish a better model from a worse model in some specific situations.",,"Add,Claim",Claim
5497,61-ARR,61-ARR_v2_16@3,,We will discuss it in more detail in Section 4.1.,,"Add,Fact/Evidence",Fact/Evidence
5498,61-ARR,61-ARR_v2_25@0,,Table 1 presents the overall results.,,"Add,Fact/Evidence",Fact/Evidence
5499,61-ARR,61-ARR_v2_25@1,,"Regarding slot accuracy, the difference between the largest and smallest values is solely 1.09%.",,"Add,Fact/Evidence",Fact/Evidence
5500,61-ARR,61-ARR_v2_25@2,,It can be one of the reasons that several researchers do not report it.,,"Add,Claim",Claim
5501,61-ARR,61-ARR_v2_25@3,,"Meanwhile, relative slot accuracy can explicitly highlight the deviation among models by showing a 5.47% difference between the largest and smallest values.",,"Add,Fact/Evidence",Fact/Evidence
5612,68-ARR,68-ARR_v2_5@0,,"In order to address this issue, existing methods mainly resort to external resources.",,"Add,Claim",Claim
5614,68-ARR,68-ARR_v2_66@0,,"Furthermore, we compared with a strong baseline that uses external knowledge to enhance crosslingual MRC:",,"Add,Fact/Evidence",Fact/Evidence
5615,68-ARR,68-ARR_v2_67@0,,"LAKM is a pre-trained task proposed in (Yuan et al., 2020) by introducing external sources for phrase-level masked language modeling task.",,"Add,Fact/Evidence",Fact/Evidence
5616,68-ARR,68-ARR_v2_67@1,,"The external corpus contain 363.5k passages and 534k knowledge phrases in four languages: English (en), French (fr), German (de), and Spanish (es).",,"Add,Fact/Evidence",Fact/Evidence
5618,68-ARR,68-ARR_v2_73@2,,"On the MLQA dataset, LAKM uses a larger extra corpus to train a better backbone language model, while our method with less external data can still achieve similar performance in German (de) and Spanish (es).",,"Add,Fact/Evidence",Fact/Evidence
5619,68-ARR,68-ARR_v2_83@2,,"Combined with the negative/positive results of syntactic/semantic vectors in the cross-lingual STS task in SemEval-2017, the visualization demonstrates that S 2 DM can efficiently disassociate semantics from syntax.",,"Add,Fact/Evidence",Fact/Evidence
5697,80-ARR,80-ARR_v2_3@4,,Audio samples are available at https://neuralsvb.,,"Add,Fact/Evidence",Fact/Evidence
5698,80-ARR,80-ARR_v2_3@6,,Codes: https://github. com/MoonInTheRiver/NeuralSVB.,,"Add,Fact/Evidence",Fact/Evidence
5699,80-ARR,80-ARR_v2_69@0,,"SADTW is a kind of advanced APC method, which is designed for fine-tuning the amateur recording, but not for the case when the amateur recordings are completely out of tune.",,"Add,Claim",Claim
5700,80-ARR,80-ARR_v2_69@1,,"In the latter case, we recommend people to use Singing Voice Synthesis (synthesizing waveform from PPG and MIDI) + Singing Voice Conversion (converting the vocal timbre of the synthesized waveform into the user's), or some Speech to Singing (STS) methods.",,"Add,Claim",Claim
5701,80-ARR,80-ARR_v2_69@2,,"In addition, SADTW provides a score representing the similarity of two pitch curves, which could be used to determine what kind of SVB solution should be chosen.",,"Add,Claim",Claim
5702,80-ARR,80-ARR_v2_70@0,,"This work develops a possible automatic way for singing voice beautification, which may cause unemployment for people with related occupations.",,"Add,Claim",Claim
5703,80-ARR,80-ARR_v2_70@1,,"In addition, there is the potential for harm from piracy and abuse of our released recordings.",,"Add,Claim",Claim
5704,80-ARR,80-ARR_v2_70@2,,"Thus, we choose the dataset license: CC by-nc-sa 4.0.",,"Add,Fact/Evidence",Fact/Evidence
5705,80-ARR,80-ARR_v2_71@0,,F.1 For every submission F.1.1 Did you discuss the limitations of your work?,,"Add,Other",Other
5805,9-ARR,9-ARR_v2_5@7,,"Besides the overfitting of downstream task data, a rarely studied problem is that the PLMs usually overfit the pretraining tasks and data (Qi et al., 2020), which may have significant gap with the downstream task and data.",,"Add,Claim",Claim
5806,9-ARR,9-ARR_v2_6@1,,"Different from the standard finetuning paradigm (Fig. 1 (a)) which directly finetunes PLMs on the downstream task data, the key idea of NoisyTune is to add a small amount of noise to perturb PLMs parameters before finetuning (Fig. 1 (b)).",,"Add,Fact/Evidence",Fact/Evidence
5807,9-ARR,9-ARR_v2_6@6,,"In addition, the results show NoisyTune can be easily combined with many existing PLM finetuning methods and further improve their performance.",,"Add,Claim",Claim
5808,9-ARR,9-ARR_v2_8@0,,The goal of NoisyTune is for more effective finetuning of PLMs on downstream tasks.,,"Add,Claim",Claim
5809,9-ARR,9-ARR_v2_8@1,,"The motivation of NoisyTune is that PLMs are well pretrained on some unlabeled corpus with some self-supervision tasks, and they may overfit these pretraining data and tasks (Qi et al., 2020), which usually have gap with the downstream task and data.",,"Add,Fact/Evidence",Fact/Evidence
5810,9-ARR,9-ARR_v2_9@0,,"PLMs usually have different kinds of parameter matrices, such as query, key, value, and feedforward network matrices (Devlin et al., 2019).",,"Add,Fact/Evidence",Fact/Evidence
5811,9-ARR,9-ARR_v2_22@1,,"In addition, we explore whether our proposed matrix-wise perturbing method is better than using a unified global noise for all model parameters in PLMs.",,"Add,Fact/Evidence",Fact/Evidence
5812,9-ARR,9-ARR_v2_22@7,,"The results show that matrix-wise noise is a much better choice, since the different characteristics of different parameter matrices can be taken into consideration.",,"Add,Fact/Evidence",Fact/Evidence
5813,9-ARR,9-ARR_v2_24@0,,"From Fig. 1, it is very clear that NoisyTune is independent of the specific PLM finetuning method, since it is applied at the stage before finetuning PLM on the task-specific data.",,"Add,Fact/Evidence",Fact/Evidence
5814,9-ARR,9-ARR_v2_24@1,,"Thus, it is very easy to combine NoisyTune with any kind of existing PLM finetuning method.",,"Add,Claim",Claim
5815,9-ARR,9-ARR_v2_26@2,,2 The results are shown in Fig. 4.,,"Add,Fact/Evidence",Fact/Evidence
5816,9-ARR,9-ARR_v2_28@0,,Hyperparameter Analysis,,"Add,Other",Other
5817,9-ARR,9-ARR_v2_29@0,,"We study the influence of the most important hyperparameter in NoisyTune, i.e., ฮป, which controls the relative noise intensity.",,"Add,Fact/Evidence",Fact/Evidence
5818,9-ARR,9-ARR_v2_29@1,,The average GLUE scores w.r.t. different ฮป values are shown in Fig. 6.,,"Add,Fact/Evidence",Fact/Evidence
5819,9-ARR,9-ARR_v2_29@2,,"We find that when ฮป is too small or too large, the performance is not optimal.",,"Add,Fact/Evidence",Fact/Evidence
5820,9-ARR,9-ARR_v2_29@3,,"This is because when ฮป is too small, it is difficult for PLMs to do parameter space exploration and overcome the overfitting problem.",,"Add,Fact/Evidence",Fact/Evidence
5821,9-ARR,9-ARR_v2_29@4,,"While when ฮป is too large, the useful pretrained knowledge in PLMs may be overwhelmed by random noise.",,"Add,Claim",Claim
5822,9-ARR,9-ARR_v2_29@5,,Values between 0.1 and 0.15 are more suitable for NoisyTune on the GLUE datasets.,,"Add,Fact/Evidence",Fact/Evidence
5823,9-ARR,9-ARR_v2_31@2,,"NoisyTune is a very general method, and is PLM model agnostic, downstream task agnostic, and finetuning method agnostic.",,"Add,Claim",Claim
5892,90-ARR,90-ARR_v2_69@1,,"To generate our response, we sample entire dialogues from the language model and then re-rank the predicted dialogues with a reward function.",,"Add,Fact/Evidence",Fact/Evidence
5893,90-ARR,90-ARR_v2_75@3,,We train a model to parse the customer's flight requirements from the dialogue.,,"Add,Fact/Evidence",Fact/Evidence
5894,90-ARR,90-ARR_v2_75@4,,We execute these flight requirements against the table and compare the output to the flight that was actually booked; this determines the reward (i.e. if the correct flight was booked or not).,,"Add,Fact/Evidence",Fact/Evidence
5895,90-ARR,90-ARR_v2_82@1,,"Conversations generally begin with a greeting followed by some questioning / information gathering, and then finally the agent suggests a flight before ending the conversation.",,"Add,Fact/Evidence",Fact/Evidence
5896,90-ARR,90-ARR_v2_84@0,,In Table 5 we present a detailed breakdown of model errors.,,"Add,Fact/Evidence",Fact/Evidence
5897,90-ARR,90-ARR_v2_84@1,,"As expected, determining the flight to book, if any, is consistently shown to be the most challenging sub-task, as evidenced by the lower ""flight success rate"" and the lower F1 scores for ""no flight"", ""book"", and ""change"" on LM (GPT2small).",,"Add,Fact/Evidence",Fact/Evidence
5898,90-ARR,90-ARR_v2_84@2,,"In particular, ""change"" has a low recall, precision, and F1 score for all models because it makes up a very small 0.4% of the training data.",,"Add,Fact/Evidence",Fact/Evidence
5899,90-ARR,90-ARR_v2_84@3,,"Lastly, the ""constraint success"" row shows that even when CALM books the wrong flight, the flight it does books meets >80% of the customer's flight requirements on average.",,"Add,Fact/Evidence",Fact/Evidence
5900,90-ARR,90-ARR_v2_86@1,,All models are evaluated with greedy decoding.,,"Add,Fact/Evidence",Fact/Evidence
5901,90-ARR,90-ARR_v2_86@2,,"In addition to the full task success rate, we report success rate for each sub-component of the full task (status / flight / name).",,"Add,Fact/Evidence",Fact/Evidence
5902,90-ARR,90-ARR_v2_86@3,,"We also report recall (R), precision (P), and F1 score for task success under each type of high-level action (book / no flight / cancel / change / no reservation).",,"Add,Fact/Evidence",Fact/Evidence
5903,90-ARR,90-ARR_v2_86@4,,"Lastly, we report the average fraction of the customer's flight requirements that are met when the agent books the wrong flight (constraint success).",,"Add,Fact/Evidence",Fact/Evidence
5909,90-ARR,90-ARR_v2_62@0,,Removing any single component from CALM drops performance by at least 10%.,,"Add,Fact/Evidence",Fact/Evidence
5910,90-ARR,90-ARR_v2_65@0,,"CALM optimizes for task-specific measures of success, and while such measures might be comparatively simple for domains such as AirDialogue, in general specifying the right success measure or reward function may present challenges.",,"Add,Claim",Claim
5911,90-ARR,90-ARR_v2_65@1,,"Furthermore, as with all methods based on end-to-end language models, CALM is susceptible to internal biases and inconsistencies in the language model itself.",,"Add,Claim",Claim
5912,90-ARR,90-ARR_v2_65@2,,"There is for example no constraint that ensures that CALM produces truthful answers, or that it avoids harmful or socially unacceptable outputs.",,"Add,Claim",Claim
5913,90-ARR,90-ARR_v2_65@3,,"A practical deployable dialogue system would likely require additional measures to account for such issues, analogously to how learning-based methods for self-driving vehicles might require some additional safety mechanisms to ensure constraints, and indeed further research on reward specification, ensuring truthful outputs, and other constraint strategies for dialogue systems that combine language models and reward maximization is a promising and important direction.",,"Add,Claim",Claim
5914,90-ARR,90-ARR_v2_66@0,,"The context-conditioned supervised learning strategy used by CALM provides for reward maximization, but is in general not optimal for arbitrary reinforcement learning problems: in general RL settings, learning a value function with dynamic programming in general can attain significantly better returns than imitating high-performing trajectories, by recombining good parts of multiple different trajectories (which might individually be suboptimal) (Kostrikov et al., 2021;Kumar et al., 2022).",,"Add,Claim",Claim
5915,90-ARR,90-ARR_v2_66@1,,"The simple supervised learning strategy works well in the domain we tested, but extending CALM to use value-based reinforcement learning methods is a promising direction for future work.",,"Add,Claim",Claim
5916,90-ARR,90-ARR_v2_66@2,,"Indeed, the improvement obtained from planning on top of the CALM model likely indicates that the supervised learning approach we employ has room for improvement.",,"Add,Claim",Claim
5917,90-ARR,90-ARR_v2_66@3,,"Additionally, the auxiliary objectives and relabeling strategies we employ require some amount of domain-specific design, and more general strategies could be developed in future.",,"Add,Claim",Claim
5918,90-ARR,90-ARR_v2_67@0,,"Addressing these limitations in future work and developing more advanced methods that combine end-to-end language generation via large language models with concepts from reinforcement learning and planning is a promising research direction for making dialogue systems more capable, while also making language models more task aware.",,"Add,Claim",Claim
5919,90-ARR,90-ARR_v2_67@1,,We hope that CALM will serve as an indication for the potential of such methods.,,"Add,Claim",Claim
6180,2-131,2-131_v2_60@3,,Krebs et al. have also previously reported no significant differences in retinal thickness measurements before and after correction of segmentation errors of scans taken using Cirrusโข <REF-11> .,,"Add,Fact/Evidence",Fact/Evidence
6181,2-131,2-131_v2_61@0,,"The differences in the mean thickness values before and after correction in scans taken using Spectralisโข were most obvious in the central subfields of the retina (C1, N1, S1, T1, and I1) with the peripheral subfields being spared (N2, S2, T2 and I2).",,"Add,Fact/Evidence",Fact/Evidence
6182,2-131,2-131_v2_61@1,,This may be attributed to the fact that the pathology of AMD is located centrally and therefore pathology related inaccuracies in segmentation are more likely to occur in these subfields.,,"Add,Claim",Claim
6183,2-131,2-131_v2_63@4,,Krebs et al. evaluated the repeatability of retinal thickness measurements using Spectralisโข and Cirrusโข in patients with AMD.,,"Add,Fact/Evidence",Fact/Evidence
6184,2-131,2-131_v2_63@5,,For images taken using Spectralisโข the mean difference between repeated measurements was found to be within 11ยตm before correction and within 1ยตm after correction.,,"Add,Fact/Evidence",Fact/Evidence
6185,2-131,2-131_v2_63@6,,For images taken using Cirrusโข the mean difference between repeated measurements was found to be within 6ยตm before correction and within 4ยตm after correction <REF-15> .,,"Add,Fact/Evidence",Fact/Evidence
6186,2-131,2-131_v2_66@4,,"In addition, in a subset of patients that had a difference in the severity of disease, both eyes were included in the analysis; this may also have resulted in possible bias.",,"Add,Claim",Claim
6187,2-131,2-131_v2_66@5,,The Cirrus device that was used to capture the images did not have eye tracking and may have led to the slightly larger COR values when compared to Spectralis.,,"Add,Claim",Claim
6193,2-131,2-131_v2_28@0,,No formal sample size calculation was performed before the conduct of the study.,,"Add,Fact/Evidence",Fact/Evidence
6290,2-180,2-180_v2_8@0,,We used ephrin-A2 -/- mice because they have previously been shown to have a specific learning deficit <REF-18> .,,"Add,Fact/Evidence",Fact/Evidence
6291,2-180,2-180_v2_8@2,,Thus we aimed to examine a learning-mediated effect of rTMS on dendritic spines.,,"Add,Fact/Evidence",Fact/Evidence
6292,2-180,2-180_v2_11@4,,Randomised littermates were not used because the breeding colony was structured to produce ephrin-A2/A5 double knockout mice for other studies and no WT littermates were obtained.,,"Add,Fact/Evidence",Fact/Evidence
6293,2-180,2-180_v2_13@4,,"We chose to stimulate after the task because we hypothesized that rTMS would enhance LTP-like processes, stabilizing new spines, and the associated synaptic connections, that had formed during learning.",,"Add,Claim",Claim
6294,2-180,2-180_v2_19@3,,"Unlike in cat studies, the coil was not in direct contact with the mouse head but was held as close as possible to the scalp (~1mm).",,"Add,Fact/Evidence",Fact/Evidence
6295,2-180,2-180_v2_19@4,,The gap between the coil and the head does not attenuate the field because magnetic fields decrease with distance from the source but are not modified by air or biological tissue (e.g. skin/scalp <REF-36> ).,,"Add,Fact/Evidence",Fact/Evidence
6296,2-180,2-180_v2_19@5,,"Unlike in the cat study, stereotaxic delivery was not attempted because the dimensions of the coil ensured that the field reached the entire dorsal hippocampus, which in the mouse, is relatively large in proportion to total brain size.",,"Add,Fact/Evidence",Fact/Evidence
6297,2-180,2-180_v2_40@7,,"Because of the lack of understanding of fundamental interactions between rTMS and behaviour, it would be of great interest to perform an exhaustive battery of behavioural tests in healthy wildtype mice (and eventually in animal models of disease) in conjunction with various rTMS protocols.",,"Add,Claim",Claim
6298,2-180,2-180_v2_40@8,,Subsequent anatomical and physiological analyses could then be carried out to elucidate the neural mechanisms of rTMS and gain insight into the treatment of human disease.,,"Add,Claim",Claim
6299,2-180,2-180_v2_43@0,,"Alternatively, the timing of rTMS delivery relative to the behavioural task may have influenced the outcome of our experiments.",,"Add,Claim",Claim
6300,2-180,2-180_v2_43@1,,"Here we stimulated after the task, however rTMS might have been more effective if delivered before.",,"Add,Claim",Claim
6301,2-180,2-180_v2_43@2,,"Because a single session of rTMS increases the size of dendritic spines and may activate silent synapses <REF-2> , this may โprimeโ the brain for learning.",,"Add,Claim",Claim
6302,2-180,2-180_v2_43@3,,"With such pre-treatment, an effect of rTMS might even have been detected in improved performances on a day to day basis.",,"Add,Claim",Claim
6303,2-180,2-180_v2_45@0,,"Importantly, we are conscious of the limitations of our rodent scaled rTMS delivery device which may have contributed to the lack of effect observed here.",,"Add,Claim",Claim
6304,2-180,2-180_v2_45@1,,"Although our coil had a relevant coil to brain ratio for mice, because of its small size, the intensity of the magnetic field did not reach the magnitude commonly used in humans (6mT compared to 1-2T), raising concern that our stimulation paradigm is not comparable to human rTMS.",,"Add,Claim",Claim
6305,2-180,2-180_v2_45@2,,"However, this raises a more general issue because similar criticism applies to studies that employ larger coils <REF-1> โ <REF-3> : although these deliver the same fields used in humans, the focal nature of the stimulation is lost.",,"Add,Claim",Claim
6306,2-180,2-180_v2_45@3,,Additional effort in designing appropriate small animal rTMS coils is urgently needed to improve the construct validity of animal rTMS research.,,"Add,Claim",Claim
6419,2-278,2-278_v2_5@3,,One reason is that Efron considers the particular set of twin boys as the entire population.,,"Add,Claim",Claim
6420,2-278,2-278_v2_5@4,,"In this case, statistics is not needed because there is no random sample drawn from a larger population.",,"Add,Claim",Claim
6421,2-278,2-278_v2_6@0,,"Efronโs example can be rearranged so that it fits a more realistic situation in statistical data analysis, albeit with a very low sample size: consider the twin boys that, as Efron casually mentions, turned out to be fraternal, as a random sample from the larger population of twin boys and try to draw inference about the proportion of identical twins among the population of twin boys (note that this approach is different from the calculations provided by Efron).",,"Add,Claim",Claim
6422,2-278,2-278_v2_6@3,,"We think that to illustrate the influence of non-informative priors on results of Bayesian data analyses, such an approach would be fairer than the calculations given by Efron.",,"Add,Claim",Claim
6428,2-282,2-282_v2_5@3,,This phenomenon might be induced by a cortico-cerebellar activation during voluntary movements <REF-5> .,,"Add,Fact/Evidence",Fact/Evidence
6429,2-282,2-282_v2_7@2,,QPs were calculated for each movement executed by the patients (one run contains several movements; see section B).,,"Add,Fact/Evidence",Fact/Evidence
6430,2-282,2-282_v2_7@3,,"These QPs are also considered as probabilities of stimulation, given their potential application in a tremor suppression system based on BCI-trigged FES (see also section F).",,"Add,Fact/Evidence",Fact/Evidence
6431,2-282,2-282_v2_15@4,,Patients were told to keep the most relaxed attitude.,,"Add,Fact/Evidence",Fact/Evidence
6433,2-282,2-282_v2_29@2,,We decided to use a period of 2000 msec based on the available literature which considers that 2 seconds encompasses the preparation phase at the cortical level <REF-13> .,,"Add,Fact/Evidence",Fact/Evidence
6434,2-282,2-282_v2_59@9,,"These patients exhibited reproducible low values for the cortico-muscular coherence, by contrast to reproducible high values for the other QPs.",,"Add,Fact/Evidence",Fact/Evidence
6435,2-282,2-282_v2_59@10,,This highlights the importance of our multimodal approach.,,"Add,Claim",Claim
6436,2-282,2-282_v2_90@0,,"Our protocol in neurological patients with tremor differs from those in the literature, hence our study on the multiple combinations of frequency bands.",,"Add,Fact/Evidence",Fact/Evidence
6437,2-282,2-282_v2_90@1,,"When a neurological patient with tremor is seated and assessed, he/she may exhibit a tremor of the head and trunk.",,"Add,Claim",Claim
6438,2-282,2-282_v2_90@2,,This tremor may be pretty stable or rather intermittent.,,"Add,Claim",Claim
6439,2-282,2-282_v2_90@3,,"There may even be an overlap with the main frequencies of the EEG signal, for instance in the alpha band (a rapid head tremor may be found in patients).",,"Add,Claim",Claim
6440,2-282,2-282_v2_90@4,,"Therefore, we decided to have a close look to each of these bands.",,"Add,Fact/Evidence",Fact/Evidence
6441,2-282,2-282_v2_90@5,,"For instance, we have seen patients with cerebellar disorders and orthostatic tremor in whom the sub-band 8โ10 Hz was much less informative as compared with the sub-band 10โ12 Hz.",,"Add,Fact/Evidence",Fact/Evidence
6442,2-282,2-282_v2_90@6,,"We would like to point out that in the study of Pfurtscheller et al. on single-trial classification of EEG and imagination <REF-20> , the frequency of the most reactive components was 11ยฑ0.4 Hz (meanยฑSD).",,"Add,Fact/Evidence",Fact/Evidence
6443,2-282,2-282_v2_90@7,,The SD was thus small.,,"Add,Fact/Evidence",Fact/Evidence
6444,2-282,2-282_v2_90@8,,"Although the desynchronized components were centered at 10.9 Hzยฑ0.9 Hz, the synchronized components were narrow-banded, with higher frequencies at 12.0 Hzยฑ1.0 Hz.",,"Add,Fact/Evidence",Fact/Evidence
6445,2-282,2-282_v2_90@9,,We agree with the authors that the classification of single EEG trials improves when ERD and ERS patterns are combined for multiple tasks.,,"Add,Claim",Claim
6446,2-282,2-282_v2_90@10,,We aim to pursue the use of narrow bands of frequency in multiple tasks.,,"Add,Fact/Evidence",Fact/Evidence
6447,2-282,2-282_v2_91@5,,"Techniques of multichannel EEG compression, phase congruency and graphical representations aiming at a reduction of multidimensional data have been proposed <REF-23> โ <REF-25> .",,"Add,Fact/Evidence",Fact/Evidence
6448,2-282,2-282_v2_91@6,,"However, no technique has been widely accepted so far.",,"Add,Claim",Claim
6449,2-282,2-282_v2_92@0,,"In theory, BCI is an interface between brain and computer.",,"Add,Claim",Claim
6450,2-282,2-282_v2_92@1,,"As such, our system would be a multimodal control unit, including an EEG-module like often used for BCIs, but also body modules to control a stimulation unit.",,"Add,Claim",Claim
6451,2-282,2-282_v2_92@2,,Future works could apply some feature selection algorithm and train the multimodal control unit in discriminating movements based on the multimodal input.,,"Add,Claim",Claim
6452,2-282,2-282_v2_92@3,,These two steps could further be included in one step e.g. by use of random forests.,,"Add,Claim",Claim
6453,2-282,2-282_v2_92@4,,"By doing so, the performance of the modules would be evaluated to find out which ones contribute most to a high detection rate.",,"Add,Claim",Claim
6454,2-282,2-282_v2_92@5,,"This would be done separately for each patient, thus taking into account the inter-individual variability.",,"Add,Claim",Claim
6455,2-282,2-282_v2_94@6,,Our data provide a ground for the concept of multimodal approach developed for the early detection of the intentionality of movement.,,"Add,Claim",Claim
6456,2-282,2-282_v2_94@7,,The presented probability trees are general schemes.,,"Add,Fact/Evidence",Fact/Evidence
6457,2-282,2-282_v2_94@8,,A case-by-case analysis is required.,,"Add,Claim",Claim
6458,2-282,2-282_v2_94@9,,"In order to provide the most possible accurate BCI-driven FES system, each subject needs to be studied in order to define the best combination of QPs.",,"Add,Claim",Claim
6459,2-282,2-282_v2_94@11,,The system would take into account these features.,,"Add,Claim",Claim
6460,2-282,2-282_v2_94@12,,"By analysing a larger group of patients, we might identify subgroups of patients on the basis of the results of the probability trees.",,"Add,Claim",Claim
6461,2-282,2-282_v2_94@13,,"In other words, the probability trees would be used as an elegibility procedure to multimodal BCI-driven treatments in neurological patients with tremor.",,"Add,Claim",Claim
6462,2-282,2-282_v2_95@0,,"Results obtained with the simulation study provide useful information about EEG QP in order to select patients more effectively for a BCI-based treatment, including rehabilitation.",,"Add,Claim",Claim
6463,2-282,2-282_v2_95@1,,The simulation demonstrates the relationship between the threshold and the QP.,,"Add,Fact/Evidence",Fact/Evidence
6464,2-282,2-282_v2_95@2,,Future studies could take advantage of these findings to select the best neurological candidates on the basis of the ERD/ERS for BCI-based management.,,"Add,Claim",Claim
6465,2-282,2-282_v2_100@6,,The analysis of the corticomuscular coherence shows that this parameter alone cannot be used to predict voluntary motion and be implemented in a BCI.,,"Add,Fact/Evidence",Fact/Evidence
6466,2-282,2-282_v2_100@12,,We propose to select a larger group of neurological patients to confirm the strength of the multimodal prediction.,,"Add,Claim",Claim
6467,2-282,2-282_v2_100@13,,The present study opens the door for future studies in terms of how to increase EEG-based detection of movement intention by incorporating information from multiple modules.,,"Add,Claim",Claim
6468,2-282,2-282_v2_101@0,,Data availability,,"Add,Other",Other
6469,2-282,2-282_v2_102@0,,The data referenced by this article are under copyright with the following copyright statement: Copyright: รฏยฟยฝ 2014 Grimaldi G et al.,,"Add,Fact/Evidence",Fact/Evidence
6470,2-282,2-282_v2_103@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",,"Add,Fact/Evidence",Fact/Evidence
7189,3-256,3-256_v2_98@9,,"Ultimately, whether insulin sensitivity is truly one of the mechanisms by which the aging-retarding (and thus, lifespan-extending) effects of the Ghr/bp gene disruption are mediated, and/or account for the differential response of GHR-KO mice to CR, will require direct experimental demonstration.",,"Add,Claim",Claim
7190,3-256,3-256_v2_100@0,,Certain aspects of the indirect calorimetry results deserve special treatment.,,"Add,Claim",Claim
7191,3-256,3-256_v2_100@1,,"Firstly, energy expenditure (EE) was greater in GHR-KO mice compared to their littermate controls, regardless of diet; yet heat production (HP) did not differ by genotype.",,"Add,Fact/Evidence",Fact/Evidence
7192,3-256,3-256_v2_100@2,,We suspect that a technical reason exists for this incongruence.,,"Add,Claim",Claim
7193,3-256,3-256_v2_100@3,,"The formula for calculating HP with indirect calorimetric gas-exchange data, formulated based on reconciling direct calorimetric measures with indirect calorimetric gas-exchange measures in normal-sized, lean mice [ Arch et al. , 2006 ], is [((4.33 ร VO 2 ) + (0.67 ร VCO 2 )) + (Wt.(kg.)) ร (60 Min./Hr.)]; and, thus, is not necessarily appropriate for accurately measuring HP via indirect calorimetry in dwarf mice, which have divergent body composition (most-notably obesity) compared to their littermate controls [ Berryman et al. , 2004 ; Berryman et al. , 2010 ].",,"Add,Fact/Evidence",Fact/Evidence
7194,3-256,3-256_v2_100@4,,"More directly, that the heat production equation factors in body weight results in lighter dwarf mice having lower calculated HP, for given amounts of oxygen inhalation (VO 2 ) and carbon dioxide expiration (VCO 2 ), than their normal-sized littermates.",,"Add,Fact/Evidence",Fact/Evidence
7195,3-256,3-256_v2_100@5,,"Additionally of note (although not statistically significant), caloric restriction tended to reduce HP in GHR-KO mice but tended to raise it in littermate controls.",,"Add,Fact/Evidence",Fact/Evidence
7196,3-256,3-256_v2_100@6,,"Finally, (as alluded to in the paragraph above) for VO 2 , VCO 2 , and EE, gas-exchange-based measures of metabolism were robustly heightened by CR in littermates, but this effect was muted or absent in GHR-KOโs.",,"Add,Fact/Evidence",Fact/Evidence
7197,3-256,3-256_v2_100@7,,"Therefore, CR-effected increased metabolism could be a mean by which that diet slows senescence in littermates; moreover, the attenuation of this metabolic effect in GHR-KO mice (whether due to a ceiling effect, in which VO 2 and CO 2 being innately high in GHR-KO mice on A.L. makes it so that CR is unable to raise them much higher, or other rationales) might partially explain the attenuation of the aging-retarding (including life expectancy-increasing) effect of CR on GHR-KO mice.",,"Add,Claim",Claim
7313,4-143,4-143_v2_8@1,,None of the three children had a concussion previously.,,"Add,Fact/Evidence",Fact/Evidence
7314,4-143,4-143_v2_8@3,,"Their symptoms were not assessed on the day they visited UWM, although one of the patients (cc1) was not able to speak or walk normally at that time.",,"Add,Fact/Evidence",Fact/Evidence
7315,4-143,4-143_v2_25@3,,"Third, age and sex were not matched perfectly between the patients and the controls tested in the present study.",,"Add,Fact/Evidence",Fact/Evidence
7316,4-143,4-143_v2_25@4,,"However, we are not aware of any findings that indicate significant differences between boys and girls within the age range of 12โ17 years with regard to their visuomotor adaptation capabilities.",,"Add,Claim",Claim
7317,4-143,4-143_v2_25@5,,"In addition, it has been shown that during targeted reaching movements under a visuomotor rotation condition, visuomotor representations of children who were 6 or 8 years old differed from those of adults, although those of 11 year-old children did not <REF-12> .",,"Add,Fact/Evidence",Fact/Evidence
7318,4-143,4-143_v2_25@6,,"Our data do not seem to indicate sex-related differences either, in that the adaptation pattern of cc2 (15 year-old female) was very similar to that of ctls 1 and 3 (12 and 14 year-old males, respectively).",,"Add,Fact/Evidence",Fact/Evidence
7319,4-143,4-143_v2_25@7,,"Thus, while the potential effects of age- and sex-related differences between the patients and the controls cannot be completely excluded, it seems unlikely that our results were substantially influenced by such effects.",,"Add,Claim",Claim
7527,5-1356,5-1356_v2_48@0,,"In these simulations, the overall best performance of the DM model is achieved when dispersion parameters are estimated with the Cox-Reid APL and the dispersion moderation is applied.",,"Add,Fact/Evidence",Fact/Evidence
7528,5-1356,5-1356_v2_48@1,,"This strategy leads to p-value distributions that in most of the cases are closer to the uniform distribution ( Figure 1D , Figure S4 and Figure S11 ).",,"Add,Fact/Evidence",Fact/Evidence
7529,5-1356,5-1356_v2_50@1,,For each condition three replicates were simulated resulting in 3 versus 3 comparisons.,,"Add,Fact/Evidence",Fact/Evidence
7530,5-1356,5-1356_v2_74@3,,"To further investigate characteristics of detected tuQTLs, we measured enrichment of splicing-related features as used in a previous comparison <REF-35> .",,"Add,Fact/Evidence",Fact/Evidence
7531,5-1356,5-1356_v2_74@4,,"This includes the location of tuQTLs within exons, within splice sites, in the surrounding of GWAS SNPs and distance to the closest exon.",,"Add,Fact/Evidence",Fact/Evidence
7536,5-1356,5-1356_v2_7@0,,Genes may express diverse transcript isoforms (mRNA variants) as a consequence of alternative splicing or due to the differences in transcription start sites and polyadenylation sites <REF-16> .,,"Add,Fact/Evidence",Fact/Evidence
7537,5-1356,5-1356_v2_28@0,,Detecting DTU and tuQTLs with the Dirichlet-multinomial model,,"Add,Other",Other
7713,5-1822,5-1822_v2_23@4,,Animals were pre-trained on the horizontal ladder prior to the commencement of the study.,,"Add,Fact/Evidence",Fact/Evidence
7714,5-1822,5-1822_v2_23@5,,"On the day of functional testing, the animals were allowed three practice runs prior to recording.",,"Add,Fact/Evidence",Fact/Evidence
7715,5-1822,5-1822_v2_23@10,,Animals were pre-trained on the beam prior to the commencement of the study.,,"Add,Fact/Evidence",Fact/Evidence
7716,5-1822,5-1822_v2_23@11,,"On the day of functional testing, the animals were allowed three practice runs along the beam prior to recording.",,"Add,Fact/Evidence",Fact/Evidence
7717,5-1822,5-1822_v2_35@2,,"If the slopes are found to be equal, the intercepts (elevations) are then compared.",,"Add,Fact/Evidence",Fact/Evidence
8022,6-1302,6-1302_v2_97@0,,The data referenced by this article are under copyright with the following copyright statement: Copyright: รฏยฟยฝ 2017 Rufflรฉ F et al.,,"Add,Fact/Evidence",Fact/Evidence
8023,6-1302,6-1302_v2_98@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",,"Add,Fact/Evidence",Fact/Evidence
8024,6-1302,6-1302_v2_17@2,,"The RNA-Seq was performed using polyA-selection with the truSeq RNA Lib-Prep Kit (Illumina, San Diego, CA) adjusted with GATC specific procedure for strand specificity.",,"Add,Fact/Evidence",Fact/Evidence
8346,6-577,6-577_v2_14@0,,UVR and temperature treatment of coral nubbins,,"Add,Other",Other
8347,6-577,6-577_v2_15@0,,Incubations were performed in 100 mL beakers containing one coral nubbin each and filled with 40 mL of 0.45 ฮผm filtered seawater and continuously stirred using magnetic stirrers.,,"Add,Fact/Evidence",Fact/Evidence
8348,6-577,6-577_v2_15@1,,"High temperature or/and ultraviolet radiation (UVR) stresses (i.e. four environmental conditions: control (at 25ยฐC and without UVR), thermal stress (30ยฐC without UVR), UVR stress (25ยฐC under UVR), thermal and UVR stresses (30ยฐC and under UVR)) were applied to corals and ERK activation was monitored after 30 minutes of stress.",,"Add,Fact/Evidence",Fact/Evidence
8349,6-577,6-577_v2_15@2,,Thermal stress corresponded to an increase in temperature from the normal culture condition of 25ยฐC to 30ยฐC.,,"Add,Fact/Evidence",Fact/Evidence
8350,6-577,6-577_v2_15@3,,The UVR stress corresponded to an increase in UVR from 0 (HQI lamps in the culture conditions) to a radiation intensity of about 3 W.m โ2 UVB and 30 W.m โ2 UVA (Q-Panel UVA 340 lamps).,,"Add,Fact/Evidence",Fact/Evidence
8351,6-577,6-577_v2_15@4,,"At the end of the incubation, nubbins were frozen and kept at โ 80ยฐC prior to western blot analysis.",,"Add,Fact/Evidence",Fact/Evidence
8352,6-577,6-577_v2_31@2,,"The limited thickness of the animal tissue covering the skeleton and the very large surface of contact of both ectoderm and endoderm with the seawater render S. pistillata suitable for treatment with drugs directly diluted in the seawater as we previously showed ( Courtial et al. , 2017 ).",,"Add,Fact/Evidence",Fact/Evidence
8353,6-577,6-577_v2_32@0,,"To confirm that Spi-ERK activity can dynamically respond to changes in experimental conditions, we performed an induction experiment by modifying culture conditions of the corals.",,"Add,Fact/Evidence",Fact/Evidence
8354,6-577,6-577_v2_32@1,,"Courtial et al. (2017) showed that thermal and UVR stresses induced the formation of reactive oxygen species which are known to trigger ERK phosphorylation ( McCubrey et al. , 2006 ).",,"Add,Fact/Evidence",Fact/Evidence
8355,6-577,6-577_v2_32@2,,"ERK phosphorylation was enhanced in corals exposed to UVR, high temperature or a combination of both ( Figure 3 and Supplementary Figure S2 ).",,"Add,Fact/Evidence",Fact/Evidence
8356,6-577,6-577_v2_32@3,,These results confirm that the antibodies characterized herein can be used to monitor ERK activity in corals.,,"Add,Claim",Claim
8357,6-577,6-577_v2_47@2,,The portions of the images used in the main text are outlined.,,"Add,Fact/Evidence",Fact/Evidence
8358,6-577,6-577_v2_48@0,,Supplementary Figure S3.,,"Add,Other",Other
8359,6-577,6-577_v2_48@1,,Uncropped blot images for Figure 4 and supplementary replicates.,,"Add,Fact/Evidence",Fact/Evidence
8360,6-577,6-577_v2_48@2,,The portions of the images used in the main text are outlined.,,"Add,Fact/Evidence",Fact/Evidence
8592,7-1299,7-1299_v2_16@0,,Further support for a complex NMD pathway existing in the last eukaryotic common ancestor comes from the examination of plants.,,"Add,Claim",Claim
8593,7-1299,7-1299_v2_16@11,,"However, this does not explain why the putative phosphorylation sites are lost in many species <REF-45> .",,"Add,Fact/Evidence",Fact/Evidence
8594,7-1299,7-1299_v2_16@12,,One exciting possibility is that direct interactions between SMG5/6/7 family proteins is sufficient for NMD to be activated in some species (see below).,,"Add,Claim",Claim
8595,7-1299,7-1299_v2_17@12,,"It could be that RNA decay enzymes are recruited directly to UPF1, alternative mechanism to the phosphorylation-mediated recruitment <REF-61> , <REF-63> , <REF-64> .",,"Add,Fact/Evidence",Fact/Evidence
8596,7-1299,7-1299_v2_17@13,,"Recently, the yeast EBS1 and NMD4 proteins were found to interact directly with UPF1 during NMD <REF-61> .",,"Add,Fact/Evidence",Fact/Evidence
8597,7-1299,7-1299_v2_17@14,,"NMD4, like SMG6, contains a PIN domain <REF-61> .",,"Add,Fact/Evidence",Fact/Evidence
8598,7-1299,7-1299_v2_17@15,,"Transcripts responsive to the deletion of UPF1 also increased in deletions of EBS1 and NMD4, however, to a lesser extent <REF-61> .",,"Add,Fact/Evidence",Fact/Evidence
8599,7-1299,7-1299_v2_17@16,,"Interestingly, the importance of EBS1 and NMD4 became more pronounced when yeast cells expressed a truncated UPF1 <REF-61> ; when the truncated UPF1 was expressed alone, NMD efficiency was about 30% of wild-type, in contrast, when either EBS1 and NMD4 were deleted in the truncated UPF1 lines, NMD efficiency was close to zero <REF-61> .",,"Add,Fact/Evidence",Fact/Evidence
8600,7-1299,7-1299_v2_17@17,,This suggests that EBS1 and NMD4 become essential in NMD limiting conditions.,,"Add,Claim",Claim
8601,7-1299,7-1299_v2_17@18,,"This raises the possibility that in species lacking SMG1, the phosphorylation checkpoint of NMD is not required and SMG5/6/7 family proteins directly interact with UPF1 when at a PTC.",,"Add,Claim",Claim
8602,7-1299,7-1299_v2_17@19,,"SMG1 mutants in fruit flies have been found to have a lesser effect on NMD than the mutation of other NMD factors <REF-54> , <REF-55> .",,"Add,Fact/Evidence",Fact/Evidence
8603,7-1299,7-1299_v2_17@20,,"SMG5 was found to be essential for NMD <REF-28> , and when a mild disruption of SMG5 is introduced, mutations of SMG1 enhanced the severity of the NMD phenotype <REF-28> .",,"Add,Fact/Evidence",Fact/Evidence
8604,7-1299,7-1299_v2_17@21,,"This supports the notion that NMD can be activated without phosphorylation and that phosphorylation simply enhances decay under limiting conditions <REF-28> , <REF-65> .",,"Add,Fact/Evidence",Fact/Evidence
8605,7-1299,7-1299_v2_17@22,,"Interestingly, mammalian SMG6 has also been found to bind UPF1 independent of phosphorylation <REF-66> , <REF-67> , suggesting some level of conservation of phosphorylation-independent recruitment of decay factors in NMD.",,"Add,Claim",Claim
8606,7-1299,7-1299_v2_17@23,,"However, it is not clear why a phosphorylation checkpoint is needed for NMD in some organisms like mammals <REF-20> , <REF-52> and plants <REF-51> , <REF-53> , but likely not others such as yeast, but direct interaction seems likely to be the mechanism.",,"Add,Claim",Claim
8607,7-1299,7-1299_v2_22@5,,"TOR is the only other related kinase in A. thaliana , and is involved with the regulation of translation, although the phenotype of TOR knockdown lines do not appear to match those of NMD factors in A. thaliana <REF-71> .",,"Add,Fact/Evidence",Fact/Evidence
8608,7-1299,7-1299_v2_24@7,,"NMD factors do function in other pathways, for example, UPF1 is known to be involved with mammalian DNA replication <REF-75> .",,"Add,Fact/Evidence",Fact/Evidence
8609,7-1299,7-1299_v2_24@8,,"Although in mammals, some NMD transcripts only require a subset of NMD factors <REF-37> , <REF-38> , <REF-76> , these branches of the NMD pathway support the notion that a more reduced NMD pathway may exist.",,"Add,Claim",Claim
8610,7-1299,7-1299_v2_25@2,,"Protein-protein interaction studies in yeast have revealed the species specific factor NMD4 <REF-61> , <REF-77> .",,"Add,Fact/Evidence",Fact/Evidence
8611,7-1299,7-1299_v2_25@3,,Performing similar work in other species is likely to reveal more species/lineage specific factors.,,"Add,Claim",Claim
8612,7-1299,7-1299_v2_28@4,,Although some transcripts appear to be targeted due to their length independent of the polyA tail in yeast <REF-94> .,,"Add,Fact/Evidence",Fact/Evidence
8613,7-1299,7-1299_v2_28@9,,"In yeast, the RNA binding protein Pub1 binds to sequence elements and protects some uORF-containing transcripts from NMD <REF-98> .",,"Add,Fact/Evidence",Fact/Evidence
8614,7-1299,7-1299_v2_31@11,,NMD has been proposed as a general protection mechanism against RNA viruses and TE expansion <REF-109> .,,"Add,Fact/Evidence",Fact/Evidence
8615,7-1299,7-1299_v2_34@2,,"- 3) If phosphorylation of UPF1 represents a checkpoint in the activation of NMD, what explains the variability of the presence of this checkpoint between species?",,"Add,Claim",Claim
8616,7-1299,7-1299_v2_34@6,,"- 7) To identify what precisely determines the accumulation of UPF1 on some transcripts, and why this appears to be dependent UPF1 ATPase activity <REF-111> .",,"Add,Claim",Claim
8617,7-1299,7-1299_v2_34@7,,"- 8) What is the mechanism leading to NMD of uORF transcripts? Is it EJC mode, long 3โ UTR mode, both or neither? This will need to be done for each uORF transcript of interest.",,"Add,Claim",Claim
8663,7-1306,7-1306_v2_9@1,,Users may upload a single expression file and specify whether the rows represent genes and the columns represent cells or vice-versa.,,"Add,Fact/Evidence",Fact/Evidence
8664,7-1306,7-1306_v2_9@3,,"Additionally, this notebook supports the three-file 10X output format, allowing users to upload the matrix, genes, and barcodes files.",,"Add,Fact/Evidence",Fact/Evidence
8665,7-1306,7-1306_v2_9@4,,Any of those inputs can also be provided as .zip files.,,"Add,Fact/Evidence",Fact/Evidence
8666,7-1306,7-1306_v2_10@5,,"To use the mitochondrial gene filter, the user must supply their data with gene names in HGNC format with โMT-โ prepended to each mitochondrial gene name.",,"Add,Fact/Evidence",Fact/Evidence
8667,7-1306,7-1306_v2_14@8,,"As there is debate in the field concerning the correctness of using regression on covariates such as percent mitochondrial reads ( Batson, 2018 ) we have made this step optional.",,"Add,Fact/Evidence",Fact/Evidence
8668,7-1306,7-1306_v2_14@12,,"We note that this notebook is a living, open source document and can be modified as the single cell communityโs perspectives on best practices evolves.",,"Add,Claim",Claim
8669,7-1306,7-1306_v2_35@0,,We encourage users to perform analyses on their own data using this notebook.,,"Add,Claim",Claim
8670,7-1306,7-1306_v2_35@1,,We note that all the required libraries are already installed on the public GenePattern Notebook server at https://notebook.genepattern.org .,,"Add,Fact/Evidence",Fact/Evidence
8671,7-1306,7-1306_v2_35@2,,"This resource is freely available to the community and the analysis described in this notebook falls well within the per-account memory allocations (see the Scanpy authorsโ benchmarking in Wolf et al ., 2018 ; Eulenberg et al ., 2017a ; Eulenberg et al. , 2017b ).",,"Add,Fact/Evidence",Fact/Evidence
8672,7-1306,7-1306_v2_35@3,,"To analyze larger datasets that exceed the per-user memory allocation on the public notebook server, users should deploy the open source GenePattern Notebook server using their own computational resources as described in Reich et al ., 2017 .",,"Add,Fact/Evidence",Fact/Evidence
8673,7-1306,7-1306_v2_35@4,,"The GenePattern Notebook server is available as the genepattern-notebook package through the pip ( https://pypi.org/project/genepattern-notebook/ ) or conda ( https://anaconda.org/genepattern/genepattern-notebook ) package managers, or as a Docker image ( https://hub.docker.com/r/genepattern/genepattern-notebook ).",,"Add,Fact/Evidence",Fact/Evidence
8674,7-1306,7-1306_v2_36@2,,"For example, future notebook releases may include quality control methods such as doublet detection ( McGinnis et al ., 2018 ) as well as visualization methods such as UMAP ( Becht et al ., 2019 ), which is growing in popularity in the single cell community.",,"Add,Claim",Claim
8675,7-1306,7-1306_v2_36@3,,"We also encourage advanced users to copy the notebook, add new approaches or features, and publish them as a community notebook in the GenePattern Notebook repository.",,"Add,Claim",Claim
8745,7-1891,7-1891_v2_12@2,,"In BKKBN 2017 shown that the realization of modern contraceptive prevalence rate such as use of female sterilization (MOW), male sterilization (MOP), pill, IUD, injection, implant KB (Implant) and condoms on 2017 is 57.6% of the target of 60.9% or achievement of 94.58%.",,"Add,Fact/Evidence",Fact/Evidence
8746,7-1891,7-1891_v2_12@3,,"Furthermore, the realization of unmet need which is defined as the percentage of married women who do not want to have more children but do not use contraception in 2017 is 17.5%, so that the achievement is 58.63%.",,"Add,Fact/Evidence",Fact/Evidence
8747,7-1891,7-1891_v2_12@4,,"Moreover, the percentage of active contraceptive participants using LARCs was 21.5% of the target of 21.7% or achievement of 99.07%.",,"Add,Fact/Evidence",Fact/Evidence
8748,7-1891,7-1891_v2_12@5,,"Additionally, the realization of contraceptive discontinuation rate in 2017 is 22.3% of the target of 25.3% or achievement of 88.14% <REF-14> .",,"Add,Fact/Evidence",Fact/Evidence
8749,7-1891,7-1891_v2_20@0,,The instrument used in this study was a questionnaire containing questions related to factors associated with the utilization of long-acting reversible contraceptives in the Pameungpeuk Rural Hospital work area.,,"Add,Fact/Evidence",Fact/Evidence
8750,7-1891,7-1891_v2_20@1,,"The questionnaire consisted of questions about the behavior of use LARCs methods, knowledge, beliefs, attitudes, exposure to information on LARCs, skills of health workers, support of partner, support of friends, support of health workers, and support of community leaders <REF-22> .",,"Add,Fact/Evidence",Fact/Evidence
8751,7-1891,7-1891_v2_43@5,,This study was done before the new National Health Insurance Scheme was rolled out and widely available.,,"Add,Fact/Evidence",Fact/Evidence
8752,7-1891,7-1891_v2_43@6,,"National health insurance has in recent years, overcome many of the cost barriers to contraceptive use, including use of LARCs.",,"Add,Claim",Claim
8753,7-1891,7-1891_v2_50@0,,Extended data,,"Add,Other",Other
8754,7-1891,7-1891_v2_51@0,,Extended data contains the questionnaire (Bahasa and English versions) and how data were measured: https://doi.org/10.6084/m9.figshare.9734561.v2 <REF-22> .,,"Add,Fact/Evidence",Fact/Evidence
9710,8-1204,8-1204_v2_11@5,,Similar findings may also be present in FSGS cases associated with COL4A variants.,,"Add,Claim",Claim
9711,8-1204,8-1204_v2_11@6,,One study of Chinese families with familial FSGS noted segmental thinning of the GBM similar to those which may be present with TBMN in four out of five families with a proven COL4A variant underlying their familial FSGS (electron microscopy in another family and a sporadic case with proven variants were both noted to be unremarkable) <REF-10> .,,"Add,Fact/Evidence",Fact/Evidence
9712,8-1204,8-1204_v2_11@7,,At present the sensitivity of these findings remain unknown.,,"Add,Claim",Claim
9713,8-1204,8-1204_v2_33@2,,This is in keeping with a study of Chinese FSGS families in which four patients with a demonstrated mutation in COL4A and familial FSGS all had findings suggestive of TBMN on electron microscopy <REF-10> .,,"Add,Fact/Evidence",Fact/Evidence
9714,8-1204,8-1204_v2_33@5,,The authors suggest that sequencing of these electron microscopy variants when found could be an extension of current research to expand our knowledge in this field.,,"Add,Claim",Claim
9790,8-1681,8-1681_v2_8@7,,"However, theory suggests that the pRF model is scale-invariant, that is, the pRF parameters are estimated by correlation between predicted and observed time series and only the pRF shape matters to size estimation, not amplitude.",,"Add,Claim",Claim
9791,8-1681,8-1681_v2_31@5,,The same HRF function was used for all participants given that the effect of individualized HRF on pRF parameters is expected to be small <REF-25> .,,"Add,Fact/Evidence",Fact/Evidence
9792,8-1681,8-1681_v2_60@0,,"Firstly, it is worth noting that both scanners used were manufactured by the same vendor.",,"Add,Fact/Evidence",Fact/Evidence
9793,8-1681,8-1681_v2_60@1,,"While this means our results are not currently generalizable to other platforms, it removes manufacturer as an additional source of variance between sites.",,"Add,Claim",Claim
9794,8-1681,8-1681_v2_66@4,,"Of course, the skew we observed in pRF size is related to the sequential presentation of our visual stimuli (sweeping bars, a stimuli commonly used for pRF).",,"Add,Claim",Claim
9795,8-1681,8-1681_v2_66@5,,One could hypothesise that temporal filtering would not bias for pRF sizes if the location of stimuli used for retinotopic mapping was presented randomly.,,"Add,Claim",Claim
9796,8-1681,8-1681_v2_66@6,,"However, recent studies <REF-49> , <REF-50> have shown that pRF size estimates differ between random and ordered designs (irrespective of temporal filtering), probably due to additional factors.",,"Add,Fact/Evidence",Fact/Evidence
9797,8-1681,8-1681_v2_66@7,,The choice of mapping stimulus should therefore also be taken into consideration when comparing results across studies.,,"Add,Claim",Claim
9798,8-1681,8-1681_v2_70@0,,Underlying data,,"Add,Other",Other
9799,8-1681,8-1681_v2_74@0,,Extended data,,"Add,Other",Other
9800,8-1681,8-1681_v2_77@0,,"Mean population receptive field (pRF) size (A), median cortical magnification factor CMF (B) and response amplitude (beta) (C) binned into eccentricity bands for the London 1.5T site (red) and the Auckland 3T site (black).",,"Add,Fact/Evidence",Fact/Evidence
9801,8-1681,8-1681_v2_77@1,,Panels in columns show different visual regions.,,"Add,Fact/Evidence",Fact/Evidence
9802,8-1681,8-1681_v2_77@2,,Panels in rows show the three participants.,,"Add,Fact/Evidence",Fact/Evidence
9803,8-1681,8-1681_v2_77@3,,Error bars denote 95% confidence intervals based on bootstrapping.,,"Add,Fact/Evidence",Fact/Evidence
9804,8-1681,8-1681_v2_80@0,,"Median goodness-of-fit (A), normalized goodness-of-fit (B), and noise ceiling (C) binned into eccentricity bands for the London 1.5T site (red) and the Auckland 3T site (black).",,"Add,Fact/Evidence",Fact/Evidence
9805,8-1681,8-1681_v2_80@1,,Panels in columns show different visual regions.,,"Add,Fact/Evidence",Fact/Evidence
9806,8-1681,8-1681_v2_80@2,,Panels in rows show the three participants.,,"Add,Fact/Evidence",Fact/Evidence
9807,8-1681,8-1681_v2_80@3,,Error bars denote 95% confidence intervals based on bootstrapping.,,"Add,Fact/Evidence",Fact/Evidence
9808,8-1681,8-1681_v2_83@0,,"Mean population receptive field (pRF) size (A) and median goodness-of-fit (B) binned into eccentricity bands for the Auckland data without filtering (black), and low-pass filtering with kernel 1 s (blue) and 2 s (green).",,"Add,Fact/Evidence",Fact/Evidence
9809,8-1681,8-1681_v2_83@1,,Panels in columns show different visual regions.,,"Add,Fact/Evidence",Fact/Evidence
9810,8-1681,8-1681_v2_83@2,,Panels in rows show the three participants.,,"Add,Fact/Evidence",Fact/Evidence
9811,8-1681,8-1681_v2_83@3,,Error bars denote 95% confidence intervals based on bootstrapping.,,"Add,Fact/Evidence",Fact/Evidence
10018,8-1983,8-1983_v2_2@1,,"In the dLGE, metrics of sizes and numbers of neurospheres generated using this assay has not been completely characterized.",,"Add,Claim",Claim
10019,8-1983,8-1983_v2_2@3,,The advantage of this protocol is that no expensive and specialized equipment is needed for tissue isolation.,,"Add,Claim",Claim
10021,8-1983,8-1983_v2_8@5,,"Additionally, many protocols do not provide metrics on expected numbers and sizes of neurospheres generated.",,"Add,Claim",Claim
10022,8-1983,8-1983_v2_8@6,,"Thus, it is unclear whether researchers can generate sufficient numbers of neurospheres in a particular range of sizes.",,"Add,Claim",Claim
10023,8-1983,8-1983_v2_8@10,,"Furthermore, using our method neurospheres small appears as early as 3 days.",,"Add,Claim",Claim
10024,8-1983,8-1983_v2_8@11,,Another advantage of this protocol is that it can generate neurospheres with average sizes of 50 ฮผm - 100 ฮผm after 5โ7 days in vitro .,,"Add,Claim",Claim
10026,8-1983,8-1983_v2_19@2,,The optimum magnification is approximately 5x with 3888 x 2592 dimensions.,,"Add,Fact/Evidence",Fact/Evidence
10027,8-1983,8-1983_v2_19@3,,"Camera was connected to the trinocular port of the stereomicroscope (Carl Zeiss Stemi 305; White Plains, NY) using Mount Adaptor EF-EOS (6098B007AA; Canon; Melville, NY).",,"Add,Fact/Evidence",Fact/Evidence
10028,8-1983,8-1983_v2_19@4,,The working distance was defined as the amount of room required between the top of the neurosphere and the bottom of the objective lens in order for the image to be in focus.,,"Add,Fact/Evidence",Fact/Evidence
10029,8-1983,8-1983_v2_19@5,,The steromicroscope was used at a working distance of ~110 mm.,,"Add,Fact/Evidence",Fact/Evidence
10030,8-1983,8-1983_v2_19@6,,"Due to the variation in neurosphere size, 110 mm should be adjusted to focus on the desire region of the neurosphere to provide optimal focus.",,"Add,Fact/Evidence",Fact/Evidence
10031,8-1983,8-1983_v2_27@1,,Sterilize all surgical instruments packed in aluminum foil in an autoclave at 121ยฐC (15 psi) for 30 mins.,,"Add,Fact/Evidence",Fact/Evidence
10032,8-1983,8-1983_v2_27@2,,"This includes a scissors, forceps, and razor blades.",,"Add,Fact/Evidence",Fact/Evidence
10033,8-1983,8-1983_v2_27@3,,Before starting all premade solutions should be warmed to 37ยฐC.,,"Add,Fact/Evidence",Fact/Evidence
10034,8-1983,8-1983_v2_37@1,,"Have a petri dish (100 mm) prepared with ice-cold Hankโs buffer kept on ice, which will be used to collect embryos after dissection.",,"Add,Fact/Evidence",Fact/Evidence
10035,8-1983,8-1983_v2_37@2,,"Afterwards, additional petri dishes will be needed to place in each of the dissected brains (35 mm) .",,"Add,Fact/Evidence",Fact/Evidence
10036,8-1983,8-1983_v2_46@1,,"In this section you will need the 18-gauge, 21-gauge, 23-gauge needle will be needed for trituration steps.",,"Add,Fact/Evidence",Fact/Evidence
10037,8-1983,8-1983_v2_46@2,,Trituration should be performed gently and slowly to avoid killing cells.,,"Add,Claim",Claim
10038,8-1983,8-1983_v2_46@3,,Hemocytometer will be needed to count cells .,,"Add,Fact/Evidence",Fact/Evidence
10039,8-1983,8-1983_v2_79@2,,Additional neurosphere staining using other antibodies can be found in previous published studies <REF-3> from our lab.,,"Add,Fact/Evidence",Fact/Evidence
10040,8-1983,8-1983_v2_83@4,,Another marker of an unhealthy culture is a large number of differentiated neurons surrounding neurospheres.,,"Add,Claim",Claim
10041,8-1983,8-1983_v2_83@5,,Indicators of differentiation are the large presence of axons and dendrites in your cultures.,,"Add,Claim",Claim
10042,8-1983,8-1983_v2_83@6,,This can be caused by depletion of growth factors.,,"Add,Claim",Claim
10043,8-1983,8-1983_v2_83@7,,"If this is the case, it is recommend that you increase the concentration of EGF.",,"Add,Claim",Claim
10044,8-1983,8-1983_v2_83@8,,Another cause of differentiation is too many cells in your prep.,,"Add,Claim",Claim
10045,8-1983,8-1983_v2_83@9,,This leads to over crowdedness.,,"Add,Claim",Claim
10046,8-1983,8-1983_v2_83@10,,It is recommend to split the culture to a lower density or decrease the number of neurons that are plated per 48 well.,,"Add,Claim",Claim
10047,8-1983,8-1983_v2_84@2,,"After trituration, if a uniform suspension has not been achieved an alternative method used in previous protocols are strainers <REF-15> .",,"Add,Fact/Evidence",Fact/Evidence
10168,8-42,8-42_v2_20@2,,"Note that as of the time of publication, this is in the Bioconductor development version.",,"Add,Fact/Evidence",Fact/Evidence
10169,8-42,8-42_v2_20@3,,"For details, see the Software Availability section.",,"Add,Fact/Evidence",Fact/Evidence
10170,8-42,8-42_v2_31@0,,Note that the results of the queries using the API and the packages may change as we continue to update the OMA database.,,"Add,Claim",Claim
10171,8-42,8-42_v2_31@1,,The OMA database release of June 2018 was used to generate the examples below.,,"Add,Fact/Evidence",Fact/Evidence
10172,8-42,8-42_v2_34@0,,Here we first formulate our URL of interest and use it to send a GET request to the API.,,"Add,Fact/Evidence",Fact/Evidence
10173,8-42,8-42_v2_34@1,,"This gives us the response JSON object, which can then be parsed into an R list.",,"Add,Fact/Evidence",Fact/Evidence
10174,8-42,8-42_v2_39@0,,The identified targets can be found in the seq_annotation$targets.,,"Add,Fact/Evidence",Fact/Evidence
10247,8-52,8-52_v2_11@1,,"This genotype is known to produce more yield, but susceptible to geminivirus infection.",,"Add,Claim",Claim
10248,8-52,8-52_v2_11@2,,"Resistant genotype however, is not available in our collection so far, so comparison of both two genotypes is not possible to be performed.",,"Add,Claim",Claim
10249,8-52,8-52_v2_23@0,,"PCR based cloning strategy in combination with primer walking was applied to isolate the complete full length of putative distal promoter NPR1 gene region (5,950 bp).",,"Add,Fact/Evidence",Fact/Evidence
10250,8-52,8-52_v2_23@1,,This approach is considered to be the most appropriate since the sequencing read capacity used in this study is limited for about only 500 bp on average.,,"Add,Claim",Claim
10251,8-52,8-52_v2_23@2,,"Even though the KOD-Plus-Neo could amplify up to 24 kb according to manufacturerโs claim, but the full-length fragment (5,950 bp) still can not be sequenced in one step read due to limited reading capacity of the sequencing machine.",,"Add,Fact/Evidence",Fact/Evidence
10252,8-52,8-52_v2_23@3,,Validation of every single nucleotide data was confirmed by at least two overlapping validated segment.,,"Add,Fact/Evidence",Fact/Evidence
10253,8-52,8-52_v2_24@2,,"Furthermore, BLAST analysis showed no significant homology with another promotor sequence available in the NCBI database, indicating a limitation of promoter sequence availability.",,"Add,Claim",Claim
10254,8-52,8-52_v2_24@3,,The only promoter sequence showed homology is the promoter region of Capsicum annuum pathogenesis related protein-1 (PR-1) gene ( DQ201633.1 ) published by Lee et al . (2006) .,,"Add,Fact/Evidence",Fact/Evidence
10255,8-52,8-52_v2_24@4,,"However, the comparable nucleotide of both sequences spanned only 180 bp.",,"Add,Fact/Evidence",Fact/Evidence
10256,8-52,8-52_v2_27@0,,"In order to validate our claim, we constructed a contiguous segment with our isolated core promoter ( MK310185.1 ) and the NPR1 gene sequence isolated from a similar genotype Berangkai ( Nova et al ., 2019 ).",,"Add,Fact/Evidence",Fact/Evidence
10257,8-52,8-52_v2_27@1,,"BLAST analysis using the NPR1-Berangkai cDNA sequence exhibited 43 significant identity hits with other cDNA sequences of NPR1 gene for instance, Capsicum annuum ( NM_001325099.1 ), Capsicum chinense ( AM900559.1 ), Solanum lycopersicum ( KX198701.1 , NM_001247629.2 ), Nicotiana sp. (DQ837218.1, AF480488.1) Carica papaya (XM_022041103.1, AY550242.1) and some others.",,"Add,Fact/Evidence",Fact/Evidence
10258,8-52,8-52_v2_27@2,,"Tree analysis showed that our NPR1-Berangkai cDNA sequence clustered to similar clade with AM900559.1 and NM_001325099.1 and other three solanaceae ( S. lycopersicum - KX198701.1 , NM_001247629.2 ; S. tuberosum -XM_006357647.2; S. pennellii -XM_015227358.2 and S. torvum -KJ995663.1).",,"Add,Fact/Evidence",Fact/Evidence
10259,8-52,8-52_v2_27@3,,All those data obviously indicated that our segment landed in the right chromosome segment.,,"Add,Claim",Claim
10260,8-52,8-52_v2_29@2,,The PlantCare analysis successfully showed 2 other cis -acting element motifs namely 1 TCA motif and 3 CGTA motis which could not be shown by PLACE ( Figure 3 ).,,"Add,Fact/Evidence",Fact/Evidence
10261,8-52,8-52_v2_38@0,,"Analysis with PlantCare ( Lescot et al ., 2002 ) resulting 2 new additional elements, which are designated TCA element and CGTA-motif ( Figure 3 ), while analysis with PlantPAN 3.0 did not show any new different element compared to PLACE analysis.",,"Add,Fact/Evidence",Fact/Evidence
10262,8-52,8-52_v2_38@1,,The TCA element has a function as a cis -acting element which is involved in salicylic acid responsiveness.,,"Add,Fact/Evidence",Fact/Evidence
10263,8-52,8-52_v2_38@2,,Moreover we also found the CGTA motif playing a role in regulating me-JA responsiveness.,,"Add,Fact/Evidence",Fact/Evidence
10264,8-52,8-52_v2_38@3,,In our sequence only 1 single TCA element and 3 CGTA motifs could be detected.,,"Add,Fact/Evidence",Fact/Evidence
10265,8-52,8-52_v2_40@3,,"However, their role in the gene expression has to be confirmed empirically.",,"Add,Claim",Claim
10266,8-52,8-52_v2_40@4,,"For that reason, we are currently using the cis -acting elements found in this study for replicase ( Rep ) gene expression in the bacterial system.",,"Add,Fact/Evidence",Fact/Evidence
10532,8-80,8-80_v2_19@4,,Face-to-face respondents were not compensated for their participation.,,"Add,Fact/Evidence",Fact/Evidence
10533,8-80,8-80_v2_19@5,,Online respondents were all members of an online panel company and received Reward Points.,,"Add,Fact/Evidence",Fact/Evidence
10534,8-80,8-80_v2_19@6,,"The number of points awarded for survey completion is based on survey length, complexity, and incidence rate.",,"Add,Fact/Evidence",Fact/Evidence
10535,8-80,8-80_v2_19@7,,"Once a points threshold is reached, panelists may redeem their points for online gift certificates or merchandise.",,"Add,Fact/Evidence",Fact/Evidence
10536,8-80,8-80_v2_19@8,,Each country has its own unique catalog.,,"Add,Fact/Evidence",Fact/Evidence
10776,9-1088,9-1088_v2_3@0,,Glossary,,"Add,Other",Other
10777,9-1088,9-1088_v2_28@3,,"Sunspots have been framed for communicable (CDs) and non-communicable disorders (NCDs), same as epidemics ( Hrushesky et al. , 2011 ; Stoupel et al. , 2003 ; Stienen et al. , 2015 ; Yeung, 2006 )",,"Add,Fact/Evidence",Fact/Evidence
10778,9-1088,9-1088_v2_31@4,,"Sunspot numbers correlation with other solar activity indices [UV/EUV, F10.7 flux (noontime measurement of the solar radio flux at a wavelength of 10.7 cm), Mg II] persisted at the same levels until 2000 ( Bruevich et al., 2014 ; Floyd et al., 2005 ).",,"Add,Fact/Evidence",Fact/Evidence
10779,9-1088,9-1088_v2_31@5,,"More importantly, they interact linearly except for the minima and maxima of the 11-year solar cycle, while these correlations vs sunspots and F10.7 flux, were shown to reach the lowest levels twice in each cycle ( Bruevich et al., 2014 ).",,"Add,Fact/Evidence",Fact/Evidence
10780,9-1088,9-1088_v2_31@6,,"Furthermore, the same study showed that a double-peak structure was observed in cycle 22 but not in cycle 21.",,"Add,Fact/Evidence",Fact/Evidence
10781,9-1088,9-1088_v2_31@7,,"These phenomena enhance the validity of our results, as the examined period comprise the minimum of the 21 st and the beginning of the maximum of the 22 nd cycle.",,"Add,Claim",Claim
10782,9-1088,9-1088_v2_31@8,,"It is established that the sunspot minima coincide with an increased flux of cosmic rays, whereas, in sunspot maxima the heliosphere is shielded by planetary magnetospheres- a phenomenon known as โForbush decreaseโ ( Raghav et al., 2017 ).",,"Add,Fact/Evidence",Fact/Evidence
10911,9-1193,9-1193_v2_31@3,,"Regarding non-drug interventions, 28 investigated procedures (e.g. renal replacement therapy), eight devices (e.g. various respiratory devices), and 26 trials investigated other non-drug interventions (e.g. physical activity and pulmonary rehabilitation).",,"Add,Fact/Evidence",Fact/Evidence
10912,9-1193,9-1193_v2_56@4,,"Other objective outcomes commonly used according to the COVID-19 core outcome sets <REF-20> , such as hospitalization and mechanical ventilation, may still be impacted by subjective decisions and the awareness of the randomly allocated intervention, and thus may benefit from a blinded assessment.",,"Add,Claim",Claim
10913,9-1193,9-1193_v2_63@5,,"Fourth, we did not assess in details all the different outcomes being used and the blinding of the outcome collection was not systematically reported preventing us from fully apprehending the impact of the lack of blinding.",,"Add,Claim",Claim
10914,9-1193,9-1193_v2_69@0,,The dataset for this study is provided on the Open Science Framework.,,"Add,Fact/Evidence",Fact/Evidence
10915,9-1193,9-1193_v2_69@1,,It is based on continuously evolving data sources.,,"Add,Fact/Evidence",Fact/Evidence
10916,9-1193,9-1193_v2_69@2,,"With this second version, we corrected misclassifications/duplicates affecting 6 of the 689 previously included trials.",,"Add,Fact/Evidence",Fact/Evidence
10917,9-1193,9-1193_v2_69@3,,Subsequent updates of COVID-evidence and the other sources may provide different datasets.,,"Add,Claim",Claim
11365,9-47,9-47_v2_17@4,,More advance distribution analysis is also available using the clonesizeDistribution function based on recent work using Jensen-Shannon divergence.,,"Add,Claim",Claim
11366,9-47,9-47_v2_25@3,,This function also works with the SingleCellExperiment and monocle3 class of expression objects.,,"Add,Fact/Evidence",Fact/Evidence
11367,9-47,9-47_v2_29@3,,"Visualization functions in scRepertoire have a parameter, exportTable, allowing users to examine the quantifications underlying the generation of the graphs.",,"Add,Claim",Claim
313,120-ARR,,120-ARR_v1_62@2,,8 We used the AllenNLP demo (https://demo.allennlp.org/sentiment-analysis/).,"Delete,Fact/Evidence",Fact/Evidence
314,120-ARR,,120-ARR_v1_11@2,,"For each (question, image) pair in the VQA dataset (left), VQA2.0 adds another image, for which the answer is different (right).","Delete,Fact/Evidence",Fact/Evidence
561,129-ARR,,129-ARR_v1_55@1,,"For methods that do not directly use prediction for uncertainty estimation, we aggregate the uncertainty value as g(x; ฮธ (t) ) = m t รa(x; ฮธ (t) )+(1โm t )รg(x; ฮธ (tโ1) ).","Delete,Fact/Evidence",Fact/Evidence
562,129-ARR,,129-ARR_v1_63@1,,Complexity Analysis.,"Delete,Other",Other
563,129-ARR,,129-ARR_v1_63@3,,"For self-training, the size of the memory bank g(x; ฮธ) is proportional to |X u |, while the extra computation of maintaining this dictionary is ignorable since we do not inference over the unlabeled data for multiple times in each round as BALD (Gal et al., 2017) does.","Delete,Fact/Evidence",Fact/Evidence
564,129-ARR,,129-ARR_v1_79@6,,"Gao et al. (2020); Song et al. (2019); Guo et al. (2021) design query strategies for specific semi-supervised methods, Tomanek and Hahn (2009); Rottmann et al. (2018); Simรฉoni et al. (2020) exploit the mostcertain samples from the unlabeled with pseudolabeling to augment the training set.","Delete,Fact/Evidence",Fact/Evidence
565,129-ARR,,129-ARR_v1_79@12,,"To alleviate this, we adopt a simple momentumbased method to select high confidence samples, effectively reducing the pseudo labels noise for active learning.","Delete,Fact/Evidence",Fact/Evidence
566,129-ARR,,129-ARR_v1_79@13,,"Note that although Mukherjee and Awadallah (2020); Rizve et al. ( 2021) also leverage uncertainty information for self-training, their focus is on developing better self-training methods, while we aim to jointly query high-uncertainty samples and generate pseudo-labels for low-uncertainty samples.","Delete,Claim",Claim
567,129-ARR,,129-ARR_v1_79@14,,"The experiments in Sec. 3 show that with appropriate querying methods, ACTUNE can further improve the performance of self-training.","Delete,Fact/Evidence",Fact/Evidence
568,129-ARR,,129-ARR_v1_82@1,,"Specifically, we search T 1 from 10 to 2000, T 2 from 1000 to 5000, T 3 from 10 to 500, ฮพ from 0 to 1, and ฮป from 0 to 0.5.","Delete,Fact/Evidence",Fact/Evidence
569,129-ARR,,129-ARR_v1_82@2,,All results are reported as the average over three runs.,"Delete,Fact/Evidence",Fact/Evidence
570,129-ARR,,129-ARR_v1_83@0,,"In our experiments, we keep ฮฒ = 0.5, ฮป = 1 for all datasets.","Delete,Fact/Evidence",Fact/Evidence
571,129-ARR,,129-ARR_v1_83@1,,"For other parameters, we use a grid search to find the optimal setting for each datasets.","Delete,Fact/Evidence",Fact/Evidence
572,129-ARR,,129-ARR_v1_83@2,,"Specifically, we search ฮณ from [0.5, 0.6, 0.7], m L from [0.6, 0.7, 0.8], m H from [0.8, 0.9, 1].","Delete,Fact/Evidence",Fact/Evidence
573,129-ARR,,129-ARR_v1_83@3,,"For AC-TUNE with Entropy, we use probability based aggregation and for ACTUNE with CAL, we use value based aggregation by default.","Delete,Fact/Evidence",Fact/Evidence
574,129-ARR,,129-ARR_v1_84@0,,"For other SSAL methods, we mainly tune their key hyperparameters.","Delete,Fact/Evidence",Fact/Evidence
575,129-ARR,,129-ARR_v1_84@1,,"Note that Entropy (Holub et al., 2008), BALD (Gal et al., 2017), ALPS (Yuan et al., 2020), BADGE (Ash et al., 2020) do not introduce any new hyperparameters.","Delete,Fact/Evidence",Fact/Evidence
576,129-ARR,,129-ARR_v1_84@2,,"For CAL (Margatina et al., 2021b), we tune the number for KNN k from [5,10,20] and report the best performance.","Delete,Fact/Evidence",Fact/Evidence
577,129-ARR,,129-ARR_v1_84@3,,"For ST (Lee, 2013), CEAL (Wang et al., 2016) & BASS (Rottmann et al., 2018), it uses a threshold ฮด for selecting high-confidence data.","Delete,Fact/Evidence",Fact/Evidence
578,129-ARR,,129-ARR_v1_84@4,,"We tune ฮด from [0.6, 0.7, 0.8, 0.9] to report the best performance.","Delete,Fact/Evidence",Fact/Evidence
579,129-ARR,,129-ARR_v1_84@5,,"For UST (Mukherjee and Awadallah, 2020), we tune the number of lowuncertainty samples used in the next round from [1024,2048,4096].","Delete,Fact/Evidence",Fact/Evidence
580,129-ARR,,129-ARR_v1_84@6,,"For COSINE (Yu et al., 2021), we set the weight for confidence regularization ฮป as 0.1, the threshold ฯ for selecting high-confidence data from [0.7, 0.9] and the update period of selftraining from [50,100,150].","Delete,Fact/Evidence",Fact/Evidence
581,129-ARR,,129-ARR_v1_84@7,,"For REVIVAL (Guo et al., 2021), it calculates uncertainty with adversarial perturbation, we tune the size of the perturbation from [1e โ 3, 1e โ 4, 1e โ 5].","Delete,Fact/Evidence",Fact/Evidence
582,129-ARR,,129-ARR_v1_85@0,,C Runtime Analysis.,"Delete,Other",Other
583,129-ARR,,129-ARR_v1_86@2,,"Among all baselines, we find that the running time of clustering-based method is faster than the original reported time in the paper.","Delete,Fact/Evidence",Fact/Evidence
584,129-ARR,,129-ARR_v1_86@3,,"This is because we use FAISS (Johnson et al., 2019) instead of SKLearn (Pedregosa et al., 2011) for clustering, which accelerates the clustering step significantly.","Delete,Fact/Evidence",Fact/Evidence
585,129-ARR,,129-ARR_v1_88@2,,Different colors stands for different classes.,"Delete,Fact/Evidence",Fact/Evidence
716,136-ARR,,136-ARR_v1_26@0,,The first step in our pipeline involves transforming each of the input triples X into a set of facts F in natural language by using a template t p i for each predicate p i .,"Delete,Fact/Evidence",Fact/Evidence
717,136-ARR,,136-ARR_v1_26@1,,We need at least one template for each predicate.,"Delete,Fact/Evidence",Fact/Evidence
718,136-ARR,,136-ARR_v1_26@2,,"Typically, the template will include placeholders which are filled with s i and o i .","Delete,Fact/Evidence",Fact/Evidence
719,136-ARR,,136-ARR_v1_27@1,,Note that the filled templates are allowed to contain minor disfluencies since the text will be rephrased in the final step of the pipeline.,"Delete,Fact/Evidence",Fact/Evidence
720,136-ARR,,136-ARR_v1_27@2,,See ยง5.5 for our approach to gathering the templates and Figure 2 for examples of the templates we use in our datasets.,"Delete,Fact/Evidence",Fact/Evidence
721,136-ARR,,136-ARR_v1_28@0,,"We acknowledge that this step may be a bottleneck on datasets with an unconstrained (or very large) set of predicates, which is why we also discuss possibilities for automating this step in ยง7.","Delete,Claim",Claim
722,136-ARR,,136-ARR_v1_30@1,,"To maximize the coherency of the resulting description, we apply an ordering model O to get an ordered sequence of facts: F o "" tf o 1 , . . . , f on u.","Delete,Fact/Evidence",Fact/Evidence
723,136-ARR,,136-ARR_v1_30@2,,"The coherence of the final text will also depend on the paragraph compression step, but grouping related facts together (e.g. facts mentioning birth date and birth place) helps the paragraph compression model to focus only on fusing and rephrasing the neighboring sentences.","Delete,Claim",Claim
724,136-ARR,,136-ARR_v1_36@0,,A key to our approach is building a large-scale synthetic corpus providing training data for the text operations in our pipeline.,"Delete,Fact/Evidence",Fact/Evidence
725,136-ARR,,136-ARR_v1_46@1,,"The paragraphs contain mostly concise, factbased descriptions from a wide range of domains.","Delete,Fact/Evidence",Fact/Evidence
726,136-ARR,,136-ARR_v1_50@2,,"Since we keep the referring expressions in the original human-written text, we can train the paragraph compression module to generate them in the final text description.","Delete,Fact/Evidence",Fact/Evidence
727,136-ARR,,136-ARR_v1_54@0,,We show how we build our pipeline ( ยง5.1-5.4) and discuss the D2T generation datasets which we use for our experiments ( ยง5.5).,"Delete,Fact/Evidence",Fact/Evidence
728,136-ARR,,136-ARR_v1_54@1,,The details of our training setup are included in Appendix B.,"Delete,Fact/Evidence",Fact/Evidence
729,136-ARR,,136-ARR_v1_67@0,,Ablation Study,"Delete,Other",Other
730,136-ARR,,136-ARR_v1_70@0,,D2T Datasets,"Delete,Other",Other
731,136-ARR,,136-ARR_v1_71@0,,"We test our approach on two English D2T datasets, WebNLG and E2E.","Delete,Fact/Evidence",Fact/Evidence
732,136-ARR,,136-ARR_v1_72@0,,"WebNLG The WebNLG dataset contains RDF triples from DBPedia (Auer et al., 2007) and their crowdsourced descriptions.","Delete,Fact/Evidence",Fact/Evidence
733,136-ARR,,136-ARR_v1_72@1,,"The dataset was extended for the WebNLG+ Challenge (Ferreira et al., 2020), but we use the version 1.4 for comparability to prior work.","Delete,Fact/Evidence",Fact/Evidence
734,136-ARR,,136-ARR_v1_72@2,,Templates for WebNLG could be extracted from the training data by delexicalizing single-triple examples.,"Delete,Fact/Evidence",Fact/Evidence
735,136-ARR,,136-ARR_v1_72@3,,"However, the examples are noisy and such data would not be available in a zero-shot setup.","Delete,Claim",Claim
736,136-ARR,,136-ARR_v1_72@4,,"Therefore, we handcrafted templates for all 354 predicates, including unseen predicates in the test set.","Delete,Fact/Evidence",Fact/Evidence
737,136-ARR,,136-ARR_v1_72@6,,"We use the cleaned version of the dataset (Duลกek et al., 2019).","Delete,Fact/Evidence",Fact/Evidence
738,136-ARR,,136-ARR_v1_72@7,,"Following previous work, we transformed the attribute-value pairs into RDF triples (using the restaurant name as a subject) and then applied the same setup as for WebNLG.","Delete,Fact/Evidence",Fact/Evidence
739,136-ARR,,136-ARR_v1_72@8,,We created a template for each of the 8 attributes manually.,"Delete,Fact/Evidence",Fact/Evidence
740,136-ARR,,136-ARR_v1_78@2,,"For WebNLG (see Table 2), we include the results of UPF-FORGe and MELBOURNE systems from the first run of WebNLG Challenge which are comparable in terms of automatic metrics and semantic errors, and the results of Ke et al. (2021), which is a state-of-theart system using structure-aware encoder and taskspecific pretraining.","Delete,Fact/Evidence",Fact/Evidence
741,136-ARR,,136-ARR_v1_78@3,,Laha et al. ( 2020) is (to our knowledge) the only other zero-shot D2T generation system applied on WebNLG.,"Delete,Claim",Claim
742,136-ARR,,136-ARR_v1_78@4,,"TGEN (Duลกek and Jurฤรญฤek, 2015) is the baseline system for the E2E Challenge and Harkous et al. (2020) is a state-of-the art supervised system applied on the cleaned E2E (see Table 3).","Delete,Fact/Evidence",Fact/Evidence
743,136-ARR,,136-ARR_v1_78@5,,"For both datasets, COPY is the baseline of copying the templates verbatim.","Delete,Fact/Evidence",Fact/Evidence
744,136-ARR,,136-ARR_v1_79@1,,"The COPY baseline is substantially better than the zero-shot system of Laha et al. (2020), suggesting that quality of the templates plays an important role.","Delete,Claim",Claim
745,136-ARR,,136-ARR_v1_80@0,,Manual Evaluation,"Delete,Other",Other
746,136-ARR,,136-ARR_v1_81@8,,"As we discuss in ยง7, more research is needed for ensuring the final consistency of the text.","Delete,Claim",Claim
747,136-ARR,,136-ARR_v1_85@1,,RANDOM is the baseline of generating a random order.,"Delete,Fact/Evidence",Fact/Evidence
748,136-ARR,,136-ARR_v1_85@2,,"The results show that although our approach lacks behind stateof-the-art supervised approaches, it can outperform both the random baseline and the Transformerbased approach from Ferreira et al. (2019) while not using any training examples from WebNLG.","Delete,Claim",Claim
749,136-ARR,,136-ARR_v1_91@0,,"In the current form, our pipeline can be directly applied to generating text from RDF triples (or similarly structured data) which require no extra processing.","Delete,Claim",Claim
750,136-ARR,,136-ARR_v1_91@1,,"Further extensions are needed for more complex D2T scenarios, e.g. datasets requiring content selection or common-sense and logical reasoning (Wiseman et al., 2017;Chen et al., 2020b).","Delete,Claim",Claim
751,136-ARR,,136-ARR_v1_92@0,,Our approach regarding handcrafting a single template for each predicate is quite basic.,"Delete,Claim",Claim
752,136-ARR,,136-ARR_v1_92@2,,"Moreover, explicitly including a denoising task for the paragraph compression model could help to tackle the disfluencies in the templates.","Delete,Claim",Claim
753,136-ARR,,136-ARR_v1_94@0,,Statistics for the datasets described in the paper are listed in Table 7.,"Delete,Fact/Evidence",Fact/Evidence
760,136-ARR,,136-ARR_v1_11@1,,"They also build a three-step pipeline for zero-shot D2T generation, but they use handcrafted rules for producing the output text and do not address content planning.","Delete,Fact/Evidence",Fact/Evidence
781,136-ARR,,136-ARR_v1_12@2,,"Recently, have shown that using a content plan leads to improved quality of PLM outputs.","Delete,Fact/Evidence",Fact/Evidence
827,136-ARR,,136-ARR_v1_16@0,,We first give an overview of our neural D2T generation pipeline ( ยง3.1).,"Delete,Fact/Evidence",Fact/Evidence
828,136-ARR,,136-ARR_v1_17@0,,Method Overview,"Delete,Other",Other
1343,166-ARR,,166-ARR_v1_37@1,,"For our usage, the input is the contextual token embedding (the L-th layer's output), and the side information is the static token embedding (the output of BERT's initial embedding layer).","Delete,Fact/Evidence",Fact/Evidence
1344,166-ARR,,166-ARR_v1_37@2,,"The resulting cdimensional encoded vector can be thought of as the distilled context of the input token. (Hendrycks and Gimpel, 2016).","Delete,Fact/Evidence",Fact/Evidence
1345,166-ARR,,166-ARR_v1_59@0,,3 The checkpoints will be released with the published paper.,"Delete,Fact/Evidence",Fact/Evidence
1480,172-ARR,,172-ARR_v1_45@2,,"XDBERT-b gains performance on 52 entries, while performing on par or worse on 225 entries.","Delete,Fact/Evidence",Fact/Evidence
1481,172-ARR,,172-ARR_v1_48@0,,We briefly describe the ablation studies on the adaptation process.,"Delete,Fact/Evidence",Fact/Evidence
1482,172-ARR,,172-ARR_v1_51@0,,"We show the result of Visual-Text Transformers on GLUE, reported by Tan and Bansal (2020) in Table 6.","Delete,Fact/Evidence",Fact/Evidence
1483,172-ARR,,172-ARR_v1_51@1,,All of the listed methods (except LXMERT) have their text-transformers initialized from BERT.,"Delete,Fact/Evidence",Fact/Evidence
1484,172-ARR,,172-ARR_v1_52@0,,The results show that multi-modal training for solving vision-language tasks does not improve the performance of the models on natural language understanding tasks.,"Delete,Fact/Evidence",Fact/Evidence
1485,172-ARR,,172-ARR_v1_62@0,,We report the performance of small tasks while using different loss functions.,"Delete,Fact/Evidence",Fact/Evidence
1486,172-ARR,,172-ARR_v1_63@3,,"Driven by the loss function of cross-modal matching, the model sacrifices rare word representations to better match common words.","Delete,Fact/Evidence",Fact/Evidence
1487,172-ARR,,172-ARR_v1_63@4,,"By adding VC loss, each VC token must keep its representation, which makes the cross-modal attention more robust.","Delete,Claim",Claim
1488,172-ARR,,172-ARR_v1_64@2,,"In the following examples, Bold words are visuallygrounded, while normal words are non-visuallygrounded.","Delete,Fact/Evidence",Fact/Evidence
1489,172-ARR,,172-ARR_v1_64@3,,Words in brackets are stopwords and does not count towards either category.,"Delete,Fact/Evidence",Fact/Evidence
1498,172-ARR,,172-ARR_v1_36@1,,This loss prevents the cross-modal matching to only focus on common trivial words.,"Delete,Fact/Evidence",Fact/Evidence
1499,172-ARR,,172-ARR_v1_36@2,,We show the attention maps of the cross-modal encoders in Appendix D to verify this.,"Delete,Fact/Evidence",Fact/Evidence
1676,178-ARR,,178-ARR_v1_7@3,,We will release our codes and models for further research.,"Delete,Fact/Evidence",Fact/Evidence
1963,192-ARR,,192-ARR_v1_6@5,,"Our model is developed on the basis of (Wu et al., 2020a).","Delete,Fact/Evidence",Fact/Evidence
2017,193-ARR,,193-ARR_v1_28@3,,5 We provide additional analysis on spelling accuracy by subword frequency and length in Appendices B and C.,"Delete,Fact/Evidence",Fact/Evidence
2018,193-ARR,,193-ARR_v1_36@0,,We manually analyze 100 random tokens that SpellingBee spelled incorrectly with the lemma filter to understand the nature of the spelling mistakes.,"Delete,Fact/Evidence",Fact/Evidence
2019,193-ARR,,193-ARR_v1_37@0,,Out of those 100 we display 20 mistakes in Table 3 alongside the spelling prediction of the control baseline.,"Delete,Fact/Evidence",Fact/Evidence
2020,193-ARR,,193-ARR_v1_38@0,,"We implement SpellingBee with a 6-layer encoderdecoder model, with 512 model dimensions.","Delete,Fact/Evidence",Fact/Evidence
2021,193-ARR,,193-ARR_v1_39@0,,"The model parameters are optimized with Adam (Kingma and Ba, 2015) for 1000 steps with up to 1024 tokens per batch, a learning rate of 5e-4, and a dropout rate of 0.1.","Delete,Fact/Evidence",Fact/Evidence
2022,193-ARR,,193-ARR_v1_39@1,,These are the default hyperparameters for training a transformer language model in Fairseq .,"Delete,Fact/Evidence",Fact/Evidence
2047,195-ARR,,195-ARR_v1_26@0,,Overall quality and gender translation,"Delete,Other",Other
2048,195-ARR,,195-ARR_v1_27@0,,"Table 3 presents SacreBLEU (Post, 2018), 14 coverage, and gender accuracy scores on the MuST-SHE test sets.","Delete,Fact/Evidence",Fact/Evidence
2049,195-ARR,,195-ARR_v1_27@2,,"Also, in line with previous analyses , SMALL-BPE models outperform the CHAR ones by โผ1 BLEU point.","Delete,Fact/Evidence",Fact/Evidence
2050,195-ARR,,195-ARR_v1_27@3,,"The higher overall translation quality of LARGE-BPE models is also reflected by the coverage scores (All-Cov), where 13 The scripts will be released upon paper acceptance.","Delete,Fact/Evidence",Fact/Evidence
2051,195-ARR,,195-ARR_v1_37@7,,18 We noticed that CHAR's lower translation quality may have to do more with fluency rather than lexical issues.,"Delete,Claim",Claim
2052,195-ARR,,195-ARR_v1_52@0,,A MuST-SHE Manual Annotation POS and agreement chains annotations were carried out on MuST-SHE reference translations.,"Delete,Fact/Evidence",Fact/Evidence
2053,195-ARR,,195-ARR_v1_52@1,,"To ensure precision, the two layers of linguistic information have been added i) in the course of two separate annotation processes; ii) following strict and comprehensive guidelines.","Delete,Fact/Evidence",Fact/Evidence
2054,195-ARR,,195-ARR_v1_62@0,,"Considering long-range dependencies that go beyond the phrase level, a gender relevant covariation is also that of subject-verb agreement, as the one shown in Table 9 (see also footnote 1).","Delete,Fact/Evidence",Fact/Evidence
2055,195-ARR,,195-ARR_v1_63@0,,"To account for such longer spans, we considered all MuST-SHE sentences where both i) a word (or chain) functioning as a subject, and ii) its referring verb or predicative complement are annotated as gender-marked words in the references.","Delete,Fact/Evidence",Fact/Evidence
2056,195-ARR,,195-ARR_v1_63@1,,"We identified 55 sentences for en-es, 54 for en-fr and 41 for en-it, and we manually analyzed all the corresponding systems' outputs.","Delete,Fact/Evidence",Fact/Evidence
2057,195-ARR,,195-ARR_v1_64@0,,"We found that, even in the case of dependencies within a longer range, systems largely respect agreement in translation and consistently pick the same gender form for all co-related words.","Delete,Fact/Evidence",Fact/Evidence
2058,195-ARR,,195-ARR_v1_64@2,,"Among these 4 cases, we found the above discussed weaker gender-enforcing structures (see the description of (semi-)copula verbs and their predicative complements in ยง6.2), and we also detected what resembles agreement attraction errors (Linzen et al., 2016).","Delete,Fact/Evidence",Fact/Evidence
2059,195-ARR,,195-ARR_v1_64@4,,"The following (long) sentence is an example of such an attraction error, where the complement desperate is inflected in the same masculine and plural form as its just preceding noun rather than the chain functioning as subject (the nurse).","Delete,Fact/Evidence",Fact/Evidence
2060,195-ARR,,195-ARR_v1_65@1,,"We can thus conclude that, even taking into account longer dependencies, agreement still does not emerge as an issue entrenched with gender bias.","Delete,Claim",Claim
2364,21-ARR,,21-ARR_v1_58@1,,"For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.","Delete,Claim",Claim
2682,246-ARR,,246-ARR_v1_71@5,,"Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).","Delete,Fact/Evidence",Fact/Evidence
2702,249-ARR,,249-ARR_v1_27@2,,"To avoid black-box persona inference attacks, more constraints on privacy should be added for training LM-based chatbots.","Delete,Claim",Claim
2704,249-ARR,,249-ARR_v1_12@2,,Carlini et al. (2021) performed black-box model inversion attack on GPT-2 through descriptive prompts with beam search.,"Delete,Fact/Evidence",Fact/Evidence
2705,249-ARR,,249-ARR_v1_12@3,,Lehman et al. (2021) examined BERT pretrained on Electronic Health Records via blank filling and model probing to recover Personal Health Information.,"Delete,Fact/Evidence",Fact/Evidence
2706,249-ARR,,249-ARR_v1_12@4,,"Furthermore, given black-box access to a language model's pre-train and finetune stages, Zanella-Bรฉguelin et al. (2020) showed that sensitive sequences of the fine-tuning dataset can be extracted.","Delete,Fact/Evidence",Fact/Evidence
2707,249-ARR,,249-ARR_v1_12@5,,"For the distributed client-server setup, Malekzadeh et al. (2021) considered the sensitive attribute leakage from the server side with honest-but-curious (HBC) classifiers.","Delete,Fact/Evidence",Fact/Evidence
2708,249-ARR,,249-ARR_v1_15@3,,"Lastly, we comprehensively explain our proposed defense strategies in Section 3.3.","Delete,Fact/Evidence",Fact/Evidence
2746,250-ARR,,250-ARR_v1_43@3,,Hyperparameters selection and other implementation details can be found in Appendix.,"Delete,Fact/Evidence",Fact/Evidence
2838,258-ARR,,258-ARR_v1_31@0,,Experiments,"Delete,Other",Other
2839,258-ARR,,258-ARR_v1_32@0,,Setup,"Delete,Other",Other
2840,258-ARR,,258-ARR_v1_33@0,,We use the pre-trained ELECTRA base discriminator model 5 with 12 attention heads and 12 hidden layers.,"Delete,Fact/Evidence",Fact/Evidence
2869,276-ARR,,276-ARR_v1_45@0,,See Appendix B for proof.,"Delete,Fact/Evidence",Fact/Evidence
2870,276-ARR,,276-ARR_v1_46@0,,See Appendix C for proof.,"Delete,Fact/Evidence",Fact/Evidence
2871,276-ARR,,276-ARR_v1_55@3,,"7 For example, we repeated the full experiment on ADePT (n = 2, ฮต = 0.1) 100 times which results in standard deviation 0.0008 from the mean value 0.195.","Delete,Fact/Evidence",Fact/Evidence
2872,276-ARR,,276-ARR_v1_55@4,,Better MLE precision can be simply obtained by increasing the 10 million repeats per experiment.,"Delete,Claim",Claim
2873,276-ARR,,276-ARR_v1_55@5,,All source codes are attached/will go on Github.,"Delete,Fact/Evidence",Fact/Evidence
2874,276-ARR,,276-ARR_v1_84@0,,See Figure 2.,"Delete,Fact/Evidence",Fact/Evidence
3084,30-ARR,,30-ARR_v1_14@1,,"In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.","Delete,Fact/Evidence",Fact/Evidence
3085,30-ARR,,30-ARR_v1_14@2,,"The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD using the full augmented training set of 3,414 examples.","Delete,Fact/Evidence",Fact/Evidence
3088,30-ARR,,30-ARR_v1_15@0,,"To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and humanintervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.","Delete,Claim",Claim
3089,30-ARR,,30-ARR_v1_15@1,,All resources will be released on Github.,"Delete,Fact/Evidence",Fact/Evidence
3090,30-ARR,,30-ARR_v1_16@0,,Related Work,"Delete,Other",Other
3091,30-ARR,,30-ARR_v1_17@0,,"Data augmentation has been used for resolving artefacts in training datasets before (Gururangan et al., 2018;Srivastava et al., 2020;Kaushik et al., 2021).","Delete,Fact/Evidence",Fact/Evidence
3284,315-ARR,,315-ARR_v1_32@8,,Drop#1 is probably because the constraint words within this range are mostly functional or less important.,"Delete,Claim",Claim
3285,315-ARR,,315-ARR_v1_32@9,,Such words are not as universal as ones at the leftmost that can fit in most contexts and do not have to appear in the target due to multiple modes in translation.,"Delete,Claim",Claim
3286,315-ARR,,315-ARR_v1_73@2,,"The purpose of this work is to enhance existing methods for constrained NAT, i.e., editing-based iterative NAT methods, under rare lexical constraints.","Delete,Fact/Evidence",Fact/Evidence
3287,315-ARR,,315-ARR_v1_73@3,,"It would be interesting for future research to explore new ways to impose lexical constraints on NAT models, perhaps on non-iterative NAT.","Delete,Claim",Claim
3379,326-ARR,,326-ARR_v1_54@3,,All datasets are modified into truecase format with mosesdecoder by training truecase models upon train set.,"Delete,Fact/Evidence",Fact/Evidence
3380,326-ARR,,326-ARR_v1_59@1,,"Note that, we follow suggestions in Song et al. (2021) to omit the energy calculation of activate functions, such as relu and softmax, because they are specially designed over some modern AI chips, which requires far less energy than additive operation.","Delete,Fact/Evidence",Fact/Evidence
3381,326-ARR,,326-ARR_v1_65@0,,"In order to obtain value representations, attention model requires ld 2 additive/multiplicative operations.","Delete,Fact/Evidence",Fact/Evidence
3382,326-ARR,,326-ARR_v1_65@1,,"Besides, applying weighted sum over value representations with attention weights requires l 2 d multiplicative/additive operations.","Delete,Fact/Evidence",Fact/Evidence
3383,326-ARR,,326-ARR_v1_65@2,,"Overall, the numbers of multiplicative/additive operations in the whole attention model are 3ld 2 + 2l 2 d.","Delete,Fact/Evidence",Fact/Evidence
3384,326-ARR,,326-ARR_v1_66@1,,The overall multiplication operations in a Transformer Block is 9ld 2 + 3ld 2 + 2l 2 d = 12ld 2 + 2l 2 d.,"Delete,Fact/Evidence",Fact/Evidence
3385,326-ARR,,326-ARR_v1_67@0,,"Same as the steps above, we can calculate the required energy consumption of other modules.","Delete,Fact/Evidence",Fact/Evidence
3386,326-ARR,,326-ARR_v1_68@1,,"We can get the result โ A = 34.09%, โ F = 33.83%, thus the energy reduction ratio is 1โโ A = 65.91%, 1โโ F = 66.17%.","Delete,Fact/Evidence",Fact/Evidence
3387,326-ARR,,326-ARR_v1_68@2,,"Similarly, energy consumption ratios at the level of alignment calculation are 99.55% and 99.95%.","Delete,Fact/Evidence",Fact/Evidence
3388,326-ARR,,326-ARR_v1_68@3,,"Those of TRANSFORMER block are 83.17% and 83.10%, respectively.","Delete,Fact/Evidence",Fact/Evidence
3390,326-ARR,,326-ARR_v1_20@5,,"Although Dense and RandInit significantly reduce the energy consumption, Tay et al. (2021) point out that these approaches fail to be employed into cross-attention networks, since neither linear transition nor randomly initialized matrix is able to exactly model alignment information across languages.","Delete,Fact/Evidence",Fact/Evidence
3391,326-ARR,,326-ARR_v1_2@5,,Our code will be released upon the acceptance.,"Delete,Claim",Claim
3392,326-ARR,,326-ARR_v1_40@0,,"We choose three machine translation tasks, i.e. IWSLT'15 English-Vietnamese (En-Vi), WMT'14 English-German (En-De) and WMT'17 Chinese-English (Zh-En), to evaluate the effectiveness of our approach.","Delete,Fact/Evidence",Fact/Evidence
3393,326-ARR,,326-ARR_v1_40@1,,"We follow the setting of TRANS-FORMER-Base (Vaswani et al., 2017)","Delete,Fact/Evidence",Fact/Evidence
3399,327-ARR,,327-ARR_v1_2@5,,We compute them on a model's trained token embeddings to prevent domain mismatch.,"Delete,Fact/Evidence",Fact/Evidence
3562,344-ARR,,344-ARR_v1_37@7,,Domain Setups.,"Delete,Other",Other
3563,344-ARR,,344-ARR_v1_49@2,,17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups.,"Delete,Fact/Evidence",Fact/Evidence
3564,344-ARR,,344-ARR_v1_49@3,,"We repeated the similar hyper-parameter search procedure for QA-based models, using ALB-QA.","Delete,Fact/Evidence",Fact/Evidence
3723,350-ARR,,350-ARR_v1_52@3,,"For this language XLSR-53 training converges unusually fast and floors to a high PER; in contrast, HMM-GMM and k2-conf results on this language are their best in the private dataset.","Delete,Fact/Evidence",Fact/Evidence
3724,350-ARR,,350-ARR_v1_52@5,,"The HMM-GMM system obtains the second-lowest average PER (41.1%) for public languages but is third for the private dataset, with 13.2% PER.","Delete,Fact/Evidence",Fact/Evidence
4069,40-ARR,,40-ARR_v1_32@1,,"On SST, correlations with E-Z Reader are very consistent across POS tags whereas attention flow shows weak correlations on proper nouns (.12), nouns (.16) and verbs (.16) as presented in Figure 2.","Delete,Fact/Evidence",Fact/Evidence
4070,40-ARR,,40-ARR_v1_32@2,,The BNC frequency baseline correlates well with human fixations on adpositions (ADP) which both assign comparably low values.,"Delete,Fact/Evidence",Fact/Evidence
4071,40-ARR,,40-ARR_v1_32@3,,Proper nouns (PROPN) are overestimated in BNC as a result of their infrequent occurrence.,"Delete,Fact/Evidence",Fact/Evidence
4072,40-ARR,,40-ARR_v1_52@2,,Resulting perplexity on the held-out test set was ppl = 81.9.,"Delete,Fact/Evidence",Fact/Evidence
4073,40-ARR,,40-ARR_v1_52@3,,"Then, word-based total fixation times are computed from the E-Z Readers trace files and averaged over all subjects.","Delete,Fact/Evidence",Fact/Evidence
4074,40-ARR,,40-ARR_v1_53@0,,"In addition to Spearman correlation over all tokens, we also report Pearson correlation coefficients on a sentence and token-level.","Delete,Fact/Evidence",Fact/Evidence
4075,40-ARR,,40-ARR_v1_53@1,,Results are displayed in Table 4.,"Delete,Fact/Evidence",Fact/Evidence
4076,40-ARR,,40-ARR_v1_53@2,,"Compared to Spearman correlation on all tokens, the ranking does hardly change for Pearson or sentence-level correlations.","Delete,Fact/Evidence",Fact/Evidence
4077,40-ARR,,40-ARR_v1_53@3,,Absolute correlation coefficients are higher for Spearman compared to Pearson and also are slightly higher on the sentence-level as compared to the tokenlevel analysis.,"Delete,Fact/Evidence",Fact/Evidence
4078,40-ARR,,40-ARR_v1_53@4,,Biggest changes occur in a drop for BNC when Spearman correlation is calculated on all tokens for relation extraction and an increase for self-attention (LRP) in sentiment reading.,"Delete,Fact/Evidence",Fact/Evidence
4079,40-ARR,,40-ARR_v1_53@5,,We hypothesize that both effects can be traced back to the level of sparsity and the corresponding ranking for Spearman correlations.,"Delete,Claim",Claim
4080,40-ARR,,40-ARR_v1_53@6,,"In our entropy analysis we found that, i.e. self-attention shows a sparser representation which was likely caused by the overconfidence of the model, and which could explain the higher rank-based correlation.","Delete,Claim",Claim
4081,40-ARR,,40-ARR_v1_54@0,,Figure 5 shows the full distribution of POS tags of the first tokens flipped.,"Delete,Fact/Evidence",Fact/Evidence
4082,40-ARR,,40-ARR_v1_54@1,,This extends Figure 4 where we only show the first 3 POS tags.,"Delete,Fact/Evidence",Fact/Evidence
4083,40-ARR,,40-ARR_v1_55@0,,We compute entropy values for different attention and relevance scores in both task settings.,"Delete,Fact/Evidence",Fact/Evidence
4103,402-ARR,,402-ARR_v1_17@5,,We include both the HTML and text version of the premise articles.,"Delete,Fact/Evidence",Fact/Evidence
4104,402-ARR,,402-ARR_v1_17@6,,We perform minimal cleanup in order for our dataset to reflect the real life challenges present in the task of automated fact checking.,"Delete,Fact/Evidence",Fact/Evidence
4105,402-ARR,,402-ARR_v1_33@0,,TF-IDF based similarity measure is commonly used in NLP tasks to retrieve texts similar to the target text from a large corpus.,"Delete,Claim",Claim
4106,402-ARR,,402-ARR_v1_43@0,,We evaluate the model using the claims and the associated review articles in the validation and test set.,"Delete,Fact/Evidence",Fact/Evidence
4112,402-ARR,,402-ARR_v1_12@2,,"To the best of our knowledge, none of the existing work considers the problem of claim verification based on premise articles.","Delete,Claim",Claim
4224,412-ARR,,412-ARR_v1_74@7,,"However, the remaining models improve upon the starting architecture, with the largest improvements being observed for domain adaptation and the text simplification auxiliary task (Base + Text simplification + DA), with a Pearson correlation coefficient on the test dataset of .7744, 2.42% better than the base model.","Delete,Fact/Evidence",Fact/Evidence
4225,412-ARR,,412-ARR_v1_74@8,,The improved performance can be also seen for the Mean Absolute Error score (MAE = .0652).,"Delete,Fact/Evidence",Fact/Evidence
4226,412-ARR,,412-ARR_v1_75@0,,"While the Transformer decoder auxiliary task does not offer the best performance for the single word dataset, the same architecture offers the second-best performance for the multiple word dataset, with a Pearson score of .8252 compared to the best one, .8285.","Delete,Fact/Evidence",Fact/Evidence
4227,412-ARR,,412-ARR_v1_76@0,,"The domain adaptation and VAE configuration provide improvements upon the base model (.7554 versus .7502 Pearson), but the VAE does not have an important contribution, considering that the Base + domain adaptation model has a slightly higher Pearson score of .7569.","Delete,Fact/Evidence",Fact/Evidence
4422,426-ARR,,426-ARR_v1_25@4,,"Some Key Information Extraction tasks are presented in (Stanisลawek et al., 2021).","Delete,Fact/Evidence",Fact/Evidence
4423,426-ARR,,426-ARR_v1_25@5,,"The two datasets described there contain, respectively, NDA documents and financial reports from charity organizations.","Delete,Fact/Evidence",Fact/Evidence
4424,426-ARR,,426-ARR_v1_25@6,,"The tasks for the datasets consist in detecting data points, such as effective dates, interested parties, charity address, income, spending.","Delete,Fact/Evidence",Fact/Evidence
4425,426-ARR,,426-ARR_v1_25@7,,"The authors provide several baseline solutions for the two tasks, which apply up-to-date methods, pointing out that there is still room for improvement in the KIE research area.","Delete,Fact/Evidence",Fact/Evidence
4426,426-ARR,,426-ARR_v1_25@8,,"A challenge that comprises both layout recognition and KIE is presented in (Huang et al., 2019) -the challenge is opened for the recognition of OCR-scanned receipts.","Delete,Fact/Evidence",Fact/Evidence
4729,463-ARR,,463-ARR_v1_28@1,,"In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important.","Delete,Fact/Evidence",Fact/Evidence
4730,463-ARR,,463-ARR_v1_40@1,,All results are scored by the GLUE evaluate server.,"Delete,Fact/Evidence",Fact/Evidence
4731,463-ARR,,463-ARR_v1_63@3,,"In addition, we demonstrate the robustness of AdapterBias to different PLMs.","Delete,Fact/Evidence",Fact/Evidence
4732,463-ARR,,463-ARR_v1_63@4,,"Finally, we provide analysis on what AdapterBias learns by comparing ฮฑ, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information.","Delete,Fact/Evidence",Fact/Evidence
4733,463-ARR,,463-ARR_v1_63@5,,Our study overturns previous architectures of adapters by proposing a simple adapter that can produce suitable representation shifts for different tokens.,"Delete,Claim",Claim
4774,465-ARR,,465-ARR_v1_25@0,,"In order to generate high-level semantic representations of abstracts, we introduce a dilated convolutional neural network (DCNN) to the abstract channel.","Delete,Fact/Evidence",Fact/Evidence
4806,468-ARR,,468-ARR_v1_56@5,,We think that the adapted debiased focal loss technique for regression might function in scenarios where it is possible to better isolate the bias and thus train a representative bias model.,"Delete,Claim",Claim
4959,473-ARR,,473-ARR_v1_72@3,,"The training set contains 6,574 words and 67,861 entries.","Delete,Fact/Evidence",Fact/Evidence
4960,473-ARR,,473-ARR_v1_72@4,,Statistics are listed in Table 1.,"Delete,Fact/Evidence",Fact/Evidence
4961,473-ARR,,473-ARR_v1_81@2,,"Specifically, we use the BLEU (Papineni et al., 2002) and Semantic Similarity metrics to evaluate the accuracy, and use the SARI (Xu et al., 2016), and HSK Level metrics to evaluate the simplicity.","Delete,Fact/Evidence",Fact/Evidence
5489,61-ARR,,61-ARR_v1_24@1,,Table A6 compares the slot and relative slot accuracies.,"Delete,Fact/Evidence",Fact/Evidence
5490,61-ARR,,61-ARR_v1_24@2,,The relative slot accuracy from turns 0 -3 is measured as 0 because it calculates the score based on the unique state of the current turn according to Equation 3.,"Delete,Fact/Evidence",Fact/Evidence
5491,61-ARR,,61-ARR_v1_24@3,,"In addition, regarding slot accuracy in turns 4, 5, and 6, there is no score improvement for the additional wellpredicted state by the model, whereas the score increases when the newly added state is matched in the case of relative slot accuracy.","Delete,Fact/Evidence",Fact/Evidence
5492,61-ARR,,61-ARR_v1_24@4,,"Therefore, relative slot accuracy can provide an intuitive evaluation reflecting the current belief state recording method, in which the number of slots accumulates incrementally as the conversation progresses.","Delete,Claim",Claim
5607,68-ARR,,68-ARR_v1_24@1,,"In our model, we use a bag-of-words decoder as the generator.","Delete,Fact/Evidence",Fact/Evidence
5608,68-ARR,,68-ARR_v1_70@0,,The results of 12 languages in XQuAD and M-LQA are shown in Table 3.,"Delete,Fact/Evidence",Fact/Evidence
5609,68-ARR,,68-ARR_v1_5@0,,"For the inaccuracy of boundary detection, existing methods mainly solve this issue by introducing external knowledge.","Delete,Claim",Claim
5610,68-ARR,,68-ARR_v1_76@0,,5.2 Why Use a Siamese Network in S 2 DM?,"Delete,Other",Other
5611,68-ARR,,68-ARR_v1_77@0,,"In order to separate semantic information from PLMs, an alternative way is to train a single net- Corresponding to S 2 DM, there are also two single-network variants: S 2 DM_single_POS and S 2 DM_single_SP.","Delete,Claim",Claim
5613,68-ARR,,68-ARR_v1_9@0,,"Besides, the multilingual pre-trained models have learned an amount of linguistic structure of multiple languages (Chi et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
5617,68-ARR,,68-ARR_v1_9@1,,The semantic can not directly disentangle from complex syntactic information in PLMs.,"Delete,Claim",Claim
5620,68-ARR,,68-ARR_v1_14@1,,Various approaches have been proposed on top of multilingual MRC based on PLMs.,"Delete,Claim",Claim
5621,68-ARR,,68-ARR_v1_14@2,,Cui et al. (2019) propose a method that combines multilingual BERT and back-translation for cross-lingual MRC.,"Delete,Fact/Evidence",Fact/Evidence
5622,68-ARR,,68-ARR_v1_2@4,,"Moreover, we theoretically analyze the generalization of our decoupling model, even for low-resource languages without training corpus.","Delete,Fact/Evidence",Fact/Evidence
5683,80-ARR,,80-ARR_v1_53@10,,We will release PopBuTFy and corresponding annotations once the paper is published.,"Delete,Fact/Evidence",Fact/Evidence
5684,80-ARR,,80-ARR_v1_67@2,,"After that, they got fused by being added up, then processed by tanh and sigmoid separately and then multiplied together.","Delete,Fact/Evidence",Fact/Evidence
5685,80-ARR,,80-ARR_v1_67@3,,"Finally, they produce a residual output for the next sub-layer and a skip-out.","Delete,Fact/Evidence",Fact/Evidence
5686,80-ARR,,80-ARR_v1_67@4,,"Lastly, two layers of 1D convolution and a ReLU process the summed skip-out to produce output.","Delete,Fact/Evidence",Fact/Evidence
5687,80-ARR,,80-ARR_v1_68@0,,"As shown in Figure 8, the content encoder is the combination of several conformer encoder layers in pink rectangle along with a 3-layer prenet.","Delete,Fact/Evidence",Fact/Evidence
5688,80-ARR,,80-ARR_v1_68@1,,The kernel size of the convolutional layer for prenet is 5.,"Delete,Fact/Evidence",Fact/Evidence
5689,80-ARR,,80-ARR_v1_71@0,,"Since in this paper we present a new dataset, we answer the questions in the ""Ethics"" part of the ""Author Checklist"":","Delete,Fact/Evidence",Fact/Evidence
5690,80-ARR,,80-ARR_v1_73@0,,"Yes, we sign an agreement with the participants that we only allow these singing recordings for non-commercial use.","Delete,Fact/Evidence",Fact/Evidence
5691,80-ARR,,80-ARR_v1_74@0,,D.2 Does the paper describe how participants' privacy rights were respected in the data collection process?,"Delete,Other",Other
5692,80-ARR,,80-ARR_v1_75@0,,"The participants are asked to use bogus names in the data collection process, which means we do not know who sang these recordings.","Delete,Fact/Evidence",Fact/Evidence
5693,80-ARR,,80-ARR_v1_76@0,,E Does the paper describe how crowd workers or other annotators were fairly compensated and how the compensation was determined to be fair?,"Delete,Other",Other
5694,80-ARR,,80-ARR_v1_77@0,,"Yes. We pay each worker $320 per hour when recording these songs, which is determined after the market research by the data annotation company.","Delete,Fact/Evidence",Fact/Evidence
5695,80-ARR,,80-ARR_v1_78@0,,F Does the paper indicate that the data collection process was subjected to any necessary review by an appropriate review board?,"Delete,Other",Other
5696,80-ARR,,80-ARR_v1_79@1,,The data collection process was subjected to review by the review board of the data annotation company.,"Delete,Fact/Evidence",Fact/Evidence
5706,80-ARR,,80-ARR_v1_15@1,,"Luo et al. (2018) propose Canonical Time Warping (CTW) (Zhou and Torre, 2009;Zhou and De la Torre, 2012) which aligns amateur singing recordings to professional ones according to the pitch curves only.","Delete,Fact/Evidence",Fact/Evidence
5707,80-ARR,,80-ARR_v1_15@2,,Wager et al. (2020) propose a datadriven approach to predict pitch shifts depending on both amateur recording and its accompaniment.,"Delete,Fact/Evidence",Fact/Evidence
5803,9-ARR,,9-ARR_v1_26@2,,2 The interval between two adjacent checkpoints is 50 iterations.,"Delete,Fact/Evidence",Fact/Evidence
5804,9-ARR,,9-ARR_v1_4@2,,"In addition, crosslingual pretrained language models like XLM (Conneau and Lample, 2019) and Unicoder (Huang et al., 2019) can even generate cross-lingually transferable language representations to universally handle downstream tasks in different languages.","Delete,Fact/Evidence",Fact/Evidence
5824,9-ARR,,9-ARR_v1_8@0,,"In this section, we introduce our proposed Noisy-Tune approach that adds noise to perturb PLMs for more effective finetuning.","Delete,Fact/Evidence",Fact/Evidence
5825,9-ARR,,9-ARR_v1_10@1,,"In many noisy training methods, the noise added to the parameters obeys the same distribution (LeCun et al., 2015).","Delete,Fact/Evidence",Fact/Evidence
5826,9-ARR,,9-ARR_v1_16@4,,The XTREME results are evaluated on the test set.,"Delete,Fact/Evidence",Fact/Evidence
5827,9-ARR,,9-ARR_v1_16@5,,The hyperparameter ฮป is 0.15 on GLUE and is 0.1 on XTREME.,"Delete,Fact/Evidence",Fact/Evidence
5904,90-ARR,,90-ARR_v1_30@2,,"Additionally, the structure of our specific task allows us to sample from the true posterior q(c o |ฯ, c h ), by using the rejection sampling procedure described above, rather than using approximations as in prior work (Eysenbach et al., 2020).","Delete,Fact/Evidence",Fact/Evidence
5905,90-ARR,,90-ARR_v1_67@0,,Ethical Statement,"Delete,Other",Other
5906,90-ARR,,90-ARR_v1_68@0,,We note that CALM is imperfect and in generating free-form dialogue can potentially produce harmful outputs.,"Delete,Claim",Claim
5907,90-ARR,,90-ARR_v1_68@1,,We therefore do not recommend applying this method in particularly sensitive or sufficiently wide-reaching domains without additional measures to mitigate harmful generations.,"Delete,Claim",Claim
5908,90-ARR,,90-ARR_v1_69@0,,"In this appendix, we provide all the details in our implementation for CALM.","Delete,Fact/Evidence",Fact/Evidence
6178,2-131,,2-131_v1_25@2,,Males had a mean of 76.3 with a range of 61 to 90 years.,"Delete,Fact/Evidence",Fact/Evidence
6179,2-131,,2-131_v1_25@3,,Females had a mean of 76.83 with a range of 53 to 90 years.,"Delete,Fact/Evidence",Fact/Evidence
6188,2-131,,2-131_v1_7@4,,Spectral Domain OCT (SD-OCT) is a newer technology that obtains high-resolution scans of the retina.,"Delete,Fact/Evidence",Fact/Evidence
6189,2-131,,2-131_v1_44@0,,"Overall Spectralisโข had the lowest COR, with values ranging from 5โ30 ยตm.","Delete,Fact/Evidence",Fact/Evidence
6190,2-131,,2-131_v1_9@1,,"To date, no other study has examined the effects of manual correction of the thickness algorithm in SD-OCT and TD-OCT machines in eyes with AMD.","Delete,Claim",Claim
6191,2-131,,2-131_v1_56@4,,"At this point, we are not aware of any previous study looking at the repeatability of Spectralisโข images in AMD.","Delete,Claim",Claim
6192,2-131,,2-131_v1_59@2,,"However, very few images needed to be corrected.","Delete,Fact/Evidence",Fact/Evidence
6288,2-180,,2-180_v1_39@7,,"Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.","Delete,Claim",Claim
6289,2-180,,2-180_v1_41@3,,"Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .","Delete,Fact/Evidence",Fact/Evidence
6407,2-278,,2-278_v1_5@3,,"Although there is one data point (a couple is due to be parents of twin boys, and the twins are fraternal), Efron does not use it to update prior knowledge.","Delete,Fact/Evidence",Fact/Evidence
6408,2-278,,2-278_v1_5@7,,"Without new data, our knowledge is by definition determined by prior information; thus, showing that the outcome of a Bayesian analysis with no new data is heavily influenced by the prior would not argue against Bayesian methods.","Delete,Claim",Claim
6409,2-278,,2-278_v1_5@8,,"Indeed, without data, Efronโs example is not Bayesian statistics and his conclusion about Bayesian statistics based on this example is unjustified.","Delete,Claim",Claim
6410,2-278,,2-278_v1_6@0,,We also have other more technical issues with Efronโs example.,"Delete,Claim",Claim
6411,2-278,,2-278_v1_6@1,,Efron interprets the term P(A) on the right side of the equation (see sidebar in Efron 2013a <REF-1> ) as the prior on the probability that twins are identical.,"Delete,Fact/Evidence",Fact/Evidence
6412,2-278,,2-278_v1_6@2,,"To make this prior uninformative, it is assigned a value of P(A) = 0.5 (see Efron 2013b <REF-2> ; although this is not stated in Efron 2013a <REF-1> ).","Delete,Fact/Evidence",Fact/Evidence
6413,2-278,,2-278_v1_6@3,,This uninformative prior is set in contrast to the informative โdoctorโs priorโ of P(A) = 1/3.,"Delete,Fact/Evidence",Fact/Evidence
6414,2-278,,2-278_v1_6@4,,"First, however, the parameter of interest is P(A|B) rather than P(A) according to Efronโs study question (see sidebar in Efron 2013a <REF-1> ), thus the focus should be on the appropriate prior for P(A|B).","Delete,Claim",Claim
6415,2-278,,2-278_v1_6@5,,"Second, for the uninformative prior, Efron mentions erroneously that he used a uniform distribution between zero and one, which is clearly different from the value of 0.5 that was used.","Delete,Fact/Evidence",Fact/Evidence
6416,2-278,,2-278_v1_6@6,,"Third, we find it at least debatable whether a prior can be called an uninformative prior if it has a fixed value of 0.5 given without any measurement of uncertainty.","Delete,Claim",Claim
6417,2-278,,2-278_v1_6@7,,"For example, if we knew that our chance of winning the next million-dollar jackpot were 50:50, would we really call this uninformative?","Delete,Claim",Claim
6418,2-278,,2-278_v1_8@0,,"We would very much like to check our calculations using frequentist methods; however, this is impossible because there is only one data point, and frequentist methods generally cannot handle such situations.","Delete,Claim",Claim
6432,2-282,,2-282_v1_7@2,,Patients and the experimental procedure were as detailed in the next sections.,"Delete,Fact/Evidence",Fact/Evidence
7532,5-1356,,5-1356_v1_33@0,,Detecting DS and sQTLs with the DM model,"Delete,Other",Other
7533,5-1356,,5-1356_v1_7@0,,Expressed transcripts are generated by alternatively including exons into mature mRNAs.,"Delete,Fact/Evidence",Fact/Evidence
7534,5-1356,,5-1356_v1_8@2,,"From an inference point of view, sQTL analyses are equivalent to DS analyses where covariates are defined by genotypes.","Delete,Claim",Claim
7535,5-1356,,5-1356_v1_98@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).","Delete,Fact/Evidence",Fact/Evidence
8583,7-1299,,7-1299_v1_16@0,,Further support for this comes from the examination of plant NMD pathways.,"Delete,Claim",Claim
8584,7-1299,,7-1299_v1_25@1,,PNRC2 is a vertebrate-specific NMD factor <REF-33> .,"Delete,Fact/Evidence",Fact/Evidence
8585,7-1299,,7-1299_v1_25@3,,More forward screens and biochemical studies are likely to yield more species-specific factors.,"Delete,Claim",Claim
8586,7-1299,,7-1299_v1_29@0,,"In bakerโs yeast, a downstream sequence element (DSE) was identified <REF-85> , <REF-86> .","Delete,Fact/Evidence",Fact/Evidence
8587,7-1299,,7-1299_v1_29@1,,"When this sequence is downstream of a stop codon, NMD is elicited, likely through the recruitment of an RNA binding protein <REF-85> , <REF-86> .","Delete,Fact/Evidence",Fact/Evidence
8588,7-1299,,7-1299_v1_29@2,,This mechanism is very similar to the way in which the EJC mode works in animals and plants.,"Delete,Claim",Claim
8589,7-1299,,7-1299_v1_29@3,,"The existence of DSEs in other species are possible, but to date, none have been identified.","Delete,Claim",Claim
8590,7-1299,,7-1299_v1_29@5,,Whether such a mechanism exists in other eukaryotes and what factors determine this remain to be seen.,"Delete,Claim",Claim
8591,7-1299,,7-1299_v1_35@2,,- 3) What precisely are the roles of SMG5-7 family members in lineages that have lost SMG1? Do their roles differ between species with type 2 (recent loss of SMG1) and type 3 (ancient loss of SMG1) NMD pathways?,"Delete,Claim",Claim
8662,7-1306,,7-1306_v1_22@7,,The percent expression metric shows whether a reasonable fraction of cells expresses the gene.,"Delete,Fact/Evidence",Fact/Evidence
8676,7-1306,,7-1306_v1_9@1,,Each row of the matrix should represent a gene and each column represents a cell.,"Delete,Fact/Evidence",Fact/Evidence
8744,7-1891,,7-1891_v1_49@0,,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).","Delete,Fact/Evidence",Fact/Evidence
10020,8-1983,,8-1983_v1_2@1,,The objective of this protocol is to create a simple and rapid approach to generate neurospheres from the dorsal lateral ganglionic eminence of late embryonic (day 17) mice.,"Delete,Claim",Claim
10025,8-1983,,8-1983_v1_8@8,,Our method also provides an approximation of the expected sizes for neurospheres after a week in vitro .,"Delete,Fact/Evidence",Fact/Evidence
10246,8-52,,8-52_v1_23@2,,"This finding clearly indicated that both distal promoters are highly conserved, even they originated from distantly related regions.","Delete,Claim",Claim
10531,8-80,,8-80_v1_101@1,,"Next steps include a more detailed look at the economic levers of tobacco preference, the perceived harms of cigarette smoking vs the objective realities of these harms, and a look at the characteristics and choices of ex-smokers.","Delete,Claim",Claim
11364,9-47,,9-47_v1_25@3,,The C7 and C8 clusters also have a relatively high frequency.,"Delete,Claim",Claim
