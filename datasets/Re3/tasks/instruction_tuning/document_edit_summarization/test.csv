doc_name,text_src,text_tgt,gold,sec_title_src,sec_title_tgt
10-ARR,"With this in mind, it is natural to consider how the advancement in natural language processing can be leveraged to help counseling.",,"Add,Claim",Introduction,
10-ARR,"Reflective listening asks the counselor not only to listen to the client carefully, but also to actively make a guess of what the client means.",,"Add,Claim",Introduction,
10-ARR,"However, people do not always say what they mean, which is especially the case for patients seeking mental support.",,"Add,Claim",Introduction,
10-ARR,"Reflection, as the response made based on reflective listening, sometimes needs to decode the client's meaning not explicitly expressed in words.",,"Add,Claim",Introduction,
10-ARR,"On the other hand, pressing the client to clarify the missing part may hinder them from expressing their own experience (Miller and Rollnick, 2012).",,"Add,Fact/Evidence",Introduction,
10-ARR,All these cases pose challenges to state-of-the-art language models.,,"Add,Claim",Introduction,
10-ARR,Previous research has addressed the task of automating response generation in health care and counseling settings.,,"Add,Fact/Evidence",Related Work,
10-ARR,Greer et al. (2019) used a decision tree to deliver pre-written scripts and guide the user to learn a set of positive emotion skills.,,"Add,Fact/Evidence",Related Work,
10-ARR,V et al. (2019) identified medical entities and the client's intent to fetch an answer for cancer related questions.,,"Add,Fact/Evidence",Related Work,
10-ARR,Almusharraf et al. (2020) classified client's responses to choose which question to ask next for smoking cessation.,,"Add,Fact/Evidence",Related Work,
10-ARR,"There are also commercial systems like Woebot (Fitzpatrick et al., 2017) that detect mental health issues mentioned by the user and direct them to relevant information.",,"Add,Fact/Evidence",Related Work,
10-ARR,"However, there is a limited amount of work on free-form generation as compared to the template-based approaches described above.",,"Add,Claim",Related Work,
10-ARR,Shen et al. (2020) focused on generating counseling reflections with GPT-2 based on the dialogue context and responses retrieved from similar counseling sessions.,,"Add,Fact/Evidence",Related Work,
10-ARR,"To the best of our knowledge, the effect of knowledge in counseling response generation is not yet well studied.",,"Add,Claim",Related Work,
10-ARR,We use the original implementation 6 and the pretrained weights on ConceptNet.,,"Add,Fact/Evidence",Generated Knowledge Setup,
10-ARR,,"For instance, Mao et al. (2019) generates story with multitasking learning on commonsense QA datasets.","Delete,Fact/Evidence",,Related Work
10-ARR,,Zhao et al. (2020) used BERT as a knowledge selection module for dialogue generation.,"Delete,Fact/Evidence",,Related Work
10-ARR,,Chakrabarty et al. (2020) ranked knowledge generated from the COMET for sarcasm generation.,"Delete,Fact/Evidence",,Related Work
10-ARR,,Ji et al. (2020) do multi-hop with a graph convolutional network on ConceptNet.,"Delete,Fact/Evidence",,Related Work
10-ARR,Each entity types identified during the extraction has a set of eleven distinct query templates as shown in Table 1.,Each entity types identified during the extraction has a set of distinct query templates as shown in Table 1.,"Modify,Fact/Evidence",Domain Knowledge Collection,Domain Knowledge Collection
10-ARR,4 The resulting sentences with medical concepts are then considered as knowledge candidates during our next step.,4 The resulting sentences are then considered as knowledge candidates during our next step.,"Modify,Fact/Evidence",Domain Knowledge Collection,Domain Knowledge Collection
10-ARR,"The positive samples used for this classifier consist of 1,331 sentences with cause-effect relationships (e.g., He had chest pains and headaches from mold in the bedrooms) from the SemEval10 Task 8 dataset (Hendrickx et al., 2010) and an equal amount of negative samples randomly selected from sentences containing other types of semantic relationships in the same dataset.","The positive samples used for this classifier consist of 1,331 cause-effect relationships (e.g., He had chest pains and headaches from mold in the bedrooms) from the SemEval10 Task 8 dataset (Hendrickx et 2010) and an equal amount of negative samples randomly selected from sentences containing other types of semantic relationships in the same dataset.","Modify,Clarity",Domain Knowledge Collection,Domain Knowledge Collection
10-ARR,"Across different counseling styles, reflective listening has always been a fundamental procedure underlying effective counseling practices (Katz and McNulty, 1994).","Effective counseling practice calls for reflective listening as an essential skill (Katz and McNulty, 1994).","Modify,Fact/Evidence",Introduction,Introduction
10-ARR,"If carried out the right way, it gives the client a sense of being understood and facilitates further self-exploration.","It requires the counselor to perceive the other's need or problem, and respond in a way letting the other know he is being understood.","Modify,Claim",Introduction,Introduction
10-ARR,"Following the categorization in (Hwang et al., 2021), we limit the relationships to the commonsense subset to reduce noise and to limit the number of generated knowledge triplets.","Following the categorization in (Hwang et al., 2020), we limit the relationships to the commonsense subset to reduce noise and to limit the number of generated knowledge triplets.","Modify,Fact/Evidence",Generated Knowledge Setup,Generated Knowledge Setup
10-ARR,"Thus, counseling frequently calls for counselors to make inferences based on their prior knowledge.","This process frequently involves making inferences based on the counselor's prior knowledge (Miller and Rollnick, 2012).","Modify,Clarity",Introduction,Introduction
10-ARR,"For example, when the client says I had a really hard time sticking to my diet this week, a plausible reflection may be You're wondering whether you'll be able to lose weight this way, which relates diet with losing weight as an inference based on commonsense knowledge.","For example, the client says I had a really hard time sticking to my diet this week, a plausible reflection may be You're wondering whether you'll be able to lose weight this way, which relates diet with losing weight as an inference.","Modify,Claim",Introduction,Introduction
10-ARR,"For example, to understand the client in Figure 1, the counselor needs to know that smoking can be a possible cause of emphysema, and Chantix is a medication for smoke cessation.","For example, to understand the client in Figure 1, the counselor needs to know that smoking can be a possible cause of emphysema, and Chantix is a medication for quit smoking.","Modify,Clarity",Introduction,Introduction
10-ARR,"The inplace method, which inserts the relation r and the generated e 2 next to e 1 , shows a significant improvement over the baseline.","The inplace method, which inserts the relation r and the generated e 2 next to e1, shows a significant improvement over the baseline.","Modify,Grammar",Result on K-BART Model Architecture,Result on K-BART Model Architecture
10-ARR,"This extra knowledge is needed since existing pre-trained language models struggle to produce coherent and informative responses that capture relevant knowledge, even if they have acquired some knowledge during the pre-training phase (Petroni et al., 2019a).","This is a challenging task since existing pre-trained language models struggle to produce coherent and informative responses that capture relevant knowledge, even if they have acquired some knowledge during the pretraining (Petroni et al., 2019a) phase.","Modify,Claim",Introduction,Introduction
10-ARR,"We conduct a human evaluation where we ask annotators to indicate their preferences between our best performing models from both the retrieval and the generative settings, and a model without knowledge enhancement.","We conduct a human evaluation where we ask annotators to indicate their preferences between our best performing models from both the retrieval and the generative settings ,and a model without knowledge enhancement.","Modify,Grammar",Human Evaluation,Human Evaluation
10-ARR,7 The annotators had no information on which model generated the the response being annotated.,7 The annotators have no information on which model generates the the response being annotated.,"Modify,Grammar",Human Evaluation,Human Evaluation
10-ARR,A system that generates accurate counseling reflections can serve as a tool to aid counseling training or assist counselors during a session by providing alternative reflections in response to client's statements.,A system that generates good counseling reflections can serve as a tool to aid counseling training or assist counselors during a session by providing candidate responses.,"Modify,Clarity",Introduction,Introduction
10-ARR,"Through an ablation study, we found that commonsense related to intentional and causal relationships is essential for the counseling domain.","Through an ablation study, we found that commonsense related to intentional and causal relationships are essential for the counseling domain.","Modify,Grammar",Conclusion,Conclusion
10-ARR,"The first is retrieval, which acquires sentences containing relevant knowledge based on the vector representations of sentences from the dialogue and assertions in the knowledge base using a BERT-based model (Reimers and Gurevych, 2019a).","The first is retrieval, which acquires sentences containing relevant knowledge using a BERT-based model (Reimers and Gurevych, 2019a) to get vector representations of sentences from the dialogue and assertions in the knowledge base.","Modify,Clarity",Introduction,Introduction
10-ARR,"The second strategy is generative, where we first extract key phrases from the dialogue, and query a COMET model for plausible knowledge triplets with a predefined set of relations (Bosselut et al., 2019).","The second strategy is generative, where we first extract key phrases from the dialogue, and query a COMET model for plausible knowledge triplets with a defined set of relations (Bosselut et al., 2019).","Modify,Clarity",Introduction,Introduction
10-ARR,We address a similar task but enhance the generation process by infusing commonsense and domain specific knowledge to better emulate what counselors do in practice.,"Similarly, our work uses external knowledge sources, but with several different settings to enhance text generation for counseling conversations.","Modify,Claim",Related Work,Related Work
10-ARR,External knowledge resources have been found useful for enhancing language models.,"There are various types of knowledge resources that can be used to enhance language models, focusing on different aspects.","Modify,Claim",Related Work,Related Work
10-ARR,"For example, large-scale commonsense knowledge graphs (CSKG) that store structured commonsense knowledge in the form of knowledge triplets.","For example, large-scale commonsense knowledge graphs (CSKG) store structured commonsense knowledge in the form of knowledge triplets.","Modify,Grammar",Related Work,Related Work
10-ARR,"In the following section, we describe the method to obtain relevant knowledge k c and the approach we use to incorporate knowledge into the language model.","In the following section, we describe the method to obtain relevant knowledge k c and the approach we use to incorporate knowledge in the language model.","Modify,Grammar",Task definition,Task definition
10-ARR,"Despite their large size, existing commonsense knowledge bases contain a limited amount of information on domain-specific concepts, especially for causal relationships such as the reason to take a medicine or its side effects.","Despite their large size, existing commonsense knowledge bases contain a limited amount of information on some domain-specific concepts, especially causal relationships such as the reason to take a medicine or its side effects.","Modify,Clarity",Domain Knowledge Collection,Domain Knowledge Collection
110-ARR,,PLMs lack knowledge of antonyms.,"Delete,Claim",,Results for MWR
110-ARR,,Training details.,"Delete,Other",,Meaning-Matching Task
110-ARR,,Catastrophic forgetting.,"Delete,Other",,SAR Results
110-ARR,"However, the representation hardly captures their semantic antonomy, e.g., gender.",,"Add,Claim",Issue of PLMs,
110-ARR,We multiply 100 to each value for a better readability.,,"Add,Fact/Evidence",Meaning-Matching Task,
110-ARR,Note that the lower the values the better.,,"Add,Fact/Evidence",Meaning-Matching Task,
110-ARR,We train the models for 10 epochs for each dataset and apply the early stopping technique where the patience number is set to 3.,,"Add,Fact/Evidence",Fine-Tuning on the GLUE Benchmark,
110-ARR,It is observed that the training is generally finished within 8 epochs for all the models.,,"Add,Fact/Evidence",Fine-Tuning on the GLUE Benchmark,
110-ARR,The batch size per GPU and learning rates used for each dataset are described in Table 8.,,"Add,Fact/Evidence",Fine-Tuning on the GLUE Benchmark,
110-ARR,"Datasets with large training set (e.g., MNLI, QNLI, and QQP) were not sensitive to the hyperparameters.",,"Add,Fact/Evidence",Fine-Tuning on the GLUE Benchmark,
110-ARR,"To reflect the prediction confidence score to the evaluation metric, we additionally define the weighted top-k hit rate (WHR@k) that uses the confidence score as weights.","To reflect the confidence score to the evaluation metric, we additionally define the weighted top-k hit rate (WHR@k) that uses the confidence score as weights.","Modify,Clarity",Evaluation Metrics,Evaluation Metrics
110-ARR,It is worth to mention that lower metrics mean a better model performance in both cases as the metrics assess how likely the models make inaccurate answers that they must avoid.,It is worth to mention that lower metrics mean a better model performance in both cases.,"Modify,Fact/Evidence",Evaluation Metrics,Evaluation Metrics
110-ARR,"We added the ELECTRA-small/base/large models (Clark et al., 2020) for the SAR task, but it is not used for the MKR-NQ and MWR experiments, as the discriminator of the ELECTRA models are trained with the replaced token prediction (RTP) training objective and have no MLM classifier.","We added the Electra-small/base/large models (Clark et al., 2020) for the SAR task, which are trained with the replaced token prediction (RTP) training objective.","Modify,Fact/Evidence",PLMs Lack Information of Negation and Lexical Semantics,PLMs Lack Information of Negation and Lexical Semantics
110-ARR,"We use the AdamW optimiser (Loshchilov and Hutter, 2019) for training with a learning rate of 5e −6 and a batch size of 32.","We use the AdamW optimiser (Loshchilov and Hutter, 2017) for training with a learning rate of 5e −6 and a batch size of 32.","Modify,Fact/Evidence",PLMs Lack Information of Negation and Lexical Semantics,PLMs Lack Information of Negation and Lexical Semantics
110-ARR,"Contemporary large-size PLMs, such as BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020), and GPT-2 and -3 (Radford et al., 2019;Brown et al., 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al., 2018) and Super-GLUE (Wang et al., 2019b) benchmark datasets.","Contemporary large-size PLMs, such as BERT (Devlin et al., 2019), Electra (Clark et al., 2020), and GPT-2 and -3 (Radford et al., 2019;Brown et al., 2020), have shown excellent results in many downstream tasks, even performing better than humans in the GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019b) benchmark datasets.","Modify,Grammar",Introduction,Introduction
110-ARR,"However, the difference between the large and small encoderfixed models is insignificant, except for the ELEC-TRA models that exhibit only a marginal improvement.","However, the difference between the large and small encoder-fixed models is insignificant, except for the Electra models that exhibit only a marginal improvement.","Modify,Grammar",Results for SAR,Results for SAR
110-ARR,"The two phenomenons suggest that PLMs' outstanding performance is predicated on updating many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019;McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information.","The two phenomenons suggest that PLMs' outstanding performance is predicated on updating a great many parameters to learn syntactic associations presented in training data (Niven and Kao, 2019;McCoy et al., 2019), but their contextualised representations do not carry abundant lexical meaning information.","Modify,Clarity",Results for SAR,Results for SAR
110-ARR,"However, the problem is that the distributional hypothesis has limitations in reflecting a word's semantic meanings, because words having different or even opposite semantic meanings can appear in similar or the same contexts.","However, the problem is that the distributional hypothesis does not consistently hold in natural language.","Merge+Modify,Claim",Issue of PLMs,Issue of PLMs
110-ARR,"However, the problem is that the distributional hypothesis has limitations in reflecting a word's semantic meanings, because words having different or even opposite semantic meanings can appear in similar or the same contexts.",Words having different meanings can appear in similar or even the same contexts.,"Merge+Modify,Claim",Issue of PLMs,Issue of PLMs
110-ARR,"We can readily imagine sentences in which the two words appear in the same context, e.g., ""the little boy/girl cuddled the teddy bear closely"".","Despite their antonymy, we can readily imagine sentences in which the two words appear in the same context, e.g., ""The little boy/girl cuddled the teddy bear closely."".","Modify,Clarity",Issue of PLMs,Issue of PLMs
110-ARR,"As a result, a model can learn their common functional meanings, i.e., young human beings, and the vector representations would be very similar if they were trained based on the distributional hypothesis.","As a result, the meaning of the two words would become quite similar if they were trained based on the distributional hypothesis.","Modify,Claim",Issue of PLMs,Issue of PLMs
110-ARR,"As a result, models cannot effectively learn the semantic meaning of words and negation expressions, provided they leverage only the text forms.","As a result, models can not learn the true meaning of words and negation expressions, provided they leverage only the text forms.","Modify,Clarity",Issue of PLMs,Issue of PLMs
110-ARR,"Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2021;Gupta et al., 2021; Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Many studies have conducted various probing tasks and observed that PLMs exhibit faulty behaviours, such as insensitiveness to sentence ordering (Pham et al., 2020;Gupta et al., 2021;Sinha et al., 2021b), incomprehension on number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Modify,Fact/Evidence",Introduction,Introduction
110-ARR,"We conjecture that a leading cause is that the dataset contains many words with similar meanings, mostly derived from the same stem.","We conjecture a leading cause is that the dataset contains many words with similar meanings, mostly derived from the same stem.","Modify,Clarity",Meaning-Matching Task,Meaning-Matching Task
110-ARR,"After the intermediate training, all models are fine-tuned on the SAR task with the same hyperparameters described in Section 3.","For training, we use the same hyperparameters as described in Section 3.","Modify,Fact/Evidence",SAR Results,SAR Results
110-ARR,Our results show that the proposed approach assists PLMs to learn enhanced representations with more abundant lexical semantic information.,Our results show that the proposed approach assists PLMs to learn enhanced contextualised representations with more abundant lexical semantic information.,"Modify,Clarity",SAR Results,SAR Results
110-ARR,"We find that small PLMs, such as ELECTRA-small and ALBERT models, show no significant increase in performance or are negatively impacted.","We find that small PLMs, such as Electra-small and ALBERT models, show no significant increase in performance or are negatively impacted.","Modify,Grammar",SAR Results,SAR Results
110-ARR,"We observe that the parameters of the ELECTRA-small model, which is negatively impacted, are changed considerably compared to other PLMs having parameters more than 100M.","We observe that the parameters of the Electra-small model, which is negatively impacted, are changed considerably compared to other PLMs having parameters more than 100M.","Modify,Grammar",SAR Results,SAR Results
110-ARR,"To confirm whether the issue occurs, we compare the performance of BERT, RoBERTa, and ELECTRA-large on 7 GLUE benchmark datasets (Wang et al., 2018) with their IM 2 counterparts.","To confirm whether the issue occurs, we compare the performance of BERT, RoBERTa, and Electra-large on 7 GLUE benchmark datasets (Wang et al., 2018).","Modify,Fact/Evidence",Fine-Tuning on the GLUE Benchmark,Fine-Tuning on the GLUE Benchmark
110-ARR,"We find no significant difference in performance for tasks with large datasets, such as MNLI, QNLI, QQP, and SST2.","We find no significant difference in performance for tasks with large datasets, such as the MNLI, QNLI, QQP, and SST2.","Modify,Grammar",Fine-Tuning on the GLUE Benchmark,Fine-Tuning on the GLUE Benchmark
110-ARR,"On the contrary, tasks with small datasets, like MRPC and RTE, are slightly improved.","On the contrary, tasks with small datasets, like the MRPC and RTE, are slightly improved.","Modify,Grammar",Fine-Tuning on the GLUE Benchmark,Fine-Tuning on the GLUE Benchmark
110-ARR,The result suggests that meaning-matching is a safe intermediate task that ensures a positive transfer with target downstream tasks.,The result suggests that the meaningmatching is a safe intermediate task that ensures positive transfer with target downstream tasks.,"Modify,Grammar",Fine-Tuning on the GLUE Benchmark,Fine-Tuning on the GLUE Benchmark
110-ARR,"Finally, we conduct experiments on the NegNLI benchmark dataset (Hossain et al., 2020), where negation plays an important role for NLI tasks.","Finally, we conduct experiments on the NegNLI benchmark dataset (Hossain et al., 2020) where negation plays an important role for NLI tasks.","Modify,Grammar",Experiments on the NegNLI Dataset,Experiments on the NegNLI Dataset
110-ARR,"For both SNLI and MNLI, we observe that our approach outperforms BERTNOT in the NegNLI datasets, while yielding a comparable performance in the original development datasets.","For both SNLI and MNLI, we observe that our approach outperforms BERTNOT in NegNLI datasets, while yielding a comparable performance in the original development datasets.","Modify,Grammar",Experiments on the NegNLI Dataset,Experiments on the NegNLI Dataset
110-ARR,"Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2021;Gupta et al., 2021;Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Among the many findings of these probing tasks, PLMs have been found to be insensitive to the order of sentences when generating representations (Pham et al., 2020;Gupta et al., 2021;Sinha et al., 2021a), struggle to comprehend number-related representations (Wallace et al., 2019;Lin et al., 2020;Nogueira et al., 2021), and display a lack of semantic content understanding (Ravichander et al., 2020;Elazar et al., 2021).","Modify,Fact/Evidence",Related Work,Related Work
110-ARR,Ettinger (2020) check the ability of PLMs to understand the meaning of negation in given contexts.,Ettinger (2020) check the ability of PLMs to understand of the meaning of negation in given contexts.,"Modify,Grammar",Related Work,Related Work
110-ARR,"In their remedy, they augment the language modelling objective with an unlikelihood objective (Welleck et al., 2020) based on negated sentences from the training corpus.","In their remedy, they augment the language modelling objective with an unlikelihood objective (Welleck et al., 2019) based on negated sentences from the training corpus.","Modify,Fact/Evidence",Related Work,Related Work
110-ARR,"In this method, the dependency parse of the sentences, POS tags, and morphological information of each word are taken as input, and the negation of sentences is done using sets of dependency tree regular expression patterns, such as Semgrex (Chambers et al., 2007).","In this method, the dependency parse of the sentences, POS tags and morphological information of each word are taken as input and the negation of sentences is done using sets of dependency tree regular expression patterns, such as Semgrex (Chambers et al., 2007).","Modify,Grammar",Related Work,Related Work
110-ARR,"Previous studies (e.g., Kassner and Schütze (2020)) have mostly limited the scope of the logical negation property only to the negation expressions (e.g., ""no"" and ""not"").","Previous studies, e.g., Kassner and Schütze (2020), have mostly limited the scope of the logical negation property only to the negation expressions (e.g., ""no"" and ""not"").","Modify,Grammar",Related Work,Related Work
110-ARR,"However, the core spirit of this property is the opposite meaning, which is not only limited to the negation.","However, the core spirit of the property is opposite-meaning, which is not only limited to negation.","Modify,Grammar",Related Work,Related Work
110-ARR,Welleck et al. (2020) consider negating sentences using dependency tree regular expression patterns.,Welleck et al. (2019) consider negating sentences using dependency tree regular expression patterns.,"Modify,Fact/Evidence",Related Work,Related Work
110-ARR,"This widens the scope of negation, as it is not only limited to the negation expressions ""no"" and ""not"".","This widens the scope of negation, as it is not only limited to negation expressions ""no"" and ""not"".","Modify,Grammar",Related Work,Related Work
110-ARR,"However, their approach relies on other components, such as Semgrex, and dependency and POS parsers, which could impact the quality of the data, hence impact the models' performance.","However, their approach relies on other components, such as Semgrex, and dependency and POS parsers which could impact the quality of the data, hence impact the models' performance.","Modify,Grammar",Related Work,Related Work
110-ARR,"In this work, we consider other perturbation methods to generate the opposite-meaning sentences to investigate whether PLMs satisfy the logical negation property, and we propose a remedy, called intermediate-training on meaning-matching (IM 2 ), which hardly employs additional linguistic components.","In this work, we consider other perturbation methods to generate the opposite-meaning sentences to investigate whether PLMs satisfy the logical negation property, and we propose a remedy called intermediate-training on meaning-matching (IM 2 ) that hardly employs additional linguistic components.","Modify,Clarity",Related Work,Related Work
110-ARR,We hypothesise that the distributional hypothesis is an insufficient basis for understanding the semantic meaning of texts.,We hypothesise that the distributional hypothesis results in PLMs' lack of understanding of the true meaning of texts.,"Modify,Clarity",Summary and Outlook,Summary and Outlook
110-ARR,"Hosseini et al. (2021) recently employed data augmentation and unlikelihood training (Welleck et al., 2020) to prevent models from generating unwanted words, given the augmented negated data during masked language modelling (MLM).","Hosseini et al. (2021) recently employed data augmentation and unlikelihood training (Welleck et al., 2019) to prevent models from generating unwanted words, given the augmented negated data during masked language modelling (MLM).","Modify,Fact/Evidence",Introduction,Introduction
110-ARR,"Second, the data augmentation method is contingent on many additional linguistic compo-nents, which causes the dependency of a model's performance on certain modules and precludes applying the method to other languages where such resources are unavailable.","Second, the data augmentation method is contingent on many additional linguistic components, which causes the dependency of model's performance on certain modules and precludes applying the method to other languages where such resources are unavailable.","Modify,Grammar",Introduction,Introduction
110-ARR,"Next, we propose a remedy, called intermediate-training on meaning-matching (IM 2 ), which hardly employs additional linguistic components.","Next, we propose a remedy, called intermediatetraining on meaning-matching (IM 2 ), that hardly employs additional linguistic components.","Modify,Clarity",Introduction,Introduction
110-ARR,"Our main contributions are as follows: (i) We extend the investigation of the LNP from negation to lexical semantics (Section 2), (ii) we reveal that PLMs are prone to violate the LNP (Section 3), (iii) we propose a novel remedy, named IM 2 , which is decoupled from the distributional hypothesis but learns meaning-text correspondence instead (Section 4), (iv) through experiments, we ascertain that the proposed approach improves the understanding of negation and lexical semantic information (Sections 5.1 and 5.2), and (v) we verify that meaningmatching is a stable and safe intermediate task that produces a similar or better performance in multiple downstream tasks (Sections 5.3 and 5.4).","Our main contributions are as follows: (i) We extend the investigation of the LNP from negation to lexical semantics (Section 2), (ii) we reveal that PLMs are prone to violate the LNP (Section 3), (iii) we propose a novel remedy named IM 2 that is decoupled from the distributional hypothesis but learns meaning-text correspondence instead (Section 4), (iv) through experiments, we ascertain that the proposed approach improves the understanding of negation and lexical semantic information (Sections 5.1 and 5.2), and (v) we verify that meaningmatching is a stable and safe intermediate task that produces a similar or better performance in multiple downstream tasks (Sections 5.3 and 5.4).","Modify,Clarity",Introduction,Introduction
110-ARR,"To alleviate the issue, we propose a novel intermediate training task, named meaning-matching, designed to directly learn a meaning-text correspondence, instead of relying on the distributional hypothesis.","To alleviate the issue, we propose a novel intermediate training task, named meaningmatching, designed to directly learn a meaningtext correspondence, instead of relying on the distributional hypothesis.","Modify,Grammar",Abstract,Abstract
114-ARR,,"Given that trigger-dependent types often have indicative triggers, we build a mechanism called word saliency embeddings (WSEs) in the model for T trigger to capture such regularities.","Delete,Fact/Evidence",,Trigger Saliency Attribution
114-ARR,,"Specifically, we first quantify each word's saliency value 3 as 0 or 1 based on λ, i.e., the threshold we used previously for distinguishing event types, and then use a separate embedding vector to distinguish 0 and 1, similar to word embeddings.","Delete,Fact/Evidence",,Trigger Saliency Attribution
114-ARR,,Such embeddings are incorporated into the model 4 to capture a regularity that words with high saliency values are more likely to be triggers.,"Delete,Fact/Evidence",,Trigger Saliency Attribution
114-ARR,,"Note WSEs are also incorporated in the model for the T context , which on the other hand seeks to learn the opposite regularity that words with high saliency values may not be triggers.","Delete,Fact/Evidence",,Trigger Saliency Attribution
114-ARR,,"We use Adam (Kingma and Ba, 2015) with default hyper-parameters for parameter update.","Delete,Fact/Evidence",,Trigger Saliency Attribution
114-ARR,,Experimental Setups,"Delete,Other",,Saliency as Evidence: Event Detection with Trigger Saliency Attribution
114-ARR,,Datasets.,"Delete,Other",,Experimental Setups
114-ARR,,"We conduct experiments on ACE 2005 (LDC, 2005) and MAVEN documents.","Delete,Fact/Evidence",,Experimental Setups
114-ARR,,"We adopt a common split for evaluation following previous works (Li et al., 2013;Wadden et al., 2019).","Delete,Fact/Evidence",,Experimental Setups
114-ARR,,MAVEN is a newly released corpus defining 168 more fine-grained event types .,"Delete,Fact/Evidence",,Experimental Setups
114-ARR,,"Because the MAVEN test set is not publicly available and our study is concerned with per-type performance, we instead use the MAVEN development set for assessment and divide the original MAVEN training set as 9:1 for training and validating.","Delete,Fact/Evidence",,Experimental Setups
114-ARR,,Table 1 displays the comprehensive data statistics for the two datasets.,"Delete,Fact/Evidence",,Experimental Setups
114-ARR,,Evaluation Metrics.,"Delete,Other",,Experimental Setups
114-ARR,,"(SL), which only differentiates event types for training, outperforms BERTEns by 1.6% in F1.","Delete,Fact/Evidence",,Results of Saliency Enhanced ED
114-ARR,"For example, we may further subdivide a CD type TRANSFER_MONEY into finergrained ones like LOAN and PURCHASE.",,"Add,Claim",Discussion,
114-ARR,"We provide linguistic/lexical insights by comparing the hierarchy levels of TD/CD types on WordNet (Miller, 1992).",,"Add,Fact/Evidence",Discussion,
114-ARR,"Event detection (ED) is the first and a crucial step of event extraction, which aims to identify events of certain types in plain texts (Ahn, 2006;Nguyen and Grishman, 2015;Mitamura et al., 2017).","Event detection (ED), the first and a crucial step of event extraction, aims to identify events of certain types in texts (Ahn, 2006;Nguyen and Grishman, 2015;Mitamura et al., 2017).","Modify,Clarity",Introduction,Introduction
114-ARR,"Tasking the ACE benchmark as an example, we note the state-of-the-art ED model (Wadden et al., 2019) can strike 90% in F1 for the type DIVORCE, yet only 50% for the type START-POSITION, and it is more surprising that the training set of DIVORCE is eight times smaller than that of START-POSITION.","Tasking the ACE benchmark as an example, we note the state-of-the-art ED model (Wadden et al., 2019) can strike 90% in F1 for the type DIVORCE, yet only 50% for the type START-POSITION; it is more surprising that the training set of DIVORCE is 8 times smaller than that of START-POSITION.","Modify,Clarity",Introduction,Introduction
114-ARR,"After 5 epochs, it achieves 74.8% in F1 on the ACE 2005 development set, matching the state-of-the-art performance (Liu et al., 2019c).","After 5 epochs, it achieves 74.8% in F1 on the ACE 2005 development set, matching the state-of-the-art performance .","Modify,Fact/Evidence",Trigger Saliency Attribution,Experimental Setups
114-ARR,"To allow for further investigation, we have made our code publicly available at https://github.com/ jianliu-ml/SaliencyED.","To allow for further investigation, we have made our code publicly available at http://anomynous.","Modify,Fact/Evidence",Trigger Saliency Attribution,Experimental Setups
114-ARR,In this study we take a fresh look at above problem and for the first time attribute the skewed performance to the contextual patterns of events.,This study takes a fresh look at the problem by attributing the skewed performance to the contextual patterns of events.,"Modify,Clarity",Introduction,Introduction
114-ARR,"Intuitively, they demonstrate distinct patterns: the DI-VORCE event is more trigger-dependent, and the trigger word (i.e., ""divorced"") is very indicative of the event's occurrence; by contrast, the START-POSITION event is more context-dependent -the event semantic is primarily expressed by contexts rather than the trigger ""become"", which is a merely light verb.","Intuitively, they have distinct patterns: the DI-VORCE event is more trigger-dependent, because the trigger word (divorced) is very indicative of the event's occurrence; by contrast, the START-POSITION event is more context-dependent -the event semantic is primarily expressed by contexts rather than the trigger (become), which is a merely light verb.","Modify,Clarity",Introduction,Introduction
114-ARR,"To address the first question, we introduce a brandy new concept called trigger saliency attribution, which can explicitly quantify an event's contextual pattern.",We introduce a brandy new concept called trigger saliency attribution that can explicitly quantify an event's contextual pattern.,"Modify,Clarity",Introduction,Introduction
114-ARR,"Figure 2 illustrates the key idea: to determine how much an event is trigger-dependent or context-dependent, we measure the trigger's contribution to expressing overall the event semantic.","As shown in Figure 2, to determine how much an event depends on triggers/contexts, the key notion is to measure the trigger's contribution to expressing overall the event semantic.","Modify,Clarity",Introduction,Introduction
114-ARR,"Specifically, we first assign each sentence a global event label that represents the overall event semantic.","To this end, we first assign each sentence a global event label that represents the overall event semantic.","Modify,Clarity",Introduction,Introduction
114-ARR,"Then, inspired by the feature attribution method (Simonyan et al., 2014;Sundararajan et al., 2017), we regard each word as a feature and compute its contribution (i.e., saliency value) for predicting the global event label.","Then, inspired by the feature attribution method (Simonyan et al., 2014;Sundararajan et al., 2017), we regard each word as a feature and compute its saliency value (i.e., contribution) for predicting the global event label.","Modify,Clarity",Introduction,Introduction
114-ARR,"Finally, by examining the ground-truth trigger's saliency value, we can tell how much an event depends on triggers or contexts: a higher value, for example, indicates that the trigger contributes more to the event, implying the event is more trigger-dependent.","Finally, by examining the ground-truth trigger's saliency value, we can determine how much an event depends on triggers or contexts: a higher value, for example, indicates that the trigger contributes more to the event, implying the event is more trigger-dependent.","Modify,Clarity",Introduction,Introduction
114-ARR,"To answer the second question, we develop a new training mechanism based on trigger saliency attribution, which uses saliency as evidence to enhance learning.","We also develop a new training mechanism based on trigger saliency attribution, which uses saliency as evidence to enhance learning.","Modify,Clarity",Introduction,Introduction
114-ARR,"Our method is simple and straightforward -instead of using a single model to detect all event types, we group event types with similar patterns together (assessed by trigger saliency attribution) and develop separate models for each group.","Our method is simple yet effective -instead of using a single model to detect all event types, we group event types with similar patterns together (assessed by trigger saliency attribution) and develop separate models for each group.","Modify,Clarity",Introduction,Introduction
114-ARR,"This strategy enables different models to capture distinct patterns -for example, the model for context-dependent type can focus on mining contextual information for learning.",This strategy enables different models to capture distinct patterns.,Merge+Identical,Introduction,Introduction
114-ARR,"This strategy enables different models to capture distinct patterns -for example, the model for context-dependent type can focus on mining contextual information for learning.","The model for context-dependent types, for example, can focus on mining contextual information for learning.","Merge+Modify,Clarity",Introduction,Introduction
114-ARR,"To further boost learning, we also propose two saliency-exploration strategy to augment the above framework, which can explicitly integrate saliency information into learning and produce improved performance particularly for context-dependent types ( § 6.2).","Furthermore, we augment the above framework with two saliency-exploration strategy, which can explicitly integrate saliency information into learning and produce improved performance particularly for context-dependent types ( § 6.2).","Modify,Clarity",Introduction,Introduction
114-ARR,"To verify the effectiveness of our approach, we have conducted extensive experiments on two ED benchmarks (i.e., ACE 2005(LDC, 2005 and MAVEN ).","We have conducted extensive experiments on two ED benchmarks (i.e., ACE 2005(LDC, 2005 and MAVEN ) to verify the effectiveness of our approach.","Modify,Clarity",Introduction,Introduction
114-ARR,"According to the results: (i) Our trigger saliency attribution method can capture the underlying pattern and well explain the skewed performance, obtaining Spearman's correlation coefficients of 0.72 and 0.61 with per-type F1 on ACE 2005 and MAVEN respectively; (ii) Our new training regime based on saliency demonstrates improved results on the two benchmarks.","From the results: (i) Our trigger saliency attribution method does capture the underlying pattern and can well explain the skewed performance, obtaining Spearman's correlation coefficients of 0.72 and 0.61 with per-type F1 on ACE 2005 and MAVEN respectively; (ii) Our new training regime based on saliency demonstrates improved results on the two benchmarks.","Modify,Clarity",Introduction,Introduction
114-ARR,"Finally, in ablation studies, we compare and highlight many significant characteristics (e.g., linguistic and lexical patterns) of triggerdependent and context-dependent event types; our work may inspire future research into their patterns.","Finally, we compare and emphasize several significant aspects (e.g., linguistic and lexical patterns) of trigger-dependent and contextdependent event types, and our work may inspire future research into their differences.","Modify,Clarity",Introduction,Introduction
114-ARR,"• We highlight several diverse patterns of trigger-dependent and context-dependent event types, and our findings may stimulate future research into their differences.","• We highlight many distinct patterns of triggerdependent and context-dependent event types, and our findings suggest that the traditional ""one model fits all types"" paradigm may need to be revised.","Modify,Claim",Introduction,Introduction
114-ARR,"FA have been used to interpret model predictions in applications including image classification (Simonyan et al., 2014), machine translation (Ding et al., 2017), text classification , and others (Bastings and Filippova, 2020).","FA have been used to interpret model predictions in applications including image classification (Simonyan et al., 2014), machine translation (Ding et al., 2017), text classification (Chen et al., 2018), and others (Bastings and Filippova, 2020).","Modify,Fact/Evidence",Background and Related Work,Background and Related Work
14-ARR,"• Grammatical error (Gram): Erroneous usage of past/current tense and mistakes in misplaced modifiers. • Event mismatch (Event): Stories that are offtopic, which present events that are not relevant to the image stream. • Object mismatch (Obj): Irrelevant nouns that do not appear in the images and are not semantically related.",,"Add,Fact/Evidence",Data Analysis and Findings,
14-ARR,"• Stretch-VST (Hsu et al., 2021b): a modification of KGStory that produces more sentences in the story while maintaining quality.",,"Add,Fact/Evidence",Appendix,
14-ARR,Appropriate knowledge added to the story results in a more detailed story.,,"Add,Claim",Appendix,
14-ARR,The construction of VHED is shown in Figure 2.,The construction of VHED is shown in Fig. 2.,"Modify,Clarity",Dataset Description,Dataset Description
14-ARR,"SIMCSE uses contrastive learning with dropout as augmentation, then trained on natural langauge inference datasets to obtain better sentence embeddings from BERT (Devlin et al., 2019).","SIMCSE uses contrastive learning with dropout as augmentation, then trained on natural langauge inference datasets to obtain better sentence embeddings from BERT (Devlin et al., 2018).","Modify,Fact/Evidence",Vrank,Vrank
14-ARR,"We hypothesize that utilizing this feature makes it possible to extract more information, making it easier for the model to learn human judgment.","We hypothesize that thus doing makes it possible to extract more information, making it easier for the model to learn human judgment for story pairs.","Modify,Clarity",Vrank,Vrank
14-ARR,"However, due to the small amount of data available, high variance is likely (Mosbach et al., 2020) to occur during inference.","However, due to the small amount of data available, high variance is likely (Mosbach et al., 2021) to occur during inference.","Modify,Fact/Evidence",Vrank,Vrank
14-ARR,"Hence, we used all data from VHED, including human agreement=3 to increase the stability of our model following Mosbach et al. (2020).","Hence, we used all data from VHED, including human agreement=3 to increase the stability of our model following Mosbach et al. (2021).","Modify,Fact/Evidence",Vrank,Vrank
14-ARR,"Given the story pair (x 1 , x 2 ), the automatic metric being assessed predicts the corresponding story quality scores (s 1 , s 2 ) which we compare to the averaged ranks y 1 and y 2 of x 1 and x 1 from human evaluation.","Given the story pair (x 1 , x 2 ), the autometric being assessed predicts the corresponding story quality scores (s 1 , s 2 ) which we compare to the averaged ranks y 1 and y 2 of x 1 and x 1 from human evaluation.","Modify,Clarity",Experimental Settings,Experimental Settings
14-ARR,"We also implement the more recent BERT-Score, BLEURT, and UNION as baseline metrics.","We also considered the more recent BERT-Score, BLEURT, and UNION as baseline metrics.","Modify,Clarity",Experimental Settings,Experimental Settings
14-ARR,"In addition to the above automatic metrics, we also include a random baseline, denoted as Random in Table 4, to provide a random score for each story as the lower bound.","In addition to the above automatic metrics, we also included a random baseline to provide a random score for each story, shown as Random in Table 4, as the lower bound.","Modify,Clarity",Experimental Settings,Experimental Settings
14-ARR,Learning to Rank Visual Stories from Human Ranking Data,Learning to Rank Visual Stories From Human Ranking Data,"Modify,Grammar",,
14-ARR,"This algorithm only applies when evaluating story pairs containing references, i.e., reference-machine pairs in this paper.","The Reference Absent Algorithm only applies when evaluating story pairs containing references, i.e., reference-machine pairs in this paper.","Modify,Clarity",Experimental Settings,Experimental Settings
14-ARR,"We also find that Vrank ranks correctly when machine is better than reference, showing that Vrank yields 26.5% recall when the other metrics have 0 recall without Eq. 3 and ∼18% with Eq. 3.",Another analysis to study ability of Vrank to rank correctly when machine is better than reference shows that Vrank yields 26.5% recall when the other metrics have 0 recall without Eq. 3 and ∼18% with Eq. 3.,"Modify,Clarity",Results and Discussion,Results and Discussion
14-ARR,It is crucial for automatic metrics to also recognize errors to judge generated text.,It is crucial for automatic metrics to also recognize such errors to judge generated text.,"Modify,Clarity",Results and Discussion,Results and Discussion
14-ARR,"To determine whether Vrank generalizes to textual stories, we selected MANS dataset (Guan et al., 2021), an imagefree storytelling dataset in which the stories are derived from the ROCStories corpus .","To determine whether Vrank generalizes to textual stories, we selected as the benchmark the MANS dataset (Guan et al., 2021), an image-free storytelling dataset in which the stories are derived from the ROCStories corpus.","Modify,Clarity",Dataset Generalization,Dataset Generalization
14-ARR,"MANS includes 200 story prompts, where each prompt includes five model-generated stories and a reference.","This dataset includes 200 story prompts, where each prompt includes five model-generated stories and a reference.","Modify,Clarity",Dataset Generalization,Dataset Generalization
14-ARR,"After applying Vrank to assess five recent VIST models, we present the results in Figure 4: the models are gradually approaching human-level writing, outlining an exciting development of NLG in VIST.","After applying Vrank to assess five recent VIST models, we present the results in Fig. 4: the models are gradually approaching human-level writing, outlining an exciting development of NLG in VIST.","Modify,Clarity",Appendix,Appendix
14-ARR,We also show the correlation between different error types in Figure 5.,We also show the correlation between different error types in Fig. 5.,"Modify,Clarity",Appendix,Appendix
14-ARR,"Ranking Gap Distribution The ranking gap distribution is shown in Figure 6, in which both the ranking gaps and the number of stories are normalized.","Ranking Gap Distribution The ranking gap distribution is shown in Fig. 6, in which both the ranking gaps and the number of stories are normalized.","Modify,Clarity",Appendix,Appendix
14-ARR,"The batch size is set as 32 and the random seed for training can be set as 7,777 for reproduction.",The batch size is set as 32 and the random seed for training can be set as 7777 for reproduction.,"Modify,Grammar",Appendix,Appendix
14-ARR,We then re-purposed VHED to create a better metric Vrank for VIST to rank visual stories.,We then re-purposed VHED to create a better metric for VIST named Vrank (VIST Ranker).,"Modify,Fact/Evidence",Introduction,Introduction
14-ARR,"Indeed, 38% of machine-generated stories are better than the references, which suggests that the afore-mentioned assumption may need to be revisited .","Indeed, 38% of machine-generated stories are better than the references, which suggests that the afore-mentioned assumption may need to be revisited (Clark et al., 2021).","Modify,Fact/Evidence",Introduction,Introduction
14-ARR,"In conclusion, Vrank excels in the above assessments and able to follow human behaviors in ranking.","In conclusion, Vrank excels in the above assessments and able to follow human behaviors in ranking, rank machine and human stories decently and is better at detecting story errors.",Split+Identical,Introduction,Introduction
14-ARR,"Moreover, Vrank can rank machine and human stories decently and is better at detecting story errors.","In conclusion, Vrank excels in the above assessments and able to follow human behaviors in ranking, rank machine and human stories decently and is better at detecting story errors.","Split+Modify,Clarity",Introduction,Introduction
14-ARR,"Specifically, we make three major contributions:",The contributions of this paper are threefold:,"Modify,Claim",Introduction,Introduction
152-ARR,Our proposed methods increases corpus size by a slightly larger factor because sentences that contain rare entity types are resampled multiple times.,,"Add,Fact/Evidence",Effect on Training Corpus Size,
152-ARR,"Although increased training corpus size leads to increased training time, note that our methods are especially suitable for scenarios where the annotated corpus is small and hence the training time is still relatively short.",,"Add,Claim",Effect on Training Corpus Size,
152-ARR,"Each NER model (Shallow, Bi-LSTM, BERT) is associated with a cluster of vectors sharing the same starting point in the space, which represents the performance on the original corpus.",,"Add,Fact/Evidence",Effect on Training Corpus Size,
152-ARR,Note that data augmentation and sentence-level resampling (and resampling methods in general) are complementary methods for improving NER model training.,,"Add,Claim",Conclusion and Future Work,
152-ARR,"Data augmentation improves the semantic richness of training instances by expanding the coverage of training data in the input feature space, while sentence-level resampling refines the importance weighting of training instances by bridging the gap between the training objective and evaluation metrics.",,"Add,Claim",Conclusion and Future Work,
152-ARR,"Therefore, they work in orthogonal directions.",,"Add,Claim",Conclusion and Future Work,
152-ARR,This points to a promising direction for future work: to explore the two line of methods in combination rather than in competition.,,"Add,Claim",Conclusion and Future Work,
152-ARR,"By direct analogy, sentence importance score measures the utility of a sentence with respect to the entities it contains.","By direct analogy, a sentence importance score measures the utility of a sentence respect to the entity tokens it contains.","Modify,Grammar",Sentence Importance Factors in NER,Sentence Importance Factors in NER
152-ARR,"In natural language processing, named entity recognition (NER) is an important task both on its own and for numerous downstream tasks such as entity linking and question answering.","In natural language processing, named entity recognition (NER) is an important task both on its own and supports numerous downstream tasks such as entity linking and question answering.","Modify,Clarity",Introduction,Introduction
152-ARR,By introducing the rareness of an entity type we propose another function called the smoothed resampling incorporating count and rareness (sCR):,By introducing rareness of an entity type we propose another function called smoothed resampling incorporating count and rareness (sCR):,"Modify,Grammar",Resampling Functions,Resampling Functions
152-ARR,We use √ l s instead of l s to slow down the decrease of f sCRD s when a sentence is too long.,We use √ l s instead of l s because to slow down the decrease of f sCRD s when a sentence is too long.,"Modify,Grammar",Resampling Functions,Resampling Functions
152-ARR,"Here, c(t, s) applies a sublinear increasing function on c(t, s) to implement the diminishing marginal utility when a sentence contains many tokens with the same type.","Here, c(t, s) applies a sublinearly increasing function on c(t, s) to implement the diminishing marginal utility when a sentence contains many tokens with the same type.","Modify,Grammar",Resampling Functions,Resampling Functions
152-ARR,"Similar observation was made by previous work (Devlin et al., 2018).","Similar observations was made by previous work (Devlin et al., 2018).","Modify,Grammar",Results and Discussion,Results and Discussion
152-ARR,These benefits become less salient on large corpus (CoNLL).,These benefit becomes less salient on large corpus (CoNLL).,"Modify,Grammar",Results and Discussion,Results and Discussion
152-ARR,Conclusion and Future Work,Conclusion,"Modify,Other",Sentence-Level Resampling for Named Entity Recognition,Sentence-Level Resampling for Named Entity Recognition
152-ARR,Various other avenues exist for future work.,There are multiple avenues for future work.,"Modify,Clarity",Conclusion and Future Work,Conclusion
152-ARR,"First, further theoretical and empirical research can explore more effective resampling functions that deliver consistently better performance across corpora and base NER models.","First, further theoretical and empirical research can explore more effective resampling functions that deliver consistently better performance across corpora and base models.","Modify,Clarity",Conclusion and Future Work,Conclusion
152-ARR,"Second, more corpora and models can be examined under these resampling strategies to evaluate their generalizability.","Second, more corpora and models can be examined under these resampling strategies.","Modify,Claim",Conclusion and Future Work,Conclusion
152-ARR,Future research may seek for corpora-level statistics that can assist practitioners in selecting the appropriate resampling methods.,Future research may seek for corpora-level statistics that can assist practitioners in the process of selecting the appropriate resampling method(s).,"Modify,Clarity",Conclusion and Future Work,Conclusion
152-ARR,"Data augmentation was shown to be effective by enriching entity-bearing sentences through methods like segment shuffling and mention replacement (Dai and Adel, 2020;Issifu and Ganiz, 2021;Wang and Henao, 2021).","Data augmentation was shown to be effective by enriching entity-bearing sentences through methods like segment shuffling and mention replacement (Dai and Adel, 2020).","Modify,Fact/Evidence",Introduction,Introduction
152-ARR,"Researchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Garcia, 2009).","Researchers have proposed various techniques for imbalanced learning, including resampling and cost-sensitive learning (He and Ma, 2013).","Modify,Fact/Evidence",Learning from Imbalanced Data,Learning from Imbalanced Data
156-ARR,,"As the results shown in Table 4, the combination of TurnAPE and RoleAPE achieve the best performance.","Delete,Fact/Evidence",,Position Embeddings
156-ARR,,"Both absolute and relative position embeddings improve model performance, nevertheless, including them at the same time can be harmful.","Delete,Claim",,Position Embeddings
156-ARR,,"In this paper, different ethical restrictions deserve discussion.","Delete,Claim",,Conclusion
156-ARR,,All data used in our pre-training are available online and other dialog corpus in this paper are publicly available sources.,"Delete,Fact/Evidence",,Conclusion
156-ARR,,We strictly followed the platform's policies and rules when crawling data from web platforms.,"Delete,Fact/Evidence",,Conclusion
156-ARR,,We did not employ any author-specific information in our research.,"Delete,Fact/Evidence",,Conclusion
156-ARR,,"Our corpus may includes some bias, such as political bias and social bias, and our model might have inherited some forms of these bias.","Delete,Claim",,Conclusion
156-ARR,,"In order to limit these bias as much as possible, we filter controversial articles and removed data with offensive information when possible.","Delete,Fact/Evidence",,Conclusion
156-ARR,,"We demonstrate the responses generated from our model as well as other baseline models in Table 7, 8 and 9, respectively.","Delete,Fact/Evidence",,Conclusion
156-ARR,,The results in Table 8 and 9 show that our model accurately outputs the knowledge information contained in context although we do not model knowledge explicitly.,"Delete,Fact/Evidence",,Conclusion
156-ARR,The n-stream self-attention mechanism incorporates n extra self-attention predicting streams besides main stream to predict next n continuous future tokens respectively at each time step.,,"Add,Fact/Evidence",Decoder,
156-ARR,,"Our pre-trained models and source code will be released, hoping to facilitate further research progress in dialogue generation.","Delete,Claim",,Introduction
156-ARR,We obtain 215 million 1 training samples (42GB in total) for pre-training.,,"Add,Fact/Evidence",Pre-training Corpus,
156-ARR,"To accelerate the training process and accommodate GPU memory limitations, we adopt two methods.",,"Add,Fact/Evidence",Pre-training Corpus,
156-ARR,"First, we sort the samples according to the length of the context.",,"Add,Fact/Evidence",Pre-training Corpus,
156-ARR,Samples with similar length (i.e. number of tokens in context) are assembled into a batch to minimize the amount of padding.,,"Add,Fact/Evidence",Pre-training Corpus,
156-ARR,"Secondly, due to the uneven distribution of sample lengths, we divide the Reddit corpus into two sub-datasets: Reddit-Short and Reddit-Long according to the length of context and response.",,"Add,Fact/Evidence",Pre-training Corpus,
156-ARR,This paper proposes a new pre-training framework for dialogue response generation called Di-alogVED.,,"Add,Fact/Evidence",Conclusion,
156-ARR,"The latent variable is incorporated into the sequence-to-sequence framework based on Transformer, and obtains a robust and diverse response generation model through 4 training targets.",,"Add,Fact/Evidence",Conclusion,
156-ARR,Extensive experiments prove the effectiveness of our model.,,"Add,Claim",Conclusion,
156-ARR,Additional human evaluation demonstrates the advantages of our proposed model.,,"Add,Claim",Conclusion,
156-ARR,"As shown in Table 2 and Table 3, our model Di-alogVED is very competitive compared to PLATO and other models.","As shown in Table 2 and Table 6 (in Appendix A), our model DialogVED is very competitive compared to PLATO and other models.","Modify,Fact/Evidence",Main Results,Main Results
156-ARR,"These models introduce discourse-level diversity and are able to generate diverse dialog responses (Serban et al., 2017;Zhao et al., 2017a.","These models introduce discourse-level diversity and are able to generate diverse dialog responses (Serban et al., 2017;Zhao et al., 2017aZhao et al., , 2018.","Modify,Fact/Evidence",Related Work,Related Work Encoder-Decoder dialog models
156-ARR,"However, DialogVED equipped with beam search or greedy search, can still easily beat PLATO even though it has a post-generation ranking component.","However, DialogVED equipped with beam or search, can still easily beat PLATO even though it has a post-generation ranking component.","Modify,Clarity",Main Results,Conclusion
156-ARR,"The main contributions of this paper can be summarized as follows: 1) We propose a pretrained dialog model, which incorporates continuous latent variables into the enhanced encoder-decoder pre-training framework; 2) We explore the impact of latent variable sizes, different decoding strategies, and position embeddings for turns and roles in our model; 3) Extensive experiments show that the proposed model achieves the new state-of-theart (SOTA) in multiple downstream tasks, and our model has better performance both on relevance and diversity than previous SOTA in response generation.","The main contributions of this paper can be summarized as follows: 1) We propose a pretrained dialog model, which incorporates continuous latent variables into the enhanced encoder-decoder pre-training framework; We explore the impact of latent variable sizes, different decoding strategies, and position embeddings for turns and roles in our model; Extensive experiments show that the proposed model achieves the new state-of-the-art (SOTA) in multiple downstream tasks, and our model has better performance both on relevance and diversity than previous SOTA in response generation.","Modify,Grammar",Introduction,Introduction
157-ARR,,Figure 2 shows the relevance of different content (columns) for the various stakeholders (the rows) for a 10K filing document.,"Delete,Fact/Evidence",,Persona Mapping
157-ARR,,Groups of stakeholders are made that form the personas interested in the different parts of the document.,"Delete,Fact/Evidence",,Persona Mapping
157-ARR,,The different columns are also grouped together as to indicate what kind of information those sections will contain.,"Delete,Fact/Evidence",,Persona Mapping
157-ARR,,Table 5: Results from the human experiment on using the Default Reading experience with DYNAMICTOC.,"Delete,Other",,Metric-Based Evaluation
157-ARR,We can map these columns to the aspects we get from the Aspect Detection Module and determine if a particular persona is interested in that paragraph or not.,,"Add,Fact/Evidence",Persona Mapping,
157-ARR,"We consulted a domain expert (financial domain; specifically for SEC 10-K filings) to create a matrix of personas, who read such documents, and what kind of information they are interested in.",We consulted a domain expert (financial domain; specifically for SEC 10-K filings) to get an understanding of the parties or personas who read such documents and what kind of information are they generally interested in.,"Modify,Clarity",Persona Mapping,Persona Mapping
157-ARR,Figure 2 lists out the various stakeholders of a general 10-K filing against the different sections of the document each stakeholder is interested in.,We constructed a matrix listing out the various stakeholders of a general 10-K filing against the different sections of the document each stakeholder is interested in.,"Modify,Fact/Evidence",Persona Mapping,Persona Mapping
157-ARR,"The stakeholders are grouped together to form the personas used in DYNAMICTOC, viz. employees, business partners, investors and lendors, financial bodies and advisory and regulatory firms.","The stakeholders were grouped together to form the personas used in DYNAMICTOC, viz. employees, business partners, investors and lendors, financial bodies and advisory regulatory firms.","Modify,Grammar",Persona Mapping,Persona Mapping
157-ARR,"Similarly, the columns (headings) are grouped together according to similarity to create a mapping of topics of interest for each persona.","Similarly, the columns (headings) were grouped together according to similarity to create a mapping of topics of interest for each persona.","Modify,Grammar",Persona Mapping,Persona Mapping
157-ARR,"For this, the aspects obtained from the unsupervised technique are compared against the simplified column values from the constructed matrix.",The aspects we got from our unsupervised technique were compared against the simplified column values from the constructed matrix.,"Modify,Clarity",Persona Mapping,Persona Mapping
157-ARR,The columns with the greatest similarity (above a threshold) are associated with each persona.,The columns with the greatest similarity (above a threshold) were associated with each persona.,"Modify,Grammar",Persona Mapping,Persona Mapping
157-ARR,"For getting the personas interested in each paragraph, the paragraphs are first tagged for aspect.","For getting the personas interested in each paragraph, the paragraphs were first tagged for aspect.","Modify,Grammar",Persona Mapping,Persona Mapping
157-ARR,"From the resultant vector (which represents the confidence score of the text for each aspect), the combined score for each persona is calculated using the scores of its constituent aspects.","From the resultant vector (which represents the confidence score of the text for each aspect), the combined score for each persona was calculated using the scores of its constituent aspects.","Modify,Grammar",Persona Mapping,Persona Mapping
157-ARR,"Note that for financial documents, we were able to gather domain knowledge and leverage it to obtain the persona space.","The thing to note here is, for financial documents, we were able to get some domain knowledge and leveraged it to obtain the persona space.","Modify,Clarity",Persona Mapping,Persona Mapping
157-ARR,But the proposed technique is generalizable to other domains as well.,But the technique we are proposing is generalizable to other domains as well.,"Modify,Clarity",Persona Mapping,Persona Mapping
157-ARR,We use ELI5 dataset for training the model.,"We use ELI5 (Fan et al., 2019) dataset for training the model.","Modify,Fact/Evidence",Intelligent Navigation via Question Generation,Intelligent Navigation via Question Generation
157-ARR,"Financial documents are high value documents for businesses, and are often long and complex.","Financial documents and contracts are high value documents for business entities, and are often long and complex.","Modify,Claim",Conclusion,Conclusion
157-ARR,The default ToC-based reading experience is quite limited and document consumption can be enhanced using intelligent technologies.,The default ToC-based reading experience is quite limited and there are immense opportunities to enhance the document consumption using intelligent technologies.,"Modify,Clarity",Conclusion,Conclusion
157-ARR,DY-NAMICTOC is one of the first works to pursue this exciting research direction.,"We believe that DYNAMICTOC is one of the first works to pursue this exciting research direction, and would enable further exploration in the area.","Modify,Claim",Conclusion,Conclusion
157-ARR,DYNAMICTOC would benefit from in-domain learning of aspect keywords and questions.,"For the future direction, DYNAMICTOC would benefit from a better supervised in-domain learning of aspect keywords and questions.","Modify,Clarity",Conclusion,Conclusion
157-ARR,Evaluation of paragraph segmentation and mapping of personas to the aspects are future directions.,We would also like to work on evaluation of the paragraph segmentation and mapping of personas to the aspects in future.,"Modify,Clarity",Conclusion,Conclusion
157-ARR,A better understanding of personas would generalize the work to different domains.,A better understanding of personas or entities interested in consuming the document would help to generalize the work to different domains.,"Modify,Claim",Conclusion,Conclusion
157-ARR,"Keyword detection (Liu et al., 2009;Tixier et al., 2016) & topic modeling (Blei et al., 2001) works aim is to describe the document by a few important words or topics for concise representation.","Keyword detection (Liu et al., 2009;Tixier et al., 2016) & topic modeling (Blei et al., 2003) works aim is to describe the document by a few important words or topics for concise representation.","Modify,Fact/Evidence",Related Work,Related Work
157-ARR,"Another task is compact and informative headline generation from a document (Dorr et al., 2003;Lopyrev, 2015).","Another task is compact and informative headline generation from a document (David and Zajic, 2003;Lopyrev, 2015).","Modify,Fact/Evidence",Related Work,Related Work
157-ARR,"Early unsupervised systems are dominated by Latent Dirichlet Allocation (LDA)-based topic models (García-Pablos et al., 2018;Shi et al., 2018;Álvarez-López et al., 2016).","Early unsupervised systems are dominated by Latent Dirichlet Allocation (LDA)-based topic models (García-Pablos et al., 2018;Shi et al., 2018;Alvarez-López et al., 2016).","Modify,Grammar",Related Work,Related Work
166-ARR,,"For our usage, the input is the contextual token embedding (the L-th layer's output), and the side information is the static token embedding (the output of BERT's initial embedding layer).","Delete,Fact/Evidence",,Dimensionality Reduction using
166-ARR,,"The resulting cdimensional encoded vector can be thought of as the distilled context of the input token. (Hendrycks and Gimpel, 2016).","Delete,Fact/Evidence",,Dimensionality Reduction using
166-ARR,,3 The checkpoints will be released with the published paper.,"Delete,Fact/Evidence",,Evaluation
166-ARR,"For that reason these methods are used mainly for first-stage retrieval, followed by a reranking step.",,"Add,Claim",Related Work,
166-ARR,It maps from article and section titles to relevant paragraphs.,,"Add,Fact/Evidence",Tasks and Datasets,
166-ARR,"Following , we use the automatic by-article annotations variant, which considers all paragraphs within the same article as relevant.",,"Add,Fact/Evidence",Tasks and Datasets,
166-ARR,"While this ranker architecture approach is simple (and can also be used for the retrieval step via an approximate nearest neighbor search such as FAISS (Johnson et al., 2017), ScaNN (Guo et al., 2020) or the Pinecone managed service 2 ), the overall ranking quality is generally lower compared to methods that employ a query-document crossattention interaction.","While this ranker architecture approach is simple (and can also be used for the retrieval step via an approximate nearest neighbor search such as FAISS (Johnson et al., 2017) or ScaNN (Guo et al., 2020)), the overall ranking quality is generally lower compared to methods that employ a query-document cross-attention interaction.","Modify,Claim",Related Work,Related Work
166-ARR,"To remedy this issue, we follow an approach similar to EDEN quantization (Vargaftik et al., 2022), which uses a randomized Hadamard transform prior to quantization.","To remediate this issue, we follow the DRIVE approach (Vargaftik et al., 2021a), which uses a randomized Hadamard transform prior to quantization.","Modify,Fact/Evidence",Quantization,Quantization
166-ARR,"Unlike MSMARCO-DEV, there are multiple passages annotated for each query with graded relevance labels (instead of binary labels), allowing us to use the more informative nDCG@10 metric.","Unlike the above, there are many passages annotated for each query, and there are graded relevance labels (instead of binary labels).","Merge+Modify,Clarity",Tasks and Datasets,Tasks and Datasets
166-ARR,"Unlike MSMARCO-DEV, there are multiple passages annotated for each query with graded relevance labels (instead of binary labels), allowing us to use the more informative nDCG@10 metric.",This allows us to use the more informative nDCG@10 metric.,"Merge+Modify,Clarity",Tasks and Datasets,Tasks and Datasets
166-ARR,"TREC Complex Answer Retrieval (CAR) is a dataset (Dietz et al., 2017) curated from Wikipedia.","TREC Complex Answer Retrieval (CAR) is a dataset (Dietz et al., 2017) curated from Wikipedia, and consists of 30M passages, making storage requirements a bigger challenge compared to the MS-MARCO task.",Split+Identical,Tasks and Datasets,Tasks and Datasets
166-ARR,"The dataset consists of 30M passages, making storage requirements a more significant challenge compared to the MSMARCO task.","TREC Complex Answer Retrieval (CAR) is a dataset (Dietz et al., 2017) curated from Wikipedia, and consists of 30M passages, making storage requirements a bigger challenge compared to the MS-MARCO task.","Split+Modify,Clarity",Tasks and Datasets,Tasks and Datasets
166-ARR,"For MSMARCO, we initialized the model from reduced width pre-trained weights 4 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.","For MS-MARCO, we initialized the model from reduced width pre-trained weights 2 and fine-tuned it using knowledge distillation from an ensemble of BERT-Large, BERT-Base, and ALBERT-Large (Hofstätter et al., 2020b) on the MSMARCO small training dataset, which consists of almost 40M tuples of query, a relevant document, and an irrelevant document.","Modify,Fact/Evidence",Baseline -BERT SPLIT,Baseline -BERT SPLIT
166-ARR,"In the following sections, we denote the SDR variants as ""AESI-{c}-{B}b"" where {c} is replaced with the width of the encoded vector and {B} is replaced with the number of bits in the quantization scheme.","It the following sections, we denote the SDR variants as ""AESI-{c}-{B}b"" where {c} is replaced with the width of the encoded vector and {B} is replaced with the number of bits in the quantization scheme.","Modify,Grammar",SDR Configuration and Training,SDR Configuration and Training
166-ARR,Evaluation Results,Evaluation,"Modify,Other",SDR: Efficient Neural Re-ranking using Succinct Document Representation,SDR: Efficient Neural Re-ranking using Succinct Document Representation
166-ARR,The Distilbert model (full interaction architecture) has the highest quality and smallest index size (since it is only executed online).,The Distilbert model has the highest quality and smallest index size (since it is only executed online).,"Modify,Fact/Evidence",End to End Latency Evaluation,End to End Latency Evaluation
166-ARR,"The late interaction variants we consider have the same configuration as in the MS-MARCO case, where the baseline uses 24 features (with float16 quantization) and SDR uses 16 features (with 6 bits EDEN quantization).","The late interaction variants we consider have the same configuration as in the MS-MARCO case, where the baseline uses 24 features (with float16 quantization) and SDR uses 16 features (with 6 bits DRIVE quantization).","Modify,Fact/Evidence",End to End Latency Evaluation,End to End Latency Evaluation
166-ARR,"Using a quantization technique fitted to the Gaussian distribution of post randomized Hadamard transform data further improve quality, making the EDEN quantization superior to other quantization techniques in our case.","Using a quantization technique fitted to the Gaussian distribution of post randomized Hadamard transform data further improve quality, making the DRIVE quantization superior to other quantization techniques in our case.","Modify,Fact/Evidence",Additional Measurements,Additional Measurements
166-ARR,"To better understand the impact of side information, we measure the error rate between an input vector and its reconstructed vector (i.e., after encoding and decoding).","To better understand the impact of side information, we measure the error rate between an input vector and a reconstructed vector (i.e., after encoding and decoding) for different input vectors.","Modify,Fact/Evidence",Additional Measurements,Additional Measurements
166-ARR,"In IR, the document frequency of a token is known to be negatively correlated with the token's importance.","In the information retrieval field, the document frequency of a token is known to negatively correlated with the token's importance.","Modify,Clarity",Additional Measurements,Additional Measurements
166-ARR,This shows that the AESI scheme can better focus on tokens that are important for ranking.,This shows that the AESI scheme has a better focus on tokens that are important for ranking.,"Modify,Clarity",Additional Measurements,Additional Measurements
166-ARR,"A possible explanation for this phenomena is that the static embeddings for infrequent tokens are more informative (i.e., more helpful as side information) compared to static embeddings for frequent tokens (e.g., 'the').","A possible explanation for this phenomena is that the static embeddings for infrequent tokens is more informative (i.e., more helpful as side information) compared to static embeddings for frequent tokens (e.g., 'the').","Modify,Grammar",Additional Measurements,Additional Measurements
166-ARR,"We also found AESI excels more in compressing nouns, verbs, and adjectives, while AE-2L excels more in compressing punctuation, determiners, and adpositions.","We also found AESI excels more in compressing nouns, verbs, and adjectives, while AE-2L excels more in compressing punctuations, determiners, and adpositions.","Modify,Grammar",Additional Measurements,Additional Measurements
166-ARR,The details of this evaluation are provided in Appendix C.,The details of this evaluation appear in Appendix C.,"Modify,Clarity",Additional Measurements,Additional Measurements
166-ARR,"In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction transformer based models for passage re-ranking.","In this paper, we proposed a system called SDR to solve the storage cost and latency overhead of existing late-interaction models.","Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,"In addition, we explored different quantization techniques and showed that the recently proposed EDEN performs well in our use case and presented extensive experimentation.","In addition, we explored different quantization techniques and showed that the recently proposed DRIVE performs well in our use case and presented extensive experimentation.","Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,"In addition to EDEN (Appendix A, Algorithm 1), we consider the following quantization strategies: Deterministic Rounding (DR) (Gersho and Gray, 1992).","In addition to DRIVE (Appendix A, Algorithm 1), we consider the following quantization strategies: Deterministic Rounding (DR) (Gersho and Gray, 1992).","Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,"EDEN with Bias Correction (EDEN-BC) (Vargaftik et al., 2022, Section 2.3).","DRIVE with Bias Correction (DRIVE-BC) (Vargaftik et al., 2021b, Appendix C.3).","Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,"This variant of EDEN optimizes for lower bias over the mean squared error (MSE) by multiplying the dequantization result in Algorithm 1 by a bias correction scalar: x 2 2 / H(x), ŷ .",This variant of DRIVE optimizes for lower bias over the mean squared error (MSE) by multiplying the dequantization result in Algorithm 1 by a bias correction scalar: x 2 2 / ŷ 2 2 .,"Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,"Second, we see that EDEN performs better than all other schemes.","Second, we see that DRIVE performs better than all other schemes.","Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,"This conclusion follows by observing that EDEN and the deterministic rounding methods (DR, H-DR) are respectively better than EDEN-BC and the stochastic rounding methods (SR, H-SR).","This conclusion follows by observing that DRIVE and the deterministic rounding methods (DR, H-DR) are respectively better than DRIVE-BC and the stochastic rounding methods (SR, H-SR).","Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,"We compare the baseline -an autoencoder with 2 layers and float16 quantization -to SDR scheme, with the same number of features and EDEN 6bits quantization.","We compare the baseline -an autoencoder with 2 layers and float16 quantization -to SDR scheme, with the same number of features and DRIVE 6bits quantization.","Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,The SDR scheme is able to provide solid results even for 3 features with a MAP@1K score of 0.268.,"The SDR scheme is able to provide solid results even for 3 features with 0.266 (as a comparison, BM25 using Answerini system (Yang et al., 2017) reaches 15.6 MAP@1K score).","Split+Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,"As a comparison, this is much higher than BM25 using the Answerini system (Yang et al., 2017), which reaches 0.156 MAP@1K score.","The SDR scheme is able to provide solid results even for 3 features with 0.266 (as a comparison, BM25 using Answerini system (Yang et al., 2017) reaches 15.6 MAP@1K score).","Split+Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,"Finally, with the largest size of features we tested, 64, the baseline reached a MAP@1K score of 0.313, similar to the score achieved by SDR-20 (0.314), demonstrating the effectiveness of the static embeddings as side information.","Finally, with the largest size of features we tested, 64, the baseline reached a MAP@1K score of 0.313, similar to the score achieved by SDR-20 (0.315), demonstrating the effectiveness of the static embeddings as side information.","Modify,Fact/Evidence",Conclusion,Conclusion
166-ARR,"To improve the compression-reliability tradeoff, we leverage static token embeddings, which are available since the ranker has access to the document text (as it needs to render it to the user), and are computationally cheap to obtain.","To improve the compression-reliability tradeoff, we leverage static token embeddings, which are available given that the ranker has access to the document text (as it needs to render it to the user), and are computationally cheap to obtain.","Modify,Clarity",Introduction,Introduction
166-ARR,Ablation tests verify that adding the static vectors significantly improves the compression rates for the same ranking accuracy.,Ablation tests verify that adding the static vectors significantly improves the compression rates for the same ranker accuracy.,"Modify,Grammar",Introduction,Introduction
166-ARR,"For the MSMARCO dataset, we used a distilled model with a reduced vector width (Hofstätter et al., 2020a) as the initial pre-trained weights for the late-interaction model.","For the MSMARCO dataset, we used a distilled model with a reduced vector width (Hofstätter et al., 2020a) as the initialized pre-trained weights for the late-interaction model.","Modify,Grammar",Introduction,Introduction
166-ARR,"To summarize, here are the contribution of this work 1 :",We make the following contributions:,"Modify,Clarity",Introduction,Introduction
169-ARR,,"Regret is calculated with respect to the optimal model π * ∈ arg max π∈Π E (q,c,y,r)∼D [r], where Π is the set of all models and D is the data distribution.","Delete,Fact/Evidence",,Learning and Interaction Scenario
169-ARR,,"Starting with weaker initial models and learning with a higher noise ratio may cause learning to fail (e.g., simulation on SQUAD with 64 initial examples and 20% noise).","Delete,Claim",,Online Learning
169-ARR,,"When online perturbation-free simulation fails, online learning with noisy feedback fails too.","Delete,Claim",,Online Learning
169-ARR,,"Learning progression across datasets shows that initial models trained with 1,024 examples can achieve peak performance with one third or even one quarter of feedback provided.","Delete,Fact/Evidence",,Online Learning
169-ARR,,"Training Transformerbased models has been shown to have stability issues, especially when training with limited amount of data .","Delete,Claim",,Online Learning
169-ARR,,"Our non-standard training procedure (i.e., one epoch with a fixed learning rate) may further increase instability.","Delete,Claim",,Online Learning
169-ARR,,We report experiments with online learning.,"Delete,Fact/Evidence",,Domain Adaptation
169-ARR,,Offline adaptation experiments are discussed in Appendix B.3.,"Delete,Fact/Evidence",,Domain Adaptation
169-ARR,,"While NewsQA is crowdsourced, Trischler et al. (2017) report relatively low human performance.","Delete,Fact/Evidence",,Domain Adaptation
169-ARR,The learner aims to minimize the cumulative regret.,,"Add,Fact/Evidence",Learning and Interaction Scenario,
169-ARR,The MRQA benchmark simplifies all datasets so that each example has a single span answer with a limited evidence document length (truncated at 800 tokens).,,"Add,Fact/Evidence",Simulation Setup,
169-ARR,Regret numbers are averaged over the number of feedback observations.,,"Add,Fact/Evidence",Offline Learning,
169-ARR,This is expected because later interactions in the simulation can benefit from early feedback in online learning.,,"Add,Claim",Offline Learning,
169-ARR,"This is potentially because the model is exposed to different signals from two datasets and overall sees more data, either as supervised examples or through feedback.",,"Add,Claim",Domain Adaptation,
169-ARR,"However, on SearchQA, learning with SQUAD-initialized model performs much worse than learning with the initial model trained on 1,024 in-domain examples, potentially because of the gap in initial model performance (23.5 vs. 65 F1).",,"Add,Fact/Evidence",Domain Adaptation,
169-ARR,"Implicit human feedback, where feedback is derived from human behavior rather than explicitly requested, has also been studied, including for dialogue (Jaques et al., 2020) and instruction generation (Kojima et al., 2021).",,"Add,Fact/Evidence",Related Work,
169-ARR,"We focus on explicit feedback, but implicit signals also hold promise to improve QA systems.",,"Add,Claim",Related Work,
169-ARR,Campos et al. (2020) proposes feedback-weighted learning to improves conversational QA using simulated binary feedback.,,"Add,Fact/Evidence",Related Work,
169-ARR,"Their approach relies on multiple samples (i.e., feedback signals) per example, training for multiple epochs online by re-visiting the same questions repeatedly, and tuning two additional hyperparameters.",,"Add,Fact/Evidence",Related Work,
169-ARR,"In contrast, we study improving QA systems via feedback as a bandit learning problem.",,"Add,Fact/Evidence",Related Work,
169-ARR,"In both online and offline setups, we assume only one feedback sample per example.",,"Add,Fact/Evidence",Related Work,
169-ARR,"We also provide extensive sensitivity studies to the amount of annotations available, different model initialization, and noisy feedback across various datasets.",,"Add,Fact/Evidence",Related Work,
169-ARR,Our work's limitations are discussed in Section 1 and Section 9.,,"Add,Fact/Evidence",Conclusion,
169-ARR,"All six datasets we use are from prior work, are publicly available, and are commonly used for the study of extractive QA.",,"Add,Fact/Evidence",Conclusion,
169-ARR,Section 4 reports our computational budget and experimental setup in detail.,,"Add,Fact/Evidence",Conclusion,
169-ARR,Our codebase is available at https://github.com/ lil-lab/bandit-qa.,,"Add,Fact/Evidence",Conclusion,
169-ARR,We also estimate the learner regret (Equation 1).,"We also estimate the learner regret, a common measure for evaluating bandit learning.","Modify,Fact/Evidence",Learning and Interaction Scenario,Learning and Interaction Scenario
169-ARR,"Intuitively, regret is the deficit suffered by the learner relative to the optimal policy up to a specific time step.","Intuitively, regret is the deficit suffered by the learner relative to the optimal model (i.e., policy) up to a specific time step.","Modify,Clarity",Learning and Interaction Scenario,Learning and Interaction Scenario
169-ARR,"Formally, the cumulative regret at time T is computed with respect to the optimal policy π * ∈ arg max π∈Π E (x,y,r)∼(D,π) [r]:",The cumulative regret at time T is:,"Modify,Fact/Evidence",Learning and Interaction Scenario,Learning and Interaction Scenario
169-ARR,"Initialization is critical so the model does not return random answers, which are likely to be all bad because of the large output space.","Initialization is critical so the model does not return random answers, which are likely to all be bad because of the large output space.","Modify,Grammar",Simulation Setup,Simulation Setup
169-ARR,We use relatively little supervised data from the same domain for in-domain experiments (Section 5 and 6) to focus on the data annotation reduction potential of user feedback.,We use relatively little supervised data from the same domain for in-domain experiments (Sections 5 and 6) to focus on the data annotation reduction potential of user feedback.,"Modify,Grammar",Simulation Setup,Simulation Setup
169-ARR,"If the predicted answer span is an exact match index-wise to the annotated span, the learner observes a positive reward of 1.0, and a negative reward of -0.1 otherwise.","If the predicted answer span is an exact match index-wise to the annotated span, the learner observes a positive reward of 1.0, and negative reward -0.1 otherwise.","Modify,Grammar",Simulation Setup,Simulation Setup
169-ARR,7 This reward signal is stricter than QA evaluation metrics (tokenlevel F1 or exact match after normalization).,6 This reward signal is more strict than QA evaluation metrics (tokenlevel F1 or exact match after normalization).,"Modify,Grammar",Simulation Setup,Simulation Setup
169-ARR,"We obtain the sets of 64, 256, or 1,024 examples from prior work (Ram et al., 2021).","We obtain the sets of 64, 256, or 1024 examples from prior work (Ram et al., 2021).","Modify,Grammar",Simulation Setup,Experimental Setup
169-ARR,"10 For models initially trained on complete datasets (Section 7), we use a learning rate 2e-5 with a linear schedule, batch size 40, and 4 epochs.","9 For models initially trained on complete datasets (Section 7), we use a learning rate 2e-5 with a linear schedule, batch size 40, and four epochs.","Modify,Grammar",Simulation Setup,Experimental Setup
169-ARR,We turn off dropout to simulate interaction with users in deployment.,"We turn off dropout, because all experiments simulate interaction with users.","Modify,Fact/Evidence",Simulation Setup,Experimental Setup
169-ARR,"For offline learning experiments (Section 6), we train the model for 3 epochs on the collected feedback with a linear schedule learning rate of 3e-5.","For offline learning experiments (Section 6), we train the model for three epochs on the collected feedback with a linear schedule learning rate of 3e-5.","Modify,Grammar",Simulation Setup,Experimental Setup
169-ARR,"This section focuses on online learning, where the learner updates the model parameters after each feedback is observed (Algorithm 1).","This section focuses on online learning, where the learner updates the model parameters after each feedback collection (Algorithm 1).","Modify,Clarity",Online Learning,Online Learning
169-ARR,"This may be attributed to the quality of training set annotations, which determines the accuracy of reward in our setup.","The may be attributed to the quality of training set annotations, which determines the accuracy of reward in our setup.","Modify,Clarity",Online Learning,Online Learning
169-ARR,"This illustrates the benefit of the more standard training loop, especially with our Transformerbased model that is better optimized with a linear learning rate schedule and multiple epochs, both incompatible with the online setup.","This illustrates the benefit of the more standard training loop, especially with our Transformerbased model that is best optimized with a linear learning rate schedule and multiple epochs, both incompatible with the online setup.","Modify,Clarity",Offline Learning,Offline Learning
169-ARR,Learning from user feedback creates a compelling avenue to deploy systems that target new domains not addressed by existing datasets.,Learning from user feedback creates a compelling avenue to deploy systems that target domains not addressed by existing datasets.,"Modify,Clarity",Domain Adaptation,Domain Adaptation
169-ARR,"Bandit learning has been applied to a variety of NLP problems including neural machine translation (NMT; Kreutzer et al., 2018a,b;Mendoncca et al., 2021), structured prediction (Sokolov et al., 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al., 2021).","Bandit learning has been applied to a variety of NLP problems including neural machine translation (NMT; Sokolov et al., 2017;Kreutzer et al., 2018a,b;Mendoncca et al., 2021), structured prediction (Sokolov et al., 2016), semantic parsing (Lawrence and Riezler, 2018), intent recognition (Falke and Lehnen, 2021), and summarization (Gunasekara et al., 2021).","Modify,Fact/Evidence",Related Work,Related Work
169-ARR,"Explicit human feedback has been studied as a direct learning signal for NMT (Kreutzer et al., 2018b;Mendoncca et al., 2021), semantic parsing (Artzi and Zettlemoyer, 2011;Lawrence and Riezler, 2018), and summarization (Stiennon et al., 2020).","Human feedback has been studied as a direct learning signal for NMT (Kreutzer et al., 2018b;Mendoncca et al., 2021), semantic parsing (Lawrence and Riezler, 2018), summarization (Stiennon et al., 2020), and dialogue (Jaques et al., 2020).","Modify,Fact/Evidence",Related Work,Related Work
169-ARR,"Kratzwald et al. (2020) resembles our setting in seeking binary feedback to replace span annotation, but their goal is to create supervised data more economically.","Kratzwald et al. (2020) resembles our setting in that it seeks binary feed-back to replace span annotation, but their goal is to create supervised data more economically.","Modify,Clarity",Related Work,Related Work
169-ARR,"Domain adaptation for QA has been widely studied (Fisch et al., 2019;Khashabi et al., 2020b), including using data augmentation (Yue et al., 2021), adversarial training , contrastive method (Yue et al., 2021), back-training (Kulshreshtha et al., 2021, and exploiting small lottery subnetworks (Zhu et al., 2021).","Domain adaptation for QA has been studied in prior work (Fisch et al., 2019;Khashabi et al., 2020b), including using data augmentation (Yue et al., 2021), adversarial training , contrastive method (Yue et al., 2021), back-training (Kulshreshtha et al., 2021, and exploiting small lottery subnetworks (Zhu et al., 2021).","Modify,Clarity",Related Work,Related Work
169-ARR,"13 In practice, this does not introduce a stronger learning signal, potentially because the distribution over F1 scores is bimodal and focused on extreme values: around 85 % F1 scores are either 0 or 1 for predicted spans from a SQUAD-trained model on 8% NQ training data.","13 In practice, using F1 as feedback does not introduce stronger learning signals, potentially because the distribution over F1 scores is bimodal on extreme values: around 85 % F1 scores are either 0 or 1 for predicted spans from a SQUAD-trained model on 8% NQ training data.","Modify,Clarity",Conclusion,Conclusion
169-ARR,"For example, sharing question-and answer-annotator roles (Rajpurkar et al., 2016), which is detrimental to emulate information seeking behavior (Choi et al., 2018).","For example, sharing question-annotator and answer-annotator roles (Rajpurkar et al., 2016), which is detrimental to emulate information seeking behavior (Choi et al., 2018).","Modify,Clarity",Introduction,Introduction
169-ARR,"We show that systems initially trained on a small number of examples can dramatically improve given feedback from users on modelpredicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation, but instead improving the system on-the-fly via user feedback.","We show that systems initially trained on few examples can dramatically improve given feedback from users on model-predicted answers, and that one can use existing datasets to deploy systems in new domains without any annotation effort, but instead improving the system on-the-fly via user feedback.","Modify,Clarity",Abstract,Abstract
169-ARR,Our code is publicly available at https://github.com/ lil-lab/bandit-qa.,Our code will be made available upon publication.,"Modify,Fact/Evidence",Introduction,Introduction
169-ARR,"This formulation reflects a setup where, given a question-context pair, the QA system interacts with a user, who validates the model-predicted answer in context, and provides feedback which is mapped to numerical reward.","This formulation reflects a setup where, given a question-context pair, the QA system interacts with users, who validate the model-predicted answer in context, and provide feedback which is mapped to a numerical reward.","Modify,Grammar",Learning and Interaction Scenario,Learning and Interaction Scenario
170-ARR,,Multimodal Fusion.,"Delete,Other",,Response Decoder
170-ARR,,We optimize models by joint training to minimize the cross-entropy losses to generate responses and functional programs.,"Delete,Fact/Evidence",,Experiments
170-ARR,,"For more details of data and training, please refer to Appendix B and C.","Delete,Fact/Evidence",,Experiments
170-ARR,,Future work may focus on learning better question parsers or directly deploying a better off-the-shelf parser tool.,"Delete,Claim",,Experiments
170-ARR,,"To learn compositional programs, we follow (Johnson et al., 2017a;Hu et al., 2017) and consider program generation as a sequence-tosequence task.","Delete,Fact/Evidence",,Broader Impacts
170-ARR,,"We adopt a simple template "" param 1 module 1 param 2 module 2 ..."" as the target sequence.","Delete,Fact/Evidence",,Broader Impacts
170-ARR,,The resulting target sequences for dialogue and video understanding programs are sequences P dial and P vid respectively.,"Delete,Fact/Evidence",,Broader Impacts
170-ARR,,The parsers decompose questions into subsequences to construct compositional reasoning programs for dialogue and video understanding.,"Delete,Fact/Evidence",,Broader Impacts
170-ARR,,Each parser is an attention-based Transformer decoder.,"Delete,Fact/Evidence",,Broader Impacts
170-ARR,,"The Transformer attention is a multi-head attention on query q, key k, and value v tensors, denoted as Attention(q, k, v).","Delete,Fact/Evidence",,Broader Impacts
170-ARR,,"For each token in the q sequence , the distribution over tokens in the k sequence is used to obtain the weighted sum of the corresponding representations in the v sequence.","Delete,Fact/Evidence",,Broader Impacts
170-ARR,Training Details.,,"Add,Other",Experiments,
170-ARR,These observations imply that GPT-based models can better capture video context from video caption/summary through rich pretrained representations.,,"Add,Claim",Experiments,
170-ARR,"However, without access to video caption/summary, these models may fail to understand video from visual-only representations.",,"Add,Claim",Experiments,
170-ARR,"In this setting, GPT-based models may be inferior to VGNMN, which explicitly exploits the compositional structures from textual inputs to integrate visual features.",,"Add,Claim",Experiments,
170-ARR,We also found that VGNMN applied to object-level features is competitive to the model applied to CNN-based features.,,"Add,Claim",Experiments,
170-ARR,The Robustness.,,"Add,Other",Experiments,
170-ARR,"To evaluate model robustness, we report BLEU4 and CIDEr of model variants in various experimental settings.",,"Add,Fact/Evidence",Experiments,
170-ARR,"Specifically, we compare against performance of output responses in the first dialogue turn position (i.e. 2 nd -10 th turn vs. the 1 st turn), or responses grounded on the shortest video length range (video ranges are intervals of 0-10 th , 10-20 th percentile and so on).",,"Add,Fact/Evidence",Experiments,
170-ARR,"Video features are retrieved through a token-level representation of questions (Le et al., 2019b).",,"Add,Fact/Evidence",Experiments,
170-ARR,Dialogue history is encoded by a hierarchical LSTM encoder .,,"Add,Fact/Evidence",Experiments,
170-ARR,How to locate entities?,,"Add,Other",VGNMN: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems,
170-ARR,"Since TGIF-QA questions follow a very specific type distribution (count, action, transition, and frameQA), the question structures are simpler and easier to learn than AVSD.","Since TGIF-QA questions follow a very specific question type distribution (count, action, transition, and frameQA), the question structures are simpler and easier to learn than AVSD.","Modify,Clarity",Experiments,Experiments
170-ARR,"For any potential application or extension of work, we would like to highlight some specific concerns.","For any potential application or extension of this work, we would like to highlight some specific concerns.","Modify,Clarity",Broader Impacts,Broader Impacts
170-ARR,Each attention is followed by a network applied to each position identically.,Each attention is followed by a feed-forward network applied to each position identically.,"Modify,Clarity",Broader Impacts,Broader Impacts
172-ARR,,"XDBERT-b gains performance on 52 entries, while performing on par or worse on 225 entries.","Delete,Fact/Evidence",,Analysis
172-ARR,,We briefly describe the ablation studies on the adaptation process.,"Delete,Fact/Evidence",,Ablation study
172-ARR,,"We show the result of Visual-Text Transformers on GLUE, reported by Tan and Bansal (2020) in Table 6.","Delete,Fact/Evidence",,Conclusion
172-ARR,,All of the listed methods (except LXMERT) have their text-transformers initialized from BERT.,"Delete,Fact/Evidence",,Conclusion
172-ARR,,The results show that multi-modal training for solving vision-language tasks does not improve the performance of the models on natural language understanding tasks.,"Delete,Fact/Evidence",,Conclusion
172-ARR,,We report the performance of small tasks while using different loss functions.,"Delete,Fact/Evidence",,Conclusion
172-ARR,,"Driven by the loss function of cross-modal matching, the model sacrifices rare word representations to better match common words.","Delete,Fact/Evidence",,Conclusion
172-ARR,,"By adding VC loss, each VC token must keep its representation, which makes the cross-modal attention more robust.","Delete,Claim",,Conclusion
172-ARR,,"In the following examples, Bold words are visuallygrounded, while normal words are non-visuallygrounded.","Delete,Fact/Evidence",,Conclusion
172-ARR,,Words in brackets are stopwords and does not count towards either category.,"Delete,Fact/Evidence",,Conclusion
172-ARR,"The text-image input (X i , Y i ) is paired, and every (X j , Y k ) (j ̸ = k) is a non-pair.",,"Add,Fact/Evidence",Pretraining,
172-ARR,"The Image-Text Matching (ITM) objective is widely used in multimodal learning (Tan and Bansal, 2020;Radford et al., 2021).",,"Add,Fact/Evidence",Same sentence prediction (MATCH),
172-ARR,We modify this objective to same sentence prediction as both streams of our model takes text as input.,,"Add,Fact/Evidence",Same sentence prediction (MATCH),
172-ARR,"Same as MLM, 15% of the tokens are randomly selected for reconstruction.",,"Add,Fact/Evidence",CLIP Token Classification,
172-ARR,We address concerns on trivial solutions learned by the model in Section 5 and 9 in the appendix.,,"Add,Fact/Evidence",CLIP Token Classification,
172-ARR,Entries where both models obtain the same performance are set aside.,,"Add,Fact/Evidence",Analysis,
172-ARR,"Swapping to wiki reduces potential overfitting from the 20 Epochs setting trained on wiki103, as training for the same amount of steps on wiki is less than 1 epoch.",,"Add,Fact/Evidence",Ablation study,
172-ARR,"With the CLIPTC objective, the diversity of the input embeddings corresponding to different tokens must be retained in the cross-modal encoder, leading to more robust cross-modal attention.",,"Add,Claim",Ablation study,
172-ARR,,This loss prevents the cross-modal matching to only focus on common trivial words.,"Delete,Fact/Evidence",,CLIP Token Classification
172-ARR,,We show the attention maps of the cross-modal encoders in Appendix D to verify this.,"Delete,Fact/Evidence",,CLIP Token Classification
172-ARR,"We tested our adaptation strategy on three different language encoders coupled with CLIP-T, including BERT-base, ELECTRA-base, and ELECTRA-large.","We tested our pretraining strategy on three different language encoders coupled with CLIP-T, including BERT-base, ELECTRA-base, and ELECTRA-large.","Modify,Clarity",Experimental Results,Experimental Results
172-ARR,"Analyzing the separated entries as a whole, we discovered that the betterperforming entries have a larger visually grounded ratio (Figure 4), as the quartile, median and mean values are generally higher for improved samples.",We found out that the better-performing entries have a shorter sentence length and their tokens have a larger visually grounded ratio (Figure 3).,"Modify,Fact/Evidence",Analysis,Analysis
172-ARR,"We tested these changes on RTE, MPRC, STSB, and CoLA on 5 random seeds, and the results are shown in Table 3, where MLM refers to the joint MLM objective, MATCH refers to the cross-modal matching objective, and CLIPTC refers to the CLIP token classification objective.","We tested these changes on RTE, MPRC, STSB, and CoLA on 5 random seeds, and the results are shown in Appendix D.","Merge+Modify,Fact/Evidence",Ablation study,Ablation study
172-ARR,"The issue is also present in the finetuning phase, and the maximum sequence length of GLUE and SWAG is 128; therefore we used 2 blocks of CLIP sub-sequences to model it.","The issue is also present in the finetuning phase, and the maxmum sequence length of GLUE and SWAG is 128; therefore we used 2 blocks of CLIP sub-sequences to model it.","Modify,Grammar",Conclusion,Conclusion
172-ARR,"We tested these changes on RTE, MPRC, STSB, and CoLA on 5 random seeds, and the results are shown in Table 3, where MLM refers to the joint MLM objective, MATCH refers to the cross-modal matching objective, and CLIPTC refers to the CLIP token classification objective.","The results are shown in Table 8, where MLM refers to the joint MLM, MATCH refers to cross-modal matching, and VC (visual classification) refers to the CLIP token classification.","Merge+Modify,Fact/Evidence",Ablation study,Conclusion
172-ARR,"Other experiments include changing the number of layers in the crossmodal encoder, training for longer, and swapping to a much larger wiki (14G).","Other attempts include changing the number of layers in the cross-modal encoder, training for longer, and swapping to a much larger wiki (14G).","Modify,Clarity",Ablation study,Conclusion
172-ARR,"Besides experimental evidence, we also justify the CLIPTC loss via further analysis, as the CLIPTC objective can theoretically be trivially solved by identity mapping.","Besides experimental evidence, we also justify the VC loss via further analysis because VC loss can theoretically be trivially solved by identity mapping.","Modify,Fact/Evidence",Ablation study,Conclusion
172-ARR,"Despite this possibility, we find that the loss is crucial to cross attention learning.",The importance of this loss is to balances out the cross-modal matching loss.,"Modify,Claim",Ablation study,Conclusion
172-ARR,"Since we do not impose negative hard samples from sampled sentences, the MATCH objective can be solved sufficiently simply by guiding the cross attention to focus on common trivial words.","Without VC loss, the cross-modal matching is too easy, because relying on only a few common words is sufficient, even when the word is trivial to the meaning of the sentence.","Modify,Claim",Ablation study,Conclusion
172-ARR,We show comparisons of the attention maps generated from the cross-modal encoders with a random sequence from RTE in Table 9 in the Appendix to verify this claim.,We show the attention of the cross-modal matching with a random sequence from RTE in Table 9.,"Modify,Fact/Evidence",Ablation study,Conclusion
172-ARR,XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding,XDBERT: Distilling Visual Information to BERT via Cross-Modal Encoders to Improve Language Understanding,"Modify,Clarity",,
172-ARR,"In this work, we establish the link between pretrained multimodal transformers and visuallygrounded language learning.","In this work, we established the link between pretrained multimodal transformers and visuallygrounded language learning.","Modify,Grammar",Introduction,Introduction
172-ARR,"We devise a way to distill visual information from components of a pretrained multimodal transformer (CLIP texttransfomer, abbreviated as CLIP-T) to pretrained language transformers (BERT/ELECTRA), to incorporate versatile perception of words into the model (Figure 1).","We devised a way to distill visual information from components of a pretrained multimodal transformer (CLIP texttransfomer, briefed by CLIP-T) to pretrained language transformers (BERT/ELECTRA).","Modify,Fact/Evidence",Introduction,Introduction
172-ARR,"Methodologically, we use the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot fewer than the original pretraining steps).","Methodologically, we used the cross-modal encoder structure inspired by Tan and Bansal (2019), to concatenate the two models and further adapt the ensemble for some extra steps (a lot smaller than the original pretraining steps).","Modify,Grammar",Introduction,Introduction
172-ARR,We do ablation studies to show that each of the task provides improvement (Section 5).,We do ablation studies to show that each of the task provides improvement (Appendix D).,"Modify,Fact/Evidence",Introduction,Introduction
172-ARR,"In these tasks, BERT and CLIP-T takes sentences A and B respectively as input, and losses are calculated from both BERT output and CLIP-T output.","In these tasks, BERT and CLIP-T takes sentences A and B respectively as input as we condition the output.","Modify,Fact/Evidence",Adaptation,Adaptation
172-ARR,"This is the MLM objective done on the CLIP-T side of the full model, omitting the masking part because CLIP has no mask token.","This is the MLM objective done on the CLIP-T side of the full model, but the CLIP-T sentence input tokens is neither being masked nor replaced.","Modify,Fact/Evidence",CLIP Token Classification,CLIP Token Classification
19-ARR,,The ensemble metric outperformed the top individual metric of COMET when the zero-shot model was removed.,"Delete,Fact/Evidence",,Results and Analysis
19-ARR,,"The current BILLBOARD setup is based on rubric-based, expert evaluation data from previous work, but future work can explore ways to improve crowdsourced evaluations and use them to update BILLBOARDs.","Delete,Claim",,Related and Future Work
19-ARR,,"In particular, generation models have more aspects than generation quality, such as training and inference efficiency, sample efficiency, and robustness.","Delete,Claim",,Related and Future Work
19-ARR,,"These aspects are often ignored in the current leaderboard paradigm but are important to better serving practitioners' needs (Schwartz et al., 2019;Ethayarajh and Jurafsky, 2020).","Delete,Fact/Evidence",,Related and Future Work
19-ARR,,Seen in Table 11 are ablation studies for the ensemble metrics where one of the three selected metrics is removed at a time.,"Delete,Fact/Evidence",,Conclusion
19-ARR,,"Dropping one metric often has no impact on the correlation score, suggesting that these metrics are highly redundant and capture similar aspects of the output quality.","Delete,Claim",,Conclusion
19-ARR,,BILLBOARDs encourage researchers to explore ways to diversify automatic evaluations by updating the ensemble metric every time a new metric is submitted.,"Delete,Claim",,Conclusion
19-ARR,"9 Prior work used a concatenation of author-written highlights as a reference, but here we do not add it to the reference set.",,"Add,Fact/Evidence",Mixed-Effects Models,
19-ARR,"This is because these highlights are sometimes noisy (e.g., containing URLs) or lack coherence (Fabbri et al., 2021).",,"Add,Fact/Evidence",Mixed-Effects Models,
19-ARR,"We examined all papers whose title contains ""machine translation"" and ""summarization"" and disregarded papers primarily on evaluation metrics.",,"Add,Fact/Evidence",Conclusion,
19-ARR,"""QA"" metrics use a QA system to evaluate summaries (e.g., Eyal et al., 2019).",,"Add,Fact/Evidence",Conclusion,
19-ARR,"""Specialized"" indicates specialized evaluation in a particular dimension, rather than the overall generation quality, such as document-level evaluations on contrastive sets (Voita et al., 2019).",,"Add,Fact/Evidence",Conclusion,
19-ARR,The score column indicates the score from the metric that currently correlates best with the human judgments (ensemble).,,"Add,Fact/Evidence",Conclusion,
19-ARR,"16 They share similar pipeline structure but vary in model architecture, (pre)training data, model size, and (pre)training objective.",,"Add,Fact/Evidence",Conclusion,
19-ARR,Table 12 presents fixed-effect coefficients that measure how much each automatic metric overrates machines over humans ( §2.3).,,"Add,Fact/Evidence",Conclusion,
19-ARR,"With some exceptions in CNNDM summarization, almost all automatic metrics underrate human generations (significantly positive coefficients).",,"Add,Fact/Evidence",Conclusion,
19-ARR,"Table 13 swaps the roles of human-generated text, but we still see similar patterns: almost all metrics overrate machines over humans, but the problem is mitigated in COMET-QE, a referenceless, quality estimation metric.",,"Add,Fact/Evidence",Conclusion,
19-ARR,This confirms that our findings hold independently of the design choice.,,"Add,Claim",Conclusion,
19-ARR,"To make fair comparisons, we simulate situations where the ensemble is applied to a newly submitted generator that has no human evaluations.","To make fair comparisons, we simulate situations that the ensemble is applied to a newly submitted generator that has no human evaluations.","Modify,Grammar",Ensemble of Metrics,Ensemble of Metrics
19-ARR,"Recent work (Kasai et al., 2022) observed that automatic metrics tend to overrate machine-generated text over human one on the MSCOCO image captioning task (Chen et al., 2015).","Recent work (Kasai et al., 2021b) observed that automatic metrics tend to overrate machine-generated text over human one on the MSCOCO image captioning task (Chen et al., 2015).","Modify,Fact/Evidence",Mixed-Effects Model Analysis,Mixed-Effects Model Analysis
19-ARR,This problem is particularly severe in conventional metrics that are based on n-gram overlap such as BLEU and CIDEr .,"This problem is particularly severe in conventional metrics that are based on n-gram overlap such as BLEU and CIDEr (Vedantam et al., 2015).","Modify,Fact/Evidence",Mixed-Effects Model Analysis,Mixed-Effects Model Analysis
19-ARR,"Prior work also points out other problems in ranking metrics like outlier effects where outlier systems have a disproportionately large effect on the overall correlation (Mathur et al., 2020a,b).","Prior work also points out other problems in ranking metrics like outlier effects where outlier systems have a disproportionately large effect on the overall correlation (Mathur et al., 2020b,a).","Modify,Grammar",Design Choices and Discussion,Design Choices and Discussion
19-ARR,"These crowdworker evaluations depend highly on individual annotators' discretion and understanding of the annotation scheme (Freitag et al., 2021;Clark et al., 2021), making it difficult to decompose, interpret, and validate (Kasai et al., 2022).","These crowdworker evaluations depend highly on individual annotators' discretion and understanding of the annotation scheme (Freitag et al., 2021;Clark et al., 2021), making it difficult to decompose, interpret, and validate (Kasai et al., 2021b).","Modify,Fact/Evidence",Human Evaluation,Human Evaluation
19-ARR,"Note that this aggregation method can be modified, depending on the downstream task of interest (Kasai et al., 2022).","Note that this aggregation method can be modified, depending on the downstream of interest (Kasai et al., 2021b).","Modify,Fact/Evidence",Tasks,Tasks
19-ARR,"For each of 500 test images, rubric-based evaluations (THUMB 1.0) are available for five systems, including one caption from a crowdworker (Kasai et al., 2022).","For each of 500 test images, rubric-based evaluations (THUMB 1.0) are available for five systems, including one caption from a crowdworker (Kasai et al., 2021b).","Modify,Fact/Evidence",Tasks,Tasks
19-ARR,We follow Kasai et al. (2022) for MSCOCO and score their randomly-selected Human caption using the other four as the reference.,We follow Kasai et al. (2021b) for MSCOCO and score their randomly-selected Human caption using the other four as the reference.,"Modify,Fact/Evidence",Mixed-Effects Models,Mixed-Effects Models
19-ARR,"These results suggest that the evaluation practice should be regularly updated as our generation models become stronger (and perhaps, more similar to human generation) in the future.","These results suggest that the evaluation practice should be regularly updated as our generation model becomes stronger (and perhaps, more similar to human generation) in the future.","Modify,Grammar",Results and Analysis,Results and Analysis
19-ARR,"Meanwhile, many evaluation metrics that improve correlation with human judgments have been proposed (Clark et al., 2019;Zhang et al., 2020b;Sellam et al., 2020;Hessel et al., 2021, inter alia), but this progress has yet to be broadly adopted by the community of researchers focused on advancing models.","Meanwhile, many evaluation metrics that improve correlation with human judgments have been proposed (Clark et al., 2019;Zhang et al., 2020b;Sellam et al., 2020;Hessel et al., 2021, inter alia), but this progress is largely ignored by the community of researchers focused on advancing models.","Modify,Other",Introduction,Introduction
19-ARR,"Table 3: β 0 fixed-effect coefficients from the linear mixed-effects models, quantifying how much automatic metrics overrate machines over humans, relative to human raters.","Table 3: β 0 (fixed-effect coefficients) from the linear mixed-effects models that analyze how much automatic metrics overrate machines over humans, relative to human raters.","Modify,Clarity",Results and Analysis,Results and Analysis
19-ARR,"Indeed, consistent with prior metaevaluations (Marie et al., 2021), we found that 68% of the machine translation papers from NAACL and ACL 2021 evaluated their models solely by BLEU, and only 5% measured the performance using recent metrics with contextual representations such as COMET (Rei et al., 2020).","Indeed, we found that 68% of the machine translation papers from NAACL and ACL 2020 evaluated their models solely by BLEU, and only 5% measured the performance using recent metrics with contextual representations such as COMET (Rei et al., 2020).","Modify,Fact/Evidence",Introduction,Introduction
19-ARR,"There are ongoing modeling and benchmarking efforts especially for efficient machine translation (Heafield et al., 2020;Peng et al., 2021;Kasai et al., 2021b, inter alia).","There are ongoing modeling and benchmarking efforts especially for efficient machine translation (Heafield et al., 2020;Peng et al., 2021, inter alia).","Modify,Fact/Evidence",Related and Future Work,Related and Future Work
19-ARR,"We established and released four BILL-BOARDs on machine translation, summarization, and image captioning tasks.","We established four BILLBOARDs on machine translation, summarization, and image captioning tasks.","Modify,Fact/Evidence",Conclusion,Conclusion
19-ARR,"Meanwhile, efforts to improve generation models tend to depend on simple n-gram overlap metrics (e.g., BLEU, ROUGE).","Meanwhile, efforts to improve generation models tend to focus on simple n-gram overlap metrics (e.g., BLEU, ROUGE).","Modify,Claim",Abstract,Abstract
19-ARR,"We release four BILLBOARD interfaces (https://nlp.cs.washington.edu/ billboard/) spanning three generation tasks: the WMT20 EN-DE and WMT20 ZH-EN machine translation tasks (Barrault et al., 2020), the CNNDM summarization task (Hermann et al., 2015), and the MSCOCO image captioning task (Lin et al., 2014).","We release four BILLBOARDs spanning three generation tasks: the WMT20 EN-DE and WMT20 ZH-EN machine translation tasks (Barrault et al., 2020), the CNNDM summarization task (Hermann et al., 2015), and the MSCOCO image captioning task (Lin et al., 2014).","Modify,Fact/Evidence",Introduction,Introduction
19-ARR,"We therefore propose a generalization of leaderboards, bidimensional leaderboards (BILLBOARDs), that simultaneously tracks progress in language generation models and metrics for their evaluation.","We therefore propose a generalization of leaderboards, bidimensional leaderboards (BILLBOARDs), that simultaneously tracks progress in language generation tasks and metrics for their evaluation.","Modify,Clarity",Abstract,Abstract
19-ARR,"In particular, scores from automatic metrics often diverge from human judgments in language generation tasks, especially when models become increasingly powerful (Ma et al., 2019).","In particular, scores from automatic metrics often diverge from human judgments in language generation tasks especially when models become increasingly powerful (Ma et al., 2019).","Modify,Grammar",BILLBOARD Framework,BILLBOARD Framework
19-ARR,"Much recent work proposed new evaluation metrics that improve correlations with human judgments in certain generation tasks (Clark et al., 2019;Zhang et al., 2020b;Sellam et al., 2020;Hessel et al., 2021, inter alia), but most developers of generation models are not benefiting from them (See Appendix A for our analysis of papers from NAACL/ACL 2021).","Much recent work proposed new evaluation metrics that improve correlations with human judgments in certain generation tasks (Clark et al., 2019;Zhang et al., 2020b;Sellam et al., 2020;Hessel et al., 2021, inter alia), but most developers of generation models are not benefiting from them (See Appendix A for our analysis of papers from NAACL/ACL 2020).","Modify,Fact/Evidence",BILLBOARD Framework,BILLBOARD Framework
19-ARR,"Developers of evaluation metrics, on the other hand, are missing the opportunity to apply their metrics to new generation models and compare them with the existing ones.","Developers of evaluation metrics, on the other hand, are missing the opportunity to apply their metrics to new generation models and compare with the existing ones.","Modify,Clarity",BILLBOARD Framework,BILLBOARD Framework
192-ARR,,"Our model is developed on the basis of (Wu et al., 2020a).","Delete,Fact/Evidence",,Introduction
192-ARR,"Specifically, an entity recognizer and a similarity evaluator are first trained in parallel as two teachers from the source domain.","Specifically, an entity recognizer and a similarity evaluator teachers are first trained in parallel from the source domain.","Modify,Clarity",Abstract,Abstract
192-ARR,"During the learning process, the samples from the target language are fed into the teacher model and the outputs are taken as the supervisory signal for two tasks in the student model.","During the learning process, the samples from target language are fed into the teacher model and the outputs are taken as the supervisory signal for two tasks in the student model.","Modify,Grammar",Framework,Framework
192-ARR,"Then, two tasks in the student model are supervised by these teachers simultaneously.","Then, two tasks in the student model are supervised by the two teachers simultaneously.","Modify,Clarity",Abstract,Abstract
192-ARR,We aim to find entity similarity to help the crosslingual NER model in the target language.,We aim to find entity similarity to help the cross-lingual NER model in target language.,"Modify,Grammar",Teacher Models,Teacher Models
192-ARR,"To address this challenge, we propose a binary classifier called similarity evaluator to leverage the labeled source language data for similarity prediction.","To address this challenge, we propose a binary classifier called similarity teacher to leverage the labeled source language data for similarity prediction.","Modify,Clarity",Teacher Models,Teacher Models
192-ARR,Empirical studies on the three datasets across 7 different languages confirm the effectiveness of the proposed model.,Empirical studies on the datasets across 7 different languages confirm the effectiveness of the proposed model.,"Modify,Fact/Evidence",Abstract,Abstract
192-ARR,"Our similarity evaluator model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.","Our similarity teacher model, inspired by siamese network (Koch et al., 2015), are able to acquires more powerful features via capturing the invariances to transformation in the input space.","Modify,Clarity",Teacher Models,Teacher Models
192-ARR,Entity Recognizer,Entity Recognizer Teacher,"Modify,Other",Framework,Framework
192-ARR,"To leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.","In order to leverage the entity similarity to boost the unsupervised cross-lingual NER performance, we will present our entity pairs construction method and the siamese network model in the following.","Modify,Clarity",Siamese Entity Similarity Evaluator,Siamese Entity Similarity Evaluator
192-ARR,"The inter-entities similarity is measured on the hidden representations h i and h j of the tokens queried by the entity indices < i, j > on the sequences representations.","The inter-entities similarity is measured on the tokens hidden representations h i and h j , queried by the entity indices < i, j > on the sequences representations.","Modify,Clarity",Siamese Entity Similarity Evaluator,Siamese Entity Similarity Evaluator
192-ARR,"The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) makes this task achieve significant performances.","The exploiting of deep neural networks, such as Bi-LSTM-CRF (Lample et al., 2016), Bi-LSTM-CNN (Chiu and Nichols, 2016) make this task achieves significant performances.","Modify,Grammar",Introduction,Introduction
192-ARR,Teacher-student Distillation Learning,Teacher Student Distillation Learning,"Modify,Grammar",Framework,Framework
192-ARR,"In this section, we consider transferring the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task.","In this section, we consider to transfer the named entity type and similarity knowledge learned on labeled source language corpus to unlabeled target language NER task.","Modify,Grammar",Teacher-student Distillation Learning,Teacher Student Distillation Learning
192-ARR,"The mBERT is also used as an encoder for the sentence siamese pair, and the entity token feature is queried from the latent sequence encoding representation.","The multi-lingual BERT is also used as encoder for the sentence siamese pair, and the entity token feature queried from the latent sequence encoding representation.","Modify,Clarity",Teacher-student Distillation Learning,Teacher Student Distillation Learning
192-ARR,"However, since deep neural networks highly rely on a large amount of labelled training data, the annotation acquiring process is expensive and time consuming.","However, since deep neural networks highly relies on a large amount of labelled training data, the annotation acquiring process is expensive and time consuming.","Modify,Grammar",Introduction,Introduction
192-ARR,This situation is more severe for zero-resource languages.,This situation is more severe for low-resource languages.,"Modify,Claim",Introduction,Introduction
192-ARR,Cross-lingual named entity recognition task is one of the critical problems for evaluating the potential transfer learning techniques on low resource languages.,Cross-lingual named entity recognition task is one of the critical problem for evaluating the potential transfer learning techniques on low resource languages.,"Modify,Grammar",Abstract,Abstract
192-ARR,"We set the batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014).","We set batch size to be 32, maximum sequence length to be 128, dropout rate to be 0.2, and we use Adam as optimizer (Kingma and Ba, 2014).","Modify,Grammar",Implementation Details,Implementation Details
192-ARR,"Specifically, compared with the remarkable RIKD, AdvPicker, and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].","Specifically, compared with the remarkable RIKD, AdvPicker and Unitrans, which also use knowledge distillation but ignore the entity similarity knowledge, our model obtains significant and consistent improvements in F1-score ranging from 0.23 for German[de] to 6.81 for Arabic [ar].","Modify,Grammar",Comparison,Comparison
192-ARR,Note that BERT-f performs better than our model on the Chinese dataset due to their re-tokenization of the dataset.,Note that Bert-f performs better than our model on Chinese dataset due to their re-tokenization of the dataset.,"Modify,Grammar",Comparison,Comparison
192-ARR,"That is, the teacher model has the same as the neural network structure of the student model.","That is, both of the teacher and student have the same neural network structure.","Modify,Clarity",Ablation Study,Ablation Study
192-ARR,This causes a performance drop across all languages due to two single teachers cannot make a difference with the combination.,This causes a performance drop across all languages due to two single teachers cannot make a difference with combination.,"Modify,Grammar",Ablation Study,Ablation Study
192-ARR,"(2) MTMT w/o weighting, which set the α (•) , β and γ all to be 1 in the loss of student learning.","(2) MTMT w/o weighting, which set the α 1 ,α 2 , β and γ all to be 1 in the loss of student model learning.","Modify,Fact/Evidence",Ablation Study,Ablation Study
192-ARR,"It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es), which validates that weighting loss can bring more confident knowledge to the student model.","It can be seen that the performance decrease in terms of F1-score ranges from 0.45 for Dutch(nl) to 0.98 for Spanish(es), which validates that weighting loss can bring more confident knowledge to student model.","Modify,Grammar",Ablation Study,Ablation Study
192-ARR,"In this case, our approach degrades into the single teacherstudent learning model as in TSL (Wu et al., 2020a).","In this case, our approach degrades into the Single Teacher-Student learning model as in TSL (Wu et al., 2020a).","Modify,Grammar",Ablation Study,Ablation Study
192-ARR,"Specifically, if there is a set of tokens in which every two of them have a high Entity Similarity score, and one of the tokens is predicted to have a distinct label while other tokens have identical labels, then the one with the distinct label is predicted wrongly and is corrected by the student model to have the label of all other tokens.","Specifically, if there is a set of tokens in which every two of them have high Entity Similarity score, and one of the tokens is predicted to have a distinct label while other tokens have identical labels, then the one with the distinct label is predicted wrongly and is corrected by the student model to have the label of all other tokens.","Modify,Grammar",Case Study,Case Study
192-ARR,Embedding Distribution,Embeddings Distribution,"Modify,Grammar",Experiment,Experiment
192-ARR,"It can be seen that the embedding distribution of the student model is close to similarity evaluator teacher, as illustrated in Figure 5.","It can be seen that the embeddings distribution of student model is close to similarity evaluator teacher, as illustrated in Figure 5.","Modify,Grammar",Embedding Distribution,Embeddings Distribution
192-ARR,Many studies have been done to solve this crosslingual NER problem.,Many studies have been done to solve this crosslanguage NER problem.,"Modify,Grammar",Introduction,Introduction
192-ARR,"We conjecture that the student model captures similarity knowledge from the similarity evaluator teacher, i.e. the same class of examples tend to cluster and the different class of examples tend to segregate in the embedding distribution.","We conjecture that the student model captures similarity knowledge from the similarity evaluator teacher, i.e. the same class of examples tend to cluster and the different class of examples tend to segregate in the embeddings distribution.","Modify,Grammar",Embedding Distribution,Embeddings Distribution
192-ARR,"In this section, we evaluate the effectiveness of weight loss in student learning from a quantitative perspective.","In the section, we evaluate the effectiveness of the weighting loss in student learning from quantitative perspective.","Modify,Grammar",Effect of Weights,Effect of Weights
192-ARR,"Therefore, the student model is better suited to the target language with learning fewer low-confidence misrecognitions for the target language.","Therefore, the student model is better suited to target language with learning less low-confidence misrecognitions for the target language.","Modify,Clarity",Effect of Weights,Effect of Weights
192-ARR,The encoder of the student model obtains the clustering information of the target language with the help of β.,The encoder of student model obtains the clustering information of the target language with the help of β.,"Modify,Grammar",Effect of Weights,Effect of Weights
192-ARR,"The student model learns less from unreasonable results, and it can make more accurate entity recognition for the target language.","The student model learns less from unreasonable results, and it can make more accuracy entity recognition for the target language.","Modify,Grammar",Effect of Weights,Effect of Weights
192-ARR,"Moreover, to guarantee the student learning performance, we also propose a weighting strategy to take into consideration the reliability of the teachers.","Moreover, in order to guarantee the student learning performance, we also propose a weighting strategy to take consideration of the reliability of the teachers.","Modify,Grammar",Conclusion,Conclusion
192-ARR,"Shared feature space based models exploit language-independent features, which lacks the domain-specific features for the target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).","Shared feature space based models exploit language-independent features, which lacks the domain specific features for target language (Tsai et al., 2016;Wu and Dredze, 2019;Keung et al., 2019).","Modify,Grammar",Introduction,Introduction
192-ARR,"Although the above-mentioned models solve the cross-lingual NER problem to some extent, the auxiliary tasks, as in multi-task learning, have not been studied in this problem.","Although above mentioned models solve the cross-lingual NER problem in some extent, the auxiliary tasks, as in the multi-task learning, have not been studied in this problem.","Modify,Grammar",Introduction,Introduction
192-ARR,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority in transfer.,Knowledge distillation using pre-trained multilingual language models between source and target languages have shown their superiority.,"Modify,Clarity",Abstract,Abstract
192-ARR,"Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target languages.","Due to the distributed representation of natural languages, the relatedness among the embedding of target languages, which is measured by the similarity, can be utilized to further boost the learned encoder and improve the final NER performance on target language.","Modify,Grammar",Introduction,Introduction
192-ARR,"Given a Spanish sentence ""Arévalo (Avila), 23 may (EFE)."", the token ""Arévalo"" is recognized as ORG type using the learned model from the English domain.","Given a Spanish sentence ""Arévalo (Avila), 23 may (EFE)."", the token ""Arévalo"" is recognized as ORG type using the learned model from English domain.","Modify,Grammar",Introduction,Introduction
192-ARR,"However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domains.","However, existing cross-lingual distillation models merely consider the potential transferability between two identical single tasks across both domain.","Modify,Grammar",Abstract,Abstract
192-ARR,We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results show the superiority of our presented MTMT model.,We validate the model performance on the three commonly-used datasets across 7 languages and the experimental results shows the superiority our presented MTMT model.,"Modify,Grammar",Introduction,Introduction
192-ARR,"Our approach is closely related to the existing works on cross-lingual NER, knowledge distillation, and siamese network.","Our approach is closely related to the existing works on cross-lingual NER, knowledge distillation and siamese network.","Modify,Grammar",Related Work,Related Work
192-ARR,"Recently, the pre-trained multilingual language model is effective to address the challenge (Devlin et al., 2019).","Recently, the pre-trained multilingual language models mBERT is effective to address the challenge (Devlin et al., 2019).","Modify,Fact/Evidence",Related Work,Related Work
192-ARR,"Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from the labeled source language to that of target languages (Keung et al., 2019).","Moreover, some research introduces new components on top of the mBERT by directly transferring the model learned from labeled source language to that of target languages (Keung et al., 2019).","Modify,Grammar",Related Work,Related Work
192-ARR,Translation based models generally generate pseudo-labeled target data to alleviate target data scarcity.,Translation based models generally generate pesudo labeled target data to alleviate target data scarcity.,"Modify,Grammar",Related Work,Related Work
192-ARR,"In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on the target domain.","In this study, based on the knowledge distillation framework and multitask learning, we introduce the similarity metric model as an auxiliary task to improve the cross-lingual NER performance on target domain.","Modify,Grammar",Abstract,Abstract
192-ARR,"For example, (Wu et al., 2020b;Zhang et al., 2021) gain an improvement by translating the labeled source language to the target language word-by-word.","For example, (Wu et al., 2020b;Zhang et al., 2021) gain a improvement by translating the labeled source language to the target language word-by-word.","Modify,Grammar",Related Work,Related Work
192-ARR,"Knowledge distillation based models include a teacher model and a student model (Wu et al., 2020c).","Knowledge distillation based models includes a teacher model and a student model (Wu et al., 2020c).","Modify,Grammar",Related Work,Related Work
192-ARR,The teacher model is trained on the labeled source language.,The teacher model is trained on labeled source language.,"Modify,Grammar",Related Work,Related Work
192-ARR,The student model learns from the soft label predicted by the teacher model on unlabeled target language data.,The student model learns from the soft label predicted by teacher model on unlabeled target language data.,"Modify,Grammar",Related Work,Related Work
192-ARR,"However, there is a dilemma to adapt the siamese network to tokenlevel recognition tasks such as NER.","However, there is a dilemma to adapt siamese network to token-level recognition tasks such as NER.","Modify,Grammar",Related Work,Related Work
192-ARR,Our framework is consist of two models: teacher training model learned from the source language and teacher-student distillation learning model learned from the target language.,Our framework is consist of two models: teacher training model learned from source language and teacher-student distillation learning model learned from target language.,"Modify,Grammar",Framework,Framework
21-ARR,,"For future work, we would like to further investigate the teaching skills learned by the meta teacher from a theoretical perspective and use the insights to improve conventional knowledge distillation.","Delete,Claim",,Discussion
21-ARR,Note that the student is never trained on the quiz set and the teacher only performs meta-update on the quiz set instead of fitting it.,,"Add,Fact/Evidence",Learning to Teach,
21-ARR,We do not use a dynamic quiz set strategy because otherwise the student would have been trained on the quiz set and the loss would not be informative.,,"Add,Claim",Learning to Teach,
21-ARR,"For MRPC and QQP, we report both F1 and accuracy.",,"Add,Fact/Evidence",Experimental Setup,
21-ARR,"For STS-B, we report Pearson and Spearman correlation.",,"Add,Fact/Evidence",Experimental Setup,
21-ARR,The metric for CoLA is Matthew's correlation.,,"Add,Fact/Evidence",Experimental Setup,
21-ARR,The other tasks use accuracy as the metric.,,"Add,Fact/Evidence",Experimental Setup,
21-ARR,"Although our approach takes more time to achieve its own peak performance, it can match up the performance of PKD (Sun et al., 2019) with a similar time cost.",,"Add,Fact/Evidence",Limitation,
21-ARR,MetaDistil focuses on improving the performance of knowledge distillation and does not introduce extra ethical concerns compared to vanilla KD methods.,,"Add,Fact/Evidence",Discussion,
21-ARR,"Nevertheless, we would like to point out that as suggested by Hooker et al. (2020), model compression may lead to biases.",,"Add,Fact/Evidence",Discussion,
21-ARR,"However, this is not an outstanding problem of our method but a common risk in model compression, which needs to be addressed in the future.",,"Add,Claim",Discussion,
21-ARR,"In MetaDistil, the student is trained in a dynamic manner.",,"Add,Fact/Evidence",Discussion,
21-ARR,"To investigate the effect of such a dynamic distillation process, we attempt to use the teacher at the end of MetaDistil training to perform a static conventional KD, to verify the effectiveness of our dynamic distillation strategy.",,"Add,Fact/Evidence",Discussion,
21-ARR,"As shown in Table 4, on both experiments, dynamic MetaDistil outperforms conventional KD and static distillation with the teacher at the end of MetaDistil training.",,"Add,Fact/Evidence",Discussion,
21-ARR,"As mentioned in Section 3.2, a meta teacher is optimized to transfer its knowledge to a specific student network.",,"Add,Fact/Evidence",Discussion,
21-ARR,"To justify this motivation, we conduct experiments using a teacher optimized for the ResNet-32 student to statically distill to the ResNet-20 student, and also in reverse.",,"Add,Fact/Evidence",Discussion,
21-ARR,"As shown in Table 4, the cross-taught students underperform the static students taught by their own teachers by 0.27 and 0.12 for ResNet-32 and ResNet-20, respectively.",,"Add,Fact/Evidence",Discussion,
21-ARR,This confirms our motivation that the meta teacher in MetaDistil can adjust itself according to its student.,,"Add,Fact/Evidence",Discussion,
21-ARR,"We also investigate why MetaDistil works by conducting experiments on the development sets of MNLI, SST, and MRPC, which are important tasks in GLUE that have a large, medium, and small training set, respectively.",,"Add,Fact/Evidence",Discussion,
21-ARR,"We illustrate the validation accuracy curves of the meta teacher and student models with training steps in Figure 5, and compare them to the student performance in conventional KD.",,"Add,Fact/Evidence",Discussion,
21-ARR,"We can see that the meta teacher maintains high accuracy in the first 5,000 steps and then begins to slowly degrade.",,"Add,Fact/Evidence",Discussion,
21-ARR,"Starting from step 8,000, the teacher model underperforms the student while the student's accuracy keeps increasing.",,"Add,Fact/Evidence",Discussion,
21-ARR,This verifies our assumption that a model with the best accuracy is not necessarily the optimal teacher.,,"Add,Fact/Evidence",Discussion,
21-ARR,"While MetaDistil achieves improved student accuracy on the GLUE benchmark, it is still not very clear where the performance improvement comes from.",,"Add,Claim",Discussion,
21-ARR,"There are two possibilities: (1) the student better mimics the teacher, and (2) the changes of teacher helps student perform better on hard examples that would be incorrectly classified by the student with vanilla KD.",,"Add,Claim",Discussion,
21-ARR,We conduct a series of analysis on the MRPC dataset.,,"Add,Fact/Evidence",Discussion,
21-ARR,"For the first assumption, we compute the prediction loyalty (Xu et al., 2021a) of the student model distilled with PKD and MetaDistil, respectively.",,"Add,Fact/Evidence",Discussion,
21-ARR,"For MetaDistil, we measure the loyalty with respect to both the original teacher and the final teacher.",,"Add,Fact/Evidence",Discussion,
21-ARR,We find that there is no significant difference between between PKD and MetaDistil.,,"Add,Fact/Evidence",Discussion,
21-ARR,This suggests that the improvement does not come from student better mimicking the teacher.,,"Add,Claim",Discussion,
21-ARR,"For the second assumption, we first identify the examples in the quiz set for which our model gives correct predictions while the student distilled by PKD makes a wrong prediction.",,"Add,Fact/Evidence",Discussion,
21-ARR,"Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks (Xu et al., , 2021bXu & McAuley, 2022).","Knowledge Distillation Recently, many attempts have been made to accelerate large neural networks .","Modify,Fact/Evidence",Related Work,Related Work
21-ARR,"Hinton et al. (2015b) first introduced the idea of knowledge distillation to exploit the ""dark knowledge"" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model.","Hinton et al. (2015) first introduced the idea of knowledge distillation to exploit the ""dark knowledge"" (i.e., soft label distribution) from a large teacher model as additional supervision for training a smaller student model.","Modify,Fact/Evidence",Related Work,Related Work
21-ARR,"Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015b) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015b;Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Ahn et al., 2019;Park et al., 2019;Passalis & Tefas, 2018;Heo et al., 2019;Kim et al., 2018;Shi et al., 2021;Sanh et al., 2019;Jiao et al., 2019;Wang et al., 2020b).","Among techniques for compression, knowledge distillation (KD) (Hinton et al., 2015) has shown effectiveness in both Computer Vision and Natural Language Processing tasks (Hinton et al., 2015;Romero et al., 2015;Zagoruyko & Komodakis, 2017;Tung & Mori, 2019;Ahn et al., 2019;Park et al., 2019;Passalis & Tefas, 2018;Heo et al., 2019;Kim et al., 2018;Shi et al., 2021;Sanh et al., 2019;Jiao et al., 2019;Wang et al., 2020b).","Modify,Fact/Evidence",Introduction,Introduction
21-ARR,"For example, meta pseudo labels (Pham et al., 2020) use meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model.","For example, meta pseudo labels (Pham et al., 2020) uses meta learning to optimize a pseudo label generator for better semisupervised learning; meta back-translation (Pham et al., 2021) meta-trains a back-translation model to better train a machine translation model.","Modify,Grammar",Related Work,Related Work
21-ARR,"In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problems, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.","In this paper, we introduce a pilot update mechanism, which is a simple and general method for this kind of problem, for the inner-learner to mitigate this issue and make the updated meta-learner better adapted to the inner-learner.","Modify,Grammar",Related Work,Related Work
21-ARR,"Liu et al. (2020) proposed a self-distillation network which utilizes meta-learning to train a label-generator as a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers.","Liu et al. (2020) proposed a self-distillation network and utilizes meta-learning to train a label-generator, which is a fusion of deep layers in the network, to generate more compatible soft targets for shallow layers.","Modify,Clarity",Related Work,Related Work
21-ARR,"Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error (MSE) between student and teacher logits, the similarity between the student and the teacher's attention distribution, etc.","Some popular similarity measurements include the KL divergence between the output probability distribution, the mean squared error between student and teacher logits, the similarity between the student and the teacher's attention distribution, etc.","Modify,Clarity",Knowledge Distillation,Knowledge Distillation
21-ARR,"This ""learning to teach"" paradigm naturally fits the bi-level optimization framework in meta learning literature.","This ""learning to teach"" paradigm naturally fits the bi-level optimization framework in the meta learning literature.","Modify,Grammar",Learning to Teach,Learning to Teach
21-ARR,"We apply the pilot update strategy described in Section 3.2.1 to better align the learning of the teacher and student, as shown in Algorithm 1.",We apply the pilot update strategy described in Section 3.2.1 to better align the learning of the teacher and student.,Merge+Identical,Learning to Teach,Learning to Teach
21-ARR,"We apply the pilot update strategy described in Section 3.2.1 to better align the learning of the teacher and student, as shown in Algorithm 1.",The complete algorithm is shown in Algorithm 1.,"Merge+Modify,Clarity",Learning to Teach,Learning to Teach
21-ARR,"Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably because of its ability to adjust the parameters of the teacher model.","Also, compared to conventional KD, MetaDistil is more robust to different student capacity and hyperparameters, which is probably because of its ability to adjust its parameters.","Modify,Claim",Introduction,Introduction
23-ARR,,"Our goal is to direct attention on a challenging aspect of out-of-domain generalization by providing a new evaluation benchmark, as well as an initial direction for solving this problem, and reference point for future work.","Delete,Claim",,Abstract
23-ARR,Our evaluation benchmarks along with code for reproducing our experiments are available at https://aka.ms/ text-to-sql-schema-expansion-generalization.,,"Add,Fact/Evidence",Introduction,
23-ARR,"This step will result in disproportionally more column operations being used in our test set than in our train set, which means that the model will need to learn to generalize well in this setting to do well in this dataset.",,"Add,Claim",SQUALL Repartitioning,
23-ARR,"Note that for SQUALL, researchers often also report execution accuracy, which measures the fraction of examples for which executing the predicted SQL queries results in the correct answer to the input question.",,"Add,Claim",Experimental Setup,
23-ARR,"However, we found that for 7% of the examples that are representative of out-of-domain generalization, executing the gold SQL queries does not yield the correct answer (e.g., in cases where the correct answer is a sub-string of a cell value).",,"Add,Fact/Evidence",Experimental Setup,
23-ARR,Therefore we chose to only report exact match accuracy in our experiments.,,"Add,Fact/Evidence",Experimental Setup,
23-ARR,"Through column expansion, we created a new table schema that is more friendly to downstream parsers.",,"Add,Claim",Conclusion,
23-ARR,"Our work uses heuristics based schema expansion and works well when limited to columns that have specified types (e.g., scores or timespans), but our synthetic experiments suggest much larger potential on this problem.",,"Add,Claim",Conclusion,
23-ARR,"To be applicable in real scenarios, semantic parsers must be able to generalize to new domains since collecting domain-specific labeled data is often prohibitively expensive.","However, to be applicable in real scenarios, semantic parsers should be able to generalize to new domains, since collecting domain-specific labeled data is often prohibitively expensive.","Modify,Other",Background,Background
23-ARR,"This method can be easily applied to multiple existing base parsers, and we show that it significantly outperforms baseline parsers on this domain generalization problem, boosting the underlying parsers' overall performance by up to 13.8% relative accuracy gain (5.1% absolute) on the new SQUALL data split.","We show that on this domain generalization over column operations problem, our proposed method significantly outperforms baseline parsers, and as a result boosting the underlying parsers' overall performance by up to 13.8% relative accuracy gain (5.1% absolute) on the new SQUALL data split.","Link+Modify,Clarity",Abstract,Abstract
23-ARR,"However, as Suhr et al. (2020) point out, SPIDER also uses a simplified setting which excludes examples that involve multiple columns (e.g., adding two columns together), as well as ones that require background knowledge.","However, as Suhr et al. (2020) point out, SPIDER uses a simplified setting which excludes examples that involve multiple columns (e.g., adding two columns together), as well as ones that require background knowledge.","Modify,Clarity",Background,Background
23-ARR,"Furthermore, while both WIKISQL and SPIDER assume ""simple"" tables with only String-or Number-valued columns, in practice we may encounter tables where the columns themselves may have structured types (e.g., TimeSpan).","Also, while both WIKISQL and SPIDER assume ""simple"" tables with only String-or Number-valued columns, in practice we may encounter tables where the columns themselves may have structured types (e.g., TimeSpan).","Modify,Clarity",Background,Background
23-ARR,"Therefore as we will show in the following section, we aim to address this limitation by repartitioning SQUALL into new train and test splits.","Therefore as we will show in the following section, we aim to addresses this limitation by repartitioning SQUALL into new train and test splits.","Modify,Grammar",Background,Background
23-ARR,"SMBOP, on the other hand, uses bottom-up decoding, which represents programs as abstract syntax trees and constructs these trees in a bottom-up fashion (i.e., it starts by predicting the leaf nodes and then recursively composes generated sub-trees into new trees and ranks them, in a way that resembles beam search), until it reaches the tree root.","SMBOP, on the other hand, uses bottom-up decoding, which represents programs as abstract syntax trees and constructs these trees in a bottom-up fashion (i.e., it starts by predicting the leave nodes and then recursively composes generated sub-trees into new trees and ranks them, in a way that resembles beam search), until it reaches the tree root.","Modify,Grammar",Background,Background
23-ARR,"Our goal is to design an evaluation benchmark that has the following out-of-domain generalization properties: (i) the training data involves a different set of domains from the test data, (ii) the questions and tables that appear in the train and test data are non-overlapping, not only in terms of the domains they belong to, but also in terms of the program fragments that they contain, and (iii) to simulate the more challenging setting that is often encountered in real applications, the test data is biased to contain more examples that involve both nested column access operations, like getting the start of a ""Term"" in Figure 2, as well as composite column expressions, like getting the duration of a ""Term"".","Our goal is to design an evaluation benchmark that has the following out-of-domain generalization properties: (i) the training data involves a different set of domains than the test data, (ii) the questions and tables that appear in the train and test data are non-overlapping, not only in terms of the domains they belong to, but also in terms of the program fragments that they contain, and (iii) To simulate the more challenging setting that often encountered in real applications, the test data is biased to contain more examples that involve both nested column access operations, like getting the start of a ""term"" in Figure 2, as well as composite column expressions, like getting the duration of a ""term"".","Modify,Grammar",Proposed Evaluation Benchmarks,Proposed Evaluation Benchmarks
23-ARR,"2. For each column we declare a set of noun phrases that can be used to refer to it (e.g., ""wages"" for ""Income"" and ""base salary"" for ""salary"").","2. For each column we declare a set of noun phrases that can be used to refer to it (e.g., ""wages"" for ""Income"", and ""base salary"" for ""salary"").","Modify,Grammar",Synthetic Dataset,Synthetic Dataset
23-ARR,"We also define a SQL query template that shall be used for all programs: SELECT <column> FROM t WHERE ""Year"" = <year>, and a question template What was <column> in <year>?","And we define the SQL query template that shall be used for all programs: SELECT <column> FROM t WHERE ""Year"" = <year>, and a question template What was <column> in <year>?","Modify,Clarity",Synthetic Dataset,Synthetic Dataset
23-ARR,"3. We sample a formula and a variable from that formula (e.g., ""Income"" from formula ""Income"" = ""Salary"" + ""Stock"" ).","3. We first sample a formula, and a variable from that formula (e.g., sample ""Income"" from formula ""Income"" = ""Salary"" + ""Stock"" ).","Modify,Clarity",Synthetic Dataset,Synthetic Dataset
23-ARR,The goal of schema expansion is to reduce column operation challenges to column matching by adding synthetic columns to the table schema.,"The goal of schema expansion is to reduce column operations to column matching, by adding synthetic columns to the table schema, which correspond to expressions or accessors over existing columns (e.g., it may add a column that represents the sum of two columns).","Split+Modify,Claim",Proposed Method,Proposed Method
23-ARR,"These synthetic columns correspond to expressions or accessors over existing columns (e.g., a column that represents the sum of two columns).","The goal of schema expansion is to reduce column operations to column matching, by adding synthetic columns to the table schema, which correspond to expressions or accessors over existing columns (e.g., it may add a column that represents the sum of two columns).","Split+Modify,Clarity",Proposed Method,Proposed Method
23-ARR,"Rather than learning (or memorizing) the ways in which different types of columns can be composed together, we propose to inject prior knowledge as to what kind of symbolic operations are possible based solely on the column types in a schema.",This is based on the intuition that learning (or rather memorizing) the ways in which different types of columns can be composed together requires a large amount of in-domain training data.,"Merge+Modify,Claim",Proposed Method,Proposed Method
23-ARR,"Rather than learning (or memorizing) the ways in which different types of columns can be composed together, we propose to inject prior knowledge as to what kind of symbolic operations are possible based solely on the column types in a schema.","Instead, we propose to inject prior knowledge as to what kind of symbolic operations are possible based solely on the column types in a schema.","Merge+Modify,Clarity",Proposed Method,Proposed Method
23-ARR,This reduces column operations to column matching by effectively bringing the target programs closer to their surface form in the natural language question.,"This reduces column operations to column matching by effectively bringing the target programs closer to their surface form in the natural language question (e.g., ""Income"" can now map to a synthetic column that corresponds to the sum of ""Salary"" and ""Stock"", instead of having the parser produce the sum expression directly).",Split+Identical,Proposed Method,Proposed Method
23-ARR,"For example, ""Income"" can now map to a synthetic column that corresponds to the sum of ""Salary"" and ""Stock"" instead of having the parser produce the sum expression directly.","This reduces column operations to column matching by effectively bringing the target programs closer to their surface form in the natural language question (e.g., ""Income"" can now map to a synthetic column that corresponds to the sum of ""Salary"" and ""Stock"", instead of having the parser produce the sum expression directly).","Split+Modify,Clarity",Proposed Method,Proposed Method
23-ARR,"Since our expansion is based on column types, we argue that it is reasonable to assume that all schemas are typed and our expansion could be applied to any new domain.","Since our expansion is based on column types, we argue that it's reasonable to assume that all schemas are typed and our expansion could be applied to any new domain.","Modify,Clarity",Proposed Method,Proposed Method
23-ARR,"To this end, we introduce a schema pruning component that looks at both the expanded table schema and the question and decides which columns to prune before invoking the parser.","To this end, we introduce a schema pruning component which looks at both the expanded table schema and the question and decides which columns to prune before invoking the parser.","Modify,Clarity",Proposed Method,Proposed Method
23-ARR,"Although these templates are somewhat tailored to this dataset, our main goal is to show that there is considerable room for improvement in this challenging generalization scenario, and that even a simple approach with minimal manual effort can result in significant gains.","Although these templates are somewhat tailored to this dataset, our main goal is to show that there is considerable room for improvement in this challenging generalization scenario, and that even a very simple approach requiring minimal manual effort can result in significant boosts.","Modify,Clarity",Schema Expansion,Schema Expansion
23-ARR,This will result in the underlying parser being unable to handle situations where irrelevant columns are mistakenly left unpruned by the pruning model.,This will result in the underlying parser being unable to handle situations where irrelevant columns are left in by the pruning model.,"Modify,Fact/Evidence",Schema Pruning,Schema Pruning
23-ARR,"To this end, during training we introduce some irrelevant columns to improve the robustness of the underlying parser.","To this end, during training we introduce some irrelevant columns (i.e., negative column sampling) to improve the robustness of the underlying parser.","Modify,Fact/Evidence",Schema Pruning,Schema Pruning
23-ARR,"We found that making sure to always include at least 3 columns in the resulting schemas was sufficient and equivalent to randomly sampling 1 or 2 additional columns for each training example, and so that is what we did in our experiments.","We found that making sure to always include at least 3 columns in the schemas was sufficient and equivalent to randomly sampling 1 or 2 additional columns for each training example, and so that is what we did in our experiments.","Modify,Clarity",Schema Pruning,Schema Pruning
23-ARR,"We repeat each experiment three times using different random seeds and report mean exact match accuracy (i.e., fraction of examples where the predicted SQL queries exactly match the gold queries), and standard error for this mean.","We repeat each experiment 3 times using different random seeds and report mean exact match accuracy (i.e., fraction of examples where the predicted SQL queries exactly match the gold queries), and standard error for this mean.","Modify,Grammar",Experimental Setup,Experimental Setup
23-ARR,"In such scenarios, it is common to encounter tables specific to new domains that were not encountered while training a parser.","In such scenarios, it is common to encounter tables specific to new domains that were never encountered before while training a parser.","Modify,Clarity",Introduction,Introduction
23-ARR,"In this case, our approach provides a very significant accuracy gain, rendering them useful (up to 55.0% absolute / 327.4% relative).","In this case, our approach provides a very significant accuracy boost, rendering them useful (up to 55.0% absolute / 327.4% relative).","Modify,Clarity",Results,Results
23-ARR,"We argue that two kinds of abstract operations, shown in Figure 1, are particularly challenging for new domains:","This is mainly because of two kinds of abstract operations, shown in Figure 1, that are challenging for new domains:","Modify,Claim",Introduction,Introduction
23-ARR,"In this paper, we introduced and focused on column operations, an important challenge related to out-ofdomain generalization for text-to-SQL parsing.","In this paper we introduced and focused on column operations, an important challenge related to out-of-domain generalization for Text-to-SQL parsing.","Modify,Grammar",Conclusion,Conclusion
23-ARR,"We also introduced a schema pruning component allowing us to scale schema expansion, and showed that when paired together, these two components can boost the performance of existing text-to-SQL parsers by a significant amount (up to 13.8% relative accuracy gain / 5.1% absolute in our experiments).","We also introduced a schema pruning component allowing us to scale schema expansion, and showed that when paired together, these two components can boost the performance of arbitrary underlying Text-to-SQL parsers by a significant amount (up to 13.8% relative accuracy gain / 5.1% absolute in our experiments).","Modify,Clarity",Conclusion,Conclusion
23-ARR,We argue that existing benchmarks fail to capture a certain out-of-domain generalization problem that is of significant practical importance: matching domain specific phrases to composite operations over columns.,We argue that existing benchmarks fail to capture a certain out-ofdomain generalization problem that is of significant practical importance: matching domain specific phrases to composite operation over columns.,"Modify,Grammar",Abstract,Abstract
23-ARR,We hope this work could motivate future research on creating a parserfriendly table ontology.,We hope that this work puts attention on this important challenge and provides a reference point for future work to build upon.,"Modify,Claim",Conclusion,Conclusion
23-ARR,"Future work could explore learning approaches that use models to automatically expand any table schema, for example, by showing appropriate prompts to ask pre-trained language models to tackle it (Brown et al., 2020;Petroni et al., 2019).","Possible directions include making the schema expansion component learnable or prompting models like GPT-3 (Brown et al., 2020) to address it.","Modify,Claim",Conclusion,Conclusion
23-ARR,"We then show that existing neural parsers underperform on both benchmarks because they require an impractically large amount of in-domain training datawhich is not available in our setting-to effectively ""memorize"" mappings from natural language phrases to program fragments.","We then show that existing neural parsers underperform on both benchmarks, because they require an impractically large amount of in-domain training datawhich is not available in our setting-to effectively ""memorize"" mappings from natural language phrases to program fragments.","Modify,Grammar",Introduction,Introduction
23-ARR,"To study this problem, we propose a synthetic dataset and a re-purposed train/test split of the SQUALL dataset (Shi et al., 2020) as new benchmarks to quantify domain generalization over column operations.","To study this problem, we first propose a synthetic dataset along with a repurposed train/test split of the SQUALL dataset (Shi et al., 2020) as new benchmarks to quantify domain generalization over column operations, and find existing state-of-the-art parsers struggle in these benchmarks.","Split+Modify,Clarity",Abstract,Abstract
23-ARR,Our results indicate that existing state-of-the-art parsers struggle in these benchmarks.,"To study this problem, we first propose a synthetic dataset along with a repurposed train/test split of the SQUALL dataset (Shi et al., 2020) as new benchmarks to quantify domain generalization over column operations, and find existing state-of-the-art parsers struggle in these benchmarks.","Split+Modify,Clarity",Abstract,Abstract
23-ARR,"Relying on generic types makes this method applicable to new domains, as long as they make use of similar underlying types.","Relying on generic types enables this method to apply to new domains, as long as they make use of similar underlying types.","Modify,Clarity",Introduction,Introduction
23-ARR,"While schema expansion may result in a large number of unnecessary expanded columns, schema pruning then examines both the input question and the available columns (original and expanded) and prunes the set of columns that the final parser is exposed to.","While schema expansion may result in a large number of unnecessary expanded columns, schema pruning then prunes the set of relevant columns that final parser is allowed to look at, based on both the questions and the expanded schemas.","Modify,Fact/Evidence",Introduction,Introduction
23-ARR,"We propose to address this problem by incorporating prior domain knowledge by preprocessing table schemas, and design a method that consists of two components: schema expansion and schema pruning.","We then propose to address this problem by incorporating prior domain knowledge through preprocessing table schemas, and design a method that consists of two components: schema expansion and schema pruning, which can be easily applied to different base parsers.","Link+Modify,Grammar",Abstract,Abstract
23-ARR,"This method can be easily applied to multiple existing base parsers, and we show that it significantly outperforms baseline parsers on this domain generalization problem, boosting the underlying parsers' overall performance by up to 13.8% relative accuracy gain (5.1% absolute) on the new SQUALL data split.","We then propose to address this problem by incorporating prior domain knowledge through preprocessing table schemas, and design a method that consists of two components: schema expansion and schema pruning, which can be easily applied to different base parsers.","Link+Modify,Clarity",Abstract,Abstract
25-ARR,,"This is despite evidence of adjectival scale labels being problematic in terms of bias resulting from positively and negatively worded items not being true opposites of one another, and items intended to have neutral intensity in fact proving to have specific conceptual meanings.","Delete,Claim",,Human Ratings of Dialogue Quality
25-ARR,,Comparison with Automatic Evaluation Metrics,"Delete,Other",,Achieving Reliable Human Assessment of Open-Domain Dialogue Systems
25-ARR,,As can be seen from Tables Tables 6 unfortu-9 Raw average scores for models in the Ice-breaker run are additionally provided in Table 10 in Appendix A.4.,"Delete,Fact/Evidence",,Comparison with Automatic Evaluation Metrics
25-ARR,Models are consistent with Table 3.,,"Add,Fact/Evidence",System-level Consistency,
25-ARR,Evaluating with Prescribed Topics,,"Add,Other",Achieving Reliable Human Assessment of Open-Domain Dialogue Systems,
25-ARR,Raw average scores for models in the Ice-breaker run are additionally provided in Table 11 in Appendix A.4.,,"Add,Fact/Evidence",Evaluating with Prescribed Topics,
25-ARR,It also uses the brevity penalty to penalize short outputs.,,"Add,Fact/Evidence",Evaluating with Prescribed Topics,
25-ARR,"It computes the precision and recall using longest common subsequence (LSC) instead of n-gram, and the F1 score of precision and recall is reported as the final score.",,"Add,Fact/Evidence",Evaluating with Prescribed Topics,
25-ARR,"It computes the unigram precision and recall, and have a different mechanism of choosing the brevity penalty.",,"Add,Fact/Evidence",Evaluating with Prescribed Topics,
25-ARR,The minimum of precision and recall is reported as the final GLEU score.,,"Add,Fact/Evidence",Evaluating with Prescribed Topics,
25-ARR,Reference-free Metrics,,"Add,Other",Achieving Reliable Human Assessment of Open-Domain Dialogue Systems,
25-ARR,The following introduces two reference-free automatic metrics we employed: FED and USR.,,"Add,Fact/Evidence",Reference-free Metrics,
25-ARR,Their scores are computed using the conversations collected in our experiment.,,"Add,Fact/Evidence",Reference-free Metrics,
25-ARR,"It consists of three sub-metrics: USR-MLM is to evaluate the understandability and naturalness, USR-DR(c) and USR-DR(f) are to evaluate the interestingness and consistency.",,"Add,Fact/Evidence",Reference-free Metrics,
25-ARR,The sub-metric scores then produce an overall score through a regression model.,,"Add,Fact/Evidence",Reference-free Metrics,
25-ARR,Correlation between Automatic Metrics and Human Evaluation,,"Add,Other",Achieving Reliable Human Assessment of Open-Domain Dialogue Systems,
25-ARR,"Table 2 shows subsequent proportions (%) of workers, and the detailed instructions are introduced in Figure 5 in Appendix A.4.",Table 2 shows subsequent proportions (%) of workers.,"Modify,Fact/Evidence",Meta-Evaluation,Meta-Evaluation
25-ARR,"We compute the correlation between commonly applied automatic metrics and our human evaluation methods, including word-overlap-based metrics and reference-free metrics, as shown in Tables 5 and 6 respectively.","In this section, we compute the correlation between commonly applied automatic metrics and our human evaluation methods, including word-overlapbased metrics and reference-free metrics, as shown in Tables 6 and 5 respectively.","Modify,Clarity",Correlation between Automatic Metrics and Human Evaluation,Comparison with Automatic Evaluation Metrics
25-ARR,"In terms of the live evaluation, competitions such as ConvAI2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.","In terms of the live evaluation, competitions such as Convai2 report such evaluations as highly challenging, with many of the resulting dialogues reported to be senseless, offensive, or simply not in line with instructions and ultimately live evaluation results have been discarded.","Modify,Grammar",Problems in Past Evaluations,Problems in Past Evaluations
25-ARR,"A continuous (0-100) rating scale is employed with three main motivation points (Graham et al., 2013;Novikova et al., 2018;Li et al., 2019;Santhanam and Shaikh, 2019;Barrault et al., 2020;Howcroft et al., 2020).","A continuous (0-100) rating scale is employed with three main motivation points (Graham et al., 2013;Mille et al., 2020;Barrault et al., 2020).","Modify,Fact/Evidence",Human Ratings of Dialogue Quality,Human Ratings of Dialogue Quality
30-ARR,"Then, all model-identified rationales (underlined texts in Figure 2) are examined by human annotators to identify false rationales (i.e. words or phrases that do not support the classifications but are falsely included by the model) and missing rationales (i.e. words or phrases that support the classifications but are not included by the model).",,"Add,Fact/Evidence",Method,
30-ARR,Both false rationales and missing rationales are corrected to produce augmented examples.,,"Add,Fact/Evidence",Method,
30-ARR,"Finally, newly generated examples are added into the training set to re-train the deep learning model.",,"Add,Fact/Evidence",Method,
30-ARR,,"In particular, models trained with RDL with only 50 labelled examples achieve the same or even better results than that of fullysupervised training with a full training set of 1,707 examples, and improvements are especially significant for OOD tests.","Delete,Fact/Evidence",,Introduction
30-ARR,,"The predictive model trained with RDL using only 100 labelled examples outperforms models trained with manual (Kaushik et al., 2020) and automatic CAD using the full augmented training set of 3,414 examples.","Delete,Fact/Evidence",,Introduction
30-ARR,"To address RQ1, we compare the performance of models trained by the static semi-factual generation strategy with models trained with the original 50 examples, referred to as Static.",,"Add,Fact/Evidence",Evaluating Static Semi-factual Generation,
30-ARR,"We also compare to a model trained with the full training set (1,707 labelled examples), referred to as Full.",,"Add,Fact/Evidence",Evaluating Static Semi-factual Generation,
30-ARR,,"To the best of our knowledge, we are the first to exploit the efficacy of semi-factuals and humanintervention for improving the generalisation abilities of deep neural networks in few-shot learning scenarios.","Delete,Claim",,Introduction
30-ARR,,All resources will be released on Github.,"Delete,Fact/Evidence",,Introduction
30-ARR,,Related Work,"Delete,Other",,A Rationale-Centric Framework for Human-in-the-loop Machine Learning
30-ARR,,"Data augmentation has been used for resolving artefacts in training datasets before (Gururangan et al., 2018;Srivastava et al., 2020;Kaushik et al., 2021).","Delete,Fact/Evidence",,Related Work
30-ARR,"In the future, we expect to see rationale-centric frameworks defined for different tasks, including NER, question answering, and relation extraction.",,"Add,Claim",Conclusion,
30-ARR,Ethical Statement,,"Add,Other",A Rationale-Centric Framework for Human-in-the-loop Machine Learning,
30-ARR,We honor the ACL Code of Ethics.,,"Add,Claim",Ethical Statement,
30-ARR,No private data or non-public information was used in this work.,,"Add,Fact/Evidence",Ethical Statement,
30-ARR,All annotators have received labor fees corresponding to the amount of their annotated instances.,,"Add,Fact/Evidence",Ethical Statement,
30-ARR,"Only adjectives, adverbs, nouns, and verbs were considered as rationales.","Only adjectives, adverbs, nouns, and verbs were considered.","Modify,Clarity",Rationale Marking,Rationale Marking
30-ARR,"We also limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams).","We limited rationales to at most three consecutive words (i.e. unigrams, bigrams and trigrams).","Modify,Clarity",Rationale Marking,Rationale Marking
30-ARR,"Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020;Srivastava et al., 2020) in datasets can cause sub-optimal model performance for neural networks.","Recent work finds that natural artefacts (Gururangan et al., 2018) or spurious patterns (Keith et al., 2020;Srivastava et al., 2020) in datasets cause sub-optimal model performance for neural networks.","Modify,Clarity",Introduction,Introduction
30-ARR,All re-annotated examples were approved only if all authors were happy with the quality of the annotations.,All re-annotated examples were approved only if all authors were happy with the quality of annotations.,"Modify,Grammar",Rationale Marking,Rationale Marking
30-ARR,"Our annotation procedure generated 5,073 rationales in 855 movie reviews involved in Section 3.1 and 3.3 (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).","Our annotation procedure generated 5,073 rationales in 855 movie reviews (note that we did not annotate all 1,707 examples in the training set because only 855 examples were necessarily involved in our experiments).","Modify,Fact/Evidence",Rationale Marking,Rationale Marking
30-ARR,"Note that our approach using 100 labelled examples can outperform manual CAD (Kaushik et al., 2020) using the entire training set of 1,707 examples (see Section 5.3), making our approach 300×1707 183.68×100 ≈ 27.88 times more efficient than manually generated CAD.","Note that our approach using 100 labelled examples can outperform the manual CAD (Kaushik et al., 2020) using the entire training set with 1,707 examples (see Section 5.3), making our approach 300×1707 183.68×100 ≈ 27.88 times more efficient than manually generated CAD.","Modify,Grammar",Rationale Marking,Rationale Marking
30-ARR,"As shown in Figure 1, the bold phrases-""100% bad"" and ""brain cell killing""-are underlying causes for a negative sentiment prediction that most human readers would recognise.","As shown in Figure 1, the phrases in bold-""100% bad"" and ""brain cell killing""-are underlying causes for a negative prediction most human readers would recognise.","Modify,Clarity",Introduction,Introduction
30-ARR,"To generate a semi-factual example, x ′ i , we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms.","To generate a semi-factual example, x i ', we randomly replace a certain number of non-rationales (where a ij = 0), except for punctuation, with synonymous terms.","Modify,Grammar",Static Semi-factual Generation,Static Semi-factual Generation
30-ARR,"In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x ′ i , with some replaced non-rationales and all the other tokens identical to x i .","In our experiments, we randomly replace 5% of non-rationales using mask-filling and generate a set of augmented examples, x i ', with some replaced non-rationales and all the other tokens identical to x i .","Modify,Grammar",Static Semi-factual Generation,Static Semi-factual Generation
30-ARR,We present a novel rationale-centric framework with human-in-the-loop -Rationales-centric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios.,We present a novel rational-centric framework with human-in-the-loop -Rationales-centric Double-robustness Learning (RDL) -to boost model out-of-distribution performance in few-shot learning scenarios.,"Modify,Grammar",Abstract,Abstract
30-ARR,"The underlined phrase-""acting and plot""has been incorrectly recognised as a causal term by the model used fort this example, and is referred to as a spurious pattern.","The underlined phrase-""acting and plot""-are incorrectly recognised as causal terms by the model, and are referred to as spurious patterns.","Modify,Clarity",Introduction,Introduction
30-ARR,"Spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021), and are usually useless, or even harmful, at test time.","These spurious patterns (or associations) are caused by natural artefacts or biases in training data (Lertvittayakumjorn and Toni, 2021) and are usually useless, or even harmful, at test time.","Modify,Clarity",Introduction,Introduction
30-ARR,Note that the two correction methods in dynamic human-intervened correction can operate in parallel and the generated examples are added to the small training set to re-train the model.,Note that the two correction methods in dynamic human-intervened correction can be operated in parallel and the generated examples are added to the small training set to re-train the model.,"Modify,Clarity",Dynamic Human-intervened Correction,Dynamic Human-intervened Correction
30-ARR,"Broadly speaking, our RDL framework takes advantage of invariance that makes a model less sensitive to non-rationale words or spurious patterns (Tu et al., 2020; in favour of focusing on useful mappings of rationales to labels.","Broadly speaking, our RDL framework acts like a sensible inductive bias (Wu et al., 2021b;Warstadt and Bowman, 2020) that restricts a model from superficially focusing on whole texts or spurious patterns (Tu et al., 2020; in favour of focusing on useful mappings of rationales to labels.","Modify,Fact/Evidence",Why Does RDL Work?,Why Does RDL Work?
30-ARR,"Following Kaushik et al. (2020), we fine-tune RoBERTa for up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).","Following Kaushik et al. (2020), we fine-tune RoBERTa up to 20 epochs and apply early stopping with patience of 5 (i.e. stop fine-tuning when validation loss does not decrease for 5 epochs).","Modify,Grammar",Experiment Setup,Experiment Setup
30-ARR,"We found that setting the learning rate to {5e-5, 5e-6 and 5e-6} could optimise Static, Static+n, and Full, respectively.","We found that setting the learning rate to 5e-5, 5e-6 and 5e-6 could optimise Static, Static+n, and Full, respectively.","Modify,Grammar",Experiment Setup,Experiment Setup
30-ARR,"Among all Static+n methods, Static+350 seems the best-performing method and exceeds Static with a 1.56% in-distribution improvement in average accuracy.","Among all Static+n methods, Static+350 seems the best-performing method that exceeds Static with a 1.56% in-distribution improvement in average accuracy.","Modify,Clarity",Results and Analysis,Results and Analysis
30-ARR,"Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, and 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets respectively.","Static+350 also outperforms Static with 3.26%, 1.97%, 1.5%, 0.46% OOD improvement in the SemEval-2017, SST-2, Yelp and Amazon datasets, respectively.","Modify,Clarity",Results and Analysis,Results and Analysis
30-ARR,"The Static+n methods can even outperform Full (i.e. normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset.","The Static+n methods can even outperform Full (i,e, the normal training with the full training set) on the SemEval, SST-2, and Amazon datasets and are comparable on the Yelp dataset.","Modify,Grammar",Results and Analysis,Results and Analysis
30-ARR,"It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp dataset (93.66±0.84) than on the in-distribution dataset (93.23±0.46), which could also be explained by the high similarity between the underlying distributions of these two datasets.","It is interesting to note that models trained with the entire training set perform slightly better on the OOD Yelp (93.66±0.84) than on the in-distribution dataset (93.23±0.46), which could also be explained by the high similarity between the underlying distributions of these two test sets.","Modify,Clarity",Results and Analysis,Results and Analysis
30-ARR,"We believe that the duplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.","We believe that the multiplication of original examples increases the risk of overfitting and easily magnifies artefacts or spurious patterns hidden in the small training set, which leads to worse models.","Modify,Claim",Results and Analysis,Results and Analysis
30-ARR,"In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu and MacNamee, 2020;Lu et al., 2021).","In spite of these issues, training deep neural networks using few labelled examples is a compelling scenario since unlabelled data may be abundant but labelled data is expensive to obtain in real-world applications (Lu et al., 2021).","Modify,Fact/Evidence",Introduction,Introduction
30-ARR,"As shown in Table 3, RR is slightly better than the baseline Static approach.","As shown in Table 3, RR is slightly better than the baseline Static.","Modify,Clarity",Results and Analysis,Results and Analysis
30-ARR,"This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to the original data, introducing noise that helps prevent overfitting to some extent.","This result is similar to that reported in Wei and Zou (2019), since the augmented data generated by random replacement is similar to original data, introducing noise that helps prevent overfitting to some extent.","Modify,Grammar",Results and Analysis,Results and Analysis
30-ARR,"However, the magnitude of improvement of the Static+n method is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.","However, the magnitude of improvement of Static+n methods is much larger than that of RR, demonstrating the utility of only replacing non-rationales to generate semi-factuals.","Modify,Grammar",Results and Analysis,Results and Analysis
30-ARR,"To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter).","To investigate the influence of each correction method, we also construct another two datasets that augment the same 100 original examples to 800 exclusively by False Rationale Correction (Dynamic-FR hereafter) and Missing Rationale Correction (Dynamic-MR hereafter), respectively.","Modify,Clarity",Experiment Setup,Experiment Setup
30-ARR,"Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, but no improvement for the SemEval dataset.","Dynamic also outperforms Static+350 with 1.14%, 0.16%, 0.42% OOD improvement in the SST-2, Yelp and Amazon datasets, except for the SemEval dataset.","Modify,Clarity",Results and Analysis,Results and Analysis
30-ARR,"Finally, the performance of our methods is better that the state-of-the-art manual CAD method in few-shot learning scenarios on all OOD datasets.","Finally, the performance of our methods outperforms another state-of-the-art manual CAD method in fewshot learning scenarios on all OOD datasets.","Modify,Clarity",Results and Analysis,Results and Analysis
30-ARR,"As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increase in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.","As shown in Table 5, the corrected model is much more sensitive to rationales with 13.9% average increasing in the sensitivity to rationales, which demonstrates that our double-robust method can decouple models from spurious patterns.","Modify,Grammar",Results and Analysis,Results and Analysis
30-ARR,"By using static semi-factual generation and dynamic humanintervened correction, RDL exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.","By using static semi-factual generation and dynamic human-intervened correction, RDL, acting like a sensible ""inductive bias"", exploits rationales (i.e. phrases that cause the prediction), human interventions and semi-factual augmentations to decouple spurious associations and bias models towards generally applicable underlying distributions, which enables fast and accurate generalisation.","Modify,Fact/Evidence",Abstract,Abstract
30-ARR,"In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002;Kenny and Keane, 2021), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same).","In contrast with counterfactuals (Roese, 1997) that rely on what might have been different (i.e. the label would be changed if certain terms have been changed), semi-factuals (McCloy and Byrne, 2002), as used in our work, aim to guide a model to identify terms less causally related to the label (i.e. even if certain terms had been changed, the label would be kept the same).","Modify,Fact/Evidence",Introduction,Introduction
30-ARR,"We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power, both for in-distribution and OOD predictions.","We evaluate the two modules in a few-shot setting, where a minimum number of training instances are labeled for maximum generalisation power both for in-distribution and OOD predictions.","Modify,Grammar",Introduction,Introduction
30-ARR,Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests compared to many state-of-the-art benchmarks-especially for few-shot learning scenarios.,"Experimental results show that RDL leads to significant prediction benefits on both in-distribution and out-of-distribution tests, especially for few-shot learning scenarios, compared to many state-of-the-art benchmarks.","Modify,Clarity",Abstract,Abstract
30-ARR,"Human-the-loop Machine Learning (Wu et al., 2021) has received increasing research attention.","Human-the-loop Machine Learning (Wu et al., 2021a) has received increasing research attention.","Modify,Fact/Evidence",Introduction,Related Work
30-ARR,"Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights.","Very recently, Yao et al. (2021) proposed a human-in-the-loop method to inspect more complicated models (e.g. BERT) with the help of some model-agnostic post-hoc explanation algorithms (Ribeiro et al., 2018) that can explain predictions of any linear or non-linear model without exploiting its weights.","Modify,Clarity",Introduction,Related Work
30-ARR,"However, previous work focuses on increasing the explainability of AI systems for high-stakes domains such as health and finance (Li et al., 2020;Yang et al., 2020b), instead of improving model robustness or generalisation ability.","However, previous work focuses on increasing the explainability of AI systems for high-stake domains such as health and finance, instead of improving the model robustness or generalisation ability.","Modify,Fact/Evidence",Introduction,Related Work
334-ARR,,"This provides interesting future direction, where legal knowledge is incorporated into the prediction model.","Delete,Claim",,Error Analysis
334-ARR,We also performed quantitative analysis on the model output to better understand the performance.,,"Add,Fact/Evidence",Error Analysis,
334-ARR,"Our model outputs a probabilistic score in the range {0, 1}.",,"Add,Fact/Evidence",Error Analysis,
334-ARR,"A score closer to 0 indicates our model is confident that bail would be denied, while a score closer to 1 means bail granted.",,"Add,Fact/Evidence",Error Analysis,
334-ARR,"We observe the correct bail granted predictions are shifted towards 1, and the correct bail denied predictions are shifted towards 0.",,"Add,Fact/Evidence",Error Analysis,
334-ARR,"Additionally, the incorrect samples are concentrated near the middle (≈ 0.5), which shows that our model was able to identify these as borderline cases.",,"Add,Fact/Evidence",Error Analysis,
334-ARR,Hindi Legal Documents Corpus,Hindi Legal Document Corpus,"Modify,Grammar",HLDC: Hindi Legal Documents Corpus,HLDC: Hindi Legal Documents Corpus
334-ARR,"Hindi Legal Documents Corpus (HLDC) is a corpus of 912,568 Indian legal case documents in the Hindi language.",Hindi Legal Document Corpus (HLDC) is a corpus of about 900K Indian legal case documents in the Hindi language.,"Modify,Fact/Evidence",Hindi Legal Documents Corpus,Hindi Legal Document Corpus
334-ARR,The corpus is created by downloading data from the e-Courts website (a publicly available website: https:// districts.ecourts.gov.in/).,The corpus is created by scraping data from the e-Courts website (a publicly available website: https:// districts.ecourts.gov.in/).,"Modify,Clarity",Hindi Legal Documents Corpus,Hindi Legal Document Corpus
334-ARR,We download case documents pertaining to the district courts located in the Indian northern state of Uttar Pradesh (U.P.).,We scrape case documents pertaining to the district courts located in the Indian northern state of Uttar Pradesh (U.P.).,"Modify,Clarity",Hindi Legal Documents Corpus,Hindi Legal Document Corpus
334-ARR,Experiments with different models are indicative of the need for further research in this area.,Results on different models are indicative of the need for further research in this area.,"Modify,Clarity",Abstract,Abstract
334-ARR,The first step in HLDC creation is the downloading of documents from the e-Courts website.,The first step in HLDC creation is the scraping of documents from the e-Courts website.,"Modify,Clarity",Hindi Legal Documents Corpus,Hindi Legal Document Corpus
334-ARR,Many populous countries including India are burdened with a considerable backlog of legal cases.,"Populous countries (e.g., India) are burdened with a considerable backlog of legal cases.","Modify,Clarity",Abstract,Abstract
334-ARR,"The header contains the meta-information related to the case, for example, case number, court identifier, and applicable sections of the law.","The header contains the meta-information related to the case, for example, case number, court identifier, applicable sections of the law, etc.","Modify,Clarity",Hindi Legal Documents Corpus,Hindi Legal Document Corpus
334-ARR,"For example, if a system could readily extract the required information from a legal document for a legal practitioner, then it would help expedite the legal process.","For example, if a system could readily extract the required information from a legal document for a legal practitioner, then it would help them expedite the legal process.","Modify,Clarity",Introduction,Introduction
334-ARR,"Body is further segmented into Facts and Arguments, Judge's summary and Case Result.","The body is further segmented into Facts and Arguments, Judge's summary and Case Result.","Modify,Grammar",Hindi Legal Documents Corpus,Hindi Legal Document Corpus
334-ARR,"For example, legal documents are typically quite long (tens of pages), highly unstructured and noisy (spelling and grammar mistakes since these are typed), use domainspecific language and jargon; consequently, pre-trained language models do not perform well on these (Malik et al., 2021b).","For example, legal documents are typically quite long (tens of pages), legal documents are highly unstructured and noisy (spelling and grammar mistakes, since these are typed), language in legal documents are domain-specific, and pre-trained language models do not perform well on these (Malik et al., 2021).","Modify,Fact/Evidence",Introduction,Introduction
334-ARR,"As observed in previous work (Malik et al., 2021b), anonymization of a judge's name is important as there is a correlation between a case outcome and a judge name.","As observed in previous work (Malik et al., 2021), anonymization of a judge's name is important as there is a correlation between a case outcome and a judge name.","Modify,Fact/Evidence",HLDC: Ethical Aspects,HLDC: Ethical Aspects
334-ARR,"Moreover, the Bail corpus and corresponding bail prediction systems can promote the development of explainable systems (Malik et al., 2021b), we leave research on such explainable systems for future work.","Moreover, the Bail corpus and corresponding bail prediction systems can promote the development of explainable systems (Malik et al., 2021), we leave research on such explainable systems for future work.","Modify,Fact/Evidence",HLDC: Ethical Aspects,HLDC: Ethical Aspects
334-ARR,"Thus, to develop legal text processing systems and address the challenges associated with the legal domain, there is a need for creating specialized legal domain corpora.","To develop legal text processing systems and address the challenges associated with the legal domain, there is a need for creating specialized legal domain corpora.","Modify,Clarity",Introduction,Introduction
334-ARR,Development of automated systems that could process legal documents and augment legal practitioners can mitigate this.,This calls for the development of automated systems that could process legal documents and augment legal practitioners.,"Modify,Claim",Abstract,Abstract
334-ARR,"In recent times, there have been efforts to develop such corpora.","In recent times, there have been efforts to develop such corpora for example, Chalkidis et al. (2019) have developed an English corpus of European Court of Justice documents, Malik et al. (2021) have developed an English corpus of Indian Supreme Court documents, Xiao et al. (2018) have developed Chinese Legal Document corpus.",Split+Identical,Introduction,Introduction
334-ARR,"For example, Chalkidis et al. (2019) have developed an English corpus of European Court of Justice documents, while Malik et al. (2021b) have developed an English corpus of Indian Supreme Court documents.","In recent times, there have been efforts to develop such corpora for example, Chalkidis et al. (2019) have developed an English corpus of European Court of Justice documents, Malik et al. (2021) have developed an English corpus of Indian Supreme Court documents, Xiao et al. (2018) have developed Chinese Legal Document corpus.","Split+Modify,Fact/Evidence",Introduction,Introduction
334-ARR,Xiao et al. (2018) have developed Chinese Legal Document corpus.,"In recent times, there have been efforts to develop such corpora for example, Chalkidis et al. (2019) have developed an English corpus of European Court of Justice documents, Malik et al. (2021) have developed an English corpus of Indian Supreme Court documents, Xiao et al. (2018) have developed Chinese Legal Document corpus.",Split+Identical,Introduction,Introduction
334-ARR,Testing is done on a different set of 17 districts not present in train set.,Testing is done on a different set of 17 districts not present during training.,"Modify,Clarity",Dataset Splits,Dataset Splits
334-ARR,"Another thing to note from the results is that, in general, summarization based models perform better than Doc2Vec and transformer-based models, highlighting the importance of the summarization step in the bail prediction task.","Another thing to note from the results is that, in general, summarization models perform better than Doc2Vec and transformer-based models, highlighting the importance of the summarization step in the bail prediction task.","Modify,Clarity",Results,Results
334-ARR,"Hindi uses Devanagari script (Wikipedia contributors, 2021) for the writing system.","Hindi uses Devanagri (Wikipedia contributors, 2021) script for the writing system.","Modify,Clarity",Introduction,Introduction
334-ARR,"After examining the miss-classified examples, we observed the following.",We observe a couple of things looking at the misclassified examples.,"Modify,Clarity",Error Analysis,Error Analysis
334-ARR,"In some instances, we also observed that even if the facts of the cases are similar the judgements can differ.","In some instances, we also observe that even if the facts of the cases are similar the judgements can differ.","Modify,Grammar",Error Analysis,Error Analysis
334-ARR,Most of the lower (district) courts in northern India use Hindi as the official language.,Most of the lower (district) courts in Northern India use Hindi as the official language.,"Modify,Grammar",Introduction,Introduction
334-ARR,"However, most of the legal NLP systems that currently exist in India have been developed on English, and these do not work on Hindi legal documents (Malik et al., 2021b).","However, most of the legal NLP systems that currently exist in India have been developed on English, and these do not work on Hindi legal documents (Malik et al., 2021).","Modify,Fact/Evidence",Introduction,Introduction
334-ARR,"To address this problem, in this paper, we release a large corpus of Hindi legal documents (HINDI LEGAL DOCUMENTS CORPUS or HLDC) that can be used for developing NLP systems that could augment the legal practitioners by automating some of the legal processes.","To address this problem, in this paper, we release a large corpus of Hindi legal documents (Hindi Legal Document Corpus) that can be used for developing NLP systems that could augment the legal practitioners by automating some of the legal processes.","Modify,Clarity",Introduction,Introduction
334-ARR,"Finally, all sentences labelled as judge's opinion either during reverse iteration or during paragraph level extension are extracted out as judge's summary and rest of the sentences form facts and opinions for further modelling.","Finally, all labelled as judge's opinion either during reverse iteration or during paragraph level extension are extracted out as judge's summary and rest of the sentences form facts and opinions for further modelling.","Modify,Clarity",Future Work and Conclusion,Future Work and Conclusion
334-ARR,"However, there is a dearth of high-quality corpora that is needed to develop such data-driven systems.","To develop such data-driven systems, there is a dearth of high-quality corpora.","Modify,Clarity",Abstract,Abstract
334-ARR,"India follows a Common Law system and has a three-tiered court system with District Courts (along with Subordinate Courts) at the lowest level (districts), followed by High Courts at the state level, and the Supreme Court of India at the highest level.","India follows a Common Law system and has a three-tiered court system with District Courts (along with Subordinate Courts) at the lowest levels of districts, followed by High Courts at the state level and the Supreme Court at the highest level.","Modify,Clarity",Introduction,Introduction
334-ARR,"In terms of number of cases, district courts handle the majority.","In terms of the number of cases, district courts handle the majority of the cases.","Modify,Clarity",Introduction,Introduction
334-ARR,"According to India's National Judicial Data Grid, as of November 2021, there are approximately 40 million cases pending in District Courts (National Judicial Data Grid, 2021) as opposed to 5 million cases pending in High Courts.","According to India's National Judicial Data Grid, as of November 2021, there are approximately 40 million cases pending in District courts (National Judicial Data Grid, 2021) as opposed to 5 million cases pending in High Courts.","Modify,Grammar",Introduction,Introduction
334-ARR,These statistics show an immediate need for developing models that could address the problems at the grass-root levels of the Indian legal system.,These statistics show an immediate need for developing systems that could address the problems at the grass-root levels of the Indian legal system.,"Modify,Clarity",Introduction,Introduction
334-ARR,The problem gets even more pronounced in the case of low resource languages such as Hindi.,"The problem gets even more pronounced in the case of low resource language (e.g., Hindi).","Modify,Clarity",Abstract,Abstract
334-ARR,"Out of the 40 million pending cases, approximately 20 million are from courts where the official language is Hindi (National Judicial Data Grid, 2021).","Out of 40 million pending cases, approximately 20 million cases are from courts where the official language is Hindi (National Judicial Data Grid, 2021).","Modify,Grammar",Introduction,Introduction
334-ARR,"In this resource paper, we create a large corpus of 912,568 Hindi legal documents.","In this resource paper, we create a large corpus of about 900K Hindi legal documents.","Modify,Fact/Evidence",Introduction,Introduction
334-ARR,"In particular, we collect documents from the state of Uttar Pradesh, the most populous state of India with a population of approximately 237 million (PopulationU, 2021).","In particular, we collect documents from the state of Uttar Pradesh (U.P.), the most populous state of India with a population of approximately 237 million (Popula-tionU, 2021).","Modify,Clarity",Introduction,Introduction
334-ARR,"The Hindi Legal Documents Corpus (HLDC) can be used for a number of legal applications, and as a use case, in this paper, we propose the task of Bail Prediction.","The Hindi Legal Document Corpus (HLDC) can be used for a number of legal applications, and in this paper, we propose the task of Bail Prediction.","Modify,Clarity",Introduction,Introduction
334-ARR,"In this resource paper, we introduce the Hindi Legal Documents Corpus (HLDC), a corpus of more than 900K legal documents in Hindi.","In this resource paper, we introduce the Hindi Legal Documents Corpus (HLDC), a corpus of 900K legal documents in Hindi.","Modify,Fact/Evidence",Abstract,Abstract
334-ARR,Documents are cleaned and structured to enable the development of downstream applications.,The documents are cleaned and structured to enable the development of downstream applications.,"Modify,Grammar",Abstract,Abstract
334-ARR,"Majority of the work in the legal domain has focused on the higher court (Malik et al., 2021b;Strickson and De La Iglesia, 2020;Zhong et al., 2020b); however, the lower courts handle the maximum number of cases.","Majority of the work in the legal domain has focused on the higher court (Malik et al., 2021;Strickson and De La Iglesia, 2020;Zhong et al., 2020b); however, the lower courts handle the maximum number of cases.","Modify,Fact/Evidence",Related Work,Related Work
334-ARR,"Further, as a use-case for the corpus, we introduce the task of bail prediction.","Further, as a usecase for the corpus, we introduce the task of Bail Prediction.","Modify,Grammar",Abstract,Abstract
334-ARR,"The competition has two sub-tasks, a legal information retrieval task and an entailment identification task between law articles and queries.","The competition had two sub-tasks, a legal information retrieval task and an entailment identification task between law articles and queries.","Modify,Grammar",Related Work,Related Work
39-ARR,"The rest of this paper is organized as follows: section 2 provides an thorough description of our proposed model, section 3 presents and analyses the results from our experiments, section 4 provides a brief review of related work, and section 5 includes our conclusions.",,"Add,Fact/Evidence",Introduction,
39-ARR,"Of course, this can only be done for samples in the source language as the labels for the target-language data are unavailable:",,"Add,Fact/Evidence",Problem Formulation,
39-ARR,Hyper-parameters,,"Add,Other",Experiments,
39-ARR,We fine-tune the hyper-parameters for our OA-CLED model using the development data.,,"Add,Fact/Evidence",Hyper-parameters,
39-ARR,We apply the following values based on the fine-tuning process:,,"Add,Fact/Evidence",Hyper-parameters,
39-ARR,"• AdamW as the optimizer. • 5 warm up epochs. • A learning rate of 1e −5 for the transformer parameters and of 1e −4 for the rest of the parameters. • A batch size of 16. • 300 for the dimensionality of the layers in feed-forwards networks. • A γ = 0.5 for the percentage of samples used in adversarial training. • A λ = 0.001 as the scaling factor of the GRL layer. • An α = 1 and β = 0.001 as the trade-off parameters of the LD loss and ED loss, respectively. • A dropout of 10% for added regularization during training.",,"Add,Fact/Evidence",Hyper-parameters,
39-ARR,"First, the previous state-of-the-art CLED model BERT-CRF (M'hamdi et al., 2019) as described in section 2.2.",,"Add,Fact/Evidence",Main Results,
39-ARR,"Second, the mBERT-2TA model (Majewska et al., 2021) which aims at improving cross-lingual performance by incorporating language-independent verb knowledge via task-specific adapters.",,"Add,Fact/Evidence",Main Results,
39-ARR,Figure 2 shows the results of our experiments when using different amounts of labeled target data during training.,,"Add,Fact/Evidence",Access to Labeled Target Data,
39-ARR,"We also note that, while this work focuses on the event detection task, our proposed optimization of the adversarial training process is task independent and can be generalized to other related IE tasks when leveraging ALA is deemed beneficial.",,"Add,Claim",Conclusion,
39-ARR,"Our goal is to define a model able to generate language-invariant word representations that are refined enough so that cross-lingual issues, such as the ones described in section 1, are properly handled.","Our goal is to define a model able to generate language-invariant word representations that are refined enough so that cross-lingual issues, such as the ones described previously, are properly handled.","Modify,Clarity",Problem Definition,Problem Definition
39-ARR,"Here, we briefly describe the BERT-CRF model proposed by M'hamdi et al. ( 2019) which was the previous state-of-the-art and serves as our main baseline.","Here we briefly describe the BERT-CRF model (M'hamdi et al., 2019) which was the previous state-of-the-art and serves as our baseline.","Modify,Clarity",Baseline Model,Baseline Model
39-ARR,"Using multilingual BERT (mBERT, (Devlin et al., 2019)) as its encoder, BERT-CRF generates robust, contextualized representations for words from different languages.","Using mBERT (Devlin et al., 2019) as its encoder, BERT-CRF generates robust, contextualized representations for words from different languages.","Modify,Clarity",Baseline Model,Baseline Model
39-ARR,"In summary, the contextualized representation vectors h i generated by the mBERT encoder from the words in the sequence are then fed to a CRF layer which finds the optimal label sequence.","As such, the representation vectors h i of the words in the sequence are fed to a CRF layer which finds the optimal label sequence.","Modify,Fact/Evidence",Baseline Model,Baseline Model
39-ARR,"Majewska et al. (2021), for instance, propose to address this issue by injecting external verb knowledge into the encoder via taskspecific adapter modules (Pfeiffer et al., 2020).","Majewska et al. (2021), for instance, propose to address this issue by injecting external verb knowledge into the encoder via adapter modules (Pfeiffer et al., 2020).","Modify,Fact/Evidence",Adversarial Language Adaptation,Adversarial Language Adaptation
39-ARR,"Thus, we propose leveraging Optimal Transport as a solution to naturally combine these two distinct information sources into the selection process.","Thus, we propose using Optimal Transport as a solution to naturally combine these two distinct information sources into the selection process.","Modify,Clarity",Abstract,Abstract
39-ARR,"Extensive experiments on 8 different language pairs, using 4 languages from unrelated families, show the flexibility and effectiveness of our model that achieves state-of-the-art results.","Extensive experiments on 8 different language pairs, using 4 languages from unrelated families, show the flexibility and effectiveness of our model that achieves new state-of-the-art results.","Modify,Clarity",Abstract,Abstract
39-ARR,One challenge of using the two mentioned criteria for the ALA sample selection process is that they come with two different measures which are hard to combine.,One challenge of using these two criteria for ALA sample selection process is that they come with two different measures which are hard to combine.,"Modify,Clarity",Optimal Transport,Optimal Transport
39-ARR,"To address this, we propose using Optimal Transport (OT) (Villani, 2008) as a natural way to combine these two metrics into a single framework for sample selection.","We propose using Optimal Transport (OT) (Villani, 2008) as a natural way to combine these two metrics into a single framework for sample selection.","Modify,Clarity",Optimal Transport,Optimal Transport
39-ARR,"Thus, the auxiliary dataset D aux is augmented to include an event-presence label e i for each sample.","Thus, the auxiliary dataset D aux is augmented to include an event-presence label e i for each sample, D aux = {(w 1 , l 1 , e 1 ), . . . , (w 2m , l 2m , e 2m )}, and the EP module is trained to optimize the following loss:","Split+Modify,Fact/Evidence",Problem Formulation,Problem Formulation
39-ARR,The EP module is then trained to optimize the following loss:,"Thus, the auxiliary dataset D aux is augmented to include an event-presence label e i for each sample, D aux = {(w 1 , l 1 , e 1 ), . . . , (w 2m , l 2m , e 2m )}, and the EP module is trained to optimize the following loss:","Split+Modify,Clarity",Problem Formulation,Problem Formulation
39-ARR,"To include an additional language in our experiments, we also evaluate on the ERE dataset which has annotated data in English and Spanish.","To include an additional language in our experiments, we also evaluate on the ERE version of ACE05 which has annotated data in English and Spanish.","Modify,Clarity",Datasets,Datasets
39-ARR,Note that the ACE05 and ERE datasets do not share the same label set: ACE05 involves 33 distinct event types while ERE involves 38 event types.,"The ACE05 and ACE05-ERE versions, however, do not share the same label set: ACE05 involves 33 distinct event types while ACE05-ERE involves 38 event types.","Modify,Clarity",Datasets,Datasets
39-ARR,"The Chinese-Spanish, Spanish-Chinese, Arabic-Spanish, and Spanish-Arabic language combinations are unavailable due the previously mentioned incompatibility between the event type sets in ACE05 and ERE.","The Chinese-Spanish, Spanish-Chinese, Arabic-Spanish, and Spanish-Arabic language combinations are unavailable due the previously mentioned incompatibility between the event type sets in ACE05 and ACE05-ERE.","Modify,Clarity",Main Results,Main Results
39-ARR,"Table 2 and Table 3 show the results of our experiments on the ACE05 and ERE datasets, respectively.","Tables 1 and 2 show the results of our experiments on the ACE05 and ACE05-ERE datasets, respectively.","Modify,Clarity",Main Results,Main Results
39-ARR,We compare our OACLED model against 3 relevant baselines.,We compare our OACLED model against 2 relevant baselines.,"Modify,Fact/Evidence",Main Results,Main Results
39-ARR,"And third, XLM-R-CRF which is equivalent in all regards to BERT-CRF except that it uses XLM-RoBERTa (Conneau et al., 2019) as the encoder.","BERT-CRF (M'hamdi et al., 2019), and XLM-R-CRF which is equivalent in all regards to BERT-CRF except that it uses XLM-RoBERTa as the encoder 2 .","Modify,Fact/Evidence",Main Results,Main Results
39-ARR,"In all our experiments, we use the base transformer versions bert-base-cased and xlm-robertabase as the encoders, parameters are tuned on the development data of the source language, and all entries are the average of five runs.","In our experiments, we use bertbase-cased and xlm-roberta-base for the encoders, parameters are tuned on the development data of the source language, and all entries are the average of five runs.","Modify,Clarity",Main Results,Main Results
39-ARR,"From Tables 2 and 3, it should be noted that there is a substantial performance increase by performing the trivial change of replacing mBERT with XLM-RoBERTa as the encoder.","From Tables 1 and 2, we can observe a substantial performance increase by performing the trivial change of replacing BERT with XLM-RoBERTa as the encoder.","Modify,Fact/Evidence",Main Results,Main Results
39-ARR,"Most importantly, OACLED's improvement over the XLM-R-CRF baseline is present in every configuration, which validates the effectiveness of our optimized approach to ALA training.","Most importantly, OA-CLED's improvement over the XLM-R-CRF baseline is present in every configuration, which confirms the effectiveness of our optimized approach to ALA training.","Modify,Clarity",Main Results,Main Results
39-ARR,"In order to understand the contribution of these aspects, we explore four different models: OACLED-OT presents the effects of removing sample selection entirely and using all available samples to train the LD; OACLED-L2 uses a constant distance between the unlabeled samples instead the standard L2 distance used in the Sinkhorn algorithm; OACLED-EP completely removes the EP module and a uniform distribution is used as the probability distributions for both languages; finally, OACLED-ED-Loss keeps the EP module, but removes its EP loss term from Equation 10.","In order to understand the contribution of these aspects, we explore four different models: OACLED-OT the effects of removing sample selection entirely and using all available samples to train the LD; OACLED-L2 uses a constant distance between the unlabeled samples instead the standard L2 distance used in the Sinkhorn algorithm; OACLED-EP completely removes the EP module and a uniform distribution is used as the probability distributions for both languages; finally, OACLED-ED-Loss keeps the EP module, but removes its EP loss term from Equation 10.","Modify,Clarity",Ablation Study,Ablation Study
39-ARR,"We observe that OACLED representations are closer, by several orders of magnitude, than those obtained by the baseline.","We observe that OACLED representations are closer, by several orders of magnitude, those obtained by the baseline.","Modify,Clarity",Learned Representation Distances,Learned Representation Distances
39-ARR,"Alternatively, Cross-Lingual ED (CLED) proposes the scenario of creating models that effectively perform ED on data belonging to more than one language, which brings about additional challenges.","Alternatively, Cross-Lingual ED (CLED) proposes the scenario of creating models that effectively perform ED on data belonging to more than one language, which entails additional challenges.","Modify,Clarity",Introduction,Introduction
39-ARR,"More recent efforts have primarily made use of deep learning techniques such as convolutional neural networks (Nguyen and Grishman, 2015;Chen et al., 2015;Nguyen et al., 2016b), recurrent neural networks (Nguyen et al., 2016a;Sha et al., 2018;Lai et al., 2020), graph convolutional networks (Nguyen and Grishman, 2018;Yan et al., 2019;Nguyen et al., 2021a), adversarial networks (Hong et al., 2018;Zhang et al., 2019b), and pre-trained language models (Wadden et al., 2019;Zhang et al., 2019a;Yang et al., 2019;Zhang et al., 2020;Liu et al., 2020;Pouran Ben Veyseh et al., 2021b,a).","More recent efforts have primarily made use of deep learning techniques such as convolutional neural networks (Nguyen and Grishman, 2015;Chen et al., 2015;Nguyen et al., 2016b), recurrent neural networks (Nguyen et al., 2016a;Sha et al., 2018;Nguyen andNguyen, 2019), graph convolutional networks (Nguyen andGrishman, 2018a;Yan et al., 2019), adversarial networks (Hong et al., 2018;Zhang et al., 2019b), and pre-trained language models (Wadden et al., 2019;Zhang et al., 2019a;Yang et al., 2019;Zhang et al., 2020;Liu et al., 2020).","Modify,Fact/Evidence",Related Work,Related Work
39-ARR,An frequent example of this phenomenon are verb conjugations where some tenses only exist in some languages.,"An example of this phenomenon are verb conjugations where some tenses only exist in some languages, which is commonplace in ED as event triggers are usually related to the verbs in a sentence.",Split+Identical,Introduction,Introduction
39-ARR,Accurate verb handling is of particular importance for the ED task as event triggers are usually related to the verbs in a sentence.,"An example of this phenomenon are verb conjugations where some tenses only exist in some languages, which is commonplace in ED as event triggers are usually related to the verbs in a sentence.","Split+Modify,Fact/Evidence",Introduction,Introduction
39-ARR,"Adversarial Language Adaptation, inspired by models in domain adaptation research (Ganin and Lempitsky, 2015;Naik and Rose, 2020;Ngo Trung et al., 2021), has been successfuly applied at generating language-invariant models (Joty et al., 2017;Chen et al., 2018;Nguyen et al., 2021b).","Adversarial Language Adaptation, inspired by models in domain adaptation research (Ganin and Lempitsky, 2015;Naik and Rose, 2020), has been successfuly applied at generating languageinvariant models (Joty et al., 2017;Chen et al., 2018).","Modify,Fact/Evidence",Related Work,Related Work
39-ARR,"We present OACLED, a new model for crosslingual event detection that learns fine-grained language-invariant representations by optimizing the standard ALA training through optimaltransport-based sample selection.",We present a new model for Cross-Lingual Event Detection that leverages unlabeled data through ALA and OT to achieve new state-of-the-art performance.,"Modify,Fact/Evidence",Conclusion,Conclusion
39-ARR,Our model achieves new state-of-the-art performance in our experiments on 8 different language pairs which demonstrate its robustness and effectiveness at generating refined language-invariant representations that allow for better event detection results.,Our experiments on 8 different language pairs demonstrate our approach's robustness and effectiveness at generating refined language-invariant representations that allow for better event detection results.,"Modify,Clarity",Conclusion,Conclusion
39-ARR,Our analysis of its intermediate outputs and predictions confirm that OACLED's representations are indeed closer to each other and that this proximity translates into better handling of difficult cross-lingual instances.,Our analysis of its intermediate outputs and predictions confirm that our model's representations are indeed closer to each other and that this proximity translates into better handling of difficult cross-lingual instances.,"Modify,Clarity",Conclusion,Conclusion
39-ARR,"Some recent work (Majewska et al., 2021) has attempted to address this issue by injecting external verb knowledge into the training process.","Some recent work (Majewska et al., 2021) attempts to address this issue by injecting external linguistic knowledge into the training process.","Modify,Grammar",Introduction,Introduction
39-ARR,Another similar problematic issue for CLED are triggers with different meanings that are each distinct words in different languages.,Another problematic issue are triggers with different meanings that are each distinct words in other languages.,"Modify,Clarity",Introduction,Introduction
39-ARR,"For instance, the word ""juicio"" in Spanish can either mean ""judgement"" or ""trial"" in English, depending on the context.","For instance, the word ""juicio"" in Spanish can be either ""judgement"" or ""trial"" in English, depending on the context.","Modify,Clarity",Introduction,Introduction
39-ARR,"Yet, their performance still shows room for improvement as they sometimes struggle to handle the difficult instances, unique to cross-lingual settings, mentioned earlier.","Yet, their performance still shows room for improvement as they are unable to handle the difficult instances, unique to cross-lingual settings, mentioned earlier.","Modify,Clarity",Introduction,Introduction
39-ARR,"Their performance, however, reveals there is room for improvement as the crosslingual setting entails particular challenges.","Their performance, however, reveals there is room for improvement as they mishandle delicate cross-lingual instances.","Modify,Claim",Abstract,Abstract
39-ARR,"It is our intuition that by integrating unlabeled target-language data into the training process, the model is exposed to more language context which should help deal with issues such as verb variation and multiple connotations.","It is our intuition that by integrating unlabeled data into the training process, the model is exposed to more language context which should help deal with issues such as verb variation and multiple connotations.","Modify,Clarity",Introduction,Introduction
39-ARR,The key idea is to generate language-invariant representations that are not indicative of language but remain informative for the ED task.,The key idea is to generate language-invariant representations that are not indicative of language but remain informative for the task.,"Modify,Clarity",Introduction,Introduction
39-ARR,"Thus, we suggest presenting the LD with examples that have similar contextual semantics, i.e., similar contextualized representations.","Thus, we suggest presenting the LD with examples that have similar contextual semantics, i.e., similar representations.","Modify,Clarity",Introduction,Introduction
39-ARR,"Second, we consider that sentences containing events should provide an ED system with additional task-relevant information when compared against non-event samples.","Second, we consider sentences containing events to be more relevant for the LD.","Modify,Claim",Introduction,Introduction
39-ARR,"Accordingly, we argue that event-containing sentences should have a larger probability of being selected for ALA training.","Accordingly, such sentences should have a larger probability of being selected for ALA training.","Modify,Clarity",Introduction,Introduction
39-ARR,"With these intuitions in mind, we propose Optimal Transport (OT) (Villani, 2008) as a natural solution to simultaneously incorporate both the similarity between sample representations and the likelihood of the samples containing an event into a single framework.","As such, we suggest using Optimal Transport (OT) (Villani, 2008) as a natural solution to simultaneously incorporate both the similarity between sample representations and the likelihood of the samples containing an event into a single framework.","Modify,Clarity",Introduction,Introduction
39-ARR,"For our experiments, we focus on the widely used ACE05 (Walker et al., 2006) and ERE (Song et al., 2015) datasets which, in conjuction, contain event-annotations in 4 different languages: English, Spanish, Chinese, and Arabic.","For our experiments, we focus on the widely used ACE05 and ACE05-ERE datasets (Walker et al., 2006) which, in conjuction, contain eventannotations in 4 different languages: English, Spanish, Chinese, and Arabic.","Modify,Fact/Evidence",Introduction,Introduction
39-ARR,We believe these results demonstrate our model's efficacy and applicability at creating CLED systems.,These results demonstrate our model's efficacy and applicability at creating CLED systems.,"Modify,Clarity",Introduction,Introduction
39-ARR,"Using such representations as input, a prediction network P computes a distribution over the set of possible labels and is trained in a supervised manner using the negative log-likelihood function L P :","Then, we feed the representations h i into a prediction network P to compute a distribution over the set of possible labels and train it in a supervised manner using the negative log-likelihood function L P :","Modify,Clarity",Problem Definition,Problem Definition
39-ARR,"More importantly, we optimize the adversarial training process by only presenting the discriminator with the most informative samples.","More importantly, we optimize the adversarial training .","Modify,Fact/Evidence",Abstract,Abstract
59-ARR,,We can see that the standard deviations are small and the performance over different sets of samples shows the similar trend.,"Delete,Fact/Evidence",,Benchmarking on MedLAMA
59-ARR,,This further highlights that the probing success of Contrastive-Probe is not due the selected pre-training sentences.,"Delete,Fact/Evidence",,Benchmarking on MedLAMA
59-ARR,,"Intuitively, the contrastive self-retrieving game ( §4) is equivalent to the formulation of the cloze-style filling task, hence tuning the underlying PLMs makes them better suited for knowledge elicitation needed during probing (like 'rewiring' the switchboards).","Delete,Fact/Evidence",,Benchmarking on MedLAMA
59-ARR,,"Additionally, from Figure 4 we can also observe that different relations exhibit very different trends during pre-training steps of Contrastive-Probe and peak under different steps, suggesting that we need to treat different types of relational knowledge with different tuning depths when infusing knowledge.","Delete,Fact/Evidence",,Benchmarking on MedLAMA
59-ARR,,We leave further exploration of this to future work.,"Delete,Claim",,Benchmarking on MedLAMA
59-ARR,,Probing by Relations.,"Delete,Other",,Benchmarking on MedLAMA
59-ARR,,"They each propose decoding methods that generate multi-token answers, which we have shown to work poorly on MedLAMA.","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,"BioLAMA (Sung et al., 2021) is a concurrent work that also releases a benchmark for biomedical knowledge probing.","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,"We provide a comparison between LAMA, BioLAMA and MedLAMA in terms of (# relations, # queries, avg # answers per query, avg # characters per answer) in the Appendix. 12 Probing via Prompt Engineering.","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,"Knowledge probing is sensitive to what prompt is used (Jiang et al., 2020b).","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,"To bootstrap the probing performance, Jiang et al. (2020b) mine more prompts and ensemble them during inference.","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,"Later works parameterised the prompts and made them trainable (Shin et al., 2020b;Fichtel et al., 2021;Qin and Eisner, 2021).","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,"We have opted out promptengineering methods that require training data in this work, as tuning the prompts are essentially tuning an additional (parameterised) model on top of PLMs.","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,"As pointed out by Fichtel et al. (2021), prompt tuning requires large amounts of training data from the task.","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,"Since task training data is used, the additional model parameters are exposed to the target data distribution and can solve the set set by overfitting to such biases .","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,Biomedical Knowledge Probing.,"Delete,Other",,Related Work and Discussion
59-ARR,,Nadkarni et al. (2021) train PLMs as KB completion models and test on the same task to understand how much knowledge is in biomedical PLMs.,"Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,"BioLAMA focuses on the continuous prompt learning method OptiPrompt (Zhong et al., 2021), which also requires ground-truth training data from the task.","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,"Overall, compared to BioLAMA, we have provided a more comprehensive set of probing experiments and analysis, including proposing a novel probing technique and providing human evaluations of model predictions.","Delete,Fact/Evidence",,Related Work and Discussion
59-ARR,,Table 5 shows the detailed relation names and their manual prompts of our MedLAMA.,"Delete,Fact/Evidence",,Conclusion
59-ARR,,"Moreover, Sung et al. (2021) only use two existing probing approach on their proposed BioLAMA, while in this paper we further proposed a new probing approach Contrastive-Probe.","Delete,Fact/Evidence",,Conclusion
59-ARR,This work was done at the University of Cambridge.,,"Add,Fact/Evidence",Introduction,
59-ARR,"Specifically, we utilize the cloze-style query with a single [Mask] token as the model input.",,"Add,Fact/Evidence",Existing Multi-token Knowledge Probing Approaches,
59-ARR,The model then predicts the answer entities that correspond to the [Mask] token in an autoregressive manner.,,"Add,Fact/Evidence",Existing Multi-token Knowledge Probing Approaches,
59-ARR,An illustration is provided in Figure 2(b).,,"Add,Fact/Evidence",Existing Multi-token Knowledge Probing Approaches,
59-ARR,"Alternatively, the retrieval-based probing are applied to address this issue.",,"Add,Fact/Evidence",Existing Multi-token Knowledge Probing Approaches,
59-ARR,"Instead of generating answers based on the PLM vocabulary, the retrieval-based approach finds answers by ranking the knowledge graph candidate entities based on the query and entity representations, or the entity generating scores.",,"Add,Fact/Evidence",Existing Multi-token Knowledge Probing Approaches,
59-ARR,"To probe PLMs on MedLAMA, we use mask average , an approach that takes the average log probabilities of entity's individual tokens to rank the candidates.",,"Add,Fact/Evidence",Existing Multi-token Knowledge Probing Approaches,
59-ARR,The retrieval-based approaches address the multi-token issue by restricting the output space to the valid answer set and can be used to probe knowledge in different types of PLMs (e.g. BERT vs. fastText; ).,,"Add,Fact/Evidence",Existing Multi-token Knowledge Probing Approaches,
59-ARR,"However, previous works only report results based on the type-restricted candidate set (e.g. relation) which we observed to decay drastically under the full entity set.",,"Add,Claim",Existing Multi-token Knowledge Probing Approaches,
59-ARR,This objective function encourages f to create similar representations for any query-answer pairs from the same sentence and dissimilar representations for queries/answers belonging to different sentences.,,"Add,Fact/Evidence",Existing Multi-token Knowledge Probing Approaches,
59-ARR,Retrieval-based Probing.,,"Add,Other",Existing Multi-token Knowledge Probing Approaches,
59-ARR,"For probing step, the query is created based on the prompt-based template for each knowledge triple , as shown in the following:",,"Add,Fact/Evidence",Existing Multi-token Knowledge Probing Approaches,
59-ARR,"In Figure 7, we further plot the layer-wise probing performance of PubMedBERT over different relations.",,"Add,Fact/Evidence",Benchmarking on MedLAMA,
59-ARR,"Surprisingly, we find that different relations do not show the same probing performance trends over layers.",,"Add,Fact/Evidence",Benchmarking on MedLAMA,
59-ARR,"For example, with only the first 3 layers, PubMedBERT achieves the best accuracy (>15%) on relation 11 queries.",,"Add,Fact/Evidence",Benchmarking on MedLAMA,
59-ARR,"This result demonstrates that both relation types and PLM layers are confounding variables in capturing factual knowledge, which helps to explain the difference of training steps over relations in Figure 4.",,"Add,Fact/Evidence",Benchmarking on MedLAMA,
59-ARR,This result also suggests that layer-wise and relation-wise training could be the key to effectively infuse factual knowledge for PLMs.,,"Add,Claim",Benchmarking on MedLAMA,
59-ARR,In-depth Analysis of Contrastive-Probe,,"Add,Other",Experiments,
59-ARR,Comparing with BioLAMA,,"Add,Other",Experiments,
59-ARR,"We can see that, without additional training data from the biomedical knowledge facts, Contrastive-Probe reaches a promising performance compared with OptiPrompt approach, which needs further training data.",,"Add,Claim",Comparing with BioLAMA,
59-ARR,"Additionally, since Mask Predict and OptiPrompt require using the MLM head, it is impossible to compare a model without MLM head being released (e.g. PubMedBERT).",,"Add,Claim",Comparing with BioLAMA,
59-ARR,"In contrast, our Contrastive-Probe not only provides a good indicator of comparing these models in terms of their captured knowledge, but also makes layerwise knowledge probing possible.",,"Add,Claim",Comparing with BioLAMA,
59-ARR,Limitations of Contrastive-Probe,,"Add,Other",Experiments,
59-ARR,"However, we have noticed that different models and different probing datasets have different optimal training steps.",,"Add,Fact/Evidence",Limitations of Contrastive-Probe,
59-ARR,"To truly 'rewire' the most knowledge out of each PLMs, we need a unified validation set for checkpoint selection.",,"Add,Fact/Evidence",Limitations of Contrastive-Probe,
59-ARR,What the validation set should be and how to guarantee its fairness require further investigation.,,"Add,Claim",Limitations of Contrastive-Probe,
59-ARR,Performance not very stable.,,"Add,Other",Limitations of Contrastive-Probe,
59-ARR,We have noticed that using different contrastive tuning corpus as well as different random seeds can lead to a certain variance of their probing performances (see Table 5).,,"Add,Fact/Evidence",Limitations of Contrastive-Probe,
59-ARR,"To mitigate such issue, we use average perfor-Probe Model CTD wikidata UMLS acc@1 acc@5 acc@1 acc@5 acc@1 acc@5 mance of 10 runs on 10 randomly sampled corpus.",,"Add,Fact/Evidence",Limitations of Contrastive-Probe,
59-ARR,Improving the stability of Contrastive-Probe and investigating its nature is a future challenge.,,"Add,Claim",Limitations of Contrastive-Probe,
59-ARR,We train our Contrastive-Probe based on 10k sentences which are randomly sampled from the original pre-training corpora of the corresponding PLMs.,,"Add,Fact/Evidence",Conclusion,
59-ARR,"Since most of the biomedical BERTs use PubMed texts as their pre-training corpora, for all biomedical PLMs we sampled random sentences from a version of PubMed corpus used by BlueBERT model (Peng et al., 2019), while for BERT we sampled sentences from its original Wikitext corpora.",,"Add,Fact/Evidence",Conclusion,
59-ARR,"For the hyperparamters of our Contrastive-Probe, Table 8 lists our search options and the best parameters used in our paper.",,"Add,Fact/Evidence",Conclusion,
59-ARR,"While Contrastive-Probe pushes the acc@10 to 24%, the performance gap remains notable.","While Contrastive-Probe pushes the acc@10 to 28%, the performance gap still remains notable.","Modify,Fact/Evidence",Abstract,Abstract
59-ARR,Our human expert evaluation suggests that the probing performance of our Contrastive-Probe is underestimated as UMLS does not comprehensively cover all existing factual knowledge.,Our human expert evaluation suggests that the probing performance of our Contrastive-Probe is still under-estimated as UMLS still does not include the full spectrum of factual knowledge.,"Modify,Clarity",Abstract,Abstract
59-ARR,"Both mask predict and retrieval-based approaches are tested under both the general domain and biomedical domain BERT models, i.e. Bert-based-uncased (Devlin et al., 2019), BlueBERT (Peng et al., 2019), BioBERT , PubMedBERT (Gu et al., 2020).","Both mask predict and retrieval-based approaches are tested under both the general domain and biomedical domain BERT models, i.e. Bert-based-uncased (Devlin et al., 2019), BlueBERT , BioBERT that are pre-trained on large biomedical corpora.","Modify,Fact/Evidence",Experiments,Experiments
59-ARR,"Pre-trained language models (PLMs; Devlin et al. 2019;) have orchestrated incredible progress on myriads of few-or zero-shot language understanding tasks, by pre-training model parameters in a task-agnostic way and transferring knowledge to specific downstream tasks via finetuning (Brown et al., 2020;Petroni et al., 2021).","Pre-trained language models (PLMs; ) have orchestrated incredible progress on myriads of few-or zero-shot language understanding tasks, by pre-training model parameters in a task-agnostic way and transferring knowledge to specific downstream tasks via finetuning (Brown et al., 2020;Petroni et al., 2021).","Modify,Fact/Evidence",Introduction,Introduction
59-ARR,"In contrast, our Contrastive-Probe obtains absolute improvements by up-to ∼ 5% and ∼ 21% on acc@1 and acc10 respectively comparing with the three existing approaches, which validates its effectiveness on measuring the knowledge probing performance.","In contrast, our Contrastive-Probe obtains absolute improvements by up-to ∼ 6% and ∼ 25% on acc@1 and acc10 respectively comparing with the three existing approaches, which validates its effectiveness on measuring the knowledge probing performance.","Modify,Fact/Evidence",Benchmarking on MedLAMA,Benchmarking on MedLAMA
59-ARR,"In particular, PubMedBERT model obtains the best probing performance (5.71% in accuracy) for these biomedical queries, validating its effectiveness of capturing biomedical knowledge comparing with other PLMs (i.e. BERT, BlueBERT and BioBERT).","In particular, PubMedBERT model obtains the best probing performance (7.32% in accuracy) for these biomedical queries, validating its effectiveness of capturing biomedical knowledge comparing with other PLMs (i.e. BERT, BlueBERT and BioBERT).","Modify,Fact/Evidence",Benchmarking on MedLAMA,Benchmarking on MedLAMA
59-ARR,Table 5 shows the probing results over the full and hard sets.,Table 4 shows the probing results over the full and hard sets (detailed macro and micro accuracies are provided in Appendix).,"Modify,Fact/Evidence",Benchmarking on MedLAMA,Benchmarking on MedLAMA
59-ARR,"While PubMedBERT performs the best among all the pure pre-trained models, SapBERT (Liu et al., 2021a) and CoderBERT (Yuan et al., 2020) (which are the knowledge infused PubMedBERT) further push performance to 8% and 30.41% on acc@1 and acc@10 metrics respectively, highlighting the benefits of knowledge infusion pre-training.","While PubMedBERT performs the best under all metrics, CoderBERT (which is the knowledge infused PubMedBERT) achieves better performance on micro acc@1, highlighting the benefits of knowledge infusion pre-training.","Modify,Fact/Evidence",Benchmarking on MedLAMA,Benchmarking on MedLAMA
59-ARR,"To reduce the gap, we further proposed a novel contrastive recipe which rewires the underlying PLMs without using any probing-specific data and illustrated that with a lightweight pre-training their accuracies could be pushed to 24%.","To reduce the gap, we further proposed a novel contrastive recipe which rewires the underlying PLMs without using any probing-specific data and illustrated that with a lightweight pre-training their accuracies could be pushed to 28%.","Modify,Fact/Evidence",Conclusion,Conclusion
59-ARR,"During the writing of this work, we noticed a concurrent work to ours that also released a biomedical knowledge probing benchmark, called Bio-LAMA Sung et al. (2021).","During the writing of this work, we notice that Sung et al. (2021) also released a biomedical knowledge probing benchmark, called BioLAMA, which is a work concurrent to ours.","Modify,Clarity",Comparing with BioLAMA,Conclusion
59-ARR,"In Table 6, we compare MedLAMA with LAMA (Petroni et al., 2019) and BioLAMA in terms of data statistics.","In Table 6, we compare our MedLAMA with LAMA (Petroni et al., 2019) and BioLAMA (Sung et al., 2021) in terms of their statistics.","Modify,Fact/Evidence",Comparing with BioLAMA,Conclusion
59-ARR,"We found that there is only 1 overlapped relation (i.e., may treat) between BioLAMA and MedLAMA, and no overlap exists on the queries.","We found that there is only 1 overlapped relation (i.e. may treat) between BioLAMA and our MedLAMA, and no same query can be found.","Modify,Clarity",Comparing with BioLAMA,Conclusion
59-ARR,"We further highlight that the elicited knowledge by Contrastive-Probe is not gained from the additional random sentences, but from the original pre-trained parameters, which echos the previous finding of Liu et al. (2021b); Glavaš and Vulić (2021); Su et al. (2021Su et al. ( , 2022.","We further highlight that the elicited knowledge by Contrastive-Probe is not gained from the additional random sentences, but from the original pretrained parameters, which echos the previous finding of Liu et al. (2021b); Glavaš and Vulić (2021).","Modify,Fact/Evidence",Introduction,Introduction
59-ARR,Table 2 shows the detailed relation names and their corresponding prompts.,See Appendix for the detailed relation names and their corresponding prompts.,"Modify,Fact/Evidence",MedLAMA,MedLAMA
86-ARR,,"Inference Promotion: We can achieve 11.73 and 2.06 absolute inference accuracy improvements compared to the baselines for the NLI and CQA task, respectively.","Delete,Fact/Evidence",,Main Results
86-ARR,,"For the NLI task, with our MPII framework, the Transformer baseline model can improve over 5 absolute accuracy score.","Delete,Fact/Evidence",,Main Results
86-ARR,,"The ablation study shows the contribution comes from not only the mutual interaction of inference and interpretation in the Stepwise Integration Mechanism (SIM), but also the adversarial mutual information training objective introduced in the Adversarial Fidelity Regularization (AFiRe).","Delete,Fact/Evidence",,Main Results
86-ARR,,"Moreover, with parameters initialized with the pretrained BART model, the accuracy can be further improved by a 4.53 absolute score.","Delete,Fact/Evidence",,Main Results
86-ARR,,"For the CQA task, we observe that better performance is still achieved compared with the CAGE baseline model.","Delete,Fact/Evidence",,Main Results
86-ARR,,"If we remove the AFiRe, a significant inference degradation would be witnessed.","Delete,Claim",,Main Results
86-ARR,,It also indicates the effectiveness of AFiRe for utilizing interpretability to improve the inference ability.,"Delete,Claim",,Main Results
86-ARR,,Interpretation Promotion: The quality of generated interpretation can also be significantly improved with our mutual promotion method on both NLI and CQA tasks.,"Delete,Fact/Evidence",,Main Results
86-ARR,,"For NLI task, combined with our MPII, the Transformer baseline model can provide more accurate, fluent and diverse interpretation with much better results in all metrics.","Delete,Fact/Evidence",,Main Results
86-ARR,,"Similar with the inference results, the ablation study shows that both SIM and AFiRe contribute to the performance improvement.","Delete,Fact/Evidence",,Main Results
86-ARR,,"With the pretrained BART model, we further improve the BLEU and Inter-Rep performance and get comparable PPL compared with the e-INFERSENT model.","Delete,Fact/Evidence",,Main Results
86-ARR,,"For CQA task, our method performs better in terms of BLEU score and the diversity of generated explanations.","Delete,Fact/Evidence",,Main Results
86-ARR,,"We notice that the BLEU scores are pretty low for CQA task, which may stem from the free form of expression for explanations in the dataset, i.e. several different explanations share the same commonsense knowledge.","Delete,Claim",,Main Results
86-ARR,"The input of the model is ""[CLS] a couple standing on what looks like a peer or boardwalk [SEP] a couple hugging each other at the park"", of which the ground truth label is ""contradiction"".",,"Add,Fact/Evidence",Analysis,
86-ARR,"For the second example, our MPII and MPII with AFiRe removed still capture the entailment relation well, and explain that ""at the beach"" and ""at restaurant"" can not be done at the same time.",,"Add,Fact/Evidence",Analysis,
86-ARR,"As we can see, these explanations generated by our method are also fluent.",,"Add,Fact/Evidence",Analysis,
86-ARR,"Our MPII still explains well, but fails to explain properly with AFiRe removed, even if the explanation contains the correct answer, which reveals the importance of AFiRe for promotion of interpretation.",,"Add,Claim",Analysis,
86-ARR,,Both prediction label and explanation token are generated at every decoding step.,"Delete,Fact/Evidence",,Task Description
86-ARR,,Two fusion gates are attached to enable deep interaction of their hidden representations.,"Delete,Fact/Evidence",,Task Description
86-ARR,"ReLU(•) here denotes the ReLU activation function (Nair and Hinton, 2010), σ(•) represents the sigmoid function.","ReLU(.) here denotes the ReLU activation function (Nair and Hinton, 2010), σ(.) represents the sigmoid function.","Modify,Grammar",Stepwise Integration Mechanism,Stepwise Integration Mechanism
86-ARR,"The step-by-step explanation helps the model to do better inference, and the stepwise inference in turn guides the generation of better explanation.","Step-by-step interpretation helps the model to better inference, stepwise inference in turn guides the generation of better explanation.","Modify,Clarity",Stepwise Integration Mechanism,Stepwise Integration Mechanism
86-ARR,"Because of the intractability of directly estimating the mutual information in high-dimensional space, we approximate the optimization objective with a Variational Information Maximization lower bound (Chen et al., 2016b;Zhang et al., 2018;Poole et al., 2019):","Because of the intractability of directly estimating the mutual information in high-dimensional space, we approximate the optimization objective with a Variational Information Maximization (Chen et al., 2016b;Zhang et al., 2018) lower bound (Poole et al., 2019):","Modify,Clarity",Adversarial Fidelity Regularization,Adversarial Fidelity Regularization
86-ARR,"P θ (L, E|X) and Q ϕ (X|L, E) denote the forward network (generating L, E conditioned on X) and the backward network (generating X conditioned on L, E) respectively.","P θ (L, E|X) and Q ϕ (X|L, E) denote the forward network (generating L, R conditioned on X) and the backward network (generating X conditioned on L, E) respectively.","Modify,Fact/Evidence",Adversarial Fidelity Regularization,Adversarial Fidelity Regularization
86-ARR,"In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretations to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2019;Thorne et al., 2019;Kumar and Talukdar, 2020).","In order to break the black-box of neural networks, many works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2018;Thorne et al., 2019;Kumar and Talukdar, 2020).","Modify,Fact/Evidence",Introduction,Introduction
86-ARR,"Besides, we add an objective term P θ (L, E|X) of maximize the negative likelihood of P θ to balance the positive samples as teacher-forcing algorithm (Li et al., 2017).","We also add an objective term P θ (L, E|X) of maximum the negative likelihood of P θ to balance the positive samples as teacher-forcing algorithm (Li et al., 2017).","Modify,Other",Adversarial Fidelity Regularization,Adversarial Fidelity Regularization
86-ARR,"We use six datasets as our testbeds: SNLI (Bowman et al., 2015), e-SNLI (Camburu et al., 2018), CQA (Talmor et al., 2019), CoS-E (Rajani et al., 2019, MultiNLI (Williams et al., 2018), and SICK-E (Marelli et al., 2014).","We use six datasets as our testbeds: SNLI (Bowman et al., 2015), e-SNLI (Camburu et al., 2018), CQA (Talmor et al., 2018), CoS-E (Rajani et al., 2019), MultiNLI (Williams et al., 2018), and SICK-E (Marelli et al., 2014).","Modify,Fact/Evidence",Datasets,Datasets
86-ARR,"SICK-e (Sentences Involving Compositional Knowledge for entailment) provides sentence pairs that are rich in the lexical, syntactic and semantic phenomena.","SICKe(Sentences Involving Compositional Knowledge for entailment) provides sentence pairs that are rich in the lexical, syntactic and semantic phenomena.","Modify,Grammar",Datasets,Datasets
86-ARR,The latter two datasets are used for out-of-domain evaluation.,The latter two datasets are used for out-of-domain test.,"Modify,Clarity",Datasets,Datasets
86-ARR,"The Transformer model (Vaswani et al., 2017) adds a MLP layer for making predictions.","The Transformer model (Vaswani et al., 2017) adds a MLP layer for generating sentencelevel interpretations.","Modify,Fact/Evidence",Baselines NLI:,Baselines NLI:
86-ARR,"As shown in Table 2, we evaluate our method with the Transformer baseline model on two outof-domain datasets: MultiNLI and SICK-E. The results show that our mutual promotion method enables the Transformer model to be more robust, and achieves about 3 absolute accuracy improvement on both of the out-of-domain datasets without fine-tuning.","As shown in Table 2, we evaluate our method with the Transformer baseline model on two outof-domain datasets: MultiNLI and SICK-E. The results show that our mutual promotion method enables the Transformer model to be more robust, and achieve more than 3 accuracy improvement on both of the out-of-domain datasets without fine-tuning.","Modify,Fact/Evidence",Out-of-Domain Evaluation,Out-of-Domain Evaluation
86-ARR,The ablation results demonstrate both the adversarial mutual information training strategy in AFiRe and deep integration in SIM is very effective to improve the model's generalization and robustness.,The ablation results demonstrate the adversarial mutual information training strategy in AFiRe is very effective to improve the model's generalization and robustness.,"Modify,Fact/Evidence",Out-of-Domain Evaluation,Out-of-Domain Evaluation
86-ARR,"Considering readability and comprehensibility for humans, some works turn to generate token-level explanations (Liu et al., 2019;Thorne et al., 2019), which are nevertheless prone to cause ambiguity.","Considering readability and comprehensibleness for human, some works turn to generate token-level explanations (Liu et al., 2018;Thorne et al., 2019), which nevertheless prone to cause ambiguity.","Modify,Fact/Evidence",Introduction,Introduction
86-ARR,"From the clear split of the red and blue lines when ""does"" and ""not"" are generated, we can see that the prediction is very sensitive to explanation, which demonstrates the faithfulness (Kumar and Talukdar, 2020).","From the clear split of the red and blue lines when 'does' and 'not' are generated, we can see that the prediction is very sensitive to explanation, which demonstrates the faithfulness (Kumar and Talukdar, 2020).","Modify,Grammar",Analysis,Analysis
86-ARR,"For the first example, CAGE makes wrong prediction, and generates explanation that obviously conflicts with common knowledge.","For the first exapmle, CAGE makes wrong prediction, and generates explanation that obviously conflicts with common knowledge.","Modify,Grammar",Analysis,Analysis
86-ARR,"With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2019;Thorne et al., 2019;Kumar and Talukdar, 2020).","With the great success of natual language inference, many recent works explore the interpretability of neural networks through providing interpretation to support their inference results (Ribeiro et al., 2016;Chen et al., 2018;Liu et al., 2018;Thorne et al., 2019;Kumar and Talukdar, 2020).","Modify,Fact/Evidence",Related Work,Related Work
86-ARR,"Intuitively, human language sentence-level interpretations containing reasoning logic are the best form for human to understand.","Intuitively, human language sentence-level interpretations containing reasoning logic is the best form for human to understand.","Modify,Grammar",Introduction,Introduction
86-ARR,"With annotated natural language interpretation datasets available (Camburu et al., 2018;Rajani et al., 2019), methods of generating sentence-level interpretation have been explored recently.","With the annotated natural language interpretation datasets available (Camburu et al., 2018;Rajani et al., 2019), methods of generating sentencelevel interpretation have been explored recently.","Modify,Grammar",Introduction,Introduction
86-ARR,"Kumar and Talukdar (2020) proposed to first generate sentence-level interpretations with deep pre-trained language models (such as BERT and GPT), then fed those interpretations as extra knowledge to help improve inference performance.","Kumar and Talukdar (2020) proposed to first generate sentence-level interpretation with deep pre-trained language models (such as BERT and GPT), then fed those interpretation as extra knowledge to help improve inference performance.","Modify,Grammar",Introduction,Introduction
86-ARR,"• Different from the previous works that only include one-side promotion, we mutually promote the inference and sentence-level interpretation from both the model-level and the optimization-level. • We propose a Stepwise Integration Mechanism to tightly fuse latent prediction and interpretation information at every decoding step, and an Adversarial Fidelity Regularization to further improve the fidelity with the adversarial training strategy. • Experiment results show that our method achieves significant improvement in both inference accuracy and interpretation quality compared with baseline models.","• Different from the previous works that only include one-side promotion, we mutually promote the inference and sentence-level interpretation from both the model-level and the optimization-level. • We propose a Stepwise Integration Mechanism to tightly fuse latent prediction and interpretation information at every decoding step, and an Adversarial Fidelity Regularization to further improve the fidelity with the adversarial training strategy. • Experiment results show that our method achieve significant improvement in both inference accuracy and interpretation quality compared with baseline models.","Modify,Grammar",Introduction,Introduction
86-ARR,"Utilizing the autoregressive nature of Transformer decoder, SIM enables deep interaction at every decoding step between inference and interpretation.","Utilizing the autoregressive nature of Transformer decoder, SIM allows deep interaction at every decoding step between inference and interpretation.","Modify,Clarity",Methodology,Methodology
86-ARR,"With the adversarial training strategy, AFiRe enables further integration of latent semantic information between inference and interpretation, and also improves the quality of explanation sentences by bringing them closer to human expressions.","With the adversarial training strategy, AFiRe allows further integration of latent semantic information between inference and interpretation, and also improves the quality of explanation sentences, bringing them closer to human expressions.","Modify,Clarity",Methodology,Methodology
1-12,"Since the N-terminal hydrophobic domain of plasmepsin V is not cleaved <REF-12> , it is likely a transmembrane signal anchor.",,"Add,Claim",The hypothesis,
1-12,Both TMHMM and TopPred predict it to insert into the ER membrane with the N-terminus in the lumen such that the subsequent soluble region containing the active site would be in the cytoplasm.,,"Add,Fact/Evidence",The hypothesis,
1-12,Membrane proteins would be transported by vesicular transport from the PVM to Maurer's clefts where their release from the PI3P patches could be triggered by a PI3P-phosphatase.,Membrane proteins would be transported by vesicular transport from the PVM to Maurer’s clefts where their release from the PI3P patches could be triggered by a PI3P-phosphatase.,"Modify,Grammar",The way out,The way out
1-12,"The authors' interpretation of the data was that proteins must be unfolded in order to be transported across the PVM into the erythrocyte, that the PVM therefore contained a protein-conducting channel with similar requirements for transport as the Sec61 channel in the ER membrane, and that the time window after synthesis during which proteins were transport-competent was limited.","The authors’ interpretation of the data was that proteins must be unfolded in order to be transported across the PVM into the erythrocyte, that the PVM therefore contained a protein-conducting channel with similar requirements for transport as the Sec61 channel in the ER membrane, and that the time window after synthesis during which proteins were transport-competent was limited.","Modify,Grammar",The way out,The way out
1-12,- The uncleaved targeting signal binds PI3P at the ER membrane with the same specificity required for protease cleavage and host cell targeting; the cleaved signal no longer binds PI3P <REF-13> .,- The uncleaved targeting signal binds phosphoinositol-3-phosphate (PI3P) at the ER membrane with the same specificity required for protease cleavage and host cell targeting; the cleaved signal no longer binds PI3P <REF-13> .,"Modify,Clarity",The fact(or)s,The fact(or)s
1-12,"- A putative 'translocator' complex resides in the PV membrane (PVM); it consists of 5 proteins that coprecipitate some of the proteins bearing a host cell targeting signal, but a function of the complex has not been demonstrated in any way, nor has it been investigated whether the association of the complex and the PEXEL proteins is mediated by the PEXEL signal <REF-14> , <REF-15> .","- A putative ‘translocator’ complex resides in the PV membrane (PVM); it consists of 5 proteins that coprecipitate some of the proteins bearing a host cell targeting signal, but a function of the complex has not been demonstrated in any way, nor has it been investigated whether the association of the complex and the PEXEL proteins is mediated by the PEXEL signal <REF-14> , <REF-15> .","Modify,Grammar",The fact(or)s,The fact(or)s
1-12,"This cannot be right: N-acetylation is a cytosolic modification, based on the biochemical and sequence data characterizing the protease plasmepsin V, its active site is almost certainly on the cytoplasmic face of the membrane, and the only possible location for PI3P at the ER membrane is in the cytosolic leaflet.","This cannot be right: N-acetylation is a cytosolic modification, the active site of plasmepsin V is almost certainly on the cytoplasmic face of the ER membrane (based on the biochemical & sequence data characterizing the protease), and the only possible location for PI3P at the ER membrane is in the cytosolic leaflet and the only possible location for PI3P at the ER membrane is in the cytosolic leaflet.","Modify,Clarity",The hypothesis,The hypothesis
1-12,"Russo and colleagues showed later (2010) <REF-10> that fusion of this region of plasmepsin V to a fluorescent reporter protein resulted in ring-shaped staining around the parasite cytoplasm indicative of location in the PV or at the parasite plasma membrane, and suggesting entry of the protein into the ER and transport to the cell surface.","Russo and colleagues (2010) <REF-10> showed later that this region of plasmepsin V was not able to target the protein to the ER, which demonstrates that it is not a signal sequence.","Modify,Fact/Evidence",The hypothesis,The hypothesis
1-12,In the same paper Russo and colleagues demonstrated that the C-terminal hydrophobic region of plasmepsin V was required for ER retention of the protein <REF-10> .,In the same paper Russo and colleagues demonstrated that the C-terminal hydrophobic region of plasmepsin V was required to anchor the protein to the ER membrane.,"Modify,Fact/Evidence",The hypothesis,The hypothesis
1-12,"Both Russo and colleagues and Klemba and Goldberg describe this region as a transmembrane domain, but Klemba and Goldberg show that 50% of plasmepsin V can be extracted from the membrane by carbonate, pH 11.0 <REF-12> .","Both Russo and colleagues and Klemba and Goldberg describe this region as a transmembrane domain, but Klemba & Goldberg show that 50% of plasmepsin V can be extracted from the membrane by carbonate, pH 11.0 <REF-12> .","Modify,Grammar",The hypothesis,The hypothesis
1-12,"Altogether these data suggest that plasmepsin V is tethered to the ER membrane by hydrophobic regions at both termini, and that its active site is in the cytoplasm.",Altogether these data suggest that plasmepsin V is a carboxy-terminally membrane-anchored or membrane-associated protein with its entire N-terminal domain including the active site in the cytoplasm.,"Modify,Claim",The hypothesis,The hypothesis
1-21,"Namely, whereas the bacterial populations differed with regards the side scatter parameter, forward scatter showed no change in its distribution.",,"Add,Fact/Evidence",Results,
1-21,An alternative explanation is that phage binding decreases resource uptake by bacterial cells.,,"Add,Claim",Discussion,
1-21,"However, this seems unlikely in our experiment.",,"Add,Claim",Discussion,
1-21,"Because bacteria were exposed to inactivated phages only, the total number of viral particles is predicted to stay constant (or possibly degrade) throughout the experiment.",,"Add,Claim",Discussion,
1-21,"When bacteria divide, the number of phages bound to a daughter cell should be roughly half the number on the mother cell; thus, the number of bound phages per cell will decrease exponentially with cell divisions.",,"Add,Claim",Discussion,
1-21,"Using the density of phages and bacteria employed in our experiment, we predict that there will be, on average, less than one phage individual per bacterial cell after 9 to 10 cell divisions, which based on the mean doubling time presented in Figure 1 , is reached in the first 48 hours of the experiment.",,"Add,Claim",Discussion,
1-21,Our results can explain previous observations on phage-associated increases in population size in P. fluorescens <REF-32> .,,"Add,Fact/Evidence",Discussion,
1-21,"Specifically, we predict that a significant number of phage in the experiments of Gomez and Buckling <REF-32> did not kill their bacterial hosts before some of the latter were able to accelerate their cell cycle and produce daughter cells.",,"Add,Claim",Discussion,
1-21,"This response is expected to result in smaller individual size, because energy allocated to growth is directed to reproduction when the stressor is present.",,"Add,Claim",Discussion,
1-21,Observations of c. 50 cells using TEM (Zeis EM10) showed no bound phages after the centrifugation treatment.,,"Add,Fact/Evidence",Preliminary tests,
1-21,"KB medium containing UV-inactivated phages was obtained through centrifugation of inactivated phage, which were further added into pure KB, so that the medium used in the treatments only differs from the control by the presence of phages.",,"Add,Fact/Evidence",Experiments using UV-inactivated phage,
1-21,Measures of OD will be affected by changes in particle size.,,"Add,Claim",Measures,
1-21,"At equal bacterial density, a population of smaller cells will yield a lower OD value, because fewer particles will block less of the incoming light.",,"Add,Claim",Measures,
1-21,"The practical conclusion is that whenever bacteria get smaller, we understimate their count, and thus their growth rate.",,"Add,Claim",Measures,
1-21,"Because this means that we are more conservative about the impact of phage exposure on growth rate (i.e., if there were any bias in our results, it would be an underestimation of the increase in growth rate), we did not correct for this effect.",,"Add,Claim",Measures,
1-21,"We did not observe any difference in the impact of live phage on bacterial populations exposed to the different treatments (KW, df = 2, P = 0.153), suggesting that the inducible response does not alter bacteria resistance to phage predation.","We did not observe any difference in the impact of live phage on bacterial populations exposed to the different treatments (KW, df=2, P = 0.153), suggesting that the inducible response does not alter bacteria resistance to phage predation.","Modify,Grammar",Results,Results
1-21,We hypothesize that this response increases the survival chances of bacterial progeny under natural conditions and demonstrate that this behaviour comes at a fitness cost of reduced size of daughter cells.,We hypothesize that this response increases the survival chances of progeny under natural conditions and demonstrate that this behaviour comes at a fitness cost of reduced size of daughter cells.,"Modify,Claim",Discussion,Discussion
1-21,"Furthermore, our results support and extend both theoretical <REF-33> and empirical <REF-34> , <REF-35> predictions that victims may lessen the fitness impact of their natural enemies through early reproduction, to cases where phenotypic responses are plastic and temporary.","These results support and extend both theoretical <REF-32> and empirical <REF-33> , <REF-34> predictions that victims may lessen the fitness impact of their natural enemies through early reproduction, to cases where phenotypic responses are plastic and temporary.","Modify,Clarity",Discussion,Discussion
1-21,Increased allocation to reproduction in stressful environments–termed “fecundity compensation” or “terminal investment” <REF-31> –although never studied in bacteria-phage associations to our knowledge–has been extensively studied for other host-parasite (or organism-stressor) interactions.,Increased allocation to reproduction in stressful environments–termed “fecundity compensation” or “terminal investment” <REF-31> – although never studied in bacteria-phage associations to our knowledge – has been extensively studied for other host-parasite (or organism - stressor) interactions.,"Modify,Grammar",Discussion,Discussion
1-21,"Assuming that the physiological mechanisms involved in fission rate increases are the same in the two experiments, this suggests that rapid multiplication is not adaptive for the bacterium, and indeed we report no advantage of being exposed to inactived phage in terms of a lessened population impact during live phage exposure.","Assuming that the physiological mechanisms involved in fission rate increases are the same in the two experiments, this suggests that rapid multiplication is not adaptive for the bacterium.","Modify,Fact/Evidence",Discussion,Discussion
1-21,"Future studies should therefore focus on the possible adaptive nature of this response for both bacterium and phage, by investigating in greater depth how it affects the mechanisms of infection, recovery, and resistance.",Future studies should therefore focus on the possible adaptive nature of this response for both bacterium and phage.,"Modify,Claim",Discussion,Discussion
1-21,Inactivated phage were allowed 4 h to attach to the bacterial outer membrane.,Inactivated phage were allowed 4h to attach to the bacterial outer membrane.,"Modify,Grammar",Preliminary tests,Preliminary tests
1-21,"PCR was done using TPV1f (GATGTGAGAAAGCGATACACGG) and TPV1r (GAGAGAAGCGGGAGAGTGAA) sequences developed for this study, which selectively amplify a 550 bp fragment of the phage DNA and a 1200 bp fragment of the bacterial DNA (see Supplementary Figure 1 for detailed protocols).","PCR was done using TPV1f (GATGTGAGAAAGCGATACACGG) and TPV1r (GAGAGAAGCGGGAGAGTGAA) sequences developed for this study, which selectively amplify a 550 bp fragment of the phage DNA and a 1200 bp fragment of the bacterial DNA (see Supplement Figure 1 for detailed protocols).","Modify,Grammar",Preliminary tests,Preliminary tests
1-21,"We did not find any evidence that UV-inactivated phage was present in samples putatively containing bacteria only, thus confirming that (i) the DNA of inactivated phage was not incorporated in the bacterial cell and (ii) our centrifugation method removed both bound and unbound phage.","We did not find any evidence that UV-inactivated phage was present in samples putatively containing bacteria only, thus confirming that the DNA of inactivated phage was not incorporated in the bacterial cell.","Modify,Claim",Preliminary tests,Preliminary tests
1-21,Fixed SBW25 bacteria of the smooth morphotype were first cultivated in 6 ml KB in 30 mL universal glass vials.,Fixed smooth SBW25 bacteria were first cultivated in 6 ml KB in 30 mL universal glass vials.,"Modify,Clarity",Experiments using UV-inactivated phage,Experiments using UV-inactivated phage
1-21,"Exponential phase was determined by conducting a series of windowed linear regressions over the full growth curve, and retaining the part of the curve with the largest slope (computer code given in Supplementary materials part 3 ).","Exponential phase was determined by conducting a series of windowed linear regressions over the full growth curve, and retaining the part of the curve with the largest slope (computer code given in suppl. materials part 3).","Modify,Clarity",Measures,Measures
1-21,"Terminal investment is well characterised for other host-parasite associations <REF-31> , but to our knowledge has not previously been observed in bacteria subject to phage infection.","Terminal investment is well characterised for other host-parasite associations <REF-31> , but to our knowledge has not previously been observed in bacteria and phage.","Modify,Claim",Introduction,Introduction
1-21,"Bacteria exposed to UV-inactivated phage display a statistically significant higher growth rate over the first 24 hours post-exposure than non-phage controls (Kruskal-Wallis, df = 3, P = 0.006; Figure 1 ).","Bacteria exposed to UV-inactivated phage display a statistically significant higher growth rate over the first 24 hours post-exposure than non-phage controls (Kruskal-Wallis, df=3, P = 0.006; Figure 1 ).","Modify,Grammar",Results,Results
1-21,"During the fourth day post-exposure, control and treatment bacteria showed no significant differences in doubling time (KW, df = 3, P > 0.05).","During the fourth day post-exposure, control and treatment bacteria showed no significant differences in doubling time (KW, df=3, P > 0.05).","Modify,Grammar",Results,Results
1-21,"There was a marginally significant effect on population growth for bacteria exposed to different phage concentrations (KW, df = 2, P < 0.02), suggesting that the encounter rate between bacteria and phage is important in determining the population-level strength of the fission response.","There was a marginally significant effect on population growth for bacteria exposed to different phage concentrations (KW, df=2, P < 0.02), suggesting that the encounter rate between bacteria and phage is important in determining the population-level strength of the fission response.","Modify,Grammar",Results,Results
1-21,"We hypothesized that faster doubling times would come at a cost to cell size, since cells would have less time to metabolize and convert absorbed nutrients into cell structure twenty-four hours post-exposure, we found that phage-treated bacteria were two to three times smaller (as measured by mean cellular width) than the control (KW, df = 3, P < 0.0001; Figure 2 ).","We hypothesized that faster doubling times would come at a cost to cell size, since cells would have less time to metabolize and convert absorbed nutrients into cell structure Twenty-four hours post-exposure, we found that phage-treated bacteria were two to three times smaller (as measured by mean cellular width) than the control (KW, df=3, P < 0.0001; Figure 2 ).","Modify,Grammar",Results,Results
1-21,Analyses of the distribution of several flow cytometry profiles showed that a difference in cell shape is unlikely to explain this result (see Data File below).,Analyses of the distribution of several flow cytometry profiles showed that a difference in cell shape is unlikely to explain this result (see data associated to this article).,"Modify,Clarity",Results,Results
1-21,"This implies that bacterial shape remained unchanged throughout the experiment, and indeed, additional observations using a transmission electron microscope showed that the cells remained rod-shaped for all treatments.","Finally, observations using a transmission electron microscope showed that the cells remained rod-shaped for all treatments.","Modify,Claim",Results,Results
1-23,,Data about health hazards linked to FGC were not derived from any of these studies.,"Delete,Fact/Evidence",,Comments
1-23,,"If a new test or a drug is to be prescribed for a patient, it should pass through a complicated series of tests and randomized comparisons before getting approved.","Delete,Claim",,Comments
1-23,,The same occurs with any surgical procedure.,"Delete,Claim",,Comments
1-23,,No procedure can be considered superior to another or blamed for complications except after randomized controlled trials comparing the new to standard surgery.,"Delete,Claim",,Comments
1-23,,It therefore seems unrealistic to consider data about FGC not derived from randomized or cohort studies are true and conclusive.,"Delete,Claim",,Comments
1-23,,"The ban against FGC seems to be gender based, especially because no similar act was taken against male circumcision.","Delete,Claim",,Religious and cultural views
1-23,,"If male circumcision is considered safe by anti FGC groups, they should advise how to render FGC as safe as male circumcision instead of enforcing the ban against it.","Delete,Claim",,Religious and cultural views
1-23,"In the era of evidence based medicine, level I evidence, derived from either systematic reviews or randomized controlled trials (RCTs), to support the ban against FGC is not available.",,"Add,Claim",Comments,
1-23,"In fact, the design and implementation of a RCT to address the effects of FGC cannot be justified and seems to be unethical.",,"Add,Claim",Comments,
1-23,,Research including reported data about past experiences will always be threatened by the individual’s memory and the influence of exposure status on the recalling process <REF-21> .,"Delete,Fact/Evidence",,Comments
1-23,,The strongest evidence comes from randomized controlled trials followed by cohort studies.,"Delete,Claim",,Comments
1-23,Conclusions,Final remarks,"Modify,Other",Female circumcision: Limiting the harm,Female circumcision: Limiting the harm
1-23,Claims for increased Cesarean deliveries in cut women were attributed to obstructed labor most likely due to excessive scarring at the pelvic outlet probably resulting from the imperfect healing of the genital cutting and possible associated infection.,Claims for increased Cesarean deliveries in cut women were attributed to obstructed labor most likely due to excessive scarring at the pelvic outlet.,"Modify,Claim",Late complications,Late complications
1-23,"However, the high Cesarean rate in this population cannot be attributed solely to obstruction due to excessive outlet scarring; obstructed labor may occur due to a variety of reasons.",The high Cesarean rate in this population cannot be attributed solely to obstruction due to excessive outlet scarring; obstructed labor may occur due to a variety of reasons.,"Modify,Clarity",Late complications,Late complications
1-23,"In light of this fact and in the absence of any scientific evidence to support the practice of female circumcision, the available level III evidence, derived from retrospective studies and studies depended on self-reported FGC and its health consequences, should be taken into consideration in spite of their imprecision and low reliability <REF-19> , <REF-20> .",It is the author’s view that none of these studies hold solid evidence to rely upon.,"Merge+Modify,Claim",Comments,Comments
1-23,"In light of this fact and in the absence of any scientific evidence to support the practice of female circumcision, the available level III evidence, derived from retrospective studies and studies depended on self-reported FGC and its health consequences, should be taken into consideration in spite of their imprecision and low reliability <REF-19> , <REF-20> .",These studies were either of retrospective design or studies depended on self-reported FGC and its health consequences.,"Merge+Modify,Clarity",Comments,Comments
1-23,"In light of this fact and in the absence of any scientific evidence to support the practice of female circumcision, the available level III evidence, derived from retrospective studies and studies depended on self-reported FGC and its health consequences, should be taken into consideration in spite of their imprecision and low reliability <REF-19> , <REF-20> .","Such studies are imprecise and have low reliability <REF-19> , <REF-20> .","Merge+Modify,Clarity",Comments,Comments
2-147,Approval for the use of clinical materials was obtained from the local Ethical Review Committee.,,"Add,Fact/Evidence",Tissues,
2-147,Curcumin was not used to pretreat the cells and explants prior to the addition of IL-1β.,,"Add,Fact/Evidence",Experimental design–monolayer cultures,
2-147,Curcumin and IL-1β were added simultaneously to the cultures.,,"Add,Fact/Evidence",Experimental design–monolayer cultures,
2-147,"Cartilage discs were digested in papain (Sigma-Aldrich, Gillingham, UK) for 16 hours.","Cartilage discs were digested in papain(Sigma-Aldrich, Gillingham, UK) for 16 hours.","Modify,Grammar",DMMB assays,DMMB assays
2-147,Densitometric quantification of MMP-3 bands was performed using ImageJ software.,Densitometric quantification of MMP-3 bands was performed using Image J software.,"Modify,Grammar",Western blot analysis of MMP-3 release,Western blot analysis of MMP-3 release
2-147,"In summary, although this study found that curcumin at concentrations of 25μM and above is cytotoxic to monolayer chondrocytes after five days in culture, lower concentrations effectively antagonize PG and PGE 2 release in vitro and exert a potent anti-inflammatory effect on cartilage explants treated with IL-1β.","In summary, although this study found that curcumin at concentrations of 25μM and above is cytotoxic to monolayer chondrocytes after five days in culture, lower concentrations effectively antagonize PG and PGE2 release in vitro and exert a potent anti-inflammatory effect on cartilage explants treated with IL-1β.","Modify,Grammar",Discussion,Discussion
2-147,The joint tissues were sourced from UK-based abattoirs and veterinary practices.,The joint tissues were sourced from two UK-based abattoirs.,"Modify,Fact/Evidence",Tissues,Tissues
2-147,Animals were euthanized for non-research purposes either in accordance with Welfare of Animals (Slaughter or Killing) Regulations 1995 or the Veterinary Surgeons Act with owner consent.,Animals were euthanized for non-research purposes having been stunned before slaughter for meat in accordance with Welfare of Animals (Slaughter or Killing) Regulations 1995 .,"Modify,Fact/Evidence",Tissues,Tissues
2-147,"Live and dead cells were counted with ImageJ Software (National Institutes of Health, Bethesda, MD) and the percentage of dead cells (expressed as a percentage of the total number of cells present) was calculated at 24 hours, 48 hours and five days for each treatment.","Live and dead cells were counted with Image J Software (National Institutes of Health, Bethesda, MD) and the percentage of dead cells (expressed as a percentage of the total number of cells present) was calculated at 24 hours, 48 hours and five days for each treatment.","Modify,Grammar",Cytotoxicity assays–monolayer chondrocytes,Cytotoxicity assays–monolayer chondrocytes
2-155,"Within each category of chicken purchased, we collected at least four samples of each brand.",,"Add,Fact/Evidence",Sample collection,
2-155,"Over half of all strains collected exhibited resistance to one or more antibiotics: 55%, 58%, 60%, and 76% from conventional, RWA, organic, and kosher chicken samples, respectively.",,"Add,Fact/Evidence",Results,
2-155,"Based on a national survey conducted by the USDA of poultry and hog producers in the United States, use of antibiotics at sub-therapeutic levels for growth promotion is common <REF-35> , <REF-36> .",,"Add,Fact/Evidence",Discussion,
2-155,"One estimate places growth promotion in livestock production as the single largest sector in which antibiotics are used in the US, accounting for 70% of the total of 50 million pounds for the year 2008 <REF-37> .",,"Add,Fact/Evidence",Discussion,
2-155,"Our finding that brands within categories did not differ significantly in the extent of antibiotic resistant E. coli ( Table 1 ) could arise from the fact that individual brands of chicken obtain product from multiple farms whose production practices may differ, obscuring clear patterns associated with individual brands.",,"Add,Claim",Discussion,
2-155,Our ability to detect an effect of brand might also be constrained by low statistical power.,,"Add,Claim",Discussion,
2-155,"Poultry growers use antibiotics both for therapeutic purposes and for growth promotion <REF-33> , <REF-34> .","Antibiotic use is widespread in the production of chicken both for therapeutic and non-therapeutic purposes (e.g., growth promotion).","Modify,Fact/Evidence",Discussion,Discussion
2-155,"The use of antibiotics in poultry production can select for antibiotic-resistant microorganisms including Salmonella , Campylobacter , Enterococcus , and extra-intestinal pathogenic E. coli <REF-38> .","The use of antibiotics in poultry production selects for antibiotic-resistant microorganisms including Salmonella , Campylobacter , Enterococcus , and extra-intestinal pathogenic E. coli <REF-33> .","Modify,Clarity",Discussion,Discussion
2-155,Our final sample size was limited (n=184) but not atypical for the field <REF-48> – <REF-51> .,Our sample size was limited (n=184) but not atypical for the field <REF-43> – <REF-46> .,"Modify,Clarity",Discussion,Discussion
2-155,"Consumers have a range of choices for poultry, including conventional, organic, kosher, and raised without antibiotics (RWA) – designations that are perceived to indicate differences in quality and safety.","Consumers have a range of choices for poultry, including conventional, organic, kosher, and raised without antibiotics (RWA)-designations that are perceived to indicate differences in quality and safety.","Modify,Grammar",Abstract,Abstract
2-155,"The plate was incubated at 37°C for 2 h and then at 44°C for 22 h, along with QA/QC strains ATCC E. coli 35218, Klebsiella pneumoniae , Hafnia alvei , Citrobacter freundii and Serratia plymuthica.","The plate was incubated at 37°C for 2 h and then at 44°C for 22 h, along with QA/QC strains ATCC E. coli 35218, Klebsiella pneumoniae , Hafnia alvei , Citrobacter freundii, and Serratia plymuthica.","Modify,Grammar",Laboratory analyses,Laboratory analyses
2-173,The mechanism by which 1-ABT inhibits esterases is not known.,,"Add,Claim",Inhibition of [M1] formation prevented CYP3A5 mRNA induction by BDP in A549 cells,
2-173,"Because we have not been able to evaluate this mechanism more thoroughly in primary lung cells, particularly from asthmatic subjects, the physiological and/or clinical relevance of the present study in steroid insensitive patients requires further investigation.",,"Add,Claim",Discussion,
2-173,"As such, additional studies using animal models and relevant samples from human patients need to be evaluated in order to conclusively confirm or reject the hypothesis that CYP3A genes are regulated in human subjects, particularly asthmatics, in response to glucocorticoid treatment since current in vitro models remain unexplainably limited in value for such studies.","As such, additional studies using animal models and relevant samples from human patients need to be evaluated in order to conclusively confirm or reject the hypothesis that CYP3A genes are regulated in human lung cells in response to glucocorticoid treatment since current in vitro models remain unexplainably limited in value for such studies.","Modify,Claim",Discussion,Discussion
2-173,"In summary, the data presented herein demonstrate that, in A549 cells, glucocorticoid binding to the glucocorticoid receptor regulates the expression of CYP3A5.","In summary, the data presented herein demonstrate that, in A549 cells, glucocorticoid binding to the glucocorticoid receptor regulates the expression of CYP3A5, and therefore, corroborates the hypothesis that increased metabolism of glucocorticoids may occur in some patients.","Modify,Claim",Discussion,Discussion
2-173,"However, further research is needed to determine if changes in CYP3A5 expression occur in the human respiratory tissue similar to A549 cells, the precise mechanism by which this process occurs, and whether changes in the local metabolism of glucocorticoids by CYP3A5 ultimately impact glucocorticoid efficiency in asthma patients refractory to glucocorticoid treatment.","However, further research is needed to determine if changes in CYP3A5 expression occur in the human respiratory tissue similar to A549 cells, the precise mechanism by which this process occurs, and whether changes in the local metabolism of glucocorticoids by CYP3A5 ultimately impact glucocorticoid efficiency.","Modify,Claim",Discussion,Discussion
2-173,All cells except A549 cells were plated in 12-well plates pre-coated with LHC basal medium (Life Technologies) and cultured in the presence of hydrocortisone.,All cells except A549 cells were plated in 12-well plates pre-coated with LHC basal medium (Life Technologies).,"Modify,Fact/Evidence",Cell culture,Cell culture
2-180,,"Although more research is needed, our results support that rTMS is safe to use in healthy control participants, invaluable in assessment of rTMS effects clinically.","Delete,Claim",,Long-term rTMS does not adversely affect learning in normal subjects
2-180,,"Rather, these processes involve ephrin-A3, ephrin-A5 and members of the ephrin-B family <REF-37> .","Delete,Fact/Evidence",,Long-term rTMS and hippocampal spine density
2-180,We used ephrin-A2 -/- mice because they have previously been shown to have a specific learning deficit <REF-18> .,,"Add,Fact/Evidence",Introduction,
2-180,Thus we aimed to examine a learning-mediated effect of rTMS on dendritic spines.,,"Add,Fact/Evidence",Introduction,
2-180,Randomised littermates were not used because the breeding colony was structured to produce ephrin-A2/A5 double knockout mice for other studies and no WT littermates were obtained.,,"Add,Fact/Evidence",Animals,
2-180,"We chose to stimulate after the task because we hypothesized that rTMS would enhance LTP-like processes, stabilizing new spines, and the associated synaptic connections, that had formed during learning.",,"Add,Claim",Apparatus and procedure,
2-180,"Unlike in cat studies, the coil was not in direct contact with the mouse head but was held as close as possible to the scalp (~1mm).",,"Add,Fact/Evidence",rTMS application,
2-180,The gap between the coil and the head does not attenuate the field because magnetic fields decrease with distance from the source but are not modified by air or biological tissue (e.g. skin/scalp <REF-36> ).,,"Add,Fact/Evidence",rTMS application,
2-180,"Unlike in the cat study, stereotaxic delivery was not attempted because the dimensions of the coil ensured that the field reached the entire dorsal hippocampus, which in the mouse, is relatively large in proportion to total brain size.",,"Add,Fact/Evidence",rTMS application,
2-180,"Because of the lack of understanding of fundamental interactions between rTMS and behaviour, it would be of great interest to perform an exhaustive battery of behavioural tests in healthy wildtype mice (and eventually in animal models of disease) in conjunction with various rTMS protocols.",,"Add,Claim",Long-term rTMS does not adversely affect learning in normal subjects,
2-180,Subsequent anatomical and physiological analyses could then be carried out to elucidate the neural mechanisms of rTMS and gain insight into the treatment of human disease.,,"Add,Claim",Long-term rTMS does not adversely affect learning in normal subjects,
2-180,"Alternatively, the timing of rTMS delivery relative to the behavioural task may have influenced the outcome of our experiments.",,"Add,Claim",Long-term rTMS and hippocampal spine density,
2-180,"Here we stimulated after the task, however rTMS might have been more effective if delivered before.",,"Add,Claim",Long-term rTMS and hippocampal spine density,
2-180,"Because a single session of rTMS increases the size of dendritic spines and may activate silent synapses <REF-2> , this may “prime” the brain for learning.",,"Add,Claim",Long-term rTMS and hippocampal spine density,
2-180,"With such pre-treatment, an effect of rTMS might even have been detected in improved performances on a day to day basis.",,"Add,Claim",Long-term rTMS and hippocampal spine density,
2-180,"Importantly, we are conscious of the limitations of our rodent scaled rTMS delivery device which may have contributed to the lack of effect observed here.",,"Add,Claim",Long-term rTMS and hippocampal spine density,
2-180,"Although our coil had a relevant coil to brain ratio for mice, because of its small size, the intensity of the magnetic field did not reach the magnitude commonly used in humans (6mT compared to 1-2T), raising concern that our stimulation paradigm is not comparable to human rTMS.",,"Add,Claim",Long-term rTMS and hippocampal spine density,
2-180,"However, this raises a more general issue because similar criticism applies to studies that employ larger coils <REF-1> – <REF-3> : although these deliver the same fields used in humans, the focal nature of the stimulation is lost.",,"Add,Claim",Long-term rTMS and hippocampal spine density,
2-180,Additional effort in designing appropriate small animal rTMS coils is urgently needed to improve the construct validity of animal rTMS research.,,"Add,Claim",Long-term rTMS and hippocampal spine density,
2-180,"Secondly, even though traditional rTMS is considered to be focal, magnetic fields of lower intensity are delivered outside of the focal area <REF-33> , raising the possibility that low intensity stimulation may be contributing to therapeutic effects by acting on interconnected brain regions.","Secondly, even though traditional rTMS is considered to be focal, magnetic fields of lower intensity are delivered outside of the focal area, raising the possibility that low intensity stimulation may be contributing to therapeutic effects by acting on interconnected brain regions.","Modify,Fact/Evidence",rTMS application,rTMS application
2-180,"We chose a complex pattern of stimulation that is based on biomimetic principles (described in detail <REF-23> 59.9-ms trains of 20 pulses at 3 different frequencies as follows: 1 min warm-up at 6.71 Hz, 8 min treatment at 10.1 Hz, and 1 min cool down at 6.26 Hz) and has been shown to induce structural changes in mice <REF-23> .","We chose a complex pattern of stimulation because it has been shown to induce structural changes in mice <REF-19> (59.9-ms trains of 20 pulses at 3 different frequencies as follows: 1 min warm-up at 6.71 Hz, 8 min treatment at 10.1 Hz, and 1 min cool down at 6.26 Hz.","Modify,Fact/Evidence",rTMS application,rTMS application
2-180,The pulse was monophasic with a 300µs rise time and 100µs fall time.,"The pulse duration was 200 µs, which is within the range used in human rTMS.","Modify,Fact/Evidence",rTMS application,rTMS application
2-180,This allowed the stimulation coil to be held by the experimenter above the mouse’s head.,"This allowed the stimulation coil to be held by the experimenter above, but not in contact with, the top of the animal’s head for reproducible rTMS delivery in the alert animal (as for cat studies <REF-29> ).","Split+Modify,Clarity",rTMS application,rTMS application
2-180,"We thus delivered reproducible rTMS in the awake animal (as for cat studies <REF-34> , <REF-35> ).","This allowed the stimulation coil to be held by the experimenter above, but not in contact with, the top of the animal’s head for reproducible rTMS delivery in the alert animal (as for cat studies <REF-29> ).","Split+Modify,Clarity",rTMS application,rTMS application
2-180,"Importantly, rTMS induces long term potentiation (LTP) in rodent hippocampus in vitro <REF-2> and several sessions of high-frequency rTMS increases the capacity to induce LTP compared to untreated controls, suggesting it may also regulate metaplasticity <REF-3> , <REF-4> .","Importantly, rTMS induces long term potentiation (LTP) in rodent hippocampus in vitro <REF-2> and several sessions of high-frequency rTMS increases the capacity to induce LTP (metaplasticity) compared to untreated controls <REF-3> , <REF-4> .","Modify,Claim",Introduction,Introduction
2-180,"Although it is difficult to draw conclusions from the null results presented here, the absence of observed behavioural and structural change is consistent with previously reported rTMS specificity for abnormal systems <REF-8> , <REF-23> .","The absence of observed behavioural and structural change is consistent with previously reported rTMS specificity for abnormal systems 8 , <REF-19> .","Modify,Claim",Discussion,Discussion
2-180,The lack of adverse effects in our long-term study suggests that up to 5 weeks of daily sessions of low intensity pulsed magnetic field stimulation at the parameters used in this study appears safe to use in healthy participants.,"Furthermore, the lack of adverse effects in our long term study contributes evidence that rTMS is safe to use in healthy control participants <REF-20> .","Modify,Claim",Discussion,Discussion
2-180,"This is consistent with previous reports that long-term rTMS effects are specific to abnormal brain circuitry: two weeks of rTMS improved visual tracking, visual electrophysiological function and topographical accuracy in a different strain of mice (ephrin-A2A5 -/- double knockouts) with abnormal circuitry but produced no lasting effects in wildtype mice <REF-23> .","This is consistent with previous reports that long-term rTMS effects are specific to abnormal brain circuitry: two weeks of rTMS improved visual tracking, visual electrophysiological function and topographical accuracy in mice with abnormal circuitry but produced no lasting effects in wildtype mice <REF-19> .","Modify,Fact/Evidence",Long-term rTMS does not adversely affect learning in normal subjects,Long-term rTMS does not adversely affect learning in normal subjects
2-180,Long term delivery of pulsed magnetic fields does not alter visual discrimination learning or dendritic spine density in the mouse CA1 pyramidal or dentate gyrus neurons,Long term delivery of pulsed magnetic fields does not improve learning or alter dendritic spine density in the mouse hippocampus,"Modify,Other",,
2-180,"Furthermore, there is a lack of studies assessing cognitive effects of long-term rTMS in patients together with healthy controls, which presents a large gap in knowledge <REF-8> .","To our knowledge, there have been no studies assessing cognitive effects of long-term rTMS in patients and healthy controls, which presents a large gap in knowledge.","Modify,Clarity",Long-term rTMS does not adversely affect learning in normal subjects,Long-term rTMS does not adversely affect learning in normal subjects
2-180,"Importantly, we found similar spine densities in sham wildtype and ephrin-A2 -/- mice, suggesting that spine density is not solely dependent on ephrin-A2, in agreement with the literature <REF-19> – <REF-21> .","Importantly, we found similar spine densities in sham wildtype and ephrin-A2 -/- mice, suggesting that if present, deficits in spines are subtle in ephrin-A2 -/- mice.","Modify,Fact/Evidence",Long-term rTMS and hippocampal spine density,Long-term rTMS and hippocampal spine density
2-180,"In addition, although ephrin-A2 is expressed in the mouse hippocampus throughout life and has been implicated in its topographic organisation <REF-19> , <REF-20> , there is no evidence that it is involved in synaptic plasticity or spine dynamics <REF-21> .","Although ephrin-A2 is expressed in the mouse hippocampus throughout life and has been implicated in its topographic organisation <REF-35> , <REF-36> , to our knowledge, there is no evidence that ephrin-A2 is involved in synaptic plasticity or spine dynamics.","Modify,Clarity",Introduction,Long-term rTMS and hippocampal spine density
2-180,"As such, the null effect of rTMS on dendritic spine density may be attributed to the absence of both a specific spine and learning deficit in both wildtype and ephrin-A2 -/- mice.","As such, the null effect of rTMS on dendritic spine density is in line with our behavioural results and may be attributed to the absence of a specific spine deficit for rTMS to correct in both wildtype and ephrin-A2 -/- mice.","Modify,Claim",Long-term rTMS and hippocampal spine density,Long-term rTMS and hippocampal spine density
2-180,"However, it is surprising that dendritic spine density remains unaffected after long-term stimulation, given our previous results using the same stimulation parameters, demonstrating structural reorganisation in abnormal axon terminals following multiple stimulations, but not a single rTMS session <REF-23> .","However, it is surprising that dendritic spine density remains unaffected after long-term stimulation, given our previous results using the same stimulation parameters, demonstrating structural reorganisation in abnormal axon terminals following multiple, but not single rTMS stimulation sessions <REF-19> .","Modify,Clarity",Long-term rTMS and hippocampal spine density,Long-term rTMS and hippocampal spine density
2-180,"Given the significant structural changes induced in the mouse visual system following repeated stimulation sessions, and evidence for structural changes in the human brain <REF-17> , we hypothesised that a similar long-term rTMS regime in combination with a hippocampus-dependent learning task, would rescue impaired learning strategies previously found in ephrin-A2 -/- mice <REF-18> and alter spine density in the hippocampus.","Given the significant structural changes induced in the mouse visual system following repeated stimulation sessions, we hypothesised that a similar long-term rTMS regime in combination with a hippocampal learning task, would rescue impaired learning strategies previously found in ephrin-A2 -/- mice <REF-17> and alter spine density in the hippocampus.","Modify,Fact/Evidence",Introduction,Introduction
2-180,"We delivered 5 weeks of daily pulsed rTMS stimulation to adult ephrin-A2 -/- and wildtype (C57BI/6j) mice (n=10 per genotype) undergoing a visual learning task and analysed learning performance, as well as spine density, in the dentate gyrus molecular and CA1 pyramidal cell layers in Golgi-stained brain sections.","We delivered 5 weeks of daily pulsed rTMS stimulation to ephrin-A2 -/- and wildtype mice (n=10 per genotype) undergoing a visual learning task and analysed learning performance, as well as spine density, in the dentate gyrus molecular and CA1 pyramidal cell layers in Golgi-stained brain sections.","Modify,Fact/Evidence",Abstract,Abstract
2-180,"Although mice of both genotypes learned the task, their performance remained suboptimal due to lack of motivation to obtain food rewards through insufficient food restriction <REF-22> and neither learning behaviour, nor hippocampal spine density were affected by long term rTMS.","Although the mice learned the task, their performance remained suboptimal due to lack of motivation to obtain food rewards through insufficient food restriction <REF-18> and neither learning behaviour, nor hippocampal spine density were affected by long term rTMS.","Modify,Clarity",Introduction,Introduction
2-180,"Mice were age matched, aged 8–10 weeks old (equivalent to young sexually mature adult in humans) when commencing the experiment.","Mice were age matched, aged 8–10 weeks old when commencing the experiment.","Modify,Fact/Evidence",Animals,Animals
2-180,"For the duration of the study, mice were kept in standard caging in a controlled environment (12/12 light/dark cycle; temperature 22°C±2°C, separated into cages with clear plastic walls (17 cm × 19 cm base, 16 cm high) based on sex and genotype (2–4 per cage)).","For the duration of the study, mice were kept in standard caging in a controlled environment (12/12 light/dark cycle; temperature 22°C±2°C, separated into cages with clear plastic walls (17 cm x 19 cm base, 16 cm high) based on sex and genotype (2–4 per cage)).","Modify,Grammar",Animals,Animals
2-238,The assay results are reproducible in three independent experiments.,,"Add,Fact/Evidence",Deacetylation of nuclear histones,
2-238,Statistical analysis,,"Add,Other",Materials and methods,
2-238,"All values are expressed as mean ± standard deviation and the graphs were generated using Graph-Pad Prism ® (Version 4) for Windows (GraphPad Software, San Diego, California, USA.",,"Add,Fact/Evidence",Statistical analysis,
2-238,"Statistical analysis was performed by one-way analysis of variance (ANOVA), followed by Bonferroni multiple comparison test for all parameters.",,"Add,Fact/Evidence",Statistical analysis,
2-238,Results were considered statistically significant at P < 0.05.,,"Add,Fact/Evidence",Statistical analysis,
2-238,Pathogenic fungi are increasingly responsible for life threatening infections in the elderly and immunocompromised patients.,,"Add,Claim",Discussion,
2-238,"While some species have intrinsic resistance to anti-fungals, others develop resistance during the course of treatment.",,"Add,Claim",Discussion,
2-238,Increasing antifungal resistance and treatment failures in patients is becoming a challenge.,,"Add,Claim",Discussion,
2-238,The Candida genome encodes at least 3 distinct classes of histone deacetylases in addition to sirtuins.,,"Add,Claim",Discussion,
2-238,"There are 8 different histone deacetylases ( HOS1, HOS2, HOS3, HDA1, HDA2, HDA3, RPD3, RPD31 ) which all have distinct roles in the morphogenesis of C. albicans .",,"Add,Claim",Discussion,
2-238,"The fact that, the recombinant Hos2 enzyme did not show any inhibition with the Class I inhibitor MS-275 led us to explore alternate substrates including tubulins, which are substrates for Class II histone deacetylases.",,"Add,Fact/Evidence",Discussion,
2-238,It has been shown that microtubules in the fungal hyphae drive nuclear dynamics and cell cycle progression to morphogenesis <REF-34> .,,"Add,Fact/Evidence",Discussion,
2-238,"In view of the fact that Hos2 seems to preferentially deacetylate tubulins, it would be interesting to see if Hos2 inhibitors would act as anti-fungals, either as a monotherapy or in synergy, with existing anti-tubulin agents such as benomyl, nocodazole etc.",,"Add,Claim",Discussion,
2-238,Data availability,,"Add,Other",Functional characterization of Candida albicans Hos2 histone deacetylase,
2-238,The data referenced by this article are under copyright with the following copyright statement: Copyright: ï¿½ 2014 Karthikeyan G et al.,,"Add,Fact/Evidence",Data availability,
2-238,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",,"Add,Fact/Evidence",Data availability,
2-238,"Enzymatic assay was carried out in triplicates using the fluorogenic Class I HDAC substrate, Boc-Lys (Ac)-AMC (Cat. No. I1875, Bachem AG, Bubendorf, Switzerland).","Enzymatic assay was carried out using the fluorogenic Class I HDAC substrate, Boc-Lys (Ac)-AMC (Cat. No. I1875, Bachem AG, Bubendorf, Switzerland).","Modify,Fact/Evidence",Hos2 deacetylase enzymatic assay,Hos2 deacetylase enzymatic assay
2-238,"Briefly, 0.5 μg of purified recombinant protein in a volume of 10 μl of assay buffer was incubated with 50 μl of appropriate concentration of HDAC inhibitors and 20 μM of substrate at 37°C for 1 hr in 100 μl reaction volume.","Briefly, 0.5 μg of purified recombinant protein in a volume of 10 μl of assay buffer was incubated with 50 μl of appropriate concentration of HDAC inhibitors and 20 μM of substrate at 37°C for 1 hr.","Modify,Fact/Evidence",Hos2 deacetylase enzymatic assay,Hos2 deacetylase enzymatic assay
2-238,"Female BALB/c mice were purchased from The Jackson Laboratory (Bar Harbor, ME) at 4 weeks of age and then housed at the animal facility, Orchid Chemicals and Pharmaceuticals for 2 weeks in a specific-pathogen free facility with a 12 h light cycle (6 am–6 pm) and a 12 h dark cycle (6 pm–6 am).","Female BALB/c mice were purchased from The Jackson Laboratory (Bar Harbor, ME) at 4 weeks of age and then housed at the animal facility, Orchid chemicals and Pharmaceuticals for 2 weeks in a specific-pathogen free facility with a 12 h light cycle (6 am–6 pm) and a 12 h dark cycle (6 pm–6 am).","Modify,Grammar",Production of polyclonal anti sera against Hos2 protein,Production of polyclonal anti sera against Hos2 protein
2-238,"C. albicans ATCC 90028 mycelia (~5 gm wet weight) were washed with water, centrifuged at 10,000 rpm for 10 minutes at 4°C and the mycelial pellet was resuspended in 50 ml of 0.1 mM Tris-HCl, pH 9.4, 10 mM DTT.","C. albicans ATCC 90028 mycelia (~ 5 gm wet weight) were washed with water, centrifuged at 10,000 rpm for 10 minutes at 4°C and the mycelial pellet was resuspended in 50 ml of 0.1 mM Tris-HCl, pH 9.4, 10 mM DTT.","Modify,Grammar",Isolation of histones from Candida sp .,Isolation of histones from Candida sp .
2-238,"Candida albicans is a mucosal commensal organism capable of causing superficial (oral and vaginal thrush) infections in immune normal hosts, but is a major pathogen causing systemic and mucosal infections in immunocompromised individuals.","Candida albicans is a mucosal commensal organism in normal individuals, but is a major pathogen causing systemic and mucosal infections in immunocompromised individuals.","Modify,Claim",Abstract,Abstract
2-238,"Deacetylation assays were carried out in 100 μl reaction volume for 1 hr at 37°C in reaction buffer (50 mM Tris Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 , 1 mg/ml BSA).","Deacetylation assays were carried out for 1 hr at 37°C in reaction buffer (50 mM Tris Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 , 1 mg/ml BSA).","Modify,Fact/Evidence",Deacetylation of nuclear histones,Deacetylation of nuclear histones
2-238,"Deacetylation assays were carried out in 100 μl reaction volume for 3 hr at 37°C in reaction buffer (50 mM Tris-Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 ).","Deacetylation assays were carried out for 3 hr at 37°C in reaction buffer (50 mM Tris-Cl, pH 8.0, 137 mM NaCl, 2.7 mM KCl, 2.5 mM MgCl 2 ).","Modify,Fact/Evidence",Deacetylation of acetylated tubulin,Deacetylation of acetylated tubulin
2-238,Candida albicans is a commensal organism found in the mucosa and gastrointestinal tract of most healthy individuals but can cause superficial (oral and vaginal thrush) infections in immune-normal hosts and severe systemic infection in immunocompromised patients <REF-1> .,"Candida albicans is a commensal organism found in the mucosa and gastrointestinal tract of most healthy individuals but can cause severe systemic/superficial infections, especially in immunocompromised patients <REF-1> .","Modify,Fact/Evidence",Introduction,Introduction
2-238,"HeLa nuclear extract (2 μg) or recombinant Hos2 enzyme (300 ng) was incubated with NAD+ (500 μM), Fluor de Lys ® −Sirt1 (Enzo lifescience), varying concentrations of resveratrol (Cat. No. 0219605205, MP Biomedicals, Ohio, USA) (30, 100, 250 and 500 μM) in presence or absence of the pan-HDAC inhibitor trichostatin, in 50 μl reaction volume at 37°C for 30 min.","HeLa nuclear extract (2 μg) or recombinant Hos2 enzyme (300 ng) was incubated with NAD+ (500 μM), Fluor de Lys ® −Sirt1 (Enzo lifescience), varying concentrations of resveratrol (Cat. No. 0219605205, MP Biomedicals, Ohio, USA) (30, 100, 250 and 500 μM) in presence or absence of the pan-HDAC inhibitor trichostatin, at 37°C for 30 min.","Modify,Fact/Evidence",In vitro Sirt1 activation assay,In vitro Sirt1 activation assay
2-238,The reaction was carried out in triplicate following manufacturer’s protocol (Enzo lifescience).,The reaction was carried out following manufacturer’s protocol (Enzo lifescience).,"Modify,Fact/Evidence",In vitro Sirt1 activation assay,In vitro Sirt1 activation assay
2-238,"The Hos2 enzyme was expressed in the baculoviral-insect cell expression system as a NH 2 -terminal hexa histidine tagged fusion protein, which is detected on Western blot as a ~52 kDa protein using our own polyclonal anti-Hos2 anti-sera raised against Hos2 protein in mice.","The Hos2 enzyme was expressed in the baculoviral-insect cell expression system as a NH 2 -terminal hexa histidine tagged fusion protein, which is detected on Western blot as a ~ 52 kDa protein using our own polyclonal anti-Hos2 anti-sera raised against Hos2 protein in mice.","Modify,Grammar",Protein expression and purification,Protein expression and purification
2-238,SDS-PAGE analysis of purified protein revealed a major band at ~52 kDa ( Figure 1A ).,SDS-PAGE analysis of purified protein revealed a major band at ~ 52 kDa ( Figure 1A ).,"Modify,Grammar",Protein expression and purification,Protein expression and purification
2-238,The total activity with Boc-Lys (ac) AMC showed the enzyme to be active in deacetylating the lysine residue and the activity increased significantly ( P < 0.05 ) with an increase in Hos2 concentration ( Figure 1B ).,The total activity with Boc-Lys (ac) AMC showed the enzyme to be active in deacetylating the lysine residue and the activity increased with an increase in Hos2 concentration ( Figure 1B ).,"Modify,Fact/Evidence",In vitro deacetylation assay using synthetic peptide substrate,In vitro deacetylation assay using synthetic peptide substrate
2-238,"Azole resistance in Candida sp. is mediated by up regulation of genes encoding ERG11 , a lanosterol demethylase <REF-4> – <REF-6> , MDR1 <REF-5> , <REF-7> and by CDR (Candida drug resistance) efflux pumps <REF-5> , <REF-7> – <REF-9> .","Azole resistance in Candida sp. is mediated by up regulation of genes encoding ERG11, a lanosterol demethylase <REF-4> – <REF-6> , MDR1 <REF-5> , <REF-7> and by CDR (Candida drug resistance) efflux pumps <REF-5> , <REF-7> – <REF-9> .","Modify,Grammar",Introduction,Introduction
2-238,"HDAC inhibitors, by virtue of their ability to prevent antifungal resistance in vitro, have been proposed as antifungal adjuvants.",HDAC inhibitors by virtue of their ability to prevent antifungal resistance in vitro have been proposed as antifungal adjuvants.,"Modify,Grammar",Discussion,Discussion
2-238,"Our studies with the Class I HDAC inhibitor MS-275 showed that this inhibitor did not inhibit Hos2 deacetylase as effectively as the pan HDAC inhibitors SAHA or TSA, suggesting that Candida Hos2 is more similar to Class II deacetylases.","Our studies with the class I HDAC inhibitor MS-275 showed that this inhibitor did not inhibit Hos2 deacetylase as effectively as the pan HDAC inhibitors SAHA or TSA, suggesting that Candida Hos2 is more similar to class II deacetylases.","Modify,Grammar",Discussion,Discussion
2-238,"Hos2 in essence resembles the Class II mammalian HDACs, specifically HDAC6 in its preference for tubulin deacetylation.","Hos2 in essence resembles the class II mammalian HDACs, specifically HDAC6 in its preference for tubulin deacetylation.","Modify,Grammar",Discussion,Discussion
2-238,"We did not observe any significant ( P value 0.5317 and 0.4411, in the presence and absence of trichostatin respectively) activation of NAD+ dependent deacetylase activity with the fluor-de-lys substrate.",We did not observe any significant activation of NAD+ dependent deacetylase activity with the fluor-de-lys substrate.,"Modify,Fact/Evidence",Discussion,Discussion
2-238,"For example the HDAC Class I specific inhibitor MS-275 is in advanced clinical trials (clinical trial Nos. NCT00020579, NCT00866333) for several forms of cancer, and the HDAC Class II specific inhibitor ACY-1215 is at an advanced clinical phase (clinical trial Nos. NCT01323751 , NCT01583283 ) for myeloma.","For example the HDAC Class I specific inhibitor MS-275 is in advanced clinical trials (clinical trial Nos. NCT00020579, NCT00866333) for several forms of cancer, and the HDAC class II specific inhibitor ACY-1215 is at an advanced clinical phase (clinical trial Nos. NCT01323751 , NCT01583283 ) for myeloma.","Modify,Grammar",Introduction,Introduction
2-238,"Hos2, a Class I HDAC enzyme plays an important role in gene activation in the yeast Saccharomyces cerevisiae by binding to open reading frames (ORFs) of active genes <REF-20> .","Hos2, a class I HDAC enzyme plays an important role in gene activation in the yeast Saccharomyces cerevisiae by binding to open reading frames (ORFs) of active genes <REF-20> .","Modify,Grammar",Introduction,Introduction
2-238,"Inhibition studies showed that Hos2 is susceptible to pan inhibitors such as trichostatin A (TSA) and suberoylanilide hydroxamic acid (SAHA), but is not inhibited by class I inhibitors such as MS-275.","Inhibition studies showed that Hos2 is susceptible to pan inhibitors such as trichostatin A (TSA) and suberoylanilide hydroxamic acid (SAHA), but is not inhibited by class I inhibitors such MS-275.","Modify,Grammar",Abstract,Abstract
3-189,,"Other species captured at this site included one cownose ray ( Rhinoptera bonasus , DW = 414 mm), one bullnose ray (DW = 458 mm), and four harvestfish ( Peprilus alepidotus ).","Delete,Fact/Evidence",,Results
3-189,,This site was 15670.63 m from the nearest inlet and 247.08 m from the nearest mapped seagrass bed.,"Delete,Fact/Evidence",,Results
3-189,,"Spiny dogfish feed primarily on schooling pelagic fishes ( Link et al. , 2002 ), and the harvestfish co-occurring with them in Core Sound may represent a potential food source within this estuary.","Delete,Claim",,Discussion
3-189,,Little is currently known about spiny dogfish habits within estuarine waters.,"Delete,Claim",,Discussion
3-189,,Spiny dogfish observed during this survey penetrated relatively far into the estuary (6–15 km from the nearest inlet) and were captured close to seagrass habitat areas.,"Delete,Fact/Evidence",,Discussion
3-189,,"Sharks can exert top-down influences that can have far-reaching direct and indirect effects on the ecology of estuarine environments ( Heithaus et al. , 2012 ).","Delete,Fact/Evidence",,Discussion
3-189,,Determining whether spiny dogfish are ecologically important within North Carolina inshore waters will require further observation.,"Delete,Claim",,Discussion
3-189,,"Sampling locations were chosen with the goal of sampling three different habitat types; seagrass beds, shallow sand flats, and deep channels.","Delete,Fact/Evidence",,Methods
3-189,,"Longline gear consisted of a 274.32 m mainline 6.35 mm in diameter with 50 gangions comprised of a longline clip with a swivel, a 1 m leader of 136.08 kg test monofilament line, and a size 12/0 circle hook, attached at 5–7 m intervals.","Delete,Fact/Evidence",,Methods
3-189,,"Where space allowed, both gears were deployed within 100 m of each other and allowed to soak simultaneously; otherwise only one of the gear types was deployed.","Delete,Fact/Evidence",,Methods
3-189,,Stations where dogfish were captured were approximately 6.5-15.7 km from the nearest inlet and 43.4-247.1 m from the nearest seagrass bed.,"Delete,Fact/Evidence",,Abstract
3-189,,Distance from the nearest inlet and distance from the nearest mapped seagrass bed were calculated by plotting the sampling locations in ArcGIS 10.1 and measuring the straight-line distance (m) between the sampling stations and those geographic features.,"Delete,Fact/Evidence",,Methods
3-189,,"Mapped seagrass locations were taken from ArcGIS shapefiles of submerged aquatic vegetation generated by the Albemarle-Pamlico National Estuary Partnership ( APNEP, 2008 ).","Delete,Fact/Evidence",,Methods
3-189,,"All batioids were identified, and sex and disc width (DW, mm) were recorded for each individual.","Delete,Fact/Evidence",,Methods
3-189,,"All other bycatch organisms were identified, counted, and released.","Delete,Fact/Evidence",,Methods
3-189,,"A bluntnose stingray ( Dasyatis say , DW = 450 mm) and a bullnose ray ( Myliobatis freminvillii , DW = 458 mm) were also captured in this set.","Delete,Fact/Evidence",,Results
3-189,,Spatial analysis showed that this site was 6526.40 m from the nearest inlet and 43.43 m from the nearest mapped seagrass area.,"Delete,Fact/Evidence",,Results
3-189,"A depth of 1.8 m, temperature of 24.2°C, salinity of 33.4 ppt, and dissolved oxygen of 6.9 mg/L were recorded at this site.","A depth of 1.77 m, temperature of 24.2°C, salinity of 33.4 ppt, and dissolved oxygen of 6.88 mg/L were recorded at this site.","Modify,Fact/Evidence",Results,Results
3-189,"In the vicinity of Cape Fear, North Carolina, Thorpe & Beresoff, (2000) captured spiny dogfish in commercial gillnet gear from December–April, though the sharks were most abundant in February and March and at temperatures less than 13.9°C.","In the vicinity of Cape Fear, North Carolina, Thorpe & Beresoff, (2000) captured spiny dogfish in commercial gillnet gear from December-April, though the sharks were most abundant in February and March and at temperatures less than 13.9°C.","Modify,Grammar",Discussion,Discussion
3-189,"Spiny dogfish occurring between Cape Hatteras and the Gulf of Maine mostly occurred in North Carolina waters during winter and spring, and were distributed between New England and Canadian waters during summer and autumn ( Campana et al. , 2007 ; Stehlik, 2007 ).","Spiny dogfish occurring between Cape Hatteras and the Gulf of Maine mostly occurred in North Carolina waters during winter and spring, and were distributed between New England and Canadian waters during summer and autumn ( Campana et al. , 2007 , Stehlik, 2007 ).","Modify,Grammar",Discussion,Discussion
3-189,"The NMFS seasonal trawl surveys only sample North Carolina waters during the early spring and autumn and may not account for spiny dogfish occurring in the area at other times of the year, but migration out of southern waters in spring has also been suggested by gillnet and longline surveys capable of capturing sharks year-round ( Thorpe & Beresoff, 2000 ; Thorpe et al. , 2004 ; Ulrich et al. , 2007 ), as well as acoustic telemetry ( Rulifson et al. , 2012 ).","The NMFS seasonal trawl surveys only sample North Carolina waters during the early spring and autumn and may not account for spiny dogfish occurring in the area at other times of the year, but migration out of southern waters in spring has also been suggested by gillnet and longline surveys capable of capturing sharks year-round ( Thorpe & Beresoff, 2000 , Thorpe et al. , 2004 , Ulrich et al. , 2007 ), as well as acoustic telemetry ( Rulifson et al. , 2012 ).","Modify,Grammar",Discussion,Discussion
3-189,"Previous studies have shown that spiny dogfish migration and habitat use patterns may be more complex than previously thought ( Campana et al. , 2007 ; Rulifson & Moore, 2009 ; Sulikowski et al. , 2010 ).","Previous studies have shown that spiny dogfish migration and habitat use patterns may be more complex than previously thought ( Campana et al. , 2007 , Rulifson & Moore, 2009 , Sulikowski et al. , 2010 ).","Modify,Grammar",Discussion,Discussion
3-189,"Spiny dogfish south of Cape Hatteras tend to occur in shallower water closer to shore than conspecifics north of Cape Hatteras ( Rulifson & Moore, 2009 ; Rulifson et al. , 2012 ).","Spiny dogfish south of Cape Hatteras tend to occur in shallower water closer to shore than conspecifics north of Cape Hatteras ( Rulifson & Moore, 2009 , Rulifson et al. , 2012 ).","Modify,Grammar",Introduction,Introduction
3-189,"Acoustic telemetry data suggest that these sharks are part of the population that migrates between Cape Hatteras and Cape Cod ( Rulifson et al. , 2012 ), and seem to occupy southern waters between November and April ( Ulrich et al. , 2007 ; Rulifson et al. , 2012 ).","Acoustic telemetry data suggest that these sharks are part of the population that migrates between Cape Hatteras and Cape Cod ( Rulifson et al. , 2012 ), and seem to occupy southern waters between November and April ( Ulrich et al. , 2007 , Rulifson et al. , 2012 ).","Modify,Grammar",Introduction,Introduction
3-189,"Despite this consistent behavior among acoustically tagged sharks, Rulifson et al. , (2012) reported the capture of several spiny dogfish by hook and line at Cape Lookout on June 1, 2010, long after the end of the overwintering period for this species, though environmental measurements were not recorded.","Despite this consistent behavior among acoustically tagged sharks, Rulifson et al. , (2012) captured several spiny dogfish by hook and line at Cape Lookout on June 1, 2010, long after the end of the overwintering period for this species.","Modify,Fact/Evidence",Introduction,Introduction
3-189,"All dogfish were females measuring 849-905 mm total length, well over the size at 50% maturity.","All dogfish were females over the size at maturity, and were caught at stations 1.77-2.74 m in depth, with temperatures 22.9-24.2 °C, 32.8-33.4 ppt salinity, and 6.9-8.0 mg/L dissolved oxygen.","Split+Modify,Fact/Evidence",Abstract,Abstract
3-189,"Dogfish were caught at stations 1.8-2.7 m in depth, with temperatures 22.9-24.2 °C, 32.8-33.4 ppt salinity, and 6.9-8.0 mg/L dissolved oxygen.","All dogfish were females over the size at maturity, and were caught at stations 1.77-2.74 m in depth, with temperatures 22.9-24.2 °C, 32.8-33.4 ppt salinity, and 6.9-8.0 mg/L dissolved oxygen.","Split+Modify,Fact/Evidence",Abstract,Abstract
3-189,Sharks were captured using bottom-set gillnet gear.,Sharks were captured using bottom-set longline and gillnet gear.,"Modify,Fact/Evidence",Methods,Methods
3-189,"Gillnet gear measured 50 m in length and 2.4 m in height, and was comprised of eight panel sections of monofilament mesh measuring 7.5, 10, 12.3, 15.5, 17.1, 21, 25.6, and 31 cm stretched, respectively, and was soaked for 30–60 minutes.","Gillnet gear measured 50 m in length and 2.4 m in height, and was comprised of eight panel sections of monofilament mesh measuring 7.5, 10, 12.3, 15.5, 17.1, 21, 25.6, and 31 cm stretched, respectively.",Merge+Identical,Methods,Methods
3-189,"Gillnet gear measured 50 m in length and 2.4 m in height, and was comprised of eight panel sections of monofilament mesh measuring 7.5, 10, 12.3, 15.5, 17.1, 21, 25.6, and 31 cm stretched, respectively, and was soaked for 30–60 minutes.",Both gears were soaked for 30–60 minutes.,"Merge+Modify,Clarity",Methods,Methods
3-189,"All captured sharks were identified to species and sex, fork length (FL, mm), and stretched total length (TL, mm) were recorded.","All captured sharks were identified to species and sex, fork length (FL, mm), and total length (TL, mm) were recorded.","Modify,Fact/Evidence",Methods,Methods
3-190,,"For R. terraenovae all age estimates (direct and backtransformed) were over double theoretical maximum longevities (7.1 and 6.9 years females and males respectively) from Loefer & Sedberry, 2003 ( Table 2 ).","Delete,Fact/Evidence",,Age estimates and comparisons
3-190,,"The backtransformed maximum longevity was below the theoretical maximum longevities (19.0 years, Driggers et al. , 2004 ) for female C. acronotus , but above theoretical longevity (16.4 years) for males ( Table 2 ).","Delete,Fact/Evidence",,Age estimates and comparisons
3-190,"From 1993 to 1998 a total of 3,419 R. terraenovae were tagged and released, of these 155 were recaptured.",,"Add,Fact/Evidence",Rhizoprionodon terraenovae,
3-190,"From 1993 to 2013 a total of 1,537 C. acronotus were tagged and released, of these 24 were recaptured.",,"Add,Fact/Evidence",Carcharhinus acronotus,
3-190,Three C. acronotus were recaptured (two males and one female) with times at liberty ranging from 10.9 to 12.8 years (mean ± S.D. = 11.9 ± 1.0).,Three C. acronotus were recaptured (two males and one female ).,Merge+Identical,Carcharhinus acronotus,Carcharhinus acronotus
3-190,Three C. acronotus were recaptured (two males and one female) with times at liberty ranging from 10.9 to 12.8 years (mean ± S.D. = 11.9 ± 1.0).,Time at liberty ranged from 10.9 to 12.8 years (mean ± S.D. = 11.9 ± 1.0).,"Merge+Modify,Clarity",Carcharhinus acronotus,Carcharhinus acronotus
3-190,Age estimates from vertebral sections ranged from 14.5 to 20.5 years ( Figure 2 ).,Age estimates from vertebral sections ranged from 14.5 to 20.5 years.,"Modify,Fact/Evidence",Carcharhinus acronotus,Carcharhinus acronotus
3-190,"A review of published shark age and growth studies shows that, in most studies, females have a greater or equal longevity than males (e.g. Carlson & Baremore, 2003 ; Carlson & Parsons, 1997 ; Carlson et al. , 1999 ; Driggers et al. , 2004 ; Drymon et al. , 2006 ; Frazier et al. , 2014 ).","A review of published shark age and growth studies shows that, in most studies, that females have a greater or equal longevity than males (e.g. Carlson & Baremore, 2003 ; Carlson & Parsons, 1997 ; Carlson et al. , 1999 ; Driggers et al. , 2004 ; Drymon et al. , 2006 ; Frazier et al. , 2014 ).","Modify,Grammar",Discussion,Discussion
3-190,"Therefore, we believe that we did not sample older females and suggest that the longevity estimates we observed for each species be applied to males and females.","Therefore, we believe that we did not sample older females and suggest that the longevity estimates maximum observed longevity we observed for each species be applied to males and females.","Modify,Clarity",Discussion,Discussion
3-190,The fact that eight were encountered and all exceeded published longevity estimates lends support to this assertion.,The fact that seven were encountered and all exceeded published longevity estimates lends support to this assertion.,"Modify,Fact/Evidence",Discussion,Discussion
3-190,"Based on tag-recapture data and direct age estimates, herein we report on the longevity of Atlantic Sharpnose Sharks Rhizoprionodon terraenovae (Richardson, 1836) and Blacknose Sharks Carcharhinus acronotus (Poey, 1860), both of which are common in the coastal waters off the southeastern United States.","Based on tag-recapture data and direct age estimates, herein we report on the longevity of Atlantic Sharpnose Rhizoprionodon terraenovae (Richardson, 1836) and Blacknose Carcharhinus acronotus (Poey, 1860) Sharks, both of which are common in the coastal waters off the southeastern United States.","Modify,Clarity",Introduction,Introduction
3-190,Time-at-liberty ranged from 7.7-14.0 years (mean =10.1) for R. terraenovae and 10.9-12.8 years (mean =11.9) for C. acronotus .,Time-at-liberty ranged from 7.7-12.1 years (mean =9.2) for R. terraenovae and 10.9-12.8 years (mean =11.9) for C. acronotus .,"Modify,Fact/Evidence",Abstract,Abstract
3-190,"In the case of recaptured specimens from SCDNR, a IACUC protocol approved for graduate students who had previously worked with SCDNR on elasmobranch studies was followed; the vertebral column was severed by serrated knife in two cervical locations.","In the case of recaptured specimens from SCDNR, a IACUC protocol approved for graduate students who had previously worked with SCDNR on elasmobranch studies was followed; the vertebral column was served by serrated knife in two cervical locations.","Modify,Grammar",Methods,Methods
3-190,Five R. terraenovae (four male and one female) were recaptured with times at liberty ranging from 7.7 to 14.0 years (mean ± S.D. = 10.1 ± 2.7).,Four R. terraenovae (three male and one female) were recaptured with times at liberty ranging from 7.7 to 12.1 years (mean ± S.D. = 9.2 ± 2.0).,"Modify,Fact/Evidence",Rhizoprionodon terraenovae,Rhizoprionodon terraenovae
3-190,Vertebrae were sampled from Shark L5242 and the direct age estimate from the vertebral section was 18.5 years old ( Figure 1 ).,Vertebrae were sampled from Shark L5242 and the direct age estimate from the vertebral section was 18.5 years old.,"Modify,Fact/Evidence",Rhizoprionodon terraenovae,Rhizoprionodon terraenovae
3-190,All five sharks were recaptured within 15 km of initial tagging.,All four sharks were recaptured within 15 km of initial tagging.,"Modify,Fact/Evidence",Rhizoprionodon terraenovae,Rhizoprionodon terraenovae
7-1030,"A related alternative funding system is to fund all applications where reviewers agree on a high score, and then allocate the remaining budget at random where the reviewers disagreed but some reviewers gave the application a high score <REF-6> .",,"Add,Fact/Evidence",Introduction,
7-1030,To graphically examine associations we used scatterplots of the total relative citations against the application score statistics.,,"Add,Fact/Evidence",Statistical analyses,
7-1030,"If a disagreement in scores indicates a high-risk high-return project, then there may be a greater variation in citations, with more unusually low and high citations for larger disagreements in scores.",,"Add,Claim",Statistical analyses,
7-1030,To examine this we plotted the estimated inter-quartile range in total relative citations by grouping applications using a scatterplot smoothing span <REF-23> .,,"Add,Fact/Evidence",Statistical analyses,
7-1030,"We used a span of 20 applications which was based on trial and error, and weighted the estimated inter-quartile ranges using a Gaussian kernel <REF-23> .",,"Add,Fact/Evidence",Statistical analyses,
7-1030,"We used the inter-quartile range instead of the standard deviation because of the strong skew in citations, and the standard deviation was strongly influenced by the application with the highest citations.",,"Add,Fact/Evidence",Statistical analyses,
7-1030,Regression model,,"Add,Other",Methods,
7-1030,The inter-quartile ranges in total relative citations by the application scores’ mean and standard deviation are in Figure 4 .,,"Add,Fact/Evidence",Results,
7-1030,There was a general reduction in the inter-quartile range as the application score mean increased.,,"Add,Fact/Evidence",Results,
7-1030,"The interquartile range also reduced somewhat as the application score standard deviation increased, although the reduction was not as clear as that for the mean.",,"Add,Fact/Evidence",Results,
7-1030,"It may be possible to measure disagreement using an observer who watches the panel dynamics <REF-30> , <REF-31> .",,"Add,Fact/Evidence",Discussion,
7-1030,"We only had summary statistics on the application scores and hence we could not examine the distribution of scores to look for interesting patterns such as bimodality in scores, indicating a strong split in the peer review panel.",,"Add,Claim",Limitations,
7-1030,This indicates the largest disagreement is where at least one panel member has given a poor score (remembering that the best possible score is 1.0 and the worst 5.0).,This indicates the largest disagreement is where at least one panel member has given a poor score (remembering lower scores are better).,"Modify,Fact/Evidence",Results,Results
7-1030,There was a reduction in citations for applications with a worse mean score.,There was a reduction in citations for applications with a higher (worse) mean score.,"Modify,Clarity",Results,Results
7-1030,"We found a statistically significant association between an application’s mean score and subsequent citations, with the result in the expected direction because applications with better scores had more citations (on average).","We found a statistically significant association between an application’s mean score and subsequent citation counts, with the result in the expected direction because applications with better scores had more citations (on average).","Modify,Clarity",Discussion,Discussion
7-1030,"A recent literature review found there are many unanswered questions in funding peer review, and concluded, “there is a need for open, transparent experimentation and evaluation of different ways to fund research” <REF-2> .","A recent systematic review found there are many unanswered questions in funding peer review, and concluded, “there is a need for open, transparent experimentation and evaluation of different ways to fund research” <REF-2> .","Modify,Clarity",Introduction,Introduction
7-1030,"Indeed, some recent research has indicated that there is more variability in scores across reviewers than across proposals <REF-34> and previous studies indicate inter-rater reliability as very low <REF-35> .","Indeed, some recent research has recently indicated that there is more variability in score across reviewers than across proposals <REF-31> and previous results indicate inter-rater reliability as very low <REF-32> .","Modify,Clarity",Discussion,Discussion
7-1030,"However, the scatter-plots in Figure 3 show no sign of an increasing variance in citations for higher standard deviations or ranges, and the inter-quartile ranges in citations in Figure 4 show a slight decrease for greater disagreements.","However, the scatter-plots in Figure 3 show no sign of an increasing variance in citations for higher standard deviations or ranges.","Modify,Fact/Evidence",Discussion,Discussion
7-1030,"Studies that follow funded and unfunded fellowship applicants are possible, e.g., Bornmann et al (2008) <REF-39> , but this is very difficult when examining projects that need specific funding <REF-40> .","Studies that follow funded and unfunded fellowship applicants are possible, e.g., <REF-36> , but this is very difficult when examining projects that need specific funding <REF-37> .","Modify,Fact/Evidence",Limitations,Limitations
7-1030,An earlier systematic review similarly concluded that studies to examine the accuracy and soundness of funding peer review are “urgently needed” <REF-3> .,A prior systematic review similarly concluded that studies to examine the accuracy and soundness of funding peer review are “urgently needed” <REF-3> .,"Modify,Clarity",Introduction,Introduction
7-1030,It would be useful to examine whether reviewer disagreement is associated with research impact in other funding schemes.,It would be useful to examine whether reviewer disagreement is associated with research impact in other funding schemes and in larger sample sizes.,Split+Identical,Limitations,Limitations
7-1030,"It would also be useful to repeat the study in larger sample sizes, particularly because any conclusions could be influenced by a small proportion of applications that have a very high pay-off.",It would be useful to examine whether reviewer disagreement is associated with research impact in other funding schemes and in larger sample sizes.,"Split+Modify,Claim",Limitations,Limitations
7-1030,Whilst a systematic review of innovations focused specifically on studies aiming to improve the effectiveness and efficiency in peer review funding found only eight studies and called for more studies of peer review <REF-4> .,Whilst a systematic review of innovations for effectiveness and efficiency in peer review funding found only eight studies and called for more studies <REF-4> .,"Modify,Clarity",Introduction,Introduction
7-1030,"A recent literature review found “suggestive” evidence that funding peer review can have an anti-innovation bias <REF-2> , whilst a survey of applicants and reviewers found that innovation and risk may not often be sufficiently addressed in review feedback <REF-8> .",A recent systematic review found “suggestive” evidence that funding peer review can have an anti-innovation bias <REF-2> and that innovation and risk may not often be sufficiently addressed in review feedback <REF-7> .,"Modify,Fact/Evidence",Introduction,Introduction
7-1030,Some researchers feel they need to write conservative applications that please all members of the panel to achieve a good mean score <REF-10> .,Some researchers feel they need to write conservative applications that please all members of the panel to achieve a good average score <REF-9> .,"Modify,Clarity",Introduction,Introduction
7-1030,Many studies using large sample sizes found either no association or only a weak association between the mean score and the number of citations of subsequent publications <REF-13> – <REF-17> .,Many studies using large sample sizes found either no association or only a weak association between the mean score and the citations of subsequent publications <REF-12> – <REF-16> .,"Modify,Clarity",Introduction,Introduction
7-1030,"Other studies have shown a positive association between better mean peer review scores and increased citations <REF-18> , <REF-19> , including a study that used the same data analyzed here <REF-20> .","Other studies have shown a positive association between higher mean peer review scores and increased citations <REF-17> , <REF-18> , including a study that used the same data analyzed here <REF-19> .","Modify,Clarity",Introduction,Introduction
7-1030,In this study we also consider statistics that measure within-panel disagreement which are the standard deviation and the range (largest minus smallest score).,"In this study we also consider statistics that measure within-panel disagreement, which are the standard deviation and the range (largest minus smallest score).","Modify,Grammar",Application data,Application data
7-1030,"These published average rates were determined for 2000 to 2010 by scientific field, assessed in 2011 and displayed a linear relationship with time (e.g., R 2 = 0.99 for the field of molecular biology).","These published average rates were determined for 2000–2010 by scientific field, assessed in 2011 and displayed a linear relationship with time (e.g., R 2 = 0.99 for the field of molecular biology).","Modify,Grammar",Citation counts,Citation counts
7-1235,The code and processed data are open sourced and available on github and contains a tutorial built into the application for assisting users.,The code and processed data is open sourced and available on github and with a tutorial built into the application for assisting users.,"Modify,Clarity",Abstract,Abstract
7-1235,"The interactive plots are made using shiny (v1.1.0) and ggplot2 (v3.0.0). Plots can be downloaded as .png, .pdf, or .svg files. Data used to generate the individual plots can be downloaded as .csv files.","The interactive plots are made using shiny (v1.1.0) and ggplots2 (3.0.0). Plots can be downloaded as .png, .pdf, or .svg files. Data used to generate the individual plots can be downloaded as .csv files.","Modify,Clarity",Implementation,Implementation
7-1235,Kaplan-Meier curves are generated using the survival (v2.41-3) and the survminer (v0.4-1) R packages.,Kaplan-Meier curves are generated using the survival (v2.41-3) and the survminer (0.4-1) R packages.,"Modify,Clarity",Implementation,Implementation
7-1235,"Hazard ratios for two-group comparisons, either median or optimal cut-off, utilize the Cox proportional hazards regression model in the survival R package; with the reported hazard ratio comparing high versus low protein groups.","Hazard ratio for two-group comparisons, either median or optimal cut-off, utilize the Cox proportional hazards regression model in the survival R package; with the reported hazard ratio comparing high versus low protein groups.","Modify,Grammar",Implementation,Implementation
7-1235,Improving prognostic prediction and the identification of potential therapeutic targets is of particular interest to clinicians.,Improving prognostic predictions and the identification of potential therapeutic targets is of particular interest to clinicians.,"Modify,Grammar",Introduction,Introduction
7-1235,Clinical variables dependent on the cancer type selected can be used to filter patients into user-defined groupings.,"Clinical variables dependent on the cancer type selected, can be used to filter patients into user-defined groupings.","Modify,Grammar",Implementation,Implementation
7-1235,Hazard ratios and p-values are based on the Cox regression model.,Hazard ratios and P values are based on the Cox regression model.,"Modify,Grammar",Implementation,Implementation
7-1235,"Quantification of messenger RNA at a genome-wide level has proven valuable in the discovery of gene expression profiles, which can serve as biomarkers for clinical outcomes in cancer <REF-1> .","Quantification of messenger RNA levels at a genome-wide level has proven valuable in the discovery of gene expression profiles, which can serve as biomarkers for clinical outcomes in cancer <REF-1> .","Modify,Clarity",Introduction,Introduction
7-1235,"In order to demonstrate the functionality of TRGAted, we present a basic survival analysis examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.","In order to demonstrate the functionality of TRGAted, we present a basic survival analysis of examining the aggressive, highly-metastatic subtype of breast cancer, known as basal-like breast cancer.","Modify,Grammar",Use case,Use case
7-1235,"Samples can be divided into quartiles, tertiles, median or optimally for p-values based on the protein of interest ( Figure 2C ).","The division of samples is available into quartiles, tertiles, median or optimum based on the protein of interest ( Figure 2C ).","Modify,Clarity",Use case,Use case
7-1235,Reverse-phase protein arrays (RPPAs) are a highthroughput approach to protein quantification utilizing antibody-based micro-to-nano scale dot blot.,Reverse-phase protein arrays (RPPAs) are a highthroughput approach to protein quantification utilizing an antibody-based micro-to-nano scale dot blot.,"Modify,Grammar",Abstract,Abstract
7-1235,"Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low protein groups ( Figure 3B ).","Having selected the optimal cutoff feature, a bar chart can also be generated to examine the proportion of samples in the high and low proportion groups ( Figure 3B ).","Modify,Fact/Evidence",Use case,Use case
7-1235,Protein labeling is adaptive for both the volcano plot and bar chart and will only label significant proteins (p-value ≤ 0.05).,Protein labeling is adaptive for both the volcano plot and bar chart and will only label significant proteins.,"Modify,Fact/Evidence",Use case,Use case
7-1235,"Here, RAD50 predicts poor survival in only five cancer types, prostate, adrenocortical, breast cancer, low-grade glioma, and head and neck cancers ( Figure 4A ).","Here, RAD50 predicts poor survival in only five cancer types, prostate, adrenocortical, breast cancer, low-grade glioma and head and neck cancers ( Figure 4A ).","Modify,Grammar",Use case,Use case
7-1235,The availability of protein-level quantifications for the TCGA cohort allows for more relevant clinical outcome predictions compared to mRNA levels.,The availability of protein-level quantification for the TCGA cohorts allow for more relevant clinical outcome predictions compared to mRNA levels.,"Modify,Grammar",Introduction,Introduction
7-1235,"Built on the R shiny framework, a literate code architecture, the code for TRGAted is annotated and easily modified from our GitHub repository.","Built on the R Shiny framework, a literate code architecture, the code for TRGAted is annotated and easily modified from our GitHub repository.","Modify,Grammar",Conclusions,Conclusions
7-1235,"Currently, TCGA-based applications provide entry-level analysis in correlational, differential, and survival modalities for the RPPA information.","Currently available applications provide entry-level analysis in correlational, differential, and survival modalities for the RPPA information.","Modify,Clarity",Introduction,Introduction
7-1235,"Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor and metastatic samples.","Within the Cancer Genome Atlas (TCGA), RPPAs were used to quantify over 200 proteins in 8,167 tumor or metastatic samples.","Modify,Grammar",Abstract,Abstract
7-1235,"Clinical and survival information for each cancer data set was downloaded from recent work by Liu, et al. <REF-5> .",Clinical and survival information for each cancer data set were downloaded from recently updated TCGA clinical data <REF-5> .,"Modify,Fact/Evidence",Protein and survival data,Protein and survival data
7-1235,Protein-level data has particular advantages in assessing putative prognostic or therapeutic targets in tumors.,This protein-level data has particular advantages in assessing putative prognostic or therapeutic targets in tumors.,"Modify,Grammar",Abstract,Abstract
7-1235,"Unlike other cancer types, metastatic samples were retained for skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.","Unlike other cancer types, metastatic samples were kept in the skin cutaneous melanoma (SKCM) RPPA-based dataset due to the highly metastatic nature of the disease.","Modify,Clarity",Protein and survival data,Protein and survival data
7-1235,"We developed a cloud-based application, TRGAted to enable researchers to better examine patient survival based on single or multiple proteins across 31 cancer types in the TCGA.","We developed a cloud-based application, TRGAted to enable researchers to better examine survival based on single or multiple proteins across 31 cancer types in the TCGA.","Modify,Clarity",Abstract,Abstract
7-1306,,The percent expression metric shows whether a reasonable fraction of cells expresses the gene.,"Delete,Fact/Evidence",,Visualize cluster markers
7-1306,Users may upload a single expression file and specify whether the rows represent genes and the columns represent cells or vice-versa.,,"Add,Fact/Evidence",Setup analysis,
7-1306,"Additionally, this notebook supports the three-file 10X output format, allowing users to upload the matrix, genes, and barcodes files.",,"Add,Fact/Evidence",Setup analysis,
7-1306,Any of those inputs can also be provided as .zip files.,,"Add,Fact/Evidence",Setup analysis,
7-1306,"To use the mitochondrial gene filter, the user must supply their data with gene names in HGNC format with “MT-” prepended to each mitochondrial gene name.",,"Add,Fact/Evidence",Setup analysis,
7-1306,"As there is debate in the field concerning the correctness of using regression on covariates such as percent mitochondrial reads ( Batson, 2018 ) we have made this step optional.",,"Add,Fact/Evidence",Preprocess counts,
7-1306,"We note that this notebook is a living, open source document and can be modified as the single cell community’s perspectives on best practices evolves.",,"Add,Claim",Preprocess counts,
7-1306,We encourage users to perform analyses on their own data using this notebook.,,"Add,Claim",Conclusion,
7-1306,We note that all the required libraries are already installed on the public GenePattern Notebook server at https://notebook.genepattern.org .,,"Add,Fact/Evidence",Conclusion,
7-1306,"This resource is freely available to the community and the analysis described in this notebook falls well within the per-account memory allocations (see the Scanpy authors’ benchmarking in Wolf et al ., 2018 ; Eulenberg et al ., 2017a ; Eulenberg et al. , 2017b ).",,"Add,Fact/Evidence",Conclusion,
7-1306,"To analyze larger datasets that exceed the per-user memory allocation on the public notebook server, users should deploy the open source GenePattern Notebook server using their own computational resources as described in Reich et al ., 2017 .",,"Add,Fact/Evidence",Conclusion,
7-1306,"The GenePattern Notebook server is available as the genepattern-notebook package through the pip ( https://pypi.org/project/genepattern-notebook/ ) or conda ( https://anaconda.org/genepattern/genepattern-notebook ) package managers, or as a Docker image ( https://hub.docker.com/r/genepattern/genepattern-notebook ).",,"Add,Fact/Evidence",Conclusion,
7-1306,"For example, future notebook releases may include quality control methods such as doublet detection ( McGinnis et al ., 2018 ) as well as visualization methods such as UMAP ( Becht et al ., 2019 ), which is growing in popularity in the single cell community.",,"Add,Claim",Conclusion,
7-1306,"We also encourage advanced users to copy the notebook, add new approaches or features, and publish them as a community notebook in the GenePattern Notebook repository.",,"Add,Claim",Conclusion,
7-1306,,Each row of the matrix should represent a gene and each column represents a cell.,"Delete,Fact/Evidence",,Setup analysis
7-1306,"Additional statistical information about each gene is provided in interactive plots, such as the log-fold change comparing the average expression of a gene in one cluster versus the average expression in all other cells, the percentage of cells within the cluster that express the gene, and the percentage of cells in other clusters that express the gene.","Additional statistical information about each gene is provided as an interactive table, such as the log-fold change comparing the average expression of a gene in one cluster versus the average expression in all other cells, the percentage of cells within the cluster that express the gene, and the percentage of cells in other clusters that express the gene.","Modify,Clarity",Visualize cluster markers,Visualize cluster markers
7-1306,First the data can be exported as a set of CSV (comma separated values) files suited for further independent analysis and data sharing.,"First, as a set of CSV (comma separated values) files suited for further independent analysis and data sharing.","Modify,Clarity",Export analysis data,Export analysis data
7-1306,"The data can also be exported as an H5AD file that can be re-imported into this notebook’s workflow, retaining the generated results.","Second, as an H5AD file that can be re-imported into this notebook’s workflow, retaining the generated results.","Modify,Clarity",Export analysis data,Export analysis data
7-1306,"To run this notebook, the user needs a GenePattern account or can create one on the GenePattern Notebook site .","To run the notebook, the user is required to have a GenePattern account that can be created on the GenePattern Notebook site .","Modify,Clarity",Operation,Operation
7-1306,"As the GenePattern Notebook user interface gains more features, the notebook will also be able to take advantage of these features.","As the GenePattern Notebook user interface gains more features, the notebook will also grow to take advantage of these features.","Modify,Clarity",Conclusion,Conclusion
7-1306,"Text files from read count quantification tools like HTSeq ( Anders et al ., 2015 ) and Kallisto ( Bray et al ., 2016 ) are supported as input.","Gene by cell matrices generated by the 10X Genomics Cell Ranger pipeline and flat text files from read count quantification tools like HTSeq ( Anders et al. , 2015 ) and kallisto ( Bray et al. , 2016 ) are supported as input.","Modify,Fact/Evidence",Setup analysis,Setup analysis
7-1306,A high percentage of mitochondrial genes indicates apoptotic or lysed cells.,"A high percentage of mitochondrial genes indicates the cell may have lysed before isolation, losing cytoplasmic RNA and retaining RNA enclosed in the mitochondria.","Split+Modify,Clarity",Setup analysis,Setup analysis
7-1306,These disrupted cells tend to lose cytoplasmic RNA and retain RNA enclosed in the mitochondria.,"A high percentage of mitochondrial genes indicates the cell may have lysed before isolation, losing cytoplasmic RNA and retaining RNA enclosed in the mitochondria.","Split+Modify,Clarity",Setup analysis,Setup analysis
7-1306,We also give users the option to remove sources of technical variation by performing linear regression on the total number of molecules detected and the percentage of reads mapped to mitochondrial genes.,"To remove sources of technical variation, linear regression is used to diminish the effects of the number of detected molecules and the percentage of counts mapped to mitochondrial genes.","Modify,Fact/Evidence",Preprocess counts,Preprocess counts
7-1306,A plot showing the percent variance explained of each principal component is then displayed so the user may choose a reasonable number of principal components for use in clustering ( Figure 2B ).,A plot showing the standard deviation of each principal component is then displayed so the user may choose a reasonable number of principal components for use in clustering ( Figure 2B ).,"Modify,Fact/Evidence",Preprocess counts,Preprocess counts
7-1641,"The detailed search workflow, a spreadsheet recording the results, and a bibliography of all papers deemed in scope are published <REF-1> , <REF-2> .",,"Add,Fact/Evidence",The findings of two surveys and a workshop,
7-1641,The year 2003 was chosen as the start date for the review as this was the year of the last major SARS (Severe Acute Respiratory Syndrome) outbreak.,,"Add,Fact/Evidence",The findings of two surveys and a workshop,
7-1641,There are very few evidence-based case studies that describe clearly the public health benefit that was achieved following the sharing of research data.,,"Add,Claim",Conclusion: what are the next steps for the role for funders in support of data sharing,
7-1641,Funders should ensure better monitoring of the implementation of their policies and where such evidence exists that shared data added value this should be documented and disseminated.,,"Add,Claim",Conclusion: what are the next steps for the role for funders in support of data sharing,
7-1641,Support for new mechanisms to publish data and papers rapidly during an emergency with peer review happening post-publication should serve both the need to share data and credit researchers.,,"Add,Claim",Conclusion: what are the next steps for the role for funders in support of data sharing,
7-1641,"Alongside support for technical standards there needs to be sustainable support for the infrastructure necessary to host those data with the appropriate governance mechanisms to ensure the efficient, ethical and equitable access outlined in the joint statement by funders of health research (2010) referenced above.",,"Add,Claim",Conclusion: what are the next steps for the role for funders in support of data sharing,
7-1641,Conclusion: what are the next steps for the role for funders in support of data sharing,What are the next steps: the role for funders in support of data sharing,"Modify,Other",Sharing health research data – the role of funders in improving the impact,Sharing health research data – the role of funders in improving the impact
7-1641,In part this might reflect the limited guidance and additional resources offered by the same funders in supporting their researchers to understand and undertake data sharing to implement and monitor these policies in practice.,In part this might reflect the limited guidance offered by the same funders in supporting their researchers to understand and undertake data sharing to implement and monitor these policies in practice.,"Modify,Claim",Conclusion: what are the next steps for the role for funders in support of data sharing,What are the next steps: the role for funders in support of data sharing
7-1641,One contributory role for funders would be to systematically collect the data management plans that they have requested as part of funding grants and make them publicly accessible.,For example one contributory role for funders would be to collect more systematically the data management plans that they have requested as part of funding grants and make them publicly accessible.,"Modify,Clarity",Conclusion: what are the next steps for the role for funders in support of data sharing,What are the next steps: the role for funders in support of data sharing
7-1641,"An online resource that brings together the reference material and policies that are exemplars of good practice in each of the categories that cover governance, data curation, security and longevity would provide the basis for a framework to guide the future development of new sharing resources.","An online resource that brings together the reference material and policies that are exemplars in each of the categories that cover governance, data curation, security and longevity would provide the basis for a framework to guide the future development of new sharing resources.","Modify,Clarity",Conclusion: what are the next steps for the role for funders in support of data sharing,What are the next steps: the role for funders in support of data sharing
7-1641,"In addition to defining the purpose of data sharing this would highlight the the political, technical, cultural, legal and ethical issues that need to be considered and point to examples of emerging good practice that can be used to address them.","In addition to defining the purpose of data sharing this would highlight the technical, cultural and ethical issues that need to be considered and point to examples of emerging good practice that can be used to address them.","Modify,Claim",Conclusion: what are the next steps for the role for funders in support of data sharing,What are the next steps: the role for funders in support of data sharing
7-1641,All data associated with this article are referenced and available as open access under a Creative Commons licence (CC BY).,No data are associated with this article.,"Modify,Fact/Evidence",Data availability,Data availability
7-1641,Policies that require the sharing of health research data to improve public health have been promoted by international research funders for over a decade.,"The benefits of sharing health research data to improve public health have been promoted by international research funders for over a decade but the reality is that the quality and volume of health research data shared, even in emergency situations, remains low <REF-1> , <REF-2> .","Split+Modify,Clarity",Introduction,Introduction
7-1641,"However, when measured the quality and volume of health research data that has been shared, even when related to public health emergencies, remains low <REF-1> , <REF-2> .","The benefits of sharing health research data to improve public health have been promoted by international research funders for over a decade but the reality is that the quality and volume of health research data shared, even in emergency situations, remains low <REF-1> , <REF-2> .","Modify,Claim",Introduction,Introduction
7-1641,"There are a number of ethical, legal and technical issues that act as impediments to sharing data but it seems this lack of progress is more a consequence of a cultural reluctance among researchers to ‘give up their data’ unless there are clear benefits returning to them.",This lack of progress seems to reflect a cultural reluctance among researchers to ‘give up their data’ without any clear benefits returning to them.,"Modify,Claim",Introduction,Introduction
7-1641,"This reluctance is heightened among researchers in low resource settings who feel that the requirements to share data, from funders and journals, risk turning them into data exporters unless greater efforts are made to ensure a fairer distribution of benefits.","This concern is heightened among researchers in low resource settings who feel that the requirements to share data, from funders and journals, risk turning them into data exporters unless greater efforts are made to ensure a fairer distribution of benefits.","Modify,Clarity",Introduction,Introduction
7-1641,In this paper the authors draw on their experience working for Wellcome Trust and TDR - the Special Programme for Research and Training in Tropical Diseases - of supporting data sharing initiatives and combine that with commissioned research to highlight the barriers to sharing research data and the role research funders might play to improve this situation.,In this paper we draw on our experience of supporting data sharing initiatives and some commissioned research to highlight the barriers to sharing research data and the role research funders might play to improve this situation.,"Modify,Fact/Evidence",Introduction,Introduction
7-1641,A decade of data sharing policies but little progress?,A decade of progress?,"Modify,Other",Sharing health research data – the role of funders in improving the impact,Sharing health research data – the role of funders in improving the impact
7-1641,"In January 2011, a group of research funding organizations published a joint statement on sharing health research data with the aim to harmonize their existing policies and promote the efficient use of those data to accelerate improvements in public health.","In January 2011, a group of research funding organizations published a joint statement on sharing health research data with the aim to promote the efficient use of those data to accelerate improvements in public health.","Modify,Fact/Evidence",A decade of data sharing policies but little progress?,A decade of progress?
7-1641,"To explore this further, Wellcome and TDR commissioned two surveys to review the governance arrangements and standards within existing data sharing resources.","To explore this further, we commissioned two surveys to review the governance arrangements and standards within existing data sharing resources.","Modify,Clarity",The findings of two surveys and a workshop,The findings of two surveys and a workshop
7-1641,The findings of those studies informed a workshop held in October 2017 with a set of stakeholders representing researchers and funding organizations with experience of sharing and using shared data.,The findings of those studies informed a workshop held in October 2017 with a set of stakeholders representing researchers and funding organizations.,"Modify,Fact/Evidence",The findings of two surveys and a workshop,The findings of two surveys and a workshop
7-1641,"All the reports, methods and supporting data files from these commissioned studies are published as open access under a Creative Commons licence and in free-to-access repositories.",All the reports and supporting files are published as open access under a Creative Commons licence and in free-to-access repositories.,"Modify,Clarity",The findings of two surveys and a workshop,The findings of two surveys and a workshop
7-1641,A review of all academic papers published since 2003 referencing these diseases was undertaken and attempts were then made to access the data underlying those publications via the web and through a direct survey of the corresponding authors.,A review of academic papers published since 2003 relating to these diseases was undertaken and attempts were then made to access the data underlying those publications via the web and through a direct survey of the corresponding authors.,"Modify,Clarity",The findings of two surveys and a workshop,The findings of two surveys and a workshop
7-1641,"Interviews were undertaken with a range of people either conducting or supporting research in these areas and this was supplemented with a review of institutional policies, discussion documents and academic commentaries about standards and norms in data sharing.","Interviews were undertaken with a range of people either conducting or supporting research in these areas and this was supplemented with a review of institutional policies, discussion documents and academic commentaries about standards and norms in data sharing <REF-1> , <REF-2> .","Modify,Fact/Evidence",The findings of two surveys and a workshop,The findings of two surveys and a workshop
7-1641,The second survey - Development of International Standards for Online Repositories - was designed to identify which technical ‘standards’ were being used in data sharing infrastructure relating to the neglected diseases.,The second survey - Development of International Standards for Online Repositories - was designed to identify which ‘standards’ were being used in data sharing relating to the neglected diseases.,"Modify,Fact/Evidence",The findings of two surveys and a workshop,The findings of two surveys and a workshop
7-1641,"A third report combined the findings of these two surveys referenced above and was used to shape thinking at a workshop held in Antwerp, Belgium in October 2017.","A third report combined the findings of these two surveys and was used to shape thinking at a workshop held in Antwerp, Belgium in October 2017 <REF-10> .","Modify,Fact/Evidence",The findings of two surveys and a workshop,The findings of two surveys and a workshop
7-1641,Summary of the findings,What does this tell us?,"Modify,Other",Sharing health research data – the role of funders in improving the impact,Sharing health research data – the role of funders in improving the impact
7-1641,"While a few authors will provide the data on request, 65% of the papers (207 of 319) give no information on how to find or access the data.","While a few authors will provide the data on request, 65% of these papers give no information on how to find or access the data.","Modify,Fact/Evidence",Summary of the findings,What does this tell us?
7-222,,A phylogenetic tree of Maximum Likelihood was built.,"Delete,Fact/Evidence",,Abstract
7-222,"This data contributes with base information on the biodiversity of the Parks, necessary to design and implement measures for the conservation of fungi in Ecuador.",,"Add,Claim",Abstract,
7-222,Alignments that presented 100% coverage and at least 99% identity with a sequence previously reported in GenBank were considered.,,"Add,Fact/Evidence",Sequencing and molecular identification,
7-222,"The results were compared with the previously made morphological identification at the QCAM Fungarium, to check the taxonomic designation.",,"Add,Fact/Evidence",Sequencing and molecular identification,
7-222,The number of nucleotide differences (SNPs) between the sample and the closest hit on the BLASTn search suggests that these specimens may belong to new species ( Table 1 ).,,"Add,Claim",Results,
7-222,Additional loci and more detailed morphological analyses are needed to determine this.,,"Add,Claim",Results,
7-222,"Studies related to the biological diversity of this order in the National Parks of Ecuador are scarce, more systematic field studies would surely reveal a greater diversity of families, genera and species within the Xylariales in SP and LP, as well as other regions and protected areas of Ecuador, especially if we take into account the cosmopolitan distribution of Xylaria <REF-13> .",,"Add,Claim",Results,
7-222,,"The taxonomic identification of fungi of the order Xylariales was achieved with the bioinformatic tools, to further study the phylogenetic relationships among the collected individuals and thus contribute with base information on their biological diversity, necessary to design and implement measures for the conservation of fungi.","Delete,Claim",,Abstract
7-222,These entire unidentified specimens might represent new species.,These unidentified specimens might represent new species.,"Modify,Clarity",Results,Results
7-222,To advance our understanding of the Kingdom Fungi we must start by deciphering the diversity of species present in these sites.,To advance our understanding of this Kingdom we must start by deciphering the diversity of fungi present in these sites.,"Modify,Clarity",Conclusions,Conclusions
7-222,"ITS is the accepted as primary fungal barcode marker for fungi <REF-3> , <REF-4> .",ITS is the accepted ’barcode’ for fungi <REF-3> .,"Modify,Clarity",Introduction,Introduction
7-222,"Here we present results exclusively for the Xylariales order, other fungal orders were also collected but are not shown here.",Here we present results exclusively for the Xylariales order.,"Modify,Fact/Evidence",Introduction,Introduction
7-222,Sequencing and molecular identification,Obtaining the sequences and molecular identification,"Modify,Clarity",Methods,Methods
7-222,The ITS1-5.8SITS2 region was amplified by PCR with primers ITS1F (5’-CTTGGTCATTTAGAGGAAGTAA-3’) <REF-5> and ITS4 (5’-TCCTCCGCTTATTGATATGC-3’) <REF-6> .,"The ITS1-5.8S-ITS2 region was amplified by PCR with primers (provided by Invitrogen Co., Carlsbad, CA, USA) ITS1F (5’-CTTGGTCATTTAGAGGAAGTAA-3’) <REF-4> and ITS4 (5’-TCCTCCGCTTATTGATATGC-3’) <REF-5> .","Modify,Fact/Evidence",Sequencing and molecular identification,Obtaining the sequences and molecular identification
7-222,"Xylariales: First results of mycological exploration in the Sangay and Llanganates National Parks, Ecuador","Xylariales: First results of mycological exploration in the Sangay and Llanganates National Park, Ecuador","Modify,Grammar",,
7-222,"The forward and reverse sequences obtained for each isolate were assembled in Geneious R8 (Biomatter Ltd. 2005–2012), and submitted to GenBank.",The obtained sequences were edited in Geneious R8 (Biomatter Ltd. 2005–2012) by selecting the “ de novo assemble” tool and then trimming the ends.,"Merge+Modify,Clarity",Sequencing and molecular identification,Obtaining the sequences and molecular identification
7-222,"The forward and reverse sequences obtained for each isolate were assembled in Geneious R8 (Biomatter Ltd. 2005–2012), and submitted to GenBank.","The consensus sequence was manually edited, and submitted to GenBank .","Merge+Modify,Fact/Evidence",Sequencing and molecular identification,Obtaining the sequences and molecular identification
7-222,Public sequences available in GenBank that corresponded to specimens that gave the greatest homology in BLASTn with the sequences of the collected specimens were included.,Public sequences are available in GenBank that corresponded to specimens that gave the greatest homology in BLASTn with the sequences of the collected specimens were included in the analyses.,"Modify,Grammar",Phylogenetic analysis,Phylogenetic analysis
7-222,"Phylogenetic trees were constructed in Geneious R8 using the PhyML <REF-8> plugin for Maximum Likelihood (ML) with a custom substitution model (010230), determined by jModelTest 2.1.4.",Phylogenetic trees were constructed in Geneious R8 using the PhyML <REF-7> plugin for Maximum Likelihood (ML) with a Custom (010230) substitution model determined by jModelTest 2.1.4.,"Modify,Clarity",Phylogenetic analysis,Phylogenetic analysis
7-222,All the specimens analyzed were of the genus Xylaria.,All the specimens collected were of the genus Xylaria .,"Modify,Clarity",Results,Results
7-222,"Differences in the number of samples found at each park could be due to the sampling effort, and not necessarily to the richness of the Xylareales in the Parks.","Differences in the number of samples found at each park could be due to the sampling effort that was different in each park, however, this is a sample of the high biodiversity in LP and SP.","Modify,Claim",Results,Results
7-222,"The analysis shows that there are no shared species of Xylaria at the two sampled sites ( Table 1 ), this is important for conservation decision making.","The analysis shows that there are no shared species of Xylaria at the two parks ( Table 1 ), this is important for conservation decisions.","Modify,Clarity",Results,Results
7-222,"All analyzed collections presented here belong to the genus Xylaria, of these eight belong to PL and two to SP.","All records belong to the genus Xylaria, of these eight belong to PL and two to SP.","Modify,Clarity",Abstract,Abstract
7-222,"The first major group, composed by clades A and B, is well supported, it includes specimens from LP and PS.","The first major group, composed by clades A and B, is well supported (bootstrap > 95) includes specimens from LP and PS.","Modify,Grammar",Results,Results
7-222,"Clade A includes all X. entogena specimens and Clade B includes all X. telfairii specimens, and Xylarya sp.1 specimens.",Clade A includes all X. entogena specimens and is well supported (bootstrap > 95).,"Link+Modify,Fact/Evidence",Results,Results
7-222,"Clade A includes all X. entogena specimens and Clade B includes all X. telfairii specimens, and Xylarya sp.1 specimens.","Clade B includes all X. telfairii specimens and Xylarya sp.1 specimens (bootstrap < 50), it could be supposed that Xylarya sp.1 belongs to the X. telfairii species, but because the high difference among the sequences it is considered a different species.","Link+Modify,Fact/Evidence",Results,Results
7-222,"It is possible that Xylarya sp.1 might belong to the X. telfairii group, but due to the differences among the sequences it is likely a different species.","Clade B includes all X. telfairii specimens and Xylarya sp.1 specimens (bootstrap < 50), it could be supposed that Xylarya sp.1 belongs to the X. telfairii species, but because the high difference among the sequences it is considered a different species.","Link+Modify,Clarity",Results,Results
7-222,Clade C includes all X. schweinitzii specimens.,Clade C includes all X. schweinitzii specimens (bootstrap > 95).,"Modify,Fact/Evidence",Results,Results
7-222,"Four samples were not identified at the species level, suggesting it could be a new species.","A record was not identified at the species level, suggesting that it could be a new species.","Modify,Fact/Evidence",Abstract,Abstract
7-222,Clade D includes Xylaria sp. 2.,"Clade D (bootstrap > 80) includes Xylaria sp. 2, the closest sequence to Xylaria sp. 2 from SP was a previously reported collection also from Ecuador <REF-12> in a cloud forest in the province of Imbabura, that was also identified only at the genus level.","Split+Modify,Fact/Evidence",Results,Results
7-222,"The closest sequence to Xylaria sp. 2 from SP was a previously reported collection also from Ecuador <REF-13> in a cloud forest in the province of Imbabura, that was also identified only at the genus level.","Clade D (bootstrap > 80) includes Xylaria sp. 2, the closest sequence to Xylaria sp. 2 from SP was a previously reported collection also from Ecuador <REF-12> in a cloud forest in the province of Imbabura, that was also identified only at the genus level.",Split+Identical,Results,Results
7-222,"Clade E shows Xylaria sp. 3, the closest sequence to this individual belongs to the same previously reported study <REF-13> , identified only at the genus level.","Clades E (bootstrap > 95) shows Xylaria sp. 3, the closest sequence to this individuals belongs to the same previously reported collection <REF-12> , identified only at the genus level.","Modify,Fact/Evidence",Results,Results
7-222,Clade F includes Xylaria fissilis sequences from LP and one from <REF-13> .,Clade F (bootstrap = 100) includes Xylaria fissilis sequences from LP and one from <REF-12> ; clade F also includes Xylaria sp. 4 an unidentified specimen.,"Split+Modify,Fact/Evidence",Results,Results
7-222,"Clade F also includes Xylaria sp. 4, an unidentified specimen.",Clade F (bootstrap = 100) includes Xylaria fissilis sequences from LP and one from <REF-12> ; clade F also includes Xylaria sp. 4 an unidentified specimen.,"Split+Modify,Grammar",Results,Results
7-229,"The secondary forest plots were last subject to selective logging until 2008 for BS1, 1999 for BS2, 2003 for BS3, and 2010 for BS4.",,"Add,Fact/Evidence",Study site,
7-229,"The shrub swamp plots burned in 1997, and BB2 burned yearly since.",,"Add,Claim",Study site,
7-229,"It was not possible to conduct the survey each month, or to repeat the same plot sampling sequence for the second set of recordings because of access and transportation restrictions.",,"Add,Fact/Evidence",Study site,
7-229,We checked that the models were not over-dispersed and that the standardised residuals did not indicate heteroscedasticity.,,"Add,Fact/Evidence",Data analysis,
7-229,"Error bars show the 83% confidence intervals for the mean alpha and beta richness values, which do not differ significantly at P=0.05 when they overlap (Krzywinski, 2013).",,"Add,Fact/Evidence",Results,
7-229,Mean values are represented with red dots and their 83% confidence intervals are indicated with error bars.,,"Add,Fact/Evidence",Results,
7-229,"Means are significantly different at P=0.05 when their confidence intervals do not overlap (Krzywinski, 2013), and significant differences are indicated with asterisks.",,"Add,Fact/Evidence",Results,
7-229,Abundances are split by habitat and shown for different IUCN Red List statuses.,,"Add,Fact/Evidence",Results,
7-229,This change in the community also became apparent in a shift to bigger and more mobile species with greater distribution ranges in shrub swamp.,,"Add,Claim",Discussion,
7-229,"Wild bird trapping threatens bird populations directly (Harris et al . 2016), and is especially worrisome as birds from the Berbak region are increasingly traded in the caged bird market of Jambi city (pers. obs. KD).",,"Add,Claim",Discussion,
7-229,"We pooled the data from both acoustic counts and visualized the composition of the bird communities in non-metric multidimensional scaling graphs generated with the package vegan <REF-39> , and tested the significance of the habitat in structuring these communities with an ADONIS test <REF-40> .","We visualized the composition of the bird communities in non-metric multidimensional scaling graphs generated with the package vegan <REF-39> , and tested the significance of the habitat in structuring these communities with an ADONIS test <REF-40> .","Modify,Fact/Evidence",Data analysis,Data analysis
7-229,"To investigate whether the combined, different habitats lead to higher species richness than one primary forest area of similar size, we calculated the rarefied richness based on the entire bird community, rarefied to 4 sampling plots, to compare it to the number of species found in the 4 forest plots.","To investigate whether the combined, different habitats lead to higher species richness than one primary forest area of similar size, we calculated the rarefied richness based on the entire bird community, rarefied to 4 sampling sites, to compare it to the number of species found in the 4 forest sites.","Modify,Clarity",Data analysis,Data analysis
7-229,We detected 426 birds overall.,"We detected 379 birds overall, belonging to 90 species ( Table S1 ).","Modify,Fact/Evidence",Results,Results
7-229,"Among those, 30 individuals were not identified to species level and 2 were detected above 50 m, resulting in a working dataset of 394 individuals, belonging to 88 species ( Table S1 ).","Among those, 26 individuals were not identified to species and 2 were detected above 50 m, resulting in a working dataset of 351 detections.","Modify,Fact/Evidence",Results,Results
7-229,"The three habitats differed considerably based on their vegetation structure ( Figure S1 ) and the distribution of their DBH values, with primary forest having the highest basal area, secondary forest intermediate values, and shrub swamp having the smallest basal area ( Figure S2 ).",The three habitats differed considerably based on their vegetation structure and the distribution of their DBH values ( Figure S1 and Figure S2 ).,"Modify,Fact/Evidence",Results,Results
7-229,"Species richness and abundance at the count level, and mean alpha and beta species richness at the plot level were similar between the habitats ( Figure 2 and Figure 3 ).","Species richness and abundance at the count level, and mean alpha and beta species richness at the site level were similar between the habitats ( Figure 2 and Figure 3 ).","Modify,Clarity",Results,Results
7-229,The ADONIS test revealed that habitat structured bird community composition with marginal significance (P=0.068).,The ADONIS test revealed that habitat structured bird community composition with marginal significance (P=0.053).,"Modify,Fact/Evidence",Results,Results
7-229,"All these human activities increase the risk of fires, especially during dry spells caused by the warm phases of the El Niño Southern Oscillation, which led to the especially severe fires of 1994, 1997, and 2015 (after our survey).","These human activities increase the risk of fires, especially during dry spells caused by the warm phases of the El Niño Southern Oscillation, which led to the especially severe fires of 1994, 1997, and 2015 (after our survey).","Modify,Clarity",Discussion,Discussion
7-229,"Primary forest had high conservation value because we found 16 species of conservation concern (""near threatened"" status according to IUCN Red List status), twice as many as in shrub swamp.","Primary forest sites had high conservation value because we found 16 species of conservation concern (""near threatened"" status according to IUCN Red List status), twice as much as in shrub swamp.","Modify,Grammar",Discussion,Discussion
7-229,"Secondary forest was exactly intermediary with 12 species of conservation concern, although one bird, the Javan Myna ( Acridotheres javanicus ), has recently been classified as vulnerable.","Secondary forest sites were exactly intermediary with 12 species of conservation concern, although one bird, the Javan Myna ( Acridotheres javanicus ), has recently been classified as vulnerable.","Modify,Grammar",Discussion,Discussion
7-229,Javan Mynas are commonly sold on the market in Jambi city and might establish feral populations when occasionally breaking free.,Javan Mynas are commonly sold on the market in Jambi city.,"Modify,Claim",Discussion,Discussion
7-229,Compared to the secondary forest plots surveyed by Prabowo et al .,Compared to the secondary forest sites surveyed by Prabowo et al .,"Modify,Clarity",Discussion,Discussion
7-229,"Our secondary forest plots seemed relatively well-preserved, as we could not detect any typical loss of understory and terrestrial insectivores due to changes in understory vegetation <REF-43> .","Our secondary forest sites seemed relatively well-preserved, as we could not detect any typical loss of understory and terrestrial insectivores due to changes in understory vegetation <REF-43> .","Modify,Clarity",Discussion,Discussion
7-229,"Considering that the detected abundance of birds in the other habitats was similarly high, this indicates that the National Park of Berbak still provides relatively good living conditions for birds, and especially conservation-worthy species thrive in the primary forest tracts.","Considering that the detected abundance of birds in the other habitats was similarly high, this indicates that the National Park of Berbak provides relatively good living conditions for birds, and especially conservation-worthy species thrive in the primary forest tracts.","Modify,Clarity",Discussion,Discussion
7-229,"In Southeast Asia, most primary forest bird species still occur in previously logged forests <REF-12> – <REF-14> (Azhar et al . 2011).","In Southeast Asia, most primary forest bird species still occur in previously logged forests <REF-12> – <REF-14> .","Modify,Fact/Evidence",Introduction,Introduction
7-229,"Theoretical and modelling approaches are usually used to analyse the potential benefit of disturbances for overall landscape biodiversity <REF-20> , <REF-21> .","Few studies have focused on the potential benefit of disturbances for overall landscape biodiversity apart from theoretical and modelling approaches <REF-20> , <REF-21> .","Modify,Claim",Introduction,Introduction
7-229,We recorded audible sound in all 12 plots two times.,We recorded audible sound in all 12 sites.,"Modify,Fact/Evidence",Study site,Study site
7-229,"From February to November 2013, we sampled all three plots of one site each month for 48 hours, starting at midnight.","From February to November 2013, we sampled all three habitats at one site each month for 48 hours, starting at midnight.","Modify,Clarity",Study site,Study site
7-229,We also recorded whether the plot was flooded or not at the time of the sound recorder installation.,We also recorded whether the site was flooded or not at the time of the sound recorder installation.,"Modify,Clarity",Study site,Study site
7-229,"We uploaded 20 minutes recordings starting at sunrise from each plot from each month (24 recordings in total, 2 per plot) to our online platform ( http://soundefforts.uni-goettingen.de/ ).",We uploaded 20 minutes recordings after sunrise for each plot in each month (24 recordings in total) to our online platform ( http://soundefforts.uni-goettingen.de/ ).,"Modify,Fact/Evidence",Data analysis,Data analysis
7-229,"The distance of bird vocalisations was estimated by ear to the meter; even though the accuracy is lower, estimation error was assumed to be random.","The distance of bird vocalisations was estimated by ear to the meter, based on their loudness in the sound recording compared to the ambient sound level and knowledge of the source sound level of each species.","Split+Modify,Claim",Data analysis,Data analysis
7-229,Distance estimates were made based on the call loudness in the sound recording compared to the ambient sound level and knowledge of the source sound level of each species.,"The distance of bird vocalisations was estimated by ear to the meter, based on their loudness in the sound recording compared to the ambient sound level and knowledge of the source sound level of each species.","Split+Modify,Clarity",Data analysis,Data analysis
7-229,"We excluded detections that were not identified to species, as well as detections above 50 m to compare plots at a common detection radius <REF-35> .","We excluded detections that were not identified to species, as well as detections above 50 m to compare sites at a common detection radius <REF-35> .","Modify,Clarity",Data analysis,Data analysis
7-229,"We analysed the species richness, abundance, vocalisation activity, and community composition across acoustic counts, plots, feeding guilds and IUCN Red List categories.","We analysed the species richness, abundance, vocalisation activity, and community composition across acoustic counts, sites, feeding guilds and IUCN Red List categories.","Modify,Clarity",Abstract,Abstract
7-229,Bird species richness was further computed at the plot (alpha richness) and habitat (gamma richness) levels.,Bird species richness was further computed at the site and habitat level.,"Modify,Fact/Evidence",Data analysis,Data analysis
7-229,"For counting bird abundance, we first derived the maximum number of simultaneously vocalising individuals in each species, and then summed these maxima over all species, leading to a conservative estimate of the number of individuals per count.",Bird abundance was counted as the sum of the maximum number of individuals vocalising simultaneously in each species.,"Modify,Fact/Evidence",Data analysis,Data analysis
7-229,"We calculated community-weighted means (hereafter CWM, also called community functional parameter <REF-36> ) for body mass, wing length, and distribution area for each count.","We calculated community-weighted means (or community functional parameter <REF-36> , hereafter CWM) for body mass, wing length, and distribution area for each count.","Modify,Clarity",Data analysis,Data analysis
7-407,,"Even if with our search activity we are quite sure to have reduced to a minimum the problem of publication bias, we performed a statistical estimation by using the Copas selection model which is recommended by Jin et al . (2015) .","Delete,Fact/Evidence",,Statistical methods
7-407,,"The search method used and the small number of people interested in this research field, guarantee that from an empirical point of view, any publication bias is almost absent.","Delete,Claim",,Publication bias
7-407,,"We found very interesting evidence of presentiment distilled from the conventional post-stimulus psychological research of Jolij and Bierman, who have performed a long series of experiments using a face detection paradigm.","Delete,Claim",,Discussion
7-407,,"Additionally, the work of Kittenis found prestimulus effects from a conventional research program and pre-registered single-trial work of Mossbridge represent an important conceptual replication in countering both the use of questionable research practices and expectancy effects arguments.","Delete,Claim",,Discussion
7-407,,"A promising development of this line of research is the development of paradigms that use software in real-time to predict meaningful future outcomes before they occur, e.g. ( Franklin et al ., 2014 )","Delete,Claim",,Discussion
7-407,Preregistered vs No-preregistered studies,,"Add,Other",Results,
7-407,"This distinction is relevant for assessing the impact of the so-called Questionable Research Practices and in particular p-hacking ( Head et al ., 2015 ; John et al ., 2012 ).",,"Add,Fact/Evidence",Preregistered vs No-preregistered studies,
7-407,"Preregistered studies must describe all details on how the data will be analyzed before their collection, thus reducing the degree of freedom available during and after data collection.",,"Add,Claim",Preregistered vs No-preregistered studies,
7-407,From our database it was possible to compare the estimate of the effect size obtained from the pre-registered studies with that obtained from the no-preregistered ones.,,"Add,Fact/Evidence",Preregistered vs No-preregistered studies,
7-407,The results are presented in the following Table 4 .,,"Add,Fact/Evidence",Preregistered vs No-preregistered studies,
7-407,"The effect size point estimates clearly show that the effect size of the preregistered studies is larger than that of the no-preregistered studies, however their precision estimates (see the 95% CI) reveal a considerable overlap and consequently they cannot be considered statistically different.",,"Add,Fact/Evidence",Preregistered vs No-preregistered studies,
7-407,Our very comprehensive literature search is likely to have reduced the probability of a publication bias.,,"Add,Claim",Publication bias,
7-407,Nevertheless we added a statistical estimation of the publication bias.,,"Add,Fact/Evidence",Publication bias,
7-407,"Similarly, more recent publication bias tests like the three-parameters selection model, the p-uniform* and the Vevea and Hedges’ weight-function model ( Vevea & Woods (2005) , seem not recommended for multilevel random meta-analyses with high heterogeneity like the present one.",,"Add,Claim",Publication bias,
7-407,"This phenomenon may hence be considered among the more reliable within those covered under the umbrella term “psi” (see Cardeña, 2018 for an exhaustive review of the evidence and the theoretical hypotheses of all these phenomena).",,"Add,Fact/Evidence",Conclusion,
7-407,"In order to arrive at such an ambitious goal, it is necessary to achieve a high degree of correct classifications based on prestimulus activity at the level of each trial so that the number of false positives and false negatives is reduced to a bare minimum.",,"Add,Claim",Conclusion,
7-407,The experiments of Mossbridge (2017) ; Baumgart et al . (2017) and Jolij & Bierman (2017) are promising examples in this regard.,,"Add,Claim",Conclusion,
7-407,"Data associated with the article are available under the terms of the Creative Commons Zero ""No rights reserved"" data waiver (CC0 1.0 Public domain dedication).",,"Add,Fact/Evidence",Data availability,
7-407,,"Additionally, we also found increasing evidence of presentiment research piggybacking onto mainstream psychology programs, even informing aspects of the conventional research.","Delete,Fact/Evidence",,Introduction
7-407,"Excluded records were studies where the psychophysiological variables were analysed only after and not before the stimuli presentations ( Jin et al. , 2013 ) and with an unusual procedure ( Tressoldi et al. , 2015 ), i.e. using heart rate feedback to inform a voluntary decision to predict random positive or negative events.","Excluded records were studies were the psychophysiological variables were analysed only after and not before the stimuli presentations ( Jin et al. , 2013 ) and with an unusual procedure ( Tressoldi et al. , 2015 ), i.e. using heart rate feedback to inform a voluntary decision to predict random positive or negative events.","Modify,Grammar",Study selection,Study selection
7-407,The statistical estimation of the publication bias by using the Copas selection model suggest that the main findings are not contaminated by publication bias.,The statistical estimation of the publication bias by using the Copas model suggest that the main findings are not contaminated by publication bias.,"Modify,Clarity",Abstract,Abstract
7-407,The database along with all 19 papers are available from Tressoldi (2017) .,The database along with all 18 papers are available from Tressoldi (2017) .,"Modify,Fact/Evidence",Coding procedure,Coding procedure
7-407,"In order to control the reliability of the results, a second analysis was carried out by using a multilevel approach as suggested by Assink & Wibbelink (2016) implemented with the metafor package ( Viechtbauer, 2010 ) and reported in the Table S2 in the Supplementary File 3 .","In order to control the reliability of the results, a second analysis was carried out by using a multilevel approach as suggested by ( Assink & Wibbelink, 2016 ) implemented with the metafor package ( Viechtbauer, 2010 ) and reported in the Table S2 in the Supplementary File 3 .","Modify,Clarity",Statistical methods,Statistical methods
7-407,"A Bayesian meta-analysis was implemented with the brms package ( Bürkner, 2017 ).","The Bayesian meta-analysis was implemented with the brms package ( Bürkner, 2017 ).","Modify,Grammar",Statistical methods,Statistical methods
7-407,Another “sensitivity analysis” was carried out excluding the new Mossbridge and Tressoldi studies in order to control whether different authors could obtain similar results.,Another “sensitivity analysis” was carried out excluding the Mossbridge and the Tressoldi studies in order to control whether different authors could obtain similar results.,"Modify,Clarity",Frequentist multilevel random model,Frequentist multilevel random model
7-407,"Both the frequentist and the Bayesian analyses support the evidence of an overall main effect of approximately .28, and a small difference between the peer and non-peer reviewed studies.","Both the frequentist and the Bayesian analyses support the evidence of an overall main effect of approximately .29, and a small difference between the peer and non-peer reviewed studies.","Modify,Fact/Evidence",Frequentist multilevel random model,Frequentist multilevel random model
7-407,"Anyway, we applied the Copas selection model which is recommended by Jin et al . (2015) .",We hence applied the Copas selection model which is recommended by Jin et al . (2015) .,"Modify,Clarity",Publication bias,Publication bias
7-407,"Now imagine if such prognosticating ability was possible without any sensory or other inferential cues (see Mossbridge & Radin, 2018 for a review).",Now imagine if such prognosticating ability was possible without any sensory or other inferential cues.,"Modify,Fact/Evidence",Introduction,Introduction
7-407,"This update of the Mossbridge et al. (2012) meta-analysis related to the so called predictive anticipatory activity (PAA) responses to future random stimuli, covers the period January 2008- July 2018.","This update of the Mossbridge et al. (2012) meta-analysis related to the so called predictive anticipatory activity (PAA) responses to future random stimuli, covers the years 2008- October 2017.","Modify,Fact/Evidence",Discussion,Discussion
7-407,"Overall, we found 19 new studies describing a total of 36 effect sizes.","Overall, we found 18 new studies describing a total of 34 effect sizes.","Modify,Fact/Evidence",Discussion,Discussion
7-407,Predictive physiological anticipatory activity preceding seemingly unpredictable stimuli: An update of Mossbridge et al ’s meta-analysis,Predictive physiological anticipation preceding seemingly unpredictable stimuli: An update of Mossbridge et al ’s meta-analysis,"Modify,Clarity",,
7-407,"Furthermore, we did not find substantial differences between peer and non-peer reviewed papers as in the original paper, as the confidence intervals of their mean effect size, overlap considerably.","Furthermore, we did not find substantial differences between peer and not-peer reviewed papers as in the original paper.","Modify,Fact/Evidence",Discussion,Discussion
7-407,This update confirms the main results reported in Mossbridge et al . (2012) original meta-analysis and gives further support to the hypothesis of predictive physiological anticipatory activity of future random events.,This update confirms the main results reported in Mossbridge et al . (2012) original meta-analysis and gives further support to the hypothesis of predictive physiological anticipation of future random events.,"Modify,Clarity",Conclusion,Conclusion
7-407,The limitations of the present meta-analysis are similar to most meta-analyses which include non-preregistered studies.,"The limitations of the present meta-analysis are similar to most meta-analyses which include non pre-registered studies that cannot be controlled for the degree of freedoms in the methodology and data analysis in the course of their implementations, making them prone, for example, to the so-called “questionable research practices” ( John et al ., 2012 ).","Modify,Claim",Conclusion,Conclusion
7-407,"The solution is that of prospective meta-analyses ( Watt & Kennedy, 2017 ), based on all preregistered studies where the methods and data analyses have been declared and made public beforehand.","The solution is that of prospective meta-analyses ( Watt & Kennedy, 2017 ), based on preregistered studies where the methods and data analyses have been declared and made public beforehand.","Modify,Clarity",Conclusion,Conclusion
7-407,"Disturbingly, moments before the stimulus is presented there are physiological changes ahead of time.","Disturbingly, moments before the stimulus is presented there are murmurings of activity, as if the body is predicting moments ahead of time.","Modify,Claim",Introduction,Introduction
7-407,"In both of these approaches it is difficult to envision mundane strategies that might explain the anomalous pre-stimulus effects observed, and indeed, Mossbridge et al ., went to significant lengths in refuting the leading candidate – expectancy effects, both in the 2012 meta-analysis and in post-review exchanges with sceptical psychologists and physiologists.","In both of these approaches it is difficult to envision mundane strategies that might explain the anomalous pre-stimulus effects observed, and indeed, Mossbridge et al , went to significant lengths in refuting the leading candidate – expectancy effects, both in the 2012 meta-analysis and in post-review exchanges with sceptical psychologists and physiologists.","Modify,Grammar",Introduction,Introduction
7-407,The presentiment hypothesis calls for a difference between the pre-stimulus responses of the two stimulus categories and this is calculated across sessions.,The presentiment hypothesis calls for a difference between arousing and neutral pre-stimulus responses and this is calculated across sessions.,"Modify,Clarity",Introduction,Introduction
7-407,"Because of the high profile nature of Mossbridge et al ., (over 93,000 views as of January 2018) there has been a good number of replications in the few years since publication.","Because of the high profile nature of Mossbridge et al , (over 93,000 views as of January 2018) there has been a good number of replications in the few years since publication.","Modify,Grammar",Introduction,Introduction
7-407,"Because expectancy effects have been proposed as a potential mechanism to explain at least some of the presentiment effect, it is noteworthy that several experiments in this fresh cohort of studies tackle this head on by only analysing the first trial of a run.","Because expectancy effects have been forwarded to explain at least some of the presentiment effect, it is noteworthy that several experiments in this fresh cohort of studies tackle this head on by only analysing the first trial of a run.","Modify,Claim",Introduction,Introduction
7-407,This provides another objective measure of the validity of the presentiment effect.,This provides a second objective measure of the validity of the presentiment effect.,"Modify,Clarity",Introduction,Introduction
7-407,"The whole procedure followed both the APA Meta-Analysis Reporting Standards ( APA Publications and Communications Board Working Group on Journal Article Reporting Standards, 2008 ), the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols (PRISMA) 2015 ( Moher et al ., 2015 ) and the reporting standards for literature searches and report inclusion ( Atkinson et al ., 2015 ).","The whole procedure followed both the APA Meta-Analysis Reporting Standards ( APA Publications and Communications Board Working Group on Journal Article Reporting Standards, 2008 ), the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols 2015 ( Moher et al ., 2015 ) and the reporting standards for literature searches and report inclusion ( Atkinson et al ., 2015 ).","Modify,Clarity",Methods,Methods
7-407,It is important to point out that these eligibility criteria are different from those used by Mossbridge et al. Those authors selected only studies where the anticipatory signals mirrored the post-stimulus ones.,It is important to point out that these eligibility criteria are different from those used by Mossbridge et al. Those authors selected only studies were the anticipatory signals mirrored the post-stimulus ones.,"Modify,Grammar",Study eligibility criteria,Study eligibility criteria
7-407,In addition we included all studies that used anticipatory signals to predict future events independently of the presence of post-stimulus physiological signals.,Differently we included all studies that used anticipatory signals to predict future events independently of the presence of post-stimulus physiological signals.,"Modify,Clarity",Study eligibility criteria,Study eligibility criteria
7-407,"For example, some authors, e.g. Mossbridge (2015) used heart rate variability to predict winning i.e. $4, versus losing outcomes without recording the post-stimulus physiological activity associated with hits and misses.","For example, some authors, e.g. Mossbridge (2015) used heart rate variability to predict winning i.e. $4, versus losing outcomes.","Modify,Fact/Evidence",Study eligibility criteria,Study eligibility criteria
7-407,"Furthermore, we emailed a request of the data of completed studies to all authors we knew were involved in this type of research.","Furthermore, we emailed a request of the data of completed studies to all authors we knew were involved in this type of investigations.","Modify,Clarity",Studies retrieval procedure,Studies retrieval procedure
7-407,"We searched all completed studies, both peer reviewed and non-peer reviewed, e.g. Ph.D dissertations, from January 2008 to June 2018.","We searched all completed studies, both peer reviewed and non-peer reviewed, e.g. Ph.D dissertations, from January 2008 to October 2017.","Modify,Fact/Evidence",Studies retrieval procedure,Studies retrieval procedure
7-697,RH is the distance from CO to the Go’ (the reflection of subdivision tangen from ramus and corpus mandibular to the ramus borderline).,RH is the distance from CO to the gonion.,"Modify,Fact/Evidence",Methods,Methods
7-697,"To determine the random error, inter-rater (T.B. and E.S.) and intra-rater (E.S. under supervised of T.B.) measurements of variables in this study were randomly done from 20 panoramic radiographs.","To determine the random error, inter-rater (T.B. and E.S.) and intra-rater (E.S.) measurements of variables in this study were randomly selected from 20 panoramic radiographs.","Modify,Clarity",Methods,Methods
7-697,"The validity and reliability, measured using Cohen’s κ, showed moderate agreement for inter-rater measure-ments between T.B and E.S (κ=0.538) whilst intra-rater measurements from E.S that repeated the measurements 1 week after the first examination and blinded to the initial values (κ=0.674).","Finally, this study used intra-rater measurement as reference data for assessing vertical mandibular symmetry, which repeated the measurements 1 week after the first examination, while blinded to the initial values.","Link+Modify,Clarity",Methods,Methods
7-697,"Finally, this study used intra-rater measurement as reference data for assessing vertical mandibular symmetry.","Finally, this study used intra-rater measurement as reference data for assessing vertical mandibular symmetry, which repeated the measurements 1 week after the first examination, while blinded to the initial values.",Link+Identical,Methods,Methods
7-697,"The validity and reliability, measured using Cohen’s κ, showed moderate agreement for inter-rater measure-ments between T.B and E.S (κ=0.538) whilst intra-rater measurements from E.S that repeated the measurements 1 week after the first examination and blinded to the initial values (κ=0.674).","The validity and reliability, measured using Cohen’s κ, showed moderate agreement for inter-rater measurements (κ=0.538) whilst intra-rater measurements (κ=0.674).","Link+Modify,Clarity",Methods,Methods
7-697,This study used Kjellberg’s technique because it is easier to identify the condylar height using this technique than Habets’ method in vertical mandibular symmetry assessment.,This study used Kjellberg’s technique because it is easier to identify the condylar height using this technique than Habet’s method.,"Modify,Claim",Discussion,Discussion
7-697,"Habets’ method is more complicated when making reference points of the most lateral point of the condyle due to variation in the condylar anatomy <REF-16> , <REF-17> , <REF-29> .","Habet’s method is more complicated when making reference points of the most lateral point of the condyle due to variation in the condylar anatomy <REF-16> , <REF-17> , <REF-31> .","Modify,Grammar",Discussion,Discussion
7-697,"In the future, although the result in Table 3 showed a non-significant correlation, the TMD-DI as early screening for TMD might require panoramic radiography with postero-anterior radiography or 3D-cone beam computerized tomography to analyze the complexity of TMD development and mandibular asymmetry.","In the future, although the result in Table 3 showed a non-significant correlation, the TMD-DI as early screening for TMD might require panoramic radiography with postero-anterior radiography or 3D-cone beam computerized tomography to analyze the complexity of development of TMD and mandibular asymmetry.","Modify,Clarity",Discussion,Discussion
7-697,"This condition could affect the development of TMD symptoms and vertical mandibular asymmetry; this matches the study by Halicioglu et al. , which reported a slight difference in the vertical mandibular symmetry index was found in patients with early unilateral mandibular first molar extractions <REF-37> .","This condition could affect the development of TMD symptoms and vertical mandibular asymmetry; this matches the study by Halicioglu et al. , which that reported a slight difference in the vertical mandibular symmetry index was found in patients with early unilateral mandibular first molar extractions <REF-39> .","Modify,Grammar",Discussion,Discussion
7-697,"However, a combination of questionnaires (as diagnostic indexes) and radiography analysis of predominantly vertical or horizontal mandibular asymmetries indicates that susceptibility to fluctuating asymmetry is increasing <REF-32> .","However, a combination of questionnaires (as diagnostic indexes) and radiography analysis indicates that susceptibility to fluctuating asymmetry is increasing.","Modify,Fact/Evidence",Discussion,Discussion
7-697,"Further studies on the development of TMD, mandibular asymmetry and treatment planning for growing patients are suggested, using longitudinal and transitional approaches.","Further study on the development of TMD, mandibular asymmetry and treatment planning for young patients is suggested, using longitudinal and transitional approaches.","Modify,Grammar",Conclusions,Conclusions
7-697,"The asymmetry indices of mandibular height based on the ratio of condylar height (CH) and ramus height (RH) asymmetry, according to Habets’ method and Kjellberg’s technique, correlated significantly between TMD and non-TMD patients <REF-14> , <REF-15> .","The asymmetry indices of mandibular height based on the ratio of condylar height (CH) and ramus height (RH) asymmetry, according to Habet’s method and Kjellberg’s technique, correlated significantly between TMD and non-TMD patients <REF-14> , <REF-15> .","Modify,Grammar",Introduction,Introduction
7-704,,"Although the current version of SimNIBS does not allow exporting the resulting E-field maps into FreeSurfer, this option will be available in an upcoming version.","Delete,Claim",,Usage notes
7-704,"Additionally, the segmentation of the brainstem is not accurate because it arbitrarily assigned brain tissue to white and grey matter.",,"Add,Claim",Creation of head models,
7-704,"Secondly, an extended head model with field of view covering the entire head would further increase the predictive accuracy of the head models <REF-35> .",,"Add,Fact/Evidence",Creation of head models,
7-704,"In addition, air cavities were modeled by not adding tetrahedra to these locations, similar to the air surrounding the head.",,"Add,Fact/Evidence",Creation of head models,
7-704,It was a custom pipeline developed before the official release of SimNIBS 2.1 .,,"Add,Fact/Evidence",Dataset validation,
7-704,"However, using headreco combined with the CAT12 toolbox (included with SimNIBS 2.1.1 ) for cortical reconstruction, the same accuracy can be achieved.",,"Add,Claim",Dataset validation,
7-704,"Accessing group differences between MDD and healthy subjects was not the primary aim of the current data note, however, analysis of the spatial distribution of tDCS-induced E-fields in the bilateral DLPFC and medial prefrontal cortex showed subtle group differences between the healthy and MDD groups.",,"Add,Fact/Evidence",Dataset validation,
7-704,The total volume of the brain was 1.22 dm 3 ± 0.11 dm 3 (range: 1.02 dm 3 - 1.49 dm 3 ).,The total volume of the brain was 1.22 dm 3 ± 0.11 dm 3 (range: 1.02 dm 3 – 1.49 dm 3 ).,"Modify,Grammar",Creation of head models,Creation of head models
7-704,"Except for two manual steps (removal of MRI markers from the forehead, manual correction of tissue segmentations), the process of head model creation was automated using a custom version of SimNIBS 2.1 that employed FreeSurfer 5.3.0 for brain segmentation (as described in <REF-38> and implemented in mri2mesh) and SPM12 for segmentation of the remaining tissues (similarly to <REF-39> and implemented in headreco).","Except for two manual steps (removal of MRI markers from the forehead, manual correction of tissue segmentations), the process of head model creation was automated using a pre-release version of SimNIBS 2.1 that employed FreeSurfer 5.3.0 for brain segmentation (as described in <REF-29> ) and SPM12 for segmentation of the remaining tissues (similarly to <REF-30> ).","Modify,Fact/Evidence",Dataset validation,Dataset validation
7-704,This pipeline provides more accurate tissue segmentation relative to other protocols.,This new SimNIBS pipeline provides more accurate tissue segmentation relative to other protocols.,"Modify,Clarity",Dataset validation,Dataset validation
7-704,We provide scripts compatible with SimNIBS 2.1.1 for automated simulation of tDCS-induced electric fields for all head models available for download at our data repository <REF-36> .,The scripts used for automated simulation of tDCS effects for all head models (as shown in Figure 3 ) are available for download at our data repository <REF-27> .,"Modify,Fact/Evidence",Dataset validation,Dataset validation
7-704,"Non-invasive brain stimulation (NIBS) techniques such as transcranial direct current stimulation (tDCS) and transcranial magnetic stimulation (TMS) have been used to investigate the relationship between activity in different cortical regions and cognitive processes <REF-1> , <REF-2> .",Non-invasive brain stimulation (NIBS) techniques such as transcranial direct current stimulation (tDCS) and transcranial magnetic stimulation (TMS) have been used to investigate the relationship between activity in different cortical regions and cognitive processes.,"Modify,Fact/Evidence",Introduction,Introduction
7-704,"Mean and peak E-field values corresponding to both tDCS montages were extracted by reconstructing the two-dimensional cortical surface (more precisely, the middle of the cortical sheet) of each individual along with the corresponding E-field cortical map in FreeSurfer, and an automated atlas-based parcellation of the frontal lobe <REF-41> was applied to each individual brain to delineate the lDLPFC.","Mean and peak E-field values corresponding to both tDCS montages were extracted by reconstructing the two-dimensional cortical surface (more precisely, the middle of the cortical sheet) of each individual along with the corresponding E-field cortical map in FreeSurfer, and applying automated atlas-based parcellation of the frontal lobe to delineate the lDLPFC region in each brain <REF-32> .","Modify,Clarity",Dataset validation,Dataset validation
7-704,A key advantage of NIBS is that it allows direct manipulation of neural excitability <REF-3> .,A key advantage of NIBS is that it allows direct manipulation of neural excitability.,"Modify,Fact/Evidence",Introduction,Introduction
7-704,"As a result, we show that (1) both protocols induce strong E-fields in the DLPFC (with symmetrical effects for the bipolar montage and unilateral E-field distribution for the 4x1 protocol), (2) E-field magnitudes and distributions are similar for our head models and for the NY Head, (3) all E-field measures (peak and mean strength, FI) show great degree of variability, and (4) montage-specific effects are consistent with previous results reported in the literature regarding both the spatial distribution and the magnitude of E-fields <REF-35> , <REF-42> – <REF-45> ( Figure 3 , Figure 4 ).","As a result, we show that (1) both protocols induce strong E-fields in the DLPFC (with symmetrical effects for the bipolar montage and unilateral E-field distribution for the 4×1 protocol), (2) effects are similar for our head models and for the NY Head, (3) all E-field measures (peak and mean strength, FI) show great degree of variability, and (4) montage-specific effects are consistent with previous results reported in the literature regarding both the spatial distribution and the magnitude of E-fields <REF-33> – <REF-37> ( Figure 3 , Figure 4 ).","Modify,Fact/Evidence",Dataset validation,Dataset validation
7-704,"Therefore when used carefully, it allows causal interpretation of how specific brain regions might be involved in mental phenomena such as perception <REF-4> , working memory <REF-5> , attention <REF-6> , decision-making <REF-7> or emotional regulation <REF-8> .","Therefore when used carefully, it allows causal interpretation of how specific brain regions might be involved in mental phenomena such as perception, working memory, attention, decision-making or emotional regulation.","Modify,Fact/Evidence",Introduction,Introduction
7-704,For detailed discussion of these results and that of Figure 4 we refer the reader to our accompanying paper <REF-14> .,For detailed discussion of the results presented in Figure 4 we refer the reader to our accompanying paper <REF-6> .,"Modify,Clarity",Dataset validation,Dataset validation
7-704,Our head models are compatible with SimNIBS 2.1.1 ( http://simnibs.de/ ) for simulating the effects of tDCS and TMS protocols.,Our head models are compatible with SimNIBS 2.0 ( http://simnibs.de/ ) for simulating the effects of tDCS and TMS protocols.,"Modify,Fact/Evidence",Usage notes,Usage notes
7-704,"During the past decade, it became clear that the electric field elicited by non-invasive brain stimulation (NIBS) techniques such as transcranial direct current stimulation (tDCS) and transcranial magnetic stimulation (TMS) are substantially influenced by variations in individual head and brain anatomy.","During the past decade, it became clear that the effects of non-invasive brain stimulation (NIBS) techniques such as transcranial direct current stimulation (tDCS) and transcranial magnetic stimulation (TMS) are substantially influenced by variations in individual head and brain anatomy.","Modify,Clarity",Abstract,Abstract
7-704,"The script will also output data registered to an average surface (’fsaverage’) which allows creating group averaged data, as we have shown previously <REF-14> .","This will enable registering data to an average surface (’fsaverage’) and creating group averaged data, as we have shown previously <REF-6> .","Modify,Clarity",Usage notes,Usage notes
7-704,"In addition, researchers have the opportunity to extract E-field components that are either radial (normal) or tangential relative to the cortical surface, and have been associated with different cellular effects <REF-46> .","In addition, using the upcoming new version of SimNIBS, researchers will have the opportunity to extract E-field components that are either radial (normal) or tangential relative to the cortical surface, and have been associated with different cellular effects <REF-38> .","Modify,Clarity",Usage notes,Usage notes
7-704,Head models of healthy and depressed adults for simulating the electric fields of non-invasive electric brain stimulation,Head models of healthy and depressed adults for simulating the effects of non-invasive brain stimulation,"Modify,Other",,
7-704,"Given the heterogeneity in the efficacy of NIBS protocols in modulating behavior and clinical symptoms, there has been a move towards computational modelling of the spatial distribution of E-fields in the brain to understand how stimulation parameters such as electrode placement, electrode rotation or electrode type affects current flow in the neural tissue.","Given the heterogeneity in the efficacy of NIBS protocols in modulating behavior and clinical symptoms, there has been a move towards computational modelling of the spatial distribution of E-fields in the brain to understand how stimulation parameters such as electrode placement, rotation or type affects current flow in the neural tissue.","Modify,Clarity",Introduction,Introduction
7-704,Methods and results,Methods,"Modify,Other",Head models of healthy and depressed adults for simulating the electric fields of non-invasive electric brain stimulation,Head models of healthy and depressed adults for simulating the effects of non-invasive brain stimulation
7-704,"Automated tissue segmentation was performed in SPM12 <REF-34> for skin, skull, eyeballs, CSF and major air cavities, and in FreeSurfer for gray and white matter.","Automated tissue segmentation was performed in SPM12 <REF-26> for skin, skull, eyeballs and CSF, and in FreeSurfer for gray and white matter.","Modify,Fact/Evidence",Creation of head models,Creation of head models
7-704,"Moreover, the resulting masks were not corrected for inconsistencies relating to subcortical nuclei and thus, these head models are not suitable for estimating stimulation-related E-fields in structures such as the thalamus, basal ganglia, amygdala or the cerebellum.","Moreover, the resulting masks were not corrected for inconsistencies relating to subcortical nuclei and thus, these head models are not suitable for estimating stimulation-related E-fields in structures such as the thalamus, basal ganglia, amygdala, or brainstem nuclei.","Modify,Claim",Creation of head models,Creation of head models
7-704,"By using a freely available software package for modelling the electric fields induced by different NIBS protocols, we show that our head models are well-suited for assessing inter-individual and between-group variability in the magnitude and focality of tDCS-induced electric fields for two protocols targeting the left dorsolateral prefrontal cortex.","By using a freely available software package for modelling the effects of different NIBS protocols, we show that our head models are well-suited for assessing inter-individual and between-group variability in the magnitude and focality of tDCS-induced electric fields for two protocols targeting the left dorsolateral prefrontal cortex.","Modify,Fact/Evidence",Abstract,Abstract
7-704,"Furthermore, because the original dataset was de-faced and did not include the neck/shoulder region, our head models do not include these regions.","Additionally, because the original dataset was de-faced and did not include the neck/shoulder region, our head models do not include these regions.","Modify,Clarity",Creation of head models,Creation of head models
7-704,"Head models were created with a custom version of SimNIBS 2.1 <REF-23> , a freely available software package for simulating the effects of NIBS techniques.","Head models were created with an extended, pre-release version of SimNIBS 2.1 <REF-15> , a freely available software package for simulating the effects of NIBS techniques.","Modify,Clarity",Creation of head models,Creation of head models
7-738,"In their study to assess PE and deep vein thrombosis (DVT) inpatient costs in the United States, Lamori et al . found that the mean cost of initial hospitalization for acute PE was approximately $37,006 per patient <REF-3> .",,"Add,Fact/Evidence",Introduction,
7-738,"This figure was higher for older patients, women and readmissions.",,"Add,Fact/Evidence",Introduction,
7-738,"There was no chest pain, tachypnea or tachycardia at this time.",,"Add,Fact/Evidence",Case report,
7-738,It exacts a huge economic burden both on the sufferer and the health system.,"It exacts a huge economic burden both on the sufferer and the health system with some estimates placing the annual cost of care between $7,594 to $16,644 per patient <REF-3> .","Modify,Fact/Evidence",Introduction,Introduction
7-738,"Prompt diagnosis is, therefore, essential to reduce disease burden.",Prompt diagnosis is essential to reduce disease burden.,"Modify,Clarity",Introduction,Introduction
7-738,"Chest pain was described as left-sided, non-pleuritic, non-radiating, retrosternal, squeezing in character and persistent.","Chest pain was described as left-sided non-pleuritic, non-radiating retrosternal, squeezing in character and persistent.","Modify,Grammar",Case report,Case report
7-738,"On this visit, his pulse rate was 84 beats per minute; BP 119/66 mm/Hg; respiration rate 16 breaths per minute and his oxygen saturation was 98% on room air.","In this visit, his pulse rate was 84 beats per minute; BP 119/66 mm/Hg; respiration rate 16 breaths per minute and his oxygen saturation was 98% on room air.","Modify,Grammar",Case report,Case report
7-738,EKG showed deep T wave inversions in leads V1–V6 and the inferior limb leads ( Figure 1 ).,EKG showed deep T wave inversions in leads V1-V6 and the inferior limb leads ( Figure 1 ).,"Modify,Grammar",Case report,Case report
7-738,Cardiac catheterization revealed normal coronaries ( Supplementary file 1 ).,Left heart catheterization revealed normal coronaries.,"Modify,Fact/Evidence",Case report,Case report
7-738,"While the patient was still lying on the cardiac cath table, his oxygen saturation dropped to 91%.",Oxygen saturation dropped to 91% in room air while the patient laid on catheterization table but improved with supplemental oxygen via nasal cannula.,"Split+Modify,Clarity",Case report,Case report
7-738,Supplemental oxygen at 2l/min via nasal cannula improved saturation to 97%.,Oxygen saturation dropped to 91% in room air while the patient laid on catheterization table but improved with supplemental oxygen via nasal cannula.,"Split+Modify,Fact/Evidence",Case report,Case report
7-738,A repeat EKG showed a Q 3 T 3 pattern in lead III ( Figure 2 ).,A repeat EKG at this time showed a Q 3 T 3 pattern in lead III ( Figure 2 ).,"Modify,Clarity",Case report,Case report
7-738,"In view of these new findings (low oxygen saturation and a change in the EKG pattern), a computerized tomography of the chest with angiogram (chest CTA) was ordered.","This was followed by a computerized tomography of the chest with angiogram (chest CTA), which revealed a saddle pulmonary embolus which extended into the right and left pulmonary arteries and involved all lobar branches of the pulmonary arteries.","Split+Modify,Clarity",Case report,Case report
7-738,This revealed a saddle pulmonary embolus which extended into the right and left pulmonary arteries and involved all lobar branches of the pulmonary arteries ( Figure 3 ).,"This was followed by a computerized tomography of the chest with angiogram (chest CTA), which revealed a saddle pulmonary embolus which extended into the right and left pulmonary arteries and involved all lobar branches of the pulmonary arteries.","Split+Modify,Fact/Evidence",Case report,Case report
7-738,The clues to possible acute PE in our case was the transient desaturation that occurred during cardiac catheterization and the observed change on repeat EKG.,The only clue to possible acute PE in our case was the transient desaturation that occurred during cardiac catheterization.,"Modify,Fact/Evidence",Discussion,Discussion
7-738,These dictated the urgency of getting a chest CTA.,This dictated the urgency of getting a chest CTA.,"Modify,Grammar",Discussion,Discussion
7-906,"In Berg-Nielsen ’ s review, mothers with depression andanxiety, as well as parents with certain personality disorders, have a parental style often characterized by some aspect of negativity <REF-20> .",,"Add,Fact/Evidence",Discussion,
7-906,"In a systematic review by Christian, 2017, depression and anxiety were noted as a direct link to difficulties in the parent-child relationship and poor parent-child interactions <REF-18> .",,"Add,Fact/Evidence",Discussion,
7-906,"In a meta analytic review by van der Bruggen et al ., 2008, direction association between child anxiety, and parental control was unknown <REF-22> .",,"Add,Fact/Evidence",Discussion,
7-906,When clients through the therapeutic process build skills that lead to successes in their parenting relationships they gain the confidence needed to keep improving the parent-child relationship <REF-18> .,,"Add,Fact/Evidence",Discussion,
7-906,"In line with our study, in the Erel and Burman, 1995 meta-analytic review, there was a positive relationship between the quality of the marital relationship and the quality of the parent-child relationship <REF-68> .",,"Add,Fact/Evidence",Discussion,
7-906,Parents with a satisfying marital relationship may receive more support from their spouse; the positive feeling from a satisfying marital relationship may spill over to a parent-child relationship <REF-25> .,,"Add,Fact/Evidence",Discussion,
7-906,"In line with Berg-Nielsen’s review <REF-20> , our study association between maternal drug use and dysfunctional parenting was reported.",,"Add,Fact/Evidence",Discussion,
7-906,"In Berg- Nielsen’s review, anxiety in children has been correlated with parental negative control, rejection, and inconsistency.",,"Add,Fact/Evidence",Discussion,
7-906,"And parents of depressed children may be less warm and supportive, less communicative, and more critical <REF-20> .",,"Add,Fact/Evidence",Discussion,
7-906,"In conclusion, the review showed that some child or parental psychopathologic factors contribute to dysfunctional parenting.",,"Add,Fact/Evidence",Discussion,
7-906,Implication of findings,,"Add,Other",Discussion,
7-906,"Consultants, psychologists, and therapists can use the findings of this study to provide services to families.",,"Add,Claim",Implication of findings,
7-906,,Parenting styles refer to the set of strategies adopted by parents to control the behaviors of their children <REF-1> .,"Delete,Fact/Evidence",,Introduction
7-906,,These characteristics are created in the process of parents’ growth and development <REF-8> .,"Delete,Fact/Evidence",,Discussion
7-906,,"In conclusion, considering these multiple psychological factors influencing parenting styles, we recommended including parent-child psychological status assessment in family health programs in order to identify the needs for health-oriented care and take steps towards the development of parenting skills among parents.","Delete,Claim",,Discussion
7-906,"Parenting styles consist of a constellation of parental behaviors, beliefs, and attitudes displayed across a variety of parent-child interactions and so specific parenting behaviors that parents use to socialize their child <REF-1> .",,"Add,Fact/Evidence",Introduction,
7-906,Baumrind (1971) develop a popular theory of parenting styles in which she identified three different parenting styles are mostly used in literature.,,"Add,Fact/Evidence",Introduction,
7-906,"Later, (in the 1980s) a fourth was added to her theory <REF-2> – <REF-5> .",,"Add,Fact/Evidence",Introduction,
7-906,"Authoritative parents are warm and communicative, but they also exert appropriate control.",,"Add,Fact/Evidence",Introduction,
7-906,"Authoritarian parents exert control while lacking warmth, while permissive parents show warmth but do not exert control).",,"Add,Fact/Evidence",Introduction,
7-906,"Finally, parents with lacking warmth and control have neglectful parenting.",,"Add,Fact/Evidence",Introduction,
7-906,Some researchers define parenting styles as specific interpersonal parental behaviors or characteristics that influence child development.,,"Add,Fact/Evidence",Introduction,
7-906,"For example, sensitivity, responsiveness, affect, reciprocity, negativity, involvement, harsh discipline <REF-6> , <REF-7> .",,"Add,Fact/Evidence",Introduction,
7-906,"In the present study, parental behaviors or characteristics were used as models of parenting styles <REF-8> .",,"Add,Fact/Evidence",Introduction,
7-906,Clarifying these factors is important for family therapeutic intervention.,,"Add,Claim",Introduction,
7-906,This checklist includes 8 questions for case-control studies and cohort studies with a maximum 9 score.,This checklist includes 8 questions for case control studies and cohort studies with maximum 9 score.,"Modify,Grammar",Risk of bias,Risk of bias
7-906,"In this review, studies that received ≥ 5 scores from the Newcastle–Ottawa scale and The HE QAT were included <REF-30> , <REF-33> .","In this review, studies that received ≥ 5 score from Newcastle–Ottawa scale and The HE QAT were included <REF-20> , <REF-23> .","Modify,Grammar",Risk of bias,Risk of bias
7-906,A summary of the included studies is presented in Table 1 .,A summary of included studies are presented in Table 1 .,"Modify,Grammar",Results,Results
7-906,"These parents may have low self-esteem, reduced self-efficacy, negative emotions, more anger, and distress, as well as negative worthlessness to themselves or negative attitudes towards their parenting abilities <REF-17> , <REF-35> , <REF-36> , which have an impact on the trust between parents and children <REF-18> , <REF-37> , <REF-38> .","These parents may have low self-esteem, reduced self-efficacy, negative emotions, more anger and distress, as well as negative worthlessness to themselves or negative attitudes towards their parenting abilities <REF-11> , <REF-26> , <REF-27> , which have an impact on the trust between parents and children <REF-12> , <REF-28> , <REF-29> .","Modify,Grammar",Psychological characteristics relating to parents,Psychological characteristics relating to parents
7-906,"Such parents may use harassment of their children as the first choice of parenting <REF-18> , or parents’ interactions with children and their parenting may be accompanied by excessive control and rejection <REF-39> , <REF-40> .","Such parents may use harassment of their children as a first choice of parenting <REF-12> , or parents’ interactions with children and their parenting may be accompanied by excessive control and rejection <REF-30> , <REF-31> .","Modify,Grammar",Psychological characteristics relating to parents,Psychological characteristics relating to parents
7-906,"Moreover, these parents may show their love for their children when children act in accordance with parents expectations.","Moreover, these parents may show their love to their children when children act in accordance with parents expectations.","Modify,Grammar",Psychological characteristics relating to parents,Psychological characteristics relating to parents
7-906,The purpose of this systematic review was to examine psychological factors affecting parenting style.,"Thus, the purpose of this systematic review was to examine psychological factors affecting parenting style.","Modify,Clarity",Abstract,Abstract
7-906,"As a result of emotional security, behavioral independence and social competence created in them can lead to the formation of a healthy personality and personal maturity and these people can rely more on others.","As the result of emotional security, behavioral independence and social competence created in them can lead to the formation of a healthy personality and personal maturity and these people can rely more on others.","Modify,Grammar",Psychological characteristics relating to parents,Psychological characteristics relating to parents
7-906,"Marital problems, as well as psychological disorders of substance-abusing individuals, are related to poor parenting <REF-20> .",Marital problems as well as psychological disorders of substance-abusing individuals are related to poor parenting <REF-14> .,"Modify,Grammar",Psychological characteristics relating to parents,Psychological characteristics relating to parents
7-906,"Consequently, increased self-efficacy and reduced parenting stress, as well as lower depression and anxiety in parents, could lead to the adoption of more appropriate strategies <REF-18> , <REF-23> , <REF-25> , <REF-35> .","Consequently, increased self-efficacy and reduced parenting stress as well as lower depression and anxiety in parents could lead to the adoption of more appropriate strategies <REF-12> , <REF-24> , <REF-26> , <REF-34> .","Modify,Grammar",Discussion,Discussion
7-906,"Moreover, dimensions of perfectionism in parents and parental personality traits could affect parenting styles <REF-7> , <REF-45> , <REF-47> .","Moreover, dimensions of perfectionism in parents and parental personality traits could effect parenting styles <REF-37> , <REF-39> , <REF-40> .","Modify,Grammar",Discussion,Discussion
7-906,"The range of parents’ psychological disturbances such as depression and anxiety could also affect parental dysfunction, leading to child maltreatment; and consequently, parents’ psychopathology could increase the likelihood of inappropriate and ineffective parenting <REF-20> , <REF-23> .","The range of parents’ psychological disturbances such as depression and anxiety could also affect parental dysfunction, leading to child maltreatment; and consequently parents’ psychopathology could increase the likelihood of inappropriate and ineffective parenting <REF-14> , <REF-24> .","Modify,Grammar",Discussion,Discussion
7-906,"Despite these limitations, it seems the result of this study can be used in the development and implementation of family health intervention programs.",Despite these limitations it seems the result of this study can be used in the development and implementation of family health intervention programs.,"Modify,Grammar",Limitations,Limitations
7-906,"Also, clinicians, psychologists, psychiatrists, and counselors may consider the psychological factors affecting parenting styles reported in this review for further interventions; the assessment of parent-child mental health status, as well as positive parenting education and in this way help with positive parent-child interactions.","Also clinicians, psychologists, psychiatrists, and counselors may consider the psychological factors affecting parenting styles reported in this review for further interventions; the assessment of parent-child mental health status, as well as positive parenting education and in this way help with positive parent-child interactions.","Modify,Grammar",Limitations,Limitations
7-906,A third author (FE) reviewed the evidence tables for accuracy and completeness.,Third author (FE) reviewed the evidence tables for accuracy and completeness.,"Modify,Grammar","Summarization, extraction, and data reporting","Summarization, extraction, and data reporting"
7-906,The results of the literature review led to the categorization of the contents on psychological factors contributing to parenting styles into several categories as presented in the Results section.,The results of the literature review led to categorization of the contents on psychological factors contributing to parenting styles into several categories as presented in the Results section.,"Modify,Grammar","Summarization, extraction, and data reporting","Summarization, extraction, and data reporting"
