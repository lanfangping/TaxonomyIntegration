{
    "nodes": [
        {
            "ix": "54-ARR_v1_0",
            "content": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_2",
            "content": "Early exiting allows instances to exit at different layers according to the estimation of difficulty. Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning. In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way. Though some effort has been devoted to employing such \"learn-to-exit\" modules, it is still unknown whether and how well the instance difficulty can be learned. As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty. Based on this observation, we propose a simple-yet-effective Hashbased Early Exiting approach (HASHEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer. Different from previous methods, HASHEE requires no internal classifiers nor extra parameters, and therefore is more efficient. Experimental results on classification, regression, and generation tasks demonstrate that HASHEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "54-ARR_v1_4",
            "content": "Early exiting is a widely used technique to accelerate inference of deep neural networks. With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community. At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late. Thus, how to measure instance difficulty is a crucial problem.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_5",
            "content": "Most existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty. However, these methods can not easily generalize to new tasks. On the one hand, these metrics are not accessible on some tasks such as regression. On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_6",
            "content": "Another way to measure instance difficulty is to directly learn it. Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results. They jointly train a neural model to predict for each instance the exiting layer. At their core, the learn-to-exit module is to estimate the difficulty for each instance. Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_7",
            "content": "Despite their success, it is still unknown whether or how well the instance difficulty can be learned. As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty. The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot. For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006). The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier. Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets. Experimental results demonstrate that, modern neural networks perform poorly on predicting instance difficulty. This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_8",
            "content": "Given that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules? We hypothesis that the consistency between training and inference may play an important role. That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too. Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules. If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function. In particular, we use hash functions to assign each token to a fixed exiting layer. This hash-based early exiting method is named HASHEE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_9",
            "content": "Compared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation. (c) The speed-up ratio can be easily tuned by modifying the hash function. (d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_10",
            "content": "We conduct experiments on classification, regression, and generation tasks. Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks. Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT while maintaining 97% ROUGE-1 score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_11",
            "content": "2 Can Instance Difficulty Be Learned?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_12",
            "content": "In this section, we examine whether or to what extent instance difficulty can be learned. In particular, we manage to evaluate how well a neural network that trained on some data with difficulty annotation can generalize to unseen data. Here we consider two kinds of difficulty: human-defined difficulty and model-defined difficulty.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_13",
            "content": "Human-defined Difficulty",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "54-ARR_v1_14",
            "content": "Dataset Construction Human-defined difficulty of an instance measures how difficult for human to judge its label. To construct such a dataset, we use the SNLI dataset (Bowman et al., 2015), which is a collection of 570k human-written English sentence pairs that are manually labeled with the inference relation between the two sentences: entailment, contradiction, or neutral. The labels in SNLI are determined by the majority of the crowd-sourced annotators. If there is no majority for an instance, its label would be \"Unknown\". We collect 1,119 unknown instances from SNLI dataset as our difficult instances, and collect 1,119 labeled instances from the instances of three classes (i.e., entailment, contradiction, and neutral) in equal proportion as our simple instances, obtaining a balanced binary classification (difficult or simple) dataset with 2,238 instances. We randomly sample 1,238 instances with balanced labels as training set and use the remaining 1,000 instances as test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_15",
            "content": "Learning Human-defined Difficulty We then train a BERT model (Devlin et al., 2019) with a linear classifier on the top on our constructed training set, and evaluate on the test set to see if it can predict whether an unseen instance is simple or difficult. As shown in Figure 1, the BERT model that fits well on the training set can only achieve \u223c60% accuracy on the test set, demonstrating that neural models (even BERT) can not easily learn to estimate human-defined difficulty.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_16",
            "content": "Model-defined Difficulty",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "54-ARR_v1_17",
            "content": "However, model can have a different view of instance difficulty from human. For example, an instance can be defined as a difficult one if it can not be correctly predicted by a well-trained model. Thus, we also construct datasets to characterize model-defined difficulty for each instance, which is more realistic in the context of early exiting. In particular, we construct two datasets labeled with model-defined difficulty at sentence-level and token-level, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_18",
            "content": "Sentence-level Difficulty Estimating modeldefined difficulty of a sentence (or sentence pairs) is helpful to language understanding tasks such as text classification and natural language inference (Xin et al., 2021). To obtain the sentence-level difficulty, we train a multi-exit BERT that is attached with an internal classifier at each layer on SNLI training set. Once the multi-exit BERT is trained, it can serve as an annotator to label each instance in the SNLI development set whether it can be correctly predicted by each internal classifier. In our experiments, we use BERT BASE that has 12 layers, and therefore for each instance in the SNLI development set we have 12 labels, each takes values of 0 or 1 to indicate whether or not the corresponding internal classifier correctly predict its label. By this, we label the 9,842 SNLI development instances to construct a multi-label classification dataset, from which we randomly sample 8,000 instances as training set and use the remaining 1,842 instances as test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_19",
            "content": "Token-level Difficulty We also construct a dataset for estimating model-defined difficulty of each token, which can be used in language generation tasks (Elbayad et al., 2020) and sequence labeling tasks (Li et al., 2021b). Similarly, we train a multi-exit BERT on OntoNotes NER (Hovy et al., 2006) training set, and use it to annotate each token in the OntoNotes development instances whether it can be correctly predicted by each internal classifier. By this, we obtain a token-level multilabel classification datasets consisting of 13,900 *: The majority model for the token-level task would always predict positive class for all the labels, and therefore the F1 score is not applicable.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_20",
            "content": "instances, from which we randomly sample 10,000 instances to construct a training set and use the remaining 3,900 instances as test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_21",
            "content": "Learning Model-defined Difficulty For each constructed model-defined difficulty dataset, we evaluate several models: (1) Majority model always predicts the majority class for each label, with class priors learned from the training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_22",
            "content": "(2) Linear-M is a multi-classification linear layer that takes as input the average pooled word embeddings and outputs the exiting layer. This model corresponds to the multinomial variants of Elbayad et al. (2020). Since the inputs of Linear-M is noncontextualized, we did not apply it to estimate token-level difficulty.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_23",
            "content": "(3) Linear-B is a binary classification linear layer that takes as input the hidden states at each BERT layer and outputs whether or not the instance (or token) is correctly predicted. This model corresponds to the geometric variants of Elbayad et al. (2020) and the learn-to-exit module in BERxiT (Xin et al., 2021). ( 4) We also train and evaluate a bidirectional LSTM model (Hochreiter and Schmidhuber, 1997) with one layer and hidden size of 256. It takes as input the instance and outputs the exiting layer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_24",
            "content": "(5) BERT model (Devlin et al., 2019) is also considered for this task. For these models, except for Linear-B, we use the binary cross entropy loss to handle the multi-label classification. Since most development instances are correctly predicted, our constructed datasets are label-imbalanced. To alleviate this issue, we adopt over-sampling for classes with fewer instances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_25",
            "content": "Our experimental results are shown in Figure 2, from which we find that: (1) For the task of estimating sentence-level difficulty, the shallow neural models perform as well as simple majority model. Only the BERT model can slightly outperform the majority model. ( 2) For token-level difficulty, these neural models perform slightly better than the majority model. The insignificant improvement over the majority model demonstrate that, the performance of the neural models mainly come from the learning of prior distribution of label instead of extracting difficulty-related features from instances. In the case of label imbalance, the accuracy can not well measure model performance. Besides, in the context of early exiting, we are more interested in cases that the model performs a false exit for an unsolved instance. Thus, we also report the precision, recall, and F1 score on the negative class. As shown in Table 1, all the evaluated models perform poorly on recognizing the incorrectly predicted instances and tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_26",
            "content": "Though, it can not be concluded that the instance difficulty can not be learned since there are still a variety of machine learning models and training techniques that are under explored. Our preliminary experiments demonstrate that, at least, instance difficulty, whether human-defined or modeldefined, is hard to learn for modern neural networks. In fact, our evaluated learn-to-exit models are upper baselines than that used in previous work because: (1) we also adopt more powerful deep models instead of simple linear models in previous methods (Elbayad et al., 2020;Xin et al., 2021), and (2) Different from our method that trains learnto-exit module on development set, previous methods jointly train their learn-to-exit module on the training set where few instances are incorrectly predicted, leading to more serious label imbalance. To facilitate future research, our constructed difficulty datasets will be publicly available.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_27",
            "content": "HASHEE: Hash Early Exiting",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "54-ARR_v1_28",
            "content": "What is Unnecessary and What Works?",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "54-ARR_v1_29",
            "content": "On the one hand, previous methods (Elbayad et al., 2020;Xin et al., 2021) that use learn-to-exit modules have achieved competitive results, which implies that something works in the learn-to-exit modules. On the other hand, our preliminary experiments show that instance difficulty is hard to be predicted in advance, which indicates that learning can be unnecessary to achieve a good performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_30",
            "content": "To find what works, we formally describe the prediction of an early exiting model as P (y|x) = d\u2208D P (y|x, d)P (d|x), where d is the difficulty (e.g., the exiting layer) for x. Note that in practice, P (D|x) is an one-hot distribution, so when d is predicted, the exiting layer, i.e., the model architecture is determined. Therefore, the difficulty d actually corresponds to an architecture. 1 Now given that the mapping from instance x to its difficulty d, i.e., the best architecture, is hard to be learned, a natural idea to make P (y|x) performs well is to keep P (d|x) consistent: if a training instance x i is predicted to exit at layer l, then an inference instance x j that is similar with x i should exit at layer l, too. By this, the activated architecture can well-handle the instance x j during inference because it is well-trained on similar instances such as x i . Note that this consistency between training and inference can be easily satisfied by previous learnto-exit modules due to the smoothness of neural models (Ziegel, 2003). Based on this hypothesis, we manage to remove the learning process and only stick to the consistency. In particular, we replace the neural learn-to-exit module P (d|x) with a simple hash function.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_31",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "54-ARR_v1_32",
            "content": "Without loss of generality, we first consider sequence classification tasks. A straightforward idea is to design a hash function to map semantically similar instances into the same bucket, and therefore the hash function should be some powerful sequence encoder such as Sentence-BERT (Reimers and Gurevych, 2019), which is cumbersome in computation. In addition, a high-quality sequence encoder as a hash function usually maps instances with the same label into the same bucket (i.e. the same exiting layer), which makes the internal classifier at that layer suffer from label imbalance. Due to the difficulty of holding consistency at sentencelevel, we rather propose to hold the consistency at token-level. By assigning each token into a fixed bucket, the token-level consistency between training and inference is easily satisfied.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_33",
            "content": "An overview of our method is illustrated in Figure 3. We adopt a simple and efficient hash function to map each token into a fixed bucket in advance, where each bucket corresponds to an exiting layer. We use pre-trained Transformers (Vaswani et al., 2017) as our backbones. During model's forward pass, the representation of exited tokens will not be updated through self-attention, and its hidden states of the upper layers are directly copied from the hidden states of the exiting layer. By this token-level early exiting, the computation in self-attention and the following feed-forward network is reduced.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_34",
            "content": "Hash Functions",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "54-ARR_v1_35",
            "content": "To hold the token-level consistency between training and inference, HASHEE employs hash functions to compute in advance the exiting layer for each token. During training and inference, each token exits at a fixed layer according to the precomputed hash lookup table. The hash functions can take a variety of forms. Here we consider several hash functions as possible alternatives.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_36",
            "content": "Random Hash Random hash is a lower baseline, wherein we assign each token to a fixed, random exiting layer at initialization. To examine our hypothesis, we also consider to use two different random hash functions for training and inference respectively, in which case the consistency does not hold. We denote these two random hash functions as Rand-cons and Rand-incons.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_37",
            "content": "Frequency Hash To achieve higher speed-up, a natural way is to assign frequent tokens to lower layers to exit. Intuitively, frequent tokens are usually well-trained during pre-training and therefore do not require too much refinement by looking at their contexts. Thus we can design a hash function that assigns tokens into exiting layers by frequency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_38",
            "content": "In particular, the tokens are sorted by frequency and then divided equally into B buckets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_39",
            "content": "MI Hash Further, we also consider a taskspecific hash function that is based on the mutual information (MI) between each token and the corresponding label, which, as an instance of HASHEE, is also adopted in Liu et al. (2021b). Tokens are sorted by their MI values between the task label, and then divided equally into B buckets. Tokens with higher MI values are assigned to lower layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_40",
            "content": "Clustered Hash It is also intuitive that similar tokens should be assigned to the same layer to exit, and therefore we also experiment with a clustered hash function. The clusters are obtained by performing k-means clustering using token embeddings from BERT BASE embedding layer. The clustered tokens are then sorted by norm, which often relates to token frequency (Schakel and Wilson, 2015) and difficulty . The clustered tokens with small average norms are assigned to lower layers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_41",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "54-ARR_v1_42",
            "content": "Tasks and Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "54-ARR_v1_43",
            "content": "Since HASHEE requires no supervision, it can be applied to a variety of tasks and architectures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_44",
            "content": "In our work, we conduct experiments on natural language understanding tasks including sentiment analysis, natural language inference, similarity regression, and a language generation task, text summarization. Statistics of our used datasets are shown in Appendix A.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_45",
            "content": "Understanding Tasks For the convenience of comparison with other efficient models, we evaluate our proposed HASHEE on the ELUE benchmark (Liu et al., 2021a) Generation Tasks For language generation, we evaluate HASHEE on two English summarization datasets, CNN/DailyMail (Hermann et al., 2015) and Reddit (Kim et al., 2019), and two Chinese summarization datasets: TTNews (Hua et al., 2017) and CSL (Xu et al., 2020b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_46",
            "content": "Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "54-ARR_v1_47",
            "content": "Baselines We compare HASHEE with the following competitive baseline models:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_48",
            "content": "(1) Pre-Trained",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_49",
            "content": "Table 2: Main results on the ELUE benchmark (Liu et al., 2021a). We report for each model on each task the performance and the corresponding speed-up ratio, which is calculated as the FLOPs reduction relative to BERT BASE .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_50",
            "content": "For MRPC, we report the mean of accuracy and F1. For STS-B, we report Pearson and Spearman correlation. For all other tasks we report accuracy. \"-\" indicates that the method is not applicable on that task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_51",
            "content": "Language Models. We directly fine-tune the first layers of pre-trained language models including BERT (Devlin et al., 2019), ALBERT , RoBERTa , and ElasticBERT (Liu et al., 2021a) with a MLP classifier on the top.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_52",
            "content": "(2) Static Models. We compare with several static approaches to accelerate language model inference, including Distil-BERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2020), HeadPrune (Michel et al., 2019), and BERTof-Theseus (Xu et al., 2020a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_53",
            "content": "(3) Dynamic models. We compare with DeeBERT , FastBERT (Liu et al., 2020a), PABEE , BERxiT (Xin et al., 2021), and Cascade-BERT (Li et al., 2021a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_54",
            "content": "Training For most NLU experiments we adopt the ElasticBERT BASE model (Liu et al., 2021a) as our backbone model, which is a pre-trained multi-exit Transformer encoder. For small datasets (i.e., SST-2, MRPC, and STS-B) we report the mean performance and the standard deviation (in Table 3 and 9) over 5 runs with different random seeds. For text summarization datasets we adopt BART BASE (Lewis et al., 2020) and CPT BASE as our backbone models and use the frequency hash to assign tokens Table 4: Experimental results on two English and two Chinese summarization datasets. We report ROUGE-1, ROUGE-2, and ROUGE-L for each dataset. The speedup ratios for English and Chinese models are calculated by the FLOPs reduction relative to BART BASE and CPT BASE , respectively, and averaged over the performed datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_55",
            "content": "Here we re-implement the confidence thresholding variant of DAT (Elbayad et al., 2020). ing the frequency hash that assigns tokens to the first 6 layers of ElasticBERT BASE , HASHEE can outperform most considered baselines with fewer FLOPs. To fairly compare with baselines of various speedup ratios, we also report the ELUE score (Liu et al., 2021a), which is a two-dimensional (performance and FLOPs) metric for efficient NLP models, measuring how much a model oversteps Elas-ticBERT. Table 2 shows that HASHEE achieved a new state-of-the-art ELUE score. To fairly compare with the learn-to-exit baseline we also implement BERxiT (Xin et al., 2021) with ElasticBERT BASE .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_56",
            "content": "We then evaluate HASHEE with different hash functions detailed in Section 3.3. For all these hash functions, we assign tokens to the 6 and 12 layers of ElasticBERT-6L and ElasticBERT-12L, respectively. Experimental results on SST-2, SNLI, and MRPC are given in Table 3. Among the hash functions, the frequency hash achieves the highest speedup while maintaining a considerable performance. With the backbone of ElasticBERT-12L, these hash functions, except for the frequency hash, cannot achieve considerable speedup. Besides, we find that ElasticBERT-12L did not significantly outperform ElasticBERT-6L with HASHEE. We conjecture that higher layers are not good at querying information from hidden states of tokens that exit too early. In this work, we are more interested in the case of high acceleration ratio, so we adopt ElasticBERT-6L as our main backbone. To make a more intuitive comparison of these hash functions with different speedup ratios, we also show in Figure 4 the ELUE scores on SST-2 and SNLI with ElasticBERT-6L as backbone. We find that the frequency hash outperforms other hash functions by a large margin, and therefore in the following experiments we mainly use the frequency hash. Besides, only the Rand-incons hash obtains negative ELUE score, demonstrating the benefit of maintaining consistency between training and inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_57",
            "content": "Comparison of Actual Inference Time Because most of the operations in the Transformer architecture are well optimized by modern deep learning frameworks and parallel processing hardwares such as GPU and TPU, FLOPs may not precisely reflect the actual inference time. To that end, here we also evaluate actual inference time on a single GeForce RTX 3090 GPU. Note that the speedup ratio of previous early exiting methods are usually tested on a per-instance basis, i.e. the batch size is set to 1. However, batch inference is often more favorable in both offline scenarios and low-latency scenarios (Zhang et al., 2019). Here we compare HASHEE with two baselines that have similar performance, i.e., FastBERT and PABEE. Our experiments are conducted on two datasets with very different average sentence length, i.e., SNLI and IMDb. Results are given in Table 5 and Figure 5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_58",
            "content": "We find HASHEE has an advantage in processing speed when the batch size exceeds 8. Besides, HASHEE can perform larger batch inference due to its memory-efficiency. Comparison of Different Backbones To evaluate the versatility of HASHEE, we also conduct experiments with other backbone models, i.e., BERT, ALBERT, and RoBERTa. As shown in Figure 6, HASHEE outperforms other baselines with the same backbone.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_59",
            "content": "Models Since HASHEE requires no supervision, it can also be applied to seq2seq models for generation tasks. We first evaluate HASHEE with BART BASE as our backbone on two English summarization tasks. As shown in Table 4, HASHEE can achieve significant speedup for BART encoder while maintaining considerable ROUGE scores. Besides, we find that previous early exiting methods that measure the uncertainty of internal outputs would rather slow down decoder inference due to the heavy computation of prediction over large vocabulary. In addition, to further explore the speedup potential of HASHEE, we also experiment with CPT , which has a deep encoder and a shallow decoder. Results on CSL and TTNews depict that HASHEE can achieve 2.3\u00d7 speedup relative to CPT while maintaining 97% ROUGE-1. We also report results of the 6-layer versions of BART (with 3 encoder layers and 3 decoder layers) and CPT (with 5 encoder layers and 1 decoder layer).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_60",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "54-ARR_v1_61",
            "content": "Large-scale pre-trained language models (PLMs) have achieved great success in recent years. Despite their power, the inference is time-consuming, which hinders their deployment in low-latency scenarios. To accelerate PLM inference, there are currently two streams of work: (1) Compressing a cumbersome PLM through knowledge distillation (Sanh et al., 2019;Sun et al., 2019;Jiao et al., 2020), model pruning (Gordon et al., 2020;Michel et al., 2019), quantization (Shen et al., 2020), module replacing (Xu et al., 2020a), etc. (2) Selectively activating parts of the model conditioned on the input, such as Universal Transformer (Dehghani et al., 2019), FastBERT (Liu et al., 2020a), DeeBERT , PABEE , LeeBERT (Zhu, 2021), CascadeBERT (Li et al., 2021a), ElasticBERT (Liu et al., 2021a) and other similar methods (Elbayad et al., 2020;Schwartz et al., 2020;Liao et al., 2021;Xin et al., 2021;Sun et al., 2021). Different from these methods, our proposed HASHEE requires no internal classifiers (which imply extra parameters) and supervision, and therefore can be widely used in a variety of tasks and model architectures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_62",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "54-ARR_v1_63",
            "content": "We first empirically study the learnability of instance difficulty, which is a crucial problem in early exiting. Based on the observation that modern neural models perform poorly on estimating instance difficulty, we propose a hash-based early exiting approach, named HASHEE, that removes the learning process and only sticks to the consistency between training and inference. Our experiments on classification, regression, and generation tasks show that HASHEE can achieve state-of-the-art performance with fewer computation and inference time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_64",
            "content": "Here we list the statistics of our used language understanding and generation datasets in Table 6 and Table 7.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_65",
            "content": "For small datasets in ELUE, i.e. SST-2, MRPC, and STS-B, we conduct grid search over batch sizes of {16, 32}, learning rates of {2e-5, 3e-5, 5e-5}, number of epochs of {3, 4, 5}, warmup step ratios of {0.1, 0.01}, and weight decays of {0.1, 0.01} with an AdamW optimizer. We select the hyperparameters that achieved the best performance on the development sets, and perform 5 runs with different random seeds to obtain the mean performance and standard deviation. For SNLI, SciTail, and IMDb, we use the same hyperparameters. All of the hyperparameters used in our language understanding experiments are given in Table 8. For English summarization tasks, i.e., CNN/DailyMail and Reddit, we use the same hyperparameters as BART. For Chinese summarization tasks, i.e., TTNews and CSL, we use the same hyperparameters as CPT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_66",
            "content": "In previous experiments we assign tokens to the same number of buckets as the number of layers. Here we also explore other configurations. For each configuration, we assign tokens to B buckets, corresponding to exiting layers {1 + 12b/B} B\u22121 b=0 . For instance, if we have 12 layers and 3 buckets, the 3 buckets correspond to the {1, 5, 9} layers. Overall results are given in Table 9, where we show results of 8 configurations with the frequency hash. Similar with Table 3, we find that 6-layer models perform well while achieving higher acceleration ratios. In addition, the number of buckets has no significant effect on acceleration ratio. Configurations that the number of layers equals to the number of buckets perform slightly better than other configurations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_67",
            "content": "Here we take a closer look at the HASHEE model forward process, and see which FLOPs are saved during inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_68",
            "content": "Given the hidden states at layer l as H l \u2208 R n\u00d7d and the hidden states of remaining tokens are denoted as h l \u2208 R m\u00d7d , where n is the original sequence length and m is the number of remaining tokens at layer l, the calculation of one Transformer encoder layer with HASHEE can be formally de-",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "54-ARR_v1_69",
            "content": "R Samuel, Gabor Bowman, Christopher Angeli, Christopher Potts,  Manning, A large annotated corpus for learning natural language inference, 2015-09-17, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "R Samuel",
                    "Gabor Bowman",
                    "Christopher Angeli",
                    "Christopher Potts",
                    " Manning"
                ],
                "title": "A large annotated corpus for learning natural language inference",
                "pub_date": "2015-09-17",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_70",
            "content": "UNKNOWN, None, 2017, Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation",
                "pub": "CoRR"
            }
        },
        {
            "ix": "54-ARR_v1_71",
            "content": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Lukasz Kaiser, Universal transformers, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Mostafa Dehghani",
                    "Stephan Gouws",
                    "Oriol Vinyals",
                    "Jakob Uszkoreit",
                    "Lukasz Kaiser"
                ],
                "title": "Universal transformers",
                "pub_date": "2019-05-06",
                "pub_title": "7th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_72",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: pre-training of deep bidirectional transformers for language understanding, 2019-06-02, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019-06-02",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_73",
            "content": "B William, Chris Dolan,  Brockett, Automatically constructing a corpus of sentential paraphrases, 2005-10, Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "B William",
                    "Chris Dolan",
                    " Brockett"
                ],
                "title": "Automatically constructing a corpus of sentential paraphrases",
                "pub_date": "2005-10",
                "pub_title": "Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_74",
            "content": "Maha Elbayad, Jiatao Gu, Edouard Grave, Michael Auli, Depth-adaptive transformer, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Maha Elbayad",
                    "Jiatao Gu",
                    "Edouard Grave",
                    "Michael Auli"
                ],
                "title": "Depth-adaptive transformer",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_75",
            "content": "Mitchell Gordon, Kevin Duh, Nicholas Andrews, Compressing BERT: studying the effects of weight pruning on transfer learning, 2020-07-09, Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Mitchell Gordon",
                    "Kevin Duh",
                    "Nicholas Andrews"
                ],
                "title": "Compressing BERT: studying the effects of weight pruning on transfer learning",
                "pub_date": "2020-07-09",
                "pub_title": "Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_76",
            "content": "Karl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend, 2015-12-07, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Karl Moritz Hermann",
                    "Tom\u00e1s Kocisk\u00fd",
                    "Edward Grefenstette",
                    "Lasse Espeholt",
                    "Will Kay",
                    "Mustafa Suleyman",
                    "Phil Blunsom"
                ],
                "title": "Teaching machines to read and comprehend",
                "pub_date": "2015-12-07",
                "pub_title": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_77",
            "content": "Sepp Hochreiter, J\u00fcrgen Schmidhuber, Long short-term memory, 1997, Neural Comput, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Sepp Hochreiter",
                    "J\u00fcrgen Schmidhuber"
                ],
                "title": "Long short-term memory",
                "pub_date": "1997",
                "pub_title": "Neural Comput",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_78",
            "content": "Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, Ralph Weischedel, Ontonotes: The 90% solution, 2006-06-04, Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, The Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Eduard Hovy",
                    "Mitchell Marcus",
                    "Martha Palmer",
                    "Lance Ramshaw",
                    "Ralph Weischedel"
                ],
                "title": "Ontonotes: The 90% solution",
                "pub_date": "2006-06-04",
                "pub_title": "Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings",
                "pub": "The Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_79",
            "content": "Lifeng Hua, Xiaojun Wan, Lei Li, Overview of the nlpcc 2017 shared task: Single document summarization, 2017, National CCF Conference on Natural Language Processing and Chinese Computing, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Lifeng Hua",
                    "Xiaojun Wan",
                    "Lei Li"
                ],
                "title": "Overview of the nlpcc 2017 shared task: Single document summarization",
                "pub_date": "2017",
                "pub_title": "National CCF Conference on Natural Language Processing and Chinese Computing",
                "pub": "Springer"
            }
        },
        {
            "ix": "54-ARR_v1_80",
            "content": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Tinybert: Distilling BERT for natural language understanding, 2020-11-20, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Xiaoqi Jiao",
                    "Yichun Yin",
                    "Lifeng Shang",
                    "Xin Jiang",
                    "Xiao Chen",
                    "Linlin Li",
                    "Fang Wang",
                    "Qun Liu"
                ],
                "title": "Tinybert: Distilling BERT for natural language understanding",
                "pub_date": "2020-11-20",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_81",
            "content": "Tushar Khot, Ashish Sabharwal, Peter Clark, Scitail: A textual entailment dataset from science question answering, 2018-02-02, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Tushar Khot",
                    "Ashish Sabharwal",
                    "Peter Clark"
                ],
                "title": "Scitail: A textual entailment dataset from science question answering",
                "pub_date": "2018-02-02",
                "pub_title": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "54-ARR_v1_82",
            "content": "Byeongchang Kim, Hyunwoo Kim, Gunhee Kim, Abstractive summarization of reddit posts with multi-level memory networks, 2019-06-02, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Byeongchang Kim",
                    "Hyunwoo Kim",
                    "Gunhee Kim"
                ],
                "title": "Abstractive summarization of reddit posts with multi-level memory networks",
                "pub_date": "2019-06-02",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "54-ARR_v1_83",
            "content": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, ALBERT: A lite BERT for self-supervised learning of language representations, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Zhenzhong Lan",
                    "Mingda Chen",
                    "Sebastian Goodman",
                    "Kevin Gimpel",
                    "Piyush Sharma",
                    "Radu Soricut"
                ],
                "title": "ALBERT: A lite BERT for self-supervised learning of language representations",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_84",
            "content": "Antonio Laverghetta, Jamshidbek Mirzakhalov, John Licato, Towards a task-agnostic model of difficulty estimation for supervised learning tasks, 2020-12-04, Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Antonio Laverghetta",
                    "Jamshidbek Mirzakhalov",
                    "John Licato"
                ],
                "title": "Towards a task-agnostic model of difficulty estimation for supervised learning tasks",
                "pub_date": "2020-12-04",
                "pub_title": "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_85",
            "content": "UNKNOWN, None, 2020, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_86",
            "content": ", BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [],
                "title": "BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_87",
            "content": "Lei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun, Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade, 2021, Findings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Lei Li",
                    "Yankai Lin",
                    "Deli Chen",
                    "Shuhuai Ren",
                    "Peng Li",
                    "Jie Zhou",
                    "Xu Sun"
                ],
                "title": "Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade",
                "pub_date": "2021",
                "pub_title": "Findings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_88",
            "content": "Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, Xuanjing Huang, Accelerating BERT inference for sequence labeling via earlyexit, 2021-08-01, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Xiaonan Li",
                    "Yunfan Shao",
                    "Tianxiang Sun",
                    "Hang Yan",
                    "Xipeng Qiu",
                    "Xuanjing Huang"
                ],
                "title": "Accelerating BERT inference for sequence labeling via earlyexit",
                "pub_date": "2021-08-01",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_89",
            "content": "Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, A global past-future early exit method for accelerating inference of pretrained language models, 2021-06-06, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Kaiyuan Liao",
                    "Yi Zhang",
                    "Xuancheng Ren",
                    "Qi Su"
                ],
                "title": "A global past-future early exit method for accelerating inference of pretrained language models",
                "pub_date": "2021-06-06",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_90",
            "content": "Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, Qi Ju, Fastbert: a selfdistilling BERT with adaptive inference time, 2020-07-05, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Weijie Liu",
                    "Peng Zhou",
                    "Zhiruo Wang",
                    "Zhe Zhao",
                    "Haotang Deng",
                    "Qi Ju"
                ],
                "title": "Fastbert: a selfdistilling BERT with adaptive inference time",
                "pub_date": "2020-07-05",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_91",
            "content": "UNKNOWN, None, , Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline",
                "pub": "CoRR"
            }
        },
        {
            "ix": "54-ARR_v1_92",
            "content": "Xuebo Liu, Houtim Lai, Derek Wong, Lidia Chao, Norm-based curriculum learning for neural machine translation, 2020-07-05, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Xuebo Liu",
                    "Houtim Lai",
                    "Derek Wong",
                    "Lidia Chao"
                ],
                "title": "Norm-based curriculum learning for neural machine translation",
                "pub_date": "2020-07-05",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_93",
            "content": "Yijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen, Jinan Xu, Faster depth-adaptive transformers, 2021-02-02, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Yijin Liu",
                    "Fandong Meng",
                    "Jie Zhou",
                    "Yufeng Chen",
                    "Jinan Xu"
                ],
                "title": "Faster depth-adaptive transformers",
                "pub_date": "2021-02-02",
                "pub_title": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "54-ARR_v1_94",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized BERT pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "1907",
                "pub_title": "Roberta: A robustly optimized BERT pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_95",
            "content": "Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, Christopher Potts, Learning word vectors for sentiment analysis, 2011-06-24, The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Andrew Maas",
                    "Raymond Daly",
                    "Peter Pham",
                    "Dan Huang",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Learning word vectors for sentiment analysis",
                "pub_date": "2011-06-24",
                "pub_title": "The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_96",
            "content": "Paul Michel, Omer Levy, Graham Neubig, Are sixteen heads really better than one?, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Paul Michel",
                    "Omer Levy",
                    "Graham Neubig"
                ],
                "title": "Are sixteen heads really better than one?",
                "pub_date": "2019-12-08",
                "pub_title": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_97",
            "content": "Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, Xuanjing Huang, Pre-trained models for natural language processing: A survey, 2020, SCIENCE CHINA Technological Sciences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Xipeng Qiu",
                    "Tianxiang Sun",
                    "Yige Xu",
                    "Yunfan Shao",
                    "Ning Dai",
                    "Xuanjing Huang"
                ],
                "title": "Pre-trained models for natural language processing: A survey",
                "pub_date": "2020",
                "pub_title": "SCIENCE CHINA Technological Sciences",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_98",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, J. Mach. Learn. Res, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "J. Mach. Learn. Res",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_99",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Nils Reimers",
                    "Iryna Gurevych"
                ],
                "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
                "pub_date": "2019-11-03",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_100",
            "content": "UNKNOWN, None, 2021, Hash layers for large sparse models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Hash layers for large sparse models",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_101",
            "content": "UNKNOWN, None, 2019, Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter",
                "pub": "CoRR"
            }
        },
        {
            "ix": "54-ARR_v1_102",
            "content": "UNKNOWN, None, 2015, Measuring word significance using distributed representations of words, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Measuring word significance using distributed representations of words",
                "pub": "CoRR"
            }
        },
        {
            "ix": "54-ARR_v1_103",
            "content": "Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, Noah Smith, 2020. The right tool for the job: Matching model and instance complexities, , Proceedings of the 58th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Roy Schwartz",
                    "Gabriel Stanovsky",
                    "Swabha Swayamdipta",
                    "Jesse Dodge",
                    "Noah Smith"
                ],
                "title": "2020. The right tool for the job: Matching model and instance complexities",
                "pub_date": null,
                "pub_title": "Proceedings of the 58th",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_104",
            "content": "UNKNOWN, None, 2020, Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_105",
            "content": "UNKNOWN, None, 2021, CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation",
                "pub": "CoRR"
            }
        },
        {
            "ix": "54-ARR_v1_106",
            "content": "Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael Mahoney, Kurt Keutzer, Q-BERT: hessian based ultra low precision quantization of BERT, 2020-02-07, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Sheng Shen",
                    "Zhen Dong",
                    "Jiayu Ye",
                    "Linjian Ma",
                    "Zhewei Yao",
                    "Amir Gholami",
                    "Michael Mahoney",
                    "Kurt Keutzer"
                ],
                "title": "Q-BERT: hessian based ultra low precision quantization of BERT",
                "pub_date": "2020-02-07",
                "pub_title": "The Thirty-Fourth AAAI Conference on Artificial Intelligence",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "54-ARR_v1_107",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013-10-21, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "Christopher Manning",
                    "Andrew Ng",
                    "Christopher Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013-10-21",
                "pub_title": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
                "pub": "ACL"
            }
        },
        {
            "ix": "54-ARR_v1_108",
            "content": "Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu, Patient knowledge distillation for BERT model compression, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Siqi Sun",
                    "Yu Cheng",
                    "Zhe Gan",
                    "Jingjing Liu"
                ],
                "title": "Patient knowledge distillation for BERT model compression",
                "pub_date": "2019-11-03",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_109",
            "content": "UNKNOWN, None, , Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_110",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Lukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017-12-04",
                "pub_title": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_111",
            "content": "Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, Cho-Jui Hsieh, Rethinking architecture selection in differentiable NAS, 2021-05-03, 9th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Ruochen Wang",
                    "Minhao Cheng",
                    "Xiangning Chen",
                    "Xiaocheng Tang",
                    "Cho-Jui Hsieh"
                ],
                "title": "Rethinking architecture selection in differentiable NAS",
                "pub_date": "2021-05-03",
                "pub_title": "9th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_112",
            "content": "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin, Deebert: Dynamic early exiting for accelerating BERT inference, 2020-07-05, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Ji Xin",
                    "Raphael Tang",
                    "Jaejun Lee",
                    "Yaoliang Yu",
                    "Jimmy Lin"
                ],
                "title": "Deebert: Dynamic early exiting for accelerating BERT inference",
                "pub_date": "2020-07-05",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_113",
            "content": "Ji Xin, Raphael Tang, Yaoliang Yu, Jimmy Lin, Berxit: Early exiting for BERT with better finetuning and extension to regression, 2021-04-19, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Ji Xin",
                    "Raphael Tang",
                    "Yaoliang Yu",
                    "Jimmy Lin"
                ],
                "title": "Berxit: Early exiting for BERT with better finetuning and extension to regression",
                "pub_date": "2021-04-19",
                "pub_title": "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_114",
            "content": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou, Bert-of-theseus: Compressing BERT by progressive module replacing, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Canwen Xu",
                    "Wangchunshu Zhou",
                    "Tao Ge",
                    "Furu Wei",
                    "Ming Zhou"
                ],
                "title": "Bert-of-theseus: Compressing BERT by progressive module replacing",
                "pub_date": "2020-11-16",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "54-ARR_v1_115",
            "content": "Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, Zhenzhong Lan, CLUE: A chinese language understanding evaluation benchmark, 2020-12-08, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Liang Xu",
                    "Hai Hu",
                    "Xuanwei Zhang",
                    "Lu Li",
                    "Chenjie Cao",
                    "Yudong Li",
                    "Yechen Xu",
                    "Kai Sun",
                    "Dian Yu",
                    "Cong Yu",
                    "Yin Tian",
                    "Qianqian Dong",
                    "Weitang Liu",
                    "Bo Shi",
                    "Yiming Cui",
                    "Junyi Li",
                    "Jun Zeng",
                    "Rongzhao Wang",
                    "Weijian Xie",
                    "Yanting Li",
                    "Yina Patterson",
                    "Zuoyu Tian",
                    "Yiwen Zhang",
                    "He Zhou",
                    "Shaoweihua Liu",
                    "Zhe Zhao",
                    "Qipeng Zhao",
                    "Cong Yue",
                    "Xinrui Zhang",
                    "Zhengliang Yang",
                    "Kyle Richardson",
                    "Zhenzhong Lan"
                ],
                "title": "CLUE: A chinese language understanding evaluation benchmark",
                "pub_date": "2020-12-08",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_116",
            "content": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, V Quoc,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Zhilin Yang",
                    "Zihang Dai",
                    "Yiming Yang",
                    "Jaime Carbonell",
                    "Ruslan Salakhutdinov",
                    "V Quoc",
                    " Le"
                ],
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "pub_date": "2019-12-08",
                "pub_title": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_117",
            "content": "Chengliang Zhang, Minchen Yu, Wei Wang, Feng Yan, Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving, 2019-07-10, 2019 USENIX Annual Technical Conference, USENIX ATC 2019, USENIX Association.",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Chengliang Zhang",
                    "Minchen Yu",
                    "Wei Wang",
                    "Feng Yan"
                ],
                "title": "Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving",
                "pub_date": "2019-07-10",
                "pub_title": "2019 USENIX Annual Technical Conference, USENIX ATC 2019",
                "pub": "USENIX Association"
            }
        },
        {
            "ix": "54-ARR_v1_118",
            "content": "Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian Mcauley, Ke Xu, Furu Wei, BERT loses patience: Fast and robust inference with early exit, 2020-12-06, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Wangchunshu Zhou",
                    "Canwen Xu",
                    "Tao Ge",
                    "Julian Mcauley",
                    "Ke Xu",
                    "Furu Wei"
                ],
                "title": "BERT loses patience: Fast and robust inference with early exit",
                "pub_date": "2020-12-06",
                "pub_title": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020",
                "pub": null
            }
        },
        {
            "ix": "54-ARR_v1_119",
            "content": "Wei Zhu, Leebert: Learned early exit for BERT with cross-level optimization, 2021-08-01, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Wei Zhu"
                ],
                "title": "Leebert: Learned early exit for BERT with cross-level optimization",
                "pub_date": "2021-08-01",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "54-ARR_v1_120",
            "content": "Eric Ziegel, The elements of statistical learning, 2003, Technometrics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Eric Ziegel"
                ],
                "title": "The elements of statistical learning",
                "pub_date": "2003",
                "pub_title": "Technometrics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "54-ARR_v1_0@0",
            "content": "A Simple Hash-Based Early Exiting Approach For Language Understanding and Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_0",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_2@0",
            "content": "Early exiting allows instances to exit at different layers according to the estimation of difficulty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_2",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_2@1",
            "content": "Previous works usually adopt heuristic metrics such as the entropy of internal outputs to measure instance difficulty, which suffers from generalization and threshold-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_2",
            "start": 102,
            "end": 275,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_2@2",
            "content": "In contrast, learning to exit, or learning to predict instance difficulty is a more appealing way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_2",
            "start": 277,
            "end": 374,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_2@3",
            "content": "Though some effort has been devoted to employing such \"learn-to-exit\" modules, it is still unknown whether and how well the instance difficulty can be learned.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_2",
            "start": 376,
            "end": 534,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_2@4",
            "content": "As a response, we first conduct experiments on the learnability of instance difficulty, which demonstrates that modern neural models perform poorly on predicting instance difficulty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_2",
            "start": 536,
            "end": 717,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_2@5",
            "content": "Based on this observation, we propose a simple-yet-effective Hashbased Early Exiting approach (HASHEE) that replaces the learn-to-exit modules with hash functions to assign each token to a fixed exiting layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_2",
            "start": 719,
            "end": 927,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_2@6",
            "content": "Different from previous methods, HASHEE requires no internal classifiers nor extra parameters, and therefore is more efficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_2",
            "start": 929,
            "end": 1055,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_2@7",
            "content": "Experimental results on classification, regression, and generation tasks demonstrate that HASHEE can achieve higher performance with fewer FLOPs and inference time compared with previous state-of-the-art early exiting methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_2",
            "start": 1057,
            "end": 1282,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_4@0",
            "content": "Early exiting is a widely used technique to accelerate inference of deep neural networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_4",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_4@1",
            "content": "With the rising of pre-trained language models (PLMs) (Devlin et al., 2019;Yang et al., 2019;Raffel et al., 2020;Qiu et al., 2020), early exiting is drawing increasing attention in the NLP community.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_4",
            "start": 90,
            "end": 288,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_4@2",
            "content": "At its core, early exiting allows simple instances to exit early while allowing hard instances to exit late.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_4",
            "start": 290,
            "end": 397,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_4@3",
            "content": "Thus, how to measure instance difficulty is a crucial problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_4",
            "start": 399,
            "end": 460,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_5@0",
            "content": "Most existing early exiting methods attach multiple internal classifiers to the PLM and adopt some heuristic metrics, such as entropy Liu et al., 2020a) or maximum softmax score (Schwartz et al., 2020) of internal outputs, to measure instance difficulty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_5",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_5@1",
            "content": "However, these methods can not easily generalize to new tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_5",
            "start": 255,
            "end": 316,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_5@2",
            "content": "On the one hand, these metrics are not accessible on some tasks such as regression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_5",
            "start": 318,
            "end": 400,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_5@3",
            "content": "On the other hand, In order for these methods to perform well, one usually needs to fine-tune the threshold, which varies widely across different tasks and datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_5",
            "start": 402,
            "end": 566,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_6@0",
            "content": "Another way to measure instance difficulty is to directly learn it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_6",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_6@1",
            "content": "Recent studies (Elbayad et al., 2020;Xin et al., 2021) that use the idea of \"learn-toexit\" have achieved promising results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_6",
            "start": 68,
            "end": 190,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_6@2",
            "content": "They jointly train a neural model to predict for each instance the exiting layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_6",
            "start": 192,
            "end": 272,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_6@3",
            "content": "At their core, the learn-to-exit module is to estimate the difficulty for each instance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_6",
            "start": 274,
            "end": 361,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_6@4",
            "content": "Compared with previous heuristically designed metrics for difficulty, learn-to-exit is task-agnostic and does not require threshold-tuning, therefore is a more promising way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_6",
            "start": 363,
            "end": 536,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_7@0",
            "content": "Despite their success, it is still unknown whether or how well the instance difficulty can be learned.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_7",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_7@1",
            "content": "As a response, in this work, we construct datasets for two kinds of instance difficulty: (a) Humandefined difficulty, and (b) Model-defined difficulty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_7",
            "start": 103,
            "end": 253,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_7@2",
            "content": "The dataset for human-defined difficulty has two labels, 0 for instances that can be annotated by human and 1 for instances that cannot.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_7",
            "start": 255,
            "end": 390,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_7@3",
            "content": "For modeldefined difficulty, we train a multi-exit BERT (Devlin et al., 2019), which is attached with an internal classifier at each layer, on a sentence-level classification task, SNLI (Bowman et al., 2015), and a token-level classification task, OntoNotes NER (Hovy et al., 2006).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_7",
            "start": 392,
            "end": 673,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_7@4",
            "content": "The trained multi-exit BERTs are then used to annotate for each development instance whether it can be correctly predicted by each internal classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_7",
            "start": 675,
            "end": 825,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_7@5",
            "content": "Thus, our constructed sentence-level and token-level model-defined difficulty datasets are multi-label classification datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_7",
            "start": 827,
            "end": 953,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_7@6",
            "content": "Experimental results demonstrate that, modern neural networks perform poorly on predicting instance difficulty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_7",
            "start": 955,
            "end": 1065,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_7@7",
            "content": "This observation is consistent with previous work (Laverghetta et al., 2020) on estimating instance difficulty for curriculum learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_7",
            "start": 1067,
            "end": 1201,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_8@0",
            "content": "Given that instance difficulty is hard to be predicted, then what works in the learn-to-exit modules?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_8",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_8@1",
            "content": "We hypothesis that the consistency between training and inference may play an important role.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_8",
            "start": 102,
            "end": 194,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_8@2",
            "content": "That is, for a training instance x i that is predicted to exit at layer l, an inference instance x j that is similar with x i should be predicted to exit at layer l, too.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_8",
            "start": 196,
            "end": 365,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_8@3",
            "content": "Since neural networks are usually smooth functions (Ziegel, 2003), this consistency can be easily satisfied by neural learn-to-exit modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_8",
            "start": 367,
            "end": 506,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_8@4",
            "content": "If this hypothesis holds, we can replace the learn-to-exit module with a simple hash function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_8",
            "start": 508,
            "end": 601,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_8@5",
            "content": "In particular, we use hash functions to assign each token to a fixed exiting layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_8",
            "start": 603,
            "end": 685,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_8@6",
            "content": "This hash-based early exiting method is named HASHEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_8",
            "start": 687,
            "end": 739,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_9@0",
            "content": "Compared with previous methods that use heuristic metrics for difficulty or jointly learn to exit, HASHEE offers several advantages: (a) HASHEE requires no internal classifiers nor extra parameters, which are necessary in previous work. (b) HASHEE can perform token-level early exiting without supervision, therefore can be widely used on various tasks including language understanding and generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_9",
            "start": 0,
            "end": 400,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_9@1",
            "content": "(c) The speed-up ratio can be easily tuned by modifying the hash function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_9",
            "start": 402,
            "end": 475,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_9@2",
            "content": "(d) HASHEE can significantly accelerate model inference on a per-batch basis instead of per-instance basis as in previous work Liu et al., 2020a;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_9",
            "start": 477,
            "end": 622,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_10@0",
            "content": "We conduct experiments on classification, regression, and generation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_10",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_10@1",
            "content": "Experimental results on ELUE (Liu et al., 2021a) demonstrate that HASHEE, despite its simplicity, can achieve higher performance with fewer FLOPs and inference time than previous state-of-the-art methods on various tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_10",
            "start": 76,
            "end": 296,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_10@2",
            "content": "Besides, our experiments on several text summarization datasets show that HASHEE can reduce \u223c50% FLOPs of BART (Lewis et al., 2020) and CPT while maintaining 97% ROUGE-1 score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_10",
            "start": 298,
            "end": 473,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_11@0",
            "content": "2 Can Instance Difficulty Be Learned?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_11",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_12@0",
            "content": "In this section, we examine whether or to what extent instance difficulty can be learned.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_12",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_12@1",
            "content": "In particular, we manage to evaluate how well a neural network that trained on some data with difficulty annotation can generalize to unseen data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_12",
            "start": 90,
            "end": 235,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_12@2",
            "content": "Here we consider two kinds of difficulty: human-defined difficulty and model-defined difficulty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_12",
            "start": 237,
            "end": 332,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_13@0",
            "content": "Human-defined Difficulty",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_13",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_14@0",
            "content": "Dataset Construction Human-defined difficulty of an instance measures how difficult for human to judge its label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_14",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_14@1",
            "content": "To construct such a dataset, we use the SNLI dataset (Bowman et al., 2015), which is a collection of 570k human-written English sentence pairs that are manually labeled with the inference relation between the two sentences: entailment, contradiction, or neutral.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_14",
            "start": 114,
            "end": 375,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_14@2",
            "content": "The labels in SNLI are determined by the majority of the crowd-sourced annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_14",
            "start": 377,
            "end": 458,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_14@3",
            "content": "If there is no majority for an instance, its label would be \"Unknown\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_14",
            "start": 460,
            "end": 529,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_14@4",
            "content": "We collect 1,119 unknown instances from SNLI dataset as our difficult instances, and collect 1,119 labeled instances from the instances of three classes (i.e., entailment, contradiction, and neutral) in equal proportion as our simple instances, obtaining a balanced binary classification (difficult or simple) dataset with 2,238 instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_14",
            "start": 531,
            "end": 869,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_14@5",
            "content": "We randomly sample 1,238 instances with balanced labels as training set and use the remaining 1,000 instances as test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_14",
            "start": 871,
            "end": 992,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_15@0",
            "content": "Learning Human-defined Difficulty We then train a BERT model (Devlin et al., 2019) with a linear classifier on the top on our constructed training set, and evaluate on the test set to see if it can predict whether an unseen instance is simple or difficult.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_15",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_15@1",
            "content": "As shown in Figure 1, the BERT model that fits well on the training set can only achieve \u223c60% accuracy on the test set, demonstrating that neural models (even BERT) can not easily learn to estimate human-defined difficulty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_15",
            "start": 257,
            "end": 479,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_16@0",
            "content": "Model-defined Difficulty",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_16",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_17@0",
            "content": "However, model can have a different view of instance difficulty from human.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_17",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_17@1",
            "content": "For example, an instance can be defined as a difficult one if it can not be correctly predicted by a well-trained model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_17",
            "start": 76,
            "end": 195,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_17@2",
            "content": "Thus, we also construct datasets to characterize model-defined difficulty for each instance, which is more realistic in the context of early exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_17",
            "start": 197,
            "end": 345,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_17@3",
            "content": "In particular, we construct two datasets labeled with model-defined difficulty at sentence-level and token-level, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_17",
            "start": 347,
            "end": 473,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_18@0",
            "content": "Sentence-level Difficulty Estimating modeldefined difficulty of a sentence (or sentence pairs) is helpful to language understanding tasks such as text classification and natural language inference (Xin et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_18",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_18@1",
            "content": "To obtain the sentence-level difficulty, we train a multi-exit BERT that is attached with an internal classifier at each layer on SNLI training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_18",
            "start": 217,
            "end": 364,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_18@2",
            "content": "Once the multi-exit BERT is trained, it can serve as an annotator to label each instance in the SNLI development set whether it can be correctly predicted by each internal classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_18",
            "start": 366,
            "end": 548,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_18@3",
            "content": "In our experiments, we use BERT BASE that has 12 layers, and therefore for each instance in the SNLI development set we have 12 labels, each takes values of 0 or 1 to indicate whether or not the corresponding internal classifier correctly predict its label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_18",
            "start": 550,
            "end": 806,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_18@4",
            "content": "By this, we label the 9,842 SNLI development instances to construct a multi-label classification dataset, from which we randomly sample 8,000 instances as training set and use the remaining 1,842 instances as test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_18",
            "start": 808,
            "end": 1025,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_19@0",
            "content": "Token-level Difficulty We also construct a dataset for estimating model-defined difficulty of each token, which can be used in language generation tasks (Elbayad et al., 2020) and sequence labeling tasks (Li et al., 2021b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_19",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_19@1",
            "content": "Similarly, we train a multi-exit BERT on OntoNotes NER (Hovy et al., 2006) training set, and use it to annotate each token in the OntoNotes development instances whether it can be correctly predicted by each internal classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_19",
            "start": 224,
            "end": 451,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_19@2",
            "content": "By this, we obtain a token-level multilabel classification datasets consisting of 13,900 *: The majority model for the token-level task would always predict positive class for all the labels, and therefore the F1 score is not applicable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_19",
            "start": 453,
            "end": 689,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_20@0",
            "content": "instances, from which we randomly sample 10,000 instances to construct a training set and use the remaining 3,900 instances as test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_20",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_21@0",
            "content": "Learning Model-defined Difficulty For each constructed model-defined difficulty dataset, we evaluate several models: (1) Majority model always predicts the majority class for each label, with class priors learned from the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_21",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_22@0",
            "content": "(2) Linear-M is a multi-classification linear layer that takes as input the average pooled word embeddings and outputs the exiting layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_22",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_22@1",
            "content": "This model corresponds to the multinomial variants of Elbayad et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_22",
            "start": 138,
            "end": 213,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_22@2",
            "content": "Since the inputs of Linear-M is noncontextualized, we did not apply it to estimate token-level difficulty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_22",
            "start": 215,
            "end": 320,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_23@0",
            "content": "(3) Linear-B is a binary classification linear layer that takes as input the hidden states at each BERT layer and outputs whether or not the instance (or token) is correctly predicted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_23",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_23@1",
            "content": "This model corresponds to the geometric variants of Elbayad et al. (2020) and the learn-to-exit module in BERxiT (Xin et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_23",
            "start": 185,
            "end": 316,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_23@2",
            "content": "( 4) We also train and evaluate a bidirectional LSTM model (Hochreiter and Schmidhuber, 1997) with one layer and hidden size of 256.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_23",
            "start": 318,
            "end": 449,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_23@3",
            "content": "It takes as input the instance and outputs the exiting layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_23",
            "start": 451,
            "end": 511,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_24@0",
            "content": "(5) BERT model (Devlin et al., 2019) is also considered for this task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_24",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_24@1",
            "content": "For these models, except for Linear-B, we use the binary cross entropy loss to handle the multi-label classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_24",
            "start": 71,
            "end": 187,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_24@2",
            "content": "Since most development instances are correctly predicted, our constructed datasets are label-imbalanced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_24",
            "start": 189,
            "end": 292,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_24@3",
            "content": "To alleviate this issue, we adopt over-sampling for classes with fewer instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_24",
            "start": 294,
            "end": 374,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_25@0",
            "content": "Our experimental results are shown in Figure 2, from which we find that: (1) For the task of estimating sentence-level difficulty, the shallow neural models perform as well as simple majority model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_25",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_25@1",
            "content": "Only the BERT model can slightly outperform the majority model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_25",
            "start": 199,
            "end": 261,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_25@2",
            "content": "( 2) For token-level difficulty, these neural models perform slightly better than the majority model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_25",
            "start": 263,
            "end": 363,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_25@3",
            "content": "The insignificant improvement over the majority model demonstrate that, the performance of the neural models mainly come from the learning of prior distribution of label instead of extracting difficulty-related features from instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_25",
            "start": 365,
            "end": 599,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_25@4",
            "content": "In the case of label imbalance, the accuracy can not well measure model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_25",
            "start": 601,
            "end": 684,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_25@5",
            "content": "Besides, in the context of early exiting, we are more interested in cases that the model performs a false exit for an unsolved instance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_25",
            "start": 686,
            "end": 821,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_25@6",
            "content": "Thus, we also report the precision, recall, and F1 score on the negative class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_25",
            "start": 823,
            "end": 901,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_25@7",
            "content": "As shown in Table 1, all the evaluated models perform poorly on recognizing the incorrectly predicted instances and tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_25",
            "start": 903,
            "end": 1025,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_26@0",
            "content": "Though, it can not be concluded that the instance difficulty can not be learned since there are still a variety of machine learning models and training techniques that are under explored.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_26",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_26@1",
            "content": "Our preliminary experiments demonstrate that, at least, instance difficulty, whether human-defined or modeldefined, is hard to learn for modern neural networks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_26",
            "start": 188,
            "end": 347,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_26@2",
            "content": "In fact, our evaluated learn-to-exit models are upper baselines than that used in previous work because: (1) we also adopt more powerful deep models instead of simple linear models in previous methods (Elbayad et al., 2020;Xin et al., 2021), and (2) Different from our method that trains learnto-exit module on development set, previous methods jointly train their learn-to-exit module on the training set where few instances are incorrectly predicted, leading to more serious label imbalance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_26",
            "start": 349,
            "end": 841,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_26@3",
            "content": "To facilitate future research, our constructed difficulty datasets will be publicly available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_26",
            "start": 843,
            "end": 936,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_27@0",
            "content": "HASHEE: Hash Early Exiting",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_27",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_28@0",
            "content": "What is Unnecessary and What Works?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_28",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_29@0",
            "content": "On the one hand, previous methods (Elbayad et al., 2020;Xin et al., 2021) that use learn-to-exit modules have achieved competitive results, which implies that something works in the learn-to-exit modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_29",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_29@1",
            "content": "On the other hand, our preliminary experiments show that instance difficulty is hard to be predicted in advance, which indicates that learning can be unnecessary to achieve a good performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_29",
            "start": 205,
            "end": 396,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_30@0",
            "content": "To find what works, we formally describe the prediction of an early exiting model as P (y|x) = d\u2208D P (y|x, d)P (d|x), where d is the difficulty (e.g., the exiting layer) for x.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_30",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_30@1",
            "content": "Note that in practice, P (D|x) is an one-hot distribution, so when d is predicted, the exiting layer, i.e., the model architecture is determined.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_30",
            "start": 177,
            "end": 321,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_30@2",
            "content": "Therefore, the difficulty d actually corresponds to an architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_30",
            "start": 323,
            "end": 390,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_30@3",
            "content": "1 Now given that the mapping from instance x to its difficulty d, i.e., the best architecture, is hard to be learned, a natural idea to make P (y|x) performs well is to keep P (d|x) consistent: if a training instance x i is predicted to exit at layer l, then an inference instance x j that is similar with x i should exit at layer l, too.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_30",
            "start": 392,
            "end": 729,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_30@4",
            "content": "By this, the activated architecture can well-handle the instance x j during inference because it is well-trained on similar instances such as x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_30",
            "start": 731,
            "end": 877,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_30@5",
            "content": "Note that this consistency between training and inference can be easily satisfied by previous learnto-exit modules due to the smoothness of neural models (Ziegel, 2003).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_30",
            "start": 879,
            "end": 1047,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_30@6",
            "content": "Based on this hypothesis, we manage to remove the learning process and only stick to the consistency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_30",
            "start": 1049,
            "end": 1149,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_30@7",
            "content": "In particular, we replace the neural learn-to-exit module P (d|x) with a simple hash function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_30",
            "start": 1151,
            "end": 1244,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_31@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_31",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_32@0",
            "content": "Without loss of generality, we first consider sequence classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_32",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_32@1",
            "content": "A straightforward idea is to design a hash function to map semantically similar instances into the same bucket, and therefore the hash function should be some powerful sequence encoder such as Sentence-BERT (Reimers and Gurevych, 2019), which is cumbersome in computation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_32",
            "start": 77,
            "end": 348,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_32@2",
            "content": "In addition, a high-quality sequence encoder as a hash function usually maps instances with the same label into the same bucket (i.e. the same exiting layer), which makes the internal classifier at that layer suffer from label imbalance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_32",
            "start": 350,
            "end": 586,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_32@3",
            "content": "Due to the difficulty of holding consistency at sentencelevel, we rather propose to hold the consistency at token-level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_32",
            "start": 588,
            "end": 707,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_32@4",
            "content": "By assigning each token into a fixed bucket, the token-level consistency between training and inference is easily satisfied.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_32",
            "start": 709,
            "end": 832,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_33@0",
            "content": "An overview of our method is illustrated in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_33",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_33@1",
            "content": "We adopt a simple and efficient hash function to map each token into a fixed bucket in advance, where each bucket corresponds to an exiting layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_33",
            "start": 54,
            "end": 199,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_33@2",
            "content": "We use pre-trained Transformers (Vaswani et al., 2017) as our backbones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_33",
            "start": 201,
            "end": 272,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_33@3",
            "content": "During model's forward pass, the representation of exited tokens will not be updated through self-attention, and its hidden states of the upper layers are directly copied from the hidden states of the exiting layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_33",
            "start": 274,
            "end": 488,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_33@4",
            "content": "By this token-level early exiting, the computation in self-attention and the following feed-forward network is reduced.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_33",
            "start": 490,
            "end": 608,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_34@0",
            "content": "Hash Functions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_34",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_35@0",
            "content": "To hold the token-level consistency between training and inference, HASHEE employs hash functions to compute in advance the exiting layer for each token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_35",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_35@1",
            "content": "During training and inference, each token exits at a fixed layer according to the precomputed hash lookup table.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_35",
            "start": 154,
            "end": 265,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_35@2",
            "content": "The hash functions can take a variety of forms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_35",
            "start": 267,
            "end": 313,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_35@3",
            "content": "Here we consider several hash functions as possible alternatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_35",
            "start": 315,
            "end": 379,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_36@0",
            "content": "Random Hash Random hash is a lower baseline, wherein we assign each token to a fixed, random exiting layer at initialization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_36",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_36@1",
            "content": "To examine our hypothesis, we also consider to use two different random hash functions for training and inference respectively, in which case the consistency does not hold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_36",
            "start": 126,
            "end": 297,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_36@2",
            "content": "We denote these two random hash functions as Rand-cons and Rand-incons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_36",
            "start": 299,
            "end": 369,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_37@0",
            "content": "Frequency Hash To achieve higher speed-up, a natural way is to assign frequent tokens to lower layers to exit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_37",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_37@1",
            "content": "Intuitively, frequent tokens are usually well-trained during pre-training and therefore do not require too much refinement by looking at their contexts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_37",
            "start": 111,
            "end": 262,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_37@2",
            "content": "Thus we can design a hash function that assigns tokens into exiting layers by frequency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_37",
            "start": 264,
            "end": 351,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_38@0",
            "content": "In particular, the tokens are sorted by frequency and then divided equally into B buckets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_38",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_39@0",
            "content": "MI Hash Further, we also consider a taskspecific hash function that is based on the mutual information (MI) between each token and the corresponding label, which, as an instance of HASHEE, is also adopted in Liu et al. (2021b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_39",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_39@1",
            "content": "Tokens are sorted by their MI values between the task label, and then divided equally into B buckets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_39",
            "start": 228,
            "end": 328,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_39@2",
            "content": "Tokens with higher MI values are assigned to lower layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_39",
            "start": 330,
            "end": 387,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_40@0",
            "content": "Clustered Hash It is also intuitive that similar tokens should be assigned to the same layer to exit, and therefore we also experiment with a clustered hash function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_40",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_40@1",
            "content": "The clusters are obtained by performing k-means clustering using token embeddings from BERT BASE embedding layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_40",
            "start": 167,
            "end": 279,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_40@2",
            "content": "The clustered tokens are then sorted by norm, which often relates to token frequency (Schakel and Wilson, 2015) and difficulty .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_40",
            "start": 281,
            "end": 408,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_40@3",
            "content": "The clustered tokens with small average norms are assigned to lower layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_40",
            "start": 410,
            "end": 484,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_41@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_41",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_42@0",
            "content": "Tasks and Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_42",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_43@0",
            "content": "Since HASHEE requires no supervision, it can be applied to a variety of tasks and architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_43",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_44@0",
            "content": "In our work, we conduct experiments on natural language understanding tasks including sentiment analysis, natural language inference, similarity regression, and a language generation task, text summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_44",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_44@1",
            "content": "Statistics of our used datasets are shown in Appendix A.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_44",
            "start": 209,
            "end": 266,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_45@0",
            "content": "Understanding Tasks For the convenience of comparison with other efficient models, we evaluate our proposed HASHEE on the ELUE benchmark (Liu et al., 2021a) Generation Tasks For language generation, we evaluate HASHEE on two English summarization datasets, CNN/DailyMail (Hermann et al., 2015) and Reddit (Kim et al., 2019), and two Chinese summarization datasets: TTNews (Hua et al., 2017) and CSL (Xu et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_45",
            "start": 0,
            "end": 417,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_46@0",
            "content": "Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_46",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_47@0",
            "content": "Baselines We compare HASHEE with the following competitive baseline models:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_47",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_48@0",
            "content": "(1) Pre-Trained",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_48",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_49@0",
            "content": "Table 2: Main results on the ELUE benchmark (Liu et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_49",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_49@1",
            "content": "We report for each model on each task the performance and the corresponding speed-up ratio, which is calculated as the FLOPs reduction relative to BERT BASE .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_49",
            "start": 65,
            "end": 222,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_50@0",
            "content": "For MRPC, we report the mean of accuracy and F1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_50",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_50@1",
            "content": "For STS-B, we report Pearson and Spearman correlation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_50",
            "start": 49,
            "end": 102,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_50@2",
            "content": "For all other tasks we report accuracy. \"-\" indicates that the method is not applicable on that task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_50",
            "start": 104,
            "end": 204,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_51@0",
            "content": "Language Models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_51",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_51@1",
            "content": "We directly fine-tune the first layers of pre-trained language models including BERT (Devlin et al., 2019), ALBERT , RoBERTa , and ElasticBERT (Liu et al., 2021a) with a MLP classifier on the top.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_51",
            "start": 17,
            "end": 212,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_52@0",
            "content": "(2) Static Models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_52",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_52@1",
            "content": "We compare with several static approaches to accelerate language model inference, including Distil-BERT (Sanh et al., 2019), TinyBERT (Jiao et al., 2020), HeadPrune (Michel et al., 2019), and BERTof-Theseus (Xu et al., 2020a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_52",
            "start": 19,
            "end": 244,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_53@0",
            "content": "(3) Dynamic models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_53",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_53@1",
            "content": "We compare with DeeBERT , FastBERT (Liu et al., 2020a), PABEE , BERxiT (Xin et al., 2021), and Cascade-BERT (Li et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_53",
            "start": 20,
            "end": 146,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_54@0",
            "content": "Training For most NLU experiments we adopt the ElasticBERT BASE model (Liu et al., 2021a) as our backbone model, which is a pre-trained multi-exit Transformer encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_54",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_54@1",
            "content": "For small datasets (i.e., SST-2, MRPC, and STS-B) we report the mean performance and the standard deviation (in Table 3 and 9) over 5 runs with different random seeds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_54",
            "start": 168,
            "end": 334,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_54@2",
            "content": "For text summarization datasets we adopt BART BASE (Lewis et al., 2020) and CPT BASE as our backbone models and use the frequency hash to assign tokens Table 4: Experimental results on two English and two Chinese summarization datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_54",
            "start": 336,
            "end": 571,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_54@3",
            "content": "We report ROUGE-1, ROUGE-2, and ROUGE-L for each dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_54",
            "start": 573,
            "end": 629,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_54@4",
            "content": "The speedup ratios for English and Chinese models are calculated by the FLOPs reduction relative to BART BASE and CPT BASE , respectively, and averaged over the performed datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_54",
            "start": 631,
            "end": 810,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_55@0",
            "content": "Here we re-implement the confidence thresholding variant of DAT (Elbayad et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_55",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_55@1",
            "content": "ing the frequency hash that assigns tokens to the first 6 layers of ElasticBERT BASE , HASHEE can outperform most considered baselines with fewer FLOPs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_55",
            "start": 88,
            "end": 239,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_55@2",
            "content": "To fairly compare with baselines of various speedup ratios, we also report the ELUE score (Liu et al., 2021a), which is a two-dimensional (performance and FLOPs) metric for efficient NLP models, measuring how much a model oversteps Elas-ticBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_55",
            "start": 241,
            "end": 485,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_55@3",
            "content": "Table 2 shows that HASHEE achieved a new state-of-the-art ELUE score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_55",
            "start": 487,
            "end": 555,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_55@4",
            "content": "To fairly compare with the learn-to-exit baseline we also implement BERxiT (Xin et al., 2021) with ElasticBERT BASE .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_55",
            "start": 557,
            "end": 673,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@0",
            "content": "We then evaluate HASHEE with different hash functions detailed in Section 3.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@1",
            "content": "For all these hash functions, we assign tokens to the 6 and 12 layers of ElasticBERT-6L and ElasticBERT-12L, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 79,
            "end": 200,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@2",
            "content": "Experimental results on SST-2, SNLI, and MRPC are given in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 202,
            "end": 268,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@3",
            "content": "Among the hash functions, the frequency hash achieves the highest speedup while maintaining a considerable performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 270,
            "end": 388,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@4",
            "content": "With the backbone of ElasticBERT-12L, these hash functions, except for the frequency hash, cannot achieve considerable speedup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 390,
            "end": 516,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@5",
            "content": "Besides, we find that ElasticBERT-12L did not significantly outperform ElasticBERT-6L with HASHEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 518,
            "end": 615,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@6",
            "content": "We conjecture that higher layers are not good at querying information from hidden states of tokens that exit too early.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 617,
            "end": 735,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@7",
            "content": "In this work, we are more interested in the case of high acceleration ratio, so we adopt ElasticBERT-6L as our main backbone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 737,
            "end": 861,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@8",
            "content": "To make a more intuitive comparison of these hash functions with different speedup ratios, we also show in Figure 4 the ELUE scores on SST-2 and SNLI with ElasticBERT-6L as backbone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 863,
            "end": 1044,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@9",
            "content": "We find that the frequency hash outperforms other hash functions by a large margin, and therefore in the following experiments we mainly use the frequency hash.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 1046,
            "end": 1205,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_56@10",
            "content": "Besides, only the Rand-incons hash obtains negative ELUE score, demonstrating the benefit of maintaining consistency between training and inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_56",
            "start": 1207,
            "end": 1354,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_57@0",
            "content": "Comparison of Actual Inference Time Because most of the operations in the Transformer architecture are well optimized by modern deep learning frameworks and parallel processing hardwares such as GPU and TPU, FLOPs may not precisely reflect the actual inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_57",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_57@1",
            "content": "To that end, here we also evaluate actual inference time on a single GeForce RTX 3090 GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_57",
            "start": 267,
            "end": 356,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_57@2",
            "content": "Note that the speedup ratio of previous early exiting methods are usually tested on a per-instance basis, i.e. the batch size is set to 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_57",
            "start": 358,
            "end": 495,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_57@3",
            "content": "However, batch inference is often more favorable in both offline scenarios and low-latency scenarios (Zhang et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_57",
            "start": 497,
            "end": 618,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_57@4",
            "content": "Here we compare HASHEE with two baselines that have similar performance, i.e., FastBERT and PABEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_57",
            "start": 620,
            "end": 717,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_57@5",
            "content": "Our experiments are conducted on two datasets with very different average sentence length, i.e., SNLI and IMDb.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_57",
            "start": 719,
            "end": 829,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_57@6",
            "content": "Results are given in Table 5 and Figure 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_57",
            "start": 831,
            "end": 872,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_58@0",
            "content": "We find HASHEE has an advantage in processing speed when the batch size exceeds 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_58",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_58@1",
            "content": "Besides, HASHEE can perform larger batch inference due to its memory-efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_58",
            "start": 83,
            "end": 162,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_58@2",
            "content": "Comparison of Different Backbones To evaluate the versatility of HASHEE, we also conduct experiments with other backbone models, i.e., BERT, ALBERT, and RoBERTa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_58",
            "start": 164,
            "end": 324,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_58@3",
            "content": "As shown in Figure 6, HASHEE outperforms other baselines with the same backbone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_58",
            "start": 326,
            "end": 405,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_59@0",
            "content": "Models Since HASHEE requires no supervision, it can also be applied to seq2seq models for generation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_59",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_59@1",
            "content": "We first evaluate HASHEE with BART BASE as our backbone on two English summarization tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_59",
            "start": 108,
            "end": 198,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_59@2",
            "content": "As shown in Table 4, HASHEE can achieve significant speedup for BART encoder while maintaining considerable ROUGE scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_59",
            "start": 200,
            "end": 320,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_59@3",
            "content": "Besides, we find that previous early exiting methods that measure the uncertainty of internal outputs would rather slow down decoder inference due to the heavy computation of prediction over large vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_59",
            "start": 322,
            "end": 529,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_59@4",
            "content": "In addition, to further explore the speedup potential of HASHEE, we also experiment with CPT , which has a deep encoder and a shallow decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_59",
            "start": 531,
            "end": 672,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_59@5",
            "content": "Results on CSL and TTNews depict that HASHEE can achieve 2.3\u00d7 speedup relative to CPT while maintaining 97% ROUGE-1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_59",
            "start": 674,
            "end": 789,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_59@6",
            "content": "We also report results of the 6-layer versions of BART (with 3 encoder layers and 3 decoder layers) and CPT (with 5 encoder layers and 1 decoder layer).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_59",
            "start": 791,
            "end": 942,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_60@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_60",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_61@0",
            "content": "Large-scale pre-trained language models (PLMs) have achieved great success in recent years.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_61",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_61@1",
            "content": "Despite their power, the inference is time-consuming, which hinders their deployment in low-latency scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_61",
            "start": 92,
            "end": 201,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_61@2",
            "content": "To accelerate PLM inference, there are currently two streams of work: (1) Compressing a cumbersome PLM through knowledge distillation (Sanh et al., 2019;Sun et al., 2019;Jiao et al., 2020), model pruning (Gordon et al., 2020;Michel et al., 2019), quantization (Shen et al., 2020), module replacing (Xu et al., 2020a), etc. (2) Selectively activating parts of the model conditioned on the input, such as Universal Transformer (Dehghani et al., 2019), FastBERT (Liu et al., 2020a), DeeBERT , PABEE , LeeBERT (Zhu, 2021), CascadeBERT (Li et al., 2021a), ElasticBERT (Liu et al., 2021a) and other similar methods (Elbayad et al., 2020;Schwartz et al., 2020;Liao et al., 2021;Xin et al., 2021;Sun et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_61",
            "start": 203,
            "end": 908,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_61@3",
            "content": "Different from these methods, our proposed HASHEE requires no internal classifiers (which imply extra parameters) and supervision, and therefore can be widely used in a variety of tasks and model architectures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_61",
            "start": 910,
            "end": 1119,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_62@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_62",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_63@0",
            "content": "We first empirically study the learnability of instance difficulty, which is a crucial problem in early exiting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_63",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_63@1",
            "content": "Based on the observation that modern neural models perform poorly on estimating instance difficulty, we propose a hash-based early exiting approach, named HASHEE, that removes the learning process and only sticks to the consistency between training and inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_63",
            "start": 113,
            "end": 375,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_63@2",
            "content": "Our experiments on classification, regression, and generation tasks show that HASHEE can achieve state-of-the-art performance with fewer computation and inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_63",
            "start": 377,
            "end": 544,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_64@0",
            "content": "Here we list the statistics of our used language understanding and generation datasets in Table 6 and Table 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_64",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_65@0",
            "content": "For small datasets in ELUE, i.e. SST-2, MRPC, and STS-B, we conduct grid search over batch sizes of {16, 32}, learning rates of {2e-5, 3e-5, 5e-5}, number of epochs of {3, 4, 5}, warmup step ratios of {0.1, 0.01}, and weight decays of {0.1, 0.01} with an AdamW optimizer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_65",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_65@1",
            "content": "We select the hyperparameters that achieved the best performance on the development sets, and perform 5 runs with different random seeds to obtain the mean performance and standard deviation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_65",
            "start": 272,
            "end": 462,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_65@2",
            "content": "For SNLI, SciTail, and IMDb, we use the same hyperparameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_65",
            "start": 464,
            "end": 524,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_65@3",
            "content": "All of the hyperparameters used in our language understanding experiments are given in Table 8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_65",
            "start": 526,
            "end": 620,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_65@4",
            "content": "For English summarization tasks, i.e., CNN/DailyMail and Reddit, we use the same hyperparameters as BART.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_65",
            "start": 622,
            "end": 726,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_65@5",
            "content": "For Chinese summarization tasks, i.e., TTNews and CSL, we use the same hyperparameters as CPT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_65",
            "start": 728,
            "end": 821,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_66@0",
            "content": "In previous experiments we assign tokens to the same number of buckets as the number of layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_66",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_66@1",
            "content": "Here we also explore other configurations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_66",
            "start": 96,
            "end": 137,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_66@2",
            "content": "For each configuration, we assign tokens to B buckets, corresponding to exiting layers {1 + 12b/B} B\u22121 b=0 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_66",
            "start": 139,
            "end": 246,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_66@3",
            "content": "For instance, if we have 12 layers and 3 buckets, the 3 buckets correspond to the {1, 5, 9} layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_66",
            "start": 248,
            "end": 346,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_66@4",
            "content": "Overall results are given in Table 9, where we show results of 8 configurations with the frequency hash.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_66",
            "start": 348,
            "end": 451,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_66@5",
            "content": "Similar with Table 3, we find that 6-layer models perform well while achieving higher acceleration ratios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_66",
            "start": 453,
            "end": 558,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_66@6",
            "content": "In addition, the number of buckets has no significant effect on acceleration ratio.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_66",
            "start": 560,
            "end": 642,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_66@7",
            "content": "Configurations that the number of layers equals to the number of buckets perform slightly better than other configurations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_66",
            "start": 644,
            "end": 766,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_67@0",
            "content": "Here we take a closer look at the HASHEE model forward process, and see which FLOPs are saved during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_67",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_68@0",
            "content": "Given the hidden states at layer l as H l \u2208 R n\u00d7d and the hidden states of remaining tokens are denoted as h l \u2208 R m\u00d7d , where n is the original sequence length and m is the number of remaining tokens at layer l, the calculation of one Transformer encoder layer with HASHEE can be formally de-",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_68",
            "start": 0,
            "end": 292,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_69@0",
            "content": "R Samuel, Gabor Bowman, Christopher Angeli, Christopher Potts,  Manning, A large annotated corpus for learning natural language inference, 2015-09-17, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_69",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_70@0",
            "content": "UNKNOWN, None, 2017, Semeval-2017 task 1: Semantic textual similarity -multilingual and cross-lingual focused evaluation, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_70",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_71@0",
            "content": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, Lukasz Kaiser, Universal transformers, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_71",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_72@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: pre-training of deep bidirectional transformers for language understanding, 2019-06-02, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_72",
            "start": 0,
            "end": 357,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_73@0",
            "content": "B William, Chris Dolan,  Brockett, Automatically constructing a corpus of sentential paraphrases, 2005-10, Proceedings of the Third International Workshop on Paraphrasing, IWP@IJCNLP 2005, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_73",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_74@0",
            "content": "Maha Elbayad, Jiatao Gu, Edouard Grave, Michael Auli, Depth-adaptive transformer, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_74",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_75@0",
            "content": "Mitchell Gordon, Kevin Duh, Nicholas Andrews, Compressing BERT: studying the effects of weight pruning on transfer learning, 2020-07-09, Proceedings of the 5th Workshop on Representation Learning for NLP, RepL4NLP@ACL 2020, Online, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_75",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_76@0",
            "content": "Karl Moritz Hermann, Tom\u00e1s Kocisk\u00fd, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, Phil Blunsom, Teaching machines to read and comprehend, 2015-12-07, Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_76",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_77@0",
            "content": "Sepp Hochreiter, J\u00fcrgen Schmidhuber, Long short-term memory, 1997, Neural Comput, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_77",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_78@0",
            "content": "Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, Ralph Weischedel, Ontonotes: The 90% solution, 2006-06-04, Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, Proceedings, The Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_78",
            "start": 0,
            "end": 293,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_79@0",
            "content": "Lifeng Hua, Xiaojun Wan, Lei Li, Overview of the nlpcc 2017 shared task: Single document summarization, 2017, National CCF Conference on Natural Language Processing and Chinese Computing, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_79",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_80@0",
            "content": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Tinybert: Distilling BERT for natural language understanding, 2020-11-20, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_80",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_81@0",
            "content": "Tushar Khot, Ashish Sabharwal, Peter Clark, Scitail: A textual entailment dataset from science question answering, 2018-02-02, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_81",
            "start": 0,
            "end": 385,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_82@0",
            "content": "Byeongchang Kim, Hyunwoo Kim, Gunhee Kim, Abstractive summarization of reddit posts with multi-level memory networks, 2019-06-02, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_82",
            "start": 0,
            "end": 311,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_83@0",
            "content": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, Radu Soricut, ALBERT: A lite BERT for self-supervised learning of language representations, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_83",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_84@0",
            "content": "Antonio Laverghetta, Jamshidbek Mirzakhalov, John Licato, Towards a task-agnostic model of difficulty estimation for supervised learning tasks, 2020-12-04, Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, AACL/IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_84",
            "start": 0,
            "end": 388,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_85@0",
            "content": "UNKNOWN, None, 2020, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_85",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_86@0",
            "content": ", BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_86",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_87@0",
            "content": "Lei Li, Yankai Lin, Deli Chen, Shuhuai Ren, Peng Li, Jie Zhou, Xu Sun, Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade, 2021, Findings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_87",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_88@0",
            "content": "Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, Xuanjing Huang, Accelerating BERT inference for sequence labeling via earlyexit, 2021-08-01, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_88",
            "start": 0,
            "end": 377,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_89@0",
            "content": "Kaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su, A global past-future early exit method for accelerating inference of pretrained language models, 2021-06-06, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_89",
            "start": 0,
            "end": 356,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_90@0",
            "content": "Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, Qi Ju, Fastbert: a selfdistilling BERT with adaptive inference time, 2020-07-05, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_90",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_91@0",
            "content": "UNKNOWN, None, , Xuanjing Huang, and Xipeng Qiu. 2021a. Towards efficient NLP: A standard evaluation and A strong baseline, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_91",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_92@0",
            "content": "Xuebo Liu, Houtim Lai, Derek Wong, Lidia Chao, Norm-based curriculum learning for neural machine translation, 2020-07-05, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_92",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_93@0",
            "content": "Yijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen, Jinan Xu, Faster depth-adaptive transformers, 2021-02-02, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_93",
            "start": 0,
            "end": 375,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_94@0",
            "content": "UNKNOWN, None, 1907, Roberta: A robustly optimized BERT pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_94",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_95@0",
            "content": "Andrew Maas, Raymond Daly, Peter Pham, Dan Huang, Andrew Ng, Christopher Potts, Learning word vectors for sentiment analysis, 2011-06-24, The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_95",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_96@0",
            "content": "Paul Michel, Omer Levy, Graham Neubig, Are sixteen heads really better than one?, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_96",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_97@0",
            "content": "Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, Xuanjing Huang, Pre-trained models for natural language processing: A survey, 2020, SCIENCE CHINA Technological Sciences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_97",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_98@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, J. Mach. Learn. Res, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_98",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_99@0",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-bert: Sentence embeddings using siamese bert-networks, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_99",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_100@0",
            "content": "UNKNOWN, None, 2021, Hash layers for large sparse models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_100",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_101@0",
            "content": "UNKNOWN, None, 2019, Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_101",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_102@0",
            "content": "UNKNOWN, None, 2015, Measuring word significance using distributed representations of words, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_102",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_103@0",
            "content": "Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, Noah Smith, 2020. The right tool for the job: Matching model and instance complexities, , Proceedings of the 58th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_103",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_104@0",
            "content": "UNKNOWN, None, 2020, Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_104",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_105@0",
            "content": "UNKNOWN, None, 2021, CPT: A pre-trained unbalanced transformer for both chinese language understanding and generation, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_105",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_106@0",
            "content": "Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, Michael Mahoney, Kurt Keutzer, Q-BERT: hessian based ultra low precision quantization of BERT, 2020-02-07, The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_106",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_107@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, Christopher Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013-10-21, Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_107",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_108@0",
            "content": "Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu, Patient knowledge distillation for BERT model compression, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_108",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_109@0",
            "content": "UNKNOWN, None, , Xuanjing Huang, and Xipeng Qiu. 2021. Early exiting with ensemble internal classifiers. CoRR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_109",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_110@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_110",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_111@0",
            "content": "Ruochen Wang, Minhao Cheng, Xiangning Chen, Xiaocheng Tang, Cho-Jui Hsieh, Rethinking architecture selection in differentiable NAS, 2021-05-03, 9th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_111",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_112@0",
            "content": "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin, Deebert: Dynamic early exiting for accelerating BERT inference, 2020-07-05, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_112",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_113@0",
            "content": "Ji Xin, Raphael Tang, Yaoliang Yu, Jimmy Lin, Berxit: Early exiting for BERT with better finetuning and extension to regression, 2021-04-19, Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_113",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_114@0",
            "content": "Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, Ming Zhou, Bert-of-theseus: Compressing BERT by progressive module replacing, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_114",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_115@0",
            "content": "Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, Yin Tian, Qianqian Dong, Weitang Liu, Bo Shi, Yiming Cui, Junyi Li, Jun Zeng, Rongzhao Wang, Weijian Xie, Yanting Li, Yina Patterson, Zuoyu Tian, Yiwen Zhang, He Zhou, Shaoweihua Liu, Zhe Zhao, Qipeng Zhao, Cong Yue, Xinrui Zhang, Zhengliang Yang, Kyle Richardson, Zhenzhong Lan, CLUE: A chinese language understanding evaluation benchmark, 2020-12-08, Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_115",
            "start": 0,
            "end": 534,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_116@0",
            "content": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, V Quoc,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_116",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_117@0",
            "content": "Chengliang Zhang, Minchen Yu, Wei Wang, Feng Yan, Mark: Exploiting cloud services for costeffective, slo-aware machine learning inference serving, 2019-07-10, 2019 USENIX Annual Technical Conference, USENIX ATC 2019, USENIX Association.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_117",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_118@0",
            "content": "Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian Mcauley, Ke Xu, Furu Wei, BERT loses patience: Fast and robust inference with early exit, 2020-12-06, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_118",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_119@0",
            "content": "Wei Zhu, Leebert: Learned early exit for BERT with cross-level optimization, 2021-08-01, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_119",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "54-ARR_v1_120@0",
            "content": "Eric Ziegel, The elements of statistical learning, 2003, Technometrics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "54-ARR_v1_120",
            "start": 0,
            "end": 72,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "54-ARR_v1_0",
            "tgt_ix": "54-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_0",
            "tgt_ix": "54-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_1",
            "tgt_ix": "54-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_1",
            "tgt_ix": "54-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_0",
            "tgt_ix": "54-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_2",
            "tgt_ix": "54-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_4",
            "tgt_ix": "54-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_5",
            "tgt_ix": "54-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_6",
            "tgt_ix": "54-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_7",
            "tgt_ix": "54-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_8",
            "tgt_ix": "54-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_9",
            "tgt_ix": "54-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_10",
            "tgt_ix": "54-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_11",
            "tgt_ix": "54-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_0",
            "tgt_ix": "54-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_12",
            "tgt_ix": "54-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_14",
            "tgt_ix": "54-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_13",
            "tgt_ix": "54-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_13",
            "tgt_ix": "54-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_13",
            "tgt_ix": "54-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_0",
            "tgt_ix": "54-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_15",
            "tgt_ix": "54-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_17",
            "tgt_ix": "54-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_18",
            "tgt_ix": "54-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_19",
            "tgt_ix": "54-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_20",
            "tgt_ix": "54-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_21",
            "tgt_ix": "54-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_22",
            "tgt_ix": "54-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_23",
            "tgt_ix": "54-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_24",
            "tgt_ix": "54-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_25",
            "tgt_ix": "54-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_0",
            "tgt_ix": "54-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_26",
            "tgt_ix": "54-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_27",
            "tgt_ix": "54-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_27",
            "tgt_ix": "54-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_29",
            "tgt_ix": "54-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_28",
            "tgt_ix": "54-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_28",
            "tgt_ix": "54-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_28",
            "tgt_ix": "54-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_27",
            "tgt_ix": "54-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_30",
            "tgt_ix": "54-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_32",
            "tgt_ix": "54-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_31",
            "tgt_ix": "54-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_31",
            "tgt_ix": "54-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_31",
            "tgt_ix": "54-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_27",
            "tgt_ix": "54-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_33",
            "tgt_ix": "54-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_35",
            "tgt_ix": "54-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_36",
            "tgt_ix": "54-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_37",
            "tgt_ix": "54-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_38",
            "tgt_ix": "54-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_39",
            "tgt_ix": "54-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_34",
            "tgt_ix": "54-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_34",
            "tgt_ix": "54-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_34",
            "tgt_ix": "54-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_34",
            "tgt_ix": "54-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_34",
            "tgt_ix": "54-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_34",
            "tgt_ix": "54-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_34",
            "tgt_ix": "54-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_0",
            "tgt_ix": "54-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_40",
            "tgt_ix": "54-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_41",
            "tgt_ix": "54-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_41",
            "tgt_ix": "54-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_43",
            "tgt_ix": "54-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_44",
            "tgt_ix": "54-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_42",
            "tgt_ix": "54-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_42",
            "tgt_ix": "54-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_42",
            "tgt_ix": "54-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_42",
            "tgt_ix": "54-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_41",
            "tgt_ix": "54-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_45",
            "tgt_ix": "54-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_47",
            "tgt_ix": "54-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_49",
            "tgt_ix": "54-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_50",
            "tgt_ix": "54-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_51",
            "tgt_ix": "54-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_52",
            "tgt_ix": "54-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_53",
            "tgt_ix": "54-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_54",
            "tgt_ix": "54-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_48",
            "tgt_ix": "54-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_57",
            "tgt_ix": "54-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_55",
            "tgt_ix": "54-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_58",
            "tgt_ix": "54-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_0",
            "tgt_ix": "54-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_59",
            "tgt_ix": "54-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_60",
            "tgt_ix": "54-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_60",
            "tgt_ix": "54-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_0",
            "tgt_ix": "54-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_61",
            "tgt_ix": "54-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_62",
            "tgt_ix": "54-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_62",
            "tgt_ix": "54-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_62",
            "tgt_ix": "54-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_63",
            "tgt_ix": "54-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_62",
            "tgt_ix": "54-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_64",
            "tgt_ix": "54-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_62",
            "tgt_ix": "54-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_65",
            "tgt_ix": "54-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_67",
            "tgt_ix": "54-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_62",
            "tgt_ix": "54-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_62",
            "tgt_ix": "54-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_66",
            "tgt_ix": "54-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "54-ARR_v1_0",
            "tgt_ix": "54-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_1",
            "tgt_ix": "54-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_2",
            "tgt_ix": "54-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_2",
            "tgt_ix": "54-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_2",
            "tgt_ix": "54-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_2",
            "tgt_ix": "54-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_2",
            "tgt_ix": "54-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_2",
            "tgt_ix": "54-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_2",
            "tgt_ix": "54-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_2",
            "tgt_ix": "54-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_3",
            "tgt_ix": "54-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_4",
            "tgt_ix": "54-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_4",
            "tgt_ix": "54-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_4",
            "tgt_ix": "54-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_4",
            "tgt_ix": "54-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_5",
            "tgt_ix": "54-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_5",
            "tgt_ix": "54-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_5",
            "tgt_ix": "54-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_5",
            "tgt_ix": "54-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_6",
            "tgt_ix": "54-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_6",
            "tgt_ix": "54-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_6",
            "tgt_ix": "54-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_6",
            "tgt_ix": "54-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_6",
            "tgt_ix": "54-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_7",
            "tgt_ix": "54-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_7",
            "tgt_ix": "54-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_7",
            "tgt_ix": "54-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_7",
            "tgt_ix": "54-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_7",
            "tgt_ix": "54-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_7",
            "tgt_ix": "54-ARR_v1_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_7",
            "tgt_ix": "54-ARR_v1_7@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_7",
            "tgt_ix": "54-ARR_v1_7@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_8",
            "tgt_ix": "54-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_8",
            "tgt_ix": "54-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_8",
            "tgt_ix": "54-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_8",
            "tgt_ix": "54-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_8",
            "tgt_ix": "54-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_8",
            "tgt_ix": "54-ARR_v1_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_8",
            "tgt_ix": "54-ARR_v1_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_9",
            "tgt_ix": "54-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_9",
            "tgt_ix": "54-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_9",
            "tgt_ix": "54-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_10",
            "tgt_ix": "54-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_10",
            "tgt_ix": "54-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_10",
            "tgt_ix": "54-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_11",
            "tgt_ix": "54-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_12",
            "tgt_ix": "54-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_12",
            "tgt_ix": "54-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_12",
            "tgt_ix": "54-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_13",
            "tgt_ix": "54-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_14",
            "tgt_ix": "54-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_14",
            "tgt_ix": "54-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_14",
            "tgt_ix": "54-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_14",
            "tgt_ix": "54-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_14",
            "tgt_ix": "54-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_14",
            "tgt_ix": "54-ARR_v1_14@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_15",
            "tgt_ix": "54-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_15",
            "tgt_ix": "54-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_16",
            "tgt_ix": "54-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_17",
            "tgt_ix": "54-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_17",
            "tgt_ix": "54-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_17",
            "tgt_ix": "54-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_17",
            "tgt_ix": "54-ARR_v1_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_18",
            "tgt_ix": "54-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_18",
            "tgt_ix": "54-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_18",
            "tgt_ix": "54-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_18",
            "tgt_ix": "54-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_18",
            "tgt_ix": "54-ARR_v1_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_19",
            "tgt_ix": "54-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_19",
            "tgt_ix": "54-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_19",
            "tgt_ix": "54-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_20",
            "tgt_ix": "54-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_21",
            "tgt_ix": "54-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_22",
            "tgt_ix": "54-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_22",
            "tgt_ix": "54-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_22",
            "tgt_ix": "54-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_23",
            "tgt_ix": "54-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_23",
            "tgt_ix": "54-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_23",
            "tgt_ix": "54-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_23",
            "tgt_ix": "54-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_24",
            "tgt_ix": "54-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_24",
            "tgt_ix": "54-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_24",
            "tgt_ix": "54-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_24",
            "tgt_ix": "54-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_25",
            "tgt_ix": "54-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_25",
            "tgt_ix": "54-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_25",
            "tgt_ix": "54-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_25",
            "tgt_ix": "54-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_25",
            "tgt_ix": "54-ARR_v1_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_25",
            "tgt_ix": "54-ARR_v1_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_25",
            "tgt_ix": "54-ARR_v1_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_25",
            "tgt_ix": "54-ARR_v1_25@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_26",
            "tgt_ix": "54-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_26",
            "tgt_ix": "54-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_26",
            "tgt_ix": "54-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_26",
            "tgt_ix": "54-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_27",
            "tgt_ix": "54-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_28",
            "tgt_ix": "54-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_29",
            "tgt_ix": "54-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_29",
            "tgt_ix": "54-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_30",
            "tgt_ix": "54-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_30",
            "tgt_ix": "54-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_30",
            "tgt_ix": "54-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_30",
            "tgt_ix": "54-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_30",
            "tgt_ix": "54-ARR_v1_30@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_30",
            "tgt_ix": "54-ARR_v1_30@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_30",
            "tgt_ix": "54-ARR_v1_30@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_30",
            "tgt_ix": "54-ARR_v1_30@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_31",
            "tgt_ix": "54-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_32",
            "tgt_ix": "54-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_32",
            "tgt_ix": "54-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_32",
            "tgt_ix": "54-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_32",
            "tgt_ix": "54-ARR_v1_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_32",
            "tgt_ix": "54-ARR_v1_32@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_33",
            "tgt_ix": "54-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_33",
            "tgt_ix": "54-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_33",
            "tgt_ix": "54-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_33",
            "tgt_ix": "54-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_33",
            "tgt_ix": "54-ARR_v1_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_34",
            "tgt_ix": "54-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_35",
            "tgt_ix": "54-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_35",
            "tgt_ix": "54-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_35",
            "tgt_ix": "54-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_35",
            "tgt_ix": "54-ARR_v1_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_36",
            "tgt_ix": "54-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_36",
            "tgt_ix": "54-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_36",
            "tgt_ix": "54-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_37",
            "tgt_ix": "54-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_37",
            "tgt_ix": "54-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_37",
            "tgt_ix": "54-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_38",
            "tgt_ix": "54-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_39",
            "tgt_ix": "54-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_39",
            "tgt_ix": "54-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_39",
            "tgt_ix": "54-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_40",
            "tgt_ix": "54-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_40",
            "tgt_ix": "54-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_40",
            "tgt_ix": "54-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_40",
            "tgt_ix": "54-ARR_v1_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_41",
            "tgt_ix": "54-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_42",
            "tgt_ix": "54-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_43",
            "tgt_ix": "54-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_44",
            "tgt_ix": "54-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_44",
            "tgt_ix": "54-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_45",
            "tgt_ix": "54-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_46",
            "tgt_ix": "54-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_47",
            "tgt_ix": "54-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_48",
            "tgt_ix": "54-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_49",
            "tgt_ix": "54-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_49",
            "tgt_ix": "54-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_50",
            "tgt_ix": "54-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_50",
            "tgt_ix": "54-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_50",
            "tgt_ix": "54-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_51",
            "tgt_ix": "54-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_51",
            "tgt_ix": "54-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_52",
            "tgt_ix": "54-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_52",
            "tgt_ix": "54-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_53",
            "tgt_ix": "54-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_53",
            "tgt_ix": "54-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_54",
            "tgt_ix": "54-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_54",
            "tgt_ix": "54-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_54",
            "tgt_ix": "54-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_54",
            "tgt_ix": "54-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_54",
            "tgt_ix": "54-ARR_v1_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_55",
            "tgt_ix": "54-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_55",
            "tgt_ix": "54-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_55",
            "tgt_ix": "54-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_55",
            "tgt_ix": "54-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_55",
            "tgt_ix": "54-ARR_v1_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_56",
            "tgt_ix": "54-ARR_v1_56@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_57",
            "tgt_ix": "54-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_57",
            "tgt_ix": "54-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_57",
            "tgt_ix": "54-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_57",
            "tgt_ix": "54-ARR_v1_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_57",
            "tgt_ix": "54-ARR_v1_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_57",
            "tgt_ix": "54-ARR_v1_57@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_57",
            "tgt_ix": "54-ARR_v1_57@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_58",
            "tgt_ix": "54-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_58",
            "tgt_ix": "54-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_58",
            "tgt_ix": "54-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_58",
            "tgt_ix": "54-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_59",
            "tgt_ix": "54-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_59",
            "tgt_ix": "54-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_59",
            "tgt_ix": "54-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_59",
            "tgt_ix": "54-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_59",
            "tgt_ix": "54-ARR_v1_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_59",
            "tgt_ix": "54-ARR_v1_59@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_59",
            "tgt_ix": "54-ARR_v1_59@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_60",
            "tgt_ix": "54-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_61",
            "tgt_ix": "54-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_61",
            "tgt_ix": "54-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_61",
            "tgt_ix": "54-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_61",
            "tgt_ix": "54-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_62",
            "tgt_ix": "54-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_63",
            "tgt_ix": "54-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_63",
            "tgt_ix": "54-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_63",
            "tgt_ix": "54-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_64",
            "tgt_ix": "54-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_65",
            "tgt_ix": "54-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_65",
            "tgt_ix": "54-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_65",
            "tgt_ix": "54-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_65",
            "tgt_ix": "54-ARR_v1_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_65",
            "tgt_ix": "54-ARR_v1_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_65",
            "tgt_ix": "54-ARR_v1_65@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_66",
            "tgt_ix": "54-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_66",
            "tgt_ix": "54-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_66",
            "tgt_ix": "54-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_66",
            "tgt_ix": "54-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_66",
            "tgt_ix": "54-ARR_v1_66@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_66",
            "tgt_ix": "54-ARR_v1_66@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_66",
            "tgt_ix": "54-ARR_v1_66@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_66",
            "tgt_ix": "54-ARR_v1_66@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_67",
            "tgt_ix": "54-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_68",
            "tgt_ix": "54-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_69",
            "tgt_ix": "54-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_70",
            "tgt_ix": "54-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_71",
            "tgt_ix": "54-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_72",
            "tgt_ix": "54-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_73",
            "tgt_ix": "54-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_74",
            "tgt_ix": "54-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_75",
            "tgt_ix": "54-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_76",
            "tgt_ix": "54-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_77",
            "tgt_ix": "54-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_78",
            "tgt_ix": "54-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_79",
            "tgt_ix": "54-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_80",
            "tgt_ix": "54-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_81",
            "tgt_ix": "54-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_82",
            "tgt_ix": "54-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_83",
            "tgt_ix": "54-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_84",
            "tgt_ix": "54-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_85",
            "tgt_ix": "54-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_86",
            "tgt_ix": "54-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_87",
            "tgt_ix": "54-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_88",
            "tgt_ix": "54-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_89",
            "tgt_ix": "54-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_90",
            "tgt_ix": "54-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_91",
            "tgt_ix": "54-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_92",
            "tgt_ix": "54-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_93",
            "tgt_ix": "54-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_94",
            "tgt_ix": "54-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_95",
            "tgt_ix": "54-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_96",
            "tgt_ix": "54-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_97",
            "tgt_ix": "54-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_98",
            "tgt_ix": "54-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_99",
            "tgt_ix": "54-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_100",
            "tgt_ix": "54-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_101",
            "tgt_ix": "54-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_102",
            "tgt_ix": "54-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_103",
            "tgt_ix": "54-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_104",
            "tgt_ix": "54-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_105",
            "tgt_ix": "54-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_106",
            "tgt_ix": "54-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_107",
            "tgt_ix": "54-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_108",
            "tgt_ix": "54-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_109",
            "tgt_ix": "54-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_110",
            "tgt_ix": "54-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_111",
            "tgt_ix": "54-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_112",
            "tgt_ix": "54-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_113",
            "tgt_ix": "54-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_114",
            "tgt_ix": "54-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_115",
            "tgt_ix": "54-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_116",
            "tgt_ix": "54-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_117",
            "tgt_ix": "54-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_118",
            "tgt_ix": "54-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_119",
            "tgt_ix": "54-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "54-ARR_v1_120",
            "tgt_ix": "54-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1295,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "54-ARR",
        "version": 1
    }
}