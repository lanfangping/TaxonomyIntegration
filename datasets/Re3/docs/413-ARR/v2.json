{
    "nodes": [
        {
            "ix": "413-ARR_v2_0",
            "content": "Improving negation detection with negation-focused pre-training",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_2",
            "content": "Negation is a common linguistic feature that is crucial in many language understanding tasks, yet it remains a hard problem due to diversity in its expression in different types of text. Recent work has shown that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains. We propose a new negation-focused pre-training strategy, involving targeted data augmentation and negation masking, to better incorporate negation information into language models. Extensive experiments on common benchmarks show that our proposed approach improves negation detection performance and generalizability over the strong baseline Neg-BERT (Khandelwal and Sawant, 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "413-ARR_v2_4",
            "content": "Negation is an important linguistic phenomenon that appears commonly in natural language but is underrepresented in common NLP benchmarks (Hossain et al., 2020). Furthermore, the Checklist benchmark (Ribeiro et al., 2020) shows that most sentiment analyzers and machine comprehension models struggle with samples containing negation. Negation is even more important in biomedical domain text, where patients are carefully defined as having/not having specific characteristics. Even within the biomedical domain, there are many types of text such as clinical notes, lab reports, or research publications, each with particular characteristics in relation to the use of negation. A recent study on English texts found that negation detection models do not transfer well across domains, due to variations in expression of negation (Khandelwal and Sawant, 2020). It remains a challenge to solve negation in general, even with state-of-the-art NLP models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_5",
            "content": "Negation detection is typically defined as consisting of the two sub-tasks of: (1) cue detection, detecting the cue phrase that triggers the negation; and (2) scope resolution, determining the affected spans that are negated. There are three primary datasets that have been used to evaluate negation:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_6",
            "content": "(1) the BioScope corpus (Vincze et al., 2008) includes full papers and abstracts of biological papers; (2) the SFU corpus (Konstantinova et al., 2012) is a collection of product reviews; and (3) the Sherlock dataset (Morante and Blanco, 2012) consists of short literary works. There are differences in annotation schemes across the datasets, such as whether or not the cues are included inside scope annotation, and sub-optimal cross-dataset results have been observed, providing clear indications that the datasets are highly divergent in language use and negation types.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_7",
            "content": "In this work, we aim to extend the transfer learning capability of NegBERT (Khandelwal and Sawant, 2020) through additional pre-training with task-related augmented training data, and a new masking objective. Our contributions are:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_8",
            "content": "\u2022 We introduce an approach to augmenting data to emphasize negation in pre-training. \u2022 We propose a novel extension to the standard random masked language model objective in pre-training to explicitly mask negation cues, to make the models more robust to negation. \u2022 We conduct extensive experiments on different benchmarks to evaluate cross-domain performance of large pre-trained language models as well as the effectiveness of the proposed pre-training strategies; code is available at https://github.com/joey234/ negation-focused-pretraining.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_9",
            "content": "Related work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "413-ARR_v2_10",
            "content": "To date, negation detection has been heavily reliant on rule-based systems. Chapman et al. (2001) proposed a simple system, NegEx, based on regular expressions to detect negation cues in a sentence given a concept of interest (the scope). NegEx re-mains the most popular approach to negation detection, especially in the clinical domain to determine the polarity of clinical concepts (e.g., as sourced from MetaMap (Aronson and Lang, 2010)). Further research has extended NegEx with syntactic information (Mehrabi et al., 2015;Peng et al., 2018), and shown that rule-based systems can achieve relatively good performance for detecting negation, especially in the biomedical domain, but do not generalize well to other domains or datasets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_11",
            "content": "To approach negation cue and negation scope detection with supervised machine learning, two classification tasks are defined: (1) finding negation tokens, and (2) classifying tokens as the first or last (or neither) token within the scope of negation. Most work follows a common scheme in extracting various features from the sentence, and using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016). Recently, research has shifted to applying deep learning methods to the task. Most approaches make use of RNN-based architectures to encode the input sentences, combined with a softmax layer for classification (Lazib et al., 2019;Chen, 2019). Despite the high performance on common benchmarks, results are biased by the fact that negation scope is often delimited by punctuation and other dataset artefacts (Fancellu et al., 2017). As such, they are potentially only learning domain-specific surface features rather than capturing the true semantics of negation. NegBERT applies a large pre-trained language model to the problem of negation detection, outperforming previous deep learning methods on negation detection, with especially high gains on scope resolution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_12",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "413-ARR_v2_13",
            "content": "Our proposed pre-training strategy consists of two main components: (1) negation-focused data collection in which we first collect relevant data that contains negation; and (2) negation-focused pretraining that makes use of the negation-focused data to emphasize negation instances, and adopts a novel negation-specific masking strategy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_14",
            "content": "Negation-focused data collection",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "413-ARR_v2_15",
            "content": "We aim to construct a dataset that is enriched with negation information, to support negationsensitized pre-training of large language models. To obtain sentences with negations, we extend the NegEx lexicon with additional negation cues obtained from biomedical texts (Morante, 2010), and apply it to sentences extracted from a corpus using the SpaCy English sentence tokenizer, keeping only those sentences with at least one identified negation cue.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_16",
            "content": "For the biomedical domain, we use texts in the TREC-CDS 2021 snapshot 1 of the clinical trials registry. 2 Clinical trials are documents describing the protocols and relevant patient characteristics of a clinical research study. Descriptions of clinical trials can be quite long, but a core aspect of the trial description is the patient inclusion/exclusion criteria, specifying what types of characteristics or conditions a patient must have/not have in order to be suitable for the trial. The reasons for choosing this data are that: (1) it is in-domain for the biomedical domain; (2) the texts are well-formed sentences with proper grammatical structure; and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_17",
            "content": "(3) the texts contain many negations, especially in the inclusion/exclusion criteria sections. For the general domain, we apply this approach to wikitext (Merity et al., 2016), a set of verified articles in Wikipedia. We sample the data equally from these two sets, obtaining 1, 381, 948 negation sentences.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_18",
            "content": "Negation-focused pre-training",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "413-ARR_v2_19",
            "content": "Adaptive pre-training on target domain data has been shown to be an effective strategy for domain adaptation (Gururangan et al., 2020). We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representations for sentences containing negation. Using the negation-focused data, we first apply the standard random word masking strategy (Devlin et al., 2019) and train the model with the masked language model objective.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_20",
            "content": "As part of the collection of the negation-focused data, we obtain predictions of negation cues in all the sentences, which can be explicitly incorporated to make the model more robust to negation. Inspired by work on entity and span masking (Joshi et al., 2020;Yamada et al., 2020), we explore explicitly incorporating information about negation cues into the model by masking these cues, and targeting prediction of the masked cue in the pretraining stage. Below is an example of how a sentence is tokenized under our masking scheme:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_21",
            "content": "No serious complications such as hypertension, diabetes. \u21d2 [CUE] serious complications such as [MASK], diabetes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_22",
            "content": "A new type of token [CUE] is introduced under this masking scheme, and the model needs to reconstruct the original sentence by predicting both the [CUE] token to be No, and the randomlymasked token [MASK] to be hypertension. By always masking negation cues in all the sentences, we force the model to focus more on this type of token, and thus, aim to learn better embeddings incorporating information of how a negation cue is represented in the context of the sentence. Moreover, by using a different token to mask negation cues, we ensure that the model learns to distinguish between different types of tokens. In this work, we replace the BERT encoder of NegBERT with RoBERTa and apply whole-word masking, meaning that all the sub-word tokens that constitute a word will be masked.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_23",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "413-ARR_v2_24",
            "content": "Experimental settings",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "413-ARR_v2_25",
            "content": "Following the experimental settings in NegBERT, we use the three standard benchmarks for negation cue detection and scope resolution tasks, i.e. BioScope (Vincze et al., 2008) (separated into two subsets, sourced from abstracts and full-text papers, resp.), the SFU product reviews dataset (Konstantinova et al., 2012), and the Sherlock dataset (Morante and Blanco, 2012). In addition, we use the negation-annotated subset of VetCompass UK 3 (Cheng et al., 2017), consisting of clinical notes in the veterinary domain, which are very informal compared to BioScope. It also contains abbreviations and shortening of terms, as well as certain unique negation cues. To investigate cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets. Detailed statistics of these datasets are presented in Table 1. Note that we do not experiment with the setting of training with all the combined training data from the corpora as it has been pointed out by previous work that doing so hurts the performance of the models (Jim\u00e9nez-Zafra et al., 2020;Barnes et al., 2021) due to differences in annotation schemes between the corpora introducing noise during training. Re-annotating all the datasets using a common annotation scheme would be a potential solution here, We formulate the two tasks as sequence labeling problems, where each token is tagged with a corresponding label. For cue detection, we use the annotation scheme {0: Affix, 1: Normal Cue, 2: Part of multiword cue, 3: Not part of cue}. For scope resolution, we use gold cue information and two labels {0: Outside negation scope, 1: Part of negation scope}. We adopt the same hyperparameters as NegBERT. Following the standard evaluation scheme in previous negation detection works, all systems are evaluated using token-level F 1 -score, based on whether it is inside or outside of any negation cue or scope. Methods evaluated include: (1) NegBERT; (2) AugNB = NegBERT plus pre-training on negation-focused data; and (3) CueNB = NegBERT plus pre-training on negationfocused data and the negation cue masking objective. Note that for NegBERT, we also replace the BERT encoder with RoBERTa to ensure results are comparable between the models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_26",
            "content": "Main results",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "413-ARR_v2_27",
            "content": "Tables 2 and 3 report the performance of negation cue detection and negation scope resolution, respectively. Results reported are the average of 5 runs with different random seeds. NegBERT results are produced using the official implementation. 4 To provide a more general view, we summarize the results in Table 4. In general, we observe gains in both the same-dataset setting (training and test set belongs to one corpus) and cross-dataset setting (training one one training set and testing on all others test sets) for both of the proposed models, with CueNB achieving the largest gains.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_28",
            "content": "We observe similar trends across all datasets for both cue detection and scope resolution. Regarding the in-dataset setting, AugNB outperforms the baseline NegBERT on all datasets except for Sherlock. Gains are more noticeable over the biomedical datasets (BioScope, VetCompass). For Sherlock, however, we observe a slight degradation in performance with the proposed pre-training scheme. This is likely due to the fact that Sherlock has major differences in annotation scheme compared to other corpora, specifically including scopes to the left of cues, while in BioScope and SFU, the scope is usually annotated only to the right of cues. Also, the cue itself is not considered to be part of the scope in Sherlock or SFU, unlike in BioScope.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_29",
            "content": "In the cross-dataset setting, we record gains across all benchmarks. The largest cross-dataset improvements over NegBERT are for SFU, perhaps due to SFU being the largest dataset in size, containing a relatively large number of unique cues. CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue helps the model learn better representations for negation cues and thus, better distinguish between cues and normal words. These results show that our negation-focused pre-training strategy is effective for improving the transfer learning performance of pre-trained language models on the negation detection task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_30",
            "content": "Discussion",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "413-ARR_v2_31",
            "content": "We conducted an error analysis on the VetCompass validation set to see what qualitative improvement CueNB makes over NegBERT. For cue detection, there are two main types of errors that CueNB helps alleviate. First, CueNB can detect more unique cues such as negative, won't, and also multiword cues like no longer. Second, CueNB is able to recognize cases when the negations are actually just speculative. For example, in the sentence O reports has smelled for past week, not sure if anal glands . . . , the word not is part of the speculation phrase not sure, indicating that this is not truly a negation phrase but rather expresses uncertainty. For scope resolution, CueNB mostly helps in recognizing the correct scope boundary. One common case is when the cue relates to multiple spans in a sentence. In the sentence Examination: QAR, thorac ausc and abdo palp NAD, 5 NegBERT only recognizes the nearest span abdo palp NAD to be the scope, whereas CueNB recognizes the full correct span thorac ausc and abdo palp NAD. It also helps in cases where there are multiple separate negations in the same sentence. For instance, in the We also conduct an ablation study to understand the impact of each component of the proposed pretraining strategy. Table 5 presents the results of different variations of the proposed pre-training scheme on the BioScope-Abstract validation split. We consider two variations, pre-training with: (1) only the negation-focused data (equivalent to the AugNB model); and (2) only the cue masking objective. To model the latter variation, we explicitly mask the cue in the BioScope training set, then pretrain on this training set. From the results, we see that both strategies help improve the baseline Neg-BERT on cue detection and scope resolution, with explicitly masking the cues being the most important. Combining both strategies (CueNB) further improves the overall results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_32",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "413-ARR_v2_33",
            "content": "In this work, we propose a new negation-focused pre-training strategy to explicitly incorporate negation information into pre-trained language models. Empirical results on common benchmarks show that the proposed strategy helps improve the performance of pre-trained language models on the negation detection task when evaluating on the same source dataset, as well as their transferability to target data in different domains. Despite the gains over previous methods, the sub-optimal results on some benchmarks show that negation remains a big challenge in NLP.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "413-ARR_v2_34",
            "content": "Fran\u00e7ois-Michel Alan R Aronson,  Lang, An overview of MetaMap: Historical perspective and recent advances, 2010, Journal of the American Medical Informatics Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Fran\u00e7ois-Michel Alan R Aronson",
                    " Lang"
                ],
                "title": "An overview of MetaMap: Historical perspective and recent advances",
                "pub_date": "2010",
                "pub_title": "Journal of the American Medical Informatics Association",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_35",
            "content": "Jeremy Barnes, Erik Velldal, Lilja \u00d8vrelid, Improving sentiment analysis with multi-task learning of negation, 2021, Natural Language Engineering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Jeremy Barnes",
                    "Erik Velldal",
                    "Lilja \u00d8vrelid"
                ],
                "title": "Improving sentiment analysis with multi-task learning of negation",
                "pub_date": "2021",
                "pub_title": "Natural Language Engineering",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_36",
            "content": "Will Wendy W Chapman, Paul Bridewell,  Hanbury, F Gregory, Bruce G Cooper,  Buchanan, A simple algorithm for identifying negated findings and diseases in discharge summaries, 2001, Journal of biomedical informatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Will Wendy W Chapman",
                    "Paul Bridewell",
                    " Hanbury",
                    "F Gregory",
                    "Bruce G Cooper",
                    " Buchanan"
                ],
                "title": "A simple algorithm for identifying negated findings and diseases in discharge summaries",
                "pub_date": "2001",
                "pub_title": "Journal of biomedical informatics",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_37",
            "content": "Long Chen, Attention-based deep learning system for negation and assertion detection in clinical notes, 2019, International Journal of Artificial Intelligence and Applications (IJAIA), .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Long Chen"
                ],
                "title": "Attention-based deep learning system for negation and assertion detection in clinical notes",
                "pub_date": "2019",
                "pub_title": "International Journal of Artificial Intelligence and Applications (IJAIA)",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_38",
            "content": "Katherine Cheng, Timothy Baldwin, Karin Verspoor, Automatic negation and speculation detection in veterinary clinical text, 2017, Proceedings of the Australasian Language Technology Association Workshop, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Katherine Cheng",
                    "Timothy Baldwin",
                    "Karin Verspoor"
                ],
                "title": "Automatic negation and speculation detection in veterinary clinical text",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Australasian Language Technology Association Workshop",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_39",
            "content": "P Noa, Maite Cruz, Ruslan Taboada,  Mitkov, A machine-learning approach to negation and speculation detection for sentiment analysis, 2016, Journal of the Association for Information Science and Technology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "P Noa",
                    "Maite Cruz",
                    "Ruslan Taboada",
                    " Mitkov"
                ],
                "title": "A machine-learning approach to negation and speculation detection for sentiment analysis",
                "pub_date": "2016",
                "pub_title": "Journal of the Association for Information Science and Technology",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_40",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_41",
            "content": "Federico Fancellu, Adam Lopez, Bonnie Webber, Hangfeng He, Detecting negation scope is easy, except when it isn't, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Federico Fancellu",
                    "Adam Lopez",
                    "Bonnie Webber",
                    "Hangfeng He"
                ],
                "title": "Detecting negation scope is easy, except when it isn't",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "413-ARR_v2_42",
            "content": "Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey,  Smith, Don't stop pretraining: Adapt language models to domains and tasks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Ana Suchin Gururangan",
                    "Swabha Marasovi\u0107",
                    "Kyle Swayamdipta",
                    "Iz Lo",
                    "Doug Beltagy",
                    "Noah A Downey",
                    " Smith"
                ],
                "title": "Don't stop pretraining: Adapt language models to domains and tasks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_43",
            "content": "Venelin Md Mosharaf Hossain, Pranoy Kovatchev, Tiffany Dutta, Elizabeth Kao, Eduardo Wei,  Blanco, An analysis of natural language inference benchmarks through the lens of negation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Venelin Md Mosharaf Hossain",
                    "Pranoy Kovatchev",
                    "Tiffany Dutta",
                    "Elizabeth Kao",
                    "Eduardo Wei",
                    " Blanco"
                ],
                "title": "An analysis of natural language inference benchmarks through the lens of negation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_44",
            "content": "Roser Salud Mar\u00eda Jim\u00e9nez-Zafra, Mar\u00eda Morante, L Teresa Mart\u00edn-Valdivia,  Ure\u00f1a-L\u00f3pez, Corpora annotated with negation: An overview, 2020, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Roser Salud Mar\u00eda Jim\u00e9nez-Zafra",
                    "Mar\u00eda Morante",
                    "L Teresa Mart\u00edn-Valdivia",
                    " Ure\u00f1a-L\u00f3pez"
                ],
                "title": "Corpora annotated with negation: An overview",
                "pub_date": "2020",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_45",
            "content": "Mandar Joshi, Danqi Chen, Yinhan Liu, S Daniel, Luke Weld, Omer Zettlemoyer,  Levy, Span-BERT: Improving pre-training by representing and predicting spans, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Mandar Joshi",
                    "Danqi Chen",
                    "Yinhan Liu",
                    "S Daniel",
                    "Luke Weld",
                    "Omer Zettlemoyer",
                    " Levy"
                ],
                "title": "Span-BERT: Improving pre-training by representing and predicting spans",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_46",
            "content": "Aditya Khandelwal, Suraj Sawant, NegBERT: A transfer learning approach for negation detection and scope resolution, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Aditya Khandelwal",
                    "Suraj Sawant"
                ],
                "title": "NegBERT: A transfer learning approach for negation detection and scope resolution",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 12th Language Resources and Evaluation Conference",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_47",
            "content": "Natalia Konstantinova, C Sheila, Noa De Sousa, Manuel Cruz, Maite Ma\u00f1a, Ruslan Taboada,  Mitkov, A review corpus annotated for negation, speculation and their scope, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Natalia Konstantinova",
                    "C Sheila",
                    "Noa De Sousa",
                    "Manuel Cruz",
                    "Maite Ma\u00f1a",
                    "Ruslan Taboada",
                    " Mitkov"
                ],
                "title": "A review corpus annotated for negation, speculation and their scope",
                "pub_date": "2012",
                "pub_title": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_48",
            "content": "UNKNOWN, None, 2019, Negation scope detection with recurrent neural networks models in review texts. International Journal of High Performance Computing and Networking, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Negation scope detection with recurrent neural networks models in review texts. International Journal of High Performance Computing and Networking",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_49",
            "content": "UNKNOWN, None, 2019, RoBERTa: A robustly optimized BERT pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "RoBERTa: A robustly optimized BERT pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_50",
            "content": "Saeed Mehrabi, Anand Krishnan, Sunghwan Sohn, Alexandra Roch, Heidi Schmidt, Joe Kesterson, Chris Beesley, Paul Dexter, Max Schmidt, Hongfang Liu, DEEPEN: A negation detection system for clinical text incorporating dependency relation into NegEx, 2015, Journal of Biomedical Informatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Saeed Mehrabi",
                    "Anand Krishnan",
                    "Sunghwan Sohn",
                    "Alexandra Roch",
                    "Heidi Schmidt",
                    "Joe Kesterson",
                    "Chris Beesley",
                    "Paul Dexter",
                    "Max Schmidt",
                    "Hongfang Liu"
                ],
                "title": "DEEPEN: A negation detection system for clinical text incorporating dependency relation into NegEx",
                "pub_date": "2015",
                "pub_title": "Journal of Biomedical Informatics",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_51",
            "content": "UNKNOWN, None, 2016, Pointer sentinel mixture models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Pointer sentinel mixture models",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_52",
            "content": ", Descriptive analysis of negation cues in biomedical texts, 2010, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10), .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [],
                "title": "Descriptive analysis of negation cues in biomedical texts",
                "pub_date": "2010",
                "pub_title": "Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10)",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_53",
            "content": "Roser Morante, Eduardo Blanco, *SEM 2012 shared task: Resolving the scope and focus of negation, 2012, *SEM 2012: The First Joint Conference on Lexical and Computational Semantics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Roser Morante",
                    "Eduardo Blanco"
                ],
                "title": "*SEM 2012 shared task: Resolving the scope and focus of negation",
                "pub_date": "2012",
                "pub_title": "*SEM 2012: The First Joint Conference on Lexical and Computational Semantics",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_54",
            "content": "Roser Morante, Walter Daelemans, A metalearning approach to processing the scope of negation, 2009, Proceedings of the Thirteenth Conference on Computational Natural Language Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Roser Morante",
                    "Walter Daelemans"
                ],
                "title": "A metalearning approach to processing the scope of negation",
                "pub_date": "2009",
                "pub_title": "Proceedings of the Thirteenth Conference on Computational Natural Language Learning",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_55",
            "content": "Ying Ou, Jon Patrick, Automatic negation detection in narrative pathology reports, 2015, Artificial Intelligence in Medicine, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Ying Ou",
                    "Jon Patrick"
                ],
                "title": "Automatic negation detection in narrative pathology reports",
                "pub_date": "2015",
                "pub_title": "Artificial Intelligence in Medicine",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_56",
            "content": "Yifan Peng, Xiaosong Wang, Le Lu, Mohammadhadi Bagheri, Ronald Summers, Zhiyong Lu, NegBio: a high-performance tool for negation and uncertainty detection in radiology reports, 2018, AMIA Summits on Translational Science Proceedings, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Yifan Peng",
                    "Xiaosong Wang",
                    "Le Lu",
                    "Mohammadhadi Bagheri",
                    "Ronald Summers",
                    "Zhiyong Lu"
                ],
                "title": "NegBio: a high-performance tool for negation and uncertainty detection in radiology reports",
                "pub_date": "2018",
                "pub_title": "AMIA Summits on Translational Science Proceedings",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_57",
            "content": "Tongshuang Marco Tulio Ribeiro, Carlos Wu, Sameer Guestrin,  Singh, Beyond accuracy: Behavioral testing of NLP models with checklist, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Tongshuang Marco Tulio Ribeiro",
                    "Carlos Wu",
                    "Sameer Guestrin",
                    " Singh"
                ],
                "title": "Beyond accuracy: Behavioral testing of NLP models with checklist",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_58",
            "content": "Veronika Vincze, Gy\u00f6rgy Szarvas, Rich\u00e1rd Farkas, Gy\u00f6rgy M\u00f3ra, J\u00e1nos Csirik, The Bio-Scope corpus: biomedical texts annotated for uncertainty, negation and their scopes, 2008, BMC bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Veronika Vincze",
                    "Gy\u00f6rgy Szarvas",
                    "Rich\u00e1rd Farkas",
                    "Gy\u00f6rgy M\u00f3ra",
                    "J\u00e1nos Csirik"
                ],
                "title": "The Bio-Scope corpus: biomedical texts annotated for uncertainty, negation and their scopes",
                "pub_date": "2008",
                "pub_title": "BMC bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "413-ARR_v2_59",
            "content": "Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto, LUKE: Deep contextualized entity representations with entityaware self-attention, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Ikuya Yamada",
                    "Akari Asai",
                    "Hiroyuki Shindo",
                    "Hideaki Takeda",
                    "Yuji Matsumoto"
                ],
                "title": "LUKE: Deep contextualized entity representations with entityaware self-attention",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "413-ARR_v2_0@0",
            "content": "Improving negation detection with negation-focused pre-training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_0",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_2@0",
            "content": "Negation is a common linguistic feature that is crucial in many language understanding tasks, yet it remains a hard problem due to diversity in its expression in different types of text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_2",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_2@1",
            "content": "Recent work has shown that state-of-the-art NLP models underperform on samples containing negation in various tasks, and that negation detection models do not transfer well across domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_2",
            "start": 187,
            "end": 374,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_2@2",
            "content": "We propose a new negation-focused pre-training strategy, involving targeted data augmentation and negation masking, to better incorporate negation information into language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_2",
            "start": 376,
            "end": 555,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_2@3",
            "content": "Extensive experiments on common benchmarks show that our proposed approach improves negation detection performance and generalizability over the strong baseline Neg-BERT (Khandelwal and Sawant, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_2",
            "start": 557,
            "end": 756,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_4@0",
            "content": "Negation is an important linguistic phenomenon that appears commonly in natural language but is underrepresented in common NLP benchmarks (Hossain et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_4",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_4@1",
            "content": "Furthermore, the Checklist benchmark (Ribeiro et al., 2020) shows that most sentiment analyzers and machine comprehension models struggle with samples containing negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_4",
            "start": 162,
            "end": 332,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_4@2",
            "content": "Negation is even more important in biomedical domain text, where patients are carefully defined as having/not having specific characteristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_4",
            "start": 334,
            "end": 475,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_4@3",
            "content": "Even within the biomedical domain, there are many types of text such as clinical notes, lab reports, or research publications, each with particular characteristics in relation to the use of negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_4",
            "start": 477,
            "end": 675,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_4@4",
            "content": "A recent study on English texts found that negation detection models do not transfer well across domains, due to variations in expression of negation (Khandelwal and Sawant, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_4",
            "start": 677,
            "end": 856,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_4@5",
            "content": "It remains a challenge to solve negation in general, even with state-of-the-art NLP models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_4",
            "start": 858,
            "end": 948,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_5@0",
            "content": "Negation detection is typically defined as consisting of the two sub-tasks of: (1) cue detection, detecting the cue phrase that triggers the negation; and (2) scope resolution, determining the affected spans that are negated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_5",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_5@1",
            "content": "There are three primary datasets that have been used to evaluate negation:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_5",
            "start": 226,
            "end": 299,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_6@0",
            "content": "(1) the BioScope corpus (Vincze et al., 2008) includes full papers and abstracts of biological papers; (2) the SFU corpus (Konstantinova et al., 2012) is a collection of product reviews; and (3) the Sherlock dataset (Morante and Blanco, 2012) consists of short literary works.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_6",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_6@1",
            "content": "There are differences in annotation schemes across the datasets, such as whether or not the cues are included inside scope annotation, and sub-optimal cross-dataset results have been observed, providing clear indications that the datasets are highly divergent in language use and negation types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_6",
            "start": 277,
            "end": 571,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_7@0",
            "content": "In this work, we aim to extend the transfer learning capability of NegBERT (Khandelwal and Sawant, 2020) through additional pre-training with task-related augmented training data, and a new masking objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_7",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_7@1",
            "content": "Our contributions are:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_7",
            "start": 209,
            "end": 230,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_8@0",
            "content": "\u2022 We introduce an approach to augmenting data to emphasize negation in pre-training. \u2022 We propose a novel extension to the standard random masked language model objective in pre-training to explicitly mask negation cues, to make the models more robust to negation. \u2022 We conduct extensive experiments on different benchmarks to evaluate cross-domain performance of large pre-trained language models as well as the effectiveness of the proposed pre-training strategies; code is available at https://github.com/joey234/ negation-focused-pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_8",
            "start": 0,
            "end": 545,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_9@0",
            "content": "Related work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_9",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_10@0",
            "content": "To date, negation detection has been heavily reliant on rule-based systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_10",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_10@1",
            "content": "Chapman et al. (2001) proposed a simple system, NegEx, based on regular expressions to detect negation cues in a sentence given a concept of interest (the scope).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_10",
            "start": 76,
            "end": 237,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_10@2",
            "content": "NegEx re-mains the most popular approach to negation detection, especially in the clinical domain to determine the polarity of clinical concepts (e.g., as sourced from MetaMap (Aronson and Lang, 2010)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_10",
            "start": 239,
            "end": 440,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_10@3",
            "content": "Further research has extended NegEx with syntactic information (Mehrabi et al., 2015;Peng et al., 2018), and shown that rule-based systems can achieve relatively good performance for detecting negation, especially in the biomedical domain, but do not generalize well to other domains or datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_10",
            "start": 442,
            "end": 737,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_11@0",
            "content": "To approach negation cue and negation scope detection with supervised machine learning, two classification tasks are defined: (1) finding negation tokens, and (2) classifying tokens as the first or last (or neither) token within the scope of negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_11",
            "start": 0,
            "end": 250,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_11@1",
            "content": "Most work follows a common scheme in extracting various features from the sentence, and using a classifier to classify each token as the beginning, inside, or outside of a negation cue or scope span (Morante and Daelemans, 2009;Ou and Patrick, 2015;Cruz et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_11",
            "start": 252,
            "end": 519,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_11@2",
            "content": "Recently, research has shifted to applying deep learning methods to the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_11",
            "start": 521,
            "end": 597,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_11@3",
            "content": "Most approaches make use of RNN-based architectures to encode the input sentences, combined with a softmax layer for classification (Lazib et al., 2019;Chen, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_11",
            "start": 599,
            "end": 762,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_11@4",
            "content": "Despite the high performance on common benchmarks, results are biased by the fact that negation scope is often delimited by punctuation and other dataset artefacts (Fancellu et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_11",
            "start": 764,
            "end": 951,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_11@5",
            "content": "As such, they are potentially only learning domain-specific surface features rather than capturing the true semantics of negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_11",
            "start": 953,
            "end": 1082,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_11@6",
            "content": "NegBERT applies a large pre-trained language model to the problem of negation detection, outperforming previous deep learning methods on negation detection, with especially high gains on scope resolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_11",
            "start": 1084,
            "end": 1287,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_12@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_12",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_13@0",
            "content": "Our proposed pre-training strategy consists of two main components: (1) negation-focused data collection in which we first collect relevant data that contains negation; and (2) negation-focused pretraining that makes use of the negation-focused data to emphasize negation instances, and adopts a novel negation-specific masking strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_13",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_14@0",
            "content": "Negation-focused data collection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_14",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_15@0",
            "content": "We aim to construct a dataset that is enriched with negation information, to support negationsensitized pre-training of large language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_15",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_15@1",
            "content": "To obtain sentences with negations, we extend the NegEx lexicon with additional negation cues obtained from biomedical texts (Morante, 2010), and apply it to sentences extracted from a corpus using the SpaCy English sentence tokenizer, keeping only those sentences with at least one identified negation cue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_15",
            "start": 143,
            "end": 449,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_16@0",
            "content": "For the biomedical domain, we use texts in the TREC-CDS 2021 snapshot 1 of the clinical trials registry.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_16",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_16@1",
            "content": "2 Clinical trials are documents describing the protocols and relevant patient characteristics of a clinical research study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_16",
            "start": 105,
            "end": 227,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_16@2",
            "content": "Descriptions of clinical trials can be quite long, but a core aspect of the trial description is the patient inclusion/exclusion criteria, specifying what types of characteristics or conditions a patient must have/not have in order to be suitable for the trial.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_16",
            "start": 229,
            "end": 489,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_16@3",
            "content": "The reasons for choosing this data are that: (1) it is in-domain for the biomedical domain; (2) the texts are well-formed sentences with proper grammatical structure; and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_16",
            "start": 491,
            "end": 660,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_17@0",
            "content": "(3) the texts contain many negations, especially in the inclusion/exclusion criteria sections.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_17",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_17@1",
            "content": "For the general domain, we apply this approach to wikitext (Merity et al., 2016), a set of verified articles in Wikipedia.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_17",
            "start": 95,
            "end": 216,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_17@2",
            "content": "We sample the data equally from these two sets, obtaining 1, 381, 948 negation sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_17",
            "start": 218,
            "end": 306,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_18@0",
            "content": "Negation-focused pre-training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_18",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_19@0",
            "content": "Adaptive pre-training on target domain data has been shown to be an effective strategy for domain adaptation (Gururangan et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_19",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_19@1",
            "content": "We therefore hypothesize that pre-training language models on text with negations will help the model incorporate information about negation, and learn better representations for sentences containing negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_19",
            "start": 136,
            "end": 344,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_19@2",
            "content": "Using the negation-focused data, we first apply the standard random word masking strategy (Devlin et al., 2019) and train the model with the masked language model objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_19",
            "start": 346,
            "end": 518,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_20@0",
            "content": "As part of the collection of the negation-focused data, we obtain predictions of negation cues in all the sentences, which can be explicitly incorporated to make the model more robust to negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_20",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_20@1",
            "content": "Inspired by work on entity and span masking (Joshi et al., 2020;Yamada et al., 2020), we explore explicitly incorporating information about negation cues into the model by masking these cues, and targeting prediction of the masked cue in the pretraining stage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_20",
            "start": 197,
            "end": 456,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_20@2",
            "content": "Below is an example of how a sentence is tokenized under our masking scheme:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_20",
            "start": 458,
            "end": 533,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_21@0",
            "content": "No serious complications such as hypertension, diabetes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_21",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_21@1",
            "content": "\u21d2 [CUE] serious complications such as [MASK], diabetes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_21",
            "start": 57,
            "end": 111,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_22@0",
            "content": "A new type of token [CUE] is introduced under this masking scheme, and the model needs to reconstruct the original sentence by predicting both the [CUE] token to be No, and the randomlymasked token [MASK] to be hypertension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_22",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_22@1",
            "content": "By always masking negation cues in all the sentences, we force the model to focus more on this type of token, and thus, aim to learn better embeddings incorporating information of how a negation cue is represented in the context of the sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_22",
            "start": 225,
            "end": 469,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_22@2",
            "content": "Moreover, by using a different token to mask negation cues, we ensure that the model learns to distinguish between different types of tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_22",
            "start": 471,
            "end": 611,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_22@3",
            "content": "In this work, we replace the BERT encoder of NegBERT with RoBERTa and apply whole-word masking, meaning that all the sub-word tokens that constitute a word will be masked.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_22",
            "start": 613,
            "end": 783,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_23@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_23",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_24@0",
            "content": "Experimental settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_24",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@0",
            "content": "Following the experimental settings in NegBERT, we use the three standard benchmarks for negation cue detection and scope resolution tasks, i.e. BioScope (Vincze et al., 2008) (separated into two subsets, sourced from abstracts and full-text papers, resp.), the SFU product reviews dataset (Konstantinova et al., 2012), and the Sherlock dataset (Morante and Blanco, 2012).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 0,
            "end": 371,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@1",
            "content": "In addition, we use the negation-annotated subset of VetCompass UK 3 (Cheng et al., 2017), consisting of clinical notes in the veterinary domain, which are very informal compared to BioScope.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 373,
            "end": 563,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@2",
            "content": "It also contains abbreviations and shortening of terms, as well as certain unique negation cues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 565,
            "end": 660,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@3",
            "content": "To investigate cross-domain performance, we perform cue detection and scope resolution for all 4 datasets, based on training on one dataset and evaluating on all datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 662,
            "end": 832,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@4",
            "content": "Detailed statistics of these datasets are presented in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 834,
            "end": 896,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@5",
            "content": "Note that we do not experiment with the setting of training with all the combined training data from the corpora as it has been pointed out by previous work that doing so hurts the performance of the models (Jim\u00e9nez-Zafra et al., 2020;Barnes et al., 2021) due to differences in annotation schemes between the corpora introducing noise during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 898,
            "end": 1248,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@6",
            "content": "Re-annotating all the datasets using a common annotation scheme would be a potential solution here, We formulate the two tasks as sequence labeling problems, where each token is tagged with a corresponding label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 1250,
            "end": 1461,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@7",
            "content": "For cue detection, we use the annotation scheme {0: Affix, 1: Normal Cue, 2: Part of multiword cue, 3: Not part of cue}. For scope resolution, we use gold cue information and two labels {0: Outside negation scope, 1: Part of negation scope}. We adopt the same hyperparameters as NegBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 1463,
            "end": 1749,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@8",
            "content": "Following the standard evaluation scheme in previous negation detection works, all systems are evaluated using token-level F 1 -score, based on whether it is inside or outside of any negation cue or scope.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 1751,
            "end": 1955,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@9",
            "content": "Methods evaluated include: (1) NegBERT; (2) AugNB = NegBERT plus pre-training on negation-focused data; and (3) CueNB = NegBERT plus pre-training on negationfocused data and the negation cue masking objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 1957,
            "end": 2165,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_25@10",
            "content": "Note that for NegBERT, we also replace the BERT encoder with RoBERTa to ensure results are comparable between the models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_25",
            "start": 2167,
            "end": 2287,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_26@0",
            "content": "Main results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_26",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_27@0",
            "content": "Tables 2 and 3 report the performance of negation cue detection and negation scope resolution, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_27",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_27@1",
            "content": "Results reported are the average of 5 runs with different random seeds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_27",
            "start": 109,
            "end": 179,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_27@2",
            "content": "NegBERT results are produced using the official implementation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_27",
            "start": 181,
            "end": 243,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_27@3",
            "content": "4 To provide a more general view, we summarize the results in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_27",
            "start": 245,
            "end": 314,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_27@4",
            "content": "In general, we observe gains in both the same-dataset setting (training and test set belongs to one corpus) and cross-dataset setting (training one one training set and testing on all others test sets) for both of the proposed models, with CueNB achieving the largest gains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_27",
            "start": 316,
            "end": 589,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_28@0",
            "content": "We observe similar trends across all datasets for both cue detection and scope resolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_28",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_28@1",
            "content": "Regarding the in-dataset setting, AugNB outperforms the baseline NegBERT on all datasets except for Sherlock.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_28",
            "start": 91,
            "end": 199,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_28@2",
            "content": "Gains are more noticeable over the biomedical datasets (BioScope, VetCompass).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_28",
            "start": 201,
            "end": 278,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_28@3",
            "content": "For Sherlock, however, we observe a slight degradation in performance with the proposed pre-training scheme.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_28",
            "start": 280,
            "end": 387,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_28@4",
            "content": "This is likely due to the fact that Sherlock has major differences in annotation scheme compared to other corpora, specifically including scopes to the left of cues, while in BioScope and SFU, the scope is usually annotated only to the right of cues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_28",
            "start": 389,
            "end": 638,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_28@5",
            "content": "Also, the cue itself is not considered to be part of the scope in Sherlock or SFU, unlike in BioScope.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_28",
            "start": 640,
            "end": 741,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_29@0",
            "content": "In the cross-dataset setting, we record gains across all benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_29",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_29@1",
            "content": "The largest cross-dataset improvements over NegBERT are for SFU, perhaps due to SFU being the largest dataset in size, containing a relatively large number of unique cues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_29",
            "start": 69,
            "end": 239,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_29@2",
            "content": "CueNB further improves the performance of AugNB, confirming our hypothesis that explicitly masking the cue helps the model learn better representations for negation cues and thus, better distinguish between cues and normal words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_29",
            "start": 241,
            "end": 469,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_29@3",
            "content": "These results show that our negation-focused pre-training strategy is effective for improving the transfer learning performance of pre-trained language models on the negation detection task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_29",
            "start": 471,
            "end": 660,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_30@0",
            "content": "Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_30",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@0",
            "content": "We conducted an error analysis on the VetCompass validation set to see what qualitative improvement CueNB makes over NegBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@1",
            "content": "For cue detection, there are two main types of errors that CueNB helps alleviate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 126,
            "end": 206,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@2",
            "content": "First, CueNB can detect more unique cues such as negative, won't, and also multiword cues like no longer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 208,
            "end": 312,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@3",
            "content": "Second, CueNB is able to recognize cases when the negations are actually just speculative.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 314,
            "end": 403,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@4",
            "content": "For example, in the sentence O reports has smelled for past week, not sure if anal glands . . . , the word not is part of the speculation phrase not sure, indicating that this is not truly a negation phrase but rather expresses uncertainty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 405,
            "end": 644,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@5",
            "content": "For scope resolution, CueNB mostly helps in recognizing the correct scope boundary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 646,
            "end": 728,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@6",
            "content": "One common case is when the cue relates to multiple spans in a sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 730,
            "end": 801,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@7",
            "content": "In the sentence Examination: QAR, thorac ausc and abdo palp NAD, 5 NegBERT only recognizes the nearest span abdo palp NAD to be the scope, whereas CueNB recognizes the full correct span thorac ausc and abdo palp NAD.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 803,
            "end": 1018,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@8",
            "content": "It also helps in cases where there are multiple separate negations in the same sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 1020,
            "end": 1107,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@9",
            "content": "For instance, in the We also conduct an ablation study to understand the impact of each component of the proposed pretraining strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 1109,
            "end": 1243,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@10",
            "content": "Table 5 presents the results of different variations of the proposed pre-training scheme on the BioScope-Abstract validation split.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 1245,
            "end": 1375,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@11",
            "content": "We consider two variations, pre-training with: (1) only the negation-focused data (equivalent to the AugNB model); and (2) only the cue masking objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 1377,
            "end": 1530,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@12",
            "content": "To model the latter variation, we explicitly mask the cue in the BioScope training set, then pretrain on this training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 1532,
            "end": 1654,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@13",
            "content": "From the results, we see that both strategies help improve the baseline Neg-BERT on cue detection and scope resolution, with explicitly masking the cues being the most important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 1656,
            "end": 1833,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_31@14",
            "content": "Combining both strategies (CueNB) further improves the overall results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_31",
            "start": 1835,
            "end": 1905,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_32@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_32",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_33@0",
            "content": "In this work, we propose a new negation-focused pre-training strategy to explicitly incorporate negation information into pre-trained language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_33",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_33@1",
            "content": "Empirical results on common benchmarks show that the proposed strategy helps improve the performance of pre-trained language models on the negation detection task when evaluating on the same source dataset, as well as their transferability to target data in different domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_33",
            "start": 151,
            "end": 426,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_33@2",
            "content": "Despite the gains over previous methods, the sub-optimal results on some benchmarks show that negation remains a big challenge in NLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_33",
            "start": 428,
            "end": 561,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_34@0",
            "content": "Fran\u00e7ois-Michel Alan R Aronson,  Lang, An overview of MetaMap: Historical perspective and recent advances, 2010, Journal of the American Medical Informatics Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_34",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_35@0",
            "content": "Jeremy Barnes, Erik Velldal, Lilja \u00d8vrelid, Improving sentiment analysis with multi-task learning of negation, 2021, Natural Language Engineering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_35",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_36@0",
            "content": "Will Wendy W Chapman, Paul Bridewell,  Hanbury, F Gregory, Bruce G Cooper,  Buchanan, A simple algorithm for identifying negated findings and diseases in discharge summaries, 2001, Journal of biomedical informatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_36",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_37@0",
            "content": "Long Chen, Attention-based deep learning system for negation and assertion detection in clinical notes, 2019, International Journal of Artificial Intelligence and Applications (IJAIA), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_37",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_38@0",
            "content": "Katherine Cheng, Timothy Baldwin, Karin Verspoor, Automatic negation and speculation detection in veterinary clinical text, 2017, Proceedings of the Australasian Language Technology Association Workshop, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_38",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_39@0",
            "content": "P Noa, Maite Cruz, Ruslan Taboada,  Mitkov, A machine-learning approach to negation and speculation detection for sentiment analysis, 2016, Journal of the Association for Information Science and Technology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_39",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_40@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_40",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_41@0",
            "content": "Federico Fancellu, Adam Lopez, Bonnie Webber, Hangfeng He, Detecting negation scope is easy, except when it isn't, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_41",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_42@0",
            "content": "Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey,  Smith, Don't stop pretraining: Adapt language models to domains and tasks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_42",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_43@0",
            "content": "Venelin Md Mosharaf Hossain, Pranoy Kovatchev, Tiffany Dutta, Elizabeth Kao, Eduardo Wei,  Blanco, An analysis of natural language inference benchmarks through the lens of negation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_43",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_44@0",
            "content": "Roser Salud Mar\u00eda Jim\u00e9nez-Zafra, Mar\u00eda Morante, L Teresa Mart\u00edn-Valdivia,  Ure\u00f1a-L\u00f3pez, Corpora annotated with negation: An overview, 2020, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_44",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_45@0",
            "content": "Mandar Joshi, Danqi Chen, Yinhan Liu, S Daniel, Luke Weld, Omer Zettlemoyer,  Levy, Span-BERT: Improving pre-training by representing and predicting spans, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_45",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_46@0",
            "content": "Aditya Khandelwal, Suraj Sawant, NegBERT: A transfer learning approach for negation detection and scope resolution, 2020, Proceedings of the 12th Language Resources and Evaluation Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_46",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_47@0",
            "content": "Natalia Konstantinova, C Sheila, Noa De Sousa, Manuel Cruz, Maite Ma\u00f1a, Ruslan Taboada,  Mitkov, A review corpus annotated for negation, speculation and their scope, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_47",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_48@0",
            "content": "UNKNOWN, None, 2019, Negation scope detection with recurrent neural networks models in review texts. International Journal of High Performance Computing and Networking, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_48",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_49@0",
            "content": "UNKNOWN, None, 2019, RoBERTa: A robustly optimized BERT pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_49",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_50@0",
            "content": "Saeed Mehrabi, Anand Krishnan, Sunghwan Sohn, Alexandra Roch, Heidi Schmidt, Joe Kesterson, Chris Beesley, Paul Dexter, Max Schmidt, Hongfang Liu, DEEPEN: A negation detection system for clinical text incorporating dependency relation into NegEx, 2015, Journal of Biomedical Informatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_50",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_51@0",
            "content": "UNKNOWN, None, 2016, Pointer sentinel mixture models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_51",
            "start": 0,
            "end": 54,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_52@0",
            "content": ", Descriptive analysis of negation cues in biomedical texts, 2010, Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC'10), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_52",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_53@0",
            "content": "Roser Morante, Eduardo Blanco, *SEM 2012 shared task: Resolving the scope and focus of negation, 2012, *SEM 2012: The First Joint Conference on Lexical and Computational Semantics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_53",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_54@0",
            "content": "Roser Morante, Walter Daelemans, A metalearning approach to processing the scope of negation, 2009, Proceedings of the Thirteenth Conference on Computational Natural Language Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_54",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_55@0",
            "content": "Ying Ou, Jon Patrick, Automatic negation detection in narrative pathology reports, 2015, Artificial Intelligence in Medicine, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_55",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_56@0",
            "content": "Yifan Peng, Xiaosong Wang, Le Lu, Mohammadhadi Bagheri, Ronald Summers, Zhiyong Lu, NegBio: a high-performance tool for negation and uncertainty detection in radiology reports, 2018, AMIA Summits on Translational Science Proceedings, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_56",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_57@0",
            "content": "Tongshuang Marco Tulio Ribeiro, Carlos Wu, Sameer Guestrin,  Singh, Beyond accuracy: Behavioral testing of NLP models with checklist, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_57",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_58@0",
            "content": "Veronika Vincze, Gy\u00f6rgy Szarvas, Rich\u00e1rd Farkas, Gy\u00f6rgy M\u00f3ra, J\u00e1nos Csirik, The Bio-Scope corpus: biomedical texts annotated for uncertainty, negation and their scopes, 2008, BMC bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_58",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "413-ARR_v2_59@0",
            "content": "Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto, LUKE: Deep contextualized entity representations with entityaware self-attention, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "413-ARR_v2_59",
            "start": 0,
            "end": 259,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "413-ARR_v2_0",
            "tgt_ix": "413-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_0",
            "tgt_ix": "413-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_1",
            "tgt_ix": "413-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_1",
            "tgt_ix": "413-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_0",
            "tgt_ix": "413-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_2",
            "tgt_ix": "413-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_4",
            "tgt_ix": "413-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_5",
            "tgt_ix": "413-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_6",
            "tgt_ix": "413-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_7",
            "tgt_ix": "413-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_3",
            "tgt_ix": "413-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_3",
            "tgt_ix": "413-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_3",
            "tgt_ix": "413-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_3",
            "tgt_ix": "413-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_3",
            "tgt_ix": "413-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_3",
            "tgt_ix": "413-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_0",
            "tgt_ix": "413-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_10",
            "tgt_ix": "413-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_9",
            "tgt_ix": "413-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_9",
            "tgt_ix": "413-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_9",
            "tgt_ix": "413-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_0",
            "tgt_ix": "413-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_11",
            "tgt_ix": "413-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_12",
            "tgt_ix": "413-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_12",
            "tgt_ix": "413-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_12",
            "tgt_ix": "413-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_13",
            "tgt_ix": "413-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_15",
            "tgt_ix": "413-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_16",
            "tgt_ix": "413-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_14",
            "tgt_ix": "413-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_14",
            "tgt_ix": "413-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_14",
            "tgt_ix": "413-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_14",
            "tgt_ix": "413-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_12",
            "tgt_ix": "413-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_17",
            "tgt_ix": "413-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_19",
            "tgt_ix": "413-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_20",
            "tgt_ix": "413-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_21",
            "tgt_ix": "413-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_18",
            "tgt_ix": "413-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_18",
            "tgt_ix": "413-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_18",
            "tgt_ix": "413-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_18",
            "tgt_ix": "413-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_18",
            "tgt_ix": "413-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_0",
            "tgt_ix": "413-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_22",
            "tgt_ix": "413-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_23",
            "tgt_ix": "413-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_23",
            "tgt_ix": "413-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_24",
            "tgt_ix": "413-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_24",
            "tgt_ix": "413-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_23",
            "tgt_ix": "413-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_27",
            "tgt_ix": "413-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_28",
            "tgt_ix": "413-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_26",
            "tgt_ix": "413-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_26",
            "tgt_ix": "413-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_26",
            "tgt_ix": "413-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_26",
            "tgt_ix": "413-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_23",
            "tgt_ix": "413-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_29",
            "tgt_ix": "413-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_30",
            "tgt_ix": "413-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_30",
            "tgt_ix": "413-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_0",
            "tgt_ix": "413-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_32",
            "tgt_ix": "413-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_32",
            "tgt_ix": "413-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "413-ARR_v2_0",
            "tgt_ix": "413-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_1",
            "tgt_ix": "413-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_2",
            "tgt_ix": "413-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_2",
            "tgt_ix": "413-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_2",
            "tgt_ix": "413-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_2",
            "tgt_ix": "413-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_3",
            "tgt_ix": "413-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_4",
            "tgt_ix": "413-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_4",
            "tgt_ix": "413-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_4",
            "tgt_ix": "413-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_4",
            "tgt_ix": "413-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_4",
            "tgt_ix": "413-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_4",
            "tgt_ix": "413-ARR_v2_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_5",
            "tgt_ix": "413-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_5",
            "tgt_ix": "413-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_6",
            "tgt_ix": "413-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_6",
            "tgt_ix": "413-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_7",
            "tgt_ix": "413-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_7",
            "tgt_ix": "413-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_8",
            "tgt_ix": "413-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_9",
            "tgt_ix": "413-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_10",
            "tgt_ix": "413-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_10",
            "tgt_ix": "413-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_10",
            "tgt_ix": "413-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_10",
            "tgt_ix": "413-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_11",
            "tgt_ix": "413-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_11",
            "tgt_ix": "413-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_11",
            "tgt_ix": "413-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_11",
            "tgt_ix": "413-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_11",
            "tgt_ix": "413-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_11",
            "tgt_ix": "413-ARR_v2_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_11",
            "tgt_ix": "413-ARR_v2_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_12",
            "tgt_ix": "413-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_13",
            "tgt_ix": "413-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_14",
            "tgt_ix": "413-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_15",
            "tgt_ix": "413-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_15",
            "tgt_ix": "413-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_16",
            "tgt_ix": "413-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_16",
            "tgt_ix": "413-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_16",
            "tgt_ix": "413-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_16",
            "tgt_ix": "413-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_17",
            "tgt_ix": "413-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_17",
            "tgt_ix": "413-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_17",
            "tgt_ix": "413-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_18",
            "tgt_ix": "413-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_19",
            "tgt_ix": "413-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_19",
            "tgt_ix": "413-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_19",
            "tgt_ix": "413-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_20",
            "tgt_ix": "413-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_20",
            "tgt_ix": "413-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_20",
            "tgt_ix": "413-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_21",
            "tgt_ix": "413-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_21",
            "tgt_ix": "413-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_22",
            "tgt_ix": "413-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_22",
            "tgt_ix": "413-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_22",
            "tgt_ix": "413-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_22",
            "tgt_ix": "413-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_23",
            "tgt_ix": "413-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_24",
            "tgt_ix": "413-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_25",
            "tgt_ix": "413-ARR_v2_25@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_26",
            "tgt_ix": "413-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_27",
            "tgt_ix": "413-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_27",
            "tgt_ix": "413-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_27",
            "tgt_ix": "413-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_27",
            "tgt_ix": "413-ARR_v2_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_27",
            "tgt_ix": "413-ARR_v2_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_28",
            "tgt_ix": "413-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_28",
            "tgt_ix": "413-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_28",
            "tgt_ix": "413-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_28",
            "tgt_ix": "413-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_28",
            "tgt_ix": "413-ARR_v2_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_28",
            "tgt_ix": "413-ARR_v2_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_29",
            "tgt_ix": "413-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_29",
            "tgt_ix": "413-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_29",
            "tgt_ix": "413-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_29",
            "tgt_ix": "413-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_30",
            "tgt_ix": "413-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_31",
            "tgt_ix": "413-ARR_v2_31@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_32",
            "tgt_ix": "413-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_33",
            "tgt_ix": "413-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_33",
            "tgt_ix": "413-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_33",
            "tgt_ix": "413-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_34",
            "tgt_ix": "413-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_35",
            "tgt_ix": "413-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_36",
            "tgt_ix": "413-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_37",
            "tgt_ix": "413-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_38",
            "tgt_ix": "413-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_39",
            "tgt_ix": "413-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_40",
            "tgt_ix": "413-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_41",
            "tgt_ix": "413-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_42",
            "tgt_ix": "413-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_43",
            "tgt_ix": "413-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_44",
            "tgt_ix": "413-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_45",
            "tgt_ix": "413-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_46",
            "tgt_ix": "413-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_47",
            "tgt_ix": "413-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_48",
            "tgt_ix": "413-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_49",
            "tgt_ix": "413-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_50",
            "tgt_ix": "413-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_51",
            "tgt_ix": "413-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_52",
            "tgt_ix": "413-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_53",
            "tgt_ix": "413-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_54",
            "tgt_ix": "413-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_55",
            "tgt_ix": "413-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_56",
            "tgt_ix": "413-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_57",
            "tgt_ix": "413-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_58",
            "tgt_ix": "413-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "413-ARR_v2_59",
            "tgt_ix": "413-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 413,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "413-ARR",
        "version": 2
    }
}