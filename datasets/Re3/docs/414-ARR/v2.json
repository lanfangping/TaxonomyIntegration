{
    "nodes": [
        {
            "ix": "414-ARR_v2_0",
            "content": "On Synthetic Data for Back Translation *",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_2",
            "content": "Back translation (BT) is one of the most significant technologies in NMT research fields. Existing attempts on BT share a common characteristic: they employ either beam search or random sampling to generate synthetic data with a backward model but seldom work studies the role of synthetic data in the performance of BT. This motivates us to ask a fundamental question: what kind of synthetic data contributes to BT performance? Through both theoretical and empirical studies, we identify two key factors on synthetic data controlling the back-translation NMT performance, which are quality and importance. Furthermore, based on our findings, we propose a simple yet effective method to generate synthetic data to better trade off both factors so as to yield a better performance for BT. We run extensive experiments on WMT14 DE-EN, EN-DE, and RU-EN benchmark tasks. By employing our proposed method to generate synthetic data, our BT model significantly outperforms the standard BT baselines (i.e., beam and sampling based methods for data generation), which proves the effectiveness of our proposed methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "414-ARR_v2_4",
            "content": "Since the birth of neural machine translation (NMT) (Bahdanau et al., 2014;Sutskever et al., 2014) back translation (BT) (Sennrich et al., 2016a) has quickly become one of the most significant technologies in natural language processing (NLP) research field. This is because 1) it provides a simple yet effective approach to advance the supervised NMT by leveraging monolingual data (Edunov et al., 2018) and it also serves as a key learning objective in unsupervised NMT (Artetxe et al., 2018;Lample et al., 2018); 2) back translation even plays a significant role in other NLP re-search fields beyond translation such as paraphrasing (Mallinson et al., 2017) and style transfer (Prabhumoye et al., 2018;Zhang et al., 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_5",
            "content": "Back translation consists of two steps, namely synthetic corpus generation with a backward model and parameter optimization for the forward model. Various contributions have been made on improving back translation, for instance, iterative back translation (Hoang et al., 2018), tagged back translation (Caswell et al., 2019), confidence weighting , data diversification (Nguyen et al., 2020). Although these efforts differ in some aspects, all of them share a common characteristic: they employ a default way to generate synthetic data in the first step of BT which is either beam search or random sampling with a backward model. Seldom work studies the consequences of synthetic corpus to back translation and hence it is unclear how synthetic data influences the final performance of BT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_6",
            "content": "The early study empirically suggests the quality of the synthetic corpus is vital for BT performance (Sennrich et al., 2016a). However, recent studies illustrate better test performance can be achieved by low quality synthetic corpus (Edunov et al., 2018). This contradictory observation indicates the quality of synthetic data is not the only element that affects the BT performance. Hence, this fact naturally raises a fundamental question: what kind of synthetic data contributes to back translation performance?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_7",
            "content": "In this paper, we attempt to take a step forward toward the above fundamental question. To this end, we start from a critical objective in semi-supervised learning, which is defined by the marginal distribution of a target language. Then we derive an approximate lower bound of the objective function, which is closely related to the objective of back translation. Corresponding to this lower bound, we theoretically find two related elements for maximizing such a lower bound: quality of synthetic bilingual data and importance weight of its source. Since both elements are mutually exclusive to some extent, it may induce contradictory observation if one judges the BT performance according to a single element. In addition, such a theoretical explanation is supported by our empirical experiments. Furthermore, based on our findings, we propose a new heuristic approach to generate synthetic data whose both elements are better balanced so as to yield improvements over both sampling and beam search based methods. Extensive experiments on three WMT14 tasks show that our BT consistently outperforms the standard sampling and beam search based baselines by a significant margin.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_8",
            "content": "Our contributions are three folds:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_9",
            "content": "1. We point out that importance weight and quality of synthetic candidates are two key factors that affect the NMT performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_10",
            "content": "2. We propose a simple yet effective method for synthetic corpus generation, which could better balance the quality and importance of synthetic data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_11",
            "content": "3. Our experiments prove the effectiveness of the aforementioned strategy, it outperforms beam or sampling decoding methods on three benchmark tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_12",
            "content": "Revisiting Back Translation",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "414-ARR_v2_13",
            "content": "NMT builds a probabilistic model p(y|x; \u03b8) with neural networks parameterized by \u03b8, which is used to translate a sentence x in source language X to a sentence y in target language Y. The standard wisdom to train the model is to minimize the following objective function over a given bilingual corpus B = {(x i , y i )}:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_14",
            "content": "\u2113(B; \u03b8) = (x i ,y i )\u2208B log p(y i |x i ; \u03b8)(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_15",
            "content": "Recently Sennrich et al. (2016a) propose a remarkable method called Back Translation (BT) to improve NMT by using a monolingual corpus M in target language Y besides B and back translation becomes one of the most successful techniques in NMT (Fadaee and Monz, 2018;Edunov et al., 2018). At a high level, back translation can be considered as a semi-supervised method because it leverages both labeled and unlabeled data. Suppose p(x|y; \u03c0) is the backward translation model whose parameter \u03c0 is optimized over B, the key idea of back translation can be summarized as the following two steps:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_16",
            "content": "To make BT more efficient, the standard configuration is widely adopted: each sentence y is required to generate a single source x and both two steps are performed for a single pass. We follow this standard in this paper for generality but our idea in this paper is straightforward to apply to other configurations such as (Gra\u00e7a et al., 2019;Hoang et al., 2018;Nguyen et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_17",
            "content": "In the first step, there are two main strategies to generate the synthetic corpus, i.e., deterministically decoding and randomly sampling with p(x|y; \u03c0). The first strategy aims to search the best candidate as follows, xb = arg max p(x|y; \u03c0)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_18",
            "content": "(3)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_19",
            "content": "The above optimization is achieved by the beam search decoding, which can be regarded as a degenerated shortest path problem with respect to the log p(x|y; \u03c0) with limited routing attempts. The alternative strategy is random sampling: it randomly samples a token with respect to the distribution estimated by a back-translation model at each decoding step. Such a process can be modelled by,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_20",
            "content": "xs = rand{p(x|y; \u03c0)} (4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_21",
            "content": "Research Question Prior work points out (Sennrich et al., 2016a) that the synthetic corpus with high quality is beneficial to the final performance of back translation. However, the recent studies (Edunov et al., 2018) find that NMT models with unsatisfactory BLEU score corpus, for instance, the corpus generated by sampling based strategy, also establish the state-of-the-art (SOTA) achievement among back-translation NMT models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_22",
            "content": "This contradictory fact indicates that the quality of synthetic corpus is not the sole element for back translation. This motivates us to study a fundamental question for back translation: what kind of synthetic corpus is beneficial to back translation?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_23",
            "content": "Understanding Synthetic Data by Two Factors",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "414-ARR_v2_24",
            "content": "To answer the fundamental question presented in the previous section, we first start from the marginal likelihood objective defined on the target language Y, and then we theoretically explain two factors (i.e., quality and importance) that are highly related to the training objective of back translation. Finally, we empirically explain why synthetic corpus with low quality may lead to better performance than synthetic corpus with high quality by measuring both factors.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_25",
            "content": "Theoretical Explanation",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "414-ARR_v2_26",
            "content": "Maximizing marginal likelihood is an important principle to leverage unlabeled data. Therefore, we rethink back translation from this principle because it makes use of target monolingual corpus M. For each y \u2208 M, the marginal likelihood objective can be derived by the Bayesian Equation ( 5 7is the same as the second term in BT loss (i.e., log p(y|x) in Eq. 2), and the unique difference is the multiplicative term called importance weight:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_27",
            "content": "Imp(x; y) = p(x) p(x|y)(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_28",
            "content": "The denominator is the candidate conditional probability to target, and the numerator is the candidate distribution on source language distribution. Since Imp(x; y) is constant with respect to the parameter \u03b8, maximizing log p(y|x; \u03b8) in BT loss implicitly maximizes Imp(x; y) log p(y|x), which indicates that back translation aims to implicitly maximize the marginal likelihood objective. More importantly, according to Equation 7we can find that the following two factors are critical to influence the marginal likelihood log p(y; \u03b8):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_29",
            "content": "\u2022 Factor 1: The quality of x as a translation of y corresponding to the log p(y|x; \u03b8) in Eq. 7. \u2022 Factor 2: The importance of x as a translation of y corresponding to Imp(x; y) in Eq. 7.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_30",
            "content": "Theoretically, if x is of higher quality and contains more semantic information in y, p(y|x; \u03b8) would be higher and thus it would lead to a higher log p(y; \u03b8), which is well acknowledged by prior work (Sennrich et al., 2016a;.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_31",
            "content": "In particular, if x is with higher importance weight, maximizing log p(y|x; \u03b8) is more helpful to maximize log p(y; \u03b8). On the contrary, if Imp(x; y) is very small, it needs to avoid such a sample x from p(x|y), which is essentially the rejection control strategy in importance sampling theory (Liu et al., 1998;Liu and Liu, 2001). Unfortunately, in practice, both factors are mutually exclusive to some extent: if x is with high quality, p(x|y; \u03b8) would be higher as well leading to lower importance weight. This fact can explain the contradictory observation in Sec 2 that BT with high-quality synthetic data sometimes leads to better testing performance, while it may deliver worse performance at other times, which will be later justified in Sec 3.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_32",
            "content": "Estimating Two Factors To measure the quality of x for each y, it is natural to use the evaluation metric such as BLEU if the reference translation x of y is available. Otherwise, as a surrogate, we use the log likelihood log p(x|y; \u03c0) of the backward translation model \u03c0 which is trained on the authentic data B. Similarly, in order to estimate the importance of x, we train an additional language model p(x; \u03c9) with GPT (Radford et al., 2018)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_33",
            "content": "Empirical Justification",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "414-ARR_v2_34",
            "content": "In this subsection, we aim to justify the following statements: 1) encouraging the quality of synthetic corpus may to some extent hurt the performance of BT due to the decrease of importance; 2) judging the testing performance in terms of quality only may be dangerous while it would be meaningful to judge the testing performance by taking into account both factors rather than either factor. To this end, we run some quick experiments on WMT14 datasets whose settings will be shown in Sec 5 later. We set up two back translation systems with two different options (i.e., beam search and sampling) to generate synthetic corpus by using the best checkpoint of p(x|y; \u03c0) tuned on the development set. Both beam search and sampling based BT systems are denoted by beam and sampling. In addition, we pick another checkpoint of p(x|y; \u03c0) which is trained for only 1 epoch, and we use this weak checkpoint to set up another beam search based BT system, which is denoted as beam*. Table 1 shows BLEU on test dataset, the quality and importance on the development set according to three systems on WMT14 DE-EN task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_35",
            "content": "In Table 1, beam is better than sampling in the quality of synthetic corpus but its testing performance is worse. This is meaningful because the former relies on the synthetic corpus with lower importance weight according to our theoretical ex-planation. In addition, when comparing beam with beam*, we can find that beam delivers better testing performance because its quality is better meanwhile its importance weight is almost similar to that of beam*. Table 2 consistently demonstrates that it is meaningless to take into account quality only when evaluating BT. These facts justify our statements and provide an answer to the fundamental question in section 2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_36",
            "content": "Improving Synthetic Data for BT",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "414-ARR_v2_37",
            "content": "As shown in the previous section, both importance and quality of synthetic corpus are beneficial to the overall testing performance of back translation. It is a natural idea to promote both factors when generating synthetic corpus such that running BT on such corpus leads to better testing performance. However, this is difficult because both factors are mutually exclusive as discussed in Section 3. In this section, we instead propose two methods (namely data manipulation and gamma score) to trade off both factors in the hope to yield better BT performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_38",
            "content": "Data Manipulation",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "414-ARR_v2_39",
            "content": "Since the synthetic data in sampling based BT is of high importance yet low quality whereas the case for the synthetic data in beam search based BT is opposite, we propose a data manipulation method to trade off importance and quality by combining both synthetic datasets. Through balancing the ratio between beam and sampling based synthetic corpora, we expect to find an optimized beam/sampling ratio to further improve NMT model performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_40",
            "content": "Specifically, we randomly shuffle M and divide it into two parts with the first part accounting for \u03b3 (0 < \u03b3 < 1); then we generate translations for the first part with beam search while generating translations for the second part with sampling. Formally, we use the following corpus M c as the synthetic corpus for BT:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_41",
            "content": "M c = {(x b i , y i ) k i=0 } \u222a {(x s j , y j ) |M| j=k } k =\u230a\u03b3|M|\u230b",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_42",
            "content": "Where xb denotes a translation of y generated by p(x|y; \u03c0) with beam search and xs is a translation with sampling, | \u2022 | means the size of the corpus, and \u03b3 is the combination ratio for beam and sampling synthetic corpora. By tuning \u03b3 here, one can modify the weightage for the number of beam and sampling sentences, to improve back-translation performance by training models on a combined synthetic corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_43",
            "content": "Although this method is easy to implement, its limitation is obvious. Since each x is either from beam search or from sampling, the quality of M c is generally worse than that of beam search and its importance weight is generally worse than that of sampling. Consequently, we propose an alternative method in the next part of this section.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_44",
            "content": "Gamma Score",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "414-ARR_v2_45",
            "content": "The key idea to the alternative method is that it employs a score that balances both quality and importance to generate a translation x for each y \u2208 M. A natural choice of such a score is defined by the interpolation score as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_46",
            "content": "\u03b3 log Imp(x; \u03c9, \u03c0) + (1 \u2212 \u03b3) log p(x|y; \u03c0)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_47",
            "content": "where \u03b3 is used to trade off both factors as in corpus manipulation. With the help of this score, one may optimize the x by beam search whose interpolation score is the best among all possible translations of y \u2208 M. Unfortunately, such an implementation leads to limited performance in our preliminary experiments, due to two major challenges.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_48",
            "content": "On one hand, the estimations of quality and importance weight of x are not well calibrated, and in particular, quality and importance are mutually exclusive as mentioned before. As a result, beam search with the interpolation score over the exponential space can not guarantee a desirable translation x for each y. On the other hand, quality and importance weight of x are not at the same scale for different y, it is difficult to balance both factors with a fixed \u03b3 in the interpolation score for different y.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_49",
            "content": "To alleviate these issues, we propose a simple method as follows. Specifically, firstly, instead of beam search with the interpolation score, we simply utilize the backward translation p(x|y; \u03c0) to randomly sample a set of candidate translations which is denoted by A(y) = {x i } N i (N = 50 in this paper as it works well). 1 Then we pick a xj among A(y) according to the balancing score. Secondly, for each x, we normalize the log values of importance and quality of each candidate by its 1 N -best decoding strategy with p(x|y; \u03c0) to generate N candidates may be another solution which remains as future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_50",
            "content": "sequence length, then normalize these values with respect to all N candidates as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_51",
            "content": "F(x i ) = log F(x i ) /len(x i ) \u2212 \u00b5 F \u03c3 F (9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_52",
            "content": "where F is either importance weight or quality estimations, and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_53",
            "content": "\u00b5 F = 1 N i log F(x i ) and \u03c3 F = i (log F (x i )\u2212\u00b5 F ) 2 N \u22121",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_54",
            "content": "are mean and variance of N sampled candidates with length normalized. Finally, the Gamma score is defined on the normalized values of importance and quality as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_55",
            "content": "\u0393(x i ; \u03c9, \u03c0) = exp \u03b3 \u0128mp(x i ; \u03c9, \u03c0) + (1 \u2212 \u03b3)p(x i |y, \u03c0) j exp \u03b3 \u0128mp(x j ; \u03c9, \u03c0) + (1 \u2212 \u03b3)p(x j |y, \u03c0)(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_56",
            "content": "where \u0128mp and p are the normalized log value of importance weight and backward translation model p(x|y, \u03c0) as defined in Equation 9.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_57",
            "content": "Once the gamma score in Equation 10 is computed, there are two methods to select x from A(y), which are deterministic and stochastic methods. For deterministic selection, we simply select the candidates with maximum gamma score among N translation candidates; and for sampling, we sample a candidate according to its gamma score distribution. These two methods are called gamma selection and gamma sampling in our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_58",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "414-ARR_v2_59",
            "content": "Settings",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "414-ARR_v2_60",
            "content": "We run all the experiments by using fairseq (Ott et al., 2019) framework. For dataset settings, since datasets WMT14 EN-DE and DE-EN are widely used (Li et al., 2019b;Zhu et al., 2020;Fan et al., 2021;Le et al., 2021), we follow both standard benchmarks and additionally we employ WMT14 RU-EN as the third dataset to validate the effectiveness of the proposed methods. For back translation experiment, we use an equal scale monolingual corpus randomly sampled from Newscrawl 2020 (Barrault et al., 2019) comprising 4.5 million monolingual sentences for DE-EN language pair and 2.5 million for RU-EN direction, thus total 9 million sentences for DE-EN pair and 5 million for RU-EN direction are used. We tokenize the parallel corpus using Mose tokenizer (Koehn et al., 2007), and learn a source and target shared Byte-Pair-Encoding (BPE) (Sennrich et al., 2016b with 32K types. We develop on newstest2013 and report the results on newstest2014.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_61",
            "content": "As for model architecture, we employ all the translation models using architecture transformer_wmt_en_de_big, which is a Big Transformer architecture with 6 blocks in the encoder and decoder, and is widely used as a standard backbone on various NMT research studies. We use the same hyperparameter settings across all the experiments, i.e., 1024 word representation size, 4096 inner dimensions of feed-forward layers, and dropout is set to 0.3 for all the experiments. In addition, for monolingual models, we apply transformer_lm_gpt architecture (Radford et al., 2018) on source language side of the corpus without any extra corpus. 2 The detailed hyperparameters used for training translation and language models are shown in Appendix.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_62",
            "content": "For baseline models, we train them for 400K updating steps, and train the models with backtranslation data for 1.6M updating steps. We save the checkpoints every 100k updating intervals, and only select the checkpoints with highest develop set performance. As for the back-translation data, we study beam decoding and sampling decoding as baselines since they are the common practice for BT research (Roberts et al., 2020;. We use baseline models' checkpoints at 400K updating steps to generate default beam5 decoding and sampling decoding synthetic corpus without any penalty. For monolingual models, we only select the checkpoints with the best develop set performance. When tuning \u03b3 on dev sets for data manipulation methods we select it from {0, 1/4, 1/2, 3/4, 1} and the optimal is \u03b3 = 1/2. For the Gamma Score method, \u03b3 is tuned among {0.1, 0.2, 0.3, 0.4, 0.5} and it is set \u03b3 = 0.2 for all three tasks. All the experiments are conducted using 8 Nvidia V100-32GB graphic cards without any gradient accumulation or bitext upsampling, and the results in this paper are measured in case-sensitive detokenized BLEU with SacreBLEU 3 by Post (2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_63",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "414-ARR_v2_64",
            "content": "Results on DE-EN Data Manipulation",
            "ntype": "title",
            "meta": {
                "section": "5.2.1"
            }
        },
        {
            "ix": "414-ARR_v2_65",
            "content": "We conduct two experiments to study the data manipulation for backtranslation NMT model performance using aforementioned corpus with and without authentic corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_66",
            "content": "Table 3 show the data manipulation results compared with baseline. Firstly, for synthetic corpus experiment, we find that even if only monolingual corpus is used, the performance of back-translation NMT model can still be significantly improved to 31.3 from 29.2 by sampling or 27.6 by beam, and it is only 0.7 lower than bitext baseline by BLEU score measure. Secondly, for the experiments with bitext, the best performance by data manipulation only helps the back-translation NMT model achieves almost the same performance with sampling BT. This means data manipulation methods cannot achieve a higher BLEU score than sampling or beam.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_67",
            "content": "Gamma Score In this paragraph, we conduct the experiments based on gamma score method. We conduct both of the methods in this experiment: we select the candidate with highest gamma score for the deterministic method whereas sample the candidate by gamma score distribution for the stochastic method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_68",
            "content": "Once again, we use synthetic gamma corpus combined with bitext to train the back-translation NMT models on each corpus, the results are listed in 4. From the table, we can see that our proposed gamma sampling significantly outperforms the sampling based and beam search based back-translation baselines by 0.9 and 2.3 BLEU scores in terms of SacreBLEU. And our two proposed gamma score based methods outperform the data manipulation method as well.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_69",
            "content": "In the rest of the experiments, we report results for both gamma selection and gamma sampling as the proposed methods and their hyperparameter \u03b3 for other tasks is fixed to 0.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_70",
            "content": "Results on other Datasets",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "414-ARR_v2_71",
            "content": "We conduct the experiments on WMT14 EN-DE and RU-EN for both gamma selection and gamma sampling as well, and table 5 shows that our proposed gamma based methods significantly outperform beam and sampling based back-translation methods on both en-de and ru-en translation for almost 1 and 0.4 BLEU score respectively. Recently, Edunov et al. (2020) point out that BLEU might overlook the contributions from back translation since it poorly correlates with human evaluation on the data generated in back translation scenario. Follow their suggestions, to better reflect the scenario of back translation, we also evaluate our experiment using COMET metric suggested by Rei et al. (2020). The results are shown in table 6 and we can see that the proposed methods perform well in terms of COMET.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_72",
            "content": "Discussion on Efficiency Since our method requires to run sampling with size of 50 to generate synthetic data, its efficiency is about 10x slower than that of beam BT with size of 5 and 50x slower than that of sampling BT with size 1. Luckily, because the bottleneck of BT is not the synthetic data generation but the parameter optimization on both synthetic and authentic data, our overall overhead is less than 0.5x slower than sampling BT. In addition, since decoding is very easy to be parallelized on GPU or CPU machines, the cost of decoding is not a serious issue for our method, which makes it possible to run our method on a large scale dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_73",
            "content": "Analysis on Synthetic Corpus",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "414-ARR_v2_74",
            "content": "In this subsection, we analyze the synthetic corpus of proposed gamma score methods on both sentence level and token level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_75",
            "content": "Sentence Level We evaluate the back-translation synthetic source sentences by their sentence representations. We use the baseline model to generate the hidden representations at the end-of-speech token as the sentence representation. Here, we compute the singular value spectrum of the representations for different back-translation corpora.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_76",
            "content": "The spectrum is shown in figure 1(a). From the spectrum, sampling has a more uniform distribution whereas beam has the worst variety. Our proposed methods have moderate variety between sampling and beam, and gamma sampling consists of higher linguistic information richness compared with gamma selection.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_77",
            "content": "Figure 1(b) shows the sequence length of the synthetic corpora of different generation methods. Beam generates the shortest synthetic sentences and gamma sampling generates the longest synthetic sentences on average. Between them, sampling and gamma selection generate almost the same sequence length, which means gamma selection candidates provide more learning signals than random sampling under the same length.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_78",
            "content": "Token Level Figure 1(c) is the token frequency histogram, which shows beam has higher probability to decode high frequency tokens while sampling prefers more low frequency tokens. We also measure the vocabulary size, finding that the proposed gamma sampling shares the same vocabulary size as sampling method. This could be the reason that gamma sampling is based on random sampling for candidates generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_79",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "414-ARR_v2_80",
            "content": "This section describes prior arts in back-translation for NMT, data augmentation, and semi-supervised machine translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_81",
            "content": "Back-translation NMT Bojar and Tamchyna (2011) firstly proposed back-translation, then Bertoldi and Federico (2009); Lambert et al. (2011) apply back translation to solve the domain adaptation problems in phrase-based NMT systems. Sennrich et al. (2016a) further extend the back translation for training NMT models integrally.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_82",
            "content": "For understanding the back-translation synthetic corpus, Currey et al. (2017) use a copy of target as a pseudo source, and find that NMT model performance can still be improved under the low resource settings. Caswell et al. (2019) propose tagged back-translation to indicate to the model that the given source is synthetic. To further find an optimum back-translation corpus decoding method, Imamura et al. (2018) Data Augmentation for NMT NMT researchers are the pioneers of data augmentation studies since back-translation is a natural type of data augmentation method. (Sennrich et al., 2016a;Norouzi et al., 2016;Zhang and Zong, 2016;Bi et al., 2021). To balance the token frequency in NMT corpus, Fadaee et al. (2017) create new sentences contain low-frequency words. However, as observed by , the improvement across different translation tasks is not consistent, and they invent SwitchOut data augmentation policy. Recht et al. (2018Recht et al. ( , 2019; Werpachowski et al. (2019) also observe such an inconsistency of variance between training corpus and testing set as well as in the generation tasks. Recently, Li et al. (2019a) try to understand data augmentation from input sensitivity and prediction margin, thus obtaining relatively low variance in generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_83",
            "content": "Semi-supervised Machine Translation However, as high quality bitext is always limited and costly to collect, Gulcehre et al. (2015) study methods for effectively leveraging monolingual data in NMT systems. He et al. (2016) develop a duallearning mechanism, under such a learning objective, a NMT system is able to automatically learn from unlabeled data, thus improving NMT performance iteratively. Based on iterative learning, Lample et al. (2018) investigates how to learn NMT systems when only large monolingual corpora can be used in each language.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_84",
            "content": "For supervision of models, Gulcehre et al. (2017) employ the target language model hidden states into NMT decoder to further improve performance. Edunov et al. (2020) show that back-translation improves translation quality of both naturally occurring text and translationese according professional human translators. For supervision of learning corpus, Wu et al. (2019) study both the source-side and target-side monolingual data for NMT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_85",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "414-ARR_v2_86",
            "content": "In this work, we answer a fundamental question about synthetic data for back translation. We theoretically and empirically show two key factors namely quality and importance weight of synthetic data play an important role in back translation, and then we propose a new method to generate synthetic data which better balances both factors so as to boost the back-translation performance. For future work, we think it would be of significance to apply our synthetic data generation method to other BT methods or even to more broad NLP tasks such as paraphrasing and style transfer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "414-ARR_v2_87",
            "content": "Mikel Artetxe, Gorka Labaka, Eneko Agirre, Kyunghyun Cho, Unsupervised neural machine translation, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Mikel Artetxe",
                    "Gorka Labaka",
                    "Eneko Agirre",
                    "Kyunghyun Cho"
                ],
                "title": "Unsupervised neural machine translation",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_88",
            "content": "UNKNOWN, None, 2014, Neural machine translation by jointly learning to align and translate, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2014",
                "pub_title": "Neural machine translation by jointly learning to align and translate",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_89",
            "content": "Lo\u00efc Barrault, Ond\u0159ej Bojar, Marta Costa-Juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Findings of the 2019 conference on machine translation (WMT19), 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Lo\u00efc Barrault",
                    "Ond\u0159ej Bojar",
                    "Marta Costa-Juss\u00e0",
                    "Christian Federmann",
                    "Mark Fishel",
                    "Yvette Graham",
                    "Barry Haddow",
                    "Matthias Huck",
                    "Philipp Koehn",
                    "Shervin Malmasi",
                    "Christof Monz",
                    "Mathias M\u00fcller"
                ],
                "title": "Findings of the 2019 conference on machine translation (WMT19)",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_90",
            "content": "Nicola Bertoldi, Marcello Federico, Domain adaptation for statistical machine translation with monolingual resources, 2009, Proceedings of the fourth workshop on statistical machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Nicola Bertoldi",
                    "Marcello Federico"
                ],
                "title": "Domain adaptation for statistical machine translation with monolingual resources",
                "pub_date": "2009",
                "pub_title": "Proceedings of the fourth workshop on statistical machine translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_91",
            "content": "Wei Bi, Huayang Li, Jiacheng Huang, Data augmentation for text generation without any augmented data, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Wei Bi",
                    "Huayang Li",
                    "Jiacheng Huang"
                ],
                "title": "Data augmentation for text generation without any augmented data",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "414-ARR_v2_92",
            "content": "Ond\u0159ej Bojar, Ale\u0161 Tamchyna, Improving translation model by monolingual data, 2011, Proceedings of the Sixth Workshop on Statistical Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Ond\u0159ej Bojar",
                    "Ale\u0161 Tamchyna"
                ],
                "title": "Improving translation model by monolingual data",
                "pub_date": "2011",
                "pub_title": "Proceedings of the Sixth Workshop on Statistical Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_93",
            "content": "Isaac Caswell, Ciprian Chelba, David Grangier, Tagged back-translation, 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Isaac Caswell",
                    "Ciprian Chelba",
                    "David Grangier"
                ],
                "title": "Tagged back-translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_94",
            "content": "Anna Currey, Antonio Valerio Miceli, Kenneth Barone,  Heafield, Copied monolingual data improves low-resource neural machine translation, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Anna Currey",
                    "Antonio Valerio Miceli",
                    "Kenneth Barone",
                    " Heafield"
                ],
                "title": "Copied monolingual data improves low-resource neural machine translation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "414-ARR_v2_95",
            "content": "Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Understanding back-translation at scale, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Sergey Edunov",
                    "Myle Ott",
                    "Michael Auli",
                    "David Grangier"
                ],
                "title": "Understanding back-translation at scale",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_96",
            "content": "Sergey Edunov, Myle Ott, Marc'aurelio Ranzato, Michael Auli, On the evaluation of machine translation systems trained with back-translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Sergey Edunov",
                    "Myle Ott",
                    "Marc'aurelio Ranzato",
                    "Michael Auli"
                ],
                "title": "On the evaluation of machine translation systems trained with back-translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_97",
            "content": "Marzieh Fadaee, Arianna Bisazza, Christof Monz, Data augmentation for low-resource neural machine translation, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Marzieh Fadaee",
                    "Arianna Bisazza",
                    "Christof Monz"
                ],
                "title": "Data augmentation for low-resource neural machine translation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Short Papers"
            }
        },
        {
            "ix": "414-ARR_v2_98",
            "content": "Marzieh Fadaee, Christof Monz, Backtranslation sampling by targeting difficult words in neural machine translation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Marzieh Fadaee",
                    "Christof Monz"
                ],
                "title": "Backtranslation sampling by targeting difficult words in neural machine translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_99",
            "content": "Zhihao Fan, Yeyun Gong, Dayiheng Liu, Zhongyu Wei, Siyuan Wang, Jian Jiao, Nan Duan, Ruofei Zhang, Xuan-Jing Huang, Mask attention networks: Rethinking and strengthen transformer, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Zhihao Fan",
                    "Yeyun Gong",
                    "Dayiheng Liu",
                    "Zhongyu Wei",
                    "Siyuan Wang",
                    "Jian Jiao",
                    "Nan Duan",
                    "Ruofei Zhang",
                    "Xuan-Jing Huang"
                ],
                "title": "Mask attention networks: Rethinking and strengthen transformer",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_100",
            "content": "Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, Tieyan Liu, Representation degeneration problem in training natural language generation models, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Jun Gao",
                    "Di He",
                    "Xu Tan",
                    "Tao Qin",
                    "Liwei Wang",
                    "Tieyan Liu"
                ],
                "title": "Representation degeneration problem in training natural language generation models",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_101",
            "content": "Miguel Gra\u00e7a, Yunsu Kim, Julian Schamper, Shahram Khadivi, Hermann Ney, Generalizing back-translation in neural machine translation, 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Miguel Gra\u00e7a",
                    "Yunsu Kim",
                    "Julian Schamper",
                    "Shahram Khadivi",
                    "Hermann Ney"
                ],
                "title": "Generalizing back-translation in neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_102",
            "content": "UNKNOWN, None, 2015, On using monolingual corpora in neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "On using monolingual corpora in neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_103",
            "content": "Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Yoshua Bengio, On integrating a language model into neural machine translation, 2017, Computer Speech & Language, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Caglar Gulcehre",
                    "Orhan Firat",
                    "Kelvin Xu",
                    "Kyunghyun Cho",
                    "Yoshua Bengio"
                ],
                "title": "On integrating a language model into neural machine translation",
                "pub_date": "2017",
                "pub_title": "Computer Speech & Language",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_104",
            "content": "Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, Wei-Ying Ma, Dual learning for machine translation, 2016, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Di He",
                    "Yingce Xia",
                    "Tao Qin",
                    "Liwei Wang",
                    "Nenghai Yu",
                    "Tie-Yan Liu",
                    "Wei-Ying Ma"
                ],
                "title": "Dual learning for machine translation",
                "pub_date": "2016",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "414-ARR_v2_105",
            "content": "Duy Vu Cong, Philipp Hoang, Gholamreza Koehn, Trevor Haffari,  Cohn, Iterative backtranslation for neural machine translation, 2018, Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Duy Vu Cong",
                    "Philipp Hoang",
                    "Gholamreza Koehn",
                    "Trevor Haffari",
                    " Cohn"
                ],
                "title": "Iterative backtranslation for neural machine translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_106",
            "content": "Kenji Imamura, Atsushi Fujita, Eiichiro Sumita, Enhancement of encoder and attention using target monolingual corpora in neural machine translation, 2018, Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Kenji Imamura",
                    "Atsushi Fujita",
                    "Eiichiro Sumita"
                ],
                "title": "Enhancement of encoder and attention using target monolingual corpora in neural machine translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_107",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015, ICLR (Poster), .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "P Diederik",
                    "Jimmy Kingma",
                    " Ba"
                ],
                "title": "Adam: A method for stochastic optimization",
                "pub_date": "2015",
                "pub_title": "ICLR (Poster)",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_108",
            "content": "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u0159ej Bojar, Alexandra Constantin, Evan Herbst, Moses: Open source toolkit for statistical machine translation, 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Philipp Koehn",
                    "Hieu Hoang",
                    "Alexandra Birch",
                    "Chris Callison-Burch",
                    "Marcello Federico",
                    "Nicola Bertoldi",
                    "Brooke Cowan",
                    "Wade Shen",
                    "Christine Moran",
                    "Richard Zens",
                    "Chris Dyer",
                    "Ond\u0159ej Bojar",
                    "Alexandra Constantin",
                    "Evan Herbst"
                ],
                "title": "Moses: Open source toolkit for statistical machine translation",
                "pub_date": "2007",
                "pub_title": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "414-ARR_v2_109",
            "content": "Patrik Lambert, Holger Schwenk, Christophe Servan, Sadaf Abdul-Rauf, Investigations on translation model adaptation using monolingual data, 2011, Sixth Workshop on Statistical Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Patrik Lambert",
                    "Holger Schwenk",
                    "Christophe Servan",
                    "Sadaf Abdul-Rauf"
                ],
                "title": "Investigations on translation model adaptation using monolingual data",
                "pub_date": "2011",
                "pub_title": "Sixth Workshop on Statistical Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_110",
            "content": "Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc'aurelio Ranzato, Phrasebased & neural unsupervised machine translation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Guillaume Lample",
                    "Myle Ott",
                    "Alexis Conneau",
                    "Ludovic Denoyer",
                    "Marc'aurelio Ranzato"
                ],
                "title": "Phrasebased & neural unsupervised machine translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_111",
            "content": "Giang Le, Shinka Mori, Lane Schwartz, Illinois Japanese \u2194 English News Translation for WMT 2021, 2021, Proceedings of the Sixth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Giang Le",
                    "Shinka Mori",
                    "Lane Schwartz"
                ],
                "title": "Illinois Japanese \u2194 English News Translation for WMT 2021",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Sixth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_112",
            "content": "Guanlin Li, Lemao Liu, Guoping Huang, Conghui Zhu, Tiejun Zhao, Understanding data augmentation in neural machine translation: Two perspectives towards generalization, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Guanlin Li",
                    "Lemao Liu",
                    "Guoping Huang",
                    "Conghui Zhu",
                    "Tiejun Zhao"
                ],
                "title": "Understanding data augmentation in neural machine translation: Two perspectives towards generalization",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_113",
            "content": "Jierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang, Shuming Shi, Evaluating explanation methods for neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Jierui Li",
                    "Lemao Liu",
                    "Huayang Li",
                    "Guanlin Li",
                    "Guoping Huang",
                    "Shuming Shi"
                ],
                "title": "Evaluating explanation methods for neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "414-ARR_v2_114",
            "content": "Xintong Li, Guanlin Li, Lemao Liu, Max Meng, Shuming Shi, On the word alignment from neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Xintong Li",
                    "Guanlin Li",
                    "Lemao Liu",
                    "Max Meng",
                    "Shuming Shi"
                ],
                "title": "On the word alignment from neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_115",
            "content": "S Jun, Rong Liu, Wing Hung Chen,  Wong, Rejection control and sequential importance sampling, 1998, Journal of the American Statistical Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "S Jun",
                    "Rong Liu",
                    "Wing Hung Chen",
                    " Wong"
                ],
                "title": "Rejection control and sequential importance sampling",
                "pub_date": "1998",
                "pub_title": "Journal of the American Statistical Association",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_116",
            "content": "UNKNOWN, None, 2001, Monte Carlo strategies in scientific computing, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": "2001",
                "pub_title": "Monte Carlo strategies in scientific computing",
                "pub": "Springer"
            }
        },
        {
            "ix": "414-ARR_v2_117",
            "content": "Jonathan Mallinson, Rico Sennrich, Mirella Lapata, Paraphrasing revisited with neural machine translation, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Jonathan Mallinson",
                    "Rico Sennrich",
                    "Mirella Lapata"
                ],
                "title": "Paraphrasing revisited with neural machine translation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "414-ARR_v2_118",
            "content": "UNKNOWN, None, 2020, Data diversification: A simple strategy for neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Data diversification: A simple strategy for neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_119",
            "content": "Mohammad Norouzi, Samy Bengio, Navdeep Zhifeng Chen, Mike Jaitly, Yonghui Schuster, Dale Wu,  Schuurmans, Reward augmented maximum likelihood for neural structured prediction, 2016, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Mohammad Norouzi",
                    "Samy Bengio",
                    "Navdeep Zhifeng Chen",
                    "Mike Jaitly",
                    "Yonghui Schuster",
                    "Dale Wu",
                    " Schuurmans"
                ],
                "title": "Reward augmented maximum likelihood for neural structured prediction",
                "pub_date": "2016",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "414-ARR_v2_120",
            "content": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli, fairseq: A fast, extensible toolkit for sequence modeling, 2019, Proceedings of NAACL-HLT 2019: Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Myle Ott",
                    "Sergey Edunov",
                    "Alexei Baevski",
                    "Angela Fan",
                    "Sam Gross",
                    "Nathan Ng",
                    "David Grangier",
                    "Michael Auli"
                ],
                "title": "fairseq: A fast, extensible toolkit for sequence modeling",
                "pub_date": "2019",
                "pub_title": "Proceedings of NAACL-HLT 2019: Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_121",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Matt Post"
                ],
                "title": "A call for clarity in reporting BLEU scores",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_122",
            "content": "Yulia Shrimai Prabhumoye, Ruslan Tsvetkov, Alan Salakhutdinov,  Black, Style transfer through back-translation, 2018, Proceedings of the 56th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Yulia Shrimai Prabhumoye",
                    "Ruslan Tsvetkov",
                    "Alan Salakhutdinov",
                    " Black"
                ],
                "title": "Style transfer through back-translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_123",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "414-ARR_v2_124",
            "content": "UNKNOWN, None, 2018, Improving language understanding by generative pre-training, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Improving language understanding by generative pre-training",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_125",
            "content": "UNKNOWN, None, 2018, Do cifar-10 classifiers generalize to cifar-10?, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Do cifar-10 classifiers generalize to cifar-10?",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_126",
            "content": "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, Do imagenet classifiers generalize to imagenet, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Benjamin Recht",
                    "Rebecca Roelofs",
                    "Ludwig Schmidt",
                    "Vaishaal Shankar"
                ],
                "title": "Do imagenet classifiers generalize to imagenet",
                "pub_date": "2019",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "414-ARR_v2_127",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, Comet: A neural framework for mt evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Ricardo Rei",
                    "Craig Stewart",
                    "Ana Farinha",
                    "Alon Lavie"
                ],
                "title": "Comet: A neural framework for mt evaluation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_128",
            "content": "UNKNOWN, None, 2020, Decoding and diversity in machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Decoding and diversity in machine translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_129",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Improving neural machine translation models with monolingual data, 2016, Proceedings of the 54th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Improving neural machine translation models with monolingual data",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_130",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_131",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Neural machine translation of rare words with subword units",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "414-ARR_v2_132",
            "content": "Ilya Sutskever, Oriol Vinyals, Quoc V Le, Sequence to sequence learning with neural networks, 2014, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Ilya Sutskever",
                    "Oriol Vinyals",
                    "Quoc V Le"
                ],
                "title": "Sequence to sequence learning with neural networks",
                "pub_date": "2014",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_133",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_134",
            "content": "Shuo Wang, Yang Liu, Chao Wang, Huanbo Luan, Maosong Sun, Improving back-translation with uncertainty-based confidence estimation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Shuo Wang",
                    "Yang Liu",
                    "Chao Wang",
                    "Huanbo Luan",
                    "Maosong Sun"
                ],
                "title": "Improving back-translation with uncertainty-based confidence estimation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_135",
            "content": "Xinyi Wang, Hieu Pham, Zihang Dai, Graham Neubig, Switchout: an efficient data augmentation algorithm for neural machine translation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Xinyi Wang",
                    "Hieu Pham",
                    "Zihang Dai",
                    "Graham Neubig"
                ],
                "title": "Switchout: an efficient data augmentation algorithm for neural machine translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_136",
            "content": "Roman Werpachowski, Andr\u00e1s Gy\u00f6rgy, Csaba Szepesv\u00e1ri, Detecting overfitting via adversarial examples, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Roman Werpachowski",
                    "Andr\u00e1s Gy\u00f6rgy",
                    "Csaba Szepesv\u00e1ri"
                ],
                "title": "Detecting overfitting via adversarial examples",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_137",
            "content": "Lijun Wu, Yiren Wang, Yingce Xia, Tao Qin, Jianhuang Lai, Tie-Yan Liu, Exploiting monolingual data at scale for neural machine translation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Lijun Wu",
                    "Yiren Wang",
                    "Yingce Xia",
                    "Tao Qin",
                    "Jianhuang Lai",
                    "Tie-Yan Liu"
                ],
                "title": "Exploiting monolingual data at scale for neural machine translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_138",
            "content": "Jiajun Zhang, Chengqing Zong, Exploiting source-side monolingual data in neural machine translation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Jiajun Zhang",
                    "Chengqing Zong"
                ],
                "title": "Exploiting source-side monolingual data in neural machine translation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_139",
            "content": "UNKNOWN, None, 2018, Style transfer as unsupervised machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Style transfer as unsupervised machine translation",
                "pub": null
            }
        },
        {
            "ix": "414-ARR_v2_140",
            "content": "Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, Tieyan Liu, Incorporating bert into neural machine translation, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Jinhua Zhu",
                    "Yingce Xia",
                    "Lijun Wu",
                    "Di He",
                    "Tao Qin",
                    "Wengang Zhou",
                    "Houqiang Li",
                    "Tieyan Liu"
                ],
                "title": "Incorporating bert into neural machine translation",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "414-ARR_v2_0@0",
            "content": "On Synthetic Data for Back Translation *",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_0",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_2@0",
            "content": "Back translation (BT) is one of the most significant technologies in NMT research fields.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_2",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_2@1",
            "content": "Existing attempts on BT share a common characteristic: they employ either beam search or random sampling to generate synthetic data with a backward model but seldom work studies the role of synthetic data in the performance of BT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_2",
            "start": 90,
            "end": 319,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_2@2",
            "content": "This motivates us to ask a fundamental question: what kind of synthetic data contributes to BT performance?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_2",
            "start": 321,
            "end": 427,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_2@3",
            "content": "Through both theoretical and empirical studies, we identify two key factors on synthetic data controlling the back-translation NMT performance, which are quality and importance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_2",
            "start": 429,
            "end": 605,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_2@4",
            "content": "Furthermore, based on our findings, we propose a simple yet effective method to generate synthetic data to better trade off both factors so as to yield a better performance for BT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_2",
            "start": 607,
            "end": 786,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_2@5",
            "content": "We run extensive experiments on WMT14 DE-EN, EN-DE, and RU-EN benchmark tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_2",
            "start": 788,
            "end": 865,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_2@6",
            "content": "By employing our proposed method to generate synthetic data, our BT model significantly outperforms the standard BT baselines (i.e., beam and sampling based methods for data generation), which proves the effectiveness of our proposed methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_2",
            "start": 867,
            "end": 1108,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_4@0",
            "content": "Since the birth of neural machine translation (NMT) (Bahdanau et al., 2014;Sutskever et al., 2014) back translation (BT) (Sennrich et al., 2016a) has quickly become one of the most significant technologies in natural language processing (NLP) research field.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_4",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_4@1",
            "content": "This is because 1) it provides a simple yet effective approach to advance the supervised NMT by leveraging monolingual data (Edunov et al., 2018) and it also serves as a key learning objective in unsupervised NMT (Artetxe et al., 2018;Lample et al., 2018); 2) back translation even plays a significant role in other NLP re-search fields beyond translation such as paraphrasing (Mallinson et al., 2017) and style transfer (Prabhumoye et al., 2018;Zhang et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_4",
            "start": 259,
            "end": 724,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_5@0",
            "content": "Back translation consists of two steps, namely synthetic corpus generation with a backward model and parameter optimization for the forward model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_5",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_5@1",
            "content": "Various contributions have been made on improving back translation, for instance, iterative back translation (Hoang et al., 2018), tagged back translation (Caswell et al., 2019), confidence weighting , data diversification (Nguyen et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_5",
            "start": 147,
            "end": 391,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_5@2",
            "content": "Although these efforts differ in some aspects, all of them share a common characteristic: they employ a default way to generate synthetic data in the first step of BT which is either beam search or random sampling with a backward model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_5",
            "start": 393,
            "end": 628,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_5@3",
            "content": "Seldom work studies the consequences of synthetic corpus to back translation and hence it is unclear how synthetic data influences the final performance of BT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_5",
            "start": 630,
            "end": 788,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_6@0",
            "content": "The early study empirically suggests the quality of the synthetic corpus is vital for BT performance (Sennrich et al., 2016a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_6",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_6@1",
            "content": "However, recent studies illustrate better test performance can be achieved by low quality synthetic corpus (Edunov et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_6",
            "start": 127,
            "end": 255,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_6@2",
            "content": "This contradictory observation indicates the quality of synthetic data is not the only element that affects the BT performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_6",
            "start": 257,
            "end": 383,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_6@3",
            "content": "Hence, this fact naturally raises a fundamental question: what kind of synthetic data contributes to back translation performance?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_6",
            "start": 385,
            "end": 514,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_7@0",
            "content": "In this paper, we attempt to take a step forward toward the above fundamental question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_7",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_7@1",
            "content": "To this end, we start from a critical objective in semi-supervised learning, which is defined by the marginal distribution of a target language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_7",
            "start": 88,
            "end": 231,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_7@2",
            "content": "Then we derive an approximate lower bound of the objective function, which is closely related to the objective of back translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_7",
            "start": 233,
            "end": 363,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_7@3",
            "content": "Corresponding to this lower bound, we theoretically find two related elements for maximizing such a lower bound: quality of synthetic bilingual data and importance weight of its source.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_7",
            "start": 365,
            "end": 549,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_7@4",
            "content": "Since both elements are mutually exclusive to some extent, it may induce contradictory observation if one judges the BT performance according to a single element.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_7",
            "start": 551,
            "end": 712,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_7@5",
            "content": "In addition, such a theoretical explanation is supported by our empirical experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_7",
            "start": 714,
            "end": 799,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_7@6",
            "content": "Furthermore, based on our findings, we propose a new heuristic approach to generate synthetic data whose both elements are better balanced so as to yield improvements over both sampling and beam search based methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_7",
            "start": 801,
            "end": 1016,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_7@7",
            "content": "Extensive experiments on three WMT14 tasks show that our BT consistently outperforms the standard sampling and beam search based baselines by a significant margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_7",
            "start": 1018,
            "end": 1180,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_8@0",
            "content": "Our contributions are three folds:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_8",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_9@0",
            "content": "1. We point out that importance weight and quality of synthetic candidates are two key factors that affect the NMT performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_9",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_10@0",
            "content": "2. We propose a simple yet effective method for synthetic corpus generation, which could better balance the quality and importance of synthetic data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_10",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_11@0",
            "content": "3. Our experiments prove the effectiveness of the aforementioned strategy, it outperforms beam or sampling decoding methods on three benchmark tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_11",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_12@0",
            "content": "Revisiting Back Translation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_12",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_13@0",
            "content": "NMT builds a probabilistic model p(y|x; \u03b8) with neural networks parameterized by \u03b8, which is used to translate a sentence x in source language X to a sentence y in target language Y. The standard wisdom to train the model is to minimize the following objective function over a given bilingual corpus B = {(x i , y i )}:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_13",
            "start": 0,
            "end": 318,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_14@0",
            "content": "\u2113(B; \u03b8) = (x i ,y i )\u2208B log p(y i |x i ; \u03b8)(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_14",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_15@0",
            "content": "Recently Sennrich et al. (2016a) propose a remarkable method called Back Translation (BT) to improve NMT by using a monolingual corpus M in target language Y besides B and back translation becomes one of the most successful techniques in NMT (Fadaee and Monz, 2018;Edunov et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_15",
            "start": 0,
            "end": 285,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_15@1",
            "content": "At a high level, back translation can be considered as a semi-supervised method because it leverages both labeled and unlabeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_15",
            "start": 287,
            "end": 419,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_15@2",
            "content": "Suppose p(x|y; \u03c0) is the backward translation model whose parameter \u03c0 is optimized over B, the key idea of back translation can be summarized as the following two steps:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_15",
            "start": 421,
            "end": 589,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_16@0",
            "content": "To make BT more efficient, the standard configuration is widely adopted: each sentence y is required to generate a single source x and both two steps are performed for a single pass.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_16",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_16@1",
            "content": "We follow this standard in this paper for generality but our idea in this paper is straightforward to apply to other configurations such as (Gra\u00e7a et al., 2019;Hoang et al., 2018;Nguyen et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_16",
            "start": 183,
            "end": 382,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_17@0",
            "content": "In the first step, there are two main strategies to generate the synthetic corpus, i.e., deterministically decoding and randomly sampling with p(x|y; \u03c0).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_17",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_17@1",
            "content": "The first strategy aims to search the best candidate as follows, xb = arg max p(x|y; \u03c0)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_17",
            "start": 154,
            "end": 240,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_18@0",
            "content": "(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_18",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_19@0",
            "content": "The above optimization is achieved by the beam search decoding, which can be regarded as a degenerated shortest path problem with respect to the log p(x|y; \u03c0) with limited routing attempts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_19",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_19@1",
            "content": "The alternative strategy is random sampling: it randomly samples a token with respect to the distribution estimated by a back-translation model at each decoding step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_19",
            "start": 190,
            "end": 355,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_19@2",
            "content": "Such a process can be modelled by,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_19",
            "start": 357,
            "end": 390,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_20@0",
            "content": "xs = rand{p(x|y; \u03c0)} (4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_20",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_21@0",
            "content": "Research Question Prior work points out (Sennrich et al., 2016a) that the synthetic corpus with high quality is beneficial to the final performance of back translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_21",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_21@1",
            "content": "However, the recent studies (Edunov et al., 2018) find that NMT models with unsatisfactory BLEU score corpus, for instance, the corpus generated by sampling based strategy, also establish the state-of-the-art (SOTA) achievement among back-translation NMT models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_21",
            "start": 169,
            "end": 430,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_22@0",
            "content": "This contradictory fact indicates that the quality of synthetic corpus is not the sole element for back translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_22",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_22@1",
            "content": "This motivates us to study a fundamental question for back translation: what kind of synthetic corpus is beneficial to back translation?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_22",
            "start": 117,
            "end": 252,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_23@0",
            "content": "Understanding Synthetic Data by Two Factors",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_23",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_24@0",
            "content": "To answer the fundamental question presented in the previous section, we first start from the marginal likelihood objective defined on the target language Y, and then we theoretically explain two factors (i.e., quality and importance) that are highly related to the training objective of back translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_24",
            "start": 0,
            "end": 304,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_24@1",
            "content": "Finally, we empirically explain why synthetic corpus with low quality may lead to better performance than synthetic corpus with high quality by measuring both factors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_24",
            "start": 306,
            "end": 472,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_25@0",
            "content": "Theoretical Explanation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_25",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_26@0",
            "content": "Maximizing marginal likelihood is an important principle to leverage unlabeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_26",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_26@1",
            "content": "Therefore, we rethink back translation from this principle because it makes use of target monolingual corpus M. For each y \u2208 M, the marginal likelihood objective can be derived by the Bayesian Equation ( 5 7is the same as the second term in BT loss (i.e., log p(y|x) in Eq. 2), and the unique difference is the multiplicative term called importance weight:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_26",
            "start": 85,
            "end": 440,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_27@0",
            "content": "Imp(x; y) = p(x) p(x|y)(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_27",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_28@0",
            "content": "The denominator is the candidate conditional probability to target, and the numerator is the candidate distribution on source language distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_28",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_28@1",
            "content": "Since Imp(x; y) is constant with respect to the parameter \u03b8, maximizing log p(y|x; \u03b8) in BT loss implicitly maximizes Imp(x; y) log p(y|x), which indicates that back translation aims to implicitly maximize the marginal likelihood objective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_28",
            "start": 149,
            "end": 388,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_28@2",
            "content": "More importantly, according to Equation 7we can find that the following two factors are critical to influence the marginal likelihood log p(y; \u03b8):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_28",
            "start": 390,
            "end": 535,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_29@0",
            "content": "\u2022 Factor 1: The quality of x as a translation of y corresponding to the log p(y|x; \u03b8) in Eq. 7. \u2022 Factor 2: The importance of x as a translation of y corresponding to Imp(x; y) in Eq. 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_29",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_30@0",
            "content": "Theoretically, if x is of higher quality and contains more semantic information in y, p(y|x; \u03b8) would be higher and thus it would lead to a higher log p(y; \u03b8), which is well acknowledged by prior work (Sennrich et al., 2016a;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_30",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_31@0",
            "content": "In particular, if x is with higher importance weight, maximizing log p(y|x; \u03b8) is more helpful to maximize log p(y; \u03b8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_31",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_31@1",
            "content": "On the contrary, if Imp(x; y) is very small, it needs to avoid such a sample x from p(x|y), which is essentially the rejection control strategy in importance sampling theory (Liu et al., 1998;Liu and Liu, 2001).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_31",
            "start": 120,
            "end": 330,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_31@2",
            "content": "Unfortunately, in practice, both factors are mutually exclusive to some extent: if x is with high quality, p(x|y; \u03b8) would be higher as well leading to lower importance weight.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_31",
            "start": 332,
            "end": 507,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_31@3",
            "content": "This fact can explain the contradictory observation in Sec 2 that BT with high-quality synthetic data sometimes leads to better testing performance, while it may deliver worse performance at other times, which will be later justified in Sec 3.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_31",
            "start": 509,
            "end": 753,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_32@0",
            "content": "Estimating Two Factors To measure the quality of x for each y, it is natural to use the evaluation metric such as BLEU if the reference translation x of y is available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_32",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_32@1",
            "content": "Otherwise, as a surrogate, we use the log likelihood log p(x|y; \u03c0) of the backward translation model \u03c0 which is trained on the authentic data B. Similarly, in order to estimate the importance of x, we train an additional language model p(x; \u03c9) with GPT (Radford et al., 2018)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_32",
            "start": 169,
            "end": 443,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_33@0",
            "content": "Empirical Justification",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_33",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_34@0",
            "content": "In this subsection, we aim to justify the following statements: 1) encouraging the quality of synthetic corpus may to some extent hurt the performance of BT due to the decrease of importance; 2) judging the testing performance in terms of quality only may be dangerous while it would be meaningful to judge the testing performance by taking into account both factors rather than either factor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_34",
            "start": 0,
            "end": 392,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_34@1",
            "content": "To this end, we run some quick experiments on WMT14 datasets whose settings will be shown in Sec 5 later.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_34",
            "start": 394,
            "end": 498,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_34@2",
            "content": "We set up two back translation systems with two different options (i.e., beam search and sampling) to generate synthetic corpus by using the best checkpoint of p(x|y; \u03c0) tuned on the development set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_34",
            "start": 500,
            "end": 698,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_34@3",
            "content": "Both beam search and sampling based BT systems are denoted by beam and sampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_34",
            "start": 700,
            "end": 779,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_34@4",
            "content": "In addition, we pick another checkpoint of p(x|y; \u03c0) which is trained for only 1 epoch, and we use this weak checkpoint to set up another beam search based BT system, which is denoted as beam*. Table 1 shows BLEU on test dataset, the quality and importance on the development set according to three systems on WMT14 DE-EN task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_34",
            "start": 781,
            "end": 1107,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_35@0",
            "content": "In Table 1, beam is better than sampling in the quality of synthetic corpus but its testing performance is worse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_35",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_35@1",
            "content": "This is meaningful because the former relies on the synthetic corpus with lower importance weight according to our theoretical ex-planation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_35",
            "start": 114,
            "end": 253,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_35@2",
            "content": "In addition, when comparing beam with beam*, we can find that beam delivers better testing performance because its quality is better meanwhile its importance weight is almost similar to that of beam*. Table 2 consistently demonstrates that it is meaningless to take into account quality only when evaluating BT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_35",
            "start": 255,
            "end": 565,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_35@3",
            "content": "These facts justify our statements and provide an answer to the fundamental question in section 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_35",
            "start": 567,
            "end": 664,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_36@0",
            "content": "Improving Synthetic Data for BT",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_36",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_37@0",
            "content": "As shown in the previous section, both importance and quality of synthetic corpus are beneficial to the overall testing performance of back translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_37",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_37@1",
            "content": "It is a natural idea to promote both factors when generating synthetic corpus such that running BT on such corpus leads to better testing performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_37",
            "start": 153,
            "end": 302,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_37@2",
            "content": "However, this is difficult because both factors are mutually exclusive as discussed in Section 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_37",
            "start": 304,
            "end": 400,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_37@3",
            "content": "In this section, we instead propose two methods (namely data manipulation and gamma score) to trade off both factors in the hope to yield better BT performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_37",
            "start": 402,
            "end": 561,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_38@0",
            "content": "Data Manipulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_38",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_39@0",
            "content": "Since the synthetic data in sampling based BT is of high importance yet low quality whereas the case for the synthetic data in beam search based BT is opposite, we propose a data manipulation method to trade off importance and quality by combining both synthetic datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_39",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_39@1",
            "content": "Through balancing the ratio between beam and sampling based synthetic corpora, we expect to find an optimized beam/sampling ratio to further improve NMT model performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_39",
            "start": 273,
            "end": 443,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_40@0",
            "content": "Specifically, we randomly shuffle M and divide it into two parts with the first part accounting for \u03b3 (0 < \u03b3 < 1); then we generate translations for the first part with beam search while generating translations for the second part with sampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_40",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_40@1",
            "content": "Formally, we use the following corpus M c as the synthetic corpus for BT:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_40",
            "start": 246,
            "end": 318,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_41@0",
            "content": "M c = {(x b i , y i ) k i=0 } \u222a {(x s j , y j ) |M| j=k } k =\u230a\u03b3|M|\u230b",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_41",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_42@0",
            "content": "Where xb denotes a translation of y generated by p(x|y; \u03c0) with beam search and xs is a translation with sampling, | \u2022 | means the size of the corpus, and \u03b3 is the combination ratio for beam and sampling synthetic corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_42",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_42@1",
            "content": "By tuning \u03b3 here, one can modify the weightage for the number of beam and sampling sentences, to improve back-translation performance by training models on a combined synthetic corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_42",
            "start": 223,
            "end": 406,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_43@0",
            "content": "Although this method is easy to implement, its limitation is obvious.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_43",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_43@1",
            "content": "Since each x is either from beam search or from sampling, the quality of M c is generally worse than that of beam search and its importance weight is generally worse than that of sampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_43",
            "start": 70,
            "end": 257,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_43@2",
            "content": "Consequently, we propose an alternative method in the next part of this section.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_43",
            "start": 259,
            "end": 338,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_44@0",
            "content": "Gamma Score",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_44",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_45@0",
            "content": "The key idea to the alternative method is that it employs a score that balances both quality and importance to generate a translation x for each y \u2208 M. A natural choice of such a score is defined by the interpolation score as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_45",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_46@0",
            "content": "\u03b3 log Imp(x; \u03c9, \u03c0) + (1 \u2212 \u03b3) log p(x|y; \u03c0)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_46",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_47@0",
            "content": "where \u03b3 is used to trade off both factors as in corpus manipulation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_47",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_47@1",
            "content": "With the help of this score, one may optimize the x by beam search whose interpolation score is the best among all possible translations of y \u2208 M. Unfortunately, such an implementation leads to limited performance in our preliminary experiments, due to two major challenges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_47",
            "start": 69,
            "end": 342,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_48@0",
            "content": "On one hand, the estimations of quality and importance weight of x are not well calibrated, and in particular, quality and importance are mutually exclusive as mentioned before.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_48",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_48@1",
            "content": "As a result, beam search with the interpolation score over the exponential space can not guarantee a desirable translation x for each y. On the other hand, quality and importance weight of x are not at the same scale for different y, it is difficult to balance both factors with a fixed \u03b3 in the interpolation score for different y.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_48",
            "start": 178,
            "end": 509,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_49@0",
            "content": "To alleviate these issues, we propose a simple method as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_49",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_49@1",
            "content": "Specifically, firstly, instead of beam search with the interpolation score, we simply utilize the backward translation p(x|y; \u03c0) to randomly sample a set of candidate translations which is denoted by A(y) = {x i } N i (N = 50 in this paper as it works well).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_49",
            "start": 66,
            "end": 323,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_49@2",
            "content": "1 Then we pick a xj among A(y) according to the balancing score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_49",
            "start": 325,
            "end": 388,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_49@3",
            "content": "Secondly, for each x, we normalize the log values of importance and quality of each candidate by its 1 N -best decoding strategy with p(x|y; \u03c0) to generate N candidates may be another solution which remains as future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_49",
            "start": 390,
            "end": 611,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_50@0",
            "content": "sequence length, then normalize these values with respect to all N candidates as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_50",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_51@0",
            "content": "F(x i ) = log F(x i ) /len(x i ) \u2212 \u00b5 F \u03c3 F (9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_51",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_52@0",
            "content": "where F is either importance weight or quality estimations, and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_52",
            "start": 0,
            "end": 62,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_53@0",
            "content": "\u00b5 F = 1 N i log F(x i ) and \u03c3 F = i (log F (x i )\u2212\u00b5 F ) 2 N \u22121",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_53",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_54@0",
            "content": "are mean and variance of N sampled candidates with length normalized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_54",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_54@1",
            "content": "Finally, the Gamma score is defined on the normalized values of importance and quality as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_54",
            "start": 70,
            "end": 167,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_55@0",
            "content": "\u0393(x i ; \u03c9, \u03c0) = exp \u03b3 \u0128mp(x i ; \u03c9, \u03c0) + (1 \u2212 \u03b3)p(x i |y, \u03c0) j exp \u03b3 \u0128mp(x j ; \u03c9, \u03c0) + (1 \u2212 \u03b3)p(x j |y, \u03c0)(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_55",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_56@0",
            "content": "where \u0128mp and p are the normalized log value of importance weight and backward translation model p(x|y, \u03c0) as defined in Equation 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_56",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_57@0",
            "content": "Once the gamma score in Equation 10 is computed, there are two methods to select x from A(y), which are deterministic and stochastic methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_57",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_57@1",
            "content": "For deterministic selection, we simply select the candidates with maximum gamma score among N translation candidates; and for sampling, we sample a candidate according to its gamma score distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_57",
            "start": 142,
            "end": 341,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_57@2",
            "content": "These two methods are called gamma selection and gamma sampling in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_57",
            "start": 343,
            "end": 425,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_58@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_58",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_59@0",
            "content": "Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_59",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_60@0",
            "content": "We run all the experiments by using fairseq (Ott et al., 2019) framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_60",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_60@1",
            "content": "For dataset settings, since datasets WMT14 EN-DE and DE-EN are widely used (Li et al., 2019b;Zhu et al., 2020;Fan et al., 2021;Le et al., 2021), we follow both standard benchmarks and additionally we employ WMT14 RU-EN as the third dataset to validate the effectiveness of the proposed methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_60",
            "start": 74,
            "end": 367,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_60@2",
            "content": "For back translation experiment, we use an equal scale monolingual corpus randomly sampled from Newscrawl 2020 (Barrault et al., 2019) comprising 4.5 million monolingual sentences for DE-EN language pair and 2.5 million for RU-EN direction, thus total 9 million sentences for DE-EN pair and 5 million for RU-EN direction are used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_60",
            "start": 369,
            "end": 698,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_60@3",
            "content": "We tokenize the parallel corpus using Mose tokenizer (Koehn et al., 2007), and learn a source and target shared Byte-Pair-Encoding (BPE) (Sennrich et al., 2016b with 32K types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_60",
            "start": 700,
            "end": 875,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_60@4",
            "content": "We develop on newstest2013 and report the results on newstest2014.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_60",
            "start": 877,
            "end": 942,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_61@0",
            "content": "As for model architecture, we employ all the translation models using architecture transformer_wmt_en_de_big, which is a Big Transformer architecture with 6 blocks in the encoder and decoder, and is widely used as a standard backbone on various NMT research studies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_61",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_61@1",
            "content": "We use the same hyperparameter settings across all the experiments, i.e., 1024 word representation size, 4096 inner dimensions of feed-forward layers, and dropout is set to 0.3 for all the experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_61",
            "start": 267,
            "end": 467,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_61@2",
            "content": "In addition, for monolingual models, we apply transformer_lm_gpt architecture (Radford et al., 2018) on source language side of the corpus without any extra corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_61",
            "start": 469,
            "end": 632,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_61@3",
            "content": "2 The detailed hyperparameters used for training translation and language models are shown in Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_61",
            "start": 634,
            "end": 736,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_62@0",
            "content": "For baseline models, we train them for 400K updating steps, and train the models with backtranslation data for 1.6M updating steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_62",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_62@1",
            "content": "We save the checkpoints every 100k updating intervals, and only select the checkpoints with highest develop set performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_62",
            "start": 132,
            "end": 255,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_62@2",
            "content": "As for the back-translation data, we study beam decoding and sampling decoding as baselines since they are the common practice for BT research (Roberts et al., 2020;. We use baseline models' checkpoints at 400K updating steps to generate default beam5 decoding and sampling decoding synthetic corpus without any penalty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_62",
            "start": 257,
            "end": 576,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_62@3",
            "content": "For monolingual models, we only select the checkpoints with the best develop set performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_62",
            "start": 578,
            "end": 670,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_62@4",
            "content": "When tuning \u03b3 on dev sets for data manipulation methods we select it from {0, 1/4, 1/2, 3/4, 1} and the optimal is \u03b3 = 1/2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_62",
            "start": 672,
            "end": 794,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_62@5",
            "content": "For the Gamma Score method, \u03b3 is tuned among {0.1, 0.2, 0.3, 0.4, 0.5} and it is set \u03b3 = 0.2 for all three tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_62",
            "start": 796,
            "end": 908,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_62@6",
            "content": "All the experiments are conducted using 8 Nvidia V100-32GB graphic cards without any gradient accumulation or bitext upsampling, and the results in this paper are measured in case-sensitive detokenized BLEU with SacreBLEU 3 by Post (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_62",
            "start": 910,
            "end": 1148,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_63@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_63",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_64@0",
            "content": "Results on DE-EN Data Manipulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_64",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_65@0",
            "content": "We conduct two experiments to study the data manipulation for backtranslation NMT model performance using aforementioned corpus with and without authentic corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_65",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_66@0",
            "content": "Table 3 show the data manipulation results compared with baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_66",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_66@1",
            "content": "Firstly, for synthetic corpus experiment, we find that even if only monolingual corpus is used, the performance of back-translation NMT model can still be significantly improved to 31.3 from 29.2 by sampling or 27.6 by beam, and it is only 0.7 lower than bitext baseline by BLEU score measure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_66",
            "start": 67,
            "end": 359,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_66@2",
            "content": "Secondly, for the experiments with bitext, the best performance by data manipulation only helps the back-translation NMT model achieves almost the same performance with sampling BT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_66",
            "start": 361,
            "end": 541,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_66@3",
            "content": "This means data manipulation methods cannot achieve a higher BLEU score than sampling or beam.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_66",
            "start": 543,
            "end": 636,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_67@0",
            "content": "Gamma Score In this paragraph, we conduct the experiments based on gamma score method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_67",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_67@1",
            "content": "We conduct both of the methods in this experiment: we select the candidate with highest gamma score for the deterministic method whereas sample the candidate by gamma score distribution for the stochastic method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_67",
            "start": 87,
            "end": 298,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_68@0",
            "content": "Once again, we use synthetic gamma corpus combined with bitext to train the back-translation NMT models on each corpus, the results are listed in 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_68",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_68@1",
            "content": "From the table, we can see that our proposed gamma sampling significantly outperforms the sampling based and beam search based back-translation baselines by 0.9 and 2.3 BLEU scores in terms of SacreBLEU. And our two proposed gamma score based methods outperform the data manipulation method as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_68",
            "start": 149,
            "end": 447,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_69@0",
            "content": "In the rest of the experiments, we report results for both gamma selection and gamma sampling as the proposed methods and their hyperparameter \u03b3 for other tasks is fixed to 0.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_69",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_70@0",
            "content": "Results on other Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_70",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_71@0",
            "content": "We conduct the experiments on WMT14 EN-DE and RU-EN for both gamma selection and gamma sampling as well, and table 5 shows that our proposed gamma based methods significantly outperform beam and sampling based back-translation methods on both en-de and ru-en translation for almost 1 and 0.4 BLEU score respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_71",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_71@1",
            "content": "Recently, Edunov et al. (2020) point out that BLEU might overlook the contributions from back translation since it poorly correlates with human evaluation on the data generated in back translation scenario.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_71",
            "start": 317,
            "end": 522,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_71@2",
            "content": "Follow their suggestions, to better reflect the scenario of back translation, we also evaluate our experiment using COMET metric suggested by Rei et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_71",
            "start": 524,
            "end": 683,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_71@3",
            "content": "The results are shown in table 6 and we can see that the proposed methods perform well in terms of COMET.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_71",
            "start": 685,
            "end": 789,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_72@0",
            "content": "Discussion on Efficiency Since our method requires to run sampling with size of 50 to generate synthetic data, its efficiency is about 10x slower than that of beam BT with size of 5 and 50x slower than that of sampling BT with size 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_72",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_72@1",
            "content": "Luckily, because the bottleneck of BT is not the synthetic data generation but the parameter optimization on both synthetic and authentic data, our overall overhead is less than 0.5x slower than sampling BT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_72",
            "start": 235,
            "end": 441,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_72@2",
            "content": "In addition, since decoding is very easy to be parallelized on GPU or CPU machines, the cost of decoding is not a serious issue for our method, which makes it possible to run our method on a large scale dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_72",
            "start": 443,
            "end": 653,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_73@0",
            "content": "Analysis on Synthetic Corpus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_73",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_74@0",
            "content": "In this subsection, we analyze the synthetic corpus of proposed gamma score methods on both sentence level and token level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_74",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_75@0",
            "content": "Sentence Level We evaluate the back-translation synthetic source sentences by their sentence representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_75",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_75@1",
            "content": "We use the baseline model to generate the hidden representations at the end-of-speech token as the sentence representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_75",
            "start": 110,
            "end": 232,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_75@2",
            "content": "Here, we compute the singular value spectrum of the representations for different back-translation corpora.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_75",
            "start": 234,
            "end": 340,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_76@0",
            "content": "The spectrum is shown in figure 1(a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_76",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_76@1",
            "content": "From the spectrum, sampling has a more uniform distribution whereas beam has the worst variety.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_76",
            "start": 38,
            "end": 132,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_76@2",
            "content": "Our proposed methods have moderate variety between sampling and beam, and gamma sampling consists of higher linguistic information richness compared with gamma selection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_76",
            "start": 134,
            "end": 303,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_77@0",
            "content": "Figure 1(b) shows the sequence length of the synthetic corpora of different generation methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_77",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_77@1",
            "content": "Beam generates the shortest synthetic sentences and gamma sampling generates the longest synthetic sentences on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_77",
            "start": 96,
            "end": 215,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_77@2",
            "content": "Between them, sampling and gamma selection generate almost the same sequence length, which means gamma selection candidates provide more learning signals than random sampling under the same length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_77",
            "start": 217,
            "end": 413,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_78@0",
            "content": "Token Level Figure 1(c) is the token frequency histogram, which shows beam has higher probability to decode high frequency tokens while sampling prefers more low frequency tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_78",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_78@1",
            "content": "We also measure the vocabulary size, finding that the proposed gamma sampling shares the same vocabulary size as sampling method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_78",
            "start": 180,
            "end": 308,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_78@2",
            "content": "This could be the reason that gamma sampling is based on random sampling for candidates generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_78",
            "start": 310,
            "end": 408,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_79@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_79",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_80@0",
            "content": "This section describes prior arts in back-translation for NMT, data augmentation, and semi-supervised machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_80",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_81@0",
            "content": "Back-translation NMT Bojar and Tamchyna (2011) firstly proposed back-translation, then Bertoldi and Federico (2009); Lambert et al. (2011) apply back translation to solve the domain adaptation problems in phrase-based NMT systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_81",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_81@1",
            "content": "Sennrich et al. (2016a) further extend the back translation for training NMT models integrally.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_81",
            "start": 231,
            "end": 325,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_82@0",
            "content": "For understanding the back-translation synthetic corpus, Currey et al. (2017) use a copy of target as a pseudo source, and find that NMT model performance can still be improved under the low resource settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_82",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_82@1",
            "content": "Caswell et al. (2019) propose tagged back-translation to indicate to the model that the given source is synthetic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_82",
            "start": 210,
            "end": 323,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_82@2",
            "content": "To further find an optimum back-translation corpus decoding method, Imamura et al. (2018) Data Augmentation for NMT NMT researchers are the pioneers of data augmentation studies since back-translation is a natural type of data augmentation method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_82",
            "start": 325,
            "end": 571,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_82@3",
            "content": "(Sennrich et al., 2016a;Norouzi et al., 2016;Zhang and Zong, 2016;Bi et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_82",
            "start": 573,
            "end": 655,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_82@4",
            "content": "To balance the token frequency in NMT corpus, Fadaee et al. (2017) create new sentences contain low-frequency words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_82",
            "start": 657,
            "end": 772,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_82@5",
            "content": "However, as observed by , the improvement across different translation tasks is not consistent, and they invent SwitchOut data augmentation policy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_82",
            "start": 774,
            "end": 920,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_82@6",
            "content": "Recht et al. (2018Recht et al. ( , 2019; Werpachowski et al. (2019) also observe such an inconsistency of variance between training corpus and testing set as well as in the generation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_82",
            "start": 922,
            "end": 1111,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_82@7",
            "content": "Recently, Li et al. (2019a) try to understand data augmentation from input sensitivity and prediction margin, thus obtaining relatively low variance in generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_82",
            "start": 1113,
            "end": 1275,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_83@0",
            "content": "Semi-supervised Machine Translation However, as high quality bitext is always limited and costly to collect, Gulcehre et al. (2015) study methods for effectively leveraging monolingual data in NMT systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_83",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_83@1",
            "content": "He et al. (2016) develop a duallearning mechanism, under such a learning objective, a NMT system is able to automatically learn from unlabeled data, thus improving NMT performance iteratively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_83",
            "start": 206,
            "end": 397,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_83@2",
            "content": "Based on iterative learning, Lample et al. (2018) investigates how to learn NMT systems when only large monolingual corpora can be used in each language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_83",
            "start": 399,
            "end": 551,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_84@0",
            "content": "For supervision of models, Gulcehre et al. (2017) employ the target language model hidden states into NMT decoder to further improve performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_84",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_84@1",
            "content": "Edunov et al. (2020) show that back-translation improves translation quality of both naturally occurring text and translationese according professional human translators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_84",
            "start": 146,
            "end": 315,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_84@2",
            "content": "For supervision of learning corpus, Wu et al. (2019) study both the source-side and target-side monolingual data for NMT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_84",
            "start": 317,
            "end": 437,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_85@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_85",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_86@0",
            "content": "In this work, we answer a fundamental question about synthetic data for back translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_86",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_86@1",
            "content": "We theoretically and empirically show two key factors namely quality and importance weight of synthetic data play an important role in back translation, and then we propose a new method to generate synthetic data which better balances both factors so as to boost the back-translation performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_86",
            "start": 90,
            "end": 385,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_86@2",
            "content": "For future work, we think it would be of significance to apply our synthetic data generation method to other BT methods or even to more broad NLP tasks such as paraphrasing and style transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_86",
            "start": 387,
            "end": 578,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_87@0",
            "content": "Mikel Artetxe, Gorka Labaka, Eneko Agirre, Kyunghyun Cho, Unsupervised neural machine translation, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_87",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_88@0",
            "content": "UNKNOWN, None, 2014, Neural machine translation by jointly learning to align and translate, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_88",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_89@0",
            "content": "Lo\u00efc Barrault, Ond\u0159ej Bojar, Marta Costa-Juss\u00e0, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Matthias Huck, Philipp Koehn, Shervin Malmasi, Christof Monz, Mathias M\u00fcller, Findings of the 2019 conference on machine translation (WMT19), 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_89",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_90@0",
            "content": "Nicola Bertoldi, Marcello Federico, Domain adaptation for statistical machine translation with monolingual resources, 2009, Proceedings of the fourth workshop on statistical machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_90",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_91@0",
            "content": "Wei Bi, Huayang Li, Jiacheng Huang, Data augmentation for text generation without any augmented data, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_91",
            "start": 0,
            "end": 321,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_92@0",
            "content": "Ond\u0159ej Bojar, Ale\u0161 Tamchyna, Improving translation model by monolingual data, 2011, Proceedings of the Sixth Workshop on Statistical Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_92",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_93@0",
            "content": "Isaac Caswell, Ciprian Chelba, David Grangier, Tagged back-translation, 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_93",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_94@0",
            "content": "Anna Currey, Antonio Valerio Miceli, Kenneth Barone,  Heafield, Copied monolingual data improves low-resource neural machine translation, 2017, Proceedings of the Second Conference on Machine Translation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_94",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_95@0",
            "content": "Sergey Edunov, Myle Ott, Michael Auli, David Grangier, Understanding back-translation at scale, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_95",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_96@0",
            "content": "Sergey Edunov, Myle Ott, Marc'aurelio Ranzato, Michael Auli, On the evaluation of machine translation systems trained with back-translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_96",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_97@0",
            "content": "Marzieh Fadaee, Arianna Bisazza, Christof Monz, Data augmentation for low-resource neural machine translation, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_97",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_98@0",
            "content": "Marzieh Fadaee, Christof Monz, Backtranslation sampling by targeting difficult words in neural machine translation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_98",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_99@0",
            "content": "Zhihao Fan, Yeyun Gong, Dayiheng Liu, Zhongyu Wei, Siyuan Wang, Jian Jiao, Nan Duan, Ruofei Zhang, Xuan-Jing Huang, Mask attention networks: Rethinking and strengthen transformer, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_99",
            "start": 0,
            "end": 330,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_100@0",
            "content": "Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, Tieyan Liu, Representation degeneration problem in training natural language generation models, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_100",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_101@0",
            "content": "Miguel Gra\u00e7a, Yunsu Kim, Julian Schamper, Shahram Khadivi, Hermann Ney, Generalizing back-translation in neural machine translation, 2019, Proceedings of the Fourth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_101",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_102@0",
            "content": "UNKNOWN, None, 2015, On using monolingual corpora in neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_102",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_103@0",
            "content": "Caglar Gulcehre, Orhan Firat, Kelvin Xu, Kyunghyun Cho, Yoshua Bengio, On integrating a language model into neural machine translation, 2017, Computer Speech & Language, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_103",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_104@0",
            "content": "Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, Wei-Ying Ma, Dual learning for machine translation, 2016, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_104",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_105@0",
            "content": "Duy Vu Cong, Philipp Hoang, Gholamreza Koehn, Trevor Haffari,  Cohn, Iterative backtranslation for neural machine translation, 2018, Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_105",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_106@0",
            "content": "Kenji Imamura, Atsushi Fujita, Eiichiro Sumita, Enhancement of encoder and attention using target monolingual corpora in neural machine translation, 2018, Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_106",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_107@0",
            "content": "P Diederik, Jimmy Kingma,  Ba, Adam: A method for stochastic optimization, 2015, ICLR (Poster), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_107",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_108@0",
            "content": "Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u0159ej Bojar, Alexandra Constantin, Evan Herbst, Moses: Open source toolkit for statistical machine translation, 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_108",
            "start": 0,
            "end": 480,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_109@0",
            "content": "Patrik Lambert, Holger Schwenk, Christophe Servan, Sadaf Abdul-Rauf, Investigations on translation model adaptation using monolingual data, 2011, Sixth Workshop on Statistical Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_109",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_110@0",
            "content": "Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc'aurelio Ranzato, Phrasebased & neural unsupervised machine translation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_110",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_111@0",
            "content": "Giang Le, Shinka Mori, Lane Schwartz, Illinois Japanese \u2194 English News Translation for WMT 2021, 2021, Proceedings of the Sixth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_111",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_112@0",
            "content": "Guanlin Li, Lemao Liu, Guoping Huang, Conghui Zhu, Tiejun Zhao, Understanding data augmentation in neural machine translation: Two perspectives towards generalization, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_112",
            "start": 0,
            "end": 351,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_113@0",
            "content": "Jierui Li, Lemao Liu, Huayang Li, Guanlin Li, Guoping Huang, Shuming Shi, Evaluating explanation methods for neural machine translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_113",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_114@0",
            "content": "Xintong Li, Guanlin Li, Lemao Liu, Max Meng, Shuming Shi, On the word alignment from neural machine translation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_114",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_115@0",
            "content": "S Jun, Rong Liu, Wing Hung Chen,  Wong, Rejection control and sequential importance sampling, 1998, Journal of the American Statistical Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_115",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_116@0",
            "content": "UNKNOWN, None, 2001, Monte Carlo strategies in scientific computing, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_116",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_117@0",
            "content": "Jonathan Mallinson, Rico Sennrich, Mirella Lapata, Paraphrasing revisited with neural machine translation, 2017, Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_117",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_118@0",
            "content": "UNKNOWN, None, 2020, Data diversification: A simple strategy for neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_118",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_119@0",
            "content": "Mohammad Norouzi, Samy Bengio, Navdeep Zhifeng Chen, Mike Jaitly, Yonghui Schuster, Dale Wu,  Schuurmans, Reward augmented maximum likelihood for neural structured prediction, 2016, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_119",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_120@0",
            "content": "Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, Michael Auli, fairseq: A fast, extensible toolkit for sequence modeling, 2019, Proceedings of NAACL-HLT 2019: Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_120",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_121@0",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_121",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_122@0",
            "content": "Yulia Shrimai Prabhumoye, Ruslan Tsvetkov, Alan Salakhutdinov,  Black, Style transfer through back-translation, 2018, Proceedings of the 56th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_122",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_123@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_123",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_124@0",
            "content": "UNKNOWN, None, 2018, Improving language understanding by generative pre-training, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_124",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_125@0",
            "content": "UNKNOWN, None, 2018, Do cifar-10 classifiers generalize to cifar-10?, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_125",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_126@0",
            "content": "Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, Do imagenet classifiers generalize to imagenet, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_126",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_127@0",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, Comet: A neural framework for mt evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_127",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_128@0",
            "content": "UNKNOWN, None, 2020, Decoding and diversity in machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_128",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_129@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Improving neural machine translation models with monolingual data, 2016, Proceedings of the 54th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_129",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_130@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_130",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_131@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_131",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_132@0",
            "content": "Ilya Sutskever, Oriol Vinyals, Quoc V Le, Sequence to sequence learning with neural networks, 2014, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_132",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_133@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_133",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_134@0",
            "content": "Shuo Wang, Yang Liu, Chao Wang, Huanbo Luan, Maosong Sun, Improving back-translation with uncertainty-based confidence estimation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_134",
            "start": 0,
            "end": 314,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_135@0",
            "content": "Xinyi Wang, Hieu Pham, Zihang Dai, Graham Neubig, Switchout: an efficient data augmentation algorithm for neural machine translation, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_135",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_136@0",
            "content": "Roman Werpachowski, Andr\u00e1s Gy\u00f6rgy, Csaba Szepesv\u00e1ri, Detecting overfitting via adversarial examples, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_136",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_137@0",
            "content": "Lijun Wu, Yiren Wang, Yingce Xia, Tao Qin, Jianhuang Lai, Tie-Yan Liu, Exploiting monolingual data at scale for neural machine translation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_137",
            "start": 0,
            "end": 323,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_138@0",
            "content": "Jiajun Zhang, Chengqing Zong, Exploiting source-side monolingual data in neural machine translation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_138",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_139@0",
            "content": "UNKNOWN, None, 2018, Style transfer as unsupervised machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_139",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "414-ARR_v2_140@0",
            "content": "Jinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin, Wengang Zhou, Houqiang Li, Tieyan Liu, Incorporating bert into neural machine translation, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "414-ARR_v2_140",
            "start": 0,
            "end": 201,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "414-ARR_v2_0",
            "tgt_ix": "414-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_0",
            "tgt_ix": "414-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_1",
            "tgt_ix": "414-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_1",
            "tgt_ix": "414-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_0",
            "tgt_ix": "414-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_2",
            "tgt_ix": "414-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_4",
            "tgt_ix": "414-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_5",
            "tgt_ix": "414-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_6",
            "tgt_ix": "414-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_7",
            "tgt_ix": "414-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_8",
            "tgt_ix": "414-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_9",
            "tgt_ix": "414-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_10",
            "tgt_ix": "414-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_3",
            "tgt_ix": "414-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_3",
            "tgt_ix": "414-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_3",
            "tgt_ix": "414-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_3",
            "tgt_ix": "414-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_3",
            "tgt_ix": "414-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_3",
            "tgt_ix": "414-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_3",
            "tgt_ix": "414-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_3",
            "tgt_ix": "414-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_3",
            "tgt_ix": "414-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_0",
            "tgt_ix": "414-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_11",
            "tgt_ix": "414-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_13",
            "tgt_ix": "414-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_14",
            "tgt_ix": "414-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_15",
            "tgt_ix": "414-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_16",
            "tgt_ix": "414-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_17",
            "tgt_ix": "414-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_18",
            "tgt_ix": "414-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_19",
            "tgt_ix": "414-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_20",
            "tgt_ix": "414-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_21",
            "tgt_ix": "414-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_0",
            "tgt_ix": "414-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_22",
            "tgt_ix": "414-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_23",
            "tgt_ix": "414-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_23",
            "tgt_ix": "414-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_23",
            "tgt_ix": "414-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_24",
            "tgt_ix": "414-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_26",
            "tgt_ix": "414-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_27",
            "tgt_ix": "414-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_28",
            "tgt_ix": "414-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_30",
            "tgt_ix": "414-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_31",
            "tgt_ix": "414-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_25",
            "tgt_ix": "414-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_25",
            "tgt_ix": "414-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_25",
            "tgt_ix": "414-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_25",
            "tgt_ix": "414-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_25",
            "tgt_ix": "414-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_25",
            "tgt_ix": "414-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_25",
            "tgt_ix": "414-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_25",
            "tgt_ix": "414-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_23",
            "tgt_ix": "414-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_32",
            "tgt_ix": "414-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_34",
            "tgt_ix": "414-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_33",
            "tgt_ix": "414-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_33",
            "tgt_ix": "414-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_33",
            "tgt_ix": "414-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_0",
            "tgt_ix": "414-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_35",
            "tgt_ix": "414-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_36",
            "tgt_ix": "414-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_36",
            "tgt_ix": "414-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_36",
            "tgt_ix": "414-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_37",
            "tgt_ix": "414-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_39",
            "tgt_ix": "414-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_40",
            "tgt_ix": "414-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_41",
            "tgt_ix": "414-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_42",
            "tgt_ix": "414-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_38",
            "tgt_ix": "414-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_38",
            "tgt_ix": "414-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_38",
            "tgt_ix": "414-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_38",
            "tgt_ix": "414-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_38",
            "tgt_ix": "414-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_38",
            "tgt_ix": "414-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_36",
            "tgt_ix": "414-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_43",
            "tgt_ix": "414-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_45",
            "tgt_ix": "414-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_46",
            "tgt_ix": "414-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_47",
            "tgt_ix": "414-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_48",
            "tgt_ix": "414-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_49",
            "tgt_ix": "414-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_50",
            "tgt_ix": "414-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_51",
            "tgt_ix": "414-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_52",
            "tgt_ix": "414-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_53",
            "tgt_ix": "414-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_54",
            "tgt_ix": "414-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_55",
            "tgt_ix": "414-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_56",
            "tgt_ix": "414-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_0",
            "tgt_ix": "414-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_57",
            "tgt_ix": "414-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_58",
            "tgt_ix": "414-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_58",
            "tgt_ix": "414-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_60",
            "tgt_ix": "414-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_61",
            "tgt_ix": "414-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_59",
            "tgt_ix": "414-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_59",
            "tgt_ix": "414-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_59",
            "tgt_ix": "414-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_59",
            "tgt_ix": "414-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_58",
            "tgt_ix": "414-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_62",
            "tgt_ix": "414-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_58",
            "tgt_ix": "414-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_63",
            "tgt_ix": "414-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_65",
            "tgt_ix": "414-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_66",
            "tgt_ix": "414-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_67",
            "tgt_ix": "414-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_68",
            "tgt_ix": "414-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_64",
            "tgt_ix": "414-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_64",
            "tgt_ix": "414-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_64",
            "tgt_ix": "414-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_64",
            "tgt_ix": "414-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_64",
            "tgt_ix": "414-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_64",
            "tgt_ix": "414-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_58",
            "tgt_ix": "414-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_69",
            "tgt_ix": "414-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_71",
            "tgt_ix": "414-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_70",
            "tgt_ix": "414-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_70",
            "tgt_ix": "414-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_70",
            "tgt_ix": "414-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_58",
            "tgt_ix": "414-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_72",
            "tgt_ix": "414-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_74",
            "tgt_ix": "414-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_73",
            "tgt_ix": "414-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_73",
            "tgt_ix": "414-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_73",
            "tgt_ix": "414-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_76",
            "tgt_ix": "414-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_77",
            "tgt_ix": "414-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_73",
            "tgt_ix": "414-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_73",
            "tgt_ix": "414-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_73",
            "tgt_ix": "414-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_75",
            "tgt_ix": "414-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_0",
            "tgt_ix": "414-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_78",
            "tgt_ix": "414-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_80",
            "tgt_ix": "414-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_81",
            "tgt_ix": "414-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_82",
            "tgt_ix": "414-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_83",
            "tgt_ix": "414-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_79",
            "tgt_ix": "414-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_79",
            "tgt_ix": "414-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_79",
            "tgt_ix": "414-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_79",
            "tgt_ix": "414-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_79",
            "tgt_ix": "414-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_79",
            "tgt_ix": "414-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_0",
            "tgt_ix": "414-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_84",
            "tgt_ix": "414-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_85",
            "tgt_ix": "414-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_85",
            "tgt_ix": "414-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "414-ARR_v2_0",
            "tgt_ix": "414-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_1",
            "tgt_ix": "414-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_2",
            "tgt_ix": "414-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_2",
            "tgt_ix": "414-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_2",
            "tgt_ix": "414-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_2",
            "tgt_ix": "414-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_2",
            "tgt_ix": "414-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_2",
            "tgt_ix": "414-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_2",
            "tgt_ix": "414-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_3",
            "tgt_ix": "414-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_4",
            "tgt_ix": "414-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_4",
            "tgt_ix": "414-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_5",
            "tgt_ix": "414-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_5",
            "tgt_ix": "414-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_5",
            "tgt_ix": "414-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_5",
            "tgt_ix": "414-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_6",
            "tgt_ix": "414-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_6",
            "tgt_ix": "414-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_6",
            "tgt_ix": "414-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_6",
            "tgt_ix": "414-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_7",
            "tgt_ix": "414-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_7",
            "tgt_ix": "414-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_7",
            "tgt_ix": "414-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_7",
            "tgt_ix": "414-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_7",
            "tgt_ix": "414-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_7",
            "tgt_ix": "414-ARR_v2_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_7",
            "tgt_ix": "414-ARR_v2_7@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_7",
            "tgt_ix": "414-ARR_v2_7@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_8",
            "tgt_ix": "414-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_9",
            "tgt_ix": "414-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_10",
            "tgt_ix": "414-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_11",
            "tgt_ix": "414-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_12",
            "tgt_ix": "414-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_13",
            "tgt_ix": "414-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_14",
            "tgt_ix": "414-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_15",
            "tgt_ix": "414-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_15",
            "tgt_ix": "414-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_15",
            "tgt_ix": "414-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_16",
            "tgt_ix": "414-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_16",
            "tgt_ix": "414-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_17",
            "tgt_ix": "414-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_17",
            "tgt_ix": "414-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_18",
            "tgt_ix": "414-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_19",
            "tgt_ix": "414-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_19",
            "tgt_ix": "414-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_19",
            "tgt_ix": "414-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_20",
            "tgt_ix": "414-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_21",
            "tgt_ix": "414-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_21",
            "tgt_ix": "414-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_22",
            "tgt_ix": "414-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_22",
            "tgt_ix": "414-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_23",
            "tgt_ix": "414-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_24",
            "tgt_ix": "414-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_24",
            "tgt_ix": "414-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_25",
            "tgt_ix": "414-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_26",
            "tgt_ix": "414-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_26",
            "tgt_ix": "414-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_27",
            "tgt_ix": "414-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_28",
            "tgt_ix": "414-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_28",
            "tgt_ix": "414-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_28",
            "tgt_ix": "414-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_29",
            "tgt_ix": "414-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_30",
            "tgt_ix": "414-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_31",
            "tgt_ix": "414-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_31",
            "tgt_ix": "414-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_31",
            "tgt_ix": "414-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_31",
            "tgt_ix": "414-ARR_v2_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_32",
            "tgt_ix": "414-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_32",
            "tgt_ix": "414-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_33",
            "tgt_ix": "414-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_34",
            "tgt_ix": "414-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_34",
            "tgt_ix": "414-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_34",
            "tgt_ix": "414-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_34",
            "tgt_ix": "414-ARR_v2_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_34",
            "tgt_ix": "414-ARR_v2_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_35",
            "tgt_ix": "414-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_35",
            "tgt_ix": "414-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_35",
            "tgt_ix": "414-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_35",
            "tgt_ix": "414-ARR_v2_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_36",
            "tgt_ix": "414-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_37",
            "tgt_ix": "414-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_37",
            "tgt_ix": "414-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_37",
            "tgt_ix": "414-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_37",
            "tgt_ix": "414-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_38",
            "tgt_ix": "414-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_39",
            "tgt_ix": "414-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_39",
            "tgt_ix": "414-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_40",
            "tgt_ix": "414-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_40",
            "tgt_ix": "414-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_41",
            "tgt_ix": "414-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_42",
            "tgt_ix": "414-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_42",
            "tgt_ix": "414-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_43",
            "tgt_ix": "414-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_43",
            "tgt_ix": "414-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_43",
            "tgt_ix": "414-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_44",
            "tgt_ix": "414-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_45",
            "tgt_ix": "414-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_46",
            "tgt_ix": "414-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_47",
            "tgt_ix": "414-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_47",
            "tgt_ix": "414-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_48",
            "tgt_ix": "414-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_48",
            "tgt_ix": "414-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_49",
            "tgt_ix": "414-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_49",
            "tgt_ix": "414-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_49",
            "tgt_ix": "414-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_49",
            "tgt_ix": "414-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_50",
            "tgt_ix": "414-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_51",
            "tgt_ix": "414-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_52",
            "tgt_ix": "414-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_53",
            "tgt_ix": "414-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_54",
            "tgt_ix": "414-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_54",
            "tgt_ix": "414-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_55",
            "tgt_ix": "414-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_56",
            "tgt_ix": "414-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_57",
            "tgt_ix": "414-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_57",
            "tgt_ix": "414-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_57",
            "tgt_ix": "414-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_58",
            "tgt_ix": "414-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_59",
            "tgt_ix": "414-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_60",
            "tgt_ix": "414-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_60",
            "tgt_ix": "414-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_60",
            "tgt_ix": "414-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_60",
            "tgt_ix": "414-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_60",
            "tgt_ix": "414-ARR_v2_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_61",
            "tgt_ix": "414-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_61",
            "tgt_ix": "414-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_61",
            "tgt_ix": "414-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_61",
            "tgt_ix": "414-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_62",
            "tgt_ix": "414-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_62",
            "tgt_ix": "414-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_62",
            "tgt_ix": "414-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_62",
            "tgt_ix": "414-ARR_v2_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_62",
            "tgt_ix": "414-ARR_v2_62@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_62",
            "tgt_ix": "414-ARR_v2_62@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_62",
            "tgt_ix": "414-ARR_v2_62@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_63",
            "tgt_ix": "414-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_64",
            "tgt_ix": "414-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_65",
            "tgt_ix": "414-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_66",
            "tgt_ix": "414-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_66",
            "tgt_ix": "414-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_66",
            "tgt_ix": "414-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_66",
            "tgt_ix": "414-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_67",
            "tgt_ix": "414-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_67",
            "tgt_ix": "414-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_68",
            "tgt_ix": "414-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_68",
            "tgt_ix": "414-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_69",
            "tgt_ix": "414-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_70",
            "tgt_ix": "414-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_71",
            "tgt_ix": "414-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_71",
            "tgt_ix": "414-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_71",
            "tgt_ix": "414-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_71",
            "tgt_ix": "414-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_72",
            "tgt_ix": "414-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_72",
            "tgt_ix": "414-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_72",
            "tgt_ix": "414-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_73",
            "tgt_ix": "414-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_74",
            "tgt_ix": "414-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_75",
            "tgt_ix": "414-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_75",
            "tgt_ix": "414-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_75",
            "tgt_ix": "414-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_76",
            "tgt_ix": "414-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_76",
            "tgt_ix": "414-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_76",
            "tgt_ix": "414-ARR_v2_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_77",
            "tgt_ix": "414-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_77",
            "tgt_ix": "414-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_77",
            "tgt_ix": "414-ARR_v2_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_78",
            "tgt_ix": "414-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_78",
            "tgt_ix": "414-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_78",
            "tgt_ix": "414-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_79",
            "tgt_ix": "414-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_80",
            "tgt_ix": "414-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_81",
            "tgt_ix": "414-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_81",
            "tgt_ix": "414-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_82",
            "tgt_ix": "414-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_82",
            "tgt_ix": "414-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_82",
            "tgt_ix": "414-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_82",
            "tgt_ix": "414-ARR_v2_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_82",
            "tgt_ix": "414-ARR_v2_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_82",
            "tgt_ix": "414-ARR_v2_82@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_82",
            "tgt_ix": "414-ARR_v2_82@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_82",
            "tgt_ix": "414-ARR_v2_82@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_83",
            "tgt_ix": "414-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_83",
            "tgt_ix": "414-ARR_v2_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_83",
            "tgt_ix": "414-ARR_v2_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_84",
            "tgt_ix": "414-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_84",
            "tgt_ix": "414-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_84",
            "tgt_ix": "414-ARR_v2_84@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_85",
            "tgt_ix": "414-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_86",
            "tgt_ix": "414-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_86",
            "tgt_ix": "414-ARR_v2_86@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_86",
            "tgt_ix": "414-ARR_v2_86@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_87",
            "tgt_ix": "414-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_88",
            "tgt_ix": "414-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_89",
            "tgt_ix": "414-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_90",
            "tgt_ix": "414-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_91",
            "tgt_ix": "414-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_92",
            "tgt_ix": "414-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_93",
            "tgt_ix": "414-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_94",
            "tgt_ix": "414-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_95",
            "tgt_ix": "414-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_96",
            "tgt_ix": "414-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_97",
            "tgt_ix": "414-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_98",
            "tgt_ix": "414-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_99",
            "tgt_ix": "414-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_100",
            "tgt_ix": "414-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_101",
            "tgt_ix": "414-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_102",
            "tgt_ix": "414-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_103",
            "tgt_ix": "414-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_104",
            "tgt_ix": "414-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_105",
            "tgt_ix": "414-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_106",
            "tgt_ix": "414-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_107",
            "tgt_ix": "414-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_108",
            "tgt_ix": "414-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_109",
            "tgt_ix": "414-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_110",
            "tgt_ix": "414-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_111",
            "tgt_ix": "414-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_112",
            "tgt_ix": "414-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_113",
            "tgt_ix": "414-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_114",
            "tgt_ix": "414-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_115",
            "tgt_ix": "414-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_116",
            "tgt_ix": "414-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_117",
            "tgt_ix": "414-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_118",
            "tgt_ix": "414-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_119",
            "tgt_ix": "414-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_120",
            "tgt_ix": "414-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_121",
            "tgt_ix": "414-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_122",
            "tgt_ix": "414-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_123",
            "tgt_ix": "414-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_124",
            "tgt_ix": "414-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_125",
            "tgt_ix": "414-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_126",
            "tgt_ix": "414-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_127",
            "tgt_ix": "414-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_128",
            "tgt_ix": "414-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_129",
            "tgt_ix": "414-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_130",
            "tgt_ix": "414-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_131",
            "tgt_ix": "414-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_132",
            "tgt_ix": "414-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_133",
            "tgt_ix": "414-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_134",
            "tgt_ix": "414-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_135",
            "tgt_ix": "414-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_136",
            "tgt_ix": "414-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_137",
            "tgt_ix": "414-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_138",
            "tgt_ix": "414-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_139",
            "tgt_ix": "414-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "414-ARR_v2_140",
            "tgt_ix": "414-ARR_v2_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 810,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "414-ARR",
        "version": 2
    }
}