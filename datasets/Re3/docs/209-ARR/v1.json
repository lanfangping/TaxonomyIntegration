{
    "nodes": [
        {
            "ix": "209-ARR_v1_0",
            "content": "Building Multilingual Machine Translation Systems That Serve Arbitrary XY Translations",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_2",
            "content": "Multilingual Neural Machine Translation (MNMT) enables one system to translate sentences from multiple source languages to multiple target languages, greatly reducing deployment costs compared with conventional bilingual systems. The MNMT training benefit, however, is often limited to many-to-one directions. The model suffers from poor performance in one-to-many and zero-shot directions. To address the issue, this paper discusses how to practically build MNMT systems that serve arbitrary XY translation directions while leveraging multilinguality with the two-stage training strategy of pretraining and finetuning. Experimenting in the WMT'21 multilingual translation task, we demonstrate that our systems outperform the conventional baselines of direct bilingual models and pivot translation models for most directions, averagely giving +6.0 and +4.1 BLEU, without the need for architecture change or extra data collection. Moreover, we also examine our proposed approach in an extremely large-scale data setting to accommodate practical deployment scenarios.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "209-ARR_v1_4",
            "content": "Multilingual Neural Machine Translation (MNMT) has attracted much attention in the machine translation area, enabling one system to serve translation for multiple directions (Zoph and Knight, 2016;Firat et al., 2016). Because the multilingual capability hugely reduces the deployment cost at training and inference, the MNMT has actively been employed as a machine translation system backbone in recent years (Johnson et al., 2017;Hassan et al., 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_5",
            "content": "Most MNMT systems are trained with multiple English-centric data for both directions (e.g. for English \u2192 {French, Chinese} (EX) and {French, Chi-nese} \u2192 English (XE)). However, recent work (Gu et al., 2019;Zhang et al., 2020;Yang et al., 2021) pointed out that such MNMT systems severely face an off-target translation issue, especially in translations from a non-English language X to another non-English language Y. In Freitag and Firat (2020), the authors have extended data resources with multi-way aligned data and reported that one complete many-to-many MNMT can be fully supervised, achieving competitive translation performance for all XY directions. In our preliminary experiment, we observed that the complete manyto-many training is still as challenging as one-tomany training (Johnson et al., 2017;Wang et al., 2020). The model suffers from handling many diverse languages, in contrast to successful manyto-one translation (Johnson et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_6",
            "content": "In this paper, we propose a two-stage training for complete MNMT systems that serve arbitrary XY translations by 1) pretraining a complete multilingual many-to-many model and 2) finetuning the model to effectively transferring knowledge from the complete multilingual training. Considering that MNMT is a multi-task learner of translation task with \"multiple languages\", the complete multilingual model learns more diverse and general multilingual representations. We transfer the representations to a specifically targeted task via manyto-one multilingual finetuning, and eventually build multiple many-to-one MNMT models that cover all XY directions. Experimenting on multilingual translation tasks at WMT'21, we have confirmed that our systems show substantial improvement against the conventional bilingual approaches for most directions. Besides that, we discuss our proposal in the light of feasible deployment scenarios and show that the proposed approach also works well in an extremely large-scale data settings and .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_7",
            "content": "Two-Stage Training for MNMT Models",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "209-ARR_v1_8",
            "content": "To support all possible translations with |L| languages (including English), we first train a complete MNMT system on all available parallel data for |L| \u00d7 (|L| \u2212 1) directions. We assume that (top) and Task 2 (bottom), with the respective improvement of (Base, BaseFT, Big, BigFT) = (+3.6, +4.7, +5.0, +6.0) and (+2.0, +2.9, +9.2 +3.2, +4.1) against the bilingual baseline (\"Bi\") and the pivot translation baslines (\"Pivot\"). \"Base / Big\" denote our two settings, with the suffix \"FT\" for finetuned systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_9",
            "content": "there exist data of (|L| \u2212 1) English-centric language pairs and remaining (|L|\u22121)\u00d7(|L|\u22122) 2 non-English-centric language pairs, which lets the system learn multilingual representations across all |L| languages. Usually, the volume of English-centric data is much greater than non-English-centric one. Then, we transfer the multilingual representations by finetuning the system on the training data subset for XL directions (i.e., multilingual many-to-one finetuning). This step leads the decoder towards the specifically targeted language L rather than multiple languages. As a result, we obtain |L| multilingual many-to-one systems to serve all XY translation directions. We experiment with our proposed approach in two different settings using 1) WMT'21 large-scale multilingual translation data with 487M training data, and 2) our in-house production-scale dataset with 3.7B training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_10",
            "content": "WMT'21 Multilingual Translation Task",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "209-ARR_v1_11",
            "content": "We experiment with two small tasks provided at WMT'21 large-scale multilingual translation task 1 . The tasks provide multilingual multi-way parallel data 2 from the Flores 101 data, with (Englishcentric, Non-English-centric)=(321M, 166M) in total. The data size per direction varies in a range of 0.07M-83.9M. To balance the data distribution across languages (Kudugunta et al., 2019), we upsample the low-resource languages with tempera-ture=5. We append language ID tokens at the end of source sentences to specify a target language (Johnson et al., 2017). We tokenize the data with the SentencePiece (Kudo and Richardson, 2018) and build a shared vocabulary with 64k tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_12",
            "content": "We train Transformer models Base and Big (Vaswani et al., 2017) in a complete multilingual many-to-many fashion, respectively. The model parameters are optimized by using RAdam (Liu et al., 2020) with an initial learning rate of 0.025, and warm-up steps of 10k and 30k for the Base and Big model training, respectively. The systems are pretrained on 64 V100 GPUs with a mini-batch size of 3072 tokens and graduation accumulation of 16. After pretraining, the models are finetuned on a subset of XL training data. We tune the model parameters gently on 8 V100 GPUs with the same mini-batch size, graduation accumulations, and the same optimizer with different learning rate scheduling of (init_lr, warm-up steps)=({1e-4, 1e-5, 1e-6}, 8k). The best checkpoints are selected based on development loss. The translations are obtained by a beam search decoding with a beam size of 4, unless otherwise stated.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_13",
            "content": "Baselines For system comparison, we build two different baselines: 1) a direct bilingual system and 2) a pivot translation system via English (only applicable for non-English XY evaluation). Both are based on the Transformer Base architecture. The embedding dimension is set to 256 for jv, ms, ta, and tl, because of the training data scarcity. For the XY pivot translation, a source sentence in language X is translated to English with a beam size of 5 by the XE model, then the best output is translated to the final target language Y by the EY model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_14",
            "content": "All results on the test sets are displayed in Figure 1, where we report the case-sensitive sacreBLEU score (Post, 2018) for translation accuracy. Our best systems (\"BigFT\") are significantly better by \u2265 +0.5 sacreBLEU for 83% and 88% directions against the bilingual baselines and the pivot translation baselines, respectively. However, building a larger and larger model is not always feasible. We often have limitations in the computational resources at inference time, which leads to a trade-off problem between the performance and the decoding cost caused by the model architecture.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_15",
            "content": "In the following section, we validate our proposed approach in an extremely large-scale data setting and also discuss how we can build lighter MNMT models without the performance loss.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_16",
            "content": "In-house Extremely Large-Scale Setting",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "209-ARR_v1_17",
            "content": "We validate our proposed approach in an extremely large-scale setting, while briefly touching the following three topics of 1) multi-way multilingual data collection, 2) English-centric vs. multi-centric pretraining for better XY, and 3) a lighter MNMT model that addresses the trade-off issue between performance and latency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_18",
            "content": "We build an extremely large-scale data set using our in-house English-centric data set, consisting of 10 European languages, ranging 19M-187M sentences per language 3 . From these English-centric data, we extract a multi-way multilingual XY data, by aligning EX and EY data via pivoting English. Specifically, we extracted {de, fr, es, it, pl}-centric data and concatenate them to the existing direct XY data, providing 24M-192M per direction. Similarly as in Section 2.1, we build a shared SentencePiece vocabulary with 128k tokens to address the largescale setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_19",
            "content": "In a large-scale data setting, a question might come up which pretrained model provides generalized multilingual representations to achieve better XY translation quality? Considering the dominant text data is usually English, e.g., 70% tasks are English-centric in the WMT'21 News translation task, the model supervised on English-centric corpora might learn representations enough to transfer for XY translations. To investigate the usefulness of the multi-centric data training, we pretrain Transformer Big models with deeper 24-12 layers on the English-centric data and the L-centric data (L={en,de,fr}), individually. After pretraining, we apply the multilingual many-to-one finetuning with a subset of the training data and evaluate each system for the fully supervised XY directions, i.e. xx-{en,de,fr}, and the partially supervised XY directions, i.e. xx-{es,it,pl}. We followed the same training and finetuning settings as described in Section 2.1, unless otherwise stated.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_20",
            "content": "MNMT with Light Decoder At the practical level, one drawback of the large-scale models would be latency at inference time. This is mostly caused by the high computational cost in the decoder layers due to auto-regressive models and the extra cross-attention network in each block of the decoder. Recent studies (Kasai et al., 2020;Hsu et al., 2020;Li et al., 2021) have experimentally shown that models with a deep encoder and a shallow decoder can address the the issue, without losing much performance. Fortunately, such an architecture also satisfies demands of the many-to-one MNMT training, which requires the encoder networks to be more complex to handle various source languages. To examine the light MNMT model architecture, we train the Transformer Base architecture modified with 9-3 layers (E9D3) in a bilingual setting and compare it with a standard Transformer Base model, with 6-6 layers (E6D6), as a baseline. Additionally, we also report direct XY translation performance, when distilling the best large-scale models alongside the light MNMT models as a student model. More specifically, following Kim and Rush (2016), we train five light MNMT student models (E9D3) that serve many-to-L translations (L={de, fr, es,it,pl}).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_21",
            "content": "Results Table 1 reports average sacreBLEU scores in the in-house XY test sets. For the xx-{de,fr} directions, the proposed finetuning helps both English-centric and multi-centtric pretrained models to improve the accuracy. Over- all, the finetuned multi-centric models achieved the best, largely outperforming the English pivotbased baselines by +2.6 and +2.8 points. The multicentric models surpass the corresponding finetuned English-centric systems with a large margin of +0.9 and +0.8 points. This suggests that, by pretraining a model on more multi-centric data, the model learns better multilinguality to transfer. For the xx-{es,it,pl} directions 4 , each finetuned system gains similar accuracy improvement, significantly outperforming the conventional baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_22",
            "content": "Figure 2 shows the effectiveness of our light NMT model architecture for 5 EX directions, reporting the translation performance in sacreBLEU scores and the latency measured on CPUs. Our light NMT model (E9D3) successfully achieves almost 2x speed up, without much drop of the performance for all directions. Employing this light model architecture as a student model, we report the distilled many-to-one model performance in Table 1, measured by sacreBLEU and COMET (Rei et al., 2020) that are distilled from the bilingual Teachers then obtained the English pivot-based translation performance. For all xx-{de,fr,es,it,pl} directions, our proposed models show the best performance in both metrics. We also note that, at inference time, our direct XY systems save the decoding cost with 75% against the pivot translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_23",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "209-ARR_v1_24",
            "content": "This paper proposes a simple but effective twostage training strategy for MNMT systems that serve arbitrary XY translations. To support translations across languages, we first pretrain a complete multilingual many-to-many model, then transfer the representations via finetuning the model in a many-to-one multilingual fashion. In the WMT'21 translation task, we experimentally showed that the proposed approach substantially improve translation accuracy for most XY directions against the strong conventional baselines of bilingual systems and pivot translation systems. We also examined the proposed approach in the extremely large-scale setting, while addressing the practical questions such as multi-way parallel data collection, the usefulness of multilinguality during the pretraining and finetuning, and how to save the decoding cost, achieving the better XY quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_25",
            "content": "Barret Zoph and Kevin Knight. 2016. Multi-source neural translation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 30-34, San Diego, California. Association for Computational Linguistics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "209-ARR_v1_26",
            "content": "Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan, Fatos Vural, Kyunghyun Cho, Zero-resource translation with multi-lingual neural machine translation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Orhan Firat",
                    "Baskaran Sankaran",
                    "Yaser Al-Onaizan",
                    "Fatos Vural",
                    "Kyunghyun Cho"
                ],
                "title": "Zero-resource translation with multi-lingual neural machine translation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "209-ARR_v1_27",
            "content": "Markus Freitag, Orhan Firat, Complete multilingual neural machine translation, 2020, Proceedings of the Fifth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Markus Freitag",
                    "Orhan Firat"
                ],
                "title": "Complete multilingual neural machine translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_28",
            "content": "Jiatao Gu, Yong Wang, Kyunghyun Cho, O Victor,  Li, Improved zero-shot neural machine translation via ignoring spurious correlations, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jiatao Gu",
                    "Yong Wang",
                    "Kyunghyun Cho",
                    "O Victor",
                    " Li"
                ],
                "title": "Improved zero-shot neural machine translation via ignoring spurious correlations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_29",
            "content": "UNKNOWN, None, 2018, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_30",
            "content": "UNKNOWN, None, 2020, Efficient inference for neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Efficient inference for neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_31",
            "content": "Melvin Johnson, Mike Schuster, Quoc Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean, Google's multilingual neural machine translation system: Enabling zero-shot translation, 2017, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Melvin Johnson",
                    "Mike Schuster",
                    "Quoc Le",
                    "Maxim Krikun",
                    "Yonghui Wu",
                    "Zhifeng Chen",
                    "Nikhil Thorat",
                    "Fernanda Vi\u00e9gas",
                    "Martin Wattenberg",
                    "Greg Corrado",
                    "Macduff Hughes",
                    "Jeffrey Dean"
                ],
                "title": "Google's multilingual neural machine translation system: Enabling zero-shot translation",
                "pub_date": "2017",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_32",
            "content": "UNKNOWN, None, 2020, Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation",
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_33",
            "content": "Yoon Kim, Alexander Rush, Sequencelevel knowledge distillation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Yoon Kim",
                    "Alexander Rush"
                ],
                "title": "Sequencelevel knowledge distillation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "209-ARR_v1_34",
            "content": "Taku Kudo, John Richardson, SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Taku Kudo",
                    "John Richardson"
                ],
                "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "209-ARR_v1_35",
            "content": "Sneha Kudugunta, Ankur Bapna, Isaac Caswell, Orhan Firat, Investigating multilingual NMT representations at scale, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Sneha Kudugunta",
                    "Ankur Bapna",
                    "Isaac Caswell",
                    "Orhan Firat"
                ],
                "title": "Investigating multilingual NMT representations at scale",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "209-ARR_v1_36",
            "content": "UNKNOWN, None, 2021, An efficient transformer decoder with compressed sub-layers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "An efficient transformer decoder with compressed sub-layers",
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_37",
            "content": "Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han, On the variance of the adaptive learning rate and beyond, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Liyuan Liu",
                    "Haoming Jiang",
                    "Pengcheng He",
                    "Weizhu Chen",
                    "Xiaodong Liu",
                    "Jianfeng Gao",
                    "Jiawei Han"
                ],
                "title": "On the variance of the adaptive learning rate and beyond",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_38",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Matt Post"
                ],
                "title": "A call for clarity in reporting BLEU scores",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_39",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, COMET: A neural framework for MT evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Ricardo Rei",
                    "Craig Stewart",
                    "Ana Farinha",
                    "Alon Lavie"
                ],
                "title": "COMET: A neural framework for MT evaluation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_40",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Illia Kaiser",
                    " Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "209-ARR_v1_41",
            "content": "Zirui Wang, Zachary Lipton, Yulia Tsvetkov, On negative interference in multilingual models: Findings and a meta-learning treatment, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Zirui Wang",
                    "Zachary Lipton",
                    "Yulia Tsvetkov"
                ],
                "title": "On negative interference in multilingual models: Findings and a meta-learning treatment",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "209-ARR_v1_42",
            "content": "Yilin Yang, Akiko Eriguchi, Alexandre Muzio, Prasad Tadepalli, Stefan Lee, Hany Hassan, Improving multilingual translation by representation and gradient regularization, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Yilin Yang",
                    "Akiko Eriguchi",
                    "Alexandre Muzio",
                    "Prasad Tadepalli",
                    "Stefan Lee",
                    "Hany Hassan"
                ],
                "title": "Improving multilingual translation by representation and gradient regularization",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "209-ARR_v1_43",
            "content": "Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich, Improving massively multilingual neural machine translation and zero-shot translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Biao Zhang",
                    "Philip Williams",
                    "Ivan Titov",
                    "Rico Sennrich"
                ],
                "title": "Improving massively multilingual neural machine translation and zero-shot translation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "209-ARR_v1_0@0",
            "content": "Building Multilingual Machine Translation Systems That Serve Arbitrary XY Translations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_0",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_2@0",
            "content": "Multilingual Neural Machine Translation (MNMT) enables one system to translate sentences from multiple source languages to multiple target languages, greatly reducing deployment costs compared with conventional bilingual systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_2",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_2@1",
            "content": "The MNMT training benefit, however, is often limited to many-to-one directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_2",
            "start": 230,
            "end": 308,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_2@2",
            "content": "The model suffers from poor performance in one-to-many and zero-shot directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_2",
            "start": 310,
            "end": 389,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_2@3",
            "content": "To address the issue, this paper discusses how to practically build MNMT systems that serve arbitrary XY translation directions while leveraging multilinguality with the two-stage training strategy of pretraining and finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_2",
            "start": 391,
            "end": 618,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_2@4",
            "content": "Experimenting in the WMT'21 multilingual translation task, we demonstrate that our systems outperform the conventional baselines of direct bilingual models and pivot translation models for most directions, averagely giving +6.0 and +4.1 BLEU, without the need for architecture change or extra data collection.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_2",
            "start": 620,
            "end": 928,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_2@5",
            "content": "Moreover, we also examine our proposed approach in an extremely large-scale data setting to accommodate practical deployment scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_2",
            "start": 930,
            "end": 1064,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_4@0",
            "content": "Multilingual Neural Machine Translation (MNMT) has attracted much attention in the machine translation area, enabling one system to serve translation for multiple directions (Zoph and Knight, 2016;Firat et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_4",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_4@1",
            "content": "Because the multilingual capability hugely reduces the deployment cost at training and inference, the MNMT has actively been employed as a machine translation system backbone in recent years (Johnson et al., 2017;Hassan et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_4",
            "start": 218,
            "end": 451,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_5@0",
            "content": "Most MNMT systems are trained with multiple English-centric data for both directions (e.g. for English \u2192 {French, Chinese} (EX) and {French, Chi-nese} \u2192 English (XE)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_5",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_5@1",
            "content": "However, recent work (Gu et al., 2019;Zhang et al., 2020;Yang et al., 2021) pointed out that such MNMT systems severely face an off-target translation issue, especially in translations from a non-English language X to another non-English language Y. In Freitag and Firat (2020), the authors have extended data resources with multi-way aligned data and reported that one complete many-to-many MNMT can be fully supervised, achieving competitive translation performance for all XY directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_5",
            "start": 168,
            "end": 657,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_5@2",
            "content": "In our preliminary experiment, we observed that the complete manyto-many training is still as challenging as one-tomany training (Johnson et al., 2017;Wang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_5",
            "start": 659,
            "end": 828,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_5@3",
            "content": "The model suffers from handling many diverse languages, in contrast to successful manyto-one translation (Johnson et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_5",
            "start": 830,
            "end": 957,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_6@0",
            "content": "In this paper, we propose a two-stage training for complete MNMT systems that serve arbitrary XY translations by 1) pretraining a complete multilingual many-to-many model and 2) finetuning the model to effectively transferring knowledge from the complete multilingual training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_6",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_6@1",
            "content": "Considering that MNMT is a multi-task learner of translation task with \"multiple languages\", the complete multilingual model learns more diverse and general multilingual representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_6",
            "start": 278,
            "end": 463,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_6@2",
            "content": "We transfer the representations to a specifically targeted task via manyto-one multilingual finetuning, and eventually build multiple many-to-one MNMT models that cover all XY directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_6",
            "start": 465,
            "end": 651,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_6@3",
            "content": "Experimenting on multilingual translation tasks at WMT'21, we have confirmed that our systems show substantial improvement against the conventional bilingual approaches for most directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_6",
            "start": 653,
            "end": 841,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_6@4",
            "content": "Besides that, we discuss our proposal in the light of feasible deployment scenarios and show that the proposed approach also works well in an extremely large-scale data settings and .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_6",
            "start": 843,
            "end": 1025,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_7@0",
            "content": "Two-Stage Training for MNMT Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_7",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_8@0",
            "content": "To support all possible translations with |L| languages (including English), we first train a complete MNMT system on all available parallel data for |L| \u00d7 (|L| \u2212 1) directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_8",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_8@1",
            "content": "We assume that (top) and Task 2 (bottom), with the respective improvement of (Base, BaseFT, Big, BigFT) = (+3.6, +4.7, +5.0, +6.0) and (+2.0, +2.9, +9.2 +3.2, +4.1) against the bilingual baseline (\"Bi\") and the pivot translation baslines (\"Pivot\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_8",
            "start": 178,
            "end": 425,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_8@2",
            "content": "\"Base / Big\" denote our two settings, with the suffix \"FT\" for finetuned systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_8",
            "start": 427,
            "end": 507,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_9@0",
            "content": "there exist data of (|L| \u2212 1) English-centric language pairs and remaining (|L|\u22121)\u00d7(|L|\u22122) 2 non-English-centric language pairs, which lets the system learn multilingual representations across all |L| languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_9",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_9@1",
            "content": "Usually, the volume of English-centric data is much greater than non-English-centric one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_9",
            "start": 212,
            "end": 300,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_9@2",
            "content": "Then, we transfer the multilingual representations by finetuning the system on the training data subset for XL directions (i.e., multilingual many-to-one finetuning).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_9",
            "start": 302,
            "end": 467,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_9@3",
            "content": "This step leads the decoder towards the specifically targeted language L rather than multiple languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_9",
            "start": 469,
            "end": 572,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_9@4",
            "content": "As a result, we obtain |L| multilingual many-to-one systems to serve all XY translation directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_9",
            "start": 574,
            "end": 672,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_9@5",
            "content": "We experiment with our proposed approach in two different settings using 1) WMT'21 large-scale multilingual translation data with 487M training data, and 2) our in-house production-scale dataset with 3.7B training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_9",
            "start": 674,
            "end": 892,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_10@0",
            "content": "WMT'21 Multilingual Translation Task",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_10",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_11@0",
            "content": "We experiment with two small tasks provided at WMT'21 large-scale multilingual translation task 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_11",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_11@1",
            "content": "The tasks provide multilingual multi-way parallel data 2 from the Flores 101 data, with (Englishcentric, Non-English-centric)=(321M, 166M) in total.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_11",
            "start": 100,
            "end": 247,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_11@2",
            "content": "The data size per direction varies in a range of 0.07M-83.9M.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_11",
            "start": 249,
            "end": 309,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_11@3",
            "content": "To balance the data distribution across languages (Kudugunta et al., 2019), we upsample the low-resource languages with tempera-ture=5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_11",
            "start": 311,
            "end": 445,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_11@4",
            "content": "We append language ID tokens at the end of source sentences to specify a target language (Johnson et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_11",
            "start": 447,
            "end": 558,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_11@5",
            "content": "We tokenize the data with the SentencePiece (Kudo and Richardson, 2018) and build a shared vocabulary with 64k tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_11",
            "start": 560,
            "end": 677,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_12@0",
            "content": "We train Transformer models Base and Big (Vaswani et al., 2017) in a complete multilingual many-to-many fashion, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_12",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_12@1",
            "content": "The model parameters are optimized by using RAdam (Liu et al., 2020) with an initial learning rate of 0.025, and warm-up steps of 10k and 30k for the Base and Big model training, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_12",
            "start": 127,
            "end": 318,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_12@2",
            "content": "The systems are pretrained on 64 V100 GPUs with a mini-batch size of 3072 tokens and graduation accumulation of 16.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_12",
            "start": 320,
            "end": 434,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_12@3",
            "content": "After pretraining, the models are finetuned on a subset of XL training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_12",
            "start": 436,
            "end": 511,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_12@4",
            "content": "We tune the model parameters gently on 8 V100 GPUs with the same mini-batch size, graduation accumulations, and the same optimizer with different learning rate scheduling of (init_lr, warm-up steps)=({1e-4, 1e-5, 1e-6}, 8k).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_12",
            "start": 513,
            "end": 736,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_12@5",
            "content": "The best checkpoints are selected based on development loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_12",
            "start": 738,
            "end": 797,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_12@6",
            "content": "The translations are obtained by a beam search decoding with a beam size of 4, unless otherwise stated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_12",
            "start": 799,
            "end": 901,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_13@0",
            "content": "Baselines For system comparison, we build two different baselines: 1) a direct bilingual system and 2) a pivot translation system via English (only applicable for non-English XY evaluation).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_13",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_13@1",
            "content": "Both are based on the Transformer Base architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_13",
            "start": 191,
            "end": 242,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_13@2",
            "content": "The embedding dimension is set to 256 for jv, ms, ta, and tl, because of the training data scarcity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_13",
            "start": 244,
            "end": 343,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_13@3",
            "content": "For the XY pivot translation, a source sentence in language X is translated to English with a beam size of 5 by the XE model, then the best output is translated to the final target language Y by the EY model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_13",
            "start": 345,
            "end": 552,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_14@0",
            "content": "All results on the test sets are displayed in Figure 1, where we report the case-sensitive sacreBLEU score (Post, 2018) for translation accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_14",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_14@1",
            "content": "Our best systems (\"BigFT\") are significantly better by \u2265 +0.5 sacreBLEU for 83% and 88% directions against the bilingual baselines and the pivot translation baselines, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_14",
            "start": 146,
            "end": 326,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_14@2",
            "content": "However, building a larger and larger model is not always feasible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_14",
            "start": 328,
            "end": 394,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_14@3",
            "content": "We often have limitations in the computational resources at inference time, which leads to a trade-off problem between the performance and the decoding cost caused by the model architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_14",
            "start": 396,
            "end": 585,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_15@0",
            "content": "In the following section, we validate our proposed approach in an extremely large-scale data setting and also discuss how we can build lighter MNMT models without the performance loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_15",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_16@0",
            "content": "In-house Extremely Large-Scale Setting",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_16",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_17@0",
            "content": "We validate our proposed approach in an extremely large-scale setting, while briefly touching the following three topics of 1) multi-way multilingual data collection, 2) English-centric vs. multi-centric pretraining for better XY, and 3) a lighter MNMT model that addresses the trade-off issue between performance and latency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_17",
            "start": 0,
            "end": 325,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_18@0",
            "content": "We build an extremely large-scale data set using our in-house English-centric data set, consisting of 10 European languages, ranging 19M-187M sentences per language 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_18",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_18@1",
            "content": "From these English-centric data, we extract a multi-way multilingual XY data, by aligning EX and EY data via pivoting English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_18",
            "start": 169,
            "end": 294,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_18@2",
            "content": "Specifically, we extracted {de, fr, es, it, pl}-centric data and concatenate them to the existing direct XY data, providing 24M-192M per direction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_18",
            "start": 296,
            "end": 442,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_18@3",
            "content": "Similarly as in Section 2.1, we build a shared SentencePiece vocabulary with 128k tokens to address the largescale setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_18",
            "start": 444,
            "end": 566,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_19@0",
            "content": "In a large-scale data setting, a question might come up which pretrained model provides generalized multilingual representations to achieve better XY translation quality? Considering the dominant text data is usually English, e.g., 70% tasks are English-centric in the WMT'21 News translation task, the model supervised on English-centric corpora might learn representations enough to transfer for XY translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_19",
            "start": 0,
            "end": 413,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_19@1",
            "content": "To investigate the usefulness of the multi-centric data training, we pretrain Transformer Big models with deeper 24-12 layers on the English-centric data and the L-centric data (L={en,de,fr}), individually.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_19",
            "start": 415,
            "end": 620,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_19@2",
            "content": "After pretraining, we apply the multilingual many-to-one finetuning with a subset of the training data and evaluate each system for the fully supervised XY directions, i.e. xx-{en,de,fr}, and the partially supervised XY directions, i.e. xx-{es,it,pl}. We followed the same training and finetuning settings as described in Section 2.1, unless otherwise stated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_19",
            "start": 622,
            "end": 980,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_20@0",
            "content": "MNMT with Light Decoder At the practical level, one drawback of the large-scale models would be latency at inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_20",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_20@1",
            "content": "This is mostly caused by the high computational cost in the decoder layers due to auto-regressive models and the extra cross-attention network in each block of the decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_20",
            "start": 123,
            "end": 294,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_20@2",
            "content": "Recent studies (Kasai et al., 2020;Hsu et al., 2020;Li et al., 2021) have experimentally shown that models with a deep encoder and a shallow decoder can address the the issue, without losing much performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_20",
            "start": 296,
            "end": 503,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_20@3",
            "content": "Fortunately, such an architecture also satisfies demands of the many-to-one MNMT training, which requires the encoder networks to be more complex to handle various source languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_20",
            "start": 505,
            "end": 685,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_20@4",
            "content": "To examine the light MNMT model architecture, we train the Transformer Base architecture modified with 9-3 layers (E9D3) in a bilingual setting and compare it with a standard Transformer Base model, with 6-6 layers (E6D6), as a baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_20",
            "start": 687,
            "end": 923,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_20@5",
            "content": "Additionally, we also report direct XY translation performance, when distilling the best large-scale models alongside the light MNMT models as a student model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_20",
            "start": 925,
            "end": 1083,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_20@6",
            "content": "More specifically, following Kim and Rush (2016), we train five light MNMT student models (E9D3) that serve many-to-L translations (L={de, fr, es,it,pl}).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_20",
            "start": 1085,
            "end": 1238,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_21@0",
            "content": "Results Table 1 reports average sacreBLEU scores in the in-house XY test sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_21",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_21@1",
            "content": "For the xx-{de,fr} directions, the proposed finetuning helps both English-centric and multi-centtric pretrained models to improve the accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_21",
            "start": 79,
            "end": 221,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_21@2",
            "content": "Over- all, the finetuned multi-centric models achieved the best, largely outperforming the English pivotbased baselines by +2.6 and +2.8 points.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_21",
            "start": 223,
            "end": 366,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_21@3",
            "content": "The multicentric models surpass the corresponding finetuned English-centric systems with a large margin of +0.9 and +0.8 points.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_21",
            "start": 368,
            "end": 495,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_21@4",
            "content": "This suggests that, by pretraining a model on more multi-centric data, the model learns better multilinguality to transfer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_21",
            "start": 497,
            "end": 619,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_21@5",
            "content": "For the xx-{es,it,pl} directions 4 , each finetuned system gains similar accuracy improvement, significantly outperforming the conventional baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_21",
            "start": 621,
            "end": 770,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_22@0",
            "content": "Figure 2 shows the effectiveness of our light NMT model architecture for 5 EX directions, reporting the translation performance in sacreBLEU scores and the latency measured on CPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_22",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_22@1",
            "content": "Our light NMT model (E9D3) successfully achieves almost 2x speed up, without much drop of the performance for all directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_22",
            "start": 182,
            "end": 306,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_22@2",
            "content": "Employing this light model architecture as a student model, we report the distilled many-to-one model performance in Table 1, measured by sacreBLEU and COMET (Rei et al., 2020) that are distilled from the bilingual Teachers then obtained the English pivot-based translation performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_22",
            "start": 308,
            "end": 593,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_22@3",
            "content": "For all xx-{de,fr,es,it,pl} directions, our proposed models show the best performance in both metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_22",
            "start": 595,
            "end": 696,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_22@4",
            "content": "We also note that, at inference time, our direct XY systems save the decoding cost with 75% against the pivot translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_22",
            "start": 698,
            "end": 819,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_23@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_23",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_24@0",
            "content": "This paper proposes a simple but effective twostage training strategy for MNMT systems that serve arbitrary XY translations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_24",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_24@1",
            "content": "To support translations across languages, we first pretrain a complete multilingual many-to-many model, then transfer the representations via finetuning the model in a many-to-one multilingual fashion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_24",
            "start": 125,
            "end": 325,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_24@2",
            "content": "In the WMT'21 translation task, we experimentally showed that the proposed approach substantially improve translation accuracy for most XY directions against the strong conventional baselines of bilingual systems and pivot translation systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_24",
            "start": 327,
            "end": 569,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_24@3",
            "content": "We also examined the proposed approach in the extremely large-scale setting, while addressing the practical questions such as multi-way parallel data collection, the usefulness of multilinguality during the pretraining and finetuning, and how to save the decoding cost, achieving the better XY quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_24",
            "start": 571,
            "end": 872,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_25@0",
            "content": "Barret Zoph and Kevin Knight. 2016.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_25",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_25@1",
            "content": "Multi-source neural translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_25",
            "start": 36,
            "end": 67,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_25@2",
            "content": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 30-34, San Diego, California.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_25",
            "start": 69,
            "end": 250,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_25@3",
            "content": "Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_25",
            "start": 252,
            "end": 293,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_26@0",
            "content": "Orhan Firat, Baskaran Sankaran, Yaser Al-Onaizan, Fatos Vural, Kyunghyun Cho, Zero-resource translation with multi-lingual neural machine translation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_26",
            "start": 0,
            "end": 286,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_27@0",
            "content": "Markus Freitag, Orhan Firat, Complete multilingual neural machine translation, 2020, Proceedings of the Fifth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_27",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_28@0",
            "content": "Jiatao Gu, Yong Wang, Kyunghyun Cho, O Victor,  Li, Improved zero-shot neural machine translation via ignoring spurious correlations, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_28",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_29@0",
            "content": "UNKNOWN, None, 2018, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_29",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_30@0",
            "content": "UNKNOWN, None, 2020, Efficient inference for neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_30",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_31@0",
            "content": "Melvin Johnson, Mike Schuster, Quoc Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, Jeffrey Dean, Google's multilingual neural machine translation system: Enabling zero-shot translation, 2017, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_31",
            "start": 0,
            "end": 333,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_32@0",
            "content": "UNKNOWN, None, 2020, Deep encoder, shallow decoder: Reevaluating the speed-quality tradeoff in machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_32",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_33@0",
            "content": "Yoon Kim, Alexander Rush, Sequencelevel knowledge distillation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_33",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_34@0",
            "content": "Taku Kudo, John Richardson, SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_34",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_35@0",
            "content": "Sneha Kudugunta, Ankur Bapna, Isaac Caswell, Orhan Firat, Investigating multilingual NMT representations at scale, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_35",
            "start": 0,
            "end": 339,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_36@0",
            "content": "UNKNOWN, None, 2021, An efficient transformer decoder with compressed sub-layers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_36",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_37@0",
            "content": "Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han, On the variance of the adaptive learning rate and beyond, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_37",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_38@0",
            "content": "Matt Post, A call for clarity in reporting BLEU scores, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_38",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_39@0",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, COMET: A neural framework for MT evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_39",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_40@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_40",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_41@0",
            "content": "Zirui Wang, Zachary Lipton, Yulia Tsvetkov, On negative interference in multilingual models: Findings and a meta-learning treatment, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_41",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_42@0",
            "content": "Yilin Yang, Akiko Eriguchi, Alexandre Muzio, Prasad Tadepalli, Stefan Lee, Hany Hassan, Improving multilingual translation by representation and gradient regularization, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_42",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "209-ARR_v1_43@0",
            "content": "Biao Zhang, Philip Williams, Ivan Titov, Rico Sennrich, Improving massively multilingual neural machine translation and zero-shot translation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "209-ARR_v1_43",
            "start": 0,
            "end": 238,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "209-ARR_v1_0",
            "tgt_ix": "209-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_0",
            "tgt_ix": "209-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_1",
            "tgt_ix": "209-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_1",
            "tgt_ix": "209-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_0",
            "tgt_ix": "209-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_2",
            "tgt_ix": "209-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_4",
            "tgt_ix": "209-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_5",
            "tgt_ix": "209-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_3",
            "tgt_ix": "209-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_3",
            "tgt_ix": "209-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_3",
            "tgt_ix": "209-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_3",
            "tgt_ix": "209-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_0",
            "tgt_ix": "209-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_6",
            "tgt_ix": "209-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_8",
            "tgt_ix": "209-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_7",
            "tgt_ix": "209-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_7",
            "tgt_ix": "209-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_7",
            "tgt_ix": "209-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_7",
            "tgt_ix": "209-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_9",
            "tgt_ix": "209-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_11",
            "tgt_ix": "209-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_12",
            "tgt_ix": "209-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_10",
            "tgt_ix": "209-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_10",
            "tgt_ix": "209-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_10",
            "tgt_ix": "209-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_10",
            "tgt_ix": "209-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_14",
            "tgt_ix": "209-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_10",
            "tgt_ix": "209-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_10",
            "tgt_ix": "209-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_13",
            "tgt_ix": "209-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_7",
            "tgt_ix": "209-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_15",
            "tgt_ix": "209-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_16",
            "tgt_ix": "209-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_16",
            "tgt_ix": "209-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_16",
            "tgt_ix": "209-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_17",
            "tgt_ix": "209-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_19",
            "tgt_ix": "209-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_20",
            "tgt_ix": "209-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_21",
            "tgt_ix": "209-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_16",
            "tgt_ix": "209-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_16",
            "tgt_ix": "209-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_16",
            "tgt_ix": "209-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_16",
            "tgt_ix": "209-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_18",
            "tgt_ix": "209-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_0",
            "tgt_ix": "209-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_22",
            "tgt_ix": "209-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_24",
            "tgt_ix": "209-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_23",
            "tgt_ix": "209-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_23",
            "tgt_ix": "209-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_23",
            "tgt_ix": "209-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "209-ARR_v1_0",
            "tgt_ix": "209-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_1",
            "tgt_ix": "209-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_2",
            "tgt_ix": "209-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_2",
            "tgt_ix": "209-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_2",
            "tgt_ix": "209-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_2",
            "tgt_ix": "209-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_2",
            "tgt_ix": "209-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_2",
            "tgt_ix": "209-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_3",
            "tgt_ix": "209-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_4",
            "tgt_ix": "209-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_4",
            "tgt_ix": "209-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_5",
            "tgt_ix": "209-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_5",
            "tgt_ix": "209-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_5",
            "tgt_ix": "209-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_5",
            "tgt_ix": "209-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_6",
            "tgt_ix": "209-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_6",
            "tgt_ix": "209-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_6",
            "tgt_ix": "209-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_6",
            "tgt_ix": "209-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_6",
            "tgt_ix": "209-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_7",
            "tgt_ix": "209-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_8",
            "tgt_ix": "209-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_8",
            "tgt_ix": "209-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_8",
            "tgt_ix": "209-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_9",
            "tgt_ix": "209-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_9",
            "tgt_ix": "209-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_9",
            "tgt_ix": "209-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_9",
            "tgt_ix": "209-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_9",
            "tgt_ix": "209-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_9",
            "tgt_ix": "209-ARR_v1_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_10",
            "tgt_ix": "209-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_11",
            "tgt_ix": "209-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_11",
            "tgt_ix": "209-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_11",
            "tgt_ix": "209-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_11",
            "tgt_ix": "209-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_11",
            "tgt_ix": "209-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_11",
            "tgt_ix": "209-ARR_v1_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_12",
            "tgt_ix": "209-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_12",
            "tgt_ix": "209-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_12",
            "tgt_ix": "209-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_12",
            "tgt_ix": "209-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_12",
            "tgt_ix": "209-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_12",
            "tgt_ix": "209-ARR_v1_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_12",
            "tgt_ix": "209-ARR_v1_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_13",
            "tgt_ix": "209-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_13",
            "tgt_ix": "209-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_13",
            "tgt_ix": "209-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_13",
            "tgt_ix": "209-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_14",
            "tgt_ix": "209-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_14",
            "tgt_ix": "209-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_14",
            "tgt_ix": "209-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_14",
            "tgt_ix": "209-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_15",
            "tgt_ix": "209-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_16",
            "tgt_ix": "209-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_17",
            "tgt_ix": "209-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_18",
            "tgt_ix": "209-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_18",
            "tgt_ix": "209-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_18",
            "tgt_ix": "209-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_18",
            "tgt_ix": "209-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_19",
            "tgt_ix": "209-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_19",
            "tgt_ix": "209-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_19",
            "tgt_ix": "209-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_20",
            "tgt_ix": "209-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_20",
            "tgt_ix": "209-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_20",
            "tgt_ix": "209-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_20",
            "tgt_ix": "209-ARR_v1_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_20",
            "tgt_ix": "209-ARR_v1_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_20",
            "tgt_ix": "209-ARR_v1_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_20",
            "tgt_ix": "209-ARR_v1_20@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_21",
            "tgt_ix": "209-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_21",
            "tgt_ix": "209-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_21",
            "tgt_ix": "209-ARR_v1_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_21",
            "tgt_ix": "209-ARR_v1_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_21",
            "tgt_ix": "209-ARR_v1_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_21",
            "tgt_ix": "209-ARR_v1_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_22",
            "tgt_ix": "209-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_22",
            "tgt_ix": "209-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_22",
            "tgt_ix": "209-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_22",
            "tgt_ix": "209-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_22",
            "tgt_ix": "209-ARR_v1_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_23",
            "tgt_ix": "209-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_24",
            "tgt_ix": "209-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_24",
            "tgt_ix": "209-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_24",
            "tgt_ix": "209-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_24",
            "tgt_ix": "209-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_25",
            "tgt_ix": "209-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_25",
            "tgt_ix": "209-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_25",
            "tgt_ix": "209-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_25",
            "tgt_ix": "209-ARR_v1_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_26",
            "tgt_ix": "209-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_27",
            "tgt_ix": "209-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_28",
            "tgt_ix": "209-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_29",
            "tgt_ix": "209-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_30",
            "tgt_ix": "209-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_31",
            "tgt_ix": "209-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_32",
            "tgt_ix": "209-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_33",
            "tgt_ix": "209-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_34",
            "tgt_ix": "209-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_35",
            "tgt_ix": "209-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_36",
            "tgt_ix": "209-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_37",
            "tgt_ix": "209-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_38",
            "tgt_ix": "209-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_39",
            "tgt_ix": "209-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_40",
            "tgt_ix": "209-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_41",
            "tgt_ix": "209-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_42",
            "tgt_ix": "209-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "209-ARR_v1_43",
            "tgt_ix": "209-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 547,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "209-ARR",
        "version": 1
    }
}