{
    "nodes": [
        {
            "ix": "49-ARR_v1_0",
            "content": "Using Pre-trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_2",
            "content": "In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech. We first present a comparative study to determine whether there is a particular Language Model (or class of LMs) and a particular decoding mechanism which is the most appropriate to generate CNs. Findings show that autoregressive models combined with stochastic decodings are the most promising. We then investigate how an LM performs in generating a CN with regard to an unseen target of hate. We find out that a key element for successful 'out of target' experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori. We finally introduce the idea of a pipeline based on the addition of an automatic post-editing step to refine generated CNs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "49-ARR_v1_4",
            "content": "Hate Speech (HS) has found fertile ground in Social Media Platforms. Actions undertaken by such platforms to tackle online hatred consist in identifying possible sources of hate and removing them by means of content deletion, account suspension or shadow-banning. However, these actions are often interpreted and denounced as censorship by the affected users and political groups (Myers West, 2018). For this reason, such restrictions can have the opposite effect of exacerbating the hostility of the haters (Munger, 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_5",
            "content": "An alternative strategy, that is looming on the horizon, is based on the use of Counter Narratives (CN). CNs are \"all communicative actions aimed at refuting hate speech through thoughtful and cogent reasons, and true and fact-bound arguments\" (Schieb and Preuss, 2016). As a de-escalating measure, CNs have been proven to be successful in diminishing hate, while preserving the freedom of speech (Benesch, 2014;Gagliardone et al., 2015). An example <HS, CN > pair is shown below: HS: Women are basically childlike, they remain this way most of their lives. Soft and emotional. It has devastated our once great patriarchal civilizations. CN: Without softness and emotions there would be just brutality and cruelty. Not all women are soft and emotional and many men have these characteristics. To perpetuate these socially constructed gender profiles maintains norms which oppress anybody.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_6",
            "content": "Based on their effectiveness, CNs have started being employed by NGOs to counter online hate. Since for NGO operators it is impossible to manually write responses to all instances of hate, a line of NLP research has recently emerged, focusing on designing systems to automatically generate CN suggestions to fight online hateful messages (Qian et al., 2019;Tekiroglu et al., 2020;Fanton et al., 2021;Chung et al., 2021a;Zhu and Bhat, 2021). In this study, our main goal is to compare pre-trained language models (LM) and decoding mechanisms in order to understand their pros and cons in generating CNs. Thus, we use various automatic metrics and manual evaluations with expert judgments to assess several LMs, representing the main categories of the model mechanisms, and decoding methods. We further test the robustness of the finetuned LMs in generating CNs for an unseen target. Results show that autoregressive models are in general more suited for the task, and while stochastic decoding mechanisms can generate more novel, diverse, and informative outputs, the deterministic decoding is useful in scenarios where more generic and less novel (yet 'safer') CNs are needed. Furthermore, in out-of-target experiments we find that the similarity of targets (e.g. JEWS and MUSLIMS as religious groups) in training data plays a crucial role for the effectiveness of portability to new targets. We finally show a promising research direction of leveraging gold human edits for building an additional automatic post-editing step to correct errors made by LMs during generation. To the best of our knowledge, this is the first study systematically analysing state of the art pre-trained LMs in CN generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_7",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "49-ARR_v1_8",
            "content": "In this section we first discuss standard approaches to hate countering and studies on CN effectiveness on Social Media Platforms, then the existing CN data collection and generation strategies.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_9",
            "content": "Hate countering. NLP has started addressing the phenomenon of the proliferation of HS by creating datasets for automatic detection (Mathew et al., 2021;Cao et al., 2020;Kumar et al., 2018;Hosseinmardi et al., 2015;Waseem, 2016;Burnap and Williams, 2016). Several surveys provide a review on the existing approaches on the topic (Poletto et al., 2020;Schmidt and Wiegand, 2017;Nunes and Fortuna, 2018), also addressing the ethical challenges of the task (Kiritchenko et al., 2020). Automatic detection of HS presents some drawbacks (Vidgen and Derczynski, 2020). First of all, the datasets might include biases, and the models tend to replicate such biases (Binns et al., 2017;Davidson et al., 2019;Sap et al., 2019;Tsvetkov, 2020). Moreover, the end goals for which HS detection is employed are often charged with censorship of the freedom of speech by concerned users (Munger, 2017;Myers West, 2018). In this scenario, NGOs have started employing CNs to counter online hate. CNs have been shown to be effective in reducing linguistic violence (Benesch, 2014;Gagliardone et al., 2015;Schieb and Preuss, 2016;Silverman et al., 2016;Mathew et al., 2019); moreover, even if they might not influence the view of the extremists, they are still effective in presenting alternative and non-hateful viewpoints to the bystanders (Allison and Bussey, 2016;Anderson et al., 2014).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_10",
            "content": "The existing studies for collecting CN datasets employ four main approaches. Crawling consists in automatically scraping websites, starting from an HS content and searching for possible CNs among the responses (Mathew et al., 2018(Mathew et al., , 2019. With crowdsourcing CNs are written by non-expert paid workers as responses to provided hate content (Qian et al., 2019). Nichesourcing relies on a niche group of experts for data collection (De Boer et al., 2012), and it was employed by Chung et al. (2019) for CN collection using NGO's operators. Hybrid approaches use a combination of LMs and humans to collect data (Wallace et al., 2019;Dinan et al., 2019;. Studies on CN collection are presented in more detail by Tekiroglu et al. (2020);Fanton et al. (2021). 2019) employ a mix of automatic and human intervention to generate CNs. Zhu and Bhat (2021) propose an entirely automated pipeline for synthetic CN generation. Instead, our work aims at an end-to-end system for CN generation, which is developed as a downstream task after comparing the state of the art pre-trained LMs. Other lines of work include CN generation for under-resourced languages such as for Italian (Chung et al., 2020), and the generation of knowledge-bound CNs, which allows the production of CNs based on grounded and up-to-date facts and plausible arguments, avoiding hallucination phenomena (Chung et al., 2021a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_11",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "49-ARR_v1_12",
            "content": "In this section, we present the CN dataset, the language models, and the decoding mechanisms employed for our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_13",
            "content": "Dataset for fine-tuning",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "49-ARR_v1_14",
            "content": "For this study we rely on the dataset proposed by Fanton et al. (2021), which is the only available dataset that grants both the target diversity and the CN quality we aim for. The dataset was collected with a human-in-the-loop approach, by employing an autoregressive LM (GPT-2) paired with three expert human reviewers. It features 5003 <HS, CN > pairs, covering several targets of hate including DISABLED, JEWS, LGBT+, MIGRANTS, MUSLIMS, POC, WOMEN. The residual categories are collapsed to the label OTHER. We partitioned the dataset into training, validation, and test sets with the ratio: 8 : 1 : 1 (i. e. 4003, 500 and 500 pairs), ensuring that all sets share the same target distribution, and no repetition of HS across the sets is allowed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_15",
            "content": "Models",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "49-ARR_v1_16",
            "content": "We experiment with 5 Transformer based LMs (Vaswani et al., 2017) (2019). It is a bidirectional autoencoder that can be adapted to text generation (Wang and Cho, 2019). GPT-2. The Generative Pre-trained Transformer 2 is an autoregressive model and it is specifically built for text generation (Radford et al., 2019). DialoGPT. The Dialogue Generative Pretrained Transformer is the extension of GPT-2 specifically created for conversational response generation (Zhang et al., 2020). BART. BART is a seq2seq model that combines two different Transformer architectures (Lewis et al., 2020): a bidirectional encoder and a leftto-right autoregressive decoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_17",
            "content": "The Text-to-Text Transfer Transformer proposed by Raffel et al. ( 2020) is a seq2seq model with an encoder-decoder Transformer architecture. While all the other models could be fine-tuned directly for the generation task, for BERT we needed to warmstart an encoder-decoder model using BERT checkpoints. Further details can be found in Appendix A.1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_18",
            "content": "Decoding mechanisms",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "49-ARR_v1_19",
            "content": "We utilize 4 decoding mechanisms: a deterministic (Beam Search) and three stochastic (Top-k, Top-p, and a combination of the two). Beam Search (BS). The Beam Search algorithm is designed to pick the most-likely sequence (Li et al., 2016;Wiseman et al., 2017). Top-k (Top k ). The sampling procedure proposed by Fan et al. (2018) selects a random word from the k most probable ones, at each time step. Top-p (Top p ). Also known as Nucleus Sampling, the parameter p indicates the total probability for the pooled candidates, at each time step (Holtzman et al., 2019). Combining Top-p and Top-k (Top pk ). At decoding stage, it is possible to combine the parameters p and k. This is a Nucleus Sampling constrained to the Top-k most probable words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_20",
            "content": "In our experiments we used the following parameters: Beam-Search with 5 beams and repetition penalty = 2; Top-k with k = 40; Top-p with p = .92; Top pk with k = 40 and p = .92.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_21",
            "content": "Evaluation metrics",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "49-ARR_v1_22",
            "content": "We use several metrics to evaluate various aspects of the CN generation. Overlap Metrics. These metrics depend on the n-gram similarity of the generated outputs to a set of reference texts in order to assess the quality. We used our gold CNs as references and the CNs generated by the different models, as candidates.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_23",
            "content": "In particular, we employed three BLEU variants: BLEU-1 (B-1), BLEU-3 (B-3) and BLEU-4 (B-4) (Papineni et al., 2002), and ROUGE-L (ROU) (Lin, 2004). Diversity metrics. They are used to measure how diverse and novel the produced CNs are. In particular, we utilized Repetition Rate (RR) to measure the repetitiveness among generated CNs, in terms of the average ratios of non-singleton n-grams present in the corpus (Bertoldi et al., 2013). We also used Novelty (NOV), based on Jaccard similarity (Wang and Wan, 2018;Dziri et al., 2019), to compute the amount of novel content that is present in the generated CNs as compared to the training data. Human evaluation metrics. Albeit more difficult to attain, human judgements provide a more reliable evaluation and a deeper understanding than automatic metrics (Belz and Reiter, 2006;Novikova et al., 2017). To this end, we specified the following dimensions for the evaluation of CNs. Suitableness (SUI): measures how suitable a CN is to the HS in terms of semantic relatedness and in terms of adherence to CN guidelines 1 ; Grammaticality (GRM): how grammatically correct a generated CN is; Specificity (SPE): how specific are the arguments brought by the CN in response to the HS; Choose-or-not (CHO): determines whether the annotators would select that CN to post-edit and use it in a real case scenario as in the set up presented by Chung et al. (2021b); Is-best (BEST): whether the CN is the absolute best among the ones generated for an HS (i. e. whether the annotators would pick up exactly that CN if they had to use it in a real case scenario).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_24",
            "content": "The first three dimensions are rated with a 5points Likert scale and follow the evaluation procedure described by Chung et al. (2020), whereas both choose-or-not and is-best are binary ratings (0, 1). Choose-or-not allows for multiple CNs to be selected for the same HS, while only one CN can be selected for is-best for each HS. Toxicity. 2 It determines how \"rude, disrespectful, or unreasonable\" a text is. Toxicity has been employed both to detect the bias present in LMs (Gehman et al., 2020) and as a solution to mitigate such bias (Gehman et al., 2020;Xu et al., 2020). Syntactic metrics. A high syntactic complexity can be used as a proxy for an LM's ability of generating complex arguments. We used the syntactic dependency parser of spaCy 3 for the task, focusing on the following measures: Maximum Syntactic Depth (MSD): the maximum depth among the dependency trees calculated over each sentence composing a CN. Average Syntactic Depth (ASD): the average depth of the sentences in each CN. Number of Sentences (NST): the number of sentences composing a CN.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_25",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "49-ARR_v1_26",
            "content": "We performed two sets of experiments: first, we assessed how LMs perform in the task of generating CNs with different decoding mechanisms. Then, we selected the best model from the first round of experiments and tested its generalization capabilities when confronted with an unseen target of hate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_27",
            "content": "LMs and decoding experiments",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "49-ARR_v1_28",
            "content": "For the first round of experiments, in order to avoid possible unfair assessments given by the open nature of the generative task (i. e. a highly suitable CN candidate could be scored low due to its difference from the reference CN), at test time we allowed the generation of several candidates for each HS/LM/decoding mechanism combination. We loosely drew inspiration from the Rank-N Accuracy procedure and the 'generate, prune, select' procedure (Zhu and Bhat, 2021). In particular, given an LM and a decoding mechanism, we generated 5 CNs for each HS in the test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_29",
            "content": "We set up the automatic evaluation strategy as displayed in Figure 1. First, we scored each CN with the overlap metrics presented in Section 4, using the gold CN as a reference. Next, we ranked the candidate CNs with respect to the overlap scores and computed the mean of the rankings. Then, we selected the best ones according to the following criteria: Best LM selects the single best CN among the 20 generated by each model for an HS. Best D selects the single best CN among the 25 generated by each decoding for an HS. Best LM+D selects the single best CN among the 5 generated with each model-decoding combination. Moreover, we assessed the overall corpus-wise quality of the generated CNs with respect to the models, to the decoding mechanisms, and to the model-decoding combinations via the diversity metrics. Human evaluation on a sample To perform the human evaluation we referred to the Best LM generations and sampled 200 instances from it. Each instance comprises an HS and 5 relevant CNs, each generated by a different model. We recruited 2 annotators who were trained extensively for the task following the procedure used by Fanton et al. (2021). The expert annotators were asked to evaluate the 5 CNs corresponding to the HS, according to the dimensions described in Section 4. We enriched the evaluation of this subset with the toxicity and the syntactic metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_30",
            "content": "Results of the first set of experiments",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "49-ARR_v1_31",
            "content": "The results of the experiments on the LMs and the decoding mechanisms are reported in this section 4 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_32",
            "content": "The results of the comparison of the models on the Best LM generations can be found in Table 1. Regarding the overlap and diversity metrics, DialoGPT records the best or the second best score in all the metrics, apart from novelty where it still achieves a high score (0.643) close to the best performance (0.655). T5 also achieves high scores, especially on ROUGE, BLEU-1 and novelty.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_33",
            "content": "BART, instead, is the best model according to human evaluation metrics, apart from specificity. On the other hand, it shows poor performances in terms of diversity metrics, indicating that it tends to produce grammatical and suitable but very generic responses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_34",
            "content": "BERT records the worst scores for all the overlap and diversity metrics apart from novelty. However, it also achieves the best syntactic metric results. Therefore, it is evident that BERT's output is more complex, but very repetitive. The combination of these aspects eventually affects the clarity of BERT's output such that it yields poor results in the human evaluation, in particular for grammaticality (4.2, while other models are above 4.6).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_35",
            "content": "GPT-2 overall yields very competitive results for several groups of metrics. It obtains the secondhighest novelty score (0.653) and the best RR (7.736). It also achieves the second best results on BLEU-3, maximum syntactic depth and number of sentences, and the best results on toxicity and specificity (2.880) indicating the ability to produce complex, suitable, focused and diverse CNs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_36",
            "content": "After the human evaluation we ran a qualitative interview with the annotators, whose feedback on the data strengthened the results we observed and the conclusion we drew. For instance, they reported the repetition of simple, yet catch-them-all, expressions (e.g. \"they are our brothers and sisters\") regardless of the target. Further inspections found that those CNs were mainly produced by BERT, which is in line with BERT's RR score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_37",
            "content": "Best Decoding mechanism. The results calculated on Best D output are presented in Table 2. Top k is the best performing decoding mechanism achieving the best results on the diversity metrics, BLEU-3 and BLEU-4. It is also the best performing for specificity, maximum syntactic depth and number of sentences, and the second best for average syntactic depth and toxicity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_38",
            "content": "The other stochastic decoding mechanisms perform well too. Top p yields competitive results on both diversity and overlap metrics; it is the second best for specificity, and achieves good results on the syntactic metrics. Top pk has a good performance on the overlap metrics. It obtains the second-highest scores in most of the human evaluation metrics and the lowest in toxicity, and it reaches a reasonable specificity score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_39",
            "content": "On the other hand, BS does not achieve particularly good results, except for the ROUGE score. Even if it is the best decoding with respect to the human evaluation, this comes at the cost of specificity and diversity. Through a post-hoc manual analysis we observed that it was due to the deterministic nature of BS, that tends to choose the most probable sequences, i. e. the \"safest\", thus resulting in vague and repetitive outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_40",
            "content": "Best Model-Decoding combination Here we briefly discuss the results of the evaluation obtained on the Best LM+D generations. In particular, the autoregressive models GPT-2 and DialoGPT behave similarly with similar decoding mechanisms, such that BS outputs the best results for almost all the overlap metrics, and the worst for the diversity metrics. On the contrary, for the other models, the results achieved with stochastic decoding mechanisms are the best for the overlap metrics. In almost all the cases, we observe that the stochastic decoding mechanisms perform better on syntactic and diversity metrics and on toxicity, while for the human evaluation metrics BS tends to be the best, except for specificity. A detailed discussion can be found in Appendix A.2. Discussion. In this set of experiments, we found that the autoregressive models perform the best according to a combination of several metrics. Still, it could be argued that the good performance of the GPT-2 model we fine-tuned is due to the fact that generated CNs and gold CNs derive from a similar distribution (GPT-2 was employed in the human-in-the-loop process used to create the reference dataset from Fanton et al. ( 2021)). While we recognize that this could partially explain the performance of our GPT-2 model, it does not explain the performance of DialoGPT, which is pre-trained on a completely different dataset. Therefore, we can reasonably conclude that autoregressive models are particularly suited for the task, regardless of the pre-training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_41",
            "content": "With respect to the decoding mechanisms, we record high repetitiveness and low novelty for the Example CNs generated in this session of experiments, along with some qualitative analysis, can be found in Appendix A.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_42",
            "content": "Leave One Target Out experiments",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "49-ARR_v1_43",
            "content": "In the second stage, we built a set of cross-domain experiments to capture the generalization capabilities of the best LM determined in the previous experiments. Specifically, we concentrate on assessing how much a pre-trained language model fine-tuned on a pool of hate targets can generalize to an unseen target.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_44",
            "content": "Thus, for the out of target experiment we selected the LM that we deem the most prominent in order to reduce the number of LM configurations to compare. In particular, since we want to examine the generalization capability of the LM, the generation of novel CNs, in comparison to the training data, is given primary importance. Secondly, specificity is also crucial since it signifies the ability of the LM/decoding mechanism in generating accurate CNs and avoiding vague yet suitable, catch-all CNs. In contrast, repetitiveness is an undesirable feature of CNs, as it signals the tendency of a model to produce less flexible content. Given these considerations, we chose to employ GPT-2 with Top k decoding for the Leave One Target Out (LOTO) experiments since it is the configuration achieving the best trade-off amongst all the others. This set of experiments is structured in 3 steps, replicated for each of the selected targets. We selected the targets with the highest number of examples (MUSLIMS, MIGRANTS, WOMEN, LGBT+ and JEWS) to have a sufficient sized test set for each configuration.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_45",
            "content": "First, we sampled from the Fanton et al. ( 2021) dataset 600 pairs for each LOTO target, in order to have a balanced setting. Additionally, POC and DISABLED were always kept in the training set, and we removed multi-target cases from OTHER. The resulting dataset consists of 3729 instances (further details are provided in Appendix A.4). Secondly, we fine-tuned 5 different configurations of the LM, and in each configuration one of the 5 LOTO targets is not present in the training data: LM -JEWS , LM -LGTB+ , LM -MIGRANTS , LM -MUSLIMS and LM -WOMEN . Finally, we tested each LOTO model on the 600 HSs in the test set made of \"left out\" target examples. For instance, the model LM -JEWS is used for generating the CNs for the target JEWS, after being trained on <HS, CN > data without any instances with the label JEWS. We generated 5 CNs for each HS and selected the best CN according to the procedure described in Section 5.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_46",
            "content": "We analyse the CNs generated with the LOTO models through overlap and diversity metrics (Table 3). We refer to Appendix A.4 for the comparison be- For all the targets we record higher novelty scores as compared to the previous experiments. Higher novelty ranges indicate that conditioning with new material (i. e. HS for the unseen targets) induces GPT-2 to produce new arguments. On the other hand, as expected, the overlap scores for LOTO are remarkably lower than those from the previous experiments (Table 3). Therefore, we can infer that generalizing to an unseen target is harder than generalizing to an unseen HS. We also found out that the CNs generated in the LM -MUSLIMS and LM -WOMEN configurations obtain the highest overlap scores (Table 3). We hypothesize that the high scores can be explained by the presence of a target in the LOTO training that is highly similar to the left out one. To this end, we computed the novelty between each target subset of the training data and the LOTO test data for that configuration (see Appendix A.4 for details). The reference CNs for LM -MUSLIMS record the lowest novelty scores with respect to the JEWS subset of the training set (i. e. 0.761). Thus, it can be interpreted as the most influential portion of training data for the target MUSLIMS. On the other hand, for LM -WOMEN the highest influence is recorded with the LGBT+ subset of the training data (i. e. 0.763). These results can be explained by the semantic similarity of the target MUSLIMS to JEWS, both being religious groups; and of WOMEN to LGBT+, both being related to gender issues.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_47",
            "content": "As a complementary analysis, we consider two different computations of the reference CN novelty: with respect to the most influential target for each LOTO configuration, and with respect to the LOTO training data without the most influential target. We computed the Pearson correlation between the overlap metrics and each of the two novelty computations. In Figure 2, we observe that removing the influential target from the training data strongly decreases the correlation with the over-Figure 2: The correlation between the novelty of the reference CNs and overlap metrics: in each plot, the dots and the darker line correspond to the most influential target; the triangles and the lighter line correspond to the results calculated without it. lap metrics (from an average of -0.889 to -0.416). Consequently, we can conclude that to obtain high overlap results in the LOTO experiments, it is necessary that the training data contains a target strongly connected to the left out one. Most importantly, this connection is not arbitrarily decided but it is based on an a priori semantic similarity of the targets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_48",
            "content": "Finally, we chose to generate also with the BS decoding mechanism, to use it as a baseline and compare it to the stochastic decoding mechanism (Top-k). In particular, we computed the Pearson correlation between the novelty of the reference CNs and the novelty of the candidate CNs with respect to the corresponding training data (Figure 3). We can observe that for the BS generation the novelty of the candidate CNs is lower than Top-k (0.67-0.74 vs. 0.75-0.77) and the correlation with the novelty of the reference is weaker (0.53 vs. 0.59). This confirms the lower generalization ability with the deterministic decoding mechanism (as compared to the stochastic) that tends to produce generic and repetitive responses regardless of the semantic distances of the LOTO targets from the training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_49",
            "content": "Automatic Post-Editing",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "49-ARR_v1_50",
            "content": "In the previous experiments we fine-tuned our models making resort to <HS, CN > pairs alone. Still the Fanton et al. ( 2021) dataset contains additional information that can be useful for our task: i. e. the original GPT-2 generation before undergoing human post-editing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_51",
            "content": "Thus, as a final experiment, we propose further improving CN generation by moving from an endto-end framework to a two stage pipeline, by decoupling CN generation from its 'final refinement'. In particular we propose the adoption of an Automatic Post-Editing (APE) stage in order to capture and utilize the nuances among the machine generated CNs and their human post-edited versions. APE, which is used for automatically correcting errors made by MT systems before performing actual human post-editing, has been an important tool for machine translation (Knight and Chander, 1994;do Carmo et al., 2021). Considering its effectiveness in MT, we hypothesize that building a pipeline with CN generation and APE could alleviate the requirement of the final manual post-editing (Allen and Hogan, 2000;Chatterjee et al., 2019) to achieve better constructed CNs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_52",
            "content": "To this end, we fine-tuned another instance of GPT-2 medium model specifically for the post-editing task using the <HS, CN or , CN pe > triplets 5 , where CN or and CN pe denote the CNs originally generated by an LM and their human post-edited versions, respectively. The triplets were then filtered by removing those for which CN or = CN pe . More details about the experiment settings can be found in Appendix A.5. We have conducted two human evaluations over the subsets of: i) the CN or of the Fanton et al. 5 this is in line with the APE paradigm where the triplet is made of <source sentece, M Toutput, human post-edits>.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_53",
            "content": "(2021) test samples, ii) the CN outputs of the best model and decoding mechanism combination provided as the results of the first set of experiments, that yielded the top 50 TER scores with respect to the CN or s. The two expert annotators were asked to state their preferences among the 2 randomly sorted CNs, CN or and CN ape (automatically postedited output), for a given HS. The annotators were also allowed to decide on a tie. Results, shown in Table 4, indicate that, albeit there are often ties and only a subset of CN or is actually modified, when there is a preference, it is predominantly in favour of the automatically post-edited versions over the GPT-2 generated CNs (26% vs. 14% for the test set, and 37% vs. 19% for the GPT-2 Top k generations, on average). Regarding the experiment results, we believe that APE is a highly promising direction to increase the efficacy of the CN generation models where generation quality and diversity is crucial, and considering that obtaining/enlarging expert datasets to train better models is not simple.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_54",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "49-ARR_v1_55",
            "content": "In this work, we focus on automatic CN generation as a downstream task. First, we present a comparative study to determine the performances and peculiarities of several pretrained LMs and decoding mechanisms. We observe that the best results (in term of novelty and specificity) overall are achieved by the autoregressive models with stochastic decoding: GPT-2 with the Top k decoding mechanism, and DialoGPT with the combination Top pk . At the same time deterministic decoding can be used when more generic yet 'safer' CNs are preferred.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_56",
            "content": "Then, we investigate the performance of LMs in zero-shot generation for unseen targets of hate. Hence, we fine-tuned 5 different versions of GPT-2, leaving out the examples pertaining to one target at each turn. We find out that for each configuration/version, there is a subset of the training data which is more influential with respect to the generated data (i. e. a target that shares some commonalities with the test target that can be defined a-priori). Finally, we introduce an experiment by training an automatic post-editing module to further improve the CN generation quality. The notable human evaluation results paves the way for a promising future direction that decouples CN generation from its 'final refinement'.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_57",
            "content": "Although tackling online hatred through CNs inherently protects the freedom of speech and has been proposed as a better alternative to the detectremove-ban approaches, automatization of CN generation can still raise some ethical concerns and some measures must be taken to avoid undesired effects during research. Thus, we address the relevant ethical considerations and our remedies as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_58",
            "content": "Annotation Guidelines: The well-being of the annotators was our top priority during the whole study. Therefore, we strictly followed the guidelines created for CN studies (Fanton et al., 2021) that were previously adapted from (Vidgen et al., 2019). The human evaluations have been conducted with the help of two expert annotators in CNs. We instructed the annotators in the aims of each evaluation, clearly explained the tasks, and then we exemplified proper <HS, CN > pairs with various types of CNs. Most importantly, we limited the exposure to hateful content by providing a daily time limit of annotation. Concerning the demographics, due to the harmful content that can be found in the data, all annotators were adult volunteers, perfectly aware of the objective of the study.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_59",
            "content": "Dataset. We purposefully chose an expert-based dataset in order to avoid the risk of modeling the language of real individuals to (i) prevent any privacy issue, (ii) avoid to model inappropriate CNs (e.g. containing abusive language) that could be produced by non-experts. The dataset also focuses on the CN diversity while keeping the HSs as stereotypical as possible so that our CN generation models have a very limited diversity on the hateful language, nearly precluding the misuse.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_60",
            "content": "Computational Task. CN generation models are not meant to be used in an autonomous way, since even the best models can still produce substandard CNs containing inappropriate or negative language. Instead, following a Human-computer cooperation paradigm, our focus is on building models that can be helpful to NGO operators by providing them diverse and novel CN candidates for their hate countering activities and speed up the manual CN writing to a certain extent. This approach also gives ground to some of the measures we used during evaluation (namely choose-or-not and is-best).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_61",
            "content": "Model Distribution. In addition to the limited and simplified hateful content in the dataset we selected, we further reduce the risk of misuse by choosing a specific distribution strategy: i) we only make available the non-autoregressive models in order to eliminate the risk of using over-generation for hate speech creation, ii) we distribute such models only for research purposes and through a request based procedure in order to keep track of the possible users.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_62",
            "content": "A.1 Fine-tuning details As a rule-of-thumb, we chose one version smaller than the large version of each model, this corresponds to the medium versions for both DialoGPT and GPT-2, and base versions for the other models. GPT-2 and DialoGPT achieve the lowest perplexity, training and evaluation loss, thus indicating a slightly more successful fine-tuning, which are reflected in the evaluations throughout the study. We conducted a hyper-parameter search during the training phase of each model using the search space: learning-rate:{1e \u2212 5, 2e \u2212 5, 3e \u2212 5, 4e \u2212 5, 5e \u2212 5}, warm-up ratio:{0, 0.1}, batchsize:{2, 4}, epochs:{2, 3, 4, 5}. It has been conducted using Optuna, with 10 trials, optimized on minimizing the evaluation loss during training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_63",
            "content": "Here we discuss the results for the overlap and diversity metrics obtained on the Best LM+D generations (Table 6), and those calculated on the human evaluation subset (Tables 7 and 8).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_64",
            "content": "BART. BART performs well with the stochastic decoding methods, in particular: Top p for overlap, diversity, syntactic metrics, and grammaticality; Top k for overlap metrics and toxicity, whereas Top pk is the best decoding approach on human evaluation and RR, and the second best on ROUGE and BLEU-1. On the contrary, BART does not achieve good results with deterministic approaches (i. e. BS).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_65",
            "content": "BERT. With BS, BERT achieves the best or second best result on all human evaluation metrics, except for specificity. For BERT the best decoding is Top p : it is the best performing on overlap GPT-2 -Top p : This is not true, the vast majority of Muslims is peaceful and peaceful Muslims are happy to live in any country they wish. They do not need to be a part of a hostile state and they should be treated as equals in society.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_66",
            "content": "In these examples we can observe that the CN obtained with GPT-2, albeit not perfect, is the most complete and accurate. Those generated with T5 and DialoGPT are suitable as well. On the other hand, the CNs obtained with BART and BERT present some problems: the former has a clear contradiction in it and the latter is illustrative of some of the problems detected in BERT's generations such that it is syntactically complex but very vague and repetitive, and it ends up not really conveying a clear message.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_67",
            "content": "Hate speech: The migrant crisis is not the fault of Western Europeans. Migrants are to blame for immigration.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_68",
            "content": "DialoGPT -Top k : What if an entire religion was to blame only for the migration crisis? So you think a large number of Muslims are responsible for all the problems of our world?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_69",
            "content": "BART -Top p : The migrant crisis is caused by a lack of integration and lack of knowledge of the world.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_70",
            "content": "BERT -BS: many migrants come here to work and provide for their families. they are our brothers and sisters, and we should welcome them. Once again, GPT-2 and DialoGPT generations where chosen as the best CNs by the annotators. Moreover, these CNs are all generated via Top k decoding mechanism. BART and T5 generated CNs are acceptable while requiring some post-editing to be employed. Finally, BERT's output is still very vague: we can notice the presence of the recurring expression our brothers and sisters. For fine-tuning our APE model, we have thus used the triplets <HS, CN or , CN pe > and <HS, CN pe * , CN pe >. In this way, we managed to roughly double the number of the post-edit training samples, which is highly beneficial for a better model. When we filtered the triplets with a positive TER score between CN ed and CN pe , or CN or and CN pe , we obtained 4185 training, 596 test, and 568 validation samples following the partition used in",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v1_71",
            "content": "Jeffrey Allen, Christopher Hogan, Toward the development of a post editing module for raw machine translation output: A controlled language perspective, 2000, Third International Controlled Language Applications Workshop (CLAW-00), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Jeffrey Allen",
                    "Christopher Hogan"
                ],
                "title": "Toward the development of a post editing module for raw machine translation output: A controlled language perspective",
                "pub_date": "2000",
                "pub_title": "Third International Controlled Language Applications Workshop (CLAW-00)",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_72",
            "content": "R Kimberley, Kay Allison,  Bussey, Cyberbystanding in context: A review of the literature on witnesses' responses to cyberbullying, 2016, Children and Youth Services Review, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "R Kimberley",
                    "Kay Allison",
                    " Bussey"
                ],
                "title": "Cyberbystanding in context: A review of the literature on witnesses' responses to cyberbullying",
                "pub_date": "2016",
                "pub_title": "Children and Youth Services Review",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_73",
            "content": "Jenn Anderson, Mary Bresnahan, Catherine Musatics, Combating weight-based cyberbullying on facebook with the dissenter effect, 2014, Cyberpsychology, Behavior, and Social Networking.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jenn Anderson",
                    "Mary Bresnahan",
                    "Catherine Musatics"
                ],
                "title": "Combating weight-based cyberbullying on facebook with the dissenter effect",
                "pub_date": "2014",
                "pub_title": "Cyberpsychology",
                "pub": "Behavior, and Social Networking"
            }
        },
        {
            "ix": "49-ARR_v1_74",
            "content": "Anja Belz, Ehud Reiter, Comparing automatic and human evaluation of nlg systems, 2006, 11th Conference of the European Chapter of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Anja Belz",
                    "Ehud Reiter"
                ],
                "title": "Comparing automatic and human evaluation of nlg systems",
                "pub_date": "2006",
                "pub_title": "11th Conference of the European Chapter of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_75",
            "content": "Susan Benesch, Countering dangerous speech: New ideas for genocide prevention, 2014, United States Holocaust Memorial Museum, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Susan Benesch"
                ],
                "title": "Countering dangerous speech: New ideas for genocide prevention",
                "pub_date": "2014",
                "pub_title": "United States Holocaust Memorial Museum",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_76",
            "content": "Nicola Bertoldi, Mauro Cettolo, Marcello Federico, Cache-based online adaptation for machine translation enhanced computer assisted translation, 2013, MT-Summit, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Nicola Bertoldi",
                    "Mauro Cettolo",
                    "Marcello Federico"
                ],
                "title": "Cache-based online adaptation for machine translation enhanced computer assisted translation",
                "pub_date": "2013",
                "pub_title": "MT-Summit",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_77",
            "content": "Reuben Binns, Michael Veale, Max Van Kleek, Nigel Shadbolt, Like trainer, like bot? inheritance of bias in algorithmic content moderation, 2017, Social Informatics, Springer International Publishing.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Reuben Binns",
                    "Michael Veale",
                    "Max Van Kleek",
                    "Nigel Shadbolt"
                ],
                "title": "Like trainer, like bot? inheritance of bias in algorithmic content moderation",
                "pub_date": "2017",
                "pub_title": "Social Informatics",
                "pub": "Springer International Publishing"
            }
        },
        {
            "ix": "49-ARR_v1_78",
            "content": "Pete Burnap, L Matthew,  Williams, Us and them: identifying cyber hate on twitter across multiple protected characteristics, 2016, EPJ Data Science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Pete Burnap",
                    "L Matthew",
                    " Williams"
                ],
                "title": "Us and them: identifying cyber hate on twitter across multiple protected characteristics",
                "pub_date": "2016",
                "pub_title": "EPJ Data Science",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_79",
            "content": "Rui Cao, Roy Ka-Wei Lee, Tuan-Anh Hoang, Deephate: Hate speech detection via multi-faceted text representations, 2020, 12th ACM Conference on Web Science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Rui Cao",
                    "Roy Ka-Wei Lee",
                    "Tuan-Anh Hoang"
                ],
                "title": "Deephate: Hate speech detection via multi-faceted text representations",
                "pub_date": "2020",
                "pub_title": "12th ACM Conference on Web Science",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_80",
            "content": "Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, Jianfeng Gao, Deep reinforcement learning for dialogue generation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Jiwei Li",
                    "Will Monroe",
                    "Alan Ritter",
                    "Dan Jurafsky",
                    "Michel Galley",
                    "Jianfeng Gao"
                ],
                "title": "Deep reinforcement learning for dialogue generation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "49-ARR_v1_81",
            "content": "Chin-Yew Lin, Rouge: A package for automatic evaluation of summaries, 2004, Text summarization branches out, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Chin-Yew Lin"
                ],
                "title": "Rouge: A package for automatic evaluation of summaries",
                "pub_date": "2004",
                "pub_title": "Text summarization branches out",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_82",
            "content": "UNKNOWN, None, 2018, Analyzing the hate and counter speech accounts on twitter, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Analyzing the hate and counter speech accounts on twitter",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_83",
            "content": "Binny Mathew, Punyajoy Saha, Hardik Tharad, Subham Rajgaria, Prajwal Singhania, Pawan Suman Kalyan Maity, Animesh Goyal,  Mukherjee, Thou shalt not hate: Countering online hate speech, 2019, Proceedings of the International AAAI Conference on Web and Social Media, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Binny Mathew",
                    "Punyajoy Saha",
                    "Hardik Tharad",
                    "Subham Rajgaria",
                    "Prajwal Singhania",
                    "Pawan Suman Kalyan Maity",
                    "Animesh Goyal",
                    " Mukherjee"
                ],
                "title": "Thou shalt not hate: Countering online hate speech",
                "pub_date": "2019",
                "pub_title": "Proceedings of the International AAAI Conference on Web and Social Media",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_84",
            "content": "Binny Mathew, Punyajoy Saha, Chris Seid Muhie Yimam, Pawan Biemann, Animesh Goyal,  Mukherjee, Hatexplain: A benchmark dataset for explainable hate speech detection, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Binny Mathew",
                    "Punyajoy Saha",
                    "Chris Seid Muhie Yimam",
                    "Pawan Biemann",
                    "Animesh Goyal",
                    " Mukherjee"
                ],
                "title": "Hatexplain: A benchmark dataset for explainable hate speech detection",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_85",
            "content": "Kevin Munger, Tweetment effects on the tweeted: Experimentally reducing racist harassment, 2017, Political Behavior, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Kevin Munger"
                ],
                "title": "Tweetment effects on the tweeted: Experimentally reducing racist harassment",
                "pub_date": "2017",
                "pub_title": "Political Behavior",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_86",
            "content": "Sarah Myers West, Censored, suspended, shadowbanned: User interpretations of content moderation on social media platforms, 2018, New Media & Society, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Sarah Myers West"
                ],
                "title": "Censored, suspended, shadowbanned: User interpretations of content moderation on social media platforms",
                "pub_date": "2018",
                "pub_title": "New Media & Society",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_87",
            "content": "Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry, and Verena Rieser, 2017, Why we need new evaluation metrics for nlg, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Jekaterina Novikova",
                    "Ond\u0159ej Du\u0161ek"
                ],
                "title": "Amanda Cercas Curry, and Verena Rieser",
                "pub_date": "2017",
                "pub_title": "Why we need new evaluation metrics for nlg",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_88",
            "content": "UNKNOWN, None, 2018, A survey on automatic detection of hate speech in text, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "A survey on automatic detection of hate speech in text",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_89",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting on association for computational linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th annual meeting on association for computational linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "49-ARR_v1_90",
            "content": "UNKNOWN, None, 2020, Resources and benchmark corpora for hate speech detection: a systematic review. Language Resources and Evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Resources and benchmark corpora for hate speech detection: a systematic review. Language Resources and Evaluation",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_91",
            "content": "Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding, William Wang, A benchmark dataset for learning to intervene in online hate speech, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Jing Qian",
                    "Anna Bethke",
                    "Yinyin Liu",
                    "Elizabeth Belding",
                    "William Wang"
                ],
                "title": "A benchmark dataset for learning to intervene in online hate speech",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "49-ARR_v1_92",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI Blog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Alec Radford",
                    "Jeffrey Wu",
                    "Rewon Child",
                    "David Luan",
                    "Dario Amodei",
                    "Ilya Sutskever"
                ],
                "title": "Language models are unsupervised multitask learners",
                "pub_date": "2019",
                "pub_title": "OpenAI Blog",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_93",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_94",
            "content": "Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A Smith, The risk of racial bias in hate speech detection, 2019, Proceedings of the 57th annual meeting of the association for computational linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Maarten Sap",
                    "Dallas Card",
                    "Saadia Gabriel",
                    "Yejin Choi",
                    "Noah A Smith"
                ],
                "title": "The risk of racial bias in hate speech detection",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th annual meeting of the association for computational linguistics",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_95",
            "content": "Carla Schieb, Mike Preuss, Governing hate speech by means of counterspeech on facebook, 2016, 66th ica annual conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Carla Schieb",
                    "Mike Preuss"
                ],
                "title": "Governing hate speech by means of counterspeech on facebook",
                "pub_date": "2016",
                "pub_title": "66th ica annual conference",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_96",
            "content": "Anna Schmidt, Michael Wiegand, A survey on hate speech detection using natural language processing, 2017, Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Anna Schmidt",
                    "Michael Wiegand"
                ],
                "title": "A survey on hate speech detection using natural language processing",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_97",
            "content": "UNKNOWN, None, 2016, The impact of counter-narratives. Institute for Strategic Dialogue, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "The impact of counter-narratives. Institute for Strategic Dialogue",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_98",
            "content": "Yi-Ling Serra Sinem Tekiroglu, Marco Chung,  Guerini, Generating counter narratives against online hate speech: Data and strategies, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Yi-Ling Serra Sinem Tekiroglu",
                    "Marco Chung",
                    " Guerini"
                ],
                "title": "Generating counter narratives against online hate speech: Data and strategies",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_99",
            "content": ", Demoting racial bias in hate speech detection, , Mengzhou Xia Anjalie Field Yulia Tsvetkov. 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [],
                "title": "Demoting racial bias in hate speech detection",
                "pub_date": null,
                "pub_title": "Mengzhou Xia Anjalie Field Yulia Tsvetkov. 2020",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_100",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_101",
            "content": "Bertie Vidgen, Leon Derczynski, Directions in abusive language training data, a systematic review: Garbage in, garbage out, 2020, Plos one, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Bertie Vidgen",
                    "Leon Derczynski"
                ],
                "title": "Directions in abusive language training data, a systematic review: Garbage in, garbage out",
                "pub_date": "2020",
                "pub_title": "Plos one",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_102",
            "content": "UNKNOWN, None, 2019, Challenges and frontiers in abusive content detection, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Challenges and frontiers in abusive content detection",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "49-ARR_v1_103",
            "content": "UNKNOWN, None, 2020, Learning from the worst: Dynamically generated datasets to improve online hate detection, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Learning from the worst: Dynamically generated datasets to improve online hate detection",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_104",
            "content": "Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, Jordan Boyd-Graber, Trick me if you can: Human-in-the-loop generation of adversarial question answering examples, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Eric Wallace",
                    "Pedro Rodriguez",
                    "Shi Feng",
                    "Ikuya Yamada",
                    "Jordan Boyd-Graber"
                ],
                "title": "Trick me if you can: Human-in-the-loop generation of adversarial question answering examples",
                "pub_date": "2019",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_105",
            "content": "Alex Wang, Kyunghyun Cho, Bert has a mouth, and it must speak, 2019, Bert as a markov random field language model, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Alex Wang",
                    "Kyunghyun Cho"
                ],
                "title": "Bert has a mouth, and it must speak",
                "pub_date": "2019",
                "pub_title": "Bert as a markov random field language model",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_106",
            "content": "Ke Wang, Xiaojun Wan, Sentigan: Generating sentimental texts via mixture adversarial networks, 2018, IJCAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Ke Wang",
                    "Xiaojun Wan"
                ],
                "title": "Sentigan: Generating sentimental texts via mixture adversarial networks",
                "pub_date": "2018",
                "pub_title": "IJCAI",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_107",
            "content": "Zeerak Waseem, Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter, 2016, Proceedings of the first workshop on NLP and computational social science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Zeerak Waseem"
                ],
                "title": "Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter",
                "pub_date": "2016",
                "pub_title": "Proceedings of the first workshop on NLP and computational social science",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_108",
            "content": "Sam Wiseman, Stuart Shieber, Alexander Rush, Challenges in data-to-document generation, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Sam Wiseman",
                    "Stuart Shieber",
                    "Alexander Rush"
                ],
                "title": "Challenges in data-to-document generation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "49-ARR_v1_109",
            "content": "UNKNOWN, None, , Jason Weston, and Emily Dinan. 2020. Recipes for safety in open-domain chatbots. arXiv e-prints, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Jason Weston, and Emily Dinan. 2020. Recipes for safety in open-domain chatbots. arXiv e-prints",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_110",
            "content": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan, DIALOGPT : Largescale generative pre-training for conversational response generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Yizhe Zhang",
                    "Siqi Sun",
                    "Michel Galley",
                    "Yen-Chun Chen",
                    "Chris Brockett",
                    "Xiang Gao",
                    "Jianfeng Gao",
                    "Jingjing Liu",
                    "Bill Dolan"
                ],
                "title": "DIALOGPT : Largescale generative pre-training for conversational response generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_111",
            "content": "UNKNOWN, None, 2021, Generate, prune, select: A pipeline for counterspeech generation against online hate speech, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Generate, prune, select: A pipeline for counterspeech generation against online hate speech",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_112",
            "content": "UNKNOWN, None, , The vast majority of Muslims in the world are peaceful and friendly people. I don't see how you can say such a thing about them, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "The vast majority of Muslims in the world are peaceful and friendly people. I don't see how you can say such a thing about them",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_113",
            "content": "UNKNOWN, None, , The vast majority of Muslims are peaceful people who have lived in peace and harmony for many years. How can you say this about a religion that promotes terror and war? BERT -Top k : islam is a religion of peace, and it is not a peaceful religion of violence. it is about peace and understanding of the world's problems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "The vast majority of Muslims are peaceful people who have lived in peace and harmony for many years. How can you say this about a religion that promotes terror and war? BERT -Top k : islam is a religion of peace, and it is not a peaceful religion of violence. it is about peace and understanding of the world's problems",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_114",
            "content": "UNKNOWN, None, , The vast majority of Muslims in the UK are peaceful people who share many values and respect, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "The vast majority of Muslims in the UK are peaceful people who share many values and respect",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_115",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v1_116",
            "content": "UNKNOWN, None, , the first set of experiments as described in Section 3.1. Finally, the best fine-tuning configuration of the GPT-2 medium model for APE was obtained with a learning rate of 2e-5 for 3 epochs resulting in 3, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "the first set of experiments as described in Section 3.1. Finally, the best fine-tuning configuration of the GPT-2 medium model for APE was obtained with a learning rate of 2e-5 for 3 epochs resulting in 3",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "49-ARR_v1_0@0",
            "content": "Using Pre-trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_0",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_2@0",
            "content": "In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_2",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_2@1",
            "content": "We first present a comparative study to determine whether there is a particular Language Model (or class of LMs) and a particular decoding mechanism which is the most appropriate to generate CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_2",
            "start": 175,
            "end": 369,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_2@2",
            "content": "Findings show that autoregressive models combined with stochastic decodings are the most promising.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_2",
            "start": 371,
            "end": 469,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_2@3",
            "content": "We then investigate how an LM performs in generating a CN with regard to an unseen target of hate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_2",
            "start": 471,
            "end": 568,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_2@4",
            "content": "We find out that a key element for successful 'out of target' experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_2",
            "start": 570,
            "end": 847,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_2@5",
            "content": "We finally introduce the idea of a pipeline based on the addition of an automatic post-editing step to refine generated CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_2",
            "start": 849,
            "end": 972,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_4@0",
            "content": "Hate Speech (HS) has found fertile ground in Social Media Platforms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_4",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_4@1",
            "content": "Actions undertaken by such platforms to tackle online hatred consist in identifying possible sources of hate and removing them by means of content deletion, account suspension or shadow-banning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_4",
            "start": 69,
            "end": 262,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_4@2",
            "content": "However, these actions are often interpreted and denounced as censorship by the affected users and political groups (Myers West, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_4",
            "start": 264,
            "end": 398,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_4@3",
            "content": "For this reason, such restrictions can have the opposite effect of exacerbating the hostility of the haters (Munger, 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_4",
            "start": 400,
            "end": 522,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_5@0",
            "content": "An alternative strategy, that is looming on the horizon, is based on the use of Counter Narratives (CN).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_5",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_5@1",
            "content": "CNs are \"all communicative actions aimed at refuting hate speech through thoughtful and cogent reasons, and true and fact-bound arguments\" (Schieb and Preuss, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_5",
            "start": 105,
            "end": 269,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_5@2",
            "content": "As a de-escalating measure, CNs have been proven to be successful in diminishing hate, while preserving the freedom of speech (Benesch, 2014;Gagliardone et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_5",
            "start": 271,
            "end": 437,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_5@3",
            "content": "An example <HS, CN > pair is shown below: HS: Women are basically childlike, they remain this way most of their lives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_5",
            "start": 439,
            "end": 556,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_5@4",
            "content": "Soft and emotional.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_5",
            "start": 558,
            "end": 576,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_5@5",
            "content": "It has devastated our once great patriarchal civilizations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_5",
            "start": 578,
            "end": 636,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_5@6",
            "content": "CN: Without softness and emotions there would be just brutality and cruelty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_5",
            "start": 638,
            "end": 713,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_5@7",
            "content": "Not all women are soft and emotional and many men have these characteristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_5",
            "start": 715,
            "end": 791,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_5@8",
            "content": "To perpetuate these socially constructed gender profiles maintains norms which oppress anybody.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_5",
            "start": 793,
            "end": 887,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_6@0",
            "content": "Based on their effectiveness, CNs have started being employed by NGOs to counter online hate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_6",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_6@1",
            "content": "Since for NGO operators it is impossible to manually write responses to all instances of hate, a line of NLP research has recently emerged, focusing on designing systems to automatically generate CN suggestions to fight online hateful messages (Qian et al., 2019;Tekiroglu et al., 2020;Fanton et al., 2021;Chung et al., 2021a;Zhu and Bhat, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_6",
            "start": 94,
            "end": 439,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_6@2",
            "content": "In this study, our main goal is to compare pre-trained language models (LM) and decoding mechanisms in order to understand their pros and cons in generating CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_6",
            "start": 441,
            "end": 601,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_6@3",
            "content": "Thus, we use various automatic metrics and manual evaluations with expert judgments to assess several LMs, representing the main categories of the model mechanisms, and decoding methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_6",
            "start": 603,
            "end": 788,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_6@4",
            "content": "We further test the robustness of the finetuned LMs in generating CNs for an unseen target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_6",
            "start": 790,
            "end": 880,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_6@5",
            "content": "Results show that autoregressive models are in general more suited for the task, and while stochastic decoding mechanisms can generate more novel, diverse, and informative outputs, the deterministic decoding is useful in scenarios where more generic and less novel (yet 'safer') CNs are needed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_6",
            "start": 882,
            "end": 1175,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_6@6",
            "content": "Furthermore, in out-of-target experiments we find that the similarity of targets (e.g. JEWS and MUSLIMS as religious groups) in training data plays a crucial role for the effectiveness of portability to new targets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_6",
            "start": 1177,
            "end": 1391,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_6@7",
            "content": "We finally show a promising research direction of leveraging gold human edits for building an additional automatic post-editing step to correct errors made by LMs during generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_6",
            "start": 1393,
            "end": 1573,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_6@8",
            "content": "To the best of our knowledge, this is the first study systematically analysing state of the art pre-trained LMs in CN generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_6",
            "start": 1575,
            "end": 1703,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_7@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_7",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_8@0",
            "content": "In this section we first discuss standard approaches to hate countering and studies on CN effectiveness on Social Media Platforms, then the existing CN data collection and generation strategies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_8",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_9@0",
            "content": "Hate countering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_9",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_9@1",
            "content": "NLP has started addressing the phenomenon of the proliferation of HS by creating datasets for automatic detection (Mathew et al., 2021;Cao et al., 2020;Kumar et al., 2018;Hosseinmardi et al., 2015;Waseem, 2016;Burnap and Williams, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_9",
            "start": 17,
            "end": 253,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_9@2",
            "content": "Several surveys provide a review on the existing approaches on the topic (Poletto et al., 2020;Schmidt and Wiegand, 2017;Nunes and Fortuna, 2018), also addressing the ethical challenges of the task (Kiritchenko et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_9",
            "start": 255,
            "end": 479,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_9@3",
            "content": "Automatic detection of HS presents some drawbacks (Vidgen and Derczynski, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_9",
            "start": 481,
            "end": 560,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_9@4",
            "content": "First of all, the datasets might include biases, and the models tend to replicate such biases (Binns et al., 2017;Davidson et al., 2019;Sap et al., 2019;Tsvetkov, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_9",
            "start": 562,
            "end": 730,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_9@5",
            "content": "Moreover, the end goals for which HS detection is employed are often charged with censorship of the freedom of speech by concerned users (Munger, 2017;Myers West, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_9",
            "start": 732,
            "end": 900,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_9@6",
            "content": "In this scenario, NGOs have started employing CNs to counter online hate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_9",
            "start": 902,
            "end": 974,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_9@7",
            "content": "CNs have been shown to be effective in reducing linguistic violence (Benesch, 2014;Gagliardone et al., 2015;Schieb and Preuss, 2016;Silverman et al., 2016;Mathew et al., 2019); moreover, even if they might not influence the view of the extremists, they are still effective in presenting alternative and non-hateful viewpoints to the bystanders (Allison and Bussey, 2016;Anderson et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_9",
            "start": 976,
            "end": 1368,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_10@0",
            "content": "The existing studies for collecting CN datasets employ four main approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_10",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_10@1",
            "content": "Crawling consists in automatically scraping websites, starting from an HS content and searching for possible CNs among the responses (Mathew et al., 2018(Mathew et al., , 2019.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_10",
            "start": 77,
            "end": 252,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_10@2",
            "content": "With crowdsourcing CNs are written by non-expert paid workers as responses to provided hate content (Qian et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_10",
            "start": 254,
            "end": 373,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_10@3",
            "content": "Nichesourcing relies on a niche group of experts for data collection (De Boer et al., 2012), and it was employed by Chung et al. (2019) for CN collection using NGO's operators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_10",
            "start": 375,
            "end": 550,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_10@4",
            "content": "Hybrid approaches use a combination of LMs and humans to collect data (Wallace et al., 2019;Dinan et al., 2019;. Studies on CN collection are presented in more detail by Tekiroglu et al. (2020);Fanton et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_10",
            "start": 552,
            "end": 766,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_10@5",
            "content": "2019) employ a mix of automatic and human intervention to generate CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_10",
            "start": 768,
            "end": 838,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_10@6",
            "content": "Zhu and Bhat (2021) propose an entirely automated pipeline for synthetic CN generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_10",
            "start": 840,
            "end": 926,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_10@7",
            "content": "Instead, our work aims at an end-to-end system for CN generation, which is developed as a downstream task after comparing the state of the art pre-trained LMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_10",
            "start": 928,
            "end": 1086,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_10@8",
            "content": "Other lines of work include CN generation for under-resourced languages such as for Italian (Chung et al., 2020), and the generation of knowledge-bound CNs, which allows the production of CNs based on grounded and up-to-date facts and plausible arguments, avoiding hallucination phenomena (Chung et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_10",
            "start": 1088,
            "end": 1398,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_11@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_11",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_12@0",
            "content": "In this section, we present the CN dataset, the language models, and the decoding mechanisms employed for our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_12",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_13@0",
            "content": "Dataset for fine-tuning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_13",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_14@0",
            "content": "For this study we rely on the dataset proposed by Fanton et al. (2021), which is the only available dataset that grants both the target diversity and the CN quality we aim for.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_14",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_14@1",
            "content": "The dataset was collected with a human-in-the-loop approach, by employing an autoregressive LM (GPT-2) paired with three expert human reviewers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_14",
            "start": 177,
            "end": 320,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_14@2",
            "content": "It features 5003 <HS, CN > pairs, covering several targets of hate including DISABLED, JEWS, LGBT+, MIGRANTS, MUSLIMS, POC, WOMEN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_14",
            "start": 322,
            "end": 451,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_14@3",
            "content": "The residual categories are collapsed to the label OTHER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_14",
            "start": 453,
            "end": 509,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_14@4",
            "content": "We partitioned the dataset into training, validation, and test sets with the ratio: 8 : 1 : 1 (i. e. 4003, 500 and 500 pairs), ensuring that all sets share the same target distribution, and no repetition of HS across the sets is allowed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_14",
            "start": 511,
            "end": 747,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_15@0",
            "content": "Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_15",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_16@0",
            "content": "We experiment with 5 Transformer based LMs (Vaswani et al., 2017) (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_16",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_16@1",
            "content": "It is a bidirectional autoencoder that can be adapted to text generation (Wang and Cho, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_16",
            "start": 74,
            "end": 167,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_16@2",
            "content": "GPT-2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_16",
            "start": 169,
            "end": 174,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_16@3",
            "content": "The Generative Pre-trained Transformer 2 is an autoregressive model and it is specifically built for text generation (Radford et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_16",
            "start": 176,
            "end": 315,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_16@4",
            "content": "DialoGPT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_16",
            "start": 317,
            "end": 325,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_16@5",
            "content": "The Dialogue Generative Pretrained Transformer is the extension of GPT-2 specifically created for conversational response generation (Zhang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_16",
            "start": 327,
            "end": 480,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_16@6",
            "content": "BART.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_16",
            "start": 482,
            "end": 486,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_16@7",
            "content": "BART is a seq2seq model that combines two different Transformer architectures (Lewis et al., 2020): a bidirectional encoder and a leftto-right autoregressive decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_16",
            "start": 488,
            "end": 653,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_17@0",
            "content": "The Text-to-Text Transfer Transformer proposed by Raffel et al. ( 2020) is a seq2seq model with an encoder-decoder Transformer architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_17",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_17@1",
            "content": "While all the other models could be fine-tuned directly for the generation task, for BERT we needed to warmstart an encoder-decoder model using BERT checkpoints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_17",
            "start": 141,
            "end": 301,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_17@2",
            "content": "Further details can be found in Appendix A.1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_17",
            "start": 303,
            "end": 346,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_18@0",
            "content": "Decoding mechanisms",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_18",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_19@0",
            "content": "We utilize 4 decoding mechanisms: a deterministic (Beam Search) and three stochastic (Top-k, Top-p, and a combination of the two).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_19",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_19@1",
            "content": "Beam Search (BS).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_19",
            "start": 131,
            "end": 147,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_19@2",
            "content": "The Beam Search algorithm is designed to pick the most-likely sequence (Li et al., 2016;Wiseman et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_19",
            "start": 149,
            "end": 258,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_19@3",
            "content": "Top-k (Top k ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_19",
            "start": 260,
            "end": 274,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_19@4",
            "content": "The sampling procedure proposed by Fan et al. (2018) selects a random word from the k most probable ones, at each time step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_19",
            "start": 276,
            "end": 399,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_19@5",
            "content": "Top-p (Top p ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_19",
            "start": 401,
            "end": 415,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_19@6",
            "content": "Also known as Nucleus Sampling, the parameter p indicates the total probability for the pooled candidates, at each time step (Holtzman et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_19",
            "start": 417,
            "end": 565,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_19@7",
            "content": "Combining Top-p and Top-k (Top pk ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_19",
            "start": 567,
            "end": 602,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_19@8",
            "content": "At decoding stage, it is possible to combine the parameters p and k.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_19",
            "start": 604,
            "end": 671,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_19@9",
            "content": "This is a Nucleus Sampling constrained to the Top-k most probable words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_19",
            "start": 673,
            "end": 744,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_20@0",
            "content": "In our experiments we used the following parameters: Beam-Search with 5 beams and repetition penalty = 2; Top-k with k = 40; Top-p with p = .92; Top pk with k = 40 and p = .92.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_20",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_21@0",
            "content": "Evaluation metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_21",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_22@0",
            "content": "We use several metrics to evaluate various aspects of the CN generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_22",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_22@1",
            "content": "Overlap Metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_22",
            "start": 73,
            "end": 88,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_22@2",
            "content": "These metrics depend on the n-gram similarity of the generated outputs to a set of reference texts in order to assess the quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_22",
            "start": 90,
            "end": 219,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_22@3",
            "content": "We used our gold CNs as references and the CNs generated by the different models, as candidates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_22",
            "start": 221,
            "end": 316,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_23@0",
            "content": "In particular, we employed three BLEU variants: BLEU-1 (B-1), BLEU-3 (B-3) and BLEU-4 (B-4) (Papineni et al., 2002), and ROUGE-L (ROU) (Lin, 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_23",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_23@1",
            "content": "Diversity metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_23",
            "start": 148,
            "end": 165,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_23@2",
            "content": "They are used to measure how diverse and novel the produced CNs are.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_23",
            "start": 167,
            "end": 234,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_23@3",
            "content": "In particular, we utilized Repetition Rate (RR) to measure the repetitiveness among generated CNs, in terms of the average ratios of non-singleton n-grams present in the corpus (Bertoldi et al., 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_23",
            "start": 236,
            "end": 436,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_23@4",
            "content": "We also used Novelty (NOV), based on Jaccard similarity (Wang and Wan, 2018;Dziri et al., 2019), to compute the amount of novel content that is present in the generated CNs as compared to the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_23",
            "start": 438,
            "end": 643,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_23@5",
            "content": "Human evaluation metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_23",
            "start": 645,
            "end": 669,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_23@6",
            "content": "Albeit more difficult to attain, human judgements provide a more reliable evaluation and a deeper understanding than automatic metrics (Belz and Reiter, 2006;Novikova et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_23",
            "start": 671,
            "end": 851,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_23@7",
            "content": "To this end, we specified the following dimensions for the evaluation of CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_23",
            "start": 853,
            "end": 929,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_23@8",
            "content": "Suitableness (SUI): measures how suitable a CN is to the HS in terms of semantic relatedness and in terms of adherence to CN guidelines 1 ; Grammaticality (GRM): how grammatically correct a generated CN is; Specificity (SPE): how specific are the arguments brought by the CN in response to the HS; Choose-or-not (CHO): determines whether the annotators would select that CN to post-edit and use it in a real case scenario as in the set up presented by Chung et al. (2021b); Is-best (BEST): whether the CN is the absolute best among the ones generated for an HS (i. e. whether the annotators would pick up exactly that CN if they had to use it in a real case scenario).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_23",
            "start": 931,
            "end": 1598,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_24@0",
            "content": "The first three dimensions are rated with a 5points Likert scale and follow the evaluation procedure described by Chung et al. (2020), whereas both choose-or-not and is-best are binary ratings (0, 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_24",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_24@1",
            "content": "Choose-or-not allows for multiple CNs to be selected for the same HS, while only one CN can be selected for is-best for each HS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_24",
            "start": 201,
            "end": 328,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_24@2",
            "content": "Toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_24",
            "start": 330,
            "end": 338,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_24@3",
            "content": "2 It determines how \"rude, disrespectful, or unreasonable\" a text is.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_24",
            "start": 340,
            "end": 408,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_24@4",
            "content": "Toxicity has been employed both to detect the bias present in LMs (Gehman et al., 2020) and as a solution to mitigate such bias (Gehman et al., 2020;Xu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_24",
            "start": 410,
            "end": 575,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_24@5",
            "content": "Syntactic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_24",
            "start": 577,
            "end": 594,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_24@6",
            "content": "A high syntactic complexity can be used as a proxy for an LM's ability of generating complex arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_24",
            "start": 596,
            "end": 698,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_24@7",
            "content": "We used the syntactic dependency parser of spaCy 3 for the task, focusing on the following measures: Maximum Syntactic Depth (MSD): the maximum depth among the dependency trees calculated over each sentence composing a CN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_24",
            "start": 700,
            "end": 921,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_24@8",
            "content": "Average Syntactic Depth (ASD): the average depth of the sentences in each CN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_24",
            "start": 923,
            "end": 999,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_24@9",
            "content": "Number of Sentences (NST): the number of sentences composing a CN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_24",
            "start": 1001,
            "end": 1066,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_25@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_25",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_26@0",
            "content": "We performed two sets of experiments: first, we assessed how LMs perform in the task of generating CNs with different decoding mechanisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_26",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_26@1",
            "content": "Then, we selected the best model from the first round of experiments and tested its generalization capabilities when confronted with an unseen target of hate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_26",
            "start": 139,
            "end": 296,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_27@0",
            "content": "LMs and decoding experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_27",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_28@0",
            "content": "For the first round of experiments, in order to avoid possible unfair assessments given by the open nature of the generative task (i. e. a highly suitable CN candidate could be scored low due to its difference from the reference CN), at test time we allowed the generation of several candidates for each HS/LM/decoding mechanism combination.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_28",
            "start": 0,
            "end": 340,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_28@1",
            "content": "We loosely drew inspiration from the Rank-N Accuracy procedure and the 'generate, prune, select' procedure (Zhu and Bhat, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_28",
            "start": 342,
            "end": 469,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_28@2",
            "content": "In particular, given an LM and a decoding mechanism, we generated 5 CNs for each HS in the test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_28",
            "start": 471,
            "end": 570,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@0",
            "content": "We set up the automatic evaluation strategy as displayed in Figure 1. First, we scored each CN with the overlap metrics presented in Section 4, using the gold CN as a reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@1",
            "content": "Next, we ranked the candidate CNs with respect to the overlap scores and computed the mean of the rankings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 178,
            "end": 284,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@2",
            "content": "Then, we selected the best ones according to the following criteria: Best LM selects the single best CN among the 20 generated by each model for an HS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 286,
            "end": 436,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@3",
            "content": "Best D selects the single best CN among the 25 generated by each decoding for an HS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 438,
            "end": 521,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@4",
            "content": "Best LM+D selects the single best CN among the 5 generated with each model-decoding combination.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 523,
            "end": 618,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@5",
            "content": "Moreover, we assessed the overall corpus-wise quality of the generated CNs with respect to the models, to the decoding mechanisms, and to the model-decoding combinations via the diversity metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 620,
            "end": 815,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@6",
            "content": "Human evaluation on a sample To perform the human evaluation we referred to the Best LM generations and sampled 200 instances from it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 817,
            "end": 950,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@7",
            "content": "Each instance comprises an HS and 5 relevant CNs, each generated by a different model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 952,
            "end": 1037,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@8",
            "content": "We recruited 2 annotators who were trained extensively for the task following the procedure used by Fanton et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 1039,
            "end": 1159,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@9",
            "content": "The expert annotators were asked to evaluate the 5 CNs corresponding to the HS, according to the dimensions described in Section 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 1161,
            "end": 1291,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_29@10",
            "content": "We enriched the evaluation of this subset with the toxicity and the syntactic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_29",
            "start": 1293,
            "end": 1378,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_30@0",
            "content": "Results of the first set of experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_30",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_31@0",
            "content": "The results of the experiments on the LMs and the decoding mechanisms are reported in this section 4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_31",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_32@0",
            "content": "The results of the comparison of the models on the Best LM generations can be found in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_32",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_32@1",
            "content": "Regarding the overlap and diversity metrics, DialoGPT records the best or the second best score in all the metrics, apart from novelty where it still achieves a high score (0.643) close to the best performance (0.655).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_32",
            "start": 96,
            "end": 313,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_32@2",
            "content": "T5 also achieves high scores, especially on ROUGE, BLEU-1 and novelty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_32",
            "start": 315,
            "end": 384,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_33@0",
            "content": "BART, instead, is the best model according to human evaluation metrics, apart from specificity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_33",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_33@1",
            "content": "On the other hand, it shows poor performances in terms of diversity metrics, indicating that it tends to produce grammatical and suitable but very generic responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_33",
            "start": 96,
            "end": 260,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_34@0",
            "content": "BERT records the worst scores for all the overlap and diversity metrics apart from novelty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_34",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_34@1",
            "content": "However, it also achieves the best syntactic metric results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_34",
            "start": 92,
            "end": 151,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_34@2",
            "content": "Therefore, it is evident that BERT's output is more complex, but very repetitive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_34",
            "start": 153,
            "end": 233,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_34@3",
            "content": "The combination of these aspects eventually affects the clarity of BERT's output such that it yields poor results in the human evaluation, in particular for grammaticality (4.2, while other models are above 4.6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_34",
            "start": 235,
            "end": 446,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_35@0",
            "content": "GPT-2 overall yields very competitive results for several groups of metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_35",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_35@1",
            "content": "It obtains the secondhighest novelty score (0.653) and the best RR (7.736).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_35",
            "start": 77,
            "end": 151,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_35@2",
            "content": "It also achieves the second best results on BLEU-3, maximum syntactic depth and number of sentences, and the best results on toxicity and specificity (2.880) indicating the ability to produce complex, suitable, focused and diverse CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_35",
            "start": 153,
            "end": 387,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_36@0",
            "content": "After the human evaluation we ran a qualitative interview with the annotators, whose feedback on the data strengthened the results we observed and the conclusion we drew.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_36",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_36@1",
            "content": "For instance, they reported the repetition of simple, yet catch-them-all, expressions (e.g. \"they are our brothers and sisters\") regardless of the target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_36",
            "start": 171,
            "end": 324,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_36@2",
            "content": "Further inspections found that those CNs were mainly produced by BERT, which is in line with BERT's RR score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_36",
            "start": 326,
            "end": 434,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_37@0",
            "content": "Best Decoding mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_37",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_37@1",
            "content": "The results calculated on Best D output are presented in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_37",
            "start": 25,
            "end": 89,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_37@2",
            "content": "Top k is the best performing decoding mechanism achieving the best results on the diversity metrics, BLEU-3 and BLEU-4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_37",
            "start": 91,
            "end": 209,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_37@3",
            "content": "It is also the best performing for specificity, maximum syntactic depth and number of sentences, and the second best for average syntactic depth and toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_37",
            "start": 211,
            "end": 368,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_38@0",
            "content": "The other stochastic decoding mechanisms perform well too.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_38",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_38@1",
            "content": "Top p yields competitive results on both diversity and overlap metrics; it is the second best for specificity, and achieves good results on the syntactic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_38",
            "start": 59,
            "end": 220,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_38@2",
            "content": "Top pk has a good performance on the overlap metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_38",
            "start": 222,
            "end": 274,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_38@3",
            "content": "It obtains the second-highest scores in most of the human evaluation metrics and the lowest in toxicity, and it reaches a reasonable specificity score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_38",
            "start": 276,
            "end": 426,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_39@0",
            "content": "On the other hand, BS does not achieve particularly good results, except for the ROUGE score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_39",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_39@1",
            "content": "Even if it is the best decoding with respect to the human evaluation, this comes at the cost of specificity and diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_39",
            "start": 94,
            "end": 215,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_39@2",
            "content": "Through a post-hoc manual analysis we observed that it was due to the deterministic nature of BS, that tends to choose the most probable sequences, i. e. the \"safest\", thus resulting in vague and repetitive outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_39",
            "start": 217,
            "end": 431,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_40@0",
            "content": "Best Model-Decoding combination Here we briefly discuss the results of the evaluation obtained on the Best LM+D generations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_40",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_40@1",
            "content": "In particular, the autoregressive models GPT-2 and DialoGPT behave similarly with similar decoding mechanisms, such that BS outputs the best results for almost all the overlap metrics, and the worst for the diversity metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_40",
            "start": 125,
            "end": 349,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_40@2",
            "content": "On the contrary, for the other models, the results achieved with stochastic decoding mechanisms are the best for the overlap metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_40",
            "start": 351,
            "end": 483,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_40@3",
            "content": "In almost all the cases, we observe that the stochastic decoding mechanisms perform better on syntactic and diversity metrics and on toxicity, while for the human evaluation metrics BS tends to be the best, except for specificity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_40",
            "start": 485,
            "end": 714,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_40@4",
            "content": "A detailed discussion can be found in Appendix A.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_40",
            "start": 716,
            "end": 766,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_40@5",
            "content": "Discussion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_40",
            "start": 768,
            "end": 778,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_40@6",
            "content": "In this set of experiments, we found that the autoregressive models perform the best according to a combination of several metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_40",
            "start": 780,
            "end": 910,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_40@7",
            "content": "Still, it could be argued that the good performance of the GPT-2 model we fine-tuned is due to the fact that generated CNs and gold CNs derive from a similar distribution (GPT-2 was employed in the human-in-the-loop process used to create the reference dataset from Fanton et al. ( 2021)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_40",
            "start": 912,
            "end": 1200,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_40@8",
            "content": "While we recognize that this could partially explain the performance of our GPT-2 model, it does not explain the performance of DialoGPT, which is pre-trained on a completely different dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_40",
            "start": 1202,
            "end": 1394,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_40@9",
            "content": "Therefore, we can reasonably conclude that autoregressive models are particularly suited for the task, regardless of the pre-training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_40",
            "start": 1396,
            "end": 1534,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_41@0",
            "content": "With respect to the decoding mechanisms, we record high repetitiveness and low novelty for the Example CNs generated in this session of experiments, along with some qualitative analysis, can be found in Appendix A.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_41",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_42@0",
            "content": "Leave One Target Out experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_42",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_43@0",
            "content": "In the second stage, we built a set of cross-domain experiments to capture the generalization capabilities of the best LM determined in the previous experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_43",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_43@1",
            "content": "Specifically, we concentrate on assessing how much a pre-trained language model fine-tuned on a pool of hate targets can generalize to an unseen target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_43",
            "start": 162,
            "end": 313,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_44@0",
            "content": "Thus, for the out of target experiment we selected the LM that we deem the most prominent in order to reduce the number of LM configurations to compare.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_44",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_44@1",
            "content": "In particular, since we want to examine the generalization capability of the LM, the generation of novel CNs, in comparison to the training data, is given primary importance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_44",
            "start": 153,
            "end": 326,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_44@2",
            "content": "Secondly, specificity is also crucial since it signifies the ability of the LM/decoding mechanism in generating accurate CNs and avoiding vague yet suitable, catch-all CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_44",
            "start": 328,
            "end": 499,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_44@3",
            "content": "In contrast, repetitiveness is an undesirable feature of CNs, as it signals the tendency of a model to produce less flexible content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_44",
            "start": 501,
            "end": 633,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_44@4",
            "content": "Given these considerations, we chose to employ GPT-2 with Top k decoding for the Leave One Target Out (LOTO) experiments since it is the configuration achieving the best trade-off amongst all the others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_44",
            "start": 635,
            "end": 837,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_44@5",
            "content": "This set of experiments is structured in 3 steps, replicated for each of the selected targets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_44",
            "start": 839,
            "end": 932,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_44@6",
            "content": "We selected the targets with the highest number of examples (MUSLIMS, MIGRANTS, WOMEN, LGBT+ and JEWS) to have a sufficient sized test set for each configuration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_44",
            "start": 934,
            "end": 1095,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_45@0",
            "content": "First, we sampled from the Fanton et al. ( 2021) dataset 600 pairs for each LOTO target, in order to have a balanced setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_45",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_45@1",
            "content": "Additionally, POC and DISABLED were always kept in the training set, and we removed multi-target cases from OTHER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_45",
            "start": 126,
            "end": 239,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_45@2",
            "content": "The resulting dataset consists of 3729 instances (further details are provided in Appendix A.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_45",
            "start": 241,
            "end": 336,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_45@3",
            "content": "Secondly, we fine-tuned 5 different configurations of the LM, and in each configuration one of the 5 LOTO targets is not present in the training data: LM -JEWS , LM -LGTB+ , LM -MIGRANTS , LM -MUSLIMS and LM -WOMEN .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_45",
            "start": 338,
            "end": 553,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_45@4",
            "content": "Finally, we tested each LOTO model on the 600 HSs in the test set made of \"left out\" target examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_45",
            "start": 555,
            "end": 655,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_45@5",
            "content": "For instance, the model LM -JEWS is used for generating the CNs for the target JEWS, after being trained on <HS, CN > data without any instances with the label JEWS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_45",
            "start": 657,
            "end": 821,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_45@6",
            "content": "We generated 5 CNs for each HS and selected the best CN according to the procedure described in Section 5.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_45",
            "start": 823,
            "end": 930,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@0",
            "content": "We analyse the CNs generated with the LOTO models through overlap and diversity metrics (Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@1",
            "content": "We refer to Appendix A.4 for the comparison be- For all the targets we record higher novelty scores as compared to the previous experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 99,
            "end": 238,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@2",
            "content": "Higher novelty ranges indicate that conditioning with new material (i. e. HS for the unseen targets) induces GPT-2 to produce new arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 240,
            "end": 379,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@3",
            "content": "On the other hand, as expected, the overlap scores for LOTO are remarkably lower than those from the previous experiments (Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 381,
            "end": 512,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@4",
            "content": "Therefore, we can infer that generalizing to an unseen target is harder than generalizing to an unseen HS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 514,
            "end": 619,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@5",
            "content": "We also found out that the CNs generated in the LM -MUSLIMS and LM -WOMEN configurations obtain the highest overlap scores (Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 621,
            "end": 753,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@6",
            "content": "We hypothesize that the high scores can be explained by the presence of a target in the LOTO training that is highly similar to the left out one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 755,
            "end": 899,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@7",
            "content": "To this end, we computed the novelty between each target subset of the training data and the LOTO test data for that configuration (see Appendix A.4 for details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 901,
            "end": 1062,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@8",
            "content": "The reference CNs for LM -MUSLIMS record the lowest novelty scores with respect to the JEWS subset of the training set (i. e. 0.761).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 1064,
            "end": 1196,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@9",
            "content": "Thus, it can be interpreted as the most influential portion of training data for the target MUSLIMS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 1198,
            "end": 1297,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@10",
            "content": "On the other hand, for LM -WOMEN the highest influence is recorded with the LGBT+ subset of the training data (i. e. 0.763).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 1299,
            "end": 1422,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_46@11",
            "content": "These results can be explained by the semantic similarity of the target MUSLIMS to JEWS, both being religious groups; and of WOMEN to LGBT+, both being related to gender issues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_46",
            "start": 1424,
            "end": 1600,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_47@0",
            "content": "As a complementary analysis, we consider two different computations of the reference CN novelty: with respect to the most influential target for each LOTO configuration, and with respect to the LOTO training data without the most influential target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_47",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_47@1",
            "content": "We computed the Pearson correlation between the overlap metrics and each of the two novelty computations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_47",
            "start": 250,
            "end": 354,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_47@2",
            "content": "In Figure 2, we observe that removing the influential target from the training data strongly decreases the correlation with the over-Figure 2: The correlation between the novelty of the reference CNs and overlap metrics: in each plot, the dots and the darker line correspond to the most influential target; the triangles and the lighter line correspond to the results calculated without it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_47",
            "start": 356,
            "end": 745,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_47@3",
            "content": "lap metrics (from an average of -0.889 to -0.416).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_47",
            "start": 747,
            "end": 796,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_47@4",
            "content": "Consequently, we can conclude that to obtain high overlap results in the LOTO experiments, it is necessary that the training data contains a target strongly connected to the left out one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_47",
            "start": 798,
            "end": 984,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_47@5",
            "content": "Most importantly, this connection is not arbitrarily decided but it is based on an a priori semantic similarity of the targets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_47",
            "start": 986,
            "end": 1112,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_48@0",
            "content": "Finally, we chose to generate also with the BS decoding mechanism, to use it as a baseline and compare it to the stochastic decoding mechanism (Top-k).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_48",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_48@1",
            "content": "In particular, we computed the Pearson correlation between the novelty of the reference CNs and the novelty of the candidate CNs with respect to the corresponding training data (Figure 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_48",
            "start": 152,
            "end": 339,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_48@2",
            "content": "We can observe that for the BS generation the novelty of the candidate CNs is lower than Top-k (0.67-0.74 vs. 0.75-0.77) and the correlation with the novelty of the reference is weaker (0.53 vs. 0.59).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_48",
            "start": 341,
            "end": 541,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_48@3",
            "content": "This confirms the lower generalization ability with the deterministic decoding mechanism (as compared to the stochastic) that tends to produce generic and repetitive responses regardless of the semantic distances of the LOTO targets from the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_48",
            "start": 543,
            "end": 798,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_49@0",
            "content": "Automatic Post-Editing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_49",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_50@0",
            "content": "In the previous experiments we fine-tuned our models making resort to <HS, CN > pairs alone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_50",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_50@1",
            "content": "Still the Fanton et al. ( 2021) dataset contains additional information that can be useful for our task: i. e. the original GPT-2 generation before undergoing human post-editing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_50",
            "start": 93,
            "end": 270,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_51@0",
            "content": "Thus, as a final experiment, we propose further improving CN generation by moving from an endto-end framework to a two stage pipeline, by decoupling CN generation from its 'final refinement'.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_51",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_51@1",
            "content": "In particular we propose the adoption of an Automatic Post-Editing (APE) stage in order to capture and utilize the nuances among the machine generated CNs and their human post-edited versions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_51",
            "start": 192,
            "end": 383,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_51@2",
            "content": "APE, which is used for automatically correcting errors made by MT systems before performing actual human post-editing, has been an important tool for machine translation (Knight and Chander, 1994;do Carmo et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_51",
            "start": 385,
            "end": 603,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_51@3",
            "content": "Considering its effectiveness in MT, we hypothesize that building a pipeline with CN generation and APE could alleviate the requirement of the final manual post-editing (Allen and Hogan, 2000;Chatterjee et al., 2019) to achieve better constructed CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_51",
            "start": 605,
            "end": 855,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_52@0",
            "content": "To this end, we fine-tuned another instance of GPT-2 medium model specifically for the post-editing task using the <HS, CN or , CN pe > triplets 5 , where CN or and CN pe denote the CNs originally generated by an LM and their human post-edited versions, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_52",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_52@1",
            "content": "The triplets were then filtered by removing those for which CN or = CN pe .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_52",
            "start": 268,
            "end": 342,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_52@2",
            "content": "More details about the experiment settings can be found in Appendix A.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_52",
            "start": 344,
            "end": 415,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_52@3",
            "content": "We have conducted two human evaluations over the subsets of: i) the CN or of the Fanton et al. 5 this is in line with the APE paradigm where the triplet is made of <source sentece, M Toutput, human post-edits>.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_52",
            "start": 417,
            "end": 626,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_53@0",
            "content": "(2021) test samples, ii) the CN outputs of the best model and decoding mechanism combination provided as the results of the first set of experiments, that yielded the top 50 TER scores with respect to the CN or s. The two expert annotators were asked to state their preferences among the 2 randomly sorted CNs, CN or and CN ape (automatically postedited output), for a given HS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_53",
            "start": 0,
            "end": 377,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_53@1",
            "content": "The annotators were also allowed to decide on a tie.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_53",
            "start": 379,
            "end": 430,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_53@2",
            "content": "Results, shown in Table 4, indicate that, albeit there are often ties and only a subset of CN or is actually modified, when there is a preference, it is predominantly in favour of the automatically post-edited versions over the GPT-2 generated CNs (26% vs. 14% for the test set, and 37% vs. 19% for the GPT-2 Top k generations, on average).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_53",
            "start": 432,
            "end": 771,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_53@3",
            "content": "Regarding the experiment results, we believe that APE is a highly promising direction to increase the efficacy of the CN generation models where generation quality and diversity is crucial, and considering that obtaining/enlarging expert datasets to train better models is not simple.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_53",
            "start": 773,
            "end": 1056,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_54@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_54",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_55@0",
            "content": "In this work, we focus on automatic CN generation as a downstream task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_55",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_55@1",
            "content": "First, we present a comparative study to determine the performances and peculiarities of several pretrained LMs and decoding mechanisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_55",
            "start": 72,
            "end": 207,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_55@2",
            "content": "We observe that the best results (in term of novelty and specificity) overall are achieved by the autoregressive models with stochastic decoding: GPT-2 with the Top k decoding mechanism, and DialoGPT with the combination Top pk .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_55",
            "start": 209,
            "end": 437,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_55@3",
            "content": "At the same time deterministic decoding can be used when more generic yet 'safer' CNs are preferred.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_55",
            "start": 439,
            "end": 538,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_56@0",
            "content": "Then, we investigate the performance of LMs in zero-shot generation for unseen targets of hate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_56",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_56@1",
            "content": "Hence, we fine-tuned 5 different versions of GPT-2, leaving out the examples pertaining to one target at each turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_56",
            "start": 96,
            "end": 210,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_56@2",
            "content": "We find out that for each configuration/version, there is a subset of the training data which is more influential with respect to the generated data (i. e. a target that shares some commonalities with the test target that can be defined a-priori).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_56",
            "start": 212,
            "end": 458,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_56@3",
            "content": "Finally, we introduce an experiment by training an automatic post-editing module to further improve the CN generation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_56",
            "start": 460,
            "end": 585,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_56@4",
            "content": "The notable human evaluation results paves the way for a promising future direction that decouples CN generation from its 'final refinement'.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_56",
            "start": 587,
            "end": 727,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_57@0",
            "content": "Although tackling online hatred through CNs inherently protects the freedom of speech and has been proposed as a better alternative to the detectremove-ban approaches, automatization of CN generation can still raise some ethical concerns and some measures must be taken to avoid undesired effects during research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_57",
            "start": 0,
            "end": 312,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_57@1",
            "content": "Thus, we address the relevant ethical considerations and our remedies as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_57",
            "start": 314,
            "end": 394,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_58@0",
            "content": "Annotation Guidelines: The well-being of the annotators was our top priority during the whole study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_58",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_58@1",
            "content": "Therefore, we strictly followed the guidelines created for CN studies (Fanton et al., 2021) that were previously adapted from (Vidgen et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_58",
            "start": 101,
            "end": 248,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_58@2",
            "content": "The human evaluations have been conducted with the help of two expert annotators in CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_58",
            "start": 250,
            "end": 337,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_58@3",
            "content": "We instructed the annotators in the aims of each evaluation, clearly explained the tasks, and then we exemplified proper <HS, CN > pairs with various types of CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_58",
            "start": 339,
            "end": 501,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_58@4",
            "content": "Most importantly, we limited the exposure to hateful content by providing a daily time limit of annotation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_58",
            "start": 503,
            "end": 609,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_58@5",
            "content": "Concerning the demographics, due to the harmful content that can be found in the data, all annotators were adult volunteers, perfectly aware of the objective of the study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_58",
            "start": 611,
            "end": 781,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_59@0",
            "content": "Dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_59",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_59@1",
            "content": "We purposefully chose an expert-based dataset in order to avoid the risk of modeling the language of real individuals to (i) prevent any privacy issue, (ii) avoid to model inappropriate CNs (e.g. containing abusive language) that could be produced by non-experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_59",
            "start": 9,
            "end": 271,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_59@2",
            "content": "The dataset also focuses on the CN diversity while keeping the HSs as stereotypical as possible so that our CN generation models have a very limited diversity on the hateful language, nearly precluding the misuse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_59",
            "start": 273,
            "end": 485,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_60@0",
            "content": "Computational Task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_60",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_60@1",
            "content": "CN generation models are not meant to be used in an autonomous way, since even the best models can still produce substandard CNs containing inappropriate or negative language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_60",
            "start": 20,
            "end": 194,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_60@2",
            "content": "Instead, following a Human-computer cooperation paradigm, our focus is on building models that can be helpful to NGO operators by providing them diverse and novel CN candidates for their hate countering activities and speed up the manual CN writing to a certain extent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_60",
            "start": 196,
            "end": 464,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_60@3",
            "content": "This approach also gives ground to some of the measures we used during evaluation (namely choose-or-not and is-best).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_60",
            "start": 466,
            "end": 582,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_61@0",
            "content": "Model Distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_61",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_61@1",
            "content": "In addition to the limited and simplified hateful content in the dataset we selected, we further reduce the risk of misuse by choosing a specific distribution strategy: i) we only make available the non-autoregressive models in order to eliminate the risk of using over-generation for hate speech creation, ii) we distribute such models only for research purposes and through a request based procedure in order to keep track of the possible users.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_61",
            "start": 20,
            "end": 466,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_62@0",
            "content": "A.1 Fine-tuning details As a rule-of-thumb, we chose one version smaller than the large version of each model, this corresponds to the medium versions for both DialoGPT and GPT-2, and base versions for the other models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_62",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_62@1",
            "content": "GPT-2 and DialoGPT achieve the lowest perplexity, training and evaluation loss, thus indicating a slightly more successful fine-tuning, which are reflected in the evaluations throughout the study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_62",
            "start": 220,
            "end": 415,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_62@2",
            "content": "We conducted a hyper-parameter search during the training phase of each model using the search space: learning-rate:{1e \u2212 5, 2e \u2212 5, 3e \u2212 5, 4e \u2212 5, 5e \u2212 5}, warm-up ratio:{0, 0.1}, batchsize:{2, 4}, epochs:{2, 3, 4, 5}. It has been conducted using Optuna, with 10 trials, optimized on minimizing the evaluation loss during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_62",
            "start": 417,
            "end": 749,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_63@0",
            "content": "Here we discuss the results for the overlap and diversity metrics obtained on the Best LM+D generations (Table 6), and those calculated on the human evaluation subset (Tables 7 and 8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_63",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_64@0",
            "content": "BART.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_64",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_64@1",
            "content": "BART performs well with the stochastic decoding methods, in particular: Top p for overlap, diversity, syntactic metrics, and grammaticality; Top k for overlap metrics and toxicity, whereas Top pk is the best decoding approach on human evaluation and RR, and the second best on ROUGE and BLEU-1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_64",
            "start": 6,
            "end": 299,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_64@2",
            "content": "On the contrary, BART does not achieve good results with deterministic approaches (i. e. BS).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_64",
            "start": 301,
            "end": 393,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_65@0",
            "content": "BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_65",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_65@1",
            "content": "With BS, BERT achieves the best or second best result on all human evaluation metrics, except for specificity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_65",
            "start": 6,
            "end": 115,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_65@2",
            "content": "For BERT the best decoding is Top p : it is the best performing on overlap GPT-2 -Top p : This is not true, the vast majority of Muslims is peaceful and peaceful Muslims are happy to live in any country they wish.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_65",
            "start": 117,
            "end": 329,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_65@3",
            "content": "They do not need to be a part of a hostile state and they should be treated as equals in society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_65",
            "start": 331,
            "end": 427,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_66@0",
            "content": "In these examples we can observe that the CN obtained with GPT-2, albeit not perfect, is the most complete and accurate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_66",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_66@1",
            "content": "Those generated with T5 and DialoGPT are suitable as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_66",
            "start": 121,
            "end": 178,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_66@2",
            "content": "On the other hand, the CNs obtained with BART and BERT present some problems: the former has a clear contradiction in it and the latter is illustrative of some of the problems detected in BERT's generations such that it is syntactically complex but very vague and repetitive, and it ends up not really conveying a clear message.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_66",
            "start": 180,
            "end": 507,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_67@0",
            "content": "Hate speech: The migrant crisis is not the fault of Western Europeans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_67",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_67@1",
            "content": "Migrants are to blame for immigration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_67",
            "start": 71,
            "end": 108,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_68@0",
            "content": "DialoGPT -Top k : What if an entire religion was to blame only for the migration crisis? So you think a large number of Muslims are responsible for all the problems of our world?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_68",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_69@0",
            "content": "BART -Top p : The migrant crisis is caused by a lack of integration and lack of knowledge of the world.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_69",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_70@0",
            "content": "BERT -BS: many migrants come here to work and provide for their families.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_70",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_70@1",
            "content": "they are our brothers and sisters, and we should welcome them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_70",
            "start": 74,
            "end": 135,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_70@2",
            "content": "Once again, GPT-2 and DialoGPT generations where chosen as the best CNs by the annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_70",
            "start": 137,
            "end": 226,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_70@3",
            "content": "Moreover, these CNs are all generated via Top k decoding mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_70",
            "start": 228,
            "end": 294,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_70@4",
            "content": "BART and T5 generated CNs are acceptable while requiring some post-editing to be employed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_70",
            "start": 296,
            "end": 385,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_70@5",
            "content": "Finally, BERT's output is still very vague: we can notice the presence of the recurring expression our brothers and sisters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_70",
            "start": 387,
            "end": 510,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_70@6",
            "content": "For fine-tuning our APE model, we have thus used the triplets <HS, CN or , CN pe > and <HS, CN pe * , CN pe >.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_70",
            "start": 512,
            "end": 621,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_70@7",
            "content": "In this way, we managed to roughly double the number of the post-edit training samples, which is highly beneficial for a better model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_70",
            "start": 623,
            "end": 756,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_70@8",
            "content": "When we filtered the triplets with a positive TER score between CN ed and CN pe , or CN or and CN pe , we obtained 4185 training, 596 test, and 568 validation samples following the partition used in",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_70",
            "start": 758,
            "end": 955,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_71@0",
            "content": "Jeffrey Allen, Christopher Hogan, Toward the development of a post editing module for raw machine translation output: A controlled language perspective, 2000, Third International Controlled Language Applications Workshop (CLAW-00), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_71",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_72@0",
            "content": "R Kimberley, Kay Allison,  Bussey, Cyberbystanding in context: A review of the literature on witnesses' responses to cyberbullying, 2016, Children and Youth Services Review, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_72",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_73@0",
            "content": "Jenn Anderson, Mary Bresnahan, Catherine Musatics, Combating weight-based cyberbullying on facebook with the dissenter effect, 2014, Cyberpsychology, Behavior, and Social Networking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_73",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_74@0",
            "content": "Anja Belz, Ehud Reiter, Comparing automatic and human evaluation of nlg systems, 2006, 11th Conference of the European Chapter of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_74",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_75@0",
            "content": "Susan Benesch, Countering dangerous speech: New ideas for genocide prevention, 2014, United States Holocaust Memorial Museum, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_75",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_76@0",
            "content": "Nicola Bertoldi, Mauro Cettolo, Marcello Federico, Cache-based online adaptation for machine translation enhanced computer assisted translation, 2013, MT-Summit, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_76",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_77@0",
            "content": "Reuben Binns, Michael Veale, Max Van Kleek, Nigel Shadbolt, Like trainer, like bot? inheritance of bias in algorithmic content moderation, 2017, Social Informatics, Springer International Publishing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_77",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_78@0",
            "content": "Pete Burnap, L Matthew,  Williams, Us and them: identifying cyber hate on twitter across multiple protected characteristics, 2016, EPJ Data Science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_78",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_79@0",
            "content": "Rui Cao, Roy Ka-Wei Lee, Tuan-Anh Hoang, Deephate: Hate speech detection via multi-faceted text representations, 2020, 12th ACM Conference on Web Science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_79",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_80@0",
            "content": "Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, Jianfeng Gao, Deep reinforcement learning for dialogue generation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_80",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_81@0",
            "content": "Chin-Yew Lin, Rouge: A package for automatic evaluation of summaries, 2004, Text summarization branches out, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_81",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_82@0",
            "content": "UNKNOWN, None, 2018, Analyzing the hate and counter speech accounts on twitter, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_82",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_83@0",
            "content": "Binny Mathew, Punyajoy Saha, Hardik Tharad, Subham Rajgaria, Prajwal Singhania, Pawan Suman Kalyan Maity, Animesh Goyal,  Mukherjee, Thou shalt not hate: Countering online hate speech, 2019, Proceedings of the International AAAI Conference on Web and Social Media, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_83",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_84@0",
            "content": "Binny Mathew, Punyajoy Saha, Chris Seid Muhie Yimam, Pawan Biemann, Animesh Goyal,  Mukherjee, Hatexplain: A benchmark dataset for explainable hate speech detection, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_84",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_85@0",
            "content": "Kevin Munger, Tweetment effects on the tweeted: Experimentally reducing racist harassment, 2017, Political Behavior, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_85",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_86@0",
            "content": "Sarah Myers West, Censored, suspended, shadowbanned: User interpretations of content moderation on social media platforms, 2018, New Media & Society, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_86",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_87@0",
            "content": "Jekaterina Novikova, Ond\u0159ej Du\u0161ek, Amanda Cercas Curry, and Verena Rieser, 2017, Why we need new evaluation metrics for nlg, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_87",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_88@0",
            "content": "UNKNOWN, None, 2018, A survey on automatic detection of hate speech in text, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_88",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_89@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting on association for computational linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_89",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_90@0",
            "content": "UNKNOWN, None, 2020, Resources and benchmark corpora for hate speech detection: a systematic review. Language Resources and Evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_90",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_91@0",
            "content": "Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding, William Wang, A benchmark dataset for learning to intervene in online hate speech, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_91",
            "start": 0,
            "end": 362,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_92@0",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI Blog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_92",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_93@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_93",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_94@0",
            "content": "Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A Smith, The risk of racial bias in hate speech detection, 2019, Proceedings of the 57th annual meeting of the association for computational linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_94",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_95@0",
            "content": "Carla Schieb, Mike Preuss, Governing hate speech by means of counterspeech on facebook, 2016, 66th ica annual conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_95",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_96@0",
            "content": "Anna Schmidt, Michael Wiegand, A survey on hate speech detection using natural language processing, 2017, Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_96",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_97@0",
            "content": "UNKNOWN, None, 2016, The impact of counter-narratives. Institute for Strategic Dialogue, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_97",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_98@0",
            "content": "Yi-Ling Serra Sinem Tekiroglu, Marco Chung,  Guerini, Generating counter narratives against online hate speech: Data and strategies, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_98",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_99@0",
            "content": ", Demoting racial bias in hate speech detection, , Mengzhou Xia Anjalie Field Yulia Tsvetkov. 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_99",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_100@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_100",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_101@0",
            "content": "Bertie Vidgen, Leon Derczynski, Directions in abusive language training data, a systematic review: Garbage in, garbage out, 2020, Plos one, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_101",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_102@0",
            "content": "UNKNOWN, None, 2019, Challenges and frontiers in abusive content detection, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_102",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_103@0",
            "content": "UNKNOWN, None, 2020, Learning from the worst: Dynamically generated datasets to improve online hate detection, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_103",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_104@0",
            "content": "Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, Jordan Boyd-Graber, Trick me if you can: Human-in-the-loop generation of adversarial question answering examples, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_104",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_105@0",
            "content": "Alex Wang, Kyunghyun Cho, Bert has a mouth, and it must speak, 2019, Bert as a markov random field language model, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_105",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_106@0",
            "content": "Ke Wang, Xiaojun Wan, Sentigan: Generating sentimental texts via mixture adversarial networks, 2018, IJCAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_106",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_107@0",
            "content": "Zeerak Waseem, Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter, 2016, Proceedings of the first workshop on NLP and computational social science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_107",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_108@0",
            "content": "Sam Wiseman, Stuart Shieber, Alexander Rush, Challenges in data-to-document generation, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_108",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_109@0",
            "content": "UNKNOWN, None, , Jason Weston, and Emily Dinan. 2020. Recipes for safety in open-domain chatbots. arXiv e-prints, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_109",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_110@0",
            "content": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan, DIALOGPT : Largescale generative pre-training for conversational response generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_110",
            "start": 0,
            "end": 324,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_111@0",
            "content": "UNKNOWN, None, 2021, Generate, prune, select: A pipeline for counterspeech generation against online hate speech, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_111",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_112@0",
            "content": "UNKNOWN, None, , The vast majority of Muslims in the world are peaceful and friendly people. I don't see how you can say such a thing about them, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_112",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_113@0",
            "content": "UNKNOWN, None, , The vast majority of Muslims are peaceful people who have lived in peace and harmony for many years. How can you say this about a religion that promotes terror and war? BERT -Top k : islam is a religion of peace, and it is not a peaceful religion of violence. it is about peace and understanding of the world's problems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_113",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_114@0",
            "content": "UNKNOWN, None, , The vast majority of Muslims in the UK are peaceful people who share many values and respect, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_114",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_115@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_115",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "49-ARR_v1_116@0",
            "content": "UNKNOWN, None, , the first set of experiments as described in Section 3.1. Finally, the best fine-tuning configuration of the GPT-2 medium model for APE was obtained with a learning rate of 2e-5 for 3 epochs resulting in 3, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v1_116",
            "start": 0,
            "end": 224,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "49-ARR_v1_0",
            "tgt_ix": "49-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_0",
            "tgt_ix": "49-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_1",
            "tgt_ix": "49-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_1",
            "tgt_ix": "49-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_0",
            "tgt_ix": "49-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_2",
            "tgt_ix": "49-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_4",
            "tgt_ix": "49-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_5",
            "tgt_ix": "49-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_3",
            "tgt_ix": "49-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_3",
            "tgt_ix": "49-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_3",
            "tgt_ix": "49-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_3",
            "tgt_ix": "49-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_0",
            "tgt_ix": "49-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_6",
            "tgt_ix": "49-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_8",
            "tgt_ix": "49-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_7",
            "tgt_ix": "49-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_7",
            "tgt_ix": "49-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_7",
            "tgt_ix": "49-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_7",
            "tgt_ix": "49-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_9",
            "tgt_ix": "49-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_0",
            "tgt_ix": "49-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_10",
            "tgt_ix": "49-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_11",
            "tgt_ix": "49-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_11",
            "tgt_ix": "49-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_11",
            "tgt_ix": "49-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_12",
            "tgt_ix": "49-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_13",
            "tgt_ix": "49-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_13",
            "tgt_ix": "49-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_11",
            "tgt_ix": "49-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_14",
            "tgt_ix": "49-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_15",
            "tgt_ix": "49-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_15",
            "tgt_ix": "49-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_15",
            "tgt_ix": "49-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_16",
            "tgt_ix": "49-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_11",
            "tgt_ix": "49-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_17",
            "tgt_ix": "49-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_18",
            "tgt_ix": "49-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_18",
            "tgt_ix": "49-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_18",
            "tgt_ix": "49-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_0",
            "tgt_ix": "49-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_20",
            "tgt_ix": "49-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_22",
            "tgt_ix": "49-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_23",
            "tgt_ix": "49-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_21",
            "tgt_ix": "49-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_21",
            "tgt_ix": "49-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_21",
            "tgt_ix": "49-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_21",
            "tgt_ix": "49-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_0",
            "tgt_ix": "49-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_25",
            "tgt_ix": "49-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_25",
            "tgt_ix": "49-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_25",
            "tgt_ix": "49-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_26",
            "tgt_ix": "49-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_27",
            "tgt_ix": "49-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_27",
            "tgt_ix": "49-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_27",
            "tgt_ix": "49-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_28",
            "tgt_ix": "49-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_25",
            "tgt_ix": "49-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_32",
            "tgt_ix": "49-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_33",
            "tgt_ix": "49-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_34",
            "tgt_ix": "49-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_35",
            "tgt_ix": "49-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_36",
            "tgt_ix": "49-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_37",
            "tgt_ix": "49-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_38",
            "tgt_ix": "49-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_39",
            "tgt_ix": "49-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_31",
            "tgt_ix": "49-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_25",
            "tgt_ix": "49-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_41",
            "tgt_ix": "49-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_43",
            "tgt_ix": "49-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_44",
            "tgt_ix": "49-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_42",
            "tgt_ix": "49-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_42",
            "tgt_ix": "49-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_42",
            "tgt_ix": "49-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_42",
            "tgt_ix": "49-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_47",
            "tgt_ix": "49-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_42",
            "tgt_ix": "49-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_42",
            "tgt_ix": "49-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_42",
            "tgt_ix": "49-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_45",
            "tgt_ix": "49-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_0",
            "tgt_ix": "49-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_48",
            "tgt_ix": "49-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_50",
            "tgt_ix": "49-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_51",
            "tgt_ix": "49-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_49",
            "tgt_ix": "49-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_49",
            "tgt_ix": "49-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_49",
            "tgt_ix": "49-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_49",
            "tgt_ix": "49-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_49",
            "tgt_ix": "49-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_52",
            "tgt_ix": "49-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_0",
            "tgt_ix": "49-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_53",
            "tgt_ix": "49-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_55",
            "tgt_ix": "49-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_57",
            "tgt_ix": "49-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_58",
            "tgt_ix": "49-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_59",
            "tgt_ix": "49-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_60",
            "tgt_ix": "49-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_56",
            "tgt_ix": "49-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_61",
            "tgt_ix": "49-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_63",
            "tgt_ix": "49-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_64",
            "tgt_ix": "49-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_65",
            "tgt_ix": "49-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_66",
            "tgt_ix": "49-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_67",
            "tgt_ix": "49-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_68",
            "tgt_ix": "49-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_69",
            "tgt_ix": "49-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_62",
            "tgt_ix": "49-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v1_0",
            "tgt_ix": "49-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_1",
            "tgt_ix": "49-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_2",
            "tgt_ix": "49-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_2",
            "tgt_ix": "49-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_2",
            "tgt_ix": "49-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_2",
            "tgt_ix": "49-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_2",
            "tgt_ix": "49-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_2",
            "tgt_ix": "49-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_3",
            "tgt_ix": "49-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_4",
            "tgt_ix": "49-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_4",
            "tgt_ix": "49-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_4",
            "tgt_ix": "49-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_4",
            "tgt_ix": "49-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_5",
            "tgt_ix": "49-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_5",
            "tgt_ix": "49-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_5",
            "tgt_ix": "49-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_5",
            "tgt_ix": "49-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_5",
            "tgt_ix": "49-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_5",
            "tgt_ix": "49-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_5",
            "tgt_ix": "49-ARR_v1_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_5",
            "tgt_ix": "49-ARR_v1_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_5",
            "tgt_ix": "49-ARR_v1_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_6",
            "tgt_ix": "49-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_6",
            "tgt_ix": "49-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_6",
            "tgt_ix": "49-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_6",
            "tgt_ix": "49-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_6",
            "tgt_ix": "49-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_6",
            "tgt_ix": "49-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_6",
            "tgt_ix": "49-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_6",
            "tgt_ix": "49-ARR_v1_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_6",
            "tgt_ix": "49-ARR_v1_6@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_7",
            "tgt_ix": "49-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_8",
            "tgt_ix": "49-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_9",
            "tgt_ix": "49-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_9",
            "tgt_ix": "49-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_9",
            "tgt_ix": "49-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_9",
            "tgt_ix": "49-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_9",
            "tgt_ix": "49-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_9",
            "tgt_ix": "49-ARR_v1_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_9",
            "tgt_ix": "49-ARR_v1_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_9",
            "tgt_ix": "49-ARR_v1_9@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_10",
            "tgt_ix": "49-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_10",
            "tgt_ix": "49-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_10",
            "tgt_ix": "49-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_10",
            "tgt_ix": "49-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_10",
            "tgt_ix": "49-ARR_v1_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_10",
            "tgt_ix": "49-ARR_v1_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_10",
            "tgt_ix": "49-ARR_v1_10@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_10",
            "tgt_ix": "49-ARR_v1_10@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_10",
            "tgt_ix": "49-ARR_v1_10@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_11",
            "tgt_ix": "49-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_12",
            "tgt_ix": "49-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_13",
            "tgt_ix": "49-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_14",
            "tgt_ix": "49-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_14",
            "tgt_ix": "49-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_14",
            "tgt_ix": "49-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_14",
            "tgt_ix": "49-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_14",
            "tgt_ix": "49-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_15",
            "tgt_ix": "49-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_16",
            "tgt_ix": "49-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_16",
            "tgt_ix": "49-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_16",
            "tgt_ix": "49-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_16",
            "tgt_ix": "49-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_16",
            "tgt_ix": "49-ARR_v1_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_16",
            "tgt_ix": "49-ARR_v1_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_16",
            "tgt_ix": "49-ARR_v1_16@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_16",
            "tgt_ix": "49-ARR_v1_16@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_17",
            "tgt_ix": "49-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_17",
            "tgt_ix": "49-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_17",
            "tgt_ix": "49-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_18",
            "tgt_ix": "49-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_19@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_19@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_19@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_19@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_19",
            "tgt_ix": "49-ARR_v1_19@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_20",
            "tgt_ix": "49-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_21",
            "tgt_ix": "49-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_22",
            "tgt_ix": "49-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_22",
            "tgt_ix": "49-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_22",
            "tgt_ix": "49-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_22",
            "tgt_ix": "49-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_23",
            "tgt_ix": "49-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_23",
            "tgt_ix": "49-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_23",
            "tgt_ix": "49-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_23",
            "tgt_ix": "49-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_23",
            "tgt_ix": "49-ARR_v1_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_23",
            "tgt_ix": "49-ARR_v1_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_23",
            "tgt_ix": "49-ARR_v1_23@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_23",
            "tgt_ix": "49-ARR_v1_23@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_23",
            "tgt_ix": "49-ARR_v1_23@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_24@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_24@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_24@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_24@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_24",
            "tgt_ix": "49-ARR_v1_24@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_25",
            "tgt_ix": "49-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_26",
            "tgt_ix": "49-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_26",
            "tgt_ix": "49-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_27",
            "tgt_ix": "49-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_28",
            "tgt_ix": "49-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_28",
            "tgt_ix": "49-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_28",
            "tgt_ix": "49-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_29",
            "tgt_ix": "49-ARR_v1_29@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_30",
            "tgt_ix": "49-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_31",
            "tgt_ix": "49-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_32",
            "tgt_ix": "49-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_32",
            "tgt_ix": "49-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_32",
            "tgt_ix": "49-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_33",
            "tgt_ix": "49-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_33",
            "tgt_ix": "49-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_34",
            "tgt_ix": "49-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_34",
            "tgt_ix": "49-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_34",
            "tgt_ix": "49-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_34",
            "tgt_ix": "49-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_35",
            "tgt_ix": "49-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_35",
            "tgt_ix": "49-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_35",
            "tgt_ix": "49-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_36",
            "tgt_ix": "49-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_36",
            "tgt_ix": "49-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_36",
            "tgt_ix": "49-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_37",
            "tgt_ix": "49-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_37",
            "tgt_ix": "49-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_37",
            "tgt_ix": "49-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_37",
            "tgt_ix": "49-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_38",
            "tgt_ix": "49-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_38",
            "tgt_ix": "49-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_38",
            "tgt_ix": "49-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_38",
            "tgt_ix": "49-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_39",
            "tgt_ix": "49-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_39",
            "tgt_ix": "49-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_39",
            "tgt_ix": "49-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_40@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_40@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_40@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_40@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_40",
            "tgt_ix": "49-ARR_v1_40@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_41",
            "tgt_ix": "49-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_42",
            "tgt_ix": "49-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_43",
            "tgt_ix": "49-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_43",
            "tgt_ix": "49-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_44",
            "tgt_ix": "49-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_44",
            "tgt_ix": "49-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_44",
            "tgt_ix": "49-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_44",
            "tgt_ix": "49-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_44",
            "tgt_ix": "49-ARR_v1_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_44",
            "tgt_ix": "49-ARR_v1_44@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_44",
            "tgt_ix": "49-ARR_v1_44@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_45",
            "tgt_ix": "49-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_45",
            "tgt_ix": "49-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_45",
            "tgt_ix": "49-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_45",
            "tgt_ix": "49-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_45",
            "tgt_ix": "49-ARR_v1_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_45",
            "tgt_ix": "49-ARR_v1_45@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_45",
            "tgt_ix": "49-ARR_v1_45@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_46",
            "tgt_ix": "49-ARR_v1_46@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_47",
            "tgt_ix": "49-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_47",
            "tgt_ix": "49-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_47",
            "tgt_ix": "49-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_47",
            "tgt_ix": "49-ARR_v1_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_47",
            "tgt_ix": "49-ARR_v1_47@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_47",
            "tgt_ix": "49-ARR_v1_47@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_48",
            "tgt_ix": "49-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_48",
            "tgt_ix": "49-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_48",
            "tgt_ix": "49-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_48",
            "tgt_ix": "49-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_49",
            "tgt_ix": "49-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_50",
            "tgt_ix": "49-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_50",
            "tgt_ix": "49-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_51",
            "tgt_ix": "49-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_51",
            "tgt_ix": "49-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_51",
            "tgt_ix": "49-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_51",
            "tgt_ix": "49-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_52",
            "tgt_ix": "49-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_52",
            "tgt_ix": "49-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_52",
            "tgt_ix": "49-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_52",
            "tgt_ix": "49-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_53",
            "tgt_ix": "49-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_53",
            "tgt_ix": "49-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_53",
            "tgt_ix": "49-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_53",
            "tgt_ix": "49-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_54",
            "tgt_ix": "49-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_55",
            "tgt_ix": "49-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_55",
            "tgt_ix": "49-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_55",
            "tgt_ix": "49-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_55",
            "tgt_ix": "49-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_56",
            "tgt_ix": "49-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_56",
            "tgt_ix": "49-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_56",
            "tgt_ix": "49-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_56",
            "tgt_ix": "49-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_56",
            "tgt_ix": "49-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_57",
            "tgt_ix": "49-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_57",
            "tgt_ix": "49-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_58",
            "tgt_ix": "49-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_58",
            "tgt_ix": "49-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_58",
            "tgt_ix": "49-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_58",
            "tgt_ix": "49-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_58",
            "tgt_ix": "49-ARR_v1_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_58",
            "tgt_ix": "49-ARR_v1_58@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_59",
            "tgt_ix": "49-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_59",
            "tgt_ix": "49-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_59",
            "tgt_ix": "49-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_60",
            "tgt_ix": "49-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_60",
            "tgt_ix": "49-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_60",
            "tgt_ix": "49-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_60",
            "tgt_ix": "49-ARR_v1_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_61",
            "tgt_ix": "49-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_61",
            "tgt_ix": "49-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_62",
            "tgt_ix": "49-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_62",
            "tgt_ix": "49-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_62",
            "tgt_ix": "49-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_63",
            "tgt_ix": "49-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_64",
            "tgt_ix": "49-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_64",
            "tgt_ix": "49-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_64",
            "tgt_ix": "49-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_65",
            "tgt_ix": "49-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_65",
            "tgt_ix": "49-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_65",
            "tgt_ix": "49-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_65",
            "tgt_ix": "49-ARR_v1_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_66",
            "tgt_ix": "49-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_66",
            "tgt_ix": "49-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_66",
            "tgt_ix": "49-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_67",
            "tgt_ix": "49-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_67",
            "tgt_ix": "49-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_68",
            "tgt_ix": "49-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_69",
            "tgt_ix": "49-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_70",
            "tgt_ix": "49-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_70",
            "tgt_ix": "49-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_70",
            "tgt_ix": "49-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_70",
            "tgt_ix": "49-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_70",
            "tgt_ix": "49-ARR_v1_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_70",
            "tgt_ix": "49-ARR_v1_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_70",
            "tgt_ix": "49-ARR_v1_70@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_70",
            "tgt_ix": "49-ARR_v1_70@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_70",
            "tgt_ix": "49-ARR_v1_70@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_71",
            "tgt_ix": "49-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_72",
            "tgt_ix": "49-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_73",
            "tgt_ix": "49-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_74",
            "tgt_ix": "49-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_75",
            "tgt_ix": "49-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_76",
            "tgt_ix": "49-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_77",
            "tgt_ix": "49-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_78",
            "tgt_ix": "49-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_79",
            "tgt_ix": "49-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_80",
            "tgt_ix": "49-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_81",
            "tgt_ix": "49-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_82",
            "tgt_ix": "49-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_83",
            "tgt_ix": "49-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_84",
            "tgt_ix": "49-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_85",
            "tgt_ix": "49-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_86",
            "tgt_ix": "49-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_87",
            "tgt_ix": "49-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_88",
            "tgt_ix": "49-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_89",
            "tgt_ix": "49-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_90",
            "tgt_ix": "49-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_91",
            "tgt_ix": "49-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_92",
            "tgt_ix": "49-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_93",
            "tgt_ix": "49-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_94",
            "tgt_ix": "49-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_95",
            "tgt_ix": "49-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_96",
            "tgt_ix": "49-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_97",
            "tgt_ix": "49-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_98",
            "tgt_ix": "49-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_99",
            "tgt_ix": "49-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_100",
            "tgt_ix": "49-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_101",
            "tgt_ix": "49-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_102",
            "tgt_ix": "49-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_103",
            "tgt_ix": "49-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_104",
            "tgt_ix": "49-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_105",
            "tgt_ix": "49-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_106",
            "tgt_ix": "49-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_107",
            "tgt_ix": "49-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_108",
            "tgt_ix": "49-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_109",
            "tgt_ix": "49-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_110",
            "tgt_ix": "49-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_111",
            "tgt_ix": "49-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_112",
            "tgt_ix": "49-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_113",
            "tgt_ix": "49-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_114",
            "tgt_ix": "49-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_115",
            "tgt_ix": "49-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v1_116",
            "tgt_ix": "49-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1540,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "49-ARR",
        "version": 1
    }
}