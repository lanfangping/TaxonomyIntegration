{
    "nodes": [
        {
            "ix": "49-ARR_v2_0",
            "content": "Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_2",
            "content": "In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech in English. We first present a comparative study to determine whether there is a particular Language Model (or class of LMs) and a particular decoding mechanism that are the most appropriate to generate CNs. Findings show that autoregressive models combined with stochastic decodings are the most promising. We then investigate how an LM performs in generating a CN with regard to an unseen target of hate. We find out that a key element for successful 'out of target' experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori. We finally introduce the idea of a pipeline based on the addition of an automatic post-editing step to refine generated CNs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "49-ARR_v2_4",
            "content": "Hate Speech (HS) has found fertile ground in Social Media Platforms. Actions undertaken by such platforms to tackle online hatred consist in identifying possible sources of hate and removing them by means of content deletion, account suspension or shadow-banning. However, these actions are often interpreted and denounced as censorship by the affected users and political groups (Myers West, 2018). For this reason, such restrictions can have the opposite effect of exacerbating the hostility of the haters (Munger, 2017). An alternative strategy, that is looming on the horizon, is based on the use of Counter Narratives. CNs are \"all communicative actions aimed at refuting hate speech through thoughtful and cogent reasons, and true and fact-bound arguments\" (Schieb and Preuss, 2016). As a de-escalating * Now at the University of Stuttgart, Germany. measure, CNs have been proven to be successful in diminishing hate, while preserving the freedom of speech (Benesch, 2014;Gagliardone et al., 2015). An example of <HS, CN > pair is shown below: HS: Women are basically childlike, they remain this way most of their lives. Soft and emotional. It has devastated our once great patriarchal civilizations. CN: Without softness and emotions there would be just brutality and cruelty. Not all women are soft and emotional and many men have these characteristics. To perpetuate these socially constructed gender profiles maintains norms which oppress anybody.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_5",
            "content": "Based on their effectiveness, CNs have started being employed by NGOs to counter online hate. Since for NGO operators it is impossible to manually write responses to all instances of hate, a line of NLP research has recently emerged, focusing on designing systems to automatically generate CN suggestions (Qian et al., 2019;Tekiroglu et al., 2020;Fanton et al., 2021;Chung et al., 2021a;Zhu and Bhat, 2021). In this study, our main goal is to compare pre-trained language models (LM) and decoding mechanisms in order to understand their pros and cons in generating CNs. Thus, we use various automatic metrics and manual evaluations with expert judgments to assess several LMs, representing the main categories of the model architectures, and decoding methods. We further test the robustness of the fine-tuned LMs in generating CNs for an unseen target. Results show that autoregressive models are in general more suited for the task, and while stochastic decoding mechanisms can generate more novel, diverse, and informative outputs, the deterministic decoding is useful in scenarios where more generic and less novel (yet 'safer') CNs are needed. Furthermore, in out-of-target experiments we find that the similarity of targets (e.g. JEWS and MUSLIMS as religious groups) plays a crucial role for the effectiveness of portability to new targets. We finally show a promising research direction of leveraging gold human edits for building an additional automatic post-editing step to correct errors made by LMs during generation. To the best of our knowledge, this is the first study systematically analysing state of the art pre-trained LMs in CN generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_6",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "49-ARR_v2_7",
            "content": "In this section we first discuss standard approaches to hate countering and studies on CN effectiveness on Social Media Platforms, then the existing CN data collection and generation strategies.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_8",
            "content": "Hate countering. NLP has started addressing the phenomenon of the proliferation of HS by creating datasets for automatic detection (Mathew et al., 2021;Cao et al., 2020;Hosseinmardi et al., 2015;Waseem, 2016;Burnap and Williams, 2016). Several surveys provide a review on the existing approaches on the topic (Poletto et al., 2020;Schmidt and Wiegand, 2017;Fortuna and Nunes, 2018), also addressing the ethical challenges of the task (Kiritchenko et al., 2021). Still, automatic detection of HS presents some drawbacks (Vidgen and Derczynski, 2020). First of all, the datasets might include biases, and the models tend to replicate such biases (Binns et al., 2017;Davidson et al., 2019;Sap et al., 2019;Tsvetkov, 2020). Moreover, the end goals for which HS detection is employed are often charged with censorship of the freedom of speech by concerned users (Munger, 2017;Myers West, 2018). In this scenario, NGOs have started employing CNs to counter online hate. CNs have been shown to be effective in reducing linguistic violence (Benesch, 2014;Gagliardone et al., 2015;Schieb and Preuss, 2016;Silverman et al., 2016;Mathew et al., 2019); moreover, even if they might not influence the view of extremists, they are still effective in presenting alternative and non-hateful viewpoints to bystanders (Allison and Bussey, 2016;Anderson et al., 2014).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_9",
            "content": "The existing studies for collecting CN datasets employ four main approaches. Crawling consists in automatically scraping websites, starting from an HS content and searching for possible CNs among the responses (Mathew et al., 2018(Mathew et al., , 2019. With crowdsourcing CNs are written by non-expert paid workers as responses to provided hate content (Qian et al., 2019). Nichesourcing relies on a niche group of experts for data collection (De Boer et al., 2012), and it was employed by Chung et al. (2019) for CN collection using NGO's operators. Hybrid approaches use a combination of LMs and humans to collect data (Wallace et al., 2019;Dinan et al., 2019;. Studies on CN collection are presented in more detail by Tekiroglu et al. (2020);Fanton et al. (2021). 2019) employ a mix of automatic and human intervention to generate CNs. Zhu and Bhat (2021) propose an entirely automated pipeline of candidate CN generation and filtering. Other lines of work include CN generation for under-resourced languages such as for Italian (Chung et al., 2020), and the generation of knowledge-bound CNs, which allows the production of CNs based on grounded and up-todate facts and plausible arguments, avoiding the hallucination phenomena (Chung et al., 2021a). Instead, in our work we take a more foundational perspective, which is relevant for all the LM-based pipelines described above. Therefore, we compare and assess various state of the art pre-trained LMs in an end-to-end setting, which is developed as a downstream task for CN generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_10",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "49-ARR_v2_11",
            "content": "In this section, we present the CN dataset, the language models, and the decoding mechanisms employed for our experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_12",
            "content": "Dataset for fine-tuning",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "49-ARR_v2_13",
            "content": "For this study we rely on the dataset proposed by Fanton et al. (2021), which is the only available dataset that grants both the target diversity and the CN quality we aim for. The dataset was collected with a human-in-the-loop approach, by employing an autoregressive LM (GPT-2) paired with three expert human reviewers. It features 5003 <HS, CN > pairs, covering several targets of hate including DISABLED, JEWS, LGBT+, MIGRANTS, MUSLIMS, POC, WOMEN. The residual categories are collapsed to the label OTHER. We partitioned the dataset into training, validation, and test sets with the ratio: 8 : 1 : 1 (i. e. 4003, 500 and 500 pairs), ensuring that all sets share the same target distribution, and no repetition of HS across the sets is allowed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_14",
            "content": "Models",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "49-ARR_v2_15",
            "content": "We experiment with 5 Transformer based LMs (Vaswani et al., 2017) (2019). It is a bidirectional autoencoder that can be adapted to text generation (Wang and Cho, 2019). GPT-2. The Generative Pre-trained Transformer 2 is an autoregressive model built for text generation (Radford et al., 2019). DialoGPT. The Dialogue Generative Pretrained Transformer is the extension of GPT-2 specifically created for conversational response generation (Zhang et al., 2020). BART. BART is a denoising autoencoder for pretraining seq2seq models (Lewis et al., 2020). The encoder-decoder architecture of BART is composed of a bidirectional encoder and an autoregressive decoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_16",
            "content": "The Text-to-Text Transfer Transformer proposed by Raffel et al. ( 2020) is a seq2seq model with an encoder-decoder Transformer architecture. While all the other models could be fine-tuned directly for the generation task, for BERT we warmstarted an encoder-decoder model using BERT checkpoints similar to the BERT2BERT model defined by (Rothe et al., 2020). The fine-tuning details and hyperparameter settings can be found in Appendix A.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_17",
            "content": "Decoding mechanisms",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "49-ARR_v2_18",
            "content": "We utilize 4 decoding mechanisms: a deterministic (Beam Search) and three stochastic (Top-k, Top-p, and a combination of the two). Beam Search (BS). The Beam Search algorithm is designed to pick the most-likely sequence (Li et al., 2016;Wiseman et al., 2017). Top-k (Top k ). The sampling procedure proposed by Fan et al. (2018) selects a random word from the k most probable ones, at each time step. Top-p (Top p ). Also known as Nucleus Sampling, the parameter p indicates the total probability for the pooled candidates, at each time step (Holtzman et al., 2020). Combining Top-p and Top-k (Top pk ). At decoding stage, it is possible to combine the parameters p and k. This is a Nucleus Sampling constrained to the Top-k most probable words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_19",
            "content": "In our experiments we used the following parameters as default: Beam-Search with 5 beams and repetition penalty = 2; Top-k with k = 40; Top-p with p = .92; Top pk with k = 40 and p = .92.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_20",
            "content": "Evaluation metrics",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "49-ARR_v2_21",
            "content": "We use several metrics to evaluate various aspects of the CN generation. Overlap Metrics. These metrics depend on the n-gram similarity of the generated outputs to a set of reference texts in order to assess the quality. We used our gold CNs as references and the CNs generated by the different models, as candidates.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_22",
            "content": "In particular, we employed three BLEU variants: BLEU-1 (B-1), BLEU-3 (B-3) and BLEU-4 (B-4) (Papineni et al., 2002), and ROUGE-L (ROU) (Lin, 2004). Diversity metrics. They are used to measure how diverse and novel the produced CNs are. In particular, we utilized Repetition Rate (RR) to measure the repetitiveness across generated CNs, in terms of the average ratios of non-singleton n-grams present in the corpus (Bertoldi et al., 2013). It should be noted that RR is calculated as a corpus-based repetition score , i.e. inter-CN, instead of calculating intra-CN repetition of n-grams only. We also used Novelty (NOV) (Wang and Wan, 2018), based on Jaccard similarity, to compute the amount of novel content that is present in the generated CNs as compared to the training data. Human evaluation metrics. Albeit more difficult to attain, human judgments provide a more reliable evaluation and a deeper understanding than automatic metrics (Belz and Reiter, 2006;Novikova et al., 2017). To this end, we specified the following dimensions for the evaluation of CNs. Suitableness (SUI): measures how suitable a CN is to the HS in terms of semantic relatedness and in terms of adherence to CN guidelines 1 ; Grammaticality (GRM): how grammatically correct a generated CN is; Specificity (SPE): how specific are the arguments brought by the CN in response to the HS; Choose-or-not (CHO): determines whether the annotators would select that CN to post-edit and use it in a real case scenario as in the set up presented by Chung et al. (2021b); Is-best (BEST): whether the CN is the absolute best among the ones generated for an HS (i. e. whether the annotators would pick up exactly that CN if they had to use it in a real case scenario).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_23",
            "content": "The first three dimensions are rated with a 5points Likert scale and follow the evaluation procedure described by Chung et al. (2020), whereas both choose-or-not and is-best are binary ratings (0, 1). Choose-or-not allows for multiple CNs to be selected for the same HS, while only one CN can be selected for is-best for each HS. Toxicity. 2 It determines how \"rude, disrespectful, or unreasonable\" a text is. Toxicity has been employed both to detect the bias present in LMs (Gehman et al., 2020) and as a solution to mitigate such bias (Gehman et al., 2020;Xu et al., 2020). Syntactic metrics. A high syntactic complexity can be used as a proxy for an LM's ability of generating complex arguments. We used the syntactic dependency parser of spaCy 3 For the task, focusing on the following measures: Maximum Syntactic Depth (MSD): the maximum depth among the dependency trees calculated over each sentence composing a CN. Average Syntactic Depth (ASD): the average depth of the sentences in each CN. Number of Sentences (NST): the number of sentences composing a CN.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_24",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "49-ARR_v2_25",
            "content": "We performed two sets of experiments: first, we assessed how LMs perform in the task of generating CNs with different decoding mechanisms. Then, we selected the best model from the first round of experiments and tested its generalization capabilities when confronted with an unseen target of hate.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_26",
            "content": "LMs and decoding experiments",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "49-ARR_v2_27",
            "content": "For the first round of experiments, in order to avoid possible unfair assessments given by the open nature of the generative task (i. e. a highly suitable CN candidate could be scored low due to its difference from the single reference/gold CN), at test time we allowed the generation of several candidates for each HS+LM+decoding mechanism combination. We loosely drew inspiration from the Rank-N Accuracy procedure and the 'generate, prune, select' procedure (Zhu and Bhat, 2021). In particular, given an LM and a decoding mechanism, we generated 5 CNs for each HS in the test set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_28",
            "content": "We set up the automatic evaluation strategy as displayed in Figure 1. First, we scored each CN with the overlap metrics presented in Section 4, using the gold CN as a reference. Next, we ranked the candidate CNs with respect to the overlap scores and computed the mean of the rankings. Then, we selected the best ones according to the following criteria: Best LM selects the single best CN for an HS among the 20 generated by the 4 models. Best D selects the single best CN for an HS among the 25 generated by the 5 decoding configurations. Best LM+D selects the single best CN among the 5 generated with each model-decoding combination. Moreover, we assessed the overall corpus-wise quality of the generated CNs with respect to the models, to the decoding mechanisms, and to the model-decoding combinations via the diversity metrics. Human evaluation on a sample To perform the human evaluation we referred to the Best LM generations and sampled 200 instances from it. Each instance comprises an HS and 5 relevant CNs, each generated by a different model. We recruited 2 annotators who were trained extensively for the task following the procedure used by Fanton et al. (2021). The expert annotators were asked to evaluate the 5 CNs corresponding to the HS, according to the dimensions described in Section 4. We enriched the evaluation of this subset with the toxicity and the syntactic metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_29",
            "content": "Results of the first set of experiments",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "49-ARR_v2_30",
            "content": "The results of the experiments on the LMs and the decoding mechanisms are reported in this section 4 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_31",
            "content": "The results of the comparison of the models on the Best LM generations can be found in Table 1. Regarding the overlap and diversity metrics, DialoGPT records the best or the second best score in all the metrics, apart from novelty where it still achieves a high score (0.643) close to the best performance (0.655). T5 also achieves high scores, especially on ROUGE, BLEU-1 and novelty.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_32",
            "content": "BART, instead, is the best model according to human evaluation metrics, apart from specificity. On the other hand, it shows poor performances in terms of diversity metrics, indicating that it tends to produce grammatical and suitable but very generic responses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_33",
            "content": "BERT records the worst scores for all the overlap and diversity metrics apart from novelty. However, it also achieves the best syntactic metric results. Therefore, it is evident that BERT's output is more complex, but very repetitive. The combination of these aspects eventually affects the clarity of BERT's output such that it yields poor results in the human evaluation, in particular for grammaticality (4.2, while other models are above 4.6). This poor grammaticality can also explain the syntactic scores since the spaCy dependency parser was not trained to handle ungrammatical text and this could actually inflates the ASD and MSD scores.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_34",
            "content": "GPT-2 overall yields very competitive results for several groups of metrics. It obtains the secondhighest novelty score (0.653) and the best RR (7.736). It also achieves the second best results on BLEU-3, maximum syntactic depth and number of sentences, and the best results on toxicity and specificity (2.880) indicating the ability to produce complex, suitable, focused and diverse CNs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_35",
            "content": "After the human evaluation we ran a qualitative interview with the annotators, whose feedback on the data strengthened the results we observed and the conclusion we drew. For instance, they reported the repetition of simple, yet catch-them-all, expressions (e.g. \"they are our brothers and sisters\") regardless of the target. Further inspections found that those CNs were mainly produced by BERT, which is in line with BERT's RR score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_36",
            "content": "Best Decoding mechanism. The results calculated on Best D output are presented in Table 2. Top k is the best performing decoding mechanism achieving the best results on the diversity metrics, BLEU-3 and BLEU-4. It is also the best performing for specificity, maximum syntactic depth and number of sentences, and the second best for average syntactic depth and toxicity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_37",
            "content": "The other stochastic decoding mechanisms perform well too. Top p yields competitive results on both diversity and overlap metrics; it is the second best for specificity, and achieves good results on the syntactic metrics. Top pk has a good performance on the overlap metrics. It obtains the second-highest scores in most of the human evaluation metrics and the lowest in toxicity, and it reaches a reasonable specificity score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_38",
            "content": "On the other hand, BS does not achieve particularly good results, except for the ROUGE score. Even if it is the best decoding with respect to the human evaluation, this comes at the cost of specificity and diversity. Through a post-hoc manual analysis we observed that it was due to the deterministic nature of BS, that tends to choose the most probable sequences, i. e. the \"safest\", thus resulting in vague and repetitive outputs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_39",
            "content": "Best Model-Decoding combination Here we briefly discuss the results of the evaluation obtained on the Best LM+D generations. In particular, the autoregressive models GPT-2 and DialoGPT behave similarly with similar decoding mechanisms, such that BS outputs the best results for almost all the overlap metrics, and the worst for the diversity metrics. On the contrary, for the other models, the results achieved with stochastic decoding mechanisms are the best for the overlap metrics. In almost all the cases, we observe that the stochastic decoding mechanisms perform better on syntactic and diversity metrics and on toxicity, while for the human evaluation metrics BS tends to be the best, except for specificity. A detailed discussion can be found in Appendix A.2. Discussion. In this set of experiments, we found that the autoregressive models perform the best according to a combination of several metrics that we deem particularly relevant (e.g. more novel, diverse, and informative outputs). Of course more repetitive and conservative outputs can be preferred when high precision of suitable CNs are required at the expense of being more generic and less novel.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_40",
            "content": "Still, for what concerns autoregressive models it could be argued that the good performance of the GPT-2 model we fine-tuned is due to the fact that generated CNs and gold CNs derive from a similar distribution (GPT-2 was employed in the humanin-the-loop process used to create the reference dataset from Fanton et al. ( 2021)). While we recognize that this could partially explain the performance of our GPT-2 model, it does not explain the performance of DialoGPT, which is pre-trained on a completely different dataset. Therefore, we can reasonably conclude that autoregressive models are particularly suited for the task, regardless of the pre-training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_41",
            "content": "With respect to the decoding mechanisms, we record high repetitiveness and low novelty for the deterministic decoding BS. Even if it reaches high scores in most of the human evaluation metrics, it fails to produce specific CNs ending up in generating suitable, yet generic responses. On the contrary, stochastic decoding mechanisms produce more novel and specific responses.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_42",
            "content": "Example CNs generated in this session of experiments, along with some qualitative analysis, can be found in Appendix A.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_43",
            "content": "Leave One Target Out experiments",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "49-ARR_v2_44",
            "content": "In the second stage, we built a set of cross-domain experiments to capture the generalization capabilities of the best LM determined in the previous experiments. Specifically, we concentrate on assessing how much a pre-trained language model fine-tuned on a pool of hate targets can generalize to an unseen target.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_45",
            "content": "Thus, for the out of target experiment we selected the LM that we deem the most prominent in order to reduce the number of LM configurations to compare. In particular, since we want to examine the generalization capability of the LM, the generation of novel CNs, in comparison to the training data, is given primary importance. Secondly, specificity is also crucial since it signifies the ability of the LM/decoding mechanism in generating accurate CNs and avoiding vague yet suitable, catch-all CNs. In contrast, repetitiveness is an undesirable feature of CNs, as it signals the tendency of a model to produce less flexible content. Given these considerations, we chose to employ GPT-2 with Top k decoding for the Leave One Target Out (LOTO) experiments since it is the configuration achieving the best trade-off amongst all the others. This set of experiments is structured in 3 steps, replicated for each of the selected targets. We selected the targets with the highest number of examples (MUSLIMS, MIGRANTS, WOMEN, LGBT+ and JEWS) to have a sufficient sized test set for each configuration.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_46",
            "content": "First, we sampled from the Fanton et al. ( 2021) dataset 600 pairs for each LOTO target, in order to have a balanced setting. Additionally, POC and DISABLED were always kept in the training set, and we removed multi-target cases from OTHER. The resulting dataset consists of 3729 instances (further details are provided in Appendix A.4). Sec-ondly, we fine-tuned 5 different configurations of the LM, and in each configuration one of the 5 LOTO targets is not present in the training data: LM -JEWS , LM -LGTB+ , LM -MIGRANTS , LM -MUSLIMS and LM -WOMEN . Finally, we tested each LOTO model on the 600 HSs in the test set made of \"left out\" target examples. For instance, the model LM -JEWS is used for generating the CNs for the target JEWS, after being trained on <HS, CN > data without any instances with the label JEWS. We generated 5 CNs for each HS and selected the best CN according to the procedure described in Section 5.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_47",
            "content": "We analyse the CNs generated with the LOTO models through overlap and diversity metrics (Table 3). We refer to Appendix A.4 for the comparison between RR calculated on the candidate CNs and the reference CNs of the Fanton et al. ( 2021) dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_48",
            "content": "For all the targets we record higher novelty scores as compared to the previous experiments. Higher novelty ranges indicate that conditioning with new material (i. e. HS for the unseen targets) induces GPT-2 to produce new arguments. On the other hand, as expected, the overlap scores for LOTO are remarkably lower than those from the previous experiments (Table 3). Therefore, we can infer that generalizing to an unseen target is harder than generalizing to an unseen HS. We also found out that the CNs generated in the LM -MUSLIMS and LM -WOMEN configurations obtain the highest overlap scores (Table 3). We hypothesize that the high scores can be explained by the presence of a target in the LOTO training that is highly similar to the left out one. To this end, we computed the novelty between each target subset of the training data and the LOTO test data for that configuration (see Appendix A.4 for details). The reference CNs for LM -MUSLIMS record the lowest novelty scores with respect to the JEWS subset of the training set (i. e. 0.761). Thus, it can be interpreted as the most influential portion of training data for the target MUSLIMS. On the other hand, for LM -WOMEN the highest influence is recorded with the LGBT+ subset of the training data (i. e. 0.763). These results can be explained by the semantic similarity of the target MUSLIMS to JEWS, both being religious groups; and of WOMEN to LGBT+, both being related to gender issues.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_49",
            "content": "As a complementary analysis, we consider two different computations of the reference CN novelty: with respect to the most influential target for each LOTO configuration, and with respect to the LOTO training data without the most influential target. We computed the Pearson correlation between the overlap metrics and each of the two novelty computations. In Figure 2, we observe that removing the influential target from the training data strongly decreases the correlation with the overlap metrics (from an average of -0.889 to -0.416). Consequently, we can conclude that to obtain high overlap results in the LOTO experiments, it is necessary that the training data contains a target strongly connected to the left out one. Most importantly, this connection is not arbitrarily decided but it is based on an a-priori semantic similarity of the targets as exemplified before.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_50",
            "content": "Finally, we chose to generate also with the BS decoding mechanism, to use it as a baseline and compare it to the stochastic decoding mechanism (Top-k). In particular, we computed the Pearson correlation between the novelty of the reference CNs and the novelty of the candidate CNs with respect to the corresponding training data (Figure 3). We can observe that for the BS generation the novelty of the candidate CNs is lower than Top-k (0.67-0.74 vs. 0.75-0.77) and the correlation with the novelty of the reference is weaker (0.53 vs. 0.59). This confirms the lower generalization ability with the deterministic decoding mechanism (as compared to the stochastic) that tends to produce generic and repetitive responses regardless of the semantic distances of the LOTO targets from the training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_51",
            "content": "Automatic Post-Editing",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "49-ARR_v2_52",
            "content": "In the previous experiments we fine-tuned our models making resort to <HS, CN > pairs alone. Still the Fanton et al. ( 2021) dataset contains additional information that can be useful for our task: i. e. the original GPT-2 generation before undergoing human post-editing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_53",
            "content": "Thus, as a final experiment, we propose to further improve the CN generation by moving from an end-to-end framework to a two stage pipeline, by decoupling CN generation from its 'final refinement'. In particular we propose the adoption of an Automatic Post-Editing (APE) stage in order to capture and utilize the nuances among the machine generated CNs and their human post-edited versions. APE, which is used for automatically correcting errors made by machine translation (MT) systems before performing actual human post-editing, has been an important tool for MT (Knight and Chander, 1994;do Carmo et al., 2021). Considering its effectiveness in MT, we hypothesize that building a pipeline with CN generation and APE could alleviate the requirement of the final manual post-editing (Allen and Hogan, 2000;Chatterjee et al., 2019) to achieve better constructed CNs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_54",
            "content": "To this end, we fine-tuned another instance of GPT-2 medium model specifically for the post-editing task using the <HS, CN or , CN pe > triplets 5 , where CN or and CN pe denote the CNs originally generated by an LM and their human post-edited versions, respectively. The triplets were then filtered by removing those for which CN or = CN pe . More details about the experiment settings can be found in Appendix A.5. We have conducted two human evaluations over the subsets of: i) the CN or of the Fanton et al. ( 2021) test samples, ii) the CN outputs of the best model and decoding mechanism combination provided as the results of the first set of experiments, that yielded the top 50 Translation Error Rate (TER) (Snover et al., 2006) scores with respect to the CN or s. The two expert annotators were asked to state their preferences among the 2 randomly sorted CNs, CN or and CN ape (automatically post-edited output), for a given HS. The annotators were also allowed to decide on a tie. Results, shown in Table 4, indicate that, albeit there are often ties and only a subset of CN or is actually modified, when there is a preference, it is predominantly in favour of the automatically post-edited versions over the GPT-2 generated CNs (26% vs. 14% for the test set, and 37% vs. 19% for the GPT-2 Top k generations, on average). Regarding the experiment results, we believe that APE is a highly promising direction to increase the efficacy of the CN generation models where generation quality and diversity is crucial, and considering that obtaining/enlarging expert datasets to train better models is not simple.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_55",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "49-ARR_v2_56",
            "content": "In this work, we focus on automatic CN generation as a downstream task. First, we present a comparative study to determine the performances and peculiarities of several pre-trained LMs and decoding mechanisms. We observe that the best results (in term of novelty and specificity) overall are achieved by the autoregressive models with stochastic decoding: GPT-2 with the Top k decoding mechanism, and DialoGPT with the combination Top pk . At the same time deterministic decoding can be used when more generic yet 'safer' CNs are preferred.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_57",
            "content": "Then, we investigate the performances of LMs in zero-shot generation for unseen targets of hate. Hence, we fine-tuned 5 different versions of GPT-2, leaving out the examples pertaining to one target at each turn. We find out that for each configuration/version, there is a subset of the training data which is more influential with respect to the generated data (i. e. a target that shares some commonalities with the test target that can be defined a-priori). Finally, we introduce an experiment by training an automatic post-editing module to further improve the CN generation quality. The notable human evaluation results paves the way for a future direction that decouples CN generation from its 'final refinement'.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_58",
            "content": "Although tackling online hatred through CNs inherently protects the freedom of speech and has been proposed as a better alternative to the detectremove-ban approaches, automatization of CN generation can still raise some ethical concerns and some measures must be taken to avoid undesired effects during research. Thus, we address the relevant ethical considerations and our remedies as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_59",
            "content": "Annotation Guidelines: The well-being of the annotators was our top priority during the whole study. Therefore, we strictly followed the guidelines created for CN studies (Fanton et al., 2021) that were adapted from (Vidgen et al., 2019). The human evaluations have been conducted with the help of two expert annotators in CNs. These experts were already trained for the CN generation task and employed for the work presented by (Fanton et al., 2021). We further instructed them in the aims of each experiment, clearly explained the evaluation tasks, and then we exemplified proper evaluation of <HS, CN > pairs using various types of CNs. Most importantly, we limited the exposure to hateful content by providing a daily time limit of annotation. Concerning the demographics, due to the harmful content that can be found in the data, all annotators were adult volunteers, perfectly aware of the objective of the study.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_60",
            "content": "Dataset. We purposefully chose an expert-based dataset in order to avoid the risk of modeling the language of real individuals to (i) prevent any privacy issue, (ii) avoid to model inappropriate CNs (e.g. containing abusive language) that could be produced by non-experts. The dataset also focuses on the CN diversity while keeping the HSs as stereotypical as possible so that our CN generation models have a very limited diversity on the hateful language, nearly precluding the misuse.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_61",
            "content": "Computational Task. CN generation models are not meant to be used in an autonomous way, since even the best models can still produce substandard CNs containing inappropriate or negative language. Instead, following a Human-computer cooperation paradigm, our focus is on building models that can be helpful to NGO operators by providing them diverse and novel CN candidates for their hate countering activities and speed up the manual CN writing to a certain extent. This approach also gives ground to some of the measures we used during evaluation (namely choose-or-not and is-best).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_62",
            "content": "Model Distribution. In addition to the limited and simplified hateful content in the dataset we selected, we further reduce the risk of misuse by choosing a specific distribution strategy: i) we only make available the non-autoregressive models in order to eliminate the risk of using over-generation for hate speech creation, ii) we distribute such models only for research purposes and through a request based procedure in order to keep track of the possible users.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_63",
            "content": "conference of the european chapter of the association for computational linguistics, pages 313-320.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_64",
            "content": "A.1 Fine-tuning details Since LM sizes are very different for each model and since our main focus is not studying performances according to LM dimension growth, as a rule-of-thumb, we chose one version smaller than the large version of each model provided that they all have the same order of magnitude. This corresponds to the medium versions for both DialoGPT and GPT-2, and base versions for the other models. GPT-2 and DialoGPT achieve the lowest perplexity, training and evaluation loss, thus indicating a slightly more successful fine-tuning, which are reflected in the evaluations throughout the study. We conducted a hyper-parameter search during the training phase of each model using the search space: learning-rate:{1e \u2212 5, 2e \u2212 5, 3e \u2212 5, 4e \u2212 5, 5e \u2212 5}, warm-up ratio:{0, 0.1}, batchsize:{2, 4}, epochs:{2, 3, 4, 5}. It has been conducted using Optuna, with 10 trials, optimized on minimizing the evaluation loss during training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_65",
            "content": "Here we discuss the results for the overlap and diversity metrics obtained on the Best LM+D generations (Table 6), and those calculated on the human evaluation subset (Tables 7 and 8).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_66",
            "content": "BART. BART performs well with the stochastic decoding methods, in particular: Top p for overlap, diversity, syntactic metrics, and grammaticality; Top k for overlap metrics and toxicity, whereas Top pk is the best decoding approach on human evaluation and RR, and the second best on ROUGE and BLEU-1. On the contrary, BART does not achieve good results with deterministic approaches (i. e. BS). T5 -Top p : The vast majority of Muslims in the UK are peaceful people who share many values and respect.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_67",
            "content": "GPT-2 -Top p : This is not true, the vast majority of Muslims is peaceful and peaceful Muslims are happy to live in any country they wish. They do not need to be a part of a hostile state and they should be treated as equals in society.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_68",
            "content": "In these examples we can observe that the CN obtained with GPT-2, albeit not perfect, is the most complete and accurate. Those generated with T5 and DialoGPT are suitable as well. On the other hand, the CNs obtained with BART and BERT present some problems: the former has a clear contradiction in it and the latter is illustrative of some of the problems detected in BERT's generations such that it is syntactically complex but very vague and repetitive, and it ends up not really conveying a clear message.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_69",
            "content": "Hate speech: The migrant crisis is not the fault of Western Europeans. Migrants are to blame for immigration.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_70",
            "content": "DialoGPT -Top k : What if an entire religion was to blame only for the migration crisis? So you think a large number of Muslims are responsible for all the problems of our world?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_71",
            "content": "BART -Top p : The migrant crisis is caused by a lack of integration and lack of knowledge of the world.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_72",
            "content": "BERT -BS: many migrants come here to work and provide for their families. they are our brothers and sisters, and we should welcome them. Once again, GPT-2 and DialoGPT generations where chosen as the best CNs by the annotators. Moreover, these CNs are all generated via Top k decoding mechanism. BART and T5 generated CNs are acceptable while requiring some post-editing to be employed. Finally, BERT's output is still very vague: we can notice the presence of the recurring expression our brothers and sisters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_73",
            "content": "Table 9 displays the distribution of the examples with respect to the targets, in the reference dataset and in the configurations for the LOTO experiments (Section 5.3). Table 10 presents the detailed results for the novelty of the reference CNs discussed in Section 5.3, while the RR for the CNs generated with the LOTO models and for the reference CNs are shown in Table 11. The rankings for these two RR computations are the same, and the ranges are almost overlapping. This means that leaving one target out does not impact the intra-corpora repetitiveness: instead, the CNs generated with a LOTO model gain a lower RR than the reference CNs. For the target MUSLIMS a high RR is recorded, both in candidate and in the reference CNs. A high repetitiveness in the data for this target can contribute to the good results observed on overlap metrics too (Table 3 For fine-tuning our APE model, we have thus used the triplets <HS, CN or , CN pe > and <HS, CN pe * , CN pe >. In this way, we managed to roughly double the number of the post-edit training samples, which is highly beneficial for a better model. When we filtered the triplets with a positive TER score between CN ed and CN pe , or CN or and CN pe , we obtained 4185 training, 596 test, and 568 validation samples following the partition used in the first set of experiments as described in Section 3.1. Finally, the best fine-tuning configuration of the GPT-2 medium model for APE was obtained with a learning rate of 2e-5 for 3 epochs resulting in 3.34 train loss and 1.23 eval loss.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "49-ARR_v2_74",
            "content": "Jeffrey Allen, Christopher Hogan, Toward the development of a post editing module for raw machine translation output: A controlled language perspective, 2000, Third International Controlled Language Applications Workshop (CLAW-00), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Jeffrey Allen",
                    "Christopher Hogan"
                ],
                "title": "Toward the development of a post editing module for raw machine translation output: A controlled language perspective",
                "pub_date": "2000",
                "pub_title": "Third International Controlled Language Applications Workshop (CLAW-00)",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_75",
            "content": "R Kimberley, Kay Allison,  Bussey, Cyberbystanding in context: A review of the literature on witnesses' responses to cyberbullying, 2016, Children and Youth Services Review, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "R Kimberley",
                    "Kay Allison",
                    " Bussey"
                ],
                "title": "Cyberbystanding in context: A review of the literature on witnesses' responses to cyberbullying",
                "pub_date": "2016",
                "pub_title": "Children and Youth Services Review",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_76",
            "content": "Jenn Anderson, Mary Bresnahan, Catherine Musatics, Combating weight-based cyberbullying on facebook with the dissenter effect, 2014, Cyberpsychology, Behavior, and Social Networking.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jenn Anderson",
                    "Mary Bresnahan",
                    "Catherine Musatics"
                ],
                "title": "Combating weight-based cyberbullying on facebook with the dissenter effect",
                "pub_date": "2014",
                "pub_title": "Cyberpsychology",
                "pub": "Behavior, and Social Networking"
            }
        },
        {
            "ix": "49-ARR_v2_77",
            "content": "UNKNOWN, None, 2006, Comparing automatic and human evaluation of nlg systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2006",
                "pub_title": "Comparing automatic and human evaluation of nlg systems",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_78",
            "content": "UNKNOWN, None, 2015, Detection of cyberbullying incidents on the instagram social network, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Detection of cyberbullying incidents on the instagram social network",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_79",
            "content": "Svetlana Kiritchenko, Isar Nejadgholi, Kathleen C Fraser, Confronting abusive language online: A survey from the ethical and human rights perspective, 2021, Journal of Artificial Intelligence Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Svetlana Kiritchenko",
                    "Isar Nejadgholi",
                    "Kathleen C Fraser"
                ],
                "title": "Confronting abusive language online: A survey from the ethical and human rights perspective",
                "pub_date": "2021",
                "pub_title": "Journal of Artificial Intelligence Research",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_80",
            "content": "Kevin Knight, Ishwar Chander, Automated postediting of documents, 1994, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Kevin Knight",
                    "Ishwar Chander"
                ],
                "title": "Automated postediting of documents",
                "pub_date": "1994",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_81",
            "content": "Ritesh Kumar, Atul Kr Ojha, Shervin Malmasi, Marcos Zampieri, Benchmarking aggression identification in social media, 2018, Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018), .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Ritesh Kumar",
                    "Atul Kr Ojha",
                    "Shervin Malmasi",
                    "Marcos Zampieri"
                ],
                "title": "Benchmarking aggression identification in social media",
                "pub_date": "2018",
                "pub_title": "Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018)",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_82",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal ; Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_83",
            "content": "Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, Jianfeng Gao, Deep reinforcement learning for dialogue generation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Jiwei Li",
                    "Will Monroe",
                    "Alan Ritter",
                    "Dan Jurafsky",
                    "Michel Galley",
                    "Jianfeng Gao"
                ],
                "title": "Deep reinforcement learning for dialogue generation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "49-ARR_v2_84",
            "content": "Chin-Yew Lin, Rouge: A package for automatic evaluation of summaries, 2004, Text summarization branches out, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Chin-Yew Lin"
                ],
                "title": "Rouge: A package for automatic evaluation of summaries",
                "pub_date": "2004",
                "pub_title": "Text summarization branches out",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_85",
            "content": "UNKNOWN, None, 2018, Analyzing the hate and counter speech accounts on twitter, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Analyzing the hate and counter speech accounts on twitter",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_86",
            "content": "Binny Mathew, Punyajoy Saha, Hardik Tharad, Subham Rajgaria, Prajwal Singhania, Pawan Suman Kalyan Maity, Animesh Goyal,  Mukherjee, Thou shalt not hate: Countering online hate speech, 2019, Proceedings of the International AAAI Conference on Web and Social Media, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Binny Mathew",
                    "Punyajoy Saha",
                    "Hardik Tharad",
                    "Subham Rajgaria",
                    "Prajwal Singhania",
                    "Pawan Suman Kalyan Maity",
                    "Animesh Goyal",
                    " Mukherjee"
                ],
                "title": "Thou shalt not hate: Countering online hate speech",
                "pub_date": "2019",
                "pub_title": "Proceedings of the International AAAI Conference on Web and Social Media",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_87",
            "content": "Binny Mathew, Punyajoy Saha, Chris Seid Muhie Yimam, Pawan Biemann, Animesh Goyal,  Mukherjee, Hatexplain: A benchmark dataset for explainable hate speech detection, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Binny Mathew",
                    "Punyajoy Saha",
                    "Chris Seid Muhie Yimam",
                    "Pawan Biemann",
                    "Animesh Goyal",
                    " Mukherjee"
                ],
                "title": "Hatexplain: A benchmark dataset for explainable hate speech detection",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_88",
            "content": "Kevin Munger, Tweetment effects on the tweeted: Experimentally reducing racist harassment, 2017, Political Behavior, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Kevin Munger"
                ],
                "title": "Tweetment effects on the tweeted: Experimentally reducing racist harassment",
                "pub_date": "2017",
                "pub_title": "Political Behavior",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_89",
            "content": "Sarah Myers West, Censored, suspended, shadowbanned: User interpretations of content moderation on social media platforms, 2018, New Media & Society, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Sarah Myers West"
                ],
                "title": "Censored, suspended, shadowbanned: User interpretations of content moderation on social media platforms",
                "pub_date": "2018",
                "pub_title": "New Media & Society",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_90",
            "content": "Jekaterina Novikova, Ondrej Dusek, Amanda Curry, Verena Rieser, Why we need new evaluation metrics for nlg, 2017, 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Jekaterina Novikova",
                    "Ondrej Dusek",
                    "Amanda Curry",
                    "Verena Rieser"
                ],
                "title": "Why we need new evaluation metrics for nlg",
                "pub_date": "2017",
                "pub_title": "2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "49-ARR_v2_91",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting on association for computational linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th annual meeting on association for computational linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "49-ARR_v2_92",
            "content": "UNKNOWN, None, 2020, Resources and benchmark corpora for hate speech detection: a systematic review. Language Resources and Evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Resources and benchmark corpora for hate speech detection: a systematic review. Language Resources and Evaluation",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_93",
            "content": "Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding, William Wang, A benchmark dataset for learning to intervene in online hate speech, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Jing Qian",
                    "Anna Bethke",
                    "Yinyin Liu",
                    "Elizabeth Belding",
                    "William Wang"
                ],
                "title": "A benchmark dataset for learning to intervene in online hate speech",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "49-ARR_v2_94",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI Blog, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Alec Radford",
                    "Jeffrey Wu",
                    "Rewon Child",
                    "David Luan",
                    "Dario Amodei",
                    "Ilya Sutskever"
                ],
                "title": "Language models are unsupervised multitask learners",
                "pub_date": "2019",
                "pub_title": "OpenAI Blog",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_95",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_96",
            "content": "Sascha Rothe, Shashi Narayan, Aliaksei Severyn, Leveraging pre-trained checkpoints for sequence generation tasks, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Sascha Rothe",
                    "Shashi Narayan",
                    "Aliaksei Severyn"
                ],
                "title": "Leveraging pre-trained checkpoints for sequence generation tasks",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_97",
            "content": "Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A Smith, The risk of racial bias in hate speech detection, 2019, Proceedings of the 57th annual meeting of the association for computational linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Maarten Sap",
                    "Dallas Card",
                    "Saadia Gabriel",
                    "Yejin Choi",
                    "Noah A Smith"
                ],
                "title": "The risk of racial bias in hate speech detection",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th annual meeting of the association for computational linguistics",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_98",
            "content": "Carla Schieb, Mike Preuss, Governing hate speech by means of counterspeech on facebook, 2016, 66th ICA Annual Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Carla Schieb",
                    "Mike Preuss"
                ],
                "title": "Governing hate speech by means of counterspeech on facebook",
                "pub_date": "2016",
                "pub_title": "66th ICA Annual Conference",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_99",
            "content": "Anna Schmidt, Michael Wiegand, A survey on hate speech detection using natural language processing, 2017, Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Anna Schmidt",
                    "Michael Wiegand"
                ],
                "title": "A survey on hate speech detection using natural language processing",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_100",
            "content": "UNKNOWN, None, 2016, The impact of counter-narratives. Institute for Strategic Dialogue, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "The impact of counter-narratives. Institute for Strategic Dialogue",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_101",
            "content": "Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, John Makhoul, A study of translation edit rate with targeted human annotation, 2006, Proceedings of association for machine translation in the Americas, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Matthew Snover",
                    "Bonnie Dorr",
                    "Richard Schwartz",
                    "Linnea Micciulla",
                    "John Makhoul"
                ],
                "title": "A study of translation edit rate with targeted human annotation",
                "pub_date": "2006",
                "pub_title": "Proceedings of association for machine translation in the Americas",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_102",
            "content": "Yi-Ling Serra Sinem Tekiroglu, Marco Chung,  Guerini, Generating counter narratives against online hate speech: Data and strategies, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Yi-Ling Serra Sinem Tekiroglu",
                    "Marco Chung",
                    " Guerini"
                ],
                "title": "Generating counter narratives against online hate speech: Data and strategies",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_103",
            "content": ", Demoting racial bias in hate speech detection, , Mengzhou Xia Anjalie Field Yulia Tsvetkov. 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [],
                "title": "Demoting racial bias in hate speech detection",
                "pub_date": null,
                "pub_title": "Mengzhou Xia Anjalie Field Yulia Tsvetkov. 2020",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_104",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_105",
            "content": "Bertie Vidgen, Leon Derczynski, Directions in abusive language training data, a systematic review: Garbage in, garbage out, 2020, Plos one, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Bertie Vidgen",
                    "Leon Derczynski"
                ],
                "title": "Directions in abusive language training data, a systematic review: Garbage in, garbage out",
                "pub_date": "2020",
                "pub_title": "Plos one",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_106",
            "content": "Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, Helen Margetts, Challenges and frontiers in abusive content detection, 2019, Proceedings of the third workshop on abusive language online, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Bertie Vidgen",
                    "Alex Harris",
                    "Dong Nguyen",
                    "Rebekah Tromble",
                    "Scott Hale",
                    "Helen Margetts"
                ],
                "title": "Challenges and frontiers in abusive content detection",
                "pub_date": "2019",
                "pub_title": "Proceedings of the third workshop on abusive language online",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_107",
            "content": "UNKNOWN, None, 2020, Learning from the worst: Dynamically generated datasets to improve online hate detection, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Learning from the worst: Dynamically generated datasets to improve online hate detection",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_108",
            "content": "Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, Jordan Boyd-Graber, Trick me if you can: Human-in-the-loop generation of adversarial question answering examples, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Eric Wallace",
                    "Pedro Rodriguez",
                    "Shi Feng",
                    "Ikuya Yamada",
                    "Jordan Boyd-Graber"
                ],
                "title": "Trick me if you can: Human-in-the-loop generation of adversarial question answering examples",
                "pub_date": "2019",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_109",
            "content": "Alex Wang, Kyunghyun Cho, BERT has a mouth, and it must speak: BERT as a Markov random field language model, 2019, Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Alex Wang",
                    "Kyunghyun Cho"
                ],
                "title": "BERT has a mouth, and it must speak: BERT as a Markov random field language model",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_110",
            "content": "Ke Wang, Xiaojun Wan, Sentigan: Generating sentimental texts via mixture adversarial networks, 2018, IJCAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Ke Wang",
                    "Xiaojun Wan"
                ],
                "title": "Sentigan: Generating sentimental texts via mixture adversarial networks",
                "pub_date": "2018",
                "pub_title": "IJCAI",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_111",
            "content": "Zeerak Waseem, Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter, 2016, Proceedings of the first workshop on NLP and computational social science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Zeerak Waseem"
                ],
                "title": "Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter",
                "pub_date": "2016",
                "pub_title": "Proceedings of the first workshop on NLP and computational social science",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_112",
            "content": "Sam Wiseman, Stuart Shieber, Alexander Rush, Challenges in data-to-document generation, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Sam Wiseman",
                    "Stuart Shieber",
                    "Alexander Rush"
                ],
                "title": "Challenges in data-to-document generation",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "49-ARR_v2_113",
            "content": "UNKNOWN, None, , Jason Weston, and Emily Dinan. 2020. Recipes for safety in open-domain chatbots. arXiv e-prints, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Jason Weston, and Emily Dinan. 2020. Recipes for safety in open-domain chatbots. arXiv e-prints",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_114",
            "content": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan, DIALOGPT : Largescale generative pre-training for conversational response generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Yizhe Zhang",
                    "Siqi Sun",
                    "Michel Galley",
                    "Yen-Chun Chen",
                    "Chris Brockett",
                    "Xiang Gao",
                    "Jianfeng Gao",
                    "Jingjing Liu",
                    "Bill Dolan"
                ],
                "title": "DIALOGPT : Largescale generative pre-training for conversational response generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_115",
            "content": "Wanzheng Zhu, Suma Bhat, 2021. Generate, prune, select: A pipeline for counterspeech generation against online hate speech, , Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Wanzheng Zhu",
                    "Suma Bhat"
                ],
                "title": "2021. Generate, prune, select: A pipeline for counterspeech generation against online hate speech",
                "pub_date": null,
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_116",
            "content": "UNKNOWN, None, , Examples of the generations Hate speech: Islam is a violent religion that promotes terror and war, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Examples of the generations Hate speech: Islam is a violent religion that promotes terror and war",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_117",
            "content": "UNKNOWN, None, , The vast majority of Muslims in the world are peaceful and friendly people. I don't see how you can say such a thing about them, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "The vast majority of Muslims in the world are peaceful and friendly people. I don't see how you can say such a thing about them",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_118",
            "content": "UNKNOWN, None, , The vast majority of Muslims are peaceful people who have lived in peace and harmony for many years. How can you say this about a religion that promotes terror and war? BERT -Top k : islam is a religion of peace, and it is not a peaceful religion of violence. it is about peace and understanding of the world's problems, and peace is about bringing peace and peace, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "The vast majority of Muslims are peaceful people who have lived in peace and harmony for many years. How can you say this about a religion that promotes terror and war? BERT -Top k : islam is a religion of peace, and it is not a peaceful religion of violence. it is about peace and understanding of the world's problems, and peace is about bringing peace and peace",
                "pub": null
            }
        },
        {
            "ix": "49-ARR_v2_119",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "49-ARR_v2_0@0",
            "content": "Using Pre-Trained Language Models for Producing Counter Narratives Against Hate Speech: a Comparative Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_0",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_2@0",
            "content": "In this work, we present an extensive study on the use of pre-trained language models for the task of automatic Counter Narrative (CN) generation to fight online hate speech in English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_2",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_2@1",
            "content": "We first present a comparative study to determine whether there is a particular Language Model (or class of LMs) and a particular decoding mechanism that are the most appropriate to generate CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_2",
            "start": 186,
            "end": 380,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_2@2",
            "content": "Findings show that autoregressive models combined with stochastic decodings are the most promising.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_2",
            "start": 382,
            "end": 480,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_2@3",
            "content": "We then investigate how an LM performs in generating a CN with regard to an unseen target of hate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_2",
            "start": 482,
            "end": 579,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_2@4",
            "content": "We find out that a key element for successful 'out of target' experiments is not an overall similarity with the training data but the presence of a specific subset of training data, i. e. a target that shares some commonalities with the test target that can be defined a-priori.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_2",
            "start": 581,
            "end": 858,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_2@5",
            "content": "We finally introduce the idea of a pipeline based on the addition of an automatic post-editing step to refine generated CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_2",
            "start": 860,
            "end": 983,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@0",
            "content": "Hate Speech (HS) has found fertile ground in Social Media Platforms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@1",
            "content": "Actions undertaken by such platforms to tackle online hatred consist in identifying possible sources of hate and removing them by means of content deletion, account suspension or shadow-banning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 69,
            "end": 262,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@2",
            "content": "However, these actions are often interpreted and denounced as censorship by the affected users and political groups (Myers West, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 264,
            "end": 398,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@3",
            "content": "For this reason, such restrictions can have the opposite effect of exacerbating the hostility of the haters (Munger, 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 400,
            "end": 522,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@4",
            "content": "An alternative strategy, that is looming on the horizon, is based on the use of Counter Narratives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 524,
            "end": 622,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@5",
            "content": "CNs are \"all communicative actions aimed at refuting hate speech through thoughtful and cogent reasons, and true and fact-bound arguments\" (Schieb and Preuss, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 624,
            "end": 788,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@6",
            "content": "As a de-escalating * Now at the University of Stuttgart, Germany. measure, CNs have been proven to be successful in diminishing hate, while preserving the freedom of speech (Benesch, 2014;Gagliardone et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 790,
            "end": 1003,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@7",
            "content": "An example of <HS, CN > pair is shown below: HS: Women are basically childlike, they remain this way most of their lives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 1005,
            "end": 1125,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@8",
            "content": "Soft and emotional.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 1127,
            "end": 1145,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@9",
            "content": "It has devastated our once great patriarchal civilizations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 1147,
            "end": 1205,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@10",
            "content": "CN: Without softness and emotions there would be just brutality and cruelty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 1207,
            "end": 1282,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@11",
            "content": "Not all women are soft and emotional and many men have these characteristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 1284,
            "end": 1360,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_4@12",
            "content": "To perpetuate these socially constructed gender profiles maintains norms which oppress anybody.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_4",
            "start": 1362,
            "end": 1456,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_5@0",
            "content": "Based on their effectiveness, CNs have started being employed by NGOs to counter online hate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_5",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_5@1",
            "content": "Since for NGO operators it is impossible to manually write responses to all instances of hate, a line of NLP research has recently emerged, focusing on designing systems to automatically generate CN suggestions (Qian et al., 2019;Tekiroglu et al., 2020;Fanton et al., 2021;Chung et al., 2021a;Zhu and Bhat, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_5",
            "start": 94,
            "end": 406,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_5@2",
            "content": "In this study, our main goal is to compare pre-trained language models (LM) and decoding mechanisms in order to understand their pros and cons in generating CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_5",
            "start": 408,
            "end": 568,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_5@3",
            "content": "Thus, we use various automatic metrics and manual evaluations with expert judgments to assess several LMs, representing the main categories of the model architectures, and decoding methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_5",
            "start": 570,
            "end": 758,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_5@4",
            "content": "We further test the robustness of the fine-tuned LMs in generating CNs for an unseen target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_5",
            "start": 760,
            "end": 851,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_5@5",
            "content": "Results show that autoregressive models are in general more suited for the task, and while stochastic decoding mechanisms can generate more novel, diverse, and informative outputs, the deterministic decoding is useful in scenarios where more generic and less novel (yet 'safer') CNs are needed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_5",
            "start": 853,
            "end": 1146,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_5@6",
            "content": "Furthermore, in out-of-target experiments we find that the similarity of targets (e.g. JEWS and MUSLIMS as religious groups) plays a crucial role for the effectiveness of portability to new targets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_5",
            "start": 1148,
            "end": 1345,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_5@7",
            "content": "We finally show a promising research direction of leveraging gold human edits for building an additional automatic post-editing step to correct errors made by LMs during generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_5",
            "start": 1347,
            "end": 1527,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_5@8",
            "content": "To the best of our knowledge, this is the first study systematically analysing state of the art pre-trained LMs in CN generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_5",
            "start": 1529,
            "end": 1657,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_6@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_6",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_7@0",
            "content": "In this section we first discuss standard approaches to hate countering and studies on CN effectiveness on Social Media Platforms, then the existing CN data collection and generation strategies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_7",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_8@0",
            "content": "Hate countering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_8",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_8@1",
            "content": "NLP has started addressing the phenomenon of the proliferation of HS by creating datasets for automatic detection (Mathew et al., 2021;Cao et al., 2020;Hosseinmardi et al., 2015;Waseem, 2016;Burnap and Williams, 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_8",
            "start": 17,
            "end": 234,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_8@2",
            "content": "Several surveys provide a review on the existing approaches on the topic (Poletto et al., 2020;Schmidt and Wiegand, 2017;Fortuna and Nunes, 2018), also addressing the ethical challenges of the task (Kiritchenko et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_8",
            "start": 236,
            "end": 460,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_8@3",
            "content": "Still, automatic detection of HS presents some drawbacks (Vidgen and Derczynski, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_8",
            "start": 462,
            "end": 548,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_8@4",
            "content": "First of all, the datasets might include biases, and the models tend to replicate such biases (Binns et al., 2017;Davidson et al., 2019;Sap et al., 2019;Tsvetkov, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_8",
            "start": 550,
            "end": 718,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_8@5",
            "content": "Moreover, the end goals for which HS detection is employed are often charged with censorship of the freedom of speech by concerned users (Munger, 2017;Myers West, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_8",
            "start": 720,
            "end": 888,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_8@6",
            "content": "In this scenario, NGOs have started employing CNs to counter online hate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_8",
            "start": 890,
            "end": 962,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_8@7",
            "content": "CNs have been shown to be effective in reducing linguistic violence (Benesch, 2014;Gagliardone et al., 2015;Schieb and Preuss, 2016;Silverman et al., 2016;Mathew et al., 2019); moreover, even if they might not influence the view of extremists, they are still effective in presenting alternative and non-hateful viewpoints to bystanders (Allison and Bussey, 2016;Anderson et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_8",
            "start": 964,
            "end": 1348,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_9@0",
            "content": "The existing studies for collecting CN datasets employ four main approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_9",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_9@1",
            "content": "Crawling consists in automatically scraping websites, starting from an HS content and searching for possible CNs among the responses (Mathew et al., 2018(Mathew et al., , 2019.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_9",
            "start": 77,
            "end": 252,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_9@2",
            "content": "With crowdsourcing CNs are written by non-expert paid workers as responses to provided hate content (Qian et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_9",
            "start": 254,
            "end": 373,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_9@3",
            "content": "Nichesourcing relies on a niche group of experts for data collection (De Boer et al., 2012), and it was employed by Chung et al. (2019) for CN collection using NGO's operators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_9",
            "start": 375,
            "end": 550,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_9@4",
            "content": "Hybrid approaches use a combination of LMs and humans to collect data (Wallace et al., 2019;Dinan et al., 2019;. Studies on CN collection are presented in more detail by Tekiroglu et al. (2020);Fanton et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_9",
            "start": 552,
            "end": 766,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_9@5",
            "content": "2019) employ a mix of automatic and human intervention to generate CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_9",
            "start": 768,
            "end": 838,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_9@6",
            "content": "Zhu and Bhat (2021) propose an entirely automated pipeline of candidate CN generation and filtering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_9",
            "start": 840,
            "end": 939,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_9@7",
            "content": "Other lines of work include CN generation for under-resourced languages such as for Italian (Chung et al., 2020), and the generation of knowledge-bound CNs, which allows the production of CNs based on grounded and up-todate facts and plausible arguments, avoiding the hallucination phenomena (Chung et al., 2021a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_9",
            "start": 941,
            "end": 1254,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_9@8",
            "content": "Instead, in our work we take a more foundational perspective, which is relevant for all the LM-based pipelines described above.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_9",
            "start": 1256,
            "end": 1382,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_9@9",
            "content": "Therefore, we compare and assess various state of the art pre-trained LMs in an end-to-end setting, which is developed as a downstream task for CN generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_9",
            "start": 1384,
            "end": 1541,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_10@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_10",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_11@0",
            "content": "In this section, we present the CN dataset, the language models, and the decoding mechanisms employed for our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_11",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_12@0",
            "content": "Dataset for fine-tuning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_12",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_13@0",
            "content": "For this study we rely on the dataset proposed by Fanton et al. (2021), which is the only available dataset that grants both the target diversity and the CN quality we aim for.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_13",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_13@1",
            "content": "The dataset was collected with a human-in-the-loop approach, by employing an autoregressive LM (GPT-2) paired with three expert human reviewers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_13",
            "start": 177,
            "end": 320,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_13@2",
            "content": "It features 5003 <HS, CN > pairs, covering several targets of hate including DISABLED, JEWS, LGBT+, MIGRANTS, MUSLIMS, POC, WOMEN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_13",
            "start": 322,
            "end": 451,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_13@3",
            "content": "The residual categories are collapsed to the label OTHER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_13",
            "start": 453,
            "end": 509,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_13@4",
            "content": "We partitioned the dataset into training, validation, and test sets with the ratio: 8 : 1 : 1 (i. e. 4003, 500 and 500 pairs), ensuring that all sets share the same target distribution, and no repetition of HS across the sets is allowed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_13",
            "start": 511,
            "end": 747,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_14@0",
            "content": "Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_14",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_15@0",
            "content": "We experiment with 5 Transformer based LMs (Vaswani et al., 2017) (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_15",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_15@1",
            "content": "It is a bidirectional autoencoder that can be adapted to text generation (Wang and Cho, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_15",
            "start": 74,
            "end": 167,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_15@2",
            "content": "GPT-2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_15",
            "start": 169,
            "end": 174,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_15@3",
            "content": "The Generative Pre-trained Transformer 2 is an autoregressive model built for text generation (Radford et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_15",
            "start": 176,
            "end": 292,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_15@4",
            "content": "DialoGPT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_15",
            "start": 294,
            "end": 302,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_15@5",
            "content": "The Dialogue Generative Pretrained Transformer is the extension of GPT-2 specifically created for conversational response generation (Zhang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_15",
            "start": 304,
            "end": 457,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_15@6",
            "content": "BART.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_15",
            "start": 459,
            "end": 463,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_15@7",
            "content": "BART is a denoising autoencoder for pretraining seq2seq models (Lewis et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_15",
            "start": 465,
            "end": 548,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_15@8",
            "content": "The encoder-decoder architecture of BART is composed of a bidirectional encoder and an autoregressive decoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_15",
            "start": 550,
            "end": 659,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_16@0",
            "content": "The Text-to-Text Transfer Transformer proposed by Raffel et al. ( 2020) is a seq2seq model with an encoder-decoder Transformer architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_16",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_16@1",
            "content": "While all the other models could be fine-tuned directly for the generation task, for BERT we warmstarted an encoder-decoder model using BERT checkpoints similar to the BERT2BERT model defined by (Rothe et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_16",
            "start": 141,
            "end": 356,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_16@2",
            "content": "The fine-tuning details and hyperparameter settings can be found in Appendix A.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_16",
            "start": 358,
            "end": 438,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_17@0",
            "content": "Decoding mechanisms",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_17",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_18@0",
            "content": "We utilize 4 decoding mechanisms: a deterministic (Beam Search) and three stochastic (Top-k, Top-p, and a combination of the two).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_18",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_18@1",
            "content": "Beam Search (BS).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_18",
            "start": 131,
            "end": 147,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_18@2",
            "content": "The Beam Search algorithm is designed to pick the most-likely sequence (Li et al., 2016;Wiseman et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_18",
            "start": 149,
            "end": 258,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_18@3",
            "content": "Top-k (Top k ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_18",
            "start": 260,
            "end": 274,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_18@4",
            "content": "The sampling procedure proposed by Fan et al. (2018) selects a random word from the k most probable ones, at each time step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_18",
            "start": 276,
            "end": 399,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_18@5",
            "content": "Top-p (Top p ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_18",
            "start": 401,
            "end": 415,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_18@6",
            "content": "Also known as Nucleus Sampling, the parameter p indicates the total probability for the pooled candidates, at each time step (Holtzman et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_18",
            "start": 417,
            "end": 565,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_18@7",
            "content": "Combining Top-p and Top-k (Top pk ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_18",
            "start": 567,
            "end": 602,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_18@8",
            "content": "At decoding stage, it is possible to combine the parameters p and k.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_18",
            "start": 604,
            "end": 671,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_18@9",
            "content": "This is a Nucleus Sampling constrained to the Top-k most probable words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_18",
            "start": 673,
            "end": 744,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_19@0",
            "content": "In our experiments we used the following parameters as default: Beam-Search with 5 beams and repetition penalty = 2; Top-k with k = 40; Top-p with p = .92; Top pk with k = 40 and p = .92.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_19",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_20@0",
            "content": "Evaluation metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_20",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_21@0",
            "content": "We use several metrics to evaluate various aspects of the CN generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_21",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_21@1",
            "content": "Overlap Metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_21",
            "start": 73,
            "end": 88,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_21@2",
            "content": "These metrics depend on the n-gram similarity of the generated outputs to a set of reference texts in order to assess the quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_21",
            "start": 90,
            "end": 219,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_21@3",
            "content": "We used our gold CNs as references and the CNs generated by the different models, as candidates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_21",
            "start": 221,
            "end": 316,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_22@0",
            "content": "In particular, we employed three BLEU variants: BLEU-1 (B-1), BLEU-3 (B-3) and BLEU-4 (B-4) (Papineni et al., 2002), and ROUGE-L (ROU) (Lin, 2004).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_22",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_22@1",
            "content": "Diversity metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_22",
            "start": 148,
            "end": 165,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_22@2",
            "content": "They are used to measure how diverse and novel the produced CNs are.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_22",
            "start": 167,
            "end": 234,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_22@3",
            "content": "In particular, we utilized Repetition Rate (RR) to measure the repetitiveness across generated CNs, in terms of the average ratios of non-singleton n-grams present in the corpus (Bertoldi et al., 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_22",
            "start": 236,
            "end": 437,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_22@4",
            "content": "It should be noted that RR is calculated as a corpus-based repetition score , i.e. inter-CN, instead of calculating intra-CN repetition of n-grams only.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_22",
            "start": 439,
            "end": 590,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_22@5",
            "content": "We also used Novelty (NOV) (Wang and Wan, 2018), based on Jaccard similarity, to compute the amount of novel content that is present in the generated CNs as compared to the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_22",
            "start": 592,
            "end": 778,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_22@6",
            "content": "Human evaluation metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_22",
            "start": 780,
            "end": 804,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_22@7",
            "content": "Albeit more difficult to attain, human judgments provide a more reliable evaluation and a deeper understanding than automatic metrics (Belz and Reiter, 2006;Novikova et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_22",
            "start": 806,
            "end": 985,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_22@8",
            "content": "To this end, we specified the following dimensions for the evaluation of CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_22",
            "start": 987,
            "end": 1063,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_22@9",
            "content": "Suitableness (SUI): measures how suitable a CN is to the HS in terms of semantic relatedness and in terms of adherence to CN guidelines 1 ; Grammaticality (GRM): how grammatically correct a generated CN is; Specificity (SPE): how specific are the arguments brought by the CN in response to the HS; Choose-or-not (CHO): determines whether the annotators would select that CN to post-edit and use it in a real case scenario as in the set up presented by Chung et al. (2021b); Is-best (BEST): whether the CN is the absolute best among the ones generated for an HS (i. e. whether the annotators would pick up exactly that CN if they had to use it in a real case scenario).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_22",
            "start": 1065,
            "end": 1732,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_23@0",
            "content": "The first three dimensions are rated with a 5points Likert scale and follow the evaluation procedure described by Chung et al. (2020), whereas both choose-or-not and is-best are binary ratings (0, 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_23",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_23@1",
            "content": "Choose-or-not allows for multiple CNs to be selected for the same HS, while only one CN can be selected for is-best for each HS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_23",
            "start": 201,
            "end": 328,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_23@2",
            "content": "Toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_23",
            "start": 330,
            "end": 338,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_23@3",
            "content": "2 It determines how \"rude, disrespectful, or unreasonable\" a text is.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_23",
            "start": 340,
            "end": 408,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_23@4",
            "content": "Toxicity has been employed both to detect the bias present in LMs (Gehman et al., 2020) and as a solution to mitigate such bias (Gehman et al., 2020;Xu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_23",
            "start": 410,
            "end": 575,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_23@5",
            "content": "Syntactic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_23",
            "start": 577,
            "end": 594,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_23@6",
            "content": "A high syntactic complexity can be used as a proxy for an LM's ability of generating complex arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_23",
            "start": 596,
            "end": 698,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_23@7",
            "content": "We used the syntactic dependency parser of spaCy 3 For the task, focusing on the following measures: Maximum Syntactic Depth (MSD): the maximum depth among the dependency trees calculated over each sentence composing a CN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_23",
            "start": 700,
            "end": 921,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_23@8",
            "content": "Average Syntactic Depth (ASD): the average depth of the sentences in each CN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_23",
            "start": 923,
            "end": 999,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_23@9",
            "content": "Number of Sentences (NST): the number of sentences composing a CN.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_23",
            "start": 1001,
            "end": 1066,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_24@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_24",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_25@0",
            "content": "We performed two sets of experiments: first, we assessed how LMs perform in the task of generating CNs with different decoding mechanisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_25",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_25@1",
            "content": "Then, we selected the best model from the first round of experiments and tested its generalization capabilities when confronted with an unseen target of hate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_25",
            "start": 139,
            "end": 296,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_26@0",
            "content": "LMs and decoding experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_26",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_27@0",
            "content": "For the first round of experiments, in order to avoid possible unfair assessments given by the open nature of the generative task (i. e. a highly suitable CN candidate could be scored low due to its difference from the single reference/gold CN), at test time we allowed the generation of several candidates for each HS+LM+decoding mechanism combination.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_27",
            "start": 0,
            "end": 352,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_27@1",
            "content": "We loosely drew inspiration from the Rank-N Accuracy procedure and the 'generate, prune, select' procedure (Zhu and Bhat, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_27",
            "start": 354,
            "end": 481,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_27@2",
            "content": "In particular, given an LM and a decoding mechanism, we generated 5 CNs for each HS in the test set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_27",
            "start": 483,
            "end": 582,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@0",
            "content": "We set up the automatic evaluation strategy as displayed in Figure 1. First, we scored each CN with the overlap metrics presented in Section 4, using the gold CN as a reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@1",
            "content": "Next, we ranked the candidate CNs with respect to the overlap scores and computed the mean of the rankings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 178,
            "end": 284,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@2",
            "content": "Then, we selected the best ones according to the following criteria: Best LM selects the single best CN for an HS among the 20 generated by the 4 models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 286,
            "end": 438,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@3",
            "content": "Best D selects the single best CN for an HS among the 25 generated by the 5 decoding configurations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 440,
            "end": 539,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@4",
            "content": "Best LM+D selects the single best CN among the 5 generated with each model-decoding combination.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 541,
            "end": 636,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@5",
            "content": "Moreover, we assessed the overall corpus-wise quality of the generated CNs with respect to the models, to the decoding mechanisms, and to the model-decoding combinations via the diversity metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 638,
            "end": 833,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@6",
            "content": "Human evaluation on a sample To perform the human evaluation we referred to the Best LM generations and sampled 200 instances from it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 835,
            "end": 968,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@7",
            "content": "Each instance comprises an HS and 5 relevant CNs, each generated by a different model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 970,
            "end": 1055,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@8",
            "content": "We recruited 2 annotators who were trained extensively for the task following the procedure used by Fanton et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 1057,
            "end": 1177,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@9",
            "content": "The expert annotators were asked to evaluate the 5 CNs corresponding to the HS, according to the dimensions described in Section 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 1179,
            "end": 1309,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_28@10",
            "content": "We enriched the evaluation of this subset with the toxicity and the syntactic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_28",
            "start": 1311,
            "end": 1396,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_29@0",
            "content": "Results of the first set of experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_29",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_30@0",
            "content": "The results of the experiments on the LMs and the decoding mechanisms are reported in this section 4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_30",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_31@0",
            "content": "The results of the comparison of the models on the Best LM generations can be found in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_31",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_31@1",
            "content": "Regarding the overlap and diversity metrics, DialoGPT records the best or the second best score in all the metrics, apart from novelty where it still achieves a high score (0.643) close to the best performance (0.655).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_31",
            "start": 96,
            "end": 313,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_31@2",
            "content": "T5 also achieves high scores, especially on ROUGE, BLEU-1 and novelty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_31",
            "start": 315,
            "end": 384,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_32@0",
            "content": "BART, instead, is the best model according to human evaluation metrics, apart from specificity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_32",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_32@1",
            "content": "On the other hand, it shows poor performances in terms of diversity metrics, indicating that it tends to produce grammatical and suitable but very generic responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_32",
            "start": 96,
            "end": 260,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_33@0",
            "content": "BERT records the worst scores for all the overlap and diversity metrics apart from novelty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_33",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_33@1",
            "content": "However, it also achieves the best syntactic metric results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_33",
            "start": 92,
            "end": 151,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_33@2",
            "content": "Therefore, it is evident that BERT's output is more complex, but very repetitive.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_33",
            "start": 153,
            "end": 233,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_33@3",
            "content": "The combination of these aspects eventually affects the clarity of BERT's output such that it yields poor results in the human evaluation, in particular for grammaticality (4.2, while other models are above 4.6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_33",
            "start": 235,
            "end": 446,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_33@4",
            "content": "This poor grammaticality can also explain the syntactic scores since the spaCy dependency parser was not trained to handle ungrammatical text and this could actually inflates the ASD and MSD scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_33",
            "start": 448,
            "end": 645,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_34@0",
            "content": "GPT-2 overall yields very competitive results for several groups of metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_34",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_34@1",
            "content": "It obtains the secondhighest novelty score (0.653) and the best RR (7.736).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_34",
            "start": 77,
            "end": 151,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_34@2",
            "content": "It also achieves the second best results on BLEU-3, maximum syntactic depth and number of sentences, and the best results on toxicity and specificity (2.880) indicating the ability to produce complex, suitable, focused and diverse CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_34",
            "start": 153,
            "end": 387,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_35@0",
            "content": "After the human evaluation we ran a qualitative interview with the annotators, whose feedback on the data strengthened the results we observed and the conclusion we drew.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_35",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_35@1",
            "content": "For instance, they reported the repetition of simple, yet catch-them-all, expressions (e.g. \"they are our brothers and sisters\") regardless of the target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_35",
            "start": 171,
            "end": 324,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_35@2",
            "content": "Further inspections found that those CNs were mainly produced by BERT, which is in line with BERT's RR score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_35",
            "start": 326,
            "end": 434,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_36@0",
            "content": "Best Decoding mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_36",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_36@1",
            "content": "The results calculated on Best D output are presented in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_36",
            "start": 25,
            "end": 89,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_36@2",
            "content": "Top k is the best performing decoding mechanism achieving the best results on the diversity metrics, BLEU-3 and BLEU-4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_36",
            "start": 91,
            "end": 209,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_36@3",
            "content": "It is also the best performing for specificity, maximum syntactic depth and number of sentences, and the second best for average syntactic depth and toxicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_36",
            "start": 211,
            "end": 368,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_37@0",
            "content": "The other stochastic decoding mechanisms perform well too.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_37",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_37@1",
            "content": "Top p yields competitive results on both diversity and overlap metrics; it is the second best for specificity, and achieves good results on the syntactic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_37",
            "start": 59,
            "end": 220,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_37@2",
            "content": "Top pk has a good performance on the overlap metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_37",
            "start": 222,
            "end": 274,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_37@3",
            "content": "It obtains the second-highest scores in most of the human evaluation metrics and the lowest in toxicity, and it reaches a reasonable specificity score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_37",
            "start": 276,
            "end": 426,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_38@0",
            "content": "On the other hand, BS does not achieve particularly good results, except for the ROUGE score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_38",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_38@1",
            "content": "Even if it is the best decoding with respect to the human evaluation, this comes at the cost of specificity and diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_38",
            "start": 94,
            "end": 215,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_38@2",
            "content": "Through a post-hoc manual analysis we observed that it was due to the deterministic nature of BS, that tends to choose the most probable sequences, i. e. the \"safest\", thus resulting in vague and repetitive outputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_38",
            "start": 217,
            "end": 431,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_39@0",
            "content": "Best Model-Decoding combination Here we briefly discuss the results of the evaluation obtained on the Best LM+D generations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_39",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_39@1",
            "content": "In particular, the autoregressive models GPT-2 and DialoGPT behave similarly with similar decoding mechanisms, such that BS outputs the best results for almost all the overlap metrics, and the worst for the diversity metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_39",
            "start": 125,
            "end": 349,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_39@2",
            "content": "On the contrary, for the other models, the results achieved with stochastic decoding mechanisms are the best for the overlap metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_39",
            "start": 351,
            "end": 483,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_39@3",
            "content": "In almost all the cases, we observe that the stochastic decoding mechanisms perform better on syntactic and diversity metrics and on toxicity, while for the human evaluation metrics BS tends to be the best, except for specificity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_39",
            "start": 485,
            "end": 714,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_39@4",
            "content": "A detailed discussion can be found in Appendix A.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_39",
            "start": 716,
            "end": 766,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_39@5",
            "content": "Discussion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_39",
            "start": 768,
            "end": 778,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_39@6",
            "content": "In this set of experiments, we found that the autoregressive models perform the best according to a combination of several metrics that we deem particularly relevant (e.g. more novel, diverse, and informative outputs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_39",
            "start": 780,
            "end": 997,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_39@7",
            "content": "Of course more repetitive and conservative outputs can be preferred when high precision of suitable CNs are required at the expense of being more generic and less novel.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_39",
            "start": 999,
            "end": 1167,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_40@0",
            "content": "Still, for what concerns autoregressive models it could be argued that the good performance of the GPT-2 model we fine-tuned is due to the fact that generated CNs and gold CNs derive from a similar distribution (GPT-2 was employed in the humanin-the-loop process used to create the reference dataset from Fanton et al. ( 2021)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_40",
            "start": 0,
            "end": 327,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_40@1",
            "content": "While we recognize that this could partially explain the performance of our GPT-2 model, it does not explain the performance of DialoGPT, which is pre-trained on a completely different dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_40",
            "start": 329,
            "end": 521,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_40@2",
            "content": "Therefore, we can reasonably conclude that autoregressive models are particularly suited for the task, regardless of the pre-training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_40",
            "start": 523,
            "end": 661,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_41@0",
            "content": "With respect to the decoding mechanisms, we record high repetitiveness and low novelty for the deterministic decoding BS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_41",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_41@1",
            "content": "Even if it reaches high scores in most of the human evaluation metrics, it fails to produce specific CNs ending up in generating suitable, yet generic responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_41",
            "start": 122,
            "end": 282,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_41@2",
            "content": "On the contrary, stochastic decoding mechanisms produce more novel and specific responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_41",
            "start": 284,
            "end": 373,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_42@0",
            "content": "Example CNs generated in this session of experiments, along with some qualitative analysis, can be found in Appendix A.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_42",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_43@0",
            "content": "Leave One Target Out experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_43",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_44@0",
            "content": "In the second stage, we built a set of cross-domain experiments to capture the generalization capabilities of the best LM determined in the previous experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_44",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_44@1",
            "content": "Specifically, we concentrate on assessing how much a pre-trained language model fine-tuned on a pool of hate targets can generalize to an unseen target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_44",
            "start": 162,
            "end": 313,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_45@0",
            "content": "Thus, for the out of target experiment we selected the LM that we deem the most prominent in order to reduce the number of LM configurations to compare.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_45",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_45@1",
            "content": "In particular, since we want to examine the generalization capability of the LM, the generation of novel CNs, in comparison to the training data, is given primary importance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_45",
            "start": 153,
            "end": 326,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_45@2",
            "content": "Secondly, specificity is also crucial since it signifies the ability of the LM/decoding mechanism in generating accurate CNs and avoiding vague yet suitable, catch-all CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_45",
            "start": 328,
            "end": 499,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_45@3",
            "content": "In contrast, repetitiveness is an undesirable feature of CNs, as it signals the tendency of a model to produce less flexible content.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_45",
            "start": 501,
            "end": 633,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_45@4",
            "content": "Given these considerations, we chose to employ GPT-2 with Top k decoding for the Leave One Target Out (LOTO) experiments since it is the configuration achieving the best trade-off amongst all the others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_45",
            "start": 635,
            "end": 837,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_45@5",
            "content": "This set of experiments is structured in 3 steps, replicated for each of the selected targets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_45",
            "start": 839,
            "end": 932,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_45@6",
            "content": "We selected the targets with the highest number of examples (MUSLIMS, MIGRANTS, WOMEN, LGBT+ and JEWS) to have a sufficient sized test set for each configuration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_45",
            "start": 934,
            "end": 1095,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_46@0",
            "content": "First, we sampled from the Fanton et al. ( 2021) dataset 600 pairs for each LOTO target, in order to have a balanced setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_46",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_46@1",
            "content": "Additionally, POC and DISABLED were always kept in the training set, and we removed multi-target cases from OTHER.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_46",
            "start": 126,
            "end": 239,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_46@2",
            "content": "The resulting dataset consists of 3729 instances (further details are provided in Appendix A.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_46",
            "start": 241,
            "end": 336,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_46@3",
            "content": "Sec-ondly, we fine-tuned 5 different configurations of the LM, and in each configuration one of the 5 LOTO targets is not present in the training data: LM -JEWS , LM -LGTB+ , LM -MIGRANTS , LM -MUSLIMS and LM -WOMEN .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_46",
            "start": 338,
            "end": 554,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_46@4",
            "content": "Finally, we tested each LOTO model on the 600 HSs in the test set made of \"left out\" target examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_46",
            "start": 556,
            "end": 656,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_46@5",
            "content": "For instance, the model LM -JEWS is used for generating the CNs for the target JEWS, after being trained on <HS, CN > data without any instances with the label JEWS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_46",
            "start": 658,
            "end": 822,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_46@6",
            "content": "We generated 5 CNs for each HS and selected the best CN according to the procedure described in Section 5.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_46",
            "start": 824,
            "end": 931,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_47@0",
            "content": "We analyse the CNs generated with the LOTO models through overlap and diversity metrics (Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_47",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_47@1",
            "content": "We refer to Appendix A.4 for the comparison between RR calculated on the candidate CNs and the reference CNs of the Fanton et al. ( 2021) dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_47",
            "start": 99,
            "end": 244,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@0",
            "content": "For all the targets we record higher novelty scores as compared to the previous experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@1",
            "content": "Higher novelty ranges indicate that conditioning with new material (i. e. HS for the unseen targets) induces GPT-2 to produce new arguments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 93,
            "end": 232,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@2",
            "content": "On the other hand, as expected, the overlap scores for LOTO are remarkably lower than those from the previous experiments (Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 234,
            "end": 365,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@3",
            "content": "Therefore, we can infer that generalizing to an unseen target is harder than generalizing to an unseen HS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 367,
            "end": 472,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@4",
            "content": "We also found out that the CNs generated in the LM -MUSLIMS and LM -WOMEN configurations obtain the highest overlap scores (Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 474,
            "end": 606,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@5",
            "content": "We hypothesize that the high scores can be explained by the presence of a target in the LOTO training that is highly similar to the left out one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 608,
            "end": 752,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@6",
            "content": "To this end, we computed the novelty between each target subset of the training data and the LOTO test data for that configuration (see Appendix A.4 for details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 754,
            "end": 915,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@7",
            "content": "The reference CNs for LM -MUSLIMS record the lowest novelty scores with respect to the JEWS subset of the training set (i. e. 0.761).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 917,
            "end": 1049,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@8",
            "content": "Thus, it can be interpreted as the most influential portion of training data for the target MUSLIMS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 1051,
            "end": 1150,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@9",
            "content": "On the other hand, for LM -WOMEN the highest influence is recorded with the LGBT+ subset of the training data (i. e. 0.763).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 1152,
            "end": 1275,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_48@10",
            "content": "These results can be explained by the semantic similarity of the target MUSLIMS to JEWS, both being religious groups; and of WOMEN to LGBT+, both being related to gender issues.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_48",
            "start": 1277,
            "end": 1453,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_49@0",
            "content": "As a complementary analysis, we consider two different computations of the reference CN novelty: with respect to the most influential target for each LOTO configuration, and with respect to the LOTO training data without the most influential target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_49",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_49@1",
            "content": "We computed the Pearson correlation between the overlap metrics and each of the two novelty computations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_49",
            "start": 250,
            "end": 354,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_49@2",
            "content": "In Figure 2, we observe that removing the influential target from the training data strongly decreases the correlation with the overlap metrics (from an average of -0.889 to -0.416).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_49",
            "start": 356,
            "end": 537,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_49@3",
            "content": "Consequently, we can conclude that to obtain high overlap results in the LOTO experiments, it is necessary that the training data contains a target strongly connected to the left out one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_49",
            "start": 539,
            "end": 725,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_49@4",
            "content": "Most importantly, this connection is not arbitrarily decided but it is based on an a-priori semantic similarity of the targets as exemplified before.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_49",
            "start": 727,
            "end": 875,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_50@0",
            "content": "Finally, we chose to generate also with the BS decoding mechanism, to use it as a baseline and compare it to the stochastic decoding mechanism (Top-k).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_50",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_50@1",
            "content": "In particular, we computed the Pearson correlation between the novelty of the reference CNs and the novelty of the candidate CNs with respect to the corresponding training data (Figure 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_50",
            "start": 152,
            "end": 339,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_50@2",
            "content": "We can observe that for the BS generation the novelty of the candidate CNs is lower than Top-k (0.67-0.74 vs. 0.75-0.77) and the correlation with the novelty of the reference is weaker (0.53 vs. 0.59).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_50",
            "start": 341,
            "end": 541,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_50@3",
            "content": "This confirms the lower generalization ability with the deterministic decoding mechanism (as compared to the stochastic) that tends to produce generic and repetitive responses regardless of the semantic distances of the LOTO targets from the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_50",
            "start": 543,
            "end": 798,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_51@0",
            "content": "Automatic Post-Editing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_51",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_52@0",
            "content": "In the previous experiments we fine-tuned our models making resort to <HS, CN > pairs alone.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_52",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_52@1",
            "content": "Still the Fanton et al. ( 2021) dataset contains additional information that can be useful for our task: i. e. the original GPT-2 generation before undergoing human post-editing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_52",
            "start": 93,
            "end": 270,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_53@0",
            "content": "Thus, as a final experiment, we propose to further improve the CN generation by moving from an end-to-end framework to a two stage pipeline, by decoupling CN generation from its 'final refinement'.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_53",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_53@1",
            "content": "In particular we propose the adoption of an Automatic Post-Editing (APE) stage in order to capture and utilize the nuances among the machine generated CNs and their human post-edited versions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_53",
            "start": 198,
            "end": 389,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_53@2",
            "content": "APE, which is used for automatically correcting errors made by machine translation (MT) systems before performing actual human post-editing, has been an important tool for MT (Knight and Chander, 1994;do Carmo et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_53",
            "start": 391,
            "end": 614,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_53@3",
            "content": "Considering its effectiveness in MT, we hypothesize that building a pipeline with CN generation and APE could alleviate the requirement of the final manual post-editing (Allen and Hogan, 2000;Chatterjee et al., 2019) to achieve better constructed CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_53",
            "start": 616,
            "end": 866,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_54@0",
            "content": "To this end, we fine-tuned another instance of GPT-2 medium model specifically for the post-editing task using the <HS, CN or , CN pe > triplets 5 , where CN or and CN pe denote the CNs originally generated by an LM and their human post-edited versions, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_54",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_54@1",
            "content": "The triplets were then filtered by removing those for which CN or = CN pe .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_54",
            "start": 268,
            "end": 342,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_54@2",
            "content": "More details about the experiment settings can be found in Appendix A.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_54",
            "start": 344,
            "end": 415,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_54@3",
            "content": "We have conducted two human evaluations over the subsets of: i) the CN or of the Fanton et al. ( 2021) test samples, ii) the CN outputs of the best model and decoding mechanism combination provided as the results of the first set of experiments, that yielded the top 50 Translation Error Rate (TER) (Snover et al., 2006) scores with respect to the CN or s. The two expert annotators were asked to state their preferences among the 2 randomly sorted CNs, CN or and CN ape (automatically post-edited output), for a given HS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_54",
            "start": 417,
            "end": 938,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_54@4",
            "content": "The annotators were also allowed to decide on a tie.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_54",
            "start": 940,
            "end": 991,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_54@5",
            "content": "Results, shown in Table 4, indicate that, albeit there are often ties and only a subset of CN or is actually modified, when there is a preference, it is predominantly in favour of the automatically post-edited versions over the GPT-2 generated CNs (26% vs. 14% for the test set, and 37% vs. 19% for the GPT-2 Top k generations, on average).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_54",
            "start": 993,
            "end": 1332,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_54@6",
            "content": "Regarding the experiment results, we believe that APE is a highly promising direction to increase the efficacy of the CN generation models where generation quality and diversity is crucial, and considering that obtaining/enlarging expert datasets to train better models is not simple.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_54",
            "start": 1334,
            "end": 1617,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_55@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_55",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_56@0",
            "content": "In this work, we focus on automatic CN generation as a downstream task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_56",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_56@1",
            "content": "First, we present a comparative study to determine the performances and peculiarities of several pre-trained LMs and decoding mechanisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_56",
            "start": 72,
            "end": 208,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_56@2",
            "content": "We observe that the best results (in term of novelty and specificity) overall are achieved by the autoregressive models with stochastic decoding: GPT-2 with the Top k decoding mechanism, and DialoGPT with the combination Top pk .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_56",
            "start": 210,
            "end": 438,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_56@3",
            "content": "At the same time deterministic decoding can be used when more generic yet 'safer' CNs are preferred.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_56",
            "start": 440,
            "end": 539,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_57@0",
            "content": "Then, we investigate the performances of LMs in zero-shot generation for unseen targets of hate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_57",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_57@1",
            "content": "Hence, we fine-tuned 5 different versions of GPT-2, leaving out the examples pertaining to one target at each turn.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_57",
            "start": 97,
            "end": 211,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_57@2",
            "content": "We find out that for each configuration/version, there is a subset of the training data which is more influential with respect to the generated data (i. e. a target that shares some commonalities with the test target that can be defined a-priori).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_57",
            "start": 213,
            "end": 459,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_57@3",
            "content": "Finally, we introduce an experiment by training an automatic post-editing module to further improve the CN generation quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_57",
            "start": 461,
            "end": 586,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_57@4",
            "content": "The notable human evaluation results paves the way for a future direction that decouples CN generation from its 'final refinement'.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_57",
            "start": 588,
            "end": 718,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_58@0",
            "content": "Although tackling online hatred through CNs inherently protects the freedom of speech and has been proposed as a better alternative to the detectremove-ban approaches, automatization of CN generation can still raise some ethical concerns and some measures must be taken to avoid undesired effects during research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_58",
            "start": 0,
            "end": 312,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_58@1",
            "content": "Thus, we address the relevant ethical considerations and our remedies as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_58",
            "start": 314,
            "end": 394,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_59@0",
            "content": "Annotation Guidelines: The well-being of the annotators was our top priority during the whole study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_59",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_59@1",
            "content": "Therefore, we strictly followed the guidelines created for CN studies (Fanton et al., 2021) that were adapted from (Vidgen et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_59",
            "start": 101,
            "end": 237,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_59@2",
            "content": "The human evaluations have been conducted with the help of two expert annotators in CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_59",
            "start": 239,
            "end": 326,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_59@3",
            "content": "These experts were already trained for the CN generation task and employed for the work presented by (Fanton et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_59",
            "start": 328,
            "end": 450,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_59@4",
            "content": "We further instructed them in the aims of each experiment, clearly explained the evaluation tasks, and then we exemplified proper evaluation of <HS, CN > pairs using various types of CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_59",
            "start": 452,
            "end": 638,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_59@5",
            "content": "Most importantly, we limited the exposure to hateful content by providing a daily time limit of annotation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_59",
            "start": 640,
            "end": 746,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_59@6",
            "content": "Concerning the demographics, due to the harmful content that can be found in the data, all annotators were adult volunteers, perfectly aware of the objective of the study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_59",
            "start": 748,
            "end": 918,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_60@0",
            "content": "Dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_60",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_60@1",
            "content": "We purposefully chose an expert-based dataset in order to avoid the risk of modeling the language of real individuals to (i) prevent any privacy issue, (ii) avoid to model inappropriate CNs (e.g. containing abusive language) that could be produced by non-experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_60",
            "start": 9,
            "end": 271,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_60@2",
            "content": "The dataset also focuses on the CN diversity while keeping the HSs as stereotypical as possible so that our CN generation models have a very limited diversity on the hateful language, nearly precluding the misuse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_60",
            "start": 273,
            "end": 485,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_61@0",
            "content": "Computational Task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_61",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_61@1",
            "content": "CN generation models are not meant to be used in an autonomous way, since even the best models can still produce substandard CNs containing inappropriate or negative language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_61",
            "start": 20,
            "end": 194,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_61@2",
            "content": "Instead, following a Human-computer cooperation paradigm, our focus is on building models that can be helpful to NGO operators by providing them diverse and novel CN candidates for their hate countering activities and speed up the manual CN writing to a certain extent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_61",
            "start": 196,
            "end": 464,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_61@3",
            "content": "This approach also gives ground to some of the measures we used during evaluation (namely choose-or-not and is-best).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_61",
            "start": 466,
            "end": 582,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_62@0",
            "content": "Model Distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_62",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_62@1",
            "content": "In addition to the limited and simplified hateful content in the dataset we selected, we further reduce the risk of misuse by choosing a specific distribution strategy: i) we only make available the non-autoregressive models in order to eliminate the risk of using over-generation for hate speech creation, ii) we distribute such models only for research purposes and through a request based procedure in order to keep track of the possible users.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_62",
            "start": 20,
            "end": 466,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_63@0",
            "content": "conference of the european chapter of the association for computational linguistics, pages 313-320.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_63",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_64@0",
            "content": "A.1 Fine-tuning details Since LM sizes are very different for each model and since our main focus is not studying performances according to LM dimension growth, as a rule-of-thumb, we chose one version smaller than the large version of each model provided that they all have the same order of magnitude.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_64",
            "start": 0,
            "end": 302,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_64@1",
            "content": "This corresponds to the medium versions for both DialoGPT and GPT-2, and base versions for the other models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_64",
            "start": 304,
            "end": 411,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_64@2",
            "content": "GPT-2 and DialoGPT achieve the lowest perplexity, training and evaluation loss, thus indicating a slightly more successful fine-tuning, which are reflected in the evaluations throughout the study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_64",
            "start": 413,
            "end": 608,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_64@3",
            "content": "We conducted a hyper-parameter search during the training phase of each model using the search space: learning-rate:{1e \u2212 5, 2e \u2212 5, 3e \u2212 5, 4e \u2212 5, 5e \u2212 5}, warm-up ratio:{0, 0.1}, batchsize:{2, 4}, epochs:{2, 3, 4, 5}. It has been conducted using Optuna, with 10 trials, optimized on minimizing the evaluation loss during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_64",
            "start": 610,
            "end": 942,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_65@0",
            "content": "Here we discuss the results for the overlap and diversity metrics obtained on the Best LM+D generations (Table 6), and those calculated on the human evaluation subset (Tables 7 and 8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_65",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_66@0",
            "content": "BART.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_66",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_66@1",
            "content": "BART performs well with the stochastic decoding methods, in particular: Top p for overlap, diversity, syntactic metrics, and grammaticality; Top k for overlap metrics and toxicity, whereas Top pk is the best decoding approach on human evaluation and RR, and the second best on ROUGE and BLEU-1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_66",
            "start": 6,
            "end": 299,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_66@2",
            "content": "On the contrary, BART does not achieve good results with deterministic approaches (i. e. BS).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_66",
            "start": 301,
            "end": 393,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_66@3",
            "content": "T5 -Top p : The vast majority of Muslims in the UK are peaceful people who share many values and respect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_66",
            "start": 395,
            "end": 499,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_67@0",
            "content": "GPT-2 -Top p : This is not true, the vast majority of Muslims is peaceful and peaceful Muslims are happy to live in any country they wish.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_67",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_67@1",
            "content": "They do not need to be a part of a hostile state and they should be treated as equals in society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_67",
            "start": 139,
            "end": 235,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_68@0",
            "content": "In these examples we can observe that the CN obtained with GPT-2, albeit not perfect, is the most complete and accurate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_68",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_68@1",
            "content": "Those generated with T5 and DialoGPT are suitable as well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_68",
            "start": 121,
            "end": 178,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_68@2",
            "content": "On the other hand, the CNs obtained with BART and BERT present some problems: the former has a clear contradiction in it and the latter is illustrative of some of the problems detected in BERT's generations such that it is syntactically complex but very vague and repetitive, and it ends up not really conveying a clear message.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_68",
            "start": 180,
            "end": 507,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_69@0",
            "content": "Hate speech: The migrant crisis is not the fault of Western Europeans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_69",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_69@1",
            "content": "Migrants are to blame for immigration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_69",
            "start": 71,
            "end": 108,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_70@0",
            "content": "DialoGPT -Top k : What if an entire religion was to blame only for the migration crisis? So you think a large number of Muslims are responsible for all the problems of our world?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_70",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_71@0",
            "content": "BART -Top p : The migrant crisis is caused by a lack of integration and lack of knowledge of the world.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_71",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_72@0",
            "content": "BERT -BS: many migrants come here to work and provide for their families.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_72",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_72@1",
            "content": "they are our brothers and sisters, and we should welcome them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_72",
            "start": 74,
            "end": 135,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_72@2",
            "content": "Once again, GPT-2 and DialoGPT generations where chosen as the best CNs by the annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_72",
            "start": 137,
            "end": 226,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_72@3",
            "content": "Moreover, these CNs are all generated via Top k decoding mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_72",
            "start": 228,
            "end": 294,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_72@4",
            "content": "BART and T5 generated CNs are acceptable while requiring some post-editing to be employed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_72",
            "start": 296,
            "end": 385,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_72@5",
            "content": "Finally, BERT's output is still very vague: we can notice the presence of the recurring expression our brothers and sisters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_72",
            "start": 387,
            "end": 510,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_73@0",
            "content": "Table 9 displays the distribution of the examples with respect to the targets, in the reference dataset and in the configurations for the LOTO experiments (Section 5.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_73",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_73@1",
            "content": "Table 10 presents the detailed results for the novelty of the reference CNs discussed in Section 5.3, while the RR for the CNs generated with the LOTO models and for the reference CNs are shown in Table 11.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_73",
            "start": 170,
            "end": 375,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_73@2",
            "content": "The rankings for these two RR computations are the same, and the ranges are almost overlapping.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_73",
            "start": 377,
            "end": 471,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_73@3",
            "content": "This means that leaving one target out does not impact the intra-corpora repetitiveness: instead, the CNs generated with a LOTO model gain a lower RR than the reference CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_73",
            "start": 473,
            "end": 645,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_73@4",
            "content": "For the target MUSLIMS a high RR is recorded, both in candidate and in the reference CNs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_73",
            "start": 647,
            "end": 735,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_73@5",
            "content": "A high repetitiveness in the data for this target can contribute to the good results observed on overlap metrics too (Table 3 For fine-tuning our APE model, we have thus used the triplets <HS, CN or , CN pe > and <HS, CN pe * , CN pe >.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_73",
            "start": 737,
            "end": 972,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_73@6",
            "content": "In this way, we managed to roughly double the number of the post-edit training samples, which is highly beneficial for a better model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_73",
            "start": 974,
            "end": 1107,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_73@7",
            "content": "When we filtered the triplets with a positive TER score between CN ed and CN pe , or CN or and CN pe , we obtained 4185 training, 596 test, and 568 validation samples following the partition used in the first set of experiments as described in Section 3.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_73",
            "start": 1109,
            "end": 1364,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_73@8",
            "content": "Finally, the best fine-tuning configuration of the GPT-2 medium model for APE was obtained with a learning rate of 2e-5 for 3 epochs resulting in 3.34 train loss and 1.23 eval loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_73",
            "start": 1366,
            "end": 1546,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_74@0",
            "content": "Jeffrey Allen, Christopher Hogan, Toward the development of a post editing module for raw machine translation output: A controlled language perspective, 2000, Third International Controlled Language Applications Workshop (CLAW-00), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_74",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_75@0",
            "content": "R Kimberley, Kay Allison,  Bussey, Cyberbystanding in context: A review of the literature on witnesses' responses to cyberbullying, 2016, Children and Youth Services Review, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_75",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_76@0",
            "content": "Jenn Anderson, Mary Bresnahan, Catherine Musatics, Combating weight-based cyberbullying on facebook with the dissenter effect, 2014, Cyberpsychology, Behavior, and Social Networking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_76",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_77@0",
            "content": "UNKNOWN, None, 2006, Comparing automatic and human evaluation of nlg systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_77",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_78@0",
            "content": "UNKNOWN, None, 2015, Detection of cyberbullying incidents on the instagram social network, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_78",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_79@0",
            "content": "Svetlana Kiritchenko, Isar Nejadgholi, Kathleen C Fraser, Confronting abusive language online: A survey from the ethical and human rights perspective, 2021, Journal of Artificial Intelligence Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_79",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_80@0",
            "content": "Kevin Knight, Ishwar Chander, Automated postediting of documents, 1994, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_80",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_81@0",
            "content": "Ritesh Kumar, Atul Kr Ojha, Shervin Malmasi, Marcos Zampieri, Benchmarking aggression identification in social media, 2018, Proceedings of the First Workshop on Trolling, Aggression and Cyberbullying (TRAC-2018), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_81",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_82@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_82",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_83@0",
            "content": "Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, Jianfeng Gao, Deep reinforcement learning for dialogue generation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_83",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_84@0",
            "content": "Chin-Yew Lin, Rouge: A package for automatic evaluation of summaries, 2004, Text summarization branches out, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_84",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_85@0",
            "content": "UNKNOWN, None, 2018, Analyzing the hate and counter speech accounts on twitter, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_85",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_86@0",
            "content": "Binny Mathew, Punyajoy Saha, Hardik Tharad, Subham Rajgaria, Prajwal Singhania, Pawan Suman Kalyan Maity, Animesh Goyal,  Mukherjee, Thou shalt not hate: Countering online hate speech, 2019, Proceedings of the International AAAI Conference on Web and Social Media, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_86",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_87@0",
            "content": "Binny Mathew, Punyajoy Saha, Chris Seid Muhie Yimam, Pawan Biemann, Animesh Goyal,  Mukherjee, Hatexplain: A benchmark dataset for explainable hate speech detection, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_87",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_88@0",
            "content": "Kevin Munger, Tweetment effects on the tweeted: Experimentally reducing racist harassment, 2017, Political Behavior, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_88",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_89@0",
            "content": "Sarah Myers West, Censored, suspended, shadowbanned: User interpretations of content moderation on social media platforms, 2018, New Media & Society, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_89",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_90@0",
            "content": "Jekaterina Novikova, Ondrej Dusek, Amanda Curry, Verena Rieser, Why we need new evaluation metrics for nlg, 2017, 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_90",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_91@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th annual meeting on association for computational linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_91",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_92@0",
            "content": "UNKNOWN, None, 2020, Resources and benchmark corpora for hate speech detection: a systematic review. Language Resources and Evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_92",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_93@0",
            "content": "Jing Qian, Anna Bethke, Yinyin Liu, Elizabeth Belding, William Wang, A benchmark dataset for learning to intervene in online hate speech, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_93",
            "start": 0,
            "end": 362,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_94@0",
            "content": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, Language models are unsupervised multitask learners, 2019, OpenAI Blog, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_94",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_95@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_95",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_96@0",
            "content": "Sascha Rothe, Shashi Narayan, Aliaksei Severyn, Leveraging pre-trained checkpoints for sequence generation tasks, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_96",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_97@0",
            "content": "Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, Noah A Smith, The risk of racial bias in hate speech detection, 2019, Proceedings of the 57th annual meeting of the association for computational linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_97",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_98@0",
            "content": "Carla Schieb, Mike Preuss, Governing hate speech by means of counterspeech on facebook, 2016, 66th ICA Annual Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_98",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_99@0",
            "content": "Anna Schmidt, Michael Wiegand, A survey on hate speech detection using natural language processing, 2017, Proceedings of the Fifth International Workshop on Natural Language Processing for Social Media, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_99",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_100@0",
            "content": "UNKNOWN, None, 2016, The impact of counter-narratives. Institute for Strategic Dialogue, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_100",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_101@0",
            "content": "Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, John Makhoul, A study of translation edit rate with targeted human annotation, 2006, Proceedings of association for machine translation in the Americas, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_101",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_102@0",
            "content": "Yi-Ling Serra Sinem Tekiroglu, Marco Chung,  Guerini, Generating counter narratives against online hate speech: Data and strategies, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_102",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_103@0",
            "content": ", Demoting racial bias in hate speech detection, , Mengzhou Xia Anjalie Field Yulia Tsvetkov. 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_103",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_104@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_104",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_105@0",
            "content": "Bertie Vidgen, Leon Derczynski, Directions in abusive language training data, a systematic review: Garbage in, garbage out, 2020, Plos one, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_105",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_106@0",
            "content": "Bertie Vidgen, Alex Harris, Dong Nguyen, Rebekah Tromble, Scott Hale, Helen Margetts, Challenges and frontiers in abusive content detection, 2019, Proceedings of the third workshop on abusive language online, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_106",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_107@0",
            "content": "UNKNOWN, None, 2020, Learning from the worst: Dynamically generated datasets to improve online hate detection, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_107",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_108@0",
            "content": "Eric Wallace, Pedro Rodriguez, Shi Feng, Ikuya Yamada, Jordan Boyd-Graber, Trick me if you can: Human-in-the-loop generation of adversarial question answering examples, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_108",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_109@0",
            "content": "Alex Wang, Kyunghyun Cho, BERT has a mouth, and it must speak: BERT as a Markov random field language model, 2019, Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_109",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_110@0",
            "content": "Ke Wang, Xiaojun Wan, Sentigan: Generating sentimental texts via mixture adversarial networks, 2018, IJCAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_110",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_111@0",
            "content": "Zeerak Waseem, Are you a racist or am i seeing things? annotator influence on hate speech detection on twitter, 2016, Proceedings of the first workshop on NLP and computational social science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_111",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_112@0",
            "content": "Sam Wiseman, Stuart Shieber, Alexander Rush, Challenges in data-to-document generation, 2017, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_112",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_113@0",
            "content": "UNKNOWN, None, , Jason Weston, and Emily Dinan. 2020. Recipes for safety in open-domain chatbots. arXiv e-prints, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_113",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_114@0",
            "content": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan, DIALOGPT : Largescale generative pre-training for conversational response generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_114",
            "start": 0,
            "end": 324,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_115@0",
            "content": "Wanzheng Zhu, Suma Bhat, 2021. Generate, prune, select: A pipeline for counterspeech generation against online hate speech, , Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_115",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_116@0",
            "content": "UNKNOWN, None, , Examples of the generations Hate speech: Islam is a violent religion that promotes terror and war, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_116",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_117@0",
            "content": "UNKNOWN, None, , The vast majority of Muslims in the world are peaceful and friendly people. I don't see how you can say such a thing about them, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_117",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_118@0",
            "content": "UNKNOWN, None, , The vast majority of Muslims are peaceful people who have lived in peace and harmony for many years. How can you say this about a religion that promotes terror and war? BERT -Top k : islam is a religion of peace, and it is not a peaceful religion of violence. it is about peace and understanding of the world's problems, and peace is about bringing peace and peace, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_118",
            "start": 0,
            "end": 383,
            "label": {}
        },
        {
            "ix": "49-ARR_v2_119@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "49-ARR_v2_119",
            "start": 0,
            "end": 19,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "49-ARR_v2_0",
            "tgt_ix": "49-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_0",
            "tgt_ix": "49-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_1",
            "tgt_ix": "49-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_1",
            "tgt_ix": "49-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_0",
            "tgt_ix": "49-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_2",
            "tgt_ix": "49-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_3",
            "tgt_ix": "49-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_3",
            "tgt_ix": "49-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_3",
            "tgt_ix": "49-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_0",
            "tgt_ix": "49-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_5",
            "tgt_ix": "49-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_7",
            "tgt_ix": "49-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_6",
            "tgt_ix": "49-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_6",
            "tgt_ix": "49-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_6",
            "tgt_ix": "49-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_6",
            "tgt_ix": "49-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_8",
            "tgt_ix": "49-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_0",
            "tgt_ix": "49-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_10",
            "tgt_ix": "49-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_10",
            "tgt_ix": "49-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_10",
            "tgt_ix": "49-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_11",
            "tgt_ix": "49-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_12",
            "tgt_ix": "49-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_12",
            "tgt_ix": "49-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_10",
            "tgt_ix": "49-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_13",
            "tgt_ix": "49-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_14",
            "tgt_ix": "49-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_14",
            "tgt_ix": "49-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_14",
            "tgt_ix": "49-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_15",
            "tgt_ix": "49-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_10",
            "tgt_ix": "49-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_16",
            "tgt_ix": "49-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_17",
            "tgt_ix": "49-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_17",
            "tgt_ix": "49-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_17",
            "tgt_ix": "49-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_0",
            "tgt_ix": "49-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_19",
            "tgt_ix": "49-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_21",
            "tgt_ix": "49-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_20",
            "tgt_ix": "49-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_20",
            "tgt_ix": "49-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_20",
            "tgt_ix": "49-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_20",
            "tgt_ix": "49-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_0",
            "tgt_ix": "49-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_24",
            "tgt_ix": "49-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_24",
            "tgt_ix": "49-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_24",
            "tgt_ix": "49-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_25",
            "tgt_ix": "49-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_26",
            "tgt_ix": "49-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_26",
            "tgt_ix": "49-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_26",
            "tgt_ix": "49-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_27",
            "tgt_ix": "49-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_24",
            "tgt_ix": "49-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_31",
            "tgt_ix": "49-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_32",
            "tgt_ix": "49-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_33",
            "tgt_ix": "49-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_34",
            "tgt_ix": "49-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_35",
            "tgt_ix": "49-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_36",
            "tgt_ix": "49-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_37",
            "tgt_ix": "49-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_38",
            "tgt_ix": "49-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_39",
            "tgt_ix": "49-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_40",
            "tgt_ix": "49-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_41",
            "tgt_ix": "49-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_30",
            "tgt_ix": "49-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_24",
            "tgt_ix": "49-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_42",
            "tgt_ix": "49-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_44",
            "tgt_ix": "49-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_45",
            "tgt_ix": "49-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_43",
            "tgt_ix": "49-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_43",
            "tgt_ix": "49-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_43",
            "tgt_ix": "49-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_43",
            "tgt_ix": "49-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_47",
            "tgt_ix": "49-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_49",
            "tgt_ix": "49-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_43",
            "tgt_ix": "49-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_43",
            "tgt_ix": "49-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_43",
            "tgt_ix": "49-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_43",
            "tgt_ix": "49-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_46",
            "tgt_ix": "49-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_0",
            "tgt_ix": "49-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_50",
            "tgt_ix": "49-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_52",
            "tgt_ix": "49-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_53",
            "tgt_ix": "49-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_51",
            "tgt_ix": "49-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_51",
            "tgt_ix": "49-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_51",
            "tgt_ix": "49-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_51",
            "tgt_ix": "49-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_0",
            "tgt_ix": "49-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_54",
            "tgt_ix": "49-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_56",
            "tgt_ix": "49-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_58",
            "tgt_ix": "49-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_59",
            "tgt_ix": "49-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_60",
            "tgt_ix": "49-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_61",
            "tgt_ix": "49-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_62",
            "tgt_ix": "49-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_57",
            "tgt_ix": "49-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_63",
            "tgt_ix": "49-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_65",
            "tgt_ix": "49-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_66",
            "tgt_ix": "49-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_67",
            "tgt_ix": "49-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_68",
            "tgt_ix": "49-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_69",
            "tgt_ix": "49-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_70",
            "tgt_ix": "49-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_71",
            "tgt_ix": "49-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_64",
            "tgt_ix": "49-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_72",
            "tgt_ix": "49-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "49-ARR_v2_0",
            "tgt_ix": "49-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_1",
            "tgt_ix": "49-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_2",
            "tgt_ix": "49-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_2",
            "tgt_ix": "49-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_2",
            "tgt_ix": "49-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_2",
            "tgt_ix": "49-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_2",
            "tgt_ix": "49-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_2",
            "tgt_ix": "49-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_3",
            "tgt_ix": "49-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_4",
            "tgt_ix": "49-ARR_v2_4@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_5",
            "tgt_ix": "49-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_5",
            "tgt_ix": "49-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_5",
            "tgt_ix": "49-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_5",
            "tgt_ix": "49-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_5",
            "tgt_ix": "49-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_5",
            "tgt_ix": "49-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_5",
            "tgt_ix": "49-ARR_v2_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_5",
            "tgt_ix": "49-ARR_v2_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_5",
            "tgt_ix": "49-ARR_v2_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_6",
            "tgt_ix": "49-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_7",
            "tgt_ix": "49-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_8",
            "tgt_ix": "49-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_8",
            "tgt_ix": "49-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_8",
            "tgt_ix": "49-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_8",
            "tgt_ix": "49-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_8",
            "tgt_ix": "49-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_8",
            "tgt_ix": "49-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_8",
            "tgt_ix": "49-ARR_v2_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_8",
            "tgt_ix": "49-ARR_v2_8@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_9@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_9@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_9",
            "tgt_ix": "49-ARR_v2_9@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_10",
            "tgt_ix": "49-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_11",
            "tgt_ix": "49-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_12",
            "tgt_ix": "49-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_13",
            "tgt_ix": "49-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_13",
            "tgt_ix": "49-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_13",
            "tgt_ix": "49-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_13",
            "tgt_ix": "49-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_13",
            "tgt_ix": "49-ARR_v2_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_14",
            "tgt_ix": "49-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_15",
            "tgt_ix": "49-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_15",
            "tgt_ix": "49-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_15",
            "tgt_ix": "49-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_15",
            "tgt_ix": "49-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_15",
            "tgt_ix": "49-ARR_v2_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_15",
            "tgt_ix": "49-ARR_v2_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_15",
            "tgt_ix": "49-ARR_v2_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_15",
            "tgt_ix": "49-ARR_v2_15@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_15",
            "tgt_ix": "49-ARR_v2_15@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_16",
            "tgt_ix": "49-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_16",
            "tgt_ix": "49-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_16",
            "tgt_ix": "49-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_17",
            "tgt_ix": "49-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_18@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_18@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_18@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_18@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_18@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_18",
            "tgt_ix": "49-ARR_v2_18@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_19",
            "tgt_ix": "49-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_20",
            "tgt_ix": "49-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_21",
            "tgt_ix": "49-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_21",
            "tgt_ix": "49-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_21",
            "tgt_ix": "49-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_21",
            "tgt_ix": "49-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_22@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_22@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_22@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_22@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_22",
            "tgt_ix": "49-ARR_v2_22@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_23@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_23@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_23@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_23",
            "tgt_ix": "49-ARR_v2_23@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_24",
            "tgt_ix": "49-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_25",
            "tgt_ix": "49-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_25",
            "tgt_ix": "49-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_26",
            "tgt_ix": "49-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_27",
            "tgt_ix": "49-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_27",
            "tgt_ix": "49-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_27",
            "tgt_ix": "49-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_28",
            "tgt_ix": "49-ARR_v2_28@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_29",
            "tgt_ix": "49-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_30",
            "tgt_ix": "49-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_31",
            "tgt_ix": "49-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_31",
            "tgt_ix": "49-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_31",
            "tgt_ix": "49-ARR_v2_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_32",
            "tgt_ix": "49-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_32",
            "tgt_ix": "49-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_33",
            "tgt_ix": "49-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_33",
            "tgt_ix": "49-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_33",
            "tgt_ix": "49-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_33",
            "tgt_ix": "49-ARR_v2_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_33",
            "tgt_ix": "49-ARR_v2_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_34",
            "tgt_ix": "49-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_34",
            "tgt_ix": "49-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_34",
            "tgt_ix": "49-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_35",
            "tgt_ix": "49-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_35",
            "tgt_ix": "49-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_35",
            "tgt_ix": "49-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_36",
            "tgt_ix": "49-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_36",
            "tgt_ix": "49-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_36",
            "tgt_ix": "49-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_36",
            "tgt_ix": "49-ARR_v2_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_37",
            "tgt_ix": "49-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_37",
            "tgt_ix": "49-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_37",
            "tgt_ix": "49-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_37",
            "tgt_ix": "49-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_38",
            "tgt_ix": "49-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_38",
            "tgt_ix": "49-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_38",
            "tgt_ix": "49-ARR_v2_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_39",
            "tgt_ix": "49-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_39",
            "tgt_ix": "49-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_39",
            "tgt_ix": "49-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_39",
            "tgt_ix": "49-ARR_v2_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_39",
            "tgt_ix": "49-ARR_v2_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_39",
            "tgt_ix": "49-ARR_v2_39@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_39",
            "tgt_ix": "49-ARR_v2_39@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_39",
            "tgt_ix": "49-ARR_v2_39@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_40",
            "tgt_ix": "49-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_40",
            "tgt_ix": "49-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_40",
            "tgt_ix": "49-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_41",
            "tgt_ix": "49-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_41",
            "tgt_ix": "49-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_41",
            "tgt_ix": "49-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_42",
            "tgt_ix": "49-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_43",
            "tgt_ix": "49-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_44",
            "tgt_ix": "49-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_44",
            "tgt_ix": "49-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_45",
            "tgt_ix": "49-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_45",
            "tgt_ix": "49-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_45",
            "tgt_ix": "49-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_45",
            "tgt_ix": "49-ARR_v2_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_45",
            "tgt_ix": "49-ARR_v2_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_45",
            "tgt_ix": "49-ARR_v2_45@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_45",
            "tgt_ix": "49-ARR_v2_45@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_46",
            "tgt_ix": "49-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_46",
            "tgt_ix": "49-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_46",
            "tgt_ix": "49-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_46",
            "tgt_ix": "49-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_46",
            "tgt_ix": "49-ARR_v2_46@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_46",
            "tgt_ix": "49-ARR_v2_46@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_46",
            "tgt_ix": "49-ARR_v2_46@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_47",
            "tgt_ix": "49-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_47",
            "tgt_ix": "49-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_48",
            "tgt_ix": "49-ARR_v2_48@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_49",
            "tgt_ix": "49-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_49",
            "tgt_ix": "49-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_49",
            "tgt_ix": "49-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_49",
            "tgt_ix": "49-ARR_v2_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_49",
            "tgt_ix": "49-ARR_v2_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_50",
            "tgt_ix": "49-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_50",
            "tgt_ix": "49-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_50",
            "tgt_ix": "49-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_50",
            "tgt_ix": "49-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_51",
            "tgt_ix": "49-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_52",
            "tgt_ix": "49-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_52",
            "tgt_ix": "49-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_53",
            "tgt_ix": "49-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_53",
            "tgt_ix": "49-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_53",
            "tgt_ix": "49-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_53",
            "tgt_ix": "49-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_54",
            "tgt_ix": "49-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_54",
            "tgt_ix": "49-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_54",
            "tgt_ix": "49-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_54",
            "tgt_ix": "49-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_54",
            "tgt_ix": "49-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_54",
            "tgt_ix": "49-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_54",
            "tgt_ix": "49-ARR_v2_54@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_55",
            "tgt_ix": "49-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_56",
            "tgt_ix": "49-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_56",
            "tgt_ix": "49-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_56",
            "tgt_ix": "49-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_56",
            "tgt_ix": "49-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_57",
            "tgt_ix": "49-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_57",
            "tgt_ix": "49-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_57",
            "tgt_ix": "49-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_57",
            "tgt_ix": "49-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_57",
            "tgt_ix": "49-ARR_v2_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_58",
            "tgt_ix": "49-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_58",
            "tgt_ix": "49-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_59",
            "tgt_ix": "49-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_59",
            "tgt_ix": "49-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_59",
            "tgt_ix": "49-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_59",
            "tgt_ix": "49-ARR_v2_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_59",
            "tgt_ix": "49-ARR_v2_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_59",
            "tgt_ix": "49-ARR_v2_59@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_59",
            "tgt_ix": "49-ARR_v2_59@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_60",
            "tgt_ix": "49-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_60",
            "tgt_ix": "49-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_60",
            "tgt_ix": "49-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_61",
            "tgt_ix": "49-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_61",
            "tgt_ix": "49-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_61",
            "tgt_ix": "49-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_61",
            "tgt_ix": "49-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_62",
            "tgt_ix": "49-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_62",
            "tgt_ix": "49-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_63",
            "tgt_ix": "49-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_64",
            "tgt_ix": "49-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_64",
            "tgt_ix": "49-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_64",
            "tgt_ix": "49-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_64",
            "tgt_ix": "49-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_65",
            "tgt_ix": "49-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_66",
            "tgt_ix": "49-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_66",
            "tgt_ix": "49-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_66",
            "tgt_ix": "49-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_66",
            "tgt_ix": "49-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_67",
            "tgt_ix": "49-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_67",
            "tgt_ix": "49-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_68",
            "tgt_ix": "49-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_68",
            "tgt_ix": "49-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_68",
            "tgt_ix": "49-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_69",
            "tgt_ix": "49-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_69",
            "tgt_ix": "49-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_70",
            "tgt_ix": "49-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_71",
            "tgt_ix": "49-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_72",
            "tgt_ix": "49-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_72",
            "tgt_ix": "49-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_72",
            "tgt_ix": "49-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_72",
            "tgt_ix": "49-ARR_v2_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_72",
            "tgt_ix": "49-ARR_v2_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_72",
            "tgt_ix": "49-ARR_v2_72@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_73",
            "tgt_ix": "49-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_73",
            "tgt_ix": "49-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_73",
            "tgt_ix": "49-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_73",
            "tgt_ix": "49-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_73",
            "tgt_ix": "49-ARR_v2_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_73",
            "tgt_ix": "49-ARR_v2_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_73",
            "tgt_ix": "49-ARR_v2_73@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_73",
            "tgt_ix": "49-ARR_v2_73@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_73",
            "tgt_ix": "49-ARR_v2_73@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_74",
            "tgt_ix": "49-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_75",
            "tgt_ix": "49-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_76",
            "tgt_ix": "49-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_77",
            "tgt_ix": "49-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_78",
            "tgt_ix": "49-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_79",
            "tgt_ix": "49-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_80",
            "tgt_ix": "49-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_81",
            "tgt_ix": "49-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_82",
            "tgt_ix": "49-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_83",
            "tgt_ix": "49-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_84",
            "tgt_ix": "49-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_85",
            "tgt_ix": "49-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_86",
            "tgt_ix": "49-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_87",
            "tgt_ix": "49-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_88",
            "tgt_ix": "49-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_89",
            "tgt_ix": "49-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_90",
            "tgt_ix": "49-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_91",
            "tgt_ix": "49-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_92",
            "tgt_ix": "49-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_93",
            "tgt_ix": "49-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_94",
            "tgt_ix": "49-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_95",
            "tgt_ix": "49-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_96",
            "tgt_ix": "49-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_97",
            "tgt_ix": "49-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_98",
            "tgt_ix": "49-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_99",
            "tgt_ix": "49-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_100",
            "tgt_ix": "49-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_101",
            "tgt_ix": "49-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_102",
            "tgt_ix": "49-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_103",
            "tgt_ix": "49-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_104",
            "tgt_ix": "49-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_105",
            "tgt_ix": "49-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_106",
            "tgt_ix": "49-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_107",
            "tgt_ix": "49-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_108",
            "tgt_ix": "49-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_109",
            "tgt_ix": "49-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_110",
            "tgt_ix": "49-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_111",
            "tgt_ix": "49-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_112",
            "tgt_ix": "49-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_113",
            "tgt_ix": "49-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_114",
            "tgt_ix": "49-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_115",
            "tgt_ix": "49-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_116",
            "tgt_ix": "49-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_117",
            "tgt_ix": "49-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_118",
            "tgt_ix": "49-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "49-ARR_v2_119",
            "tgt_ix": "49-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 977,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "49-ARR",
        "version": 2
    }
}