{
    "nodes": [
        {
            "ix": "246-ARR_v1_0",
            "content": "Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_2",
            "content": "How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by systemlevel correlations. We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect. First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice. We demonstrate how this small change leads to more precise estimates of system-level correlations. Second, we propose to calculate correlations only on pairs of systems which are separated by differences in automatic scores that are commonly used to argue one system is of higher quality. This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios. The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "246-ARR_v1_4",
            "content": "Automatic evaluation metrics are the most common method that researchers use to quickly and cheaply approximate how humans would rate the quality of a summarization system (Lin, 2004;Louis and Nenkova, 2013;Zhao et al., 2019;Zhang et al., 2020;Deutsch et al., 2021a, among others). The quality of a metric -how similarly it replicates human judgments of systems -is quantified by calculating the correlation between the metric's scores and human judgments on a set of systems, known as the system-level correlation (Louis and Nenkova, 2013;Deutsch et al., 2021b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_5",
            "content": "Accurately estimating system-level correlations is critically important. Summarization researchers 1 Our code will be released after publication. use automatic metrics during system development to make decisions about which ideas work and which do not, and systems from different research groups are ranked by automatic metrics to define which system is the \"state-of-the-art.\" If we do not have precise estimates of metric quality, it is not clear how much trust the community should put in such evaluation methodologies.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_6",
            "content": "At present, there are disconnects between how automatic metrics are evaluated and how they are used to evaluate systems. First, the metrics' scores which are used in practice are not the ones which are evaluated in system-level correlations: Researchers compare systems based on metric scores calculated on the entire test set but calculate scores for system-level correlations when evaluating metrics on a much smaller subset of judged summaries. Second, metrics are evaluated in a setting that is much easier than how they are actually used. Metric correlations are calculated using systems that vary greatly in quality, whereas researchers compare new systems to recent work, which are likely to be very close in quality. Discriminating between two systems of similar quality is much harder than doing so between low and high quality systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_7",
            "content": "In this work, we re-examine how system-level correlations are calculated and propose two independent changes to make the evaluation of metrics better aligned to how they are actually used to evaluate systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_8",
            "content": "First, we propose to modify the system-level correlation definition to use the entire test set to calculate the system scores for automatic metrics instead of only the subset of summaries judged by humans ( \u00a73). With this change, the scores which are used to compare systems are directly evaluated, and we further demonstrate how the precision of our estimate of system-level correlations improves as a result. Calculating system scores over a larger number of instances reduces the variance of the scores, which results in confidence intervals (CIs) for the correlations that are 16-51% more narrow on average ( \u00a73.2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_9",
            "content": "Second, we redefine a high quality metric to be one for which a small difference in score reliably indicates a difference in quality ( \u00a74). Then, instead of calculating the correlation with all available system pairs, we only evaluate with pairs of systems whose automatic metric scores differ by some threshold. This allows us to show that a ROUGE-1 score difference of less than 0.5 between systems has almost no correlation to how humans would rank the same two systems according to our best estimates ( \u00a74.2). For two other metrics, BERTScore (Zhang et al., 2020) and QAEval (Deutsch et al., 2021a), we show their correlations calculated on system pairs of similar quality are much worse than under the standard correlation definition. These results cast doubt on how reliable automatic evaluation metrics are for measuring summarization system quality in realistic scenarios.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_10",
            "content": "Our analyses point to the need to collect more high-quality human judgments of summaries in order to have more accurate estimates of metric correlations as well as the need to improve the ability of automatic metrics to discriminate between similarly performing systems.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_11",
            "content": "Background",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "246-ARR_v1_12",
            "content": "Automatic evaluation metrics are most commonly used to argue that one summarization system is better than another, typically by showing that the value of a metric improves with the \"better\" system. How similarly automatic metrics replicate human judgments of system quality is quantified by system-level correlations as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_13",
            "content": "The summaries from N systems on M jud input documents are judged by humans Z and scored with an automatic metric X . Then, the systemlevel correlation between X and Z is calculated as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_14",
            "content": "r SYS = CORR \uf8eb \uf8ec \uf8ed \uf8f1 \uf8f2 \uf8f3 \uf8eb \uf8ed 1 M jud M jud j x j i , 1 M jud M jud j z j i \uf8f6 \uf8f8 \uf8fc \uf8fd \uf8fe N i=1 \uf8f6 \uf8f7 \uf8f8",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_15",
            "content": "where x j i and z j i are the scores of X and Z for the summary produced by the i-th system on the j-th input document and CORR is some correlation function. See Fig. 1 for an illustration of this calculation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_16",
            "content": "In this work, we use Kendall's \u03c4 (the \"b\" vari-Figure 1: The system-level correlation is calculated between the average X and Z scores on a set of summarization systems. x j i and z j i are the scores for the summary produced by system i (represented by rows) on input document j (represented by columns).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_17",
            "content": "ant 2 ) as the correlation function because we are most concerned with a metric's ability to correctly determine whether one system is better than another since that is how metrics are used in practice. Kendall's \u03c4 is computed based on the number of system pairs out of N 2 which are ranked the same by X and Z. It is defined as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_18",
            "content": "\u03c4 = P \u2212 Q (P + Q + T ) \u2022 (P + Q + U )(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_19",
            "content": "where P and Q are the number of pairs ranked the same or different by X and Z, respectively, and T and U are the number of ties only in X or Z, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_20",
            "content": "Because the computation of r SYS involves randomness -its value depends on which M jud input documents (and even which N systems) were used -it is only an approximation of the true correlation between X and Z. As such, Deutsch et al. (2021b) proposed various methods for calculating confidence intervals for r SYS . For instance, their BOOT-INPUTS method uses bootstrapping to repeatedly resample the M jud input documents used to calculate r SYS , thereby calculating a confidence interval for the true r SYS value for X and Z.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_21",
            "content": "The datasets that are used in this paper's analyses are SummEval (Fabbri et al., 2021) and REALSumm (Bhandari et al., 2020), two recently collected datasets with human annotations for summary quality collected from the CNN/DailyMail dataset (Nallapati et al., 2016).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_22",
            "content": "SummEval has M jud = 100 summaries annotated with a summary relevance score for N = 16 systems. REALSumm has M jud = 100 summaries annotated with a Lightweight Pyramid score (Shapira et al., 2019) for N = 25 systems. We correlate the scores of the automatic metrics to these annotations. The CNN/DailyMail test split has 11, 490 instances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_23",
            "content": "Automatic Metrics Our experiments will analyze three different reference-based automatic evaluation metrics which were chosen because they were demonstrated to have the best correlations with human judgments on the SummEval and REALSumm datasets (Deutsch et al., 2021b). ROUGE-n (Lin, 2004) evaluates a generated summary by calculating an F 1 score on the number of ngrams it has in common with a human-written reference summary. BERTScore (Zhang et al., 2020) aligns the generated and reference summaries' tokens based on their BERT embeddings (Devlin et al., 2019) and calculates a score based on the similarity of the aligned tokens' embeddings. QA-Eval (Deutsch et al., 2021a) compares the two summaries by automatically generating questions from the reference and calculating what proportion of those questions are answered correctly by the generated summary.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_24",
            "content": "Evaluating with All Available Instances",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "246-ARR_v1_25",
            "content": "Although the above definition of the system-level correlation has been used by recent meta-evaluation studies of metrics (Bhandari et al., 2020;Fabbri et al., 2021;Deutsch et al., 2021b), there is a disconnect between how the automatic metrics are evaluated and how they are used in practice.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_26",
            "content": "Researchers who develop summarization systems evaluate those systems with automatic metrics on all M test test instances, not just the subset of M jud instances which were judged by humans. Evaluating a system on a larger number of summaries may end up changing the system's score, which could potentially alter the overall ranking of a set of systems. Therefore, the rankings that are used by practitioners to determine system quality are not the ones which are being evaluated in the standard definition of system-level correlation. 3 To that end, we propose to modify the correlation definition to use all M test instances to calculate the system scores for the automatic metrics. That is (differences in bold):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_27",
            "content": "r SYS = CORR \uf8eb \uf8ec \uf8ed \uf8f1 \uf8f2 \uf8f3 \uf8eb \uf8ed 1 M test Mtest j x j i , 1 M jud M jud j z j i \uf8f6 \uf8f8 \uf8fc \uf8fd \uf8fe N i=1 \uf8f6 \uf8f7 \uf8f8",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_28",
            "content": "In practice with modern, large-scale datasets, this minor change could mean estimating system quality based on \u224810k inputs instead of around 100. This new definition now properly evaluates the way metrics are actually used by researchers. We expect that scoring systems with M test inputs instead of M jud should lead to a better estimate of the true automatic metric score, which would in turn result in a lower-variance estimate of the correlation between X and Z in the form of smaller confidence intervals for r SYS . In the next sections, we carry out analyses to demonstrate that this is true.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_29",
            "content": "Reducing Automatic Metric Variance",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "246-ARR_v1_30",
            "content": "First, we empirically show that scoring systems with M test instances instead of M jud does indeed reduce the variance of the estimate of the automatic metric scores and subsequently increases the stabilities of the system rankings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_31",
            "content": "Ideally, the X score for a system would be its \"oracle\" X score, equal to the expected value of X for a document sampled from the latent distribution over documents defined by the dataset (e.g., a system's ROUGE score on an infinite number of examples from a dataset). Since this cannot be calculated, it is approximated by averaging the X score on a sample (i.e., either the M jud or M test input documents). Because M test M jud , we expect that the variance of this estimate using M test inputs should be lower than when using M jud . To quantify this, we calculated the variance of estimating the oracle X score using both M jud and M test input documents via bootstrapping. We randomly sampled M input documents with replacement, recomputed the system scores, and calculated the variance of those scores over 1k iterations. For all three metrics on both datasets, we found around a 99% reduction in the variance when M test inputs were used instead of M jud , clearly demonstrating that evaluating systems with M test inputs results in a better estimate of the system scores. In Fig. 2, this is visualized for BERTScore on the REALSumm dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_32",
            "content": "However, because we are interested in evaluating the metrics' rankings, we also quantify how much of an effect this reduction in variance has on the stability of the system rankings induced by X . Similarly to the system scores, there is an oracle ranking of systems for X , equal to the ordering of systems by their respective oracle X scores (e.g., systems sorted by their ROUGE scores calculated on an infinite number of examples from a dataset). As the variance of the system score estimates decreases, the computed ranking of systems should begin to converge to the oracle X ranking. We aim to understand to what extent this happens if M test instances are used for evaluation instead of M jud .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_33",
            "content": "To quantify this notion, we calculate the Kendall's \u03c4 between two system rankings for X that were based on two sets of M input documents, each sampled with replacement from the set of available documents. This simulates how much the system rankings would change if the evaluation procedure was run twice, each time with M random input documents. This quantity is calculated 1k times for various values of M and plotted in Fig. 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_34",
            "content": "As M approaches M test , the automatic metrics' \u03c4 values approach 1, which is significantly higher than the respective values at M jud , typically around 0.6-0.8. A value near 1 means that the rankings calculated using M test inputs are almost constant, implying the rankings have converged to the oracle ranking. Therefore, the reduction in variance from evaluating on M test instances does indeed greatly stabilize the system rankings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_35",
            "content": "Fig. 3 also contains the same analysis performed for the human judgments Z in both datasets, although it is limited to a maximum of M jud input documents. We see that on both datasets the judgments' rankings are still quite variable, reaching a maximum of around 0.8-0.85 \u03c4 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_36",
            "content": "Confidence Interval Analysis",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "246-ARR_v1_37",
            "content": "Next, we show that the improved estimate of system scores leads to a more precise estimate of r SYS by demonstrating the widths of the confidence intervals for r SYS decrease.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_38",
            "content": "The confidence intervals for r SYS calculated using bootstrapping methods proposed by Deutsch et al. (2021b) are rather wide. For instance, the 95% CI for ROUGE-2 on SummEval is [\u2212.09, .84], demonstrating a rather high level of uncertainty in its value. This is problematic because it means we do not have a good picture of how reliable automatic evaluation metrics are. Reducing the width of the CIs will help us better understand the true metric quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_39",
            "content": "We suspect that the large width of the confidence interval is due to the variance of the system rankings of the automatic metrics and human judgments. The more unstable the rankings are with respect to the M inputs, the larger the variance of the estimate of r SYS should be since very different system rankings would be compared on each bootstrapping iteration. Deutsch et al. (2021b) used M jud input documents to calculate their CIs. Therefore, we expect the improved stability of the automatic metric system rankings from evaluating on M test instances should result in a more narrow confidence interval for r SYS since some noise has been removed from this computation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_40",
            "content": "To demonstrate this, we calculated 95% CIs for r SYS using the BOOT-INPUT method on SummEval and REALSumm using both M jud and M test input documents, shown in Fig. 4. We find that the widths of the CIs shrank on average by 51% on SummEval and 16% on REALSumm. The largest decrease in width is in the ROUGE family of metrics on SummEval, likely because that metric and dataset combination saw the biggest improvement in ranking stability (see Fig. 3). Thus, the improved estimate of the system scores did result in more precise estimates of r SYS . We repeated this analysis using the other bootstrapping methods proposed by Deutsch et al. (2021b), and the results are discussed in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_41",
            "content": "Conclusions & Recommendations",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "246-ARR_v1_42",
            "content": "By estimating system quality using automatic metrics on all available instances instead of only those which were judged, we showed that the variances of the system scores and subsequent rankings reduce significantly, resulting in better estimates of r SYS . Because this methodology additionally directly evaluates the system scores used by researchers, we recommend future work do the same.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_43",
            "content": "In order to continue to improve the estimate of r SYS , as much variance as possible needs to be removed from the system rankings. Evaluating systems using M test instances removed a large amount of variance from the automatic metric rankings, but as demonstrated in Fig. 3, the human judgments still have a large amount of variance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_44",
            "content": "The human rankings' variances can either be reduced by judging more summaries per system or making the judgments more consistent. Since the human rankings' stabilities in Fig. 3 are mostly beginning to plateau -especially for SummEval -it may be prohibitively expensive to collect a sufficient number of judgments to better stabilize the rankings (Wei and Jia, 2021). Therefore, we expect the more feasible solution is to improve the consistency of the human judgments, for example by better training the annotators or improving the annotation protocol.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_45",
            "content": "Evaluating with Realistic System Pairs",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "246-ARR_v1_46",
            "content": "Next, we argue that the set of systems used to evaluate metrics is not reflective of how metrics are used in practice and propose a new system-level correlation variant to address this problem.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_47",
            "content": "Evaluating with All System Pairs",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "246-ARR_v1_48",
            "content": "The N systems which are used for calculating system-level correlations are typically those which participated in a shared task, as in DUC/TAC (Dang and Owczarzak, 2008, among others), or those which have been published in the previous 3-4 years (Bhandari et al., 2020;Fabbri et al., 2021). As such, they are typically rather diverse in terms of their qualities, both as rated by human annotators and automatic metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_49",
            "content": "The system scores of all of the systems in the REALSumm dataset as evaluated by humans and automatic metrics are shown in Fig. 5. Clearly, the scores are rather diverse. For example, the systems cluster into low, medium, and high quality groups (with an additional outlier) as evaluated by ROUGE. A difference of around 5 ROUGE points between them is a rather large gap for ROUGE scores.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_50",
            "content": "The standard definition for a high quality evaluation metric is one which correctly ranks a set of systems with respect to human judgments. As such, the implementation of the system-level correlation calculated with Kendall's \u03c4 will rank all N systems according to the human judgments and an automatic metric, then count how many pairs were ranked the same out of all N 2 pairs (see \u00a72). As a consequence, even pairs of systems which are separated by a large margin according to the automatic metric -likely systems with a clear difference in quality -are included in the evaluation. Therefore, automatic metrics are rewarded for correctly ranking such \"easy\" system pairs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_51",
            "content": "Evaluating with Realistic Pairs",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "246-ARR_v1_52",
            "content": "This standard evaluation setting does not reflect how summarization metrics are actually used by researchers. New systems are typically only slightly better than previous work. Based on a survey of summarization papers in *ACL conferences over the past few years, we found that the average improvement over baseline/state-of-the-art models that was reported on the CNN/Dailymail dataset was on average 0.5 ROUGE-1. It is rarely the case that the improvement in automatic metrics is very large. Therefore, evaluating metrics using pairs of systems which are separated by a large margin does not reflect the reality that metrics are very frequently used to compare those separated by a small margin. Including \"easy\" system pairs in the system-level correlation likely overestimates the quality of the metrics in settings which occur in practice.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_53",
            "content": "To that extent, we redefine a high quality evaluation metric to be one for which a small difference in scores reliably indicates a difference in quality. We quantify this by proposing a variant of the system-level \u03c4 which is calculated between system pairs which are separated by a pre-defined automatic metric score margin. Instead of using all N 2 system pairs, only pairs whose difference in scores falls within the margin are used to calculate the system-level correlation. We denote this correlation variant as r SYS \u2206( , u) where and u are the lower-and upper-bounds of the allowable differences in automatic metrics' scores. This would enable, for example, evaluating how well ROUGE correlates to human judgments on system pairs that are separated by 0.0-0.5 ROUGE points, thereby directly evaluating the scenario in which ROUGE is used to make decisions about system quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_54",
            "content": "In Fig. 6 we report the r SYS \u2206( , u) correlations for = 0.0 and various values of u on both the SummEval and REALSumm datasets (more combinations of and u are included in Appendix B). That is, we evaluate r SYS only on system pairs which are separated by at most an automatic score of u. The values of u were selected by picking the minimum u which would result in evaluating on 10%, 20%, . . . , 100% of the N 2 possible system pairs closest in score to be consistent across all three metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_55",
            "content": "The correlations for each metric on the system pairs closest in score are far lower than the correlations evaluated on all of the system pairs. For instance, the correlation of BERTScore on Summ-Eval with the closest 20% of system pairs (u \u2248 0.2) is only 0.42 compared to 0.77 under the standard definition of r SYS . Thus, it is clear that the metrics are much less reliable approximations of human judgments when the system scores are close than was previously known. Evaluating on all possible system pairs leads to an overly optimistic view of automatic metric quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_56",
            "content": "The r SYS \u2206( , u) correlation of ROUGE for = 0.0 and u = 0.5 -a typical improvement reported by researchers -is 0.08 and 0.0 on the Summ-Eval and REALSumm datasets. Therefore, these results suggest the most popular summarization evaluation metric agrees with human judgments of system quality in realistic scenarios only slightly better than or equal to random chance. For instance, 20% of the N 2 system pairs on SummEval are separated by < 0.5 ROUGE-1, and the system-level correlation on those pairs is around 0.08. As more systems are used in the correlation calculation, the allowable gap in scores between system pairs increases, and are therefore likely easier to rank, resulting in higher correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_57",
            "content": "This result also offers an explanation for why a naive metric such as ROUGE achieves moderately strong correlations under the standard definition of the system-level correlation (0.45 and 0.73 on SummEval and REALSumm) despite well known flaws and criticisms (Passonneau et al., 2005;Conroy and Dang, 2008;Deutsch and Roth, 2020, among others): It has benefited from an easy evaluation protocol. Despite its simplicity, it is not too surprising that a large gap of 5-10 ROUGE points actually does correctly rank system pairs. Most of its positive correlation comes from such easy examples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_58",
            "content": "Conclusions & Recommendations",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "246-ARR_v1_59",
            "content": "If it is assumed that we have enough high-quality judgments to accurately discriminate between two similarly performing systems, then the results in Fig. 6 show that the correlations in realistic settings are trending very low, meaning automatic metrics are not nearly sensitive enough to distinguish between systems with only minor differences in quality. This is problematic because this is the scenario in which metrics are most frequently used, and therefore they are not very reliable methods of evaluating summarization systems. However, it is not all bad news. Because the standard systemlevel \u03c4 values are moderately positive, consistent improvements in automatic metrics over time will likely result in better quality systems. Similarly to stochastic gradient descent, not every reported improvement is real, but on average over time, the quality does improve. Nonetheless, future work should focus on improving the quality of evaluation metrics when the differences in system performance are small, and researchers who compare systems should invest more effort into their human evaluations since automatic evaluations are not very reliable.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_60",
            "content": "However, because the available number of system pairs to calculate the correlations in Fig. 6 is rather small -especially when evaluating on the closest system pairs -and recent work suggests we may not have enough human judgments to accurately distinguish between similarly performing systems (Wei and Jia, 2021), it could be difficult to reach any definitive conclusions about the metrics' correlations. That being said, these are our best estimates of the correlations with the available data. Not knowing how much we can trust automatic metrics is not a good outcome. In this scenario, future work should focus on collecting more, high-quality human judgments so that we can better meta-evaluate automatic metrics. Since we argue that it is important to distinguish between similarly performing systems, new data collection efforts should consider using targeted pairwise judgments between those systems instead of direct assessments across a variety of systems of diverse quality.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_61",
            "content": "We recommend that proposals of new evaluation metrics also report correlations on system pairs with various differences in scores in addition to the standard system-level correlation definition. Reporting this information would better inform users of metrics about how likely humans would agree their observed improvement is real based on its value.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_62",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "246-ARR_v1_63",
            "content": "The methodology behind meta-evaluating summarization evaluation metrics was established during the DUC/TAC shared tasks (Dang and Owczarzak, 2008, among others). In addition to competitions for developing high-quality summarization systems, there were also shared tasks for creating automatic metrics that correlated well with human judgments. The benchmark datasets created during DUC/TAC were small in size by today's standards because they were manually collected multi-document summarization datasets, which are hard to create at scale. As such, all of the model-generated summaries on the full test set were judged (so M jud = M test ; \u00a73), unlike for current datasets which are too large to fully judge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_64",
            "content": "Recently, there has been growing interest in revisiting the meta-evaluation of automatic evaluation metrics for summarization, in part due to the large differences between currently popular summarization datasets and those used in DUC/TAC. We view our work as continuing this direction of research. Peyrard (2019) argues that current evaluation metrics do not work as well when they are used to evaluate high-performing systems compared to those which were evaluated in DUC/TAC.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_65",
            "content": "Both Fabbri et al. (2021) and Bhandari et al. (2020) re-evaluated how well existing evaluation metrics work on the popular CNN/DailyMail dataset (Nallapati et al., 2016) by collecting judgments of summary quality using recent state-ofthe-art systems. These datasets were used in our analyses. While the goal of these works was to identify which metrics correlated best with human judgments, our goal is to point out the ways in which the current methodology of meta-evaluating metrics is inconsistent with how they are used.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_66",
            "content": "Then, the work of Deutsch et al. (2021b) proposed statistical methods for estimating and comparing correlation values. In contrast to our work, they provide statistical tools for analyzing correlations, whereas we propose new definitions of correlations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_67",
            "content": "Finally, Wei and Jia (2021) provided a theoretical analysis of the bias and variance of automatic and human evaluations of machine translations and summaries. Among their conclusions, they argue for evaluating metrics with pairwise accuracy (Kendall's \u03c4 ) and that it may be prohibitively expensive to collect enough human judgments to distinguish between two systems with very similar quality. Our work further argues that metrics should be evaluated with a variant of Kendall's \u03c4 calculated using realistic system pairs ( \u00a74). Unfortunately, their results suggest that collecting enough human judgments to accurately measure how well automatic metrics perform in this setting may be very difficult.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_68",
            "content": "Related studies to ours have examined how the choice of which systems to include in metric evaluations impacts the correlation values. Both Mathur et al. (2020) and Bhandari et al. (2020) identify that metrics perform worse when scoring only the top-k systems in machine translation and summarization, respectively, and examine the use of pairwise comparisons for metric evaluation. Further, Mathur et al. (2020) demonstrate that outlier systems have an out-sized influence on the correlation values and recommend removing them from the metric evaluations. In contrast, our work proposes to change the evaluation methodology for metrics so that it more closely resembles how they are used in practice. This results in evaluating only on system pairs which are realistically compared by researchers, that is, those separated by small margins in automatic metric scores. We believe that this is a more principled approach to how to select which system pairs to evaluate on compared to previous work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_69",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "246-ARR_v1_70",
            "content": "In this work, we proposed two independent changes to how the system-level correlation of metrics is calculated to better align with how they are used to evaluate systems. Our analyses showed that these modifications led to lower-variance estimates of correlations and that commonly reported improvements in metric scores may not reliably predict how humans would judge system quality. The results from the analyses point to the need for future data collection efforts of high-quality human judgments and improving automatic evaluation metrics when differences in system performance are small.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_71",
            "content": "In addition to the BOOT-INPUTS CI method proposed by Deutsch et al. (2021b), the authors also proposed BOOT-SYSTEMS and BOOT-BOTH. Each of the three methods makes assumptions about whether the set of N systems and M input documents are fixed or variable during the bootstrapping calculation. For instance, BOOT-INPUTS assumes the N systems are always the same and the M input documents are random, then subsequently resamples M input documents on each bootstrapping iteration to calculate the confidence interval. BOOT-SYSTEMS does the opposite by resampling which N systems are used while holding the original M input documents fixed. BOOT-BOTH assumes both the systems and inputs are variable. Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4). The widths of the BOOT-BOTH CIs decreased by 14% and 12%, whereas the BOOT-SYSTEMS CIs only decreased by 1% and 6%.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_72",
            "content": "The BOOT-SYSTEMS widths likely decreased less because its estimation of r SYS is not dependent on the variance of the system score estimates. Since the set of M input documents is fixed, the system scores do not change at all during bootstrapping, so increasing the number of summaries used to estimate those scores should not have a major effect on the estimation of r SYS .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_73",
            "content": "Fig. 9 contains the r SYS \u2206( , u) correlations for when = 0 for ROUGE-1, ROUGE-2, and ROUGE-L, equivalent to those shown in Fig. 6 in the main body of the paper (ROUGE-1 is shown in both). The ROUGE-2 and ROUGE-L results are largely consistent with those of ROUGE-1. The metrics' correlations to human annotations are low (or even negative) when the differences between system scores are small. As more pairs are added that differ by larger margins, the correlations increase. 1.6 3.9 6.4 9.0 18.8 1.0 1.9 2.9 4.5 9.1 1.8 3.5 5. each heatmap are plotted in Figs. 6 and 9. We see that as the allowed score gap between system pairs is allowed to increase (i.e., adding \"easier\" pairs to rank), the correlation increases by a large margin over the correlation on pairs close in score. All of the metrics have nearly perfect correlation when the system pairs are separated by large margins. The values of and u were chosen so that each value in the heatmaps evaluates on 10% more system pairs than the value to its left. For instance, the first row evaluates on 10%, 20%, . . . , 100% of the system pairs. The second row evaluates on 10%, 20%, . . . , 90% of the system pairs, never including the 10% of pairs which are closest in score. The first row of each of the heatmaps is plotted in Fig. 6. The correlations on realistic score differences between systems are in the upper left portion of the heatmaps and contain the lowest correlations overall. Evaluating on all pairs is the top-rightmost entry, and the \"easiest\" pairs (those separated by a large score margin) are in the bottom right.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "246-ARR_v1_74",
            "content": "Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, Graham Neubig, Reevaluating Evaluation in Text Summarization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Manik Bhandari",
                    "Pranav Narayan Gour",
                    "Atabak Ashfaq",
                    "Pengfei Liu",
                    "Graham Neubig"
                ],
                "title": "Reevaluating Evaluation in Text Summarization",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_75",
            "content": "John Conroy, Hoa Dang, Mind the Gap: Dangers of Divorcing Evaluations of Summary Content from Linguistic Quality, 2008, Proceedings of the 22nd International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "John Conroy",
                    "Hoa Dang"
                ],
                "title": "Mind the Gap: Dangers of Divorcing Evaluations of Summary Content from Linguistic Quality",
                "pub_date": "2008",
                "pub_title": "Proceedings of the 22nd International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_76",
            "content": "Trang Hoa, Karolina Dang,  Owczarzak, Overview of the TAC 2008 Update Summarization Task, 2008, Proc. of the Text Analysis Conference, TAC.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Trang Hoa",
                    "Karolina Dang",
                    " Owczarzak"
                ],
                "title": "Overview of the TAC 2008 Update Summarization Task",
                "pub_date": "2008",
                "pub_title": "Proc. of the Text Analysis Conference",
                "pub": "TAC"
            }
        },
        {
            "ix": "246-ARR_v1_77",
            "content": "Daniel Deutsch, Tania Bedrax-Weiss, Dan Roth, Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Daniel Deutsch",
                    "Tania Bedrax-Weiss",
                    "Dan Roth"
                ],
                "title": "Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary",
                "pub_date": "2021",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_78",
            "content": "Daniel Deutsch, Rotem Dror, Dan Roth, A Statistical Analysis of Summarization Evaluation Metrics Using Resampling Methods, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Daniel Deutsch",
                    "Rotem Dror",
                    "Dan Roth"
                ],
                "title": "A Statistical Analysis of Summarization Evaluation Metrics Using Resampling Methods",
                "pub_date": "2021",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_79",
            "content": "Daniel Deutsch, Dan Roth, Understanding the Extent to which Summarization Evaluation Metrics Measure the Information Quality of Summaries, 2020, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Daniel Deutsch",
                    "Dan Roth"
                ],
                "title": "Understanding the Extent to which Summarization Evaluation Metrics Measure the Information Quality of Summaries",
                "pub_date": "2020",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_80",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "246-ARR_v1_81",
            "content": "Alexander Fabbri, Wojciech Kryscinski, Bryan Mc-Cann, R Socher, Dragomir Radev, Sum-mEval: Re-evaluating Summarization Evaluation, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Alexander Fabbri",
                    "Wojciech Kryscinski",
                    "Bryan Mc-Cann",
                    "R Socher",
                    "Dragomir Radev"
                ],
                "title": "Sum-mEval: Re-evaluating Summarization Evaluation",
                "pub_date": "2021",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_82",
            "content": "Chin-Yew Lin, ROUGE: A Package for Automatic Evaluation of Summaries, 2004, Text Summarization Branches Out, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Chin-Yew Lin"
                ],
                "title": "ROUGE: A Package for Automatic Evaluation of Summaries",
                "pub_date": "2004",
                "pub_title": "Text Summarization Branches Out",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "246-ARR_v1_83",
            "content": "Annie Louis, Ani Nenkova, Automatically Assessing Machine Summary Content Without a Gold Standard, 2013, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Annie Louis",
                    "Ani Nenkova"
                ],
                "title": "Automatically Assessing Machine Summary Content Without a Gold Standard",
                "pub_date": "2013",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_84",
            "content": "Nitika Mathur, Timothy Baldwin, Trevor Cohn, Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Nitika Mathur",
                    "Timothy Baldwin",
                    "Trevor Cohn"
                ],
                "title": "Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_85",
            "content": "Ramesh Nallapati, Bowen Zhou, C\u00edcero Nogueira, \u00c7aglar Santos, Bing G\u00fcl\u00e7ehre,  Xiang, Abstractive Text Summarization using Sequence-tosequence RNNs and Beyond, 2016-08-11, Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Ramesh Nallapati",
                    "Bowen Zhou",
                    "C\u00edcero Nogueira",
                    "\u00c7aglar Santos",
                    "Bing G\u00fcl\u00e7ehre",
                    " Xiang"
                ],
                "title": "Abstractive Text Summarization using Sequence-tosequence RNNs and Beyond",
                "pub_date": "2016-08-11",
                "pub_title": "Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning",
                "pub": "ACL"
            }
        },
        {
            "ix": "246-ARR_v1_86",
            "content": "J Rebecca, Ani Passonneau, Kathleen Nenkova, Sergey Mckeown,  Sigelman, Applying the Pyramid Method in DUC, 2005, Proceedings of the document understanding conference (DUC 05), .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "J Rebecca",
                    "Ani Passonneau",
                    "Kathleen Nenkova",
                    "Sergey Mckeown",
                    " Sigelman"
                ],
                "title": "Applying the Pyramid Method in DUC",
                "pub_date": "2005",
                "pub_title": "Proceedings of the document understanding conference (DUC 05)",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_87",
            "content": "Maxime Peyrard, Studying Summarization Evaluation Metrics in the Appropriate Scoring Range, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Maxime Peyrard"
                ],
                "title": "Studying Summarization Evaluation Metrics in the Appropriate Scoring Range",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_88",
            "content": "Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth Pasunuru, Mohit Bansal, Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation, 2019-06-02, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Ori Shapira",
                    "David Gabay",
                    "Yang Gao",
                    "Hadar Ronen",
                    "Ramakanth Pasunuru",
                    "Mohit Bansal"
                ],
                "title": "Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation",
                "pub_date": "2019-06-02",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_89",
            "content": "Johnny Wei, Robin Jia, The Statistical Advantage of Automatic NLG Metrics at the System Level, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Johnny Wei",
                    "Robin Jia"
                ],
                "title": "The Statistical Advantage of Automatic NLG Metrics at the System Level",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "246-ARR_v1_90",
            "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, Yoav Artzi, BERTScore: Evaluating Text Generation with BERT, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Tianyi Zhang",
                    "Varsha Kishore",
                    "Felix Wu",
                    "Kilian Weinberger",
                    "Yoav Artzi"
                ],
                "title": "BERTScore: Evaluating Text Generation with BERT",
                "pub_date": "2020-04-26",
                "pub_title": "8th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "246-ARR_v1_91",
            "content": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian Meyer, Steffen Eger, MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance, 2019, Proc. of the Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Wei Zhao",
                    "Maxime Peyrard",
                    "Fei Liu",
                    "Yang Gao",
                    "Christian Meyer",
                    "Steffen Eger"
                ],
                "title": "MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance",
                "pub_date": "2019",
                "pub_title": "Proc. of the Conference on Empirical Methods in Natural Language Processing",
                "pub": "EMNLP"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "246-ARR_v1_0@0",
            "content": "Re-Examining System-Level Correlations of Automatic Summarization Evaluation Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_0",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_2@0",
            "content": "How reliably an automatic summarization evaluation metric replicates human judgments of summary quality is quantified by systemlevel correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_2",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_2@1",
            "content": "We identify two ways in which the definition of the system-level correlation is inconsistent with how metrics are used to evaluate systems in practice and propose changes to rectify this disconnect.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_2",
            "start": 147,
            "end": 344,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_2@2",
            "content": "First, we calculate the system score for an automatic metric using the full test set instead of the subset of summaries judged by humans, which is currently standard practice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_2",
            "start": 346,
            "end": 520,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_2@3",
            "content": "We demonstrate how this small change leads to more precise estimates of system-level correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_2",
            "start": 522,
            "end": 619,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_2@4",
            "content": "Second, we propose to calculate correlations only on pairs of systems which are separated by differences in automatic scores that are commonly used to argue one system is of higher quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_2",
            "start": 621,
            "end": 809,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_2@5",
            "content": "This allows us to demonstrate that our best estimate of the correlation of ROUGE to human judgments is near 0 in realistic scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_2",
            "start": 811,
            "end": 943,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_2@6",
            "content": "The results from the analyses point to the need to collect more high-quality human judgments and to improve automatic metrics when differences in system scores are small.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_2",
            "start": 945,
            "end": 1114,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_2@7",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_2",
            "start": 1116,
            "end": 1116,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_4@0",
            "content": "Automatic evaluation metrics are the most common method that researchers use to quickly and cheaply approximate how humans would rate the quality of a summarization system (Lin, 2004;Louis and Nenkova, 2013;Zhao et al., 2019;Zhang et al., 2020;Deutsch et al., 2021a, among others).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_4",
            "start": 0,
            "end": 280,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_4@1",
            "content": "The quality of a metric -how similarly it replicates human judgments of systems -is quantified by calculating the correlation between the metric's scores and human judgments on a set of systems, known as the system-level correlation (Louis and Nenkova, 2013;Deutsch et al., 2021b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_4",
            "start": 282,
            "end": 562,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_5@0",
            "content": "Accurately estimating system-level correlations is critically important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_5",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_5@1",
            "content": "Summarization researchers 1 Our code will be released after publication.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_5",
            "start": 73,
            "end": 144,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_5@2",
            "content": "use automatic metrics during system development to make decisions about which ideas work and which do not, and systems from different research groups are ranked by automatic metrics to define which system is the \"state-of-the-art.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_5",
            "start": 146,
            "end": 376,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_5@3",
            "content": "If we do not have precise estimates of metric quality, it is not clear how much trust the community should put in such evaluation methodologies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_5",
            "start": 378,
            "end": 521,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_6@0",
            "content": "At present, there are disconnects between how automatic metrics are evaluated and how they are used to evaluate systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_6",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_6@1",
            "content": "First, the metrics' scores which are used in practice are not the ones which are evaluated in system-level correlations: Researchers compare systems based on metric scores calculated on the entire test set but calculate scores for system-level correlations when evaluating metrics on a much smaller subset of judged summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_6",
            "start": 121,
            "end": 446,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_6@2",
            "content": "Second, metrics are evaluated in a setting that is much easier than how they are actually used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_6",
            "start": 448,
            "end": 542,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_6@3",
            "content": "Metric correlations are calculated using systems that vary greatly in quality, whereas researchers compare new systems to recent work, which are likely to be very close in quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_6",
            "start": 544,
            "end": 723,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_6@4",
            "content": "Discriminating between two systems of similar quality is much harder than doing so between low and high quality systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_6",
            "start": 725,
            "end": 844,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_7@0",
            "content": "In this work, we re-examine how system-level correlations are calculated and propose two independent changes to make the evaluation of metrics better aligned to how they are actually used to evaluate systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_7",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_8@0",
            "content": "First, we propose to modify the system-level correlation definition to use the entire test set to calculate the system scores for automatic metrics instead of only the subset of summaries judged by humans ( \u00a73).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_8",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_8@1",
            "content": "With this change, the scores which are used to compare systems are directly evaluated, and we further demonstrate how the precision of our estimate of system-level correlations improves as a result.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_8",
            "start": 212,
            "end": 409,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_8@2",
            "content": "Calculating system scores over a larger number of instances reduces the variance of the scores, which results in confidence intervals (CIs) for the correlations that are 16-51% more narrow on average ( \u00a73.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_8",
            "start": 411,
            "end": 618,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_9@0",
            "content": "Second, we redefine a high quality metric to be one for which a small difference in score reliably indicates a difference in quality ( \u00a74).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_9",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_9@1",
            "content": "Then, instead of calculating the correlation with all available system pairs, we only evaluate with pairs of systems whose automatic metric scores differ by some threshold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_9",
            "start": 140,
            "end": 311,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_9@2",
            "content": "This allows us to show that a ROUGE-1 score difference of less than 0.5 between systems has almost no correlation to how humans would rank the same two systems according to our best estimates ( \u00a74.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_9",
            "start": 313,
            "end": 512,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_9@3",
            "content": "For two other metrics, BERTScore (Zhang et al., 2020) and QAEval (Deutsch et al., 2021a), we show their correlations calculated on system pairs of similar quality are much worse than under the standard correlation definition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_9",
            "start": 514,
            "end": 738,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_9@4",
            "content": "These results cast doubt on how reliable automatic evaluation metrics are for measuring summarization system quality in realistic scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_9",
            "start": 740,
            "end": 879,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_10@0",
            "content": "Our analyses point to the need to collect more high-quality human judgments of summaries in order to have more accurate estimates of metric correlations as well as the need to improve the ability of automatic metrics to discriminate between similarly performing systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_10",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_11@0",
            "content": "Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_11",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_12@0",
            "content": "Automatic evaluation metrics are most commonly used to argue that one summarization system is better than another, typically by showing that the value of a metric improves with the \"better\" system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_12",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_12@1",
            "content": "How similarly automatic metrics replicate human judgments of system quality is quantified by system-level correlations as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_12",
            "start": 198,
            "end": 327,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_13@0",
            "content": "The summaries from N systems on M jud input documents are judged by humans Z and scored with an automatic metric X .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_13",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_13@1",
            "content": "Then, the systemlevel correlation between X and Z is calculated as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_13",
            "start": 117,
            "end": 182,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_14@0",
            "content": "r SYS = CORR \uf8eb \uf8ec \uf8ed \uf8f1 \uf8f2 \uf8f3 \uf8eb \uf8ed 1 M jud M jud j x j i , 1 M jud M jud j z j i \uf8f6 \uf8f8 \uf8fc \uf8fd \uf8fe N i=1 \uf8f6 \uf8f7 \uf8f8",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_14",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_15@0",
            "content": "where x j i and z j i are the scores of X and Z for the summary produced by the i-th system on the j-th input document and CORR is some correlation function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_15",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_15@1",
            "content": "See Fig. 1 for an illustration of this calculation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_15",
            "start": 158,
            "end": 208,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_16@0",
            "content": "In this work, we use Kendall's \u03c4 (the \"b\" vari-Figure 1: The system-level correlation is calculated between the average X and Z scores on a set of summarization systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_16",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_16@1",
            "content": "x j i and z j i are the scores for the summary produced by system i (represented by rows) on input document j (represented by columns).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_16",
            "start": 170,
            "end": 304,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_17@0",
            "content": "ant 2 ) as the correlation function because we are most concerned with a metric's ability to correctly determine whether one system is better than another since that is how metrics are used in practice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_17",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_17@1",
            "content": "Kendall's \u03c4 is computed based on the number of system pairs out of N 2 which are ranked the same by X and Z. It is defined as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_17",
            "start": 203,
            "end": 327,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_18@0",
            "content": "\u03c4 = P \u2212 Q (P + Q + T ) \u2022 (P + Q + U )(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_18",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_19@0",
            "content": "where P and Q are the number of pairs ranked the same or different by X and Z, respectively, and T and U are the number of ties only in X or Z, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_19",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_20@0",
            "content": "Because the computation of r SYS involves randomness -its value depends on which M jud input documents (and even which N systems) were used -it is only an approximation of the true correlation between X and Z. As such, Deutsch et al. (2021b) proposed various methods for calculating confidence intervals for r SYS .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_20",
            "start": 0,
            "end": 314,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_20@1",
            "content": "For instance, their BOOT-INPUTS method uses bootstrapping to repeatedly resample the M jud input documents used to calculate r SYS , thereby calculating a confidence interval for the true r SYS value for X and Z.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_20",
            "start": 316,
            "end": 527,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_21@0",
            "content": "The datasets that are used in this paper's analyses are SummEval (Fabbri et al., 2021) and REALSumm (Bhandari et al., 2020), two recently collected datasets with human annotations for summary quality collected from the CNN/DailyMail dataset (Nallapati et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_21",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_22@0",
            "content": "SummEval has M jud = 100 summaries annotated with a summary relevance score for N = 16 systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_22",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_22@1",
            "content": "REALSumm has M jud = 100 summaries annotated with a Lightweight Pyramid score (Shapira et al., 2019) for N = 25 systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_22",
            "start": 96,
            "end": 215,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_22@2",
            "content": "We correlate the scores of the automatic metrics to these annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_22",
            "start": 217,
            "end": 286,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_22@3",
            "content": "The CNN/DailyMail test split has 11, 490 instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_22",
            "start": 288,
            "end": 338,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_23@0",
            "content": "Automatic Metrics Our experiments will analyze three different reference-based automatic evaluation metrics which were chosen because they were demonstrated to have the best correlations with human judgments on the SummEval and REALSumm datasets (Deutsch et al., 2021b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_23",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_23@1",
            "content": "ROUGE-n (Lin, 2004) evaluates a generated summary by calculating an F 1 score on the number of ngrams it has in common with a human-written reference summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_23",
            "start": 271,
            "end": 428,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_23@2",
            "content": "BERTScore (Zhang et al., 2020) aligns the generated and reference summaries' tokens based on their BERT embeddings (Devlin et al., 2019) and calculates a score based on the similarity of the aligned tokens' embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_23",
            "start": 430,
            "end": 647,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_23@3",
            "content": "QA-Eval (Deutsch et al., 2021a) compares the two summaries by automatically generating questions from the reference and calculating what proportion of those questions are answered correctly by the generated summary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_23",
            "start": 649,
            "end": 863,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_24@0",
            "content": "Evaluating with All Available Instances",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_24",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_25@0",
            "content": "Although the above definition of the system-level correlation has been used by recent meta-evaluation studies of metrics (Bhandari et al., 2020;Fabbri et al., 2021;Deutsch et al., 2021b), there is a disconnect between how the automatic metrics are evaluated and how they are used in practice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_25",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_26@0",
            "content": "Researchers who develop summarization systems evaluate those systems with automatic metrics on all M test test instances, not just the subset of M jud instances which were judged by humans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_26",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_26@1",
            "content": "Evaluating a system on a larger number of summaries may end up changing the system's score, which could potentially alter the overall ranking of a set of systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_26",
            "start": 190,
            "end": 351,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_26@2",
            "content": "Therefore, the rankings that are used by practitioners to determine system quality are not the ones which are being evaluated in the standard definition of system-level correlation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_26",
            "start": 353,
            "end": 533,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_26@3",
            "content": "3 To that end, we propose to modify the correlation definition to use all M test instances to calculate the system scores for the automatic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_26",
            "start": 535,
            "end": 682,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_26@4",
            "content": "That is (differences in bold):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_26",
            "start": 684,
            "end": 713,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_27@0",
            "content": "r SYS = CORR \uf8eb \uf8ec \uf8ed \uf8f1 \uf8f2 \uf8f3 \uf8eb \uf8ed 1 M test Mtest j x j i , 1 M jud M jud j z j i \uf8f6 \uf8f8 \uf8fc \uf8fd \uf8fe N i=1 \uf8f6 \uf8f7 \uf8f8",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_27",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_28@0",
            "content": "In practice with modern, large-scale datasets, this minor change could mean estimating system quality based on \u224810k inputs instead of around 100.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_28",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_28@1",
            "content": "This new definition now properly evaluates the way metrics are actually used by researchers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_28",
            "start": 146,
            "end": 237,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_28@2",
            "content": "We expect that scoring systems with M test inputs instead of M jud should lead to a better estimate of the true automatic metric score, which would in turn result in a lower-variance estimate of the correlation between X and Z in the form of smaller confidence intervals for r SYS .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_28",
            "start": 239,
            "end": 520,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_28@3",
            "content": "In the next sections, we carry out analyses to demonstrate that this is true.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_28",
            "start": 522,
            "end": 598,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_29@0",
            "content": "Reducing Automatic Metric Variance",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_29",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_30@0",
            "content": "First, we empirically show that scoring systems with M test instances instead of M jud does indeed reduce the variance of the estimate of the automatic metric scores and subsequently increases the stabilities of the system rankings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_30",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_31@0",
            "content": "Ideally, the X score for a system would be its \"oracle\" X score, equal to the expected value of X for a document sampled from the latent distribution over documents defined by the dataset (e.g., a system's ROUGE score on an infinite number of examples from a dataset).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_31",
            "start": 0,
            "end": 267,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_31@1",
            "content": "Since this cannot be calculated, it is approximated by averaging the X score on a sample (i.e., either the M jud or M test input documents).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_31",
            "start": 269,
            "end": 408,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_31@2",
            "content": "Because M test M jud , we expect that the variance of this estimate using M test inputs should be lower than when using M jud .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_31",
            "start": 410,
            "end": 536,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_31@3",
            "content": "To quantify this, we calculated the variance of estimating the oracle X score using both M jud and M test input documents via bootstrapping.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_31",
            "start": 538,
            "end": 677,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_31@4",
            "content": "We randomly sampled M input documents with replacement, recomputed the system scores, and calculated the variance of those scores over 1k iterations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_31",
            "start": 679,
            "end": 827,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_31@5",
            "content": "For all three metrics on both datasets, we found around a 99% reduction in the variance when M test inputs were used instead of M jud , clearly demonstrating that evaluating systems with M test inputs results in a better estimate of the system scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_31",
            "start": 829,
            "end": 1079,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_31@6",
            "content": "In Fig. 2, this is visualized for BERTScore on the REALSumm dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_31",
            "start": 1081,
            "end": 1148,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_32@0",
            "content": "However, because we are interested in evaluating the metrics' rankings, we also quantify how much of an effect this reduction in variance has on the stability of the system rankings induced by X .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_32",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_32@1",
            "content": "Similarly to the system scores, there is an oracle ranking of systems for X , equal to the ordering of systems by their respective oracle X scores (e.g., systems sorted by their ROUGE scores calculated on an infinite number of examples from a dataset).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_32",
            "start": 197,
            "end": 448,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_32@2",
            "content": "As the variance of the system score estimates decreases, the computed ranking of systems should begin to converge to the oracle X ranking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_32",
            "start": 450,
            "end": 587,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_32@3",
            "content": "We aim to understand to what extent this happens if M test instances are used for evaluation instead of M jud .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_32",
            "start": 589,
            "end": 699,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_33@0",
            "content": "To quantify this notion, we calculate the Kendall's \u03c4 between two system rankings for X that were based on two sets of M input documents, each sampled with replacement from the set of available documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_33",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_33@1",
            "content": "This simulates how much the system rankings would change if the evaluation procedure was run twice, each time with M random input documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_33",
            "start": 205,
            "end": 344,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_33@2",
            "content": "This quantity is calculated 1k times for various values of M and plotted in Fig. 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_33",
            "start": 346,
            "end": 428,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_34@0",
            "content": "As M approaches M test , the automatic metrics' \u03c4 values approach 1, which is significantly higher than the respective values at M jud , typically around 0.6-0.8.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_34",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_34@1",
            "content": "A value near 1 means that the rankings calculated using M test inputs are almost constant, implying the rankings have converged to the oracle ranking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_34",
            "start": 163,
            "end": 312,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_34@2",
            "content": "Therefore, the reduction in variance from evaluating on M test instances does indeed greatly stabilize the system rankings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_34",
            "start": 314,
            "end": 436,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_35@0",
            "content": "Fig. 3 also contains the same analysis performed for the human judgments Z in both datasets, although it is limited to a maximum of M jud input documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_35",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_35@1",
            "content": "We see that on both datasets the judgments' rankings are still quite variable, reaching a maximum of around 0.8-0.85 \u03c4 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_35",
            "start": 155,
            "end": 274,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_36@0",
            "content": "Confidence Interval Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_36",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_37@0",
            "content": "Next, we show that the improved estimate of system scores leads to a more precise estimate of r SYS by demonstrating the widths of the confidence intervals for r SYS decrease.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_37",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_38@0",
            "content": "The confidence intervals for r SYS calculated using bootstrapping methods proposed by Deutsch et al. (2021b) are rather wide.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_38",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_38@1",
            "content": "For instance, the 95% CI for ROUGE-2 on SummEval is [\u2212.09, .84], demonstrating a rather high level of uncertainty in its value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_38",
            "start": 126,
            "end": 252,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_38@2",
            "content": "This is problematic because it means we do not have a good picture of how reliable automatic evaluation metrics are.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_38",
            "start": 254,
            "end": 369,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_38@3",
            "content": "Reducing the width of the CIs will help us better understand the true metric quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_38",
            "start": 371,
            "end": 455,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_39@0",
            "content": "We suspect that the large width of the confidence interval is due to the variance of the system rankings of the automatic metrics and human judgments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_39",
            "start": 0,
            "end": 149,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_39@1",
            "content": "The more unstable the rankings are with respect to the M inputs, the larger the variance of the estimate of r SYS should be since very different system rankings would be compared on each bootstrapping iteration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_39",
            "start": 151,
            "end": 361,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_39@2",
            "content": "Deutsch et al. (2021b) used M jud input documents to calculate their CIs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_39",
            "start": 363,
            "end": 435,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_39@3",
            "content": "Therefore, we expect the improved stability of the automatic metric system rankings from evaluating on M test instances should result in a more narrow confidence interval for r SYS since some noise has been removed from this computation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_39",
            "start": 437,
            "end": 673,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_40@0",
            "content": "To demonstrate this, we calculated 95% CIs for r SYS using the BOOT-INPUT method on SummEval and REALSumm using both M jud and M test input documents, shown in Fig. 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_40",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_40@1",
            "content": "We find that the widths of the CIs shrank on average by 51% on SummEval and 16% on REALSumm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_40",
            "start": 168,
            "end": 259,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_40@2",
            "content": "The largest decrease in width is in the ROUGE family of metrics on SummEval, likely because that metric and dataset combination saw the biggest improvement in ranking stability (see Fig. 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_40",
            "start": 261,
            "end": 450,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_40@3",
            "content": "Thus, the improved estimate of the system scores did result in more precise estimates of r SYS .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_40",
            "start": 452,
            "end": 547,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_40@4",
            "content": "We repeated this analysis using the other bootstrapping methods proposed by Deutsch et al. (2021b), and the results are discussed in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_40",
            "start": 549,
            "end": 692,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_41@0",
            "content": "Conclusions & Recommendations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_41",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_42@0",
            "content": "By estimating system quality using automatic metrics on all available instances instead of only those which were judged, we showed that the variances of the system scores and subsequent rankings reduce significantly, resulting in better estimates of r SYS . Because this methodology additionally directly evaluates the system scores used by researchers, we recommend future work do the same.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_42",
            "start": 0,
            "end": 390,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_43@0",
            "content": "In order to continue to improve the estimate of r SYS , as much variance as possible needs to be removed from the system rankings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_43",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_43@1",
            "content": "Evaluating systems using M test instances removed a large amount of variance from the automatic metric rankings, but as demonstrated in Fig. 3, the human judgments still have a large amount of variance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_43",
            "start": 131,
            "end": 332,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_44@0",
            "content": "The human rankings' variances can either be reduced by judging more summaries per system or making the judgments more consistent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_44",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_44@1",
            "content": "Since the human rankings' stabilities in Fig. 3 are mostly beginning to plateau -especially for SummEval -it may be prohibitively expensive to collect a sufficient number of judgments to better stabilize the rankings (Wei and Jia, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_44",
            "start": 130,
            "end": 366,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_44@2",
            "content": "Therefore, we expect the more feasible solution is to improve the consistency of the human judgments, for example by better training the annotators or improving the annotation protocol.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_44",
            "start": 368,
            "end": 552,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_45@0",
            "content": "Evaluating with Realistic System Pairs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_45",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_46@0",
            "content": "Next, we argue that the set of systems used to evaluate metrics is not reflective of how metrics are used in practice and propose a new system-level correlation variant to address this problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_46",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_47@0",
            "content": "Evaluating with All System Pairs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_47",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_48@0",
            "content": "The N systems which are used for calculating system-level correlations are typically those which participated in a shared task, as in DUC/TAC (Dang and Owczarzak, 2008, among others), or those which have been published in the previous 3-4 years (Bhandari et al., 2020;Fabbri et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_48",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_48@1",
            "content": "As such, they are typically rather diverse in terms of their qualities, both as rated by human annotators and automatic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_48",
            "start": 290,
            "end": 417,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_49@0",
            "content": "The system scores of all of the systems in the REALSumm dataset as evaluated by humans and automatic metrics are shown in Fig. 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_49",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_49@1",
            "content": "Clearly, the scores are rather diverse.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_49",
            "start": 130,
            "end": 168,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_49@2",
            "content": "For example, the systems cluster into low, medium, and high quality groups (with an additional outlier) as evaluated by ROUGE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_49",
            "start": 170,
            "end": 295,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_49@3",
            "content": "A difference of around 5 ROUGE points between them is a rather large gap for ROUGE scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_49",
            "start": 297,
            "end": 386,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_50@0",
            "content": "The standard definition for a high quality evaluation metric is one which correctly ranks a set of systems with respect to human judgments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_50",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_50@1",
            "content": "As such, the implementation of the system-level correlation calculated with Kendall's \u03c4 will rank all N systems according to the human judgments and an automatic metric, then count how many pairs were ranked the same out of all N 2 pairs (see \u00a72).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_50",
            "start": 140,
            "end": 386,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_50@2",
            "content": "As a consequence, even pairs of systems which are separated by a large margin according to the automatic metric -likely systems with a clear difference in quality -are included in the evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_50",
            "start": 388,
            "end": 582,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_50@3",
            "content": "Therefore, automatic metrics are rewarded for correctly ranking such \"easy\" system pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_50",
            "start": 584,
            "end": 672,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_51@0",
            "content": "Evaluating with Realistic Pairs",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_51",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_52@0",
            "content": "This standard evaluation setting does not reflect how summarization metrics are actually used by researchers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_52",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_52@1",
            "content": "New systems are typically only slightly better than previous work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_52",
            "start": 110,
            "end": 175,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_52@2",
            "content": "Based on a survey of summarization papers in *ACL conferences over the past few years, we found that the average improvement over baseline/state-of-the-art models that was reported on the CNN/Dailymail dataset was on average 0.5 ROUGE-1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_52",
            "start": 177,
            "end": 413,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_52@3",
            "content": "It is rarely the case that the improvement in automatic metrics is very large.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_52",
            "start": 415,
            "end": 492,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_52@4",
            "content": "Therefore, evaluating metrics using pairs of systems which are separated by a large margin does not reflect the reality that metrics are very frequently used to compare those separated by a small margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_52",
            "start": 494,
            "end": 696,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_52@5",
            "content": "Including \"easy\" system pairs in the system-level correlation likely overestimates the quality of the metrics in settings which occur in practice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_52",
            "start": 698,
            "end": 843,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_53@0",
            "content": "To that extent, we redefine a high quality evaluation metric to be one for which a small difference in scores reliably indicates a difference in quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_53",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_53@1",
            "content": "We quantify this by proposing a variant of the system-level \u03c4 which is calculated between system pairs which are separated by a pre-defined automatic metric score margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_53",
            "start": 154,
            "end": 323,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_53@2",
            "content": "Instead of using all N 2 system pairs, only pairs whose difference in scores falls within the margin are used to calculate the system-level correlation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_53",
            "start": 325,
            "end": 476,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_53@3",
            "content": "We denote this correlation variant as r SYS \u2206( , u) where and u are the lower-and upper-bounds of the allowable differences in automatic metrics' scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_53",
            "start": 478,
            "end": 630,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_53@4",
            "content": "This would enable, for example, evaluating how well ROUGE correlates to human judgments on system pairs that are separated by 0.0-0.5 ROUGE points, thereby directly evaluating the scenario in which ROUGE is used to make decisions about system quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_53",
            "start": 632,
            "end": 882,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_54@0",
            "content": "In Fig. 6 we report the r SYS \u2206( , u) correlations for = 0.0 and various values of u on both the SummEval and REALSumm datasets (more combinations of and u are included in Appendix B).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_54",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_54@1",
            "content": "That is, we evaluate r SYS only on system pairs which are separated by at most an automatic score of u. The values of u were selected by picking the minimum u which would result in evaluating on 10%, 20%, . . . , 100% of the N 2 possible system pairs closest in score to be consistent across all three metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_54",
            "start": 185,
            "end": 494,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_55@0",
            "content": "The correlations for each metric on the system pairs closest in score are far lower than the correlations evaluated on all of the system pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_55",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_55@1",
            "content": "For instance, the correlation of BERTScore on Summ-Eval with the closest 20% of system pairs (u \u2248 0.2) is only 0.42 compared to 0.77 under the standard definition of r SYS .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_55",
            "start": 144,
            "end": 316,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_55@2",
            "content": "Thus, it is clear that the metrics are much less reliable approximations of human judgments when the system scores are close than was previously known.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_55",
            "start": 318,
            "end": 468,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_55@3",
            "content": "Evaluating on all possible system pairs leads to an overly optimistic view of automatic metric quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_55",
            "start": 470,
            "end": 572,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_56@0",
            "content": "The r SYS \u2206( , u) correlation of ROUGE for = 0.0 and u = 0.5 -a typical improvement reported by researchers -is 0.08 and 0.0 on the Summ-Eval and REALSumm datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_56",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_56@1",
            "content": "Therefore, these results suggest the most popular summarization evaluation metric agrees with human judgments of system quality in realistic scenarios only slightly better than or equal to random chance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_56",
            "start": 165,
            "end": 367,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_56@2",
            "content": "For instance, 20% of the N 2 system pairs on SummEval are separated by < 0.5 ROUGE-1, and the system-level correlation on those pairs is around 0.08.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_56",
            "start": 369,
            "end": 517,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_56@3",
            "content": "As more systems are used in the correlation calculation, the allowable gap in scores between system pairs increases, and are therefore likely easier to rank, resulting in higher correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_56",
            "start": 519,
            "end": 709,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_57@0",
            "content": "This result also offers an explanation for why a naive metric such as ROUGE achieves moderately strong correlations under the standard definition of the system-level correlation (0.45 and 0.73 on SummEval and REALSumm) despite well known flaws and criticisms (Passonneau et al., 2005;Conroy and Dang, 2008;Deutsch and Roth, 2020, among others): It has benefited from an easy evaluation protocol.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_57",
            "start": 0,
            "end": 394,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_57@1",
            "content": "Despite its simplicity, it is not too surprising that a large gap of 5-10 ROUGE points actually does correctly rank system pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_57",
            "start": 396,
            "end": 524,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_57@2",
            "content": "Most of its positive correlation comes from such easy examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_57",
            "start": 526,
            "end": 588,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_58@0",
            "content": "Conclusions & Recommendations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_58",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_59@0",
            "content": "If it is assumed that we have enough high-quality judgments to accurately discriminate between two similarly performing systems, then the results in Fig. 6 show that the correlations in realistic settings are trending very low, meaning automatic metrics are not nearly sensitive enough to distinguish between systems with only minor differences in quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_59",
            "start": 0,
            "end": 355,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_59@1",
            "content": "This is problematic because this is the scenario in which metrics are most frequently used, and therefore they are not very reliable methods of evaluating summarization systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_59",
            "start": 357,
            "end": 533,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_59@2",
            "content": "However, it is not all bad news.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_59",
            "start": 535,
            "end": 566,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_59@3",
            "content": "Because the standard systemlevel \u03c4 values are moderately positive, consistent improvements in automatic metrics over time will likely result in better quality systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_59",
            "start": 568,
            "end": 734,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_59@4",
            "content": "Similarly to stochastic gradient descent, not every reported improvement is real, but on average over time, the quality does improve.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_59",
            "start": 736,
            "end": 868,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_59@5",
            "content": "Nonetheless, future work should focus on improving the quality of evaluation metrics when the differences in system performance are small, and researchers who compare systems should invest more effort into their human evaluations since automatic evaluations are not very reliable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_59",
            "start": 870,
            "end": 1149,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_60@0",
            "content": "However, because the available number of system pairs to calculate the correlations in Fig. 6 is rather small -especially when evaluating on the closest system pairs -and recent work suggests we may not have enough human judgments to accurately distinguish between similarly performing systems (Wei and Jia, 2021), it could be difficult to reach any definitive conclusions about the metrics' correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_60",
            "start": 0,
            "end": 404,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_60@1",
            "content": "That being said, these are our best estimates of the correlations with the available data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_60",
            "start": 406,
            "end": 495,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_60@2",
            "content": "Not knowing how much we can trust automatic metrics is not a good outcome.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_60",
            "start": 497,
            "end": 570,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_60@3",
            "content": "In this scenario, future work should focus on collecting more, high-quality human judgments so that we can better meta-evaluate automatic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_60",
            "start": 572,
            "end": 717,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_60@4",
            "content": "Since we argue that it is important to distinguish between similarly performing systems, new data collection efforts should consider using targeted pairwise judgments between those systems instead of direct assessments across a variety of systems of diverse quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_60",
            "start": 719,
            "end": 984,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_61@0",
            "content": "We recommend that proposals of new evaluation metrics also report correlations on system pairs with various differences in scores in addition to the standard system-level correlation definition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_61",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_61@1",
            "content": "Reporting this information would better inform users of metrics about how likely humans would agree their observed improvement is real based on its value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_61",
            "start": 195,
            "end": 348,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_62@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_62",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_63@0",
            "content": "The methodology behind meta-evaluating summarization evaluation metrics was established during the DUC/TAC shared tasks (Dang and Owczarzak, 2008, among others).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_63",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_63@1",
            "content": "In addition to competitions for developing high-quality summarization systems, there were also shared tasks for creating automatic metrics that correlated well with human judgments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_63",
            "start": 162,
            "end": 342,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_63@2",
            "content": "The benchmark datasets created during DUC/TAC were small in size by today's standards because they were manually collected multi-document summarization datasets, which are hard to create at scale.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_63",
            "start": 344,
            "end": 539,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_63@3",
            "content": "As such, all of the model-generated summaries on the full test set were judged (so M jud = M test ; \u00a73), unlike for current datasets which are too large to fully judge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_63",
            "start": 541,
            "end": 708,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_64@0",
            "content": "Recently, there has been growing interest in revisiting the meta-evaluation of automatic evaluation metrics for summarization, in part due to the large differences between currently popular summarization datasets and those used in DUC/TAC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_64",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_64@1",
            "content": "We view our work as continuing this direction of research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_64",
            "start": 240,
            "end": 297,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_64@2",
            "content": "Peyrard (2019) argues that current evaluation metrics do not work as well when they are used to evaluate high-performing systems compared to those which were evaluated in DUC/TAC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_64",
            "start": 299,
            "end": 477,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_65@0",
            "content": "Both Fabbri et al. (2021) and Bhandari et al. (2020) re-evaluated how well existing evaluation metrics work on the popular CNN/DailyMail dataset (Nallapati et al., 2016) by collecting judgments of summary quality using recent state-ofthe-art systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_65",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_65@1",
            "content": "These datasets were used in our analyses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_65",
            "start": 251,
            "end": 291,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_65@2",
            "content": "While the goal of these works was to identify which metrics correlated best with human judgments, our goal is to point out the ways in which the current methodology of meta-evaluating metrics is inconsistent with how they are used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_65",
            "start": 293,
            "end": 523,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_66@0",
            "content": "Then, the work of Deutsch et al. (2021b) proposed statistical methods for estimating and comparing correlation values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_66",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_66@1",
            "content": "In contrast to our work, they provide statistical tools for analyzing correlations, whereas we propose new definitions of correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_66",
            "start": 119,
            "end": 253,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_67@0",
            "content": "Finally, Wei and Jia (2021) provided a theoretical analysis of the bias and variance of automatic and human evaluations of machine translations and summaries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_67",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_67@1",
            "content": "Among their conclusions, they argue for evaluating metrics with pairwise accuracy (Kendall's \u03c4 ) and that it may be prohibitively expensive to collect enough human judgments to distinguish between two systems with very similar quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_67",
            "start": 159,
            "end": 393,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_67@2",
            "content": "Our work further argues that metrics should be evaluated with a variant of Kendall's \u03c4 calculated using realistic system pairs ( \u00a74).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_67",
            "start": 395,
            "end": 527,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_67@3",
            "content": "Unfortunately, their results suggest that collecting enough human judgments to accurately measure how well automatic metrics perform in this setting may be very difficult.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_67",
            "start": 529,
            "end": 699,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_68@0",
            "content": "Related studies to ours have examined how the choice of which systems to include in metric evaluations impacts the correlation values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_68",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_68@1",
            "content": "Both Mathur et al. (2020) and Bhandari et al. (2020) identify that metrics perform worse when scoring only the top-k systems in machine translation and summarization, respectively, and examine the use of pairwise comparisons for metric evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_68",
            "start": 135,
            "end": 381,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_68@2",
            "content": "Further, Mathur et al. (2020) demonstrate that outlier systems have an out-sized influence on the correlation values and recommend removing them from the metric evaluations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_68",
            "start": 383,
            "end": 555,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_68@3",
            "content": "In contrast, our work proposes to change the evaluation methodology for metrics so that it more closely resembles how they are used in practice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_68",
            "start": 557,
            "end": 700,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_68@4",
            "content": "This results in evaluating only on system pairs which are realistically compared by researchers, that is, those separated by small margins in automatic metric scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_68",
            "start": 702,
            "end": 867,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_68@5",
            "content": "We believe that this is a more principled approach to how to select which system pairs to evaluate on compared to previous work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_68",
            "start": 869,
            "end": 996,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_69@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_69",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_70@0",
            "content": "In this work, we proposed two independent changes to how the system-level correlation of metrics is calculated to better align with how they are used to evaluate systems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_70",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_70@1",
            "content": "Our analyses showed that these modifications led to lower-variance estimates of correlations and that commonly reported improvements in metric scores may not reliably predict how humans would judge system quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_70",
            "start": 171,
            "end": 383,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_70@2",
            "content": "The results from the analyses point to the need for future data collection efforts of high-quality human judgments and improving automatic evaluation metrics when differences in system performance are small.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_70",
            "start": 385,
            "end": 591,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_71@0",
            "content": "In addition to the BOOT-INPUTS CI method proposed by Deutsch et al. (2021b), the authors also proposed BOOT-SYSTEMS and BOOT-BOTH.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_71",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_71@1",
            "content": "Each of the three methods makes assumptions about whether the set of N systems and M input documents are fixed or variable during the bootstrapping calculation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_71",
            "start": 131,
            "end": 290,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_71@2",
            "content": "For instance, BOOT-INPUTS assumes the N systems are always the same and the M input documents are random, then subsequently resamples M input documents on each bootstrapping iteration to calculate the confidence interval.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_71",
            "start": 292,
            "end": 512,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_71@3",
            "content": "BOOT-SYSTEMS does the opposite by resampling which N systems are used while holding the original M input documents fixed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_71",
            "start": 514,
            "end": 634,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_71@4",
            "content": "BOOT-BOTH assumes both the systems and inputs are variable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_71",
            "start": 636,
            "end": 694,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_71@5",
            "content": "Figs. 7 and 8 contain the 95% CIs for ROUGE, BERTScore, and QAEval on the SummEval and REALSumm datasets using the BOOT-SYSTEMS and BOOT-BOTH methods calculated using all M t test instances and only the M a annotated instances (BOOT-INPUTS included in the main body of the paper, Fig. 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_71",
            "start": 696,
            "end": 983,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_71@6",
            "content": "The widths of the BOOT-BOTH CIs decreased by 14% and 12%, whereas the BOOT-SYSTEMS CIs only decreased by 1% and 6%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_71",
            "start": 985,
            "end": 1099,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_72@0",
            "content": "The BOOT-SYSTEMS widths likely decreased less because its estimation of r SYS is not dependent on the variance of the system score estimates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_72",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_72@1",
            "content": "Since the set of M input documents is fixed, the system scores do not change at all during bootstrapping, so increasing the number of summaries used to estimate those scores should not have a major effect on the estimation of r SYS .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_72",
            "start": 142,
            "end": 374,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@0",
            "content": "Fig. 9 contains the r SYS \u2206( , u) correlations for when = 0 for ROUGE-1, ROUGE-2, and ROUGE-L, equivalent to those shown in Fig. 6 in the main body of the paper (ROUGE-1 is shown in both).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@1",
            "content": "The ROUGE-2 and ROUGE-L results are largely consistent with those of ROUGE-1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 189,
            "end": 265,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@2",
            "content": "The metrics' correlations to human annotations are low (or even negative) when the differences between system scores are small.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 267,
            "end": 393,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@3",
            "content": "As more pairs are added that differ by larger margins, the correlations increase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 395,
            "end": 475,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@4",
            "content": "1.6 3.9 6.4 9.0 18.8 1.0 1.9 2.9 4.5 9.1 1.8 3.5 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 477,
            "end": 527,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@5",
            "content": "each heatmap are plotted in Figs. 6 and 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 529,
            "end": 570,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@6",
            "content": "We see that as the allowed score gap between system pairs is allowed to increase (i.e., adding \"easier\" pairs to rank), the correlation increases by a large margin over the correlation on pairs close in score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 572,
            "end": 780,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@7",
            "content": "All of the metrics have nearly perfect correlation when the system pairs are separated by large margins.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 782,
            "end": 885,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@8",
            "content": "The values of and u were chosen so that each value in the heatmaps evaluates on 10% more system pairs than the value to its left.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 887,
            "end": 1015,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@9",
            "content": "For instance, the first row evaluates on 10%, 20%, . . . , 100% of the system pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 1017,
            "end": 1100,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@10",
            "content": "The second row evaluates on 10%, 20%, . . . , 90% of the system pairs, never including the 10% of pairs which are closest in score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 1102,
            "end": 1232,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@11",
            "content": "The first row of each of the heatmaps is plotted in Fig. 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 1234,
            "end": 1292,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@12",
            "content": "The correlations on realistic score differences between systems are in the upper left portion of the heatmaps and contain the lowest correlations overall.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 1294,
            "end": 1447,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_73@13",
            "content": "Evaluating on all pairs is the top-rightmost entry, and the \"easiest\" pairs (those separated by a large score margin) are in the bottom right.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_73",
            "start": 1449,
            "end": 1590,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_74@0",
            "content": "Manik Bhandari, Pranav Narayan Gour, Atabak Ashfaq, Pengfei Liu, Graham Neubig, Reevaluating Evaluation in Text Summarization, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_74",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_75@0",
            "content": "John Conroy, Hoa Dang, Mind the Gap: Dangers of Divorcing Evaluations of Summary Content from Linguistic Quality, 2008, Proceedings of the 22nd International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_75",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_76@0",
            "content": "Trang Hoa, Karolina Dang,  Owczarzak, Overview of the TAC 2008 Update Summarization Task, 2008, Proc. of the Text Analysis Conference, TAC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_76",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_77@0",
            "content": "Daniel Deutsch, Tania Bedrax-Weiss, Dan Roth, Towards Question-Answering as an Automatic Metric for Evaluating the Content Quality of a Summary, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_77",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_78@0",
            "content": "Daniel Deutsch, Rotem Dror, Dan Roth, A Statistical Analysis of Summarization Evaluation Metrics Using Resampling Methods, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_78",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_79@0",
            "content": "Daniel Deutsch, Dan Roth, Understanding the Extent to which Summarization Evaluation Metrics Measure the Information Quality of Summaries, 2020, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_79",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_80@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_80",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_81@0",
            "content": "Alexander Fabbri, Wojciech Kryscinski, Bryan Mc-Cann, R Socher, Dragomir Radev, Sum-mEval: Re-evaluating Summarization Evaluation, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_81",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_82@0",
            "content": "Chin-Yew Lin, ROUGE: A Package for Automatic Evaluation of Summaries, 2004, Text Summarization Branches Out, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_82",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_83@0",
            "content": "Annie Louis, Ani Nenkova, Automatically Assessing Machine Summary Content Without a Gold Standard, 2013, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_83",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_84@0",
            "content": "Nitika Mathur, Timothy Baldwin, Trevor Cohn, Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_84",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_85@0",
            "content": "Ramesh Nallapati, Bowen Zhou, C\u00edcero Nogueira, \u00c7aglar Santos, Bing G\u00fcl\u00e7ehre,  Xiang, Abstractive Text Summarization using Sequence-tosequence RNNs and Beyond, 2016-08-11, Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_85",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_86@0",
            "content": "J Rebecca, Ani Passonneau, Kathleen Nenkova, Sergey Mckeown,  Sigelman, Applying the Pyramid Method in DUC, 2005, Proceedings of the document understanding conference (DUC 05), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_86",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_87@0",
            "content": "Maxime Peyrard, Studying Summarization Evaluation Metrics in the Appropriate Scoring Range, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_87",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_88@0",
            "content": "Ori Shapira, David Gabay, Yang Gao, Hadar Ronen, Ramakanth Pasunuru, Mohit Bansal, Crowdsourcing Lightweight Pyramids for Manual Summary Evaluation, 2019-06-02, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_88",
            "start": 0,
            "end": 321,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_89@0",
            "content": "Johnny Wei, Robin Jia, The Statistical Advantage of Automatic NLG Metrics at the System Level, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_89",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_90@0",
            "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, Yoav Artzi, BERTScore: Evaluating Text Generation with BERT, 2020-04-26, 8th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_90",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "246-ARR_v1_91@0",
            "content": "Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian Meyer, Steffen Eger, MoverScore: Text Generation Evaluating with Contextualized Embeddings and Earth Mover Distance, 2019, Proc. of the Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "246-ARR_v1_91",
            "start": 0,
            "end": 260,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "246-ARR_v1_0",
            "tgt_ix": "246-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_0",
            "tgt_ix": "246-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_1",
            "tgt_ix": "246-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_1",
            "tgt_ix": "246-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_0",
            "tgt_ix": "246-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_2",
            "tgt_ix": "246-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_4",
            "tgt_ix": "246-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_5",
            "tgt_ix": "246-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_6",
            "tgt_ix": "246-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_7",
            "tgt_ix": "246-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_8",
            "tgt_ix": "246-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_9",
            "tgt_ix": "246-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_3",
            "tgt_ix": "246-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_3",
            "tgt_ix": "246-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_3",
            "tgt_ix": "246-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_3",
            "tgt_ix": "246-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_3",
            "tgt_ix": "246-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_3",
            "tgt_ix": "246-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_3",
            "tgt_ix": "246-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_3",
            "tgt_ix": "246-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_0",
            "tgt_ix": "246-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_10",
            "tgt_ix": "246-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_12",
            "tgt_ix": "246-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_13",
            "tgt_ix": "246-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_14",
            "tgt_ix": "246-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_15",
            "tgt_ix": "246-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_16",
            "tgt_ix": "246-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_17",
            "tgt_ix": "246-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_18",
            "tgt_ix": "246-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_19",
            "tgt_ix": "246-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_21",
            "tgt_ix": "246-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_22",
            "tgt_ix": "246-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_20",
            "tgt_ix": "246-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_0",
            "tgt_ix": "246-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_23",
            "tgt_ix": "246-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_25",
            "tgt_ix": "246-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_26",
            "tgt_ix": "246-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_27",
            "tgt_ix": "246-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_24",
            "tgt_ix": "246-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_24",
            "tgt_ix": "246-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_24",
            "tgt_ix": "246-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_24",
            "tgt_ix": "246-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_24",
            "tgt_ix": "246-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_24",
            "tgt_ix": "246-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_28",
            "tgt_ix": "246-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_30",
            "tgt_ix": "246-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_31",
            "tgt_ix": "246-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_32",
            "tgt_ix": "246-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_33",
            "tgt_ix": "246-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_34",
            "tgt_ix": "246-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_29",
            "tgt_ix": "246-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_29",
            "tgt_ix": "246-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_29",
            "tgt_ix": "246-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_29",
            "tgt_ix": "246-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_29",
            "tgt_ix": "246-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_29",
            "tgt_ix": "246-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_29",
            "tgt_ix": "246-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_24",
            "tgt_ix": "246-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_35",
            "tgt_ix": "246-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_37",
            "tgt_ix": "246-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_38",
            "tgt_ix": "246-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_39",
            "tgt_ix": "246-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_36",
            "tgt_ix": "246-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_36",
            "tgt_ix": "246-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_36",
            "tgt_ix": "246-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_36",
            "tgt_ix": "246-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_36",
            "tgt_ix": "246-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_24",
            "tgt_ix": "246-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_40",
            "tgt_ix": "246-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_42",
            "tgt_ix": "246-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_43",
            "tgt_ix": "246-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_41",
            "tgt_ix": "246-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_41",
            "tgt_ix": "246-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_41",
            "tgt_ix": "246-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_41",
            "tgt_ix": "246-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_0",
            "tgt_ix": "246-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_44",
            "tgt_ix": "246-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_45",
            "tgt_ix": "246-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_45",
            "tgt_ix": "246-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_45",
            "tgt_ix": "246-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_46",
            "tgt_ix": "246-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_48",
            "tgt_ix": "246-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_49",
            "tgt_ix": "246-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_47",
            "tgt_ix": "246-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_47",
            "tgt_ix": "246-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_47",
            "tgt_ix": "246-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_47",
            "tgt_ix": "246-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_45",
            "tgt_ix": "246-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_50",
            "tgt_ix": "246-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_52",
            "tgt_ix": "246-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_53",
            "tgt_ix": "246-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_54",
            "tgt_ix": "246-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_55",
            "tgt_ix": "246-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_56",
            "tgt_ix": "246-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_51",
            "tgt_ix": "246-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_51",
            "tgt_ix": "246-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_51",
            "tgt_ix": "246-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_51",
            "tgt_ix": "246-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_51",
            "tgt_ix": "246-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_51",
            "tgt_ix": "246-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_51",
            "tgt_ix": "246-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_45",
            "tgt_ix": "246-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_57",
            "tgt_ix": "246-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_59",
            "tgt_ix": "246-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_60",
            "tgt_ix": "246-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_58",
            "tgt_ix": "246-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_58",
            "tgt_ix": "246-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_58",
            "tgt_ix": "246-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_58",
            "tgt_ix": "246-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_0",
            "tgt_ix": "246-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_61",
            "tgt_ix": "246-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_63",
            "tgt_ix": "246-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_64",
            "tgt_ix": "246-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_65",
            "tgt_ix": "246-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_66",
            "tgt_ix": "246-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_67",
            "tgt_ix": "246-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_62",
            "tgt_ix": "246-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_62",
            "tgt_ix": "246-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_62",
            "tgt_ix": "246-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_62",
            "tgt_ix": "246-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_62",
            "tgt_ix": "246-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_62",
            "tgt_ix": "246-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_62",
            "tgt_ix": "246-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_0",
            "tgt_ix": "246-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_68",
            "tgt_ix": "246-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_69",
            "tgt_ix": "246-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_69",
            "tgt_ix": "246-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_71",
            "tgt_ix": "246-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_69",
            "tgt_ix": "246-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_69",
            "tgt_ix": "246-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_70",
            "tgt_ix": "246-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_69",
            "tgt_ix": "246-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_72",
            "tgt_ix": "246-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "246-ARR_v1_0",
            "tgt_ix": "246-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_1",
            "tgt_ix": "246-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_2",
            "tgt_ix": "246-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_2",
            "tgt_ix": "246-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_2",
            "tgt_ix": "246-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_2",
            "tgt_ix": "246-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_2",
            "tgt_ix": "246-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_2",
            "tgt_ix": "246-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_2",
            "tgt_ix": "246-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_2",
            "tgt_ix": "246-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_3",
            "tgt_ix": "246-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_4",
            "tgt_ix": "246-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_4",
            "tgt_ix": "246-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_5",
            "tgt_ix": "246-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_5",
            "tgt_ix": "246-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_5",
            "tgt_ix": "246-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_5",
            "tgt_ix": "246-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_6",
            "tgt_ix": "246-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_6",
            "tgt_ix": "246-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_6",
            "tgt_ix": "246-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_6",
            "tgt_ix": "246-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_6",
            "tgt_ix": "246-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_7",
            "tgt_ix": "246-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_8",
            "tgt_ix": "246-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_8",
            "tgt_ix": "246-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_8",
            "tgt_ix": "246-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_9",
            "tgt_ix": "246-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_9",
            "tgt_ix": "246-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_9",
            "tgt_ix": "246-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_9",
            "tgt_ix": "246-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_9",
            "tgt_ix": "246-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_10",
            "tgt_ix": "246-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_11",
            "tgt_ix": "246-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_12",
            "tgt_ix": "246-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_12",
            "tgt_ix": "246-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_13",
            "tgt_ix": "246-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_13",
            "tgt_ix": "246-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_14",
            "tgt_ix": "246-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_15",
            "tgt_ix": "246-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_15",
            "tgt_ix": "246-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_16",
            "tgt_ix": "246-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_16",
            "tgt_ix": "246-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_17",
            "tgt_ix": "246-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_17",
            "tgt_ix": "246-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_18",
            "tgt_ix": "246-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_19",
            "tgt_ix": "246-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_20",
            "tgt_ix": "246-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_20",
            "tgt_ix": "246-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_21",
            "tgt_ix": "246-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_22",
            "tgt_ix": "246-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_22",
            "tgt_ix": "246-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_22",
            "tgt_ix": "246-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_22",
            "tgt_ix": "246-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_23",
            "tgt_ix": "246-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_23",
            "tgt_ix": "246-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_23",
            "tgt_ix": "246-ARR_v1_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_23",
            "tgt_ix": "246-ARR_v1_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_24",
            "tgt_ix": "246-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_25",
            "tgt_ix": "246-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_26",
            "tgt_ix": "246-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_26",
            "tgt_ix": "246-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_26",
            "tgt_ix": "246-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_26",
            "tgt_ix": "246-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_26",
            "tgt_ix": "246-ARR_v1_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_27",
            "tgt_ix": "246-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_28",
            "tgt_ix": "246-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_28",
            "tgt_ix": "246-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_28",
            "tgt_ix": "246-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_28",
            "tgt_ix": "246-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_29",
            "tgt_ix": "246-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_30",
            "tgt_ix": "246-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_31",
            "tgt_ix": "246-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_31",
            "tgt_ix": "246-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_31",
            "tgt_ix": "246-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_31",
            "tgt_ix": "246-ARR_v1_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_31",
            "tgt_ix": "246-ARR_v1_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_31",
            "tgt_ix": "246-ARR_v1_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_31",
            "tgt_ix": "246-ARR_v1_31@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_32",
            "tgt_ix": "246-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_32",
            "tgt_ix": "246-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_32",
            "tgt_ix": "246-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_32",
            "tgt_ix": "246-ARR_v1_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_33",
            "tgt_ix": "246-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_33",
            "tgt_ix": "246-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_33",
            "tgt_ix": "246-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_34",
            "tgt_ix": "246-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_34",
            "tgt_ix": "246-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_34",
            "tgt_ix": "246-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_35",
            "tgt_ix": "246-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_35",
            "tgt_ix": "246-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_36",
            "tgt_ix": "246-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_37",
            "tgt_ix": "246-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_38",
            "tgt_ix": "246-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_38",
            "tgt_ix": "246-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_38",
            "tgt_ix": "246-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_38",
            "tgt_ix": "246-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_39",
            "tgt_ix": "246-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_39",
            "tgt_ix": "246-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_39",
            "tgt_ix": "246-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_39",
            "tgt_ix": "246-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_40",
            "tgt_ix": "246-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_40",
            "tgt_ix": "246-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_40",
            "tgt_ix": "246-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_40",
            "tgt_ix": "246-ARR_v1_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_40",
            "tgt_ix": "246-ARR_v1_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_41",
            "tgt_ix": "246-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_42",
            "tgt_ix": "246-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_43",
            "tgt_ix": "246-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_43",
            "tgt_ix": "246-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_44",
            "tgt_ix": "246-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_44",
            "tgt_ix": "246-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_44",
            "tgt_ix": "246-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_45",
            "tgt_ix": "246-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_46",
            "tgt_ix": "246-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_47",
            "tgt_ix": "246-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_48",
            "tgt_ix": "246-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_48",
            "tgt_ix": "246-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_49",
            "tgt_ix": "246-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_49",
            "tgt_ix": "246-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_49",
            "tgt_ix": "246-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_49",
            "tgt_ix": "246-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_50",
            "tgt_ix": "246-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_50",
            "tgt_ix": "246-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_50",
            "tgt_ix": "246-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_50",
            "tgt_ix": "246-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_51",
            "tgt_ix": "246-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_52",
            "tgt_ix": "246-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_52",
            "tgt_ix": "246-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_52",
            "tgt_ix": "246-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_52",
            "tgt_ix": "246-ARR_v1_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_52",
            "tgt_ix": "246-ARR_v1_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_52",
            "tgt_ix": "246-ARR_v1_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_53",
            "tgt_ix": "246-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_53",
            "tgt_ix": "246-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_53",
            "tgt_ix": "246-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_53",
            "tgt_ix": "246-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_53",
            "tgt_ix": "246-ARR_v1_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_54",
            "tgt_ix": "246-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_54",
            "tgt_ix": "246-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_55",
            "tgt_ix": "246-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_55",
            "tgt_ix": "246-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_55",
            "tgt_ix": "246-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_55",
            "tgt_ix": "246-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_56",
            "tgt_ix": "246-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_56",
            "tgt_ix": "246-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_56",
            "tgt_ix": "246-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_56",
            "tgt_ix": "246-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_57",
            "tgt_ix": "246-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_57",
            "tgt_ix": "246-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_57",
            "tgt_ix": "246-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_58",
            "tgt_ix": "246-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_59",
            "tgt_ix": "246-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_59",
            "tgt_ix": "246-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_59",
            "tgt_ix": "246-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_59",
            "tgt_ix": "246-ARR_v1_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_59",
            "tgt_ix": "246-ARR_v1_59@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_59",
            "tgt_ix": "246-ARR_v1_59@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_60",
            "tgt_ix": "246-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_60",
            "tgt_ix": "246-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_60",
            "tgt_ix": "246-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_60",
            "tgt_ix": "246-ARR_v1_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_60",
            "tgt_ix": "246-ARR_v1_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_61",
            "tgt_ix": "246-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_61",
            "tgt_ix": "246-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_62",
            "tgt_ix": "246-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_63",
            "tgt_ix": "246-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_63",
            "tgt_ix": "246-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_63",
            "tgt_ix": "246-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_63",
            "tgt_ix": "246-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_64",
            "tgt_ix": "246-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_64",
            "tgt_ix": "246-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_64",
            "tgt_ix": "246-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_65",
            "tgt_ix": "246-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_65",
            "tgt_ix": "246-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_65",
            "tgt_ix": "246-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_66",
            "tgt_ix": "246-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_66",
            "tgt_ix": "246-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_67",
            "tgt_ix": "246-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_67",
            "tgt_ix": "246-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_67",
            "tgt_ix": "246-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_67",
            "tgt_ix": "246-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_68",
            "tgt_ix": "246-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_68",
            "tgt_ix": "246-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_68",
            "tgt_ix": "246-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_68",
            "tgt_ix": "246-ARR_v1_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_68",
            "tgt_ix": "246-ARR_v1_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_68",
            "tgt_ix": "246-ARR_v1_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_69",
            "tgt_ix": "246-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_70",
            "tgt_ix": "246-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_70",
            "tgt_ix": "246-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_70",
            "tgt_ix": "246-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_71",
            "tgt_ix": "246-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_71",
            "tgt_ix": "246-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_71",
            "tgt_ix": "246-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_71",
            "tgt_ix": "246-ARR_v1_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_71",
            "tgt_ix": "246-ARR_v1_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_71",
            "tgt_ix": "246-ARR_v1_71@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_71",
            "tgt_ix": "246-ARR_v1_71@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_72",
            "tgt_ix": "246-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_72",
            "tgt_ix": "246-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_73",
            "tgt_ix": "246-ARR_v1_73@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_74",
            "tgt_ix": "246-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_75",
            "tgt_ix": "246-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_76",
            "tgt_ix": "246-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_77",
            "tgt_ix": "246-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_78",
            "tgt_ix": "246-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_79",
            "tgt_ix": "246-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_80",
            "tgt_ix": "246-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_81",
            "tgt_ix": "246-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_82",
            "tgt_ix": "246-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_83",
            "tgt_ix": "246-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_84",
            "tgt_ix": "246-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_85",
            "tgt_ix": "246-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_86",
            "tgt_ix": "246-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_87",
            "tgt_ix": "246-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_88",
            "tgt_ix": "246-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_89",
            "tgt_ix": "246-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_90",
            "tgt_ix": "246-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "246-ARR_v1_91",
            "tgt_ix": "246-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1246,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "246-ARR",
        "version": 1
    }
}