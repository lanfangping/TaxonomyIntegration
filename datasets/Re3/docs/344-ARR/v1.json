{
    "nodes": [
        {
            "ix": "344-ARR_v1_0",
            "content": "NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_2",
            "content": "We present NLU++, a novel dataset for natural language understanding (NLU) in taskoriented dialogue (ToD) systems, with the aim to provide a much more challenging evaluation environment for dialogue NLU models, up to date with the current application and industry requirements. NLU++ is divided into two domains (BANKING and HOTELS) and brings several crucial improvements over current commonly used NLU datasets. 1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences combined with finer-grained and thus more challenging slot sets. 2) The ontology is divided into domain-specific and generic (i.e., domainuniversal) intents that overlap across domains, promoting cross-domain reusability of annotated examples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_3",
            "content": "3) The dataset design has been inspired by the problems observed in industrial ToD systems, and 4) it has been collected, filtered and carefully annotated by dialogue NLU experts, yielding high-quality annotated data. Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, and call for further research on ToD NLU.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_4",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "344-ARR_v1_5",
            "content": "Research on task-oriented dialogue (ToD) systems (Levin and Pieraccini, 1995;Young et al., 2002) has become a key aspect in industry: e.g., ToD is used to automate telephone customer service tasks ranging from hospitality over healthcare to banking (Raux et al., 2003;Young, 2010;El Asri et al., 2017). Typical ToD systems still rely on a modular design: (i) the Natural Language Understanding (NLU) module maps user utterances into a domainspecific set of intent labels and values (Rastogi et al., 2019;Heck et al., 2020;Dai et al., 2021), followed by (ii) the policy module, which makes decisions based on the information extracted by the NLU (Lubis et al., 2020;Wang et al., 2020a) Yes, I need this card to arrive before 3pm on Jan 14 The NLU module is a critical part of any ToD system, as it must extract the relevant information from the user's utterances. The information relevance is denoted by the structured dialogue domain ontology, which enables the policy module to make decisions about next system actions. The domain ontology covers the information on 1) intents and 2) slots, see Figure 1. The former is aimed at extracting general conversational ideas (i.e., the user's intents) and corresponds to the standard NLU task of intent detection (ID); the latter extracts specific slot values and corresponds to the NLU task of slot labeling (SL) . 1 In order to make the policy operational and tractable, NLU should extract only the minimal information required by the policy. Therefore, the ontologies differ for each domain of ToD application and are typically built from scratch for each domain. Consequently, this makes domain-relevant NLU data extremely expensive to collect and annotate, and prevents its reusability (Budzianowski et al., 2018). Due to this, NLU research in recent years has heavily focused on very data-efficient models that can effectively operate in low-data regimes. Current state-of-the-art (SotA) NLU models leverage large pretrained language models (PLMs) (De-vlin et al., 2019;Liu et al., 2019c; and fine-tune them with small task-specific datasets (Larson et al., 2019b;Coucke et al., 2018) At the same time, the progress in creation of NLU datasets has not kept up with the impressive pace of NLU methodology development. However, designing domain ontologies and NLU datasets is also critical for steering further progress in NLU, both from methodology and application perspective. Put simply, current publicly available NLU datasets do not keep up to date with current industry/application requirements for many reasons. 1) They are usually crowdsourced by untrained annotators (thus typically optimised for quantity rather than quality), yielding examples with low lexical diversity and prone to annotation errors. 2) They typically assume one intent per example, and thus enable only much simpler single-label ID experiments; such setups are not realistic in more complex industry settings (see Figure 1 again) and lead to unnecessarily large intent sets. 3) Their ontologies are tied to specific domains, making it difficult to reuse already available annotated data in other domains. 4) The complexity of the defined tasks and ontologies is limited; the undesired artefact is that current NLU datasets might overestimate the NLU models' abilities, and are not able to separate models any more performance-wise. 2 In order to address all these gaps, we introduce NLU++, a novel NLU dataset which provides highquality NLU data annotated by dialogue experts. NLU++ provides multi-intent, slot-rich and semantically varied NLU data, and is inspired by a number of NLU challenges which ToD systems typically face in production environments. Unlike previous ID datasets, examples are annotated with multiple labels, with some examples naturally obtaining even up to 6-7 labels. NLU++ defines a rich set of slots which are combined with multi-intent sentences. NLU++ is divided into two domains (BANKING and HOTELS) where the two domain ontologies blend a set of domain-specific intents and slots with a set of generic (i.e., domain-universal) intents and slots. This design makes a crucial step towards generalisation and data reusability in NLU.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_6",
            "content": "Finally, we run a series of experiments on NLU++ with current SotA ID and SL models, demonstrating the challenging nature of NLU++ and ample room for future improvement, especially in lowdata setups. Our benchmark comparisons also demonstrate strong performance and shed new light on the (ability of) recently emerging QA-based NLU models, and warrant further research on ToD NLU. The NLU++ dataset is available at: [URL] under CC BY 4.0 license.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_7",
            "content": "Background and Motivation",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "344-ARR_v1_8",
            "content": "A Brief History of NLU Datasets. As a core module of ToD systems, NLU has been researched since the early 1990s, when the Airline Travel Information System (ATIS) project was started (Hemphill et al., 1990), consisting of spoken queries on flightrelated information. 3 Over the next two decades, very few NLU resources were released. 4 The lack of ToD NLU resources ended in 2013, with the beginning of the 'dialogue state tracking (DST) era' (Williams et al., 2013;Henderson et al., 2014;Kim et al., 2016). Instead of just classifying each turn of the user, DST deals with keeping track of the user's goal over the entire dialogue history, i.e., all the previous user and system turns. Several datasets where released during the DST challenges, all of them comprising simple intent sets (usually tagged as dialogue acts).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_9",
            "content": "To adapt to the increasing data requirements of deep learning models, increasingly larger dialogue datasets have been released in recent years (Budzianowski et al., 2018;Wei et al., 2018;Rastogi et al., 2019;Peskov et al., 2019). However, the design of ToD datasets comes with some profound differences to datasets for e.g. machine translation or speech recognition, which affect current ToD datasets. 1) The domain-specific nature of ToD datasets made the data tied to its ontologies, not allowing data reusability across different domains.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_10",
            "content": "2) The domain-specific ontologies required a lot of expertise for annotation, therefore many annotation mistakes were made (Eric et al., 2019;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_11",
            "content": "3) Collecting datasets of that size is unfeasible for development cycles in production, where new domains and models for them need to be very quickly developed and deployed.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_12",
            "content": "Current NLU Trends, inspired by such production requirements, thus deviate from previous DSToriented NLU research in two main aspects. First, the models went back to focusing on single-turn utterances, which 1) simplifies the NLU design and 2) renders the NLU tasks more tractable. 5 The requirement of fast development cycles also instigated more research on NLU (i.e., ID and SL tasks) in low-data scenarios. This way, systems can be developed and maintained faster by reducing the data collection and annotation effort. In addition, the NLU focus shifted from ontologies with only a handful of simple intents and slots (Coucke et al., 2018) to complex ontologies with much larger intent sets (Larson et al., 2019b;Liu et al., 2019b;Casanueva et al., 2020, inter alia).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_13",
            "content": "Inspired by these NLU datasets and empowered by transfer learning with PLMs and sentence encoders Liu et al., 2019a;, there have been great improvements in single-turn NLU systems recently, especially in low-data scenarios (Coope et al., 2020;Mehri et al., 2020;Wu et al., 2020b,a;Krone et al., 2020;Henderson and Vuli\u0107, 2021;Namazifar et al., 2021;Dopierre et al., 2021;Zhang et al., 2021a,b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_14",
            "content": "Current Gaps in NLU Datasets. However, existing NLU datasets are still not up to the current industry requirements. 1) They use crowdworkers for data collection and annotation, often through simple rephrasings; they thus suffer from low lexical diversity and annotation errors (Larson et al., 2019a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_15",
            "content": "2) ID datasets always assume a single intent per sentence, 6 which does not support modern production 5 While DST is theoretically more accurate, it requires amounts of data that grow exponentially with the number of turns; moreover, rule-based trackers have proven to be on par with the learned/statistical ones and require no data (Wang and Lemon, 2013). 6 There has been some work on multi-label ID on ATIS, MultiWOZ and DSTC4 as multi-intent datasets; however, their multi-label examples remain very limited, simple, and span a small number of intents (Gangadharaiah and Narayanaswamy, requirements.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_16",
            "content": "3) The ontologies of these datasets are very domain-specific (i.e., they thus do not allow data reusability) and narrow (i.e., they tend to overestimate abilities of the current SotA NLU models). 4) Current NLU datasets do not combine a large set of fine-grained intents (again, with multiintent examples) and a large set of fine-grained slots, which prevents proper and more insightful evaluations of joint NLU models Gangadharaiah and Narayanaswamy, 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_17",
            "content": "NLU++ Dataset",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "344-ARR_v1_18",
            "content": "The NLU++ dataset has been designed with the aim of addressing some of the major shortcomings of the current NLU datasets. In what follows, we describe the main improvements and new evaluation opportunities offered by NLU++.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_19",
            "content": "Ontology",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "344-ARR_v1_20",
            "content": "NLU++ comprises two domains: BANKING and HOTELS. The former represents a banking services task (e.g., making transfers, depositing cheques, reporting lost cards, requesting mortgage information) and the latter is a hotel 'bell desk' reception task (e.g., booking rooms, asking about pools or gyms, requesting room service). Both domains combine a large set of intents with a rich set of slots, with the ontologies inspired by requirements in production. A large number of intents and slots is shared between the two domains, in an attempt to increase data reusability/transferability. Table 1 provides the main statistics of the NLU++ dataset, while the full ontology is presented in Appendix A.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_21",
            "content": "Multi-Intent Examples",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "344-ARR_v1_22",
            "content": "One of the main contributions of this work is the design of the intent space, defined in a highly modular manner that natively supports intent recombinations and multi-intent annotations. For instance, Table 2 shows several multi-intent examples based 2019). Further, synthetic multi-intent datasets have been created by concatenating single-intent sentences, but such datasets also do not capture the complexity of true and natural multi-intent sentences (Qin et al., 2020). on the intent sets (termed intent modules) from Table 8 in Appendix A. This design brings several benefits. 1) The modular nature of the ontology allows for expressing a much more complex set of ideas through different combinations of intent modules (see Table 2), while reducing the overall size of the intent set compared to previous ID datasets 7 (see Table 4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_23",
            "content": "2) It allows for the definition of partial intents (e.g. \"The savings one\"). This is crucial in multi-turn interactions, where the user often has to answer disambiguation questions (e.g. \"Which account would you like to close?\").",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_24",
            "content": "3) The modular approach allows the models to generalise to unseen combinations of intent modules. 8 4) The design also allows us to distinguish between domain-specific versus generic intent modules. 9 Finally, the modular design also allows us to study semantic variation of intent modules. Some intents (e.g., especially the domain-specific ones) can only be expressed in a few ways (e.g. overdraft, direct_debit, swimming_pool), while others can have much more varied surface semantic realisations, (e.g. make, not_working). Table 8 in Appendix A provides an estimation of the semantic variability of each intent (module).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_25",
            "content": "Slots",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "344-ARR_v1_26",
            "content": "NLU++ further includes a rich set of 17 slots, defined in Table 9 in Appendix A. Table 3 displays several NLU++ examples where complex combinations of intents and slots occur, showcasing how NLU++ might provide a much more challenging environment for the evaluation of joint ID and SL models in future research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_27",
            "content": "Following the design of previous standard SL datasets (Hemphill et al., 1990;Coucke et al., 2018;Coope et al., 2020), we provide span annotations for slots. On top of of this, to also support training and evaluation of SL models which are not span-based, we also provide value annotations (or canonical values as named by Rastogi et al. (2019)) for times, dates, and numeric values.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_28",
            "content": "Similarly to intent modules, slots can also be divided into the generic ones (e.g. time, date) and the domain-specific ones (e.g company_name, rooms, kids) (see Table 9). Again, this distinction allows for the cross-domain reusability of annotated data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_29",
            "content": "Data Collection and Annotation",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "344-ARR_v1_30",
            "content": "Previous NLU datasets have usually relied on crowdworkers, aiming to collect a large numbers of examples, and typically optimising for quantity over quality. However, even with much simpler ontologies, workers are prone to make annotation mistakes, leading to very noisy datasets (Eric et al., 2019). In addition, when workers are asked to rephrase a sentence, they often change its semantic meaning or tend to provide rephrasings with extremely low lexical variability (Kang et al., 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_31",
            "content": "NLU++ reflects true production requirements and focuses on data quality. Instead of relying on crowdworkers, 4 highly skilled annotators with dialogue and NLP expertise, also familiar with production environments, collected, annotated, and corrected the data. The process started by defining the ontology for BANKING and HOTELS. Then, real user examples were fully anonymised and reannotated following the defined ontology. Finally, new examples were created in order to cover less frequent intents and slots, aiming at creating realistic and semantically varied sentences with new (Hemphill et al., 1990), SNIPS (Coucke et al., 2018), OOS (Larson et al., 2019b) and BANKING77 . combinations of intents and slots.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_32",
            "content": "Comparison with Other NLU Datasets",
            "ntype": "title",
            "meta": {
                "section": "3.5"
            }
        },
        {
            "ix": "344-ARR_v1_33",
            "content": "Aiming to reflect the differences between NLU++ and the most popular ToD NLU datasets, Table 4 compares their general statistics. Since the focus of NLU++ is on curated high-quality data, NLU++ covers a fewer number of examples than the other datasets, but it is evident that NLU++ is the only real multi-intent dataset: it averages 2.01 intents per example with a high standard deviation. In addition, NLU++ is the only dataset that combines a large set of intents with a large set of slots.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_34",
            "content": "In order to asses the quality and diversity of the NLU data, we include two additional metrics: 1) Type-Token Ratio (TTR) (Jurafsky and Martin, 2000) which measures lexical diversity) and semantic diversity. Both metrics are computed for the set of examples sharing an intent, weighted by the frequency of that intent 10 and finally averaged over intents. The semantic diversity per intent is computed as follows: (i) sentence encodings, obtained by the ConveRT sentence encoder , 11 are computed for the set of sentences sharing the same intent; (ii) the centroid of these encodings is then computed; (iii) finally, the average cosine distance from each encoding to the centroid is computed. The overall scores clearly indicate that NLU++ offers a much higher lexical and semantic diversity than previous datasets, which should also render it more challenging for current SotA NLU models. 12",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_35",
            "content": "Experiments and Results",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "344-ARR_v1_36",
            "content": "In hope to establish NLU++ as a more challenging production-oriented testbed for dialogue NLU, especially in low-data scenarios, we evaluate a series of current cutting-edge models for both NLU tasks: intent detection ( \u00a74.1) and slot labeling ( \u00a74.2). Our aim is to assess and analyse their performance across different setups, and provide solid baseline reference points for future evaluations on NLU++.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_37",
            "content": "Data Setups. Unless noted otherwise, for both tasks we adopt the standard K-fold cross-validation as done e.g. by Liu et al. (2019b). Through such folding evaluation, (i) we avoid overfitting to any particular test set and (ii) we ensure more stable results with smaller training and test data (i.e., when simulating low-data regimes typically met in production) through averaging over different folds. 13 The experiments are run with K = 20 (20-Fold) and K = 10 (10-Fold), where we train on 1 fold and evalute on the remaining K \u2212 1 folds. These setups simulate different degrees of data scarcity: e.g., the average training fold comprises \u2248 100 examples for BANKING and \u2248 50 for HOTELS for 20-Fold experiments, and twice as much for 10-Fold experiments. Besides these low-data training setups, we also run experiments in a Large-data setup, where we train the models on merged 9 folds, and evaluate on the single held-out fold. 14 The key questions we aim to answer with these data setups are: Which NLU models are better adapted to low-data scenarios? How much does NLU performance improve with the increase of annotated NLU data? How challenging is NLU++ in low-data versus large-data scenarios? Domain Setups. Further, experiments are run in the following domain setups: (i) single-domain experiments where we only use the BANKING or the HOTELS portion of the entire dataset; (ii) bothdomain experiments (termed ALL) where we use the entire dataset and combine the two domain ontologies (see Table 1); (iii) cross-domain experiments where we train on the examples associated with one domain and test on the examples from the other domain, keeping only shared intents and slots for evaluation. The key questions we aim to answer are: Are there major performance differences between the two domains and can they be merged into a single (and more complex) domain? Is it possible to use examples labeled with generic intents from one domain to boost another domain, effectively increasing reusability of data annotations and reducing data scarcity?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_38",
            "content": "F 1 (micro) is the main evaluation measure in all ID and SL experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_39",
            "content": "Intent Detection: Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "344-ARR_v1_40",
            "content": "We evaluate two groups of SotA intent detection models: (i) MLP-Based, and (ii) QA-Based ones.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_41",
            "content": "MLP-Based ID Baselines. and Gerz et al. (2021) have recently shown that, for the ID task, full and expensive fine-tuning of large pretrained models such as BERT or RoBERTa (Liu et al., 2019a) is not needed to reach strong ID performance. As an alternative, they propose a much more efficient MLP-based approach to intent detection which works on par or even outperforms full fine-tuning on the ID task. 15 In a nutshell, the idea is to use fixed/frozen \"offthe-shelf\" universal sentence encoders such as Con-veRT or Sentence-BERT (Reimers and Gurevych, 2019) models to encode input sentences. A standard multi-layer perceptron (MLP) classifier is then learnt on top of the sentence encodings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_42",
            "content": "Two core differences to the previous work stem from the fact that we now deal with the multi-label ID task: 1) to this end, we replace the output softmax layer with the sigmoid layer; and 2) we define a threshold \u03b8 which determines the final classification: only intents with probability scores \u2265 \u03b8 are taken as positives. This way, the hyper-parameter \u03b8 effectively controls the trade-off between precision and recall of the multi-label classifier.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_43",
            "content": "We comparatively evaluate several widely used state-of-the-art (SotA) sentence encoders, but remind the reader that this decoupling of the MLP classification layers from the fixed encoder allows for a much wider empirical comparison of sentence encoders in future work. The evalauted sentence encoders are: 1) CONVERT , which produces 1,024-dimensional sentence encodings; 2) LABSE (Feng et al., 2020) (768-dim); 3) ROBL-1B (1,024-dim) and 4) LM12-1B (384dim) (Reimers and Gurevych, 2019;Thakur et al., 2021). For completeness, we provide brief descriptions of each encoder in our evaluation, along with their public URLs, in Appendix B, and refer the reader to the original work for more details about each sentence encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_44",
            "content": "QA-Based ID Baselines. Another group of SotA ID baselines reformulates the ID task into the (extractive) question-answering (QA) problem (Namazifar et al., 2021). This QA-oriented reformatting then allows for additional specialised QA-tuning of large PLMs. In a nutshell, the idea is to (i) fine-tune the original PLM such as BERT/RoBERTa on readily available large generalpurpose QA data such as SQuAD (Rajpurkar et al., 2016), and then (ii) further fine-tune this general QA model with in-domain ID data. This strategy has recently shown very strong performance on single-label ATIS data (Namazifar et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_45",
            "content": "The main 'trick' is to reformat the input ID examples into the following format: \"yes. no. [SEN-TENCE]\" and pose a question such as: \"is the intent to ask about [INTENT]?\" (see Appendix A for the actual questions associated with each intent, also shared with the dataset). Here, [SENTENCE] is the placeholder for the actual input sentence, and [INTENT] is the placeholder for a short manually defined text (akin to language modeling prompts , see again Appendix A) which briefly describes the intent. The QA formulation lends itself naturally to the multi-label ID setup as each 'intent-related' question is posed separately. In other words, for each input example and for each of the L intents in the ontology the QA model must extract yes or no as the answer, where correct intent labels are the ones for which the answer is yes. 16 We note that our work is the first to apply and evaluate the QA approach on multi-label ID.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_46",
            "content": "We experiment with two pretrained language models, both fine-tuned on the SQuAD2.0 dataset (Rajpurkar et al., 2018) before additional QAtuning on NLU++ examples converted to the aforementioned QA format: ROBB-QA uses RoBERTa-Base as the underlying LM, while ALB-QA relies on the more compact ALBERT (Lan et al., 2019).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_47",
            "content": "ID: Training and Evaluation. All MLP-based baselines rely on the same training protocol and hyper-parameters in all data and domain setups. The MLP classifier consists of 1 hidden layer of size 512, and is trained via binary cross-entropy loss for 500 epochs with the batch size of 32 and the dropout rate is 0.6. We use the standard AdamW optimizer (Loshchilov and Hutter, 2018) with the learning rate of 0.003 and linear decay; weight decay is 0.02. The threshold \u03b8 is set to 0.4. 17 For QA models, we largely follow Namazifar et al. (2021) and fine-tune all models for 5 epochs, using AdamW; the learning rate of 2e\u22125 with linear decay; weight decay is 0; batch size is 32. 17",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_48",
            "content": "Slot Labeling: Experimental Setup",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "344-ARR_v1_49",
            "content": "For slot labeling, we benchmark two current SotA models: (i) ConvEx (Henderson and Vuli\u0107, 2021), as a SotA span-extraction SL model and (ii) the QA-based SL model (Namazifar et al., 2021) based on ROBB-QA, which operates similarly to QAbased ID baselines discussed in \u00a74.1, and relies on the same fine-tuning regime as our QA-based ID baselines. Again, we refer the reader to the original work for further details, and provide brief 16 For instance, for the input sentence \"I need to increase my overdraft\" from the BANKING domain, we would pose all 48 questions associated with each of the L = 48 intents in BANKING, where the QA model should extract yes as the answer for intents change, overdraft and more_higher_after, and extract no for the remaining 45 intents in BANKING. 17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups. We repeated the similar hyper-parameter search procedure for QA-based models, using ALB-QA. descriptions in Appendix D.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_50",
            "content": "Results and Discussion",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "344-ARR_v1_51",
            "content": "Main results with all the evaluated baselines are summarised in Table 5 (for ID) and Table 6",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_52",
            "content": "ID: MLP versus QA Models. First, the comparisons among only MLP-based models reveal that 1) all sentence encoders offer ID performance in similar, reasonably narrow score intervals (e.g., the variations in F 1 scores between all sentence encoders are typically below 4-6 F 1 points in all setups), and 2) that CONVERT is the best-performing sentence encoder on average, which corroborates findings from prior work on other ID datasets Wu and Xiong, 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_53",
            "content": "One very apparent and important indication in the reported results is the superiority of QA-based ID models over their MLP-based competitors. QAbased models largely outperform MLP-Based baselines in all domain setups, as well as in all data setups. The gains are visible even in Large-data setups, but the benefits of QA-based ID are immense in the lowest-data 20-Fold setups: e.g., 12 F 1 points over the strongest MLP ID model on HOTELS and 20 F 1 points on BANKING.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_54",
            "content": "Moreover, the use of larger underlying LMs might push the scores with QA even further: using SQuAD-tuned Roberta-Large (ROBL-QA) instead of Base (ROBB) yields further gains -e.g., F 1 rises from 85.6 to 87.8 on 10-Fold BANKING, and similar trends are observed in other low-data setups.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_55",
            "content": "Slot Labeling. In the SL task, the QA-based model also demonstrates its superiority, again with huge gains in low-data 20-Fold and 10-Fold setups, confirming that such QA-based or prompt-based methods Gao et al., 2021) are especially well suited for low-data setups. The use of manually defined questions/prompts, which are typically easy to write by humans, combined with the expressive power of QA-based task formatting yields immense gains on low-resource dialogue NLU.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_56",
            "content": "Given these very promising ID and SL results on NLU++, our work also calls for further and more intensive future research on QA-based models for dialogue NLU. However, we note that QA-based ID and SL methods do come with efficiency detriments, especially with larger intent and slot sets: the model must copy the input utterance and run a separate answer extraction for each intent/slot from the set, which is by several order of magnitudes more costly at both training and inference than Table 5: F 1 scores (\u00d7100%) of benchmarked state-of-the-art intent detection models on NLU++ in three data setups (see \u00a74.1). We also refer to \u00a74 for the brief descriptions of each sentence encoder (for MLP-based baselines) and the two QA-pretrained models. *All models were retrieved from the HuggingFace model repository (Wolf et al., 2020) MLP-based models. A promising future research avenue is thus to investigate combined approaches that could combine and trade off the performance benefits of QA-based models and the efficiency advantages of, e.g., MLP-based ID.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_57",
            "content": "Low-Data vs. Large-Data. We also note that scores on both tasks, as reported in Tables 5-6, leave ample room for improvement in NLU methodology in future work, especially on SL (even in Large-data setups), and in low-data setups.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_58",
            "content": "Cross-Domain Experiments. We also verify potential reusability of annotated data across domains with a simple ID experiment, where we train ID models on BANKING and evaluate on HOTELS, and vice versa. The results are summarised in Table 7. Besides (again) indicating that QA-based models outscore MLP-based ID, the results also suggest that for some generic intents it is possible to meet high ID performance without any in-domain annotations. For instance, we observe particularly high scores for highly generic and reusable intent modules such as change, how, how_much, thank, when, and affirm, all with per-intent F 1 scores \u2265 90. We hope that these preliminary results might inspire similar ontology (re)designs in future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_59",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "344-ARR_v1_60",
            "content": "We have presented NLU++, a novel dataset for task-oriented dialogue (ToD) NLU that overcomes the shortcomings of previous NLU evaluation sets. NLU++ presents a multi-intent and slot-rich ontology, defines generic and domain-specific intents and slots to promote data reusability, and it focuses on the creation of high-quality complex examples and annotations collected by dialogue experts. Experimental results show that NLU++ raises the bar with respect to current NLU benchmarks, helping better discriminate and compare the performance of current state-of-the-art NLU models, particularly in low-data setups. We hope that NLU++ will be valuable in guiding future modeling efforts for ToD NLU, both in academia and in industry.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_61",
            "content": "Limitations and Future Work. This work has shown that a better design of the intent set can improve data reusability. However, the current ontology does not cover generic sets of intents exhaustively, and we acknowledge a (sometimes) fine line between truly generic intents versus intents 'anecdotally' shared by two domains (e.g., refund). Further, NLU++ currently provides fine-grained slots such as date_from, date_to and date to enable more complex scenarios, but such a design might slow down annotation process and make it cumbersome. Future work should also look into alternatives to fine-grained slot annotations for such slots.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_62",
            "content": "Ethical Considerations During data collection: we did not include any personal information (e.g. personal names or addresses) and all the examples that included any had been fully anonymised or removed from the dataset. All the names in the dataset are created by randomly concatenating names and surnames from the list of the top 10K names from the US registry. Upon collection, the dataset has undergone an additional check by the internal Ethics committee. It is licensed under CC BY 4.0.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_63",
            "content": "The complete ontology of NLU++ is provided in Table 8 and Table 9. B Appendix: Sentence Encoders in Intent Detection Experiments",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_64",
            "content": "CONVERT is trained with the conversational response selection objective (Henderson et al., 2019b) on large Reddit data (Al-Rfou et al., 2016;Henderson et al., 2019a), spanning more than 700M (context, response) sentence pairs. Thanks to its naturally conversational pretraining objective, it has been shown to be especially well-suited for conversational tasks such as intent detection and slot labelling (Coope et al., 2020). It outputs 1,024-dim sentence encodings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_65",
            "content": "github.com/davidalami/ConveRT LABSE. Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) using a dual-encoder framework with larger embedding capacity (i.e., a shared multilingual vocabulary of 500k subwords). While LaBSE is the current state-of-the-art multilingual encoder, it also displays very strong monolingual English performance (Feng et al., 2020). It produces 768-dim sentence encodings. huggingface.co/sentence-transformers/ LaBSE ROBL-1B and LM12-1B (Reimers and Gurevych, 2019;Thakur et al., 2021) are sentence encoders which fine-tune the pretrained Roberta-Large (ROBL) language model (Liu et al., 2019a) and the 12-layer MiniLM (Wang et al., 2020b), respectively, again using a contrastive dual-encoder framework (Reimers and Gurevych, 2019). The models are fine-tuned on a set of more than 1B sentence pairs: this set comprises various data such as Reddit 2015-2018 comments (Henderson et al., 2019a), Natural Questions (Kwiatkowski et al., 2019), PAQ (question, answer) pairs (Lewis et al., 2021), to name only a few. 18 ROBL-1B outputs 18 In a nutshell, the contrastive fine-tuning task which combines all the heterogeneous datasets is as follows: given a 'query' sentence from each sentence pair, and a set of R randomly sampled negatives plus 1 true positive (the sentence from the same pair), the model should predict which sentence from the set of R + 1 sentences is actually paired with the query sentence in the dataset. The full list of all datasets along with the exact model specifications is at:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_66",
            "content": "1,024-dim encodings, while LM12-1B produces 384-dim encodings.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_67",
            "content": "We opted for those two models in particular as one represents a class of large sentence encoders (ROBL-1B), and the other is lightweight (LM12-1B), while both display very strong performance in a myriad of sentence similarity and semantic search tasks, see www.sbert.net/docs/ pretrained_models.html. We rely on the same SQuAD-tuned language models as Namazifar et al. (2021). ROBB-QA can be found online at: https://huggingface. co/deepset/roberta-base-squad2; ALB-QA is available at: https://huggingface.co/twmkn9/ albert-base-v2-squad2 D Appendix: Slot Labeling Baselines CONVEX (Henderson and Vuli\u0107, 2021) demonstrates strong SL performance, especially in fewshot settings. It is pretrained on a pairwise cloze task extracted from the Reddit examples (Henderson et al., 2019a), and the majority of the pretrained model's parameters in CONVEX are kept frozen during fine-tuning, making it an extremely efficient model. We adopt the suggested hyper-parameters from Henderson and Vuli\u0107 (2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_68",
            "content": "QA-Based: Namazifar et al. ( 2021) train an extractive QA-based model to extract the spans of the slots from the input user utterance as answers to manually defined natural language questions (one per slot). It follows the same idea as QA-based ID models. We also provide such questions for each slot along with NLU++ for model training and inference: see the questions in Table 9.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_69",
            "content": "huggingface.co/sentence-transformers/ all-roberta-large-v1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "344-ARR_v1_70",
            "content": "Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, SemEval-2012 Task 6: A pilot on semantic textual similarity, 2012, Proceedings of *SEM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Eneko Agirre",
                    "Daniel Cer",
                    "Mona Diab",
                    "Aitor Gonzalez-Agirre"
                ],
                "title": "SemEval-2012 Task 6: A pilot on semantic textual similarity",
                "pub_date": "2012",
                "pub_title": "Proceedings of *SEM",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_71",
            "content": "UNKNOWN, None, 2016, Conversational contextual cues: The case of personalization and history for response ranking, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Conversational contextual cues: The case of personalization and history for response ranking",
                "pub": "CoRR"
            }
        },
        {
            "ix": "344-ARR_v1_72",
            "content": "Wei Bi, James Tin-Yau Kwok, Efficient multilabel classification with many labels, 2013-06, Proceedings of the 30th International Conference on Machine Learning, ICML 2013, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Wei Bi",
                    "James Tin-Yau Kwok"
                ],
                "title": "Efficient multilabel classification with many labels",
                "pub_date": "2013-06",
                "pub_title": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_73",
            "content": "Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Milica Osman Ramadan,  Ga\u0161i\u0107, MultiWOZ -A large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling, 2018, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Pawe\u0142 Budzianowski",
                    "Tsung-Hsien Wen",
                    "Bo-Hsiang Tseng",
                    "I\u00f1igo Casanueva",
                    "Stefan Ultes",
                    "Milica Osman Ramadan",
                    " Ga\u0161i\u0107"
                ],
                "title": "MultiWOZ -A large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling",
                "pub_date": "2018",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_74",
            "content": "I\u00f1igo Casanueva, Tadas Temcinas, Daniela Gerz, Matthew Henderson, Ivan Vuli\u0107, Efficient intent detection with dual sentence encoders, 2020, Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "I\u00f1igo Casanueva",
                    "Tadas Temcinas",
                    "Daniela Gerz",
                    "Matthew Henderson",
                    "Ivan Vuli\u0107"
                ],
                "title": "Efficient intent detection with dual sentence encoders",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_75",
            "content": "UNKNOWN, None, 2019, Bert for joint intent classification and slot filling, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Bert for joint intent classification and slot filling",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_76",
            "content": "Sam Coope, Tyler Farghly, Daniela Gerz, Ivan Vuli\u0107, Matthew Henderson, Span-ConveRT: Fewshot span extraction for dialog with pretrained conversational representations, 2020, Proceedings of ACL 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Sam Coope",
                    "Tyler Farghly",
                    "Daniela Gerz",
                    "Ivan Vuli\u0107",
                    "Matthew Henderson"
                ],
                "title": "Span-ConveRT: Fewshot span extraction for dialog with pretrained conversational representations",
                "pub_date": "2020",
                "pub_title": "Proceedings of ACL 2020",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_77",
            "content": "UNKNOWN, None, 2018, Snips Voice Platform: An embedded spoken language understanding system for private-by-design voice interfaces, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Snips Voice Platform: An embedded spoken language understanding system for private-by-design voice interfaces",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_78",
            "content": "UNKNOWN, None, , 2021. Preview, attend and review: Schema-aware curriculum learning for multi-domain dialog state tracking, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "2021. Preview, attend and review: Schema-aware curriculum learning for multi-domain dialog state tracking",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_79",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of NAACL-HLT 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of NAACL-HLT 2019",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_80",
            "content": "B William, Chris Dolan,  Brockett, Automatically constructing a corpus of sentential paraphrases, 2005, Proceedings of the Third International Workshop on Paraphrasing (IWP2005), .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "B William",
                    "Chris Dolan",
                    " Brockett"
                ],
                "title": "Automatically constructing a corpus of sentential paraphrases",
                "pub_date": "2005",
                "pub_title": "Proceedings of the Third International Workshop on Paraphrasing (IWP2005)",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_81",
            "content": "Thomas Dopierre, Christophe Gravier, Wilfried Logerais, PROTAUGMENT: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Thomas Dopierre",
                    "Christophe Gravier",
                    "Wilfried Logerais"
                ],
                "title": "PROTAUGMENT: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "344-ARR_v1_82",
            "content": "Layla Asri, Hannes Schulz, Shikhar Sharma, Jeremie Zumer, Justin Harris, Emery Fine, Rahul Mehrotra, Kaheer Suleman, Frames: A corpus for adding memory to goal-oriented dialogue systems, 2017, Proceedings of SIGDIAL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Layla Asri",
                    "Hannes Schulz",
                    "Shikhar Sharma",
                    "Jeremie Zumer",
                    "Justin Harris",
                    "Emery Fine",
                    "Rahul Mehrotra",
                    "Kaheer Suleman"
                ],
                "title": "Frames: A corpus for adding memory to goal-oriented dialogue systems",
                "pub_date": "2017",
                "pub_title": "Proceedings of SIGDIAL",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_83",
            "content": "UNKNOWN, None, 1669, Multiwoz 2.1: Multi-domain dialogue state corrections and state tracking baselines, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "1669",
                "pub_title": "Multiwoz 2.1: Multi-domain dialogue state corrections and state tracking baselines",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_84",
            "content": "Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, Wei Wang, Language-agnostic BERT sentence embedding. CoRR, abs/2007.01852. Rashmi Gangadharaiah and Balakrishnan Narayanaswamy, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Fangxiaoyu Feng",
                    "Yinfei Yang",
                    "Daniel Cer",
                    "Naveen Arivazhagan",
                    "Wei Wang"
                ],
                "title": "Language-agnostic BERT sentence embedding. CoRR, abs/2007.01852. Rashmi Gangadharaiah and Balakrishnan Narayanaswamy",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "344-ARR_v1_85",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Tianyu Gao",
                    "Adam Fisch",
                    "Danqi Chen"
                ],
                "title": "Making pre-trained language models better few-shot learners",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "344-ARR_v1_86",
            "content": "Daniela Gerz, Pei-Hao Su, Razvan Kusztos, Avishek Mondal, Michal Lis, Eshan Singhal, Nikola Mrk\u0161i\u0107, Tsung-Hsien Wen, and Ivan Vuli\u0107. 2021. Multilingual and cross-lingual intent detection from spoken data, , Proceedings of EMNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Daniela Gerz",
                    "Pei-Hao Su",
                    "Razvan Kusztos",
                    "Avishek Mondal",
                    "Michal Lis",
                    "Eshan Singhal",
                    "Nikola Mrk\u0161i\u0107"
                ],
                "title": "Tsung-Hsien Wen, and Ivan Vuli\u0107. 2021. Multilingual and cross-lingual intent detection from spoken data",
                "pub_date": null,
                "pub_title": "Proceedings of EMNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_87",
            "content": "Arshit Gupta, John Hewitt, Katrin Kirchhoff, Simple, fast, accurate intent classification and slot labeling for goal-oriented dialogue systems, 2019, Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Arshit Gupta",
                    "John Hewitt",
                    "Katrin Kirchhoff"
                ],
                "title": "Simple, fast, accurate intent classification and slot labeling for goal-oriented dialogue systems",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_88",
            "content": "UNKNOWN, None, 2020, Trippy: A triple copy strategy for value independent neural dialog state tracking, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Trippy: A triple copy strategy for value independent neural dialog state tracking",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_89",
            "content": "Charles Hemphill, John Godfrey, George Doddington, The ATIS Spoken Language Systems Pilot Corpus, 1990, Proceedings of the Workshop on Speech and Natural Language, HLT '90, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Charles Hemphill",
                    "John Godfrey",
                    "George Doddington"
                ],
                "title": "The ATIS Spoken Language Systems Pilot Corpus",
                "pub_date": "1990",
                "pub_title": "Proceedings of the Workshop on Speech and Natural Language, HLT '90",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_90",
            "content": "Matthew Henderson, Pawel Budzianowski, I\u00f1igo Casanueva, Sam Coope, Daniela Gerz, Girish Kumar, Nikola Mrk\u0161i\u0107, Georgios Spithourakis, Pei-Hao Su, Ivan Vuli\u0107, Tsung-Hsien Wen, A repository of conversational datasets, 2019, Proceedings of the 1st Workshop on Natural Language Processing for Conversational AI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Matthew Henderson",
                    "Pawel Budzianowski",
                    "I\u00f1igo Casanueva",
                    "Sam Coope",
                    "Daniela Gerz",
                    "Girish Kumar",
                    "Nikola Mrk\u0161i\u0107",
                    "Georgios Spithourakis",
                    "Pei-Hao Su",
                    "Ivan Vuli\u0107",
                    "Tsung-Hsien Wen"
                ],
                "title": "A repository of conversational datasets",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 1st Workshop on Natural Language Processing for Conversational AI",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_91",
            "content": "Matthew Henderson, I\u00f1igo Casanueva, Nikola Mrk\u0161i\u0107, Pei-Hao Su, Tsung-Hsien Wen, Ivan Vuli\u0107, ConveRT: Efficient and accurate conversational representations from transformers, 2020, Findings of EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Matthew Henderson",
                    "I\u00f1igo Casanueva",
                    "Nikola Mrk\u0161i\u0107",
                    "Pei-Hao Su",
                    "Tsung-Hsien Wen",
                    "Ivan Vuli\u0107"
                ],
                "title": "ConveRT: Efficient and accurate conversational representations from transformers",
                "pub_date": "2020",
                "pub_title": "Findings of EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_92",
            "content": "Matthew Henderson, Blaise Thomson, Jason Wiliams, The Second Dialog State Tracking Challenge, 2014, Proceedings of SIGDIAL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Matthew Henderson",
                    "Blaise Thomson",
                    "Jason Wiliams"
                ],
                "title": "The Second Dialog State Tracking Challenge",
                "pub_date": "2014",
                "pub_title": "Proceedings of SIGDIAL",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_93",
            "content": "Matthew Henderson, Ivan Vuli\u0107, ConVEx: Data-efficient and few-shot slot labeling, 2021, Proceedings of NAACL-HLT 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Matthew Henderson",
                    "Ivan Vuli\u0107"
                ],
                "title": "ConVEx: Data-efficient and few-shot slot labeling",
                "pub_date": "2021",
                "pub_title": "Proceedings of NAACL-HLT 2021",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_94",
            "content": "Matthew Henderson, Ivan Vuli\u0107, Daniela Gerz, I\u00f1igo Casanueva, Pawe\u0142 Budzianowski, Sam Coope, Georgios Spithourakis, Tsung-Hsien Wen, Nikola Mrk\u0161i\u0107, Pei-Hao Su, Training neural response selection for task-oriented dialogue systems, 2019, Proceedings of ACL 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Matthew Henderson",
                    "Ivan Vuli\u0107",
                    "Daniela Gerz",
                    "I\u00f1igo Casanueva",
                    "Pawe\u0142 Budzianowski",
                    "Sam Coope",
                    "Georgios Spithourakis",
                    "Tsung-Hsien Wen",
                    "Nikola Mrk\u0161i\u0107",
                    "Pei-Hao Su"
                ],
                "title": "Training neural response selection for task-oriented dialogue systems",
                "pub_date": "2019",
                "pub_title": "Proceedings of ACL 2019",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_95",
            "content": "UNKNOWN, None, 2020, Few-shot learning for multilabel intent detection, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Few-shot learning for multilabel intent detection",
                "pub": "CoRR"
            }
        },
        {
            "ix": "344-ARR_v1_96",
            "content": "Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin, The use of external knowledge in factoid qa, 2001, TREC, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Eduard Hovy",
                    "Ulf Hermjakob",
                    "Chin-Yew Lin"
                ],
                "title": "The use of external knowledge in factoid qa",
                "pub_date": "2001",
                "pub_title": "TREC",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_97",
            "content": "UNKNOWN, None, 2000, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, Prentice Hall PTR.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2000",
                "pub_title": "Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition",
                "pub": "Prentice Hall PTR"
            }
        },
        {
            "ix": "344-ARR_v1_98",
            "content": "Yiping Kang, Yunqi Zhang, Jonathan Kummerfeld, Lingjia Tang, Jason Mars, Data collection for dialogue system: A startup perspective, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Yiping Kang",
                    "Yunqi Zhang",
                    "Jonathan Kummerfeld",
                    "Lingjia Tang",
                    "Jason Mars"
                ],
                "title": "Data collection for dialogue system: A startup perspective",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_99",
            "content": "Seokhwan Kim, Luis Fernando, D' Haro, Rafael Banchs, Jason Williams, Matthew Henderson, The Fourth Dialog State Tracking Challenge, 2016, Proceedings of IWSDS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Seokhwan Kim",
                    "Luis Fernando",
                    "D' Haro",
                    "Rafael Banchs",
                    "Jason Williams",
                    "Matthew Henderson"
                ],
                "title": "The Fourth Dialog State Tracking Challenge",
                "pub_date": "2016",
                "pub_title": "Proceedings of IWSDS",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_100",
            "content": "Jason Krone, Yi Zhang, Mona Diab, Learning to classify intents and slot labels given a handful of examples, 2020, Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Jason Krone",
                    "Yi Zhang",
                    "Mona Diab"
                ],
                "title": "Learning to classify intents and slot labels given a handful of examples",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_101",
            "content": "UNKNOWN, None, 2019, Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_102",
            "content": "UNKNOWN, None, 2019, Albert: A Lite BERT for selfsupervised learning of language representations, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Albert: A Lite BERT for selfsupervised learning of language representations",
                "pub": "CoRR"
            }
        },
        {
            "ix": "344-ARR_v1_103",
            "content": "UNKNOWN, None, , Lingjia Tang, and Jason Mars. 2019a. Outlier detection for improved data quality and diversity in dialog systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Lingjia Tang, and Jason Mars. 2019a. Outlier detection for improved data quality and diversity in dialog systems",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_104",
            "content": "Stefan Larson, Anish Mahendran, Joseph Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan Kummerfeld, Kevin Leach, Michael Laurenzano, Lingjia Tang, Jason Mars, An evaluation dataset for intent classification and out-of-scope prediction, 2019, Proceedings of EMNLP-IJCNLP 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Stefan Larson",
                    "Anish Mahendran",
                    "Joseph Peper",
                    "Christopher Clarke",
                    "Andrew Lee",
                    "Parker Hill",
                    "Jonathan Kummerfeld",
                    "Kevin Leach",
                    "Michael Laurenzano",
                    "Lingjia Tang",
                    "Jason Mars"
                ],
                "title": "An evaluation dataset for intent classification and out-of-scope prediction",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP-IJCNLP 2019",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_105",
            "content": "E Levin, R Pieraccini, Chronus, the next generation, 1995, Proceedings of the ARPA Workshop on Spoken Language Technology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "E Levin",
                    "R Pieraccini"
                ],
                "title": "Chronus, the next generation",
                "pub_date": "1995",
                "pub_title": "Proceedings of the ARPA Workshop on Spoken Language Technology",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_106",
            "content": "Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich K\u00fcttler, Aleksandra Piktus, Pontus Stenetorp, Sebastian Riedel, PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Patrick Lewis",
                    "Yuxiang Wu",
                    "Linqing Liu",
                    "Pasquale Minervini",
                    "Heinrich K\u00fcttler",
                    "Aleksandra Piktus",
                    "Pontus Stenetorp",
                    "Sebastian Riedel"
                ],
                "title": "PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them",
                "pub_date": "2021",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_107",
            "content": "UNKNOWN, None, 2021, Pretrain, prompt, and predict: A systematic survey of prompting methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_108",
            "content": "UNKNOWN, None, 1901, Multi-task deep neural networks for natural language understanding. CoRR, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": null,
                "title": null,
                "pub_date": "1901",
                "pub_title": "Multi-task deep neural networks for natural language understanding. CoRR, abs",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_109",
            "content": "Xingkun Liu, Arash Eshghi, Pawel Swietojanski, Verena Rieser, Benchmarking natural language understanding services for building conversational agents, 2019, Proceedings of IWSDS 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Xingkun Liu",
                    "Arash Eshghi",
                    "Pawel Swietojanski",
                    "Verena Rieser"
                ],
                "title": "Benchmarking natural language understanding services for building conversational agents",
                "pub_date": "2019",
                "pub_title": "Proceedings of IWSDS 2019",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_110",
            "content": "UNKNOWN, None, 1907, RoBERTa: A robustly optimized BERT pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": null,
                "title": null,
                "pub_date": "1907",
                "pub_title": "RoBERTa: A robustly optimized BERT pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_111",
            "content": "Ilya Loshchilov, Frank Hutter, Decoupled weight decay regularization, 2018, Proceedings of ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Ilya Loshchilov",
                    "Frank Hutter"
                ],
                "title": "Decoupled weight decay regularization",
                "pub_date": "2018",
                "pub_title": "Proceedings of ICLR",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_112",
            "content": "Nurul Lubis, Christian Geishauser, Michael Heck, Hsien-Chin Lin, Marco Moresi, Carel van Niekerk, and Milica Gasic. 2020. Lava: Latent action spaces via variational auto-encoding for dialogue policy optimization, , Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Nurul Lubis",
                    "Christian Geishauser",
                    "Michael Heck",
                    "Hsien-Chin Lin",
                    "Marco Moresi"
                ],
                "title": "Carel van Niekerk, and Milica Gasic. 2020. Lava: Latent action spaces via variational auto-encoding for dialogue policy optimization",
                "pub_date": null,
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_113",
            "content": "UNKNOWN, None, 2020, Example-driven intent prediction with observers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Example-driven intent prediction with observers",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_114",
            "content": "Mahdi Namazifar, Alexandros Papangelis, Gokhan Tur, Dilek Hakkani-T\u00fcr, Language model is all you need: Natural language understanding as question answering, 2021, Proceedings of ICASSP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Mahdi Namazifar",
                    "Alexandros Papangelis",
                    "Gokhan Tur",
                    "Dilek Hakkani-T\u00fcr"
                ],
                "title": "Language model is all you need: Natural language understanding as question answering",
                "pub_date": "2021",
                "pub_title": "Proceedings of ICASSP 2021",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_115",
            "content": "Denis Peskov, Nancy Clarke, Jason Krone, Brigi Fodor, Yi Zhang, Adel Youssef, Mona Diab, Multi-domain goal-oriented dialogues (multidogo): Strategies toward curating and annotating large scale dialogue data, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Denis Peskov",
                    "Nancy Clarke",
                    "Jason Krone",
                    "Brigi Fodor",
                    "Yi Zhang",
                    "Adel Youssef",
                    "Mona Diab"
                ],
                "title": "Multi-domain goal-oriented dialogues (multidogo): Strategies toward curating and annotating large scale dialogue data",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_116",
            "content": "UNKNOWN, None, 2020, Agif: An adaptive graph-interactive framework for joint multiple intent detection and slot filling, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Agif: An adaptive graph-interactive framework for joint multiple intent detection and slot filling",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_117",
            "content": "Pranav Rajpurkar, Robin Jia, Percy Liang, Know what you don't know: Unanswerable questions for SQuAD, 2018, Proceedings of ACL 2018, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Pranav Rajpurkar",
                    "Robin Jia",
                    "Percy Liang"
                ],
                "title": "Know what you don't know: Unanswerable questions for SQuAD",
                "pub_date": "2018",
                "pub_title": "Proceedings of ACL 2018",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_118",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100,000+ questions for Machine Comprehension of Text, 2016, Proceedings of EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Pranav Rajpurkar",
                    "Jian Zhang",
                    "Konstantin Lopyrev",
                    "Percy Liang"
                ],
                "title": "SQuAD: 100,000+ questions for Machine Comprehension of Text",
                "pub_date": "2016",
                "pub_title": "Proceedings of EMNLP",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_119",
            "content": "UNKNOWN, None, 2019, Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_120",
            "content": "Antoine Raux, Brian Langner, Alan Black, Maxine Esk\u00e9nazi, LET's GO: Improving spoken dialog systems for the elderly and non-natives, 2003, Proceedings of EUROSPEECH, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Antoine Raux",
                    "Brian Langner",
                    "Alan Black",
                    "Maxine Esk\u00e9nazi"
                ],
                "title": "LET's GO: Improving spoken dialog systems for the elderly and non-natives",
                "pub_date": "2003",
                "pub_title": "Proceedings of EUROSPEECH",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_121",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-BERT: Sentence embeddings using Siamese BERTnetworks, 2019, Proceedings of EMNLP 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "Nils Reimers",
                    "Iryna Gurevych"
                ],
                "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP 2019",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_122",
            "content": "Nandan Thakur, Nils Reimers, Johannes Daxenberger, Iryna Gurevych, Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "Nandan Thakur",
                    "Nils Reimers",
                    "Johannes Daxenberger",
                    "Iryna Gurevych"
                ],
                "title": "Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_123",
            "content": "Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, Jianxing Yu, Multi-domain dialogue acts and response co-generation, 2020, Proceedings of the 58th, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Kai Wang",
                    "Junfeng Tian",
                    "Rui Wang",
                    "Xiaojun Quan",
                    "Jianxing Yu"
                ],
                "title": "Multi-domain dialogue acts and response co-generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_124",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_125",
            "content": "UNKNOWN, None, 2020, MiniLM: Deep selfattention distillation for task-agnostic compression of pre-trained Transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "MiniLM: Deep selfattention distillation for task-agnostic compression of pre-trained Transformers",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_126",
            "content": "Zhuoran Wang, Oliver Lemon, A Simple and Generic Belief Tracking Mechanism for the Dialog State Tracking Challenge: On the believability of observed information, 2013, Proceedings of SIGDIAL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Zhuoran Wang",
                    "Oliver Lemon"
                ],
                "title": "A Simple and Generic Belief Tracking Mechanism for the Dialog State Tracking Challenge: On the believability of observed information",
                "pub_date": "2013",
                "pub_title": "Proceedings of SIGDIAL",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_127",
            "content": "Wei Wei, Quoc Le, Andrew Dai, Jia Li, Airdialogue: An environment for goal-oriented dialogue research, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Wei Wei",
                    "Quoc Le",
                    "Andrew Dai",
                    "Jia Li"
                ],
                "title": "Airdialogue: An environment for goal-oriented dialogue research",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_128",
            "content": "Jason Williams, Antoine Raux, Deepak Ramachandran, Alan Black, The Dialogue State Tracking Challenge, 2013, Proceedings of SIGDIAL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Jason Williams",
                    "Antoine Raux",
                    "Deepak Ramachandran",
                    "Alan Black"
                ],
                "title": "The Dialogue State Tracking Challenge",
                "pub_date": "2013",
                "pub_title": "Proceedings of SIGDIAL",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_129",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_130",
            "content": "Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of EMNLP 2020: System Demonstrations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": [
                    "Alexander Lhoest",
                    " Rush"
                ],
                "title": "Transformers: State-of-the-art natural language processing",
                "pub_date": "2020",
                "pub_title": "Proceedings of EMNLP 2020: System Demonstrations",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_131",
            "content": "Chien-Sheng Wu, C Steven, Richard Hoi, Caiming Socher,  Xiong, TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b61",
                "authors": [
                    "Chien-Sheng Wu",
                    "C Steven",
                    "Richard Hoi",
                    "Caiming Socher",
                    " Xiong"
                ],
                "title": "TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "344-ARR_v1_132",
            "content": "Sheng Chien, Caiming Wu,  Xiong, Probing task-oriented dialogue representation from language models, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b62",
                "authors": [
                    "Sheng Chien",
                    "Caiming Wu",
                    " Xiong"
                ],
                "title": "Probing task-oriented dialogue representation from language models",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "344-ARR_v1_133",
            "content": "Di Wu, Liang Ding, Fan Lu, Jian Xie, SlotRefine: A fast non-autoregressive model for joint intent detection and slot filling, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b63",
                "authors": [
                    "Di Wu",
                    "Liang Ding",
                    "Fan Lu",
                    "Jian Xie"
                ],
                "title": "SlotRefine: A fast non-autoregressive model for joint intent detection and slot filling",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "344-ARR_v1_134",
            "content": "Weijia Xu, Batool Haider, Saab Mansour, End-to-end slot alignment and recognition for crosslingual NLU, 2020, Proceedings of EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b64",
                "authors": [
                    "Weijia Xu",
                    "Batool Haider",
                    "Saab Mansour"
                ],
                "title": "End-to-end slot alignment and recognition for crosslingual NLU",
                "pub_date": "2020",
                "pub_title": "Proceedings of EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_135",
            "content": "Yinfei Yang, Gustavo Hernandez Abrego, Steve Yuan, Mandy Guo, Qinlan Shen, Daniel Cer, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil, Improving multilingual sentence embedding using bidirectional dual encoder with additive margin softmax, 2019, Proceedings of IJCAI 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b65",
                "authors": [
                    "Yinfei Yang",
                    "Gustavo Hernandez Abrego",
                    "Steve Yuan",
                    "Mandy Guo",
                    "Qinlan Shen",
                    "Daniel Cer",
                    "Yun-Hsuan Sung",
                    "Brian Strope",
                    "Ray Kurzweil"
                ],
                "title": "Improving multilingual sentence embedding using bidirectional dual encoder with additive margin softmax",
                "pub_date": "2019",
                "pub_title": "Proceedings of IJCAI 2019",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_136",
            "content": "Steve Young, Cognitive user interfaces, 2010, IEEE Signal Processing Magazine, .",
            "ntype": "ref",
            "meta": {
                "xid": "b66",
                "authors": [
                    "Steve Young"
                ],
                "title": "Cognitive user interfaces",
                "pub_date": "2010",
                "pub_title": "IEEE Signal Processing Magazine",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_137",
            "content": "UNKNOWN, None, 2002, The HTK book, .",
            "ntype": "ref",
            "meta": {
                "xid": "b67",
                "authors": null,
                "title": null,
                "pub_date": "2002",
                "pub_title": "The HTK book",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_138",
            "content": "UNKNOWN, None, 2020, Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines, .",
            "ntype": "ref",
            "meta": {
                "xid": "b68",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines",
                "pub": null
            }
        },
        {
            "ix": "344-ARR_v1_139",
            "content": "Haode Zhang, Yuwei Zhang, Li-Ming Zhan, Jiaxin Chen, Guangyuan Shi, Xiao-Ming Wu, Albert Lam, Effectiveness of pre-training for few-shot intent classification, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b69",
                "authors": [
                    "Haode Zhang",
                    "Yuwei Zhang",
                    "Li-Ming Zhan",
                    "Jiaxin Chen",
                    "Guangyuan Shi",
                    "Xiao-Ming Wu",
                    "Albert Lam"
                ],
                "title": "Effectiveness of pre-training for few-shot intent classification",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "344-ARR_v1_140",
            "content": "Jianguo Zhang, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Hung Quan, Walter Tran, Philip Chang,  Yu, Few-shot intent detection via contrastive pre-training and fine-tuning, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b70",
                "authors": [
                    "Jianguo Zhang",
                    "Trung Bui",
                    "Seunghyun Yoon",
                    "Xiang Chen",
                    "Zhiwei Liu",
                    "Congying Xia",
                    "Hung Quan",
                    "Walter Tran",
                    "Philip Chang",
                    " Yu"
                ],
                "title": "Few-shot intent detection via contrastive pre-training and fine-tuning",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "344-ARR_v1_141",
            "content": "Jinghan Zhang, Yuxiao Ye, Yue Zhang, Likun Qiu, Bin Fu, Yang Li, Zhenglu Yang, Jian Sun, Multi-point semantic representation for intent classification, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b71",
                "authors": [
                    "Jinghan Zhang",
                    "Yuxiao Ye",
                    "Yue Zhang",
                    "Likun Qiu",
                    "Bin Fu",
                    "Yang Li",
                    "Zhenglu Yang",
                    "Jian Sun"
                ],
                "title": "Multi-point semantic representation for intent classification",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "344-ARR_v1_0@0",
            "content": "NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_0",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_2@0",
            "content": "We present NLU++, a novel dataset for natural language understanding (NLU) in taskoriented dialogue (ToD) systems, with the aim to provide a much more challenging evaluation environment for dialogue NLU models, up to date with the current application and industry requirements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_2",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_2@1",
            "content": "NLU++ is divided into two domains (BANKING and HOTELS) and brings several crucial improvements over current commonly used NLU datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_2",
            "start": 278,
            "end": 412,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_2@2",
            "content": "1) NLU++ provides fine-grained domain ontologies with a large set of challenging multi-intent sentences combined with finer-grained and thus more challenging slot sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_2",
            "start": 414,
            "end": 581,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_2@3",
            "content": "2) The ontology is divided into domain-specific and generic (i.e., domainuniversal) intents that overlap across domains, promoting cross-domain reusability of annotated examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_2",
            "start": 583,
            "end": 760,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_3@0",
            "content": "3) The dataset design has been inspired by the problems observed in industrial ToD systems, and 4) it has been collected, filtered and carefully annotated by dialogue NLU experts, yielding high-quality annotated data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_3",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_3@1",
            "content": "Finally, we benchmark a series of current state-of-the-art NLU models on NLU++; the results demonstrate the challenging nature of the dataset, especially in low-data regimes, and call for further research on ToD NLU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_3",
            "start": 218,
            "end": 433,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_4@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_4",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@0",
            "content": "Research on task-oriented dialogue (ToD) systems (Levin and Pieraccini, 1995;Young et al., 2002) has become a key aspect in industry: e.g., ToD is used to automate telephone customer service tasks ranging from hospitality over healthcare to banking (Raux et al., 2003;Young, 2010;El Asri et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@1",
            "content": "Typical ToD systems still rely on a modular design: (i) the Natural Language Understanding (NLU) module maps user utterances into a domainspecific set of intent labels and values (Rastogi et al., 2019;Heck et al., 2020;Dai et al., 2021), followed by (ii) the policy module, which makes decisions based on the information extracted by the NLU (Lubis et al., 2020;Wang et al., 2020a) Yes, I need this card to arrive before 3pm on Jan 14 The NLU module is a critical part of any ToD system, as it must extract the relevant information from the user's utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 303,
            "end": 861,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@2",
            "content": "The information relevance is denoted by the structured dialogue domain ontology, which enables the policy module to make decisions about next system actions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 863,
            "end": 1019,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@3",
            "content": "The domain ontology covers the information on 1) intents and 2) slots, see Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 1021,
            "end": 1104,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@4",
            "content": "The former is aimed at extracting general conversational ideas (i.e., the user's intents) and corresponds to the standard NLU task of intent detection (ID); the latter extracts specific slot values and corresponds to the NLU task of slot labeling (SL) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 1106,
            "end": 1358,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@5",
            "content": "1 In order to make the policy operational and tractable, NLU should extract only the minimal information required by the policy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 1360,
            "end": 1487,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@6",
            "content": "Therefore, the ontologies differ for each domain of ToD application and are typically built from scratch for each domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 1489,
            "end": 1609,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@7",
            "content": "Consequently, this makes domain-relevant NLU data extremely expensive to collect and annotate, and prevents its reusability (Budzianowski et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 1611,
            "end": 1762,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@8",
            "content": "Due to this, NLU research in recent years has heavily focused on very data-efficient models that can effectively operate in low-data regimes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 1764,
            "end": 1904,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@9",
            "content": "Current state-of-the-art (SotA) NLU models leverage large pretrained language models (PLMs) (De-vlin et al., 2019;Liu et al., 2019c; and fine-tune them with small task-specific datasets (Larson et al., 2019b;Coucke et al., 2018) At the same time, the progress in creation of NLU datasets has not kept up with the impressive pace of NLU methodology development.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 1906,
            "end": 2265,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@10",
            "content": "However, designing domain ontologies and NLU datasets is also critical for steering further progress in NLU, both from methodology and application perspective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 2267,
            "end": 2425,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@11",
            "content": "Put simply, current publicly available NLU datasets do not keep up to date with current industry/application requirements for many reasons.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 2427,
            "end": 2565,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@12",
            "content": "1) They are usually crowdsourced by untrained annotators (thus typically optimised for quantity rather than quality), yielding examples with low lexical diversity and prone to annotation errors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 2567,
            "end": 2760,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@13",
            "content": "2) They typically assume one intent per example, and thus enable only much simpler single-label ID experiments; such setups are not realistic in more complex industry settings (see Figure 1 again) and lead to unnecessarily large intent sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 2762,
            "end": 3002,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@14",
            "content": "3) Their ontologies are tied to specific domains, making it difficult to reuse already available annotated data in other domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 3004,
            "end": 3132,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@15",
            "content": "4) The complexity of the defined tasks and ontologies is limited; the undesired artefact is that current NLU datasets might overestimate the NLU models' abilities, and are not able to separate models any more performance-wise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 3134,
            "end": 3359,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@16",
            "content": "2 In order to address all these gaps, we introduce NLU++, a novel NLU dataset which provides highquality NLU data annotated by dialogue experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 3361,
            "end": 3504,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@17",
            "content": "NLU++ provides multi-intent, slot-rich and semantically varied NLU data, and is inspired by a number of NLU challenges which ToD systems typically face in production environments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 3506,
            "end": 3684,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@18",
            "content": "Unlike previous ID datasets, examples are annotated with multiple labels, with some examples naturally obtaining even up to 6-7 labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 3686,
            "end": 3820,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@19",
            "content": "NLU++ defines a rich set of slots which are combined with multi-intent sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 3822,
            "end": 3902,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@20",
            "content": "NLU++ is divided into two domains (BANKING and HOTELS) where the two domain ontologies blend a set of domain-specific intents and slots with a set of generic (i.e., domain-universal) intents and slots.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 3904,
            "end": 4104,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_5@21",
            "content": "This design makes a crucial step towards generalisation and data reusability in NLU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_5",
            "start": 4106,
            "end": 4189,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_6@0",
            "content": "Finally, we run a series of experiments on NLU++ with current SotA ID and SL models, demonstrating the challenging nature of NLU++ and ample room for future improvement, especially in lowdata setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_6",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_6@1",
            "content": "Our benchmark comparisons also demonstrate strong performance and shed new light on the (ability of) recently emerging QA-based NLU models, and warrant further research on ToD NLU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_6",
            "start": 200,
            "end": 379,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_6@2",
            "content": "The NLU++ dataset is available at: [URL] under CC BY 4.0 license.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_6",
            "start": 381,
            "end": 445,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_7@0",
            "content": "Background and Motivation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_7",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_8@0",
            "content": "A Brief History of NLU Datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_8",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_8@1",
            "content": "As a core module of ToD systems, NLU has been researched since the early 1990s, when the Airline Travel Information System (ATIS) project was started (Hemphill et al., 1990), consisting of spoken queries on flightrelated information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_8",
            "start": 33,
            "end": 265,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_8@2",
            "content": "3 Over the next two decades, very few NLU resources were released.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_8",
            "start": 267,
            "end": 332,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_8@3",
            "content": "4 The lack of ToD NLU resources ended in 2013, with the beginning of the 'dialogue state tracking (DST) era' (Williams et al., 2013;Henderson et al., 2014;Kim et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_8",
            "start": 334,
            "end": 506,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_8@4",
            "content": "Instead of just classifying each turn of the user, DST deals with keeping track of the user's goal over the entire dialogue history, i.e., all the previous user and system turns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_8",
            "start": 508,
            "end": 685,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_8@5",
            "content": "Several datasets where released during the DST challenges, all of them comprising simple intent sets (usually tagged as dialogue acts).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_8",
            "start": 687,
            "end": 821,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_9@0",
            "content": "To adapt to the increasing data requirements of deep learning models, increasingly larger dialogue datasets have been released in recent years (Budzianowski et al., 2018;Wei et al., 2018;Rastogi et al., 2019;Peskov et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_9",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_9@1",
            "content": "However, the design of ToD datasets comes with some profound differences to datasets for e.g. machine translation or speech recognition, which affect current ToD datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_9",
            "start": 230,
            "end": 400,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_9@2",
            "content": "1) The domain-specific nature of ToD datasets made the data tied to its ontologies, not allowing data reusability across different domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_9",
            "start": 402,
            "end": 540,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_10@0",
            "content": "2) The domain-specific ontologies required a lot of expertise for annotation, therefore many annotation mistakes were made (Eric et al., 2019;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_10",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_11@0",
            "content": "3) Collecting datasets of that size is unfeasible for development cycles in production, where new domains and models for them need to be very quickly developed and deployed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_11",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_12@0",
            "content": "Current NLU Trends, inspired by such production requirements, thus deviate from previous DSToriented NLU research in two main aspects.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_12",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_12@1",
            "content": "First, the models went back to focusing on single-turn utterances, which 1) simplifies the NLU design and 2) renders the NLU tasks more tractable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_12",
            "start": 135,
            "end": 280,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_12@2",
            "content": "5 The requirement of fast development cycles also instigated more research on NLU (i.e., ID and SL tasks) in low-data scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_12",
            "start": 282,
            "end": 409,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_12@3",
            "content": "This way, systems can be developed and maintained faster by reducing the data collection and annotation effort.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_12",
            "start": 411,
            "end": 521,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_12@4",
            "content": "In addition, the NLU focus shifted from ontologies with only a handful of simple intents and slots (Coucke et al., 2018) to complex ontologies with much larger intent sets (Larson et al., 2019b;Liu et al., 2019b;Casanueva et al., 2020, inter alia).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_12",
            "start": 523,
            "end": 770,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_13@0",
            "content": "Inspired by these NLU datasets and empowered by transfer learning with PLMs and sentence encoders Liu et al., 2019a;, there have been great improvements in single-turn NLU systems recently, especially in low-data scenarios (Coope et al., 2020;Mehri et al., 2020;Wu et al., 2020b,a;Krone et al., 2020;Henderson and Vuli\u0107, 2021;Namazifar et al., 2021;Dopierre et al., 2021;Zhang et al., 2021a,b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_13",
            "start": 0,
            "end": 393,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_14@0",
            "content": "Current Gaps in NLU Datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_14",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_14@1",
            "content": "However, existing NLU datasets are still not up to the current industry requirements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_14",
            "start": 30,
            "end": 114,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_14@2",
            "content": "1) They use crowdworkers for data collection and annotation, often through simple rephrasings; they thus suffer from low lexical diversity and annotation errors (Larson et al., 2019a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_14",
            "start": 116,
            "end": 299,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_15@0",
            "content": "2) ID datasets always assume a single intent per sentence, 6 which does not support modern production 5 While DST is theoretically more accurate, it requires amounts of data that grow exponentially with the number of turns; moreover, rule-based trackers have proven to be on par with the learned/statistical ones and require no data (Wang and Lemon, 2013).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_15",
            "start": 0,
            "end": 355,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_15@1",
            "content": "6 There has been some work on multi-label ID on ATIS, MultiWOZ and DSTC4 as multi-intent datasets; however, their multi-label examples remain very limited, simple, and span a small number of intents (Gangadharaiah and Narayanaswamy, requirements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_15",
            "start": 357,
            "end": 602,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_16@0",
            "content": "3) The ontologies of these datasets are very domain-specific (i.e., they thus do not allow data reusability) and narrow (i.e., they tend to overestimate abilities of the current SotA NLU models).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_16",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_16@1",
            "content": "4) Current NLU datasets do not combine a large set of fine-grained intents (again, with multiintent examples) and a large set of fine-grained slots, which prevents proper and more insightful evaluations of joint NLU models Gangadharaiah and Narayanaswamy, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_16",
            "start": 196,
            "end": 457,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_17@0",
            "content": "NLU++ Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_17",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_18@0",
            "content": "The NLU++ dataset has been designed with the aim of addressing some of the major shortcomings of the current NLU datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_18",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_18@1",
            "content": "In what follows, we describe the main improvements and new evaluation opportunities offered by NLU++.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_18",
            "start": 123,
            "end": 223,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_19@0",
            "content": "Ontology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_19",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_20@0",
            "content": "NLU++ comprises two domains: BANKING and HOTELS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_20",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_20@1",
            "content": "The former represents a banking services task (e.g., making transfers, depositing cheques, reporting lost cards, requesting mortgage information) and the latter is a hotel 'bell desk' reception task (e.g., booking rooms, asking about pools or gyms, requesting room service).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_20",
            "start": 49,
            "end": 322,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_20@2",
            "content": "Both domains combine a large set of intents with a rich set of slots, with the ontologies inspired by requirements in production.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_20",
            "start": 324,
            "end": 452,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_20@3",
            "content": "A large number of intents and slots is shared between the two domains, in an attempt to increase data reusability/transferability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_20",
            "start": 454,
            "end": 583,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_20@4",
            "content": "Table 1 provides the main statistics of the NLU++ dataset, while the full ontology is presented in Appendix A.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_20",
            "start": 585,
            "end": 694,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_21@0",
            "content": "Multi-Intent Examples",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_21",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_22@0",
            "content": "One of the main contributions of this work is the design of the intent space, defined in a highly modular manner that natively supports intent recombinations and multi-intent annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_22",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_22@1",
            "content": "For instance, Table 2 shows several multi-intent examples based 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_22",
            "start": 188,
            "end": 257,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_22@2",
            "content": "Further, synthetic multi-intent datasets have been created by concatenating single-intent sentences, but such datasets also do not capture the complexity of true and natural multi-intent sentences (Qin et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_22",
            "start": 259,
            "end": 474,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_22@3",
            "content": "on the intent sets (termed intent modules) from Table 8 in Appendix A. This design brings several benefits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_22",
            "start": 476,
            "end": 582,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_22@4",
            "content": "1) The modular nature of the ontology allows for expressing a much more complex set of ideas through different combinations of intent modules (see Table 2), while reducing the overall size of the intent set compared to previous ID datasets 7 (see Table 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_22",
            "start": 584,
            "end": 839,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_23@0",
            "content": "2) It allows for the definition of partial intents (e.g. \"The savings one\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_23",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_23@1",
            "content": "This is crucial in multi-turn interactions, where the user often has to answer disambiguation questions (e.g. \"Which account would you like to close?\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_23",
            "start": 77,
            "end": 228,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_24@0",
            "content": "3) The modular approach allows the models to generalise to unseen combinations of intent modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_24",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_24@1",
            "content": "8 4) The design also allows us to distinguish between domain-specific versus generic intent modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_24",
            "start": 98,
            "end": 197,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_24@2",
            "content": "9 Finally, the modular design also allows us to study semantic variation of intent modules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_24",
            "start": 199,
            "end": 289,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_24@3",
            "content": "Some intents (e.g., especially the domain-specific ones) can only be expressed in a few ways (e.g. overdraft, direct_debit, swimming_pool), while others can have much more varied surface semantic realisations, (e.g. make, not_working).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_24",
            "start": 291,
            "end": 525,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_24@4",
            "content": "Table 8 in Appendix A provides an estimation of the semantic variability of each intent (module).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_24",
            "start": 527,
            "end": 623,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_25@0",
            "content": "Slots",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_25",
            "start": 0,
            "end": 4,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_26@0",
            "content": "NLU++ further includes a rich set of 17 slots, defined in Table 9 in Appendix A. Table 3 displays several NLU++ examples where complex combinations of intents and slots occur, showcasing how NLU++ might provide a much more challenging environment for the evaluation of joint ID and SL models in future research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_26",
            "start": 0,
            "end": 310,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_27@0",
            "content": "Following the design of previous standard SL datasets (Hemphill et al., 1990;Coucke et al., 2018;Coope et al., 2020), we provide span annotations for slots.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_27",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_27@1",
            "content": "On top of of this, to also support training and evaluation of SL models which are not span-based, we also provide value annotations (or canonical values as named by Rastogi et al. (2019)) for times, dates, and numeric values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_27",
            "start": 157,
            "end": 381,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_28@0",
            "content": "Similarly to intent modules, slots can also be divided into the generic ones (e.g. time, date) and the domain-specific ones (e.g company_name, rooms, kids) (see Table 9).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_28",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_28@1",
            "content": "Again, this distinction allows for the cross-domain reusability of annotated data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_28",
            "start": 171,
            "end": 252,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_29@0",
            "content": "Data Collection and Annotation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_29",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_30@0",
            "content": "Previous NLU datasets have usually relied on crowdworkers, aiming to collect a large numbers of examples, and typically optimising for quantity over quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_30",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_30@1",
            "content": "However, even with much simpler ontologies, workers are prone to make annotation mistakes, leading to very noisy datasets (Eric et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_30",
            "start": 158,
            "end": 299,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_30@2",
            "content": "In addition, when workers are asked to rephrase a sentence, they often change its semantic meaning or tend to provide rephrasings with extremely low lexical variability (Kang et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_30",
            "start": 301,
            "end": 489,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_31@0",
            "content": "NLU++ reflects true production requirements and focuses on data quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_31",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_31@1",
            "content": "Instead of relying on crowdworkers, 4 highly skilled annotators with dialogue and NLP expertise, also familiar with production environments, collected, annotated, and corrected the data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_31",
            "start": 73,
            "end": 258,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_31@2",
            "content": "The process started by defining the ontology for BANKING and HOTELS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_31",
            "start": 260,
            "end": 327,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_31@3",
            "content": "Then, real user examples were fully anonymised and reannotated following the defined ontology.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_31",
            "start": 329,
            "end": 422,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_31@4",
            "content": "Finally, new examples were created in order to cover less frequent intents and slots, aiming at creating realistic and semantically varied sentences with new (Hemphill et al., 1990), SNIPS (Coucke et al., 2018), OOS (Larson et al., 2019b) and BANKING77 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_31",
            "start": 424,
            "end": 677,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_31@5",
            "content": "combinations of intents and slots.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_31",
            "start": 679,
            "end": 712,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_32@0",
            "content": "Comparison with Other NLU Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_32",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_33@0",
            "content": "Aiming to reflect the differences between NLU++ and the most popular ToD NLU datasets, Table 4 compares their general statistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_33",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_33@1",
            "content": "Since the focus of NLU++ is on curated high-quality data, NLU++ covers a fewer number of examples than the other datasets, but it is evident that NLU++ is the only real multi-intent dataset: it averages 2.01 intents per example with a high standard deviation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_33",
            "start": 130,
            "end": 388,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_33@2",
            "content": "In addition, NLU++ is the only dataset that combines a large set of intents with a large set of slots.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_33",
            "start": 390,
            "end": 491,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_34@0",
            "content": "In order to asses the quality and diversity of the NLU data, we include two additional metrics: 1) Type-Token Ratio (TTR) (Jurafsky and Martin, 2000) which measures lexical diversity) and semantic diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_34",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_34@1",
            "content": "Both metrics are computed for the set of examples sharing an intent, weighted by the frequency of that intent 10 and finally averaged over intents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_34",
            "start": 208,
            "end": 354,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_34@2",
            "content": "The semantic diversity per intent is computed as follows: (i) sentence encodings, obtained by the ConveRT sentence encoder , 11 are computed for the set of sentences sharing the same intent; (ii) the centroid of these encodings is then computed; (iii) finally, the average cosine distance from each encoding to the centroid is computed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_34",
            "start": 356,
            "end": 691,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_34@3",
            "content": "The overall scores clearly indicate that NLU++ offers a much higher lexical and semantic diversity than previous datasets, which should also render it more challenging for current SotA NLU models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_34",
            "start": 693,
            "end": 888,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_34@4",
            "content": "12",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_34",
            "start": 890,
            "end": 891,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_35@0",
            "content": "Experiments and Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_35",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_36@0",
            "content": "In hope to establish NLU++ as a more challenging production-oriented testbed for dialogue NLU, especially in low-data scenarios, we evaluate a series of current cutting-edge models for both NLU tasks: intent detection ( \u00a74.1) and slot labeling ( \u00a74.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_36",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_36@1",
            "content": "Our aim is to assess and analyse their performance across different setups, and provide solid baseline reference points for future evaluations on NLU++.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_36",
            "start": 253,
            "end": 404,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_37@0",
            "content": "Data Setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_37",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_37@1",
            "content": "Unless noted otherwise, for both tasks we adopt the standard K-fold cross-validation as done e.g. by Liu et al. (2019b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_37",
            "start": 13,
            "end": 132,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_37@2",
            "content": "Through such folding evaluation, (i) we avoid overfitting to any particular test set and (ii) we ensure more stable results with smaller training and test data (i.e., when simulating low-data regimes typically met in production) through averaging over different folds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_37",
            "start": 134,
            "end": 401,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_37@3",
            "content": "13 The experiments are run with K = 20 (20-Fold) and K = 10 (10-Fold), where we train on 1 fold and evalute on the remaining K \u2212 1 folds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_37",
            "start": 403,
            "end": 539,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_37@4",
            "content": "These setups simulate different degrees of data scarcity: e.g., the average training fold comprises \u2248 100 examples for BANKING and \u2248 50 for HOTELS for 20-Fold experiments, and twice as much for 10-Fold experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_37",
            "start": 541,
            "end": 754,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_37@5",
            "content": "Besides these low-data training setups, we also run experiments in a Large-data setup, where we train the models on merged 9 folds, and evaluate on the single held-out fold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_37",
            "start": 756,
            "end": 928,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_37@6",
            "content": "14 The key questions we aim to answer with these data setups are: Which NLU models are better adapted to low-data scenarios? How much does NLU performance improve with the increase of annotated NLU data? How challenging is NLU++ in low-data versus large-data scenarios?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_37",
            "start": 930,
            "end": 1198,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_37@7",
            "content": "Domain Setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_37",
            "start": 1200,
            "end": 1213,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_37@8",
            "content": "Further, experiments are run in the following domain setups: (i) single-domain experiments where we only use the BANKING or the HOTELS portion of the entire dataset; (ii) bothdomain experiments (termed ALL) where we use the entire dataset and combine the two domain ontologies (see Table 1); (iii) cross-domain experiments where we train on the examples associated with one domain and test on the examples from the other domain, keeping only shared intents and slots for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_37",
            "start": 1215,
            "end": 1696,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_37@9",
            "content": "The key questions we aim to answer are: Are there major performance differences between the two domains and can they be merged into a single (and more complex) domain? Is it possible to use examples labeled with generic intents from one domain to boost another domain, effectively increasing reusability of data annotations and reducing data scarcity?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_37",
            "start": 1698,
            "end": 2048,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_38@0",
            "content": "F 1 (micro) is the main evaluation measure in all ID and SL experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_38",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_39@0",
            "content": "Intent Detection: Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_39",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_40@0",
            "content": "We evaluate two groups of SotA intent detection models: (i) MLP-Based, and (ii) QA-Based ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_40",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_41@0",
            "content": "MLP-Based ID Baselines. and Gerz et al. (2021) have recently shown that, for the ID task, full and expensive fine-tuning of large pretrained models such as BERT or RoBERTa (Liu et al., 2019a) is not needed to reach strong ID performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_41",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_41@1",
            "content": "As an alternative, they propose a much more efficient MLP-based approach to intent detection which works on par or even outperforms full fine-tuning on the ID task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_41",
            "start": 238,
            "end": 401,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_41@2",
            "content": "15 In a nutshell, the idea is to use fixed/frozen \"offthe-shelf\" universal sentence encoders such as Con-veRT or Sentence-BERT (Reimers and Gurevych, 2019) models to encode input sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_41",
            "start": 403,
            "end": 591,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_41@3",
            "content": "A standard multi-layer perceptron (MLP) classifier is then learnt on top of the sentence encodings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_41",
            "start": 593,
            "end": 691,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_42@0",
            "content": "Two core differences to the previous work stem from the fact that we now deal with the multi-label ID task: 1) to this end, we replace the output softmax layer with the sigmoid layer; and 2) we define a threshold \u03b8 which determines the final classification: only intents with probability scores \u2265 \u03b8 are taken as positives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_42",
            "start": 0,
            "end": 321,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_42@1",
            "content": "This way, the hyper-parameter \u03b8 effectively controls the trade-off between precision and recall of the multi-label classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_42",
            "start": 323,
            "end": 448,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_43@0",
            "content": "We comparatively evaluate several widely used state-of-the-art (SotA) sentence encoders, but remind the reader that this decoupling of the MLP classification layers from the fixed encoder allows for a much wider empirical comparison of sentence encoders in future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_43",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_43@1",
            "content": "The evalauted sentence encoders are: 1) CONVERT , which produces 1,024-dimensional sentence encodings; 2) LABSE (Feng et al., 2020) (768-dim); 3) ROBL-1B (1,024-dim) and 4) LM12-1B (384dim) (Reimers and Gurevych, 2019;Thakur et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_43",
            "start": 270,
            "end": 508,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_43@2",
            "content": "For completeness, we provide brief descriptions of each encoder in our evaluation, along with their public URLs, in Appendix B, and refer the reader to the original work for more details about each sentence encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_43",
            "start": 510,
            "end": 724,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_44@0",
            "content": "QA-Based ID Baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_44",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_44@1",
            "content": "Another group of SotA ID baselines reformulates the ID task into the (extractive) question-answering (QA) problem (Namazifar et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_44",
            "start": 23,
            "end": 161,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_44@2",
            "content": "This QA-oriented reformatting then allows for additional specialised QA-tuning of large PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_44",
            "start": 163,
            "end": 255,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_44@3",
            "content": "In a nutshell, the idea is to (i) fine-tune the original PLM such as BERT/RoBERTa on readily available large generalpurpose QA data such as SQuAD (Rajpurkar et al., 2016), and then (ii) further fine-tune this general QA model with in-domain ID data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_44",
            "start": 257,
            "end": 505,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_44@4",
            "content": "This strategy has recently shown very strong performance on single-label ATIS data (Namazifar et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_44",
            "start": 507,
            "end": 614,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_45@0",
            "content": "The main 'trick' is to reformat the input ID examples into the following format: \"yes. no. [SEN-TENCE]\" and pose a question such as: \"is the intent to ask about [INTENT]?\" (see Appendix A for the actual questions associated with each intent, also shared with the dataset).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_45",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_45@1",
            "content": "Here, [SENTENCE] is the placeholder for the actual input sentence, and [INTENT] is the placeholder for a short manually defined text (akin to language modeling prompts , see again Appendix A) which briefly describes the intent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_45",
            "start": 273,
            "end": 499,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_45@2",
            "content": "The QA formulation lends itself naturally to the multi-label ID setup as each 'intent-related' question is posed separately.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_45",
            "start": 501,
            "end": 624,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_45@3",
            "content": "In other words, for each input example and for each of the L intents in the ontology the QA model must extract yes or no as the answer, where correct intent labels are the ones for which the answer is yes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_45",
            "start": 626,
            "end": 830,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_45@4",
            "content": "16 We note that our work is the first to apply and evaluate the QA approach on multi-label ID.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_45",
            "start": 832,
            "end": 925,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_46@0",
            "content": "We experiment with two pretrained language models, both fine-tuned on the SQuAD2.0 dataset (Rajpurkar et al., 2018) before additional QAtuning on NLU++ examples converted to the aforementioned QA format: ROBB-QA uses RoBERTa-Base as the underlying LM, while ALB-QA relies on the more compact ALBERT (Lan et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_46",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_47@0",
            "content": "ID: Training and Evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_47",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_47@1",
            "content": "All MLP-based baselines rely on the same training protocol and hyper-parameters in all data and domain setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_47",
            "start": 29,
            "end": 138,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_47@2",
            "content": "The MLP classifier consists of 1 hidden layer of size 512, and is trained via binary cross-entropy loss for 500 epochs with the batch size of 32 and the dropout rate is 0.6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_47",
            "start": 140,
            "end": 312,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_47@3",
            "content": "We use the standard AdamW optimizer (Loshchilov and Hutter, 2018) with the learning rate of 0.003 and linear decay; weight decay is 0.02.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_47",
            "start": 314,
            "end": 450,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_47@4",
            "content": "The threshold \u03b8 is set to 0.4. 17 For QA models, we largely follow Namazifar et al. (2021) and fine-tune all models for 5 epochs, using AdamW; the learning rate of 2e\u22125 with linear decay; weight decay is 0; batch size is 32.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_47",
            "start": 452,
            "end": 675,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_47@5",
            "content": "17",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_47",
            "start": 677,
            "end": 678,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_48@0",
            "content": "Slot Labeling: Experimental Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_48",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_49@0",
            "content": "For slot labeling, we benchmark two current SotA models: (i) ConvEx (Henderson and Vuli\u0107, 2021), as a SotA span-extraction SL model and (ii) the QA-based SL model (Namazifar et al., 2021) based on ROBB-QA, which operates similarly to QAbased ID baselines discussed in \u00a74.1, and relies on the same fine-tuning regime as our QA-based ID baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_49",
            "start": 0,
            "end": 344,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_49@1",
            "content": "Again, we refer the reader to the original work for further details, and provide brief 16 For instance, for the input sentence \"I need to increase my overdraft\" from the BANKING domain, we would pose all 48 questions associated with each of the L = 48 intents in BANKING, where the QA model should extract yes as the answer for intents change, overdraft and more_higher_after, and extract no for the remaining 45 intents in BANKING.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_49",
            "start": 346,
            "end": 777,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_49@2",
            "content": "17 These hyper-parameters were selected based on preliminary experiments with a single (most efficient) sentence encoder LM12-1B and training only on Fold 0 of the 10-Fold BANKING setup; they were then propagated without change to all other MLP-based experiments with other encoders and in other setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_49",
            "start": 779,
            "end": 1081,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_49@3",
            "content": "We repeated the similar hyper-parameter search procedure for QA-based models, using ALB-QA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_49",
            "start": 1083,
            "end": 1173,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_49@4",
            "content": "descriptions in Appendix D.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_49",
            "start": 1175,
            "end": 1201,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_50@0",
            "content": "Results and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_50",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_51@0",
            "content": "Main results with all the evaluated baselines are summarised in Table 5 (for ID) and Table 6",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_51",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_52@0",
            "content": "ID: MLP versus QA Models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_52",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_52@1",
            "content": "First, the comparisons among only MLP-based models reveal that 1) all sentence encoders offer ID performance in similar, reasonably narrow score intervals (e.g., the variations in F 1 scores between all sentence encoders are typically below 4-6 F 1 points in all setups), and 2) that CONVERT is the best-performing sentence encoder on average, which corroborates findings from prior work on other ID datasets Wu and Xiong, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_52",
            "start": 26,
            "end": 454,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_53@0",
            "content": "One very apparent and important indication in the reported results is the superiority of QA-based ID models over their MLP-based competitors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_53",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_53@1",
            "content": "QAbased models largely outperform MLP-Based baselines in all domain setups, as well as in all data setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_53",
            "start": 142,
            "end": 247,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_53@2",
            "content": "The gains are visible even in Large-data setups, but the benefits of QA-based ID are immense in the lowest-data 20-Fold setups: e.g., 12 F 1 points over the strongest MLP ID model on HOTELS and 20 F 1 points on BANKING.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_53",
            "start": 249,
            "end": 467,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_54@0",
            "content": "Moreover, the use of larger underlying LMs might push the scores with QA even further: using SQuAD-tuned Roberta-Large (ROBL-QA) instead of Base (ROBB) yields further gains -e.g., F 1 rises from 85.6 to 87.8 on 10-Fold BANKING, and similar trends are observed in other low-data setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_54",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_55@0",
            "content": "Slot Labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_55",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_55@1",
            "content": "In the SL task, the QA-based model also demonstrates its superiority, again with huge gains in low-data 20-Fold and 10-Fold setups, confirming that such QA-based or prompt-based methods Gao et al., 2021) are especially well suited for low-data setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_55",
            "start": 15,
            "end": 265,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_55@2",
            "content": "The use of manually defined questions/prompts, which are typically easy to write by humans, combined with the expressive power of QA-based task formatting yields immense gains on low-resource dialogue NLU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_55",
            "start": 267,
            "end": 471,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_56@0",
            "content": "Given these very promising ID and SL results on NLU++, our work also calls for further and more intensive future research on QA-based models for dialogue NLU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_56",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_56@1",
            "content": "However, we note that QA-based ID and SL methods do come with efficiency detriments, especially with larger intent and slot sets: the model must copy the input utterance and run a separate answer extraction for each intent/slot from the set, which is by several order of magnitudes more costly at both training and inference than Table 5: F 1 scores (\u00d7100%) of benchmarked state-of-the-art intent detection models on NLU++ in three data setups (see \u00a74.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_56",
            "start": 159,
            "end": 613,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_56@2",
            "content": "We also refer to \u00a74 for the brief descriptions of each sentence encoder (for MLP-based baselines) and the two QA-pretrained models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_56",
            "start": 615,
            "end": 745,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_56@3",
            "content": "*All models were retrieved from the HuggingFace model repository (Wolf et al., 2020) MLP-based models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_56",
            "start": 747,
            "end": 848,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_56@4",
            "content": "A promising future research avenue is thus to investigate combined approaches that could combine and trade off the performance benefits of QA-based models and the efficiency advantages of, e.g., MLP-based ID.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_56",
            "start": 850,
            "end": 1057,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_57@0",
            "content": "Low-Data vs. Large-Data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_57",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_57@1",
            "content": "We also note that scores on both tasks, as reported in Tables 5-6, leave ample room for improvement in NLU methodology in future work, especially on SL (even in Large-data setups), and in low-data setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_57",
            "start": 25,
            "end": 228,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_58@0",
            "content": "Cross-Domain Experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_58",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_58@1",
            "content": "We also verify potential reusability of annotated data across domains with a simple ID experiment, where we train ID models on BANKING and evaluate on HOTELS, and vice versa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_58",
            "start": 26,
            "end": 199,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_58@2",
            "content": "The results are summarised in Table 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_58",
            "start": 201,
            "end": 238,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_58@3",
            "content": "Besides (again) indicating that QA-based models outscore MLP-based ID, the results also suggest that for some generic intents it is possible to meet high ID performance without any in-domain annotations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_58",
            "start": 240,
            "end": 442,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_58@4",
            "content": "For instance, we observe particularly high scores for highly generic and reusable intent modules such as change, how, how_much, thank, when, and affirm, all with per-intent F 1 scores \u2265 90.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_58",
            "start": 444,
            "end": 632,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_58@5",
            "content": "We hope that these preliminary results might inspire similar ontology (re)designs in future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_58",
            "start": 634,
            "end": 730,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_59@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_59",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_60@0",
            "content": "We have presented NLU++, a novel dataset for task-oriented dialogue (ToD) NLU that overcomes the shortcomings of previous NLU evaluation sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_60",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_60@1",
            "content": "NLU++ presents a multi-intent and slot-rich ontology, defines generic and domain-specific intents and slots to promote data reusability, and it focuses on the creation of high-quality complex examples and annotations collected by dialogue experts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_60",
            "start": 143,
            "end": 389,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_60@2",
            "content": "Experimental results show that NLU++ raises the bar with respect to current NLU benchmarks, helping better discriminate and compare the performance of current state-of-the-art NLU models, particularly in low-data setups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_60",
            "start": 391,
            "end": 610,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_60@3",
            "content": "We hope that NLU++ will be valuable in guiding future modeling efforts for ToD NLU, both in academia and in industry.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_60",
            "start": 612,
            "end": 728,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_61@0",
            "content": "Limitations and Future Work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_61",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_61@1",
            "content": "This work has shown that a better design of the intent set can improve data reusability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_61",
            "start": 29,
            "end": 116,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_61@2",
            "content": "However, the current ontology does not cover generic sets of intents exhaustively, and we acknowledge a (sometimes) fine line between truly generic intents versus intents 'anecdotally' shared by two domains (e.g., refund).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_61",
            "start": 118,
            "end": 339,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_61@3",
            "content": "Further, NLU++ currently provides fine-grained slots such as date_from, date_to and date to enable more complex scenarios, but such a design might slow down annotation process and make it cumbersome.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_61",
            "start": 341,
            "end": 539,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_61@4",
            "content": "Future work should also look into alternatives to fine-grained slot annotations for such slots.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_61",
            "start": 541,
            "end": 635,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_62@0",
            "content": "Ethical Considerations During data collection: we did not include any personal information (e.g. personal names or addresses) and all the examples that included any had been fully anonymised or removed from the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_62",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_62@1",
            "content": "All the names in the dataset are created by randomly concatenating names and surnames from the list of the top 10K names from the US registry.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_62",
            "start": 220,
            "end": 361,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_62@2",
            "content": "Upon collection, the dataset has undergone an additional check by the internal Ethics committee.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_62",
            "start": 363,
            "end": 458,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_62@3",
            "content": "It is licensed under CC BY 4.0.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_62",
            "start": 460,
            "end": 490,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_63@0",
            "content": "The complete ontology of NLU++ is provided in Table 8 and Table 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_63",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_63@1",
            "content": "B Appendix: Sentence Encoders in Intent Detection Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_63",
            "start": 67,
            "end": 127,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_64@0",
            "content": "CONVERT is trained with the conversational response selection objective (Henderson et al., 2019b) on large Reddit data (Al-Rfou et al., 2016;Henderson et al., 2019a), spanning more than 700M (context, response) sentence pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_64",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_64@1",
            "content": "Thanks to its naturally conversational pretraining objective, it has been shown to be especially well-suited for conversational tasks such as intent detection and slot labelling (Coope et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_64",
            "start": 227,
            "end": 425,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_64@2",
            "content": "It outputs 1,024-dim sentence encodings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_64",
            "start": 427,
            "end": 466,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_65@0",
            "content": "github.com/davidalami/ConveRT LABSE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_65",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_65@1",
            "content": "Language-agnostic BERT Sentence Embedding (LaBSE) (Feng et al., 2020) adapts pretrained multilingual BERT (mBERT) using a dual-encoder framework with larger embedding capacity (i.e., a shared multilingual vocabulary of 500k subwords).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_65",
            "start": 37,
            "end": 270,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_65@2",
            "content": "While LaBSE is the current state-of-the-art multilingual encoder, it also displays very strong monolingual English performance (Feng et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_65",
            "start": 272,
            "end": 418,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_65@3",
            "content": "It produces 768-dim sentence encodings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_65",
            "start": 420,
            "end": 458,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_65@4",
            "content": "huggingface.co/sentence-transformers/ LaBSE ROBL-1B and LM12-1B (Reimers and Gurevych, 2019;Thakur et al., 2021) are sentence encoders which fine-tune the pretrained Roberta-Large (ROBL) language model (Liu et al., 2019a) and the 12-layer MiniLM (Wang et al., 2020b), respectively, again using a contrastive dual-encoder framework (Reimers and Gurevych, 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_65",
            "start": 460,
            "end": 819,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_65@5",
            "content": "The models are fine-tuned on a set of more than 1B sentence pairs: this set comprises various data such as Reddit 2015-2018 comments (Henderson et al., 2019a), Natural Questions (Kwiatkowski et al., 2019), PAQ (question, answer) pairs (Lewis et al., 2021), to name only a few.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_65",
            "start": 821,
            "end": 1096,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_65@6",
            "content": "18 ROBL-1B outputs 18 In a nutshell, the contrastive fine-tuning task which combines all the heterogeneous datasets is as follows: given a 'query' sentence from each sentence pair, and a set of R randomly sampled negatives plus 1 true positive (the sentence from the same pair), the model should predict which sentence from the set of R + 1 sentences is actually paired with the query sentence in the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_65",
            "start": 1098,
            "end": 1506,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_65@7",
            "content": "The full list of all datasets along with the exact model specifications is at:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_65",
            "start": 1508,
            "end": 1585,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_66@0",
            "content": "1,024-dim encodings, while LM12-1B produces 384-dim encodings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_66",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_67@0",
            "content": "We opted for those two models in particular as one represents a class of large sentence encoders (ROBL-1B), and the other is lightweight (LM12-1B), while both display very strong performance in a myriad of sentence similarity and semantic search tasks, see www.sbert.net/docs/ pretrained_models.html.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_67",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_67@1",
            "content": "We rely on the same SQuAD-tuned language models as Namazifar et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_67",
            "start": 301,
            "end": 375,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_67@2",
            "content": "ROBB-QA can be found online at: https://huggingface.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_67",
            "start": 377,
            "end": 428,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_67@3",
            "content": "co/deepset/roberta-base-squad2; ALB-QA is available at: https://huggingface.co/twmkn9/ albert-base-v2-squad2 D Appendix: Slot Labeling Baselines CONVEX (Henderson and Vuli\u0107, 2021) demonstrates strong SL performance, especially in fewshot settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_67",
            "start": 430,
            "end": 676,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_67@4",
            "content": "It is pretrained on a pairwise cloze task extracted from the Reddit examples (Henderson et al., 2019a), and the majority of the pretrained model's parameters in CONVEX are kept frozen during fine-tuning, making it an extremely efficient model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_67",
            "start": 678,
            "end": 920,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_67@5",
            "content": "We adopt the suggested hyper-parameters from Henderson and Vuli\u0107 (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_67",
            "start": 922,
            "end": 993,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_68@0",
            "content": "QA-Based: Namazifar et al. ( 2021) train an extractive QA-based model to extract the spans of the slots from the input user utterance as answers to manually defined natural language questions (one per slot).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_68",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_68@1",
            "content": "It follows the same idea as QA-based ID models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_68",
            "start": 208,
            "end": 254,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_68@2",
            "content": "We also provide such questions for each slot along with NLU++ for model training and inference: see the questions in Table 9.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_68",
            "start": 256,
            "end": 380,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_69@0",
            "content": "huggingface.co/sentence-transformers/ all-roberta-large-v1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_69",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_70@0",
            "content": "Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, SemEval-2012 Task 6: A pilot on semantic textual similarity, 2012, Proceedings of *SEM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_70",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_71@0",
            "content": "UNKNOWN, None, 2016, Conversational contextual cues: The case of personalization and history for response ranking, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_71",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_72@0",
            "content": "Wei Bi, James Tin-Yau Kwok, Efficient multilabel classification with many labels, 2013-06, Proceedings of the 30th International Conference on Machine Learning, ICML 2013, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_72",
            "start": 0,
            "end": 172,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_73@0",
            "content": "Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Milica Osman Ramadan,  Ga\u0161i\u0107, MultiWOZ -A large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling, 2018, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_73",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_74@0",
            "content": "I\u00f1igo Casanueva, Tadas Temcinas, Daniela Gerz, Matthew Henderson, Ivan Vuli\u0107, Efficient intent detection with dual sentence encoders, 2020, Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_74",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_75@0",
            "content": "UNKNOWN, None, 2019, Bert for joint intent classification and slot filling, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_75",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_76@0",
            "content": "Sam Coope, Tyler Farghly, Daniela Gerz, Ivan Vuli\u0107, Matthew Henderson, Span-ConveRT: Fewshot span extraction for dialog with pretrained conversational representations, 2020, Proceedings of ACL 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_76",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_77@0",
            "content": "UNKNOWN, None, 2018, Snips Voice Platform: An embedded spoken language understanding system for private-by-design voice interfaces, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_77",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_78@0",
            "content": "UNKNOWN, None, , 2021. Preview, attend and review: Schema-aware curriculum learning for multi-domain dialog state tracking, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_78",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_79@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of NAACL-HLT 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_79",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_80@0",
            "content": "B William, Chris Dolan,  Brockett, Automatically constructing a corpus of sentential paraphrases, 2005, Proceedings of the Third International Workshop on Paraphrasing (IWP2005), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_80",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_81@0",
            "content": "Thomas Dopierre, Christophe Gravier, Wilfried Logerais, PROTAUGMENT: Unsupervised diverse short-texts paraphrasing for intent detection meta-learning, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_81",
            "start": 0,
            "end": 370,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_82@0",
            "content": "Layla Asri, Hannes Schulz, Shikhar Sharma, Jeremie Zumer, Justin Harris, Emery Fine, Rahul Mehrotra, Kaheer Suleman, Frames: A corpus for adding memory to goal-oriented dialogue systems, 2017, Proceedings of SIGDIAL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_82",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_83@0",
            "content": "UNKNOWN, None, 1669, Multiwoz 2.1: Multi-domain dialogue state corrections and state tracking baselines, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_83",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_84@0",
            "content": "Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen Arivazhagan, Wei Wang, Language-agnostic BERT sentence embedding. CoRR, abs/2007.01852. Rashmi Gangadharaiah and Balakrishnan Narayanaswamy, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_84",
            "start": 0,
            "end": 381,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_85@0",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_85",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_86@0",
            "content": "Daniela Gerz, Pei-Hao Su, Razvan Kusztos, Avishek Mondal, Michal Lis, Eshan Singhal, Nikola Mrk\u0161i\u0107, Tsung-Hsien Wen, and Ivan Vuli\u0107. 2021. Multilingual and cross-lingual intent detection from spoken data, , Proceedings of EMNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_86",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_87@0",
            "content": "Arshit Gupta, John Hewitt, Katrin Kirchhoff, Simple, fast, accurate intent classification and slot labeling for goal-oriented dialogue systems, 2019, Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_87",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_88@0",
            "content": "UNKNOWN, None, 2020, Trippy: A triple copy strategy for value independent neural dialog state tracking, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_88",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_89@0",
            "content": "Charles Hemphill, John Godfrey, George Doddington, The ATIS Spoken Language Systems Pilot Corpus, 1990, Proceedings of the Workshop on Speech and Natural Language, HLT '90, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_89",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_90@0",
            "content": "Matthew Henderson, Pawel Budzianowski, I\u00f1igo Casanueva, Sam Coope, Daniela Gerz, Girish Kumar, Nikola Mrk\u0161i\u0107, Georgios Spithourakis, Pei-Hao Su, Ivan Vuli\u0107, Tsung-Hsien Wen, A repository of conversational datasets, 2019, Proceedings of the 1st Workshop on Natural Language Processing for Conversational AI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_90",
            "start": 0,
            "end": 307,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_91@0",
            "content": "Matthew Henderson, I\u00f1igo Casanueva, Nikola Mrk\u0161i\u0107, Pei-Hao Su, Tsung-Hsien Wen, Ivan Vuli\u0107, ConveRT: Efficient and accurate conversational representations from transformers, 2020, Findings of EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_91",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_92@0",
            "content": "Matthew Henderson, Blaise Thomson, Jason Wiliams, The Second Dialog State Tracking Challenge, 2014, Proceedings of SIGDIAL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_92",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_93@0",
            "content": "Matthew Henderson, Ivan Vuli\u0107, ConVEx: Data-efficient and few-shot slot labeling, 2021, Proceedings of NAACL-HLT 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_93",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_94@0",
            "content": "Matthew Henderson, Ivan Vuli\u0107, Daniela Gerz, I\u00f1igo Casanueva, Pawe\u0142 Budzianowski, Sam Coope, Georgios Spithourakis, Tsung-Hsien Wen, Nikola Mrk\u0161i\u0107, Pei-Hao Su, Training neural response selection for task-oriented dialogue systems, 2019, Proceedings of ACL 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_94",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_95@0",
            "content": "UNKNOWN, None, 2020, Few-shot learning for multilabel intent detection, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_95",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_96@0",
            "content": "Eduard Hovy, Ulf Hermjakob, Chin-Yew Lin, The use of external knowledge in factoid qa, 2001, TREC, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_96",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_97@0",
            "content": "UNKNOWN, None, 2000, Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition, Prentice Hall PTR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_97",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_98@0",
            "content": "Yiping Kang, Yunqi Zhang, Jonathan Kummerfeld, Lingjia Tang, Jason Mars, Data collection for dialogue system: A startup perspective, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_98",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_99@0",
            "content": "Seokhwan Kim, Luis Fernando, D' Haro, Rafael Banchs, Jason Williams, Matthew Henderson, The Fourth Dialog State Tracking Challenge, 2016, Proceedings of IWSDS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_99",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_100@0",
            "content": "Jason Krone, Yi Zhang, Mona Diab, Learning to classify intents and slot labels given a handful of examples, 2020, Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_100",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_101@0",
            "content": "UNKNOWN, None, 2019, Natural questions: A benchmark for question answering research. Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_101",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_102@0",
            "content": "UNKNOWN, None, 2019, Albert: A Lite BERT for selfsupervised learning of language representations, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_102",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_103@0",
            "content": "UNKNOWN, None, , Lingjia Tang, and Jason Mars. 2019a. Outlier detection for improved data quality and diversity in dialog systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_103",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_104@0",
            "content": "Stefan Larson, Anish Mahendran, Joseph Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan Kummerfeld, Kevin Leach, Michael Laurenzano, Lingjia Tang, Jason Mars, An evaluation dataset for intent classification and out-of-scope prediction, 2019, Proceedings of EMNLP-IJCNLP 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_104",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_105@0",
            "content": "E Levin, R Pieraccini, Chronus, the next generation, 1995, Proceedings of the ARPA Workshop on Spoken Language Technology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_105",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_106@0",
            "content": "Patrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich K\u00fcttler, Aleksandra Piktus, Pontus Stenetorp, Sebastian Riedel, PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them, 2021, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_106",
            "start": 0,
            "end": 274,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_107@0",
            "content": "UNKNOWN, None, 2021, Pretrain, prompt, and predict: A systematic survey of prompting methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_107",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_108@0",
            "content": "UNKNOWN, None, 1901, Multi-task deep neural networks for natural language understanding. CoRR, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_108",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_109@0",
            "content": "Xingkun Liu, Arash Eshghi, Pawel Swietojanski, Verena Rieser, Benchmarking natural language understanding services for building conversational agents, 2019, Proceedings of IWSDS 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_109",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_110@0",
            "content": "UNKNOWN, None, 1907, RoBERTa: A robustly optimized BERT pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_110",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_111@0",
            "content": "Ilya Loshchilov, Frank Hutter, Decoupled weight decay regularization, 2018, Proceedings of ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_111",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_112@0",
            "content": "Nurul Lubis, Christian Geishauser, Michael Heck, Hsien-Chin Lin, Marco Moresi, Carel van Niekerk, and Milica Gasic. 2020. Lava: Latent action spaces via variational auto-encoding for dialogue policy optimization, , Proceedings of the 28th International Conference on Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_112",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_113@0",
            "content": "UNKNOWN, None, 2020, Example-driven intent prediction with observers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_113",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_114@0",
            "content": "Mahdi Namazifar, Alexandros Papangelis, Gokhan Tur, Dilek Hakkani-T\u00fcr, Language model is all you need: Natural language understanding as question answering, 2021, Proceedings of ICASSP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_114",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_115@0",
            "content": "Denis Peskov, Nancy Clarke, Jason Krone, Brigi Fodor, Yi Zhang, Adel Youssef, Mona Diab, Multi-domain goal-oriented dialogues (multidogo): Strategies toward curating and annotating large scale dialogue data, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_115",
            "start": 0,
            "end": 391,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_116@0",
            "content": "UNKNOWN, None, 2020, Agif: An adaptive graph-interactive framework for joint multiple intent detection and slot filling, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_116",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_117@0",
            "content": "Pranav Rajpurkar, Robin Jia, Percy Liang, Know what you don't know: Unanswerable questions for SQuAD, 2018, Proceedings of ACL 2018, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_117",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_118@0",
            "content": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, SQuAD: 100,000+ questions for Machine Comprehension of Text, 2016, Proceedings of EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_118",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_119@0",
            "content": "UNKNOWN, None, 2019, Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_119",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_120@0",
            "content": "Antoine Raux, Brian Langner, Alan Black, Maxine Esk\u00e9nazi, LET's GO: Improving spoken dialog systems for the elderly and non-natives, 2003, Proceedings of EUROSPEECH, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_120",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_121@0",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-BERT: Sentence embeddings using Siamese BERTnetworks, 2019, Proceedings of EMNLP 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_121",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_122@0",
            "content": "Nandan Thakur, Nils Reimers, Johannes Daxenberger, Iryna Gurevych, Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_122",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_123@0",
            "content": "Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, Jianxing Yu, Multi-domain dialogue acts and response co-generation, 2020, Proceedings of the 58th, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_123",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_124@0",
            "content": "UNKNOWN, None, , Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_124",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_125@0",
            "content": "UNKNOWN, None, 2020, MiniLM: Deep selfattention distillation for task-agnostic compression of pre-trained Transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_125",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_126@0",
            "content": "Zhuoran Wang, Oliver Lemon, A Simple and Generic Belief Tracking Mechanism for the Dialog State Tracking Challenge: On the believability of observed information, 2013, Proceedings of SIGDIAL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_126",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_127@0",
            "content": "Wei Wei, Quoc Le, Andrew Dai, Jia Li, Airdialogue: An environment for goal-oriented dialogue research, 2018, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_127",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_128@0",
            "content": "Jason Williams, Antoine Raux, Deepak Ramachandran, Alan Black, The Dialogue State Tracking Challenge, 2013, Proceedings of SIGDIAL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_128",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_129@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_129",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_130@0",
            "content": "Alexander Lhoest,  Rush, Transformers: State-of-the-art natural language processing, 2020, Proceedings of EMNLP 2020: System Demonstrations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_130",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_131@0",
            "content": "Chien-Sheng Wu, C Steven, Richard Hoi, Caiming Socher,  Xiong, TOD-BERT: Pre-trained natural language understanding for task-oriented dialogue, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_131",
            "start": 0,
            "end": 295,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_132@0",
            "content": "Sheng Chien, Caiming Wu,  Xiong, Probing task-oriented dialogue representation from language models, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_132",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_133@0",
            "content": "Di Wu, Liang Ding, Fan Lu, Jian Xie, SlotRefine: A fast non-autoregressive model for joint intent detection and slot filling, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_133",
            "start": 0,
            "end": 277,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_134@0",
            "content": "Weijia Xu, Batool Haider, Saab Mansour, End-to-end slot alignment and recognition for crosslingual NLU, 2020, Proceedings of EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_134",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_135@0",
            "content": "Yinfei Yang, Gustavo Hernandez Abrego, Steve Yuan, Mandy Guo, Qinlan Shen, Daniel Cer, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil, Improving multilingual sentence embedding using bidirectional dual encoder with additive margin softmax, 2019, Proceedings of IJCAI 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_135",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_136@0",
            "content": "Steve Young, Cognitive user interfaces, 2010, IEEE Signal Processing Magazine, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_136",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_137@0",
            "content": "UNKNOWN, None, 2002, The HTK book, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_137",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_138@0",
            "content": "UNKNOWN, None, 2020, Multiwoz 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_138",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_139@0",
            "content": "Haode Zhang, Yuwei Zhang, Li-Ming Zhan, Jiaxin Chen, Guangyuan Shi, Xiao-Ming Wu, Albert Lam, Effectiveness of pre-training for few-shot intent classification, 2021, Findings of the Association for Computational Linguistics: EMNLP 2021, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_139",
            "start": 0,
            "end": 278,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_140@0",
            "content": "Jianguo Zhang, Trung Bui, Seunghyun Yoon, Xiang Chen, Zhiwei Liu, Congying Xia, Hung Quan, Walter Tran, Philip Chang,  Yu, Few-shot intent detection via contrastive pre-training and fine-tuning, 2021, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_140",
            "start": 0,
            "end": 330,
            "label": {}
        },
        {
            "ix": "344-ARR_v1_141@0",
            "content": "Jinghan Zhang, Yuxiao Ye, Yue Zhang, Likun Qiu, Bin Fu, Yang Li, Zhenglu Yang, Jian Sun, Multi-point semantic representation for intent classification, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "344-ARR_v1_141",
            "start": 0,
            "end": 221,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "344-ARR_v1_0",
            "tgt_ix": "344-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_0",
            "tgt_ix": "344-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_1",
            "tgt_ix": "344-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_1",
            "tgt_ix": "344-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_1",
            "tgt_ix": "344-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_2",
            "tgt_ix": "344-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_0",
            "tgt_ix": "344-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_3",
            "tgt_ix": "344-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_4",
            "tgt_ix": "344-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_4",
            "tgt_ix": "344-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_4",
            "tgt_ix": "344-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_0",
            "tgt_ix": "344-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_6",
            "tgt_ix": "344-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_8",
            "tgt_ix": "344-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_9",
            "tgt_ix": "344-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_11",
            "tgt_ix": "344-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_12",
            "tgt_ix": "344-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_13",
            "tgt_ix": "344-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_14",
            "tgt_ix": "344-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_15",
            "tgt_ix": "344-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_10",
            "tgt_ix": "344-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_0",
            "tgt_ix": "344-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_16",
            "tgt_ix": "344-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_17",
            "tgt_ix": "344-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_17",
            "tgt_ix": "344-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_17",
            "tgt_ix": "344-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_18",
            "tgt_ix": "344-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_19",
            "tgt_ix": "344-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_19",
            "tgt_ix": "344-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_17",
            "tgt_ix": "344-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_20",
            "tgt_ix": "344-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_21",
            "tgt_ix": "344-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_21",
            "tgt_ix": "344-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_23",
            "tgt_ix": "344-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_21",
            "tgt_ix": "344-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_21",
            "tgt_ix": "344-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_22",
            "tgt_ix": "344-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_17",
            "tgt_ix": "344-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_24",
            "tgt_ix": "344-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_26",
            "tgt_ix": "344-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_27",
            "tgt_ix": "344-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_25",
            "tgt_ix": "344-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_25",
            "tgt_ix": "344-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_25",
            "tgt_ix": "344-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_25",
            "tgt_ix": "344-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_17",
            "tgt_ix": "344-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_28",
            "tgt_ix": "344-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_30",
            "tgt_ix": "344-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_29",
            "tgt_ix": "344-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_29",
            "tgt_ix": "344-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_29",
            "tgt_ix": "344-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_17",
            "tgt_ix": "344-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_31",
            "tgt_ix": "344-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_33",
            "tgt_ix": "344-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_32",
            "tgt_ix": "344-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_32",
            "tgt_ix": "344-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_32",
            "tgt_ix": "344-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_0",
            "tgt_ix": "344-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_34",
            "tgt_ix": "344-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_36",
            "tgt_ix": "344-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_35",
            "tgt_ix": "344-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_35",
            "tgt_ix": "344-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_35",
            "tgt_ix": "344-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_35",
            "tgt_ix": "344-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_35",
            "tgt_ix": "344-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_38",
            "tgt_ix": "344-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_40",
            "tgt_ix": "344-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_41",
            "tgt_ix": "344-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_42",
            "tgt_ix": "344-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_43",
            "tgt_ix": "344-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_44",
            "tgt_ix": "344-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_45",
            "tgt_ix": "344-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_46",
            "tgt_ix": "344-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_39",
            "tgt_ix": "344-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_39",
            "tgt_ix": "344-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_39",
            "tgt_ix": "344-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_39",
            "tgt_ix": "344-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_39",
            "tgt_ix": "344-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_39",
            "tgt_ix": "344-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_39",
            "tgt_ix": "344-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_39",
            "tgt_ix": "344-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_39",
            "tgt_ix": "344-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_35",
            "tgt_ix": "344-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_47",
            "tgt_ix": "344-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_48",
            "tgt_ix": "344-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_48",
            "tgt_ix": "344-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_35",
            "tgt_ix": "344-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_49",
            "tgt_ix": "344-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_50",
            "tgt_ix": "344-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_50",
            "tgt_ix": "344-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_52",
            "tgt_ix": "344-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_53",
            "tgt_ix": "344-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_54",
            "tgt_ix": "344-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_55",
            "tgt_ix": "344-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_56",
            "tgt_ix": "344-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_57",
            "tgt_ix": "344-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_50",
            "tgt_ix": "344-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_50",
            "tgt_ix": "344-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_50",
            "tgt_ix": "344-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_50",
            "tgt_ix": "344-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_50",
            "tgt_ix": "344-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_50",
            "tgt_ix": "344-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_50",
            "tgt_ix": "344-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_51",
            "tgt_ix": "344-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_0",
            "tgt_ix": "344-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_58",
            "tgt_ix": "344-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_60",
            "tgt_ix": "344-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_61",
            "tgt_ix": "344-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_63",
            "tgt_ix": "344-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_64",
            "tgt_ix": "344-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_65",
            "tgt_ix": "344-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_66",
            "tgt_ix": "344-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_67",
            "tgt_ix": "344-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_68",
            "tgt_ix": "344-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_62",
            "tgt_ix": "344-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "344-ARR_v1_0",
            "tgt_ix": "344-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_1",
            "tgt_ix": "344-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_2",
            "tgt_ix": "344-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_2",
            "tgt_ix": "344-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_2",
            "tgt_ix": "344-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_2",
            "tgt_ix": "344-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_3",
            "tgt_ix": "344-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_3",
            "tgt_ix": "344-ARR_v1_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_4",
            "tgt_ix": "344-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@16",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@17",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@18",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@19",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@20",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_5",
            "tgt_ix": "344-ARR_v1_5@21",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_6",
            "tgt_ix": "344-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_6",
            "tgt_ix": "344-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_6",
            "tgt_ix": "344-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_7",
            "tgt_ix": "344-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_8",
            "tgt_ix": "344-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_8",
            "tgt_ix": "344-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_8",
            "tgt_ix": "344-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_8",
            "tgt_ix": "344-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_8",
            "tgt_ix": "344-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_8",
            "tgt_ix": "344-ARR_v1_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_9",
            "tgt_ix": "344-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_9",
            "tgt_ix": "344-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_9",
            "tgt_ix": "344-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_10",
            "tgt_ix": "344-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_11",
            "tgt_ix": "344-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_12",
            "tgt_ix": "344-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_12",
            "tgt_ix": "344-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_12",
            "tgt_ix": "344-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_12",
            "tgt_ix": "344-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_12",
            "tgt_ix": "344-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_13",
            "tgt_ix": "344-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_14",
            "tgt_ix": "344-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_14",
            "tgt_ix": "344-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_14",
            "tgt_ix": "344-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_15",
            "tgt_ix": "344-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_15",
            "tgt_ix": "344-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_16",
            "tgt_ix": "344-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_16",
            "tgt_ix": "344-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_17",
            "tgt_ix": "344-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_18",
            "tgt_ix": "344-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_18",
            "tgt_ix": "344-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_19",
            "tgt_ix": "344-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_20",
            "tgt_ix": "344-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_20",
            "tgt_ix": "344-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_20",
            "tgt_ix": "344-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_20",
            "tgt_ix": "344-ARR_v1_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_20",
            "tgt_ix": "344-ARR_v1_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_21",
            "tgt_ix": "344-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_22",
            "tgt_ix": "344-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_22",
            "tgt_ix": "344-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_22",
            "tgt_ix": "344-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_22",
            "tgt_ix": "344-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_22",
            "tgt_ix": "344-ARR_v1_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_23",
            "tgt_ix": "344-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_23",
            "tgt_ix": "344-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_24",
            "tgt_ix": "344-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_24",
            "tgt_ix": "344-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_24",
            "tgt_ix": "344-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_24",
            "tgt_ix": "344-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_24",
            "tgt_ix": "344-ARR_v1_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_25",
            "tgt_ix": "344-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_26",
            "tgt_ix": "344-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_27",
            "tgt_ix": "344-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_27",
            "tgt_ix": "344-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_28",
            "tgt_ix": "344-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_28",
            "tgt_ix": "344-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_29",
            "tgt_ix": "344-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_30",
            "tgt_ix": "344-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_30",
            "tgt_ix": "344-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_30",
            "tgt_ix": "344-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_31",
            "tgt_ix": "344-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_31",
            "tgt_ix": "344-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_31",
            "tgt_ix": "344-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_31",
            "tgt_ix": "344-ARR_v1_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_31",
            "tgt_ix": "344-ARR_v1_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_31",
            "tgt_ix": "344-ARR_v1_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_32",
            "tgt_ix": "344-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_33",
            "tgt_ix": "344-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_33",
            "tgt_ix": "344-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_33",
            "tgt_ix": "344-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_34",
            "tgt_ix": "344-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_34",
            "tgt_ix": "344-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_34",
            "tgt_ix": "344-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_34",
            "tgt_ix": "344-ARR_v1_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_34",
            "tgt_ix": "344-ARR_v1_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_35",
            "tgt_ix": "344-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_36",
            "tgt_ix": "344-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_36",
            "tgt_ix": "344-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_37@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_37@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_37@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_37",
            "tgt_ix": "344-ARR_v1_37@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_38",
            "tgt_ix": "344-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_39",
            "tgt_ix": "344-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_40",
            "tgt_ix": "344-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_41",
            "tgt_ix": "344-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_41",
            "tgt_ix": "344-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_41",
            "tgt_ix": "344-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_41",
            "tgt_ix": "344-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_42",
            "tgt_ix": "344-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_42",
            "tgt_ix": "344-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_43",
            "tgt_ix": "344-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_43",
            "tgt_ix": "344-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_43",
            "tgt_ix": "344-ARR_v1_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_44",
            "tgt_ix": "344-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_44",
            "tgt_ix": "344-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_44",
            "tgt_ix": "344-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_44",
            "tgt_ix": "344-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_44",
            "tgt_ix": "344-ARR_v1_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_45",
            "tgt_ix": "344-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_45",
            "tgt_ix": "344-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_45",
            "tgt_ix": "344-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_45",
            "tgt_ix": "344-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_45",
            "tgt_ix": "344-ARR_v1_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_46",
            "tgt_ix": "344-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_47",
            "tgt_ix": "344-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_47",
            "tgt_ix": "344-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_47",
            "tgt_ix": "344-ARR_v1_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_47",
            "tgt_ix": "344-ARR_v1_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_47",
            "tgt_ix": "344-ARR_v1_47@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_47",
            "tgt_ix": "344-ARR_v1_47@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_48",
            "tgt_ix": "344-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_49",
            "tgt_ix": "344-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_49",
            "tgt_ix": "344-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_49",
            "tgt_ix": "344-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_49",
            "tgt_ix": "344-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_49",
            "tgt_ix": "344-ARR_v1_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_50",
            "tgt_ix": "344-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_51",
            "tgt_ix": "344-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_52",
            "tgt_ix": "344-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_52",
            "tgt_ix": "344-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_53",
            "tgt_ix": "344-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_53",
            "tgt_ix": "344-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_53",
            "tgt_ix": "344-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_54",
            "tgt_ix": "344-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_55",
            "tgt_ix": "344-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_55",
            "tgt_ix": "344-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_55",
            "tgt_ix": "344-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_56",
            "tgt_ix": "344-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_56",
            "tgt_ix": "344-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_56",
            "tgt_ix": "344-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_56",
            "tgt_ix": "344-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_56",
            "tgt_ix": "344-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_57",
            "tgt_ix": "344-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_57",
            "tgt_ix": "344-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_58",
            "tgt_ix": "344-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_58",
            "tgt_ix": "344-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_58",
            "tgt_ix": "344-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_58",
            "tgt_ix": "344-ARR_v1_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_58",
            "tgt_ix": "344-ARR_v1_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_58",
            "tgt_ix": "344-ARR_v1_58@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_59",
            "tgt_ix": "344-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_60",
            "tgt_ix": "344-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_60",
            "tgt_ix": "344-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_60",
            "tgt_ix": "344-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_60",
            "tgt_ix": "344-ARR_v1_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_61",
            "tgt_ix": "344-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_61",
            "tgt_ix": "344-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_61",
            "tgt_ix": "344-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_61",
            "tgt_ix": "344-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_61",
            "tgt_ix": "344-ARR_v1_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_62",
            "tgt_ix": "344-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_62",
            "tgt_ix": "344-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_62",
            "tgt_ix": "344-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_62",
            "tgt_ix": "344-ARR_v1_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_63",
            "tgt_ix": "344-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_63",
            "tgt_ix": "344-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_64",
            "tgt_ix": "344-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_64",
            "tgt_ix": "344-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_64",
            "tgt_ix": "344-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_65",
            "tgt_ix": "344-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_65",
            "tgt_ix": "344-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_65",
            "tgt_ix": "344-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_65",
            "tgt_ix": "344-ARR_v1_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_65",
            "tgt_ix": "344-ARR_v1_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_65",
            "tgt_ix": "344-ARR_v1_65@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_65",
            "tgt_ix": "344-ARR_v1_65@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_65",
            "tgt_ix": "344-ARR_v1_65@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_66",
            "tgt_ix": "344-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_67",
            "tgt_ix": "344-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_67",
            "tgt_ix": "344-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_67",
            "tgt_ix": "344-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_67",
            "tgt_ix": "344-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_67",
            "tgt_ix": "344-ARR_v1_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_67",
            "tgt_ix": "344-ARR_v1_67@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_68",
            "tgt_ix": "344-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_68",
            "tgt_ix": "344-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_68",
            "tgt_ix": "344-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_69",
            "tgt_ix": "344-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_70",
            "tgt_ix": "344-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_71",
            "tgt_ix": "344-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_72",
            "tgt_ix": "344-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_73",
            "tgt_ix": "344-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_74",
            "tgt_ix": "344-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_75",
            "tgt_ix": "344-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_76",
            "tgt_ix": "344-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_77",
            "tgt_ix": "344-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_78",
            "tgt_ix": "344-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_79",
            "tgt_ix": "344-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_80",
            "tgt_ix": "344-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_81",
            "tgt_ix": "344-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_82",
            "tgt_ix": "344-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_83",
            "tgt_ix": "344-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_84",
            "tgt_ix": "344-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_85",
            "tgt_ix": "344-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_86",
            "tgt_ix": "344-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_87",
            "tgt_ix": "344-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_88",
            "tgt_ix": "344-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_89",
            "tgt_ix": "344-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_90",
            "tgt_ix": "344-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_91",
            "tgt_ix": "344-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_92",
            "tgt_ix": "344-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_93",
            "tgt_ix": "344-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_94",
            "tgt_ix": "344-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_95",
            "tgt_ix": "344-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_96",
            "tgt_ix": "344-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_97",
            "tgt_ix": "344-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_98",
            "tgt_ix": "344-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_99",
            "tgt_ix": "344-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_100",
            "tgt_ix": "344-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_101",
            "tgt_ix": "344-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_102",
            "tgt_ix": "344-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_103",
            "tgt_ix": "344-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_104",
            "tgt_ix": "344-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_105",
            "tgt_ix": "344-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_106",
            "tgt_ix": "344-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_107",
            "tgt_ix": "344-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_108",
            "tgt_ix": "344-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_109",
            "tgt_ix": "344-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_110",
            "tgt_ix": "344-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_111",
            "tgt_ix": "344-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_112",
            "tgt_ix": "344-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_113",
            "tgt_ix": "344-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_114",
            "tgt_ix": "344-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_115",
            "tgt_ix": "344-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_116",
            "tgt_ix": "344-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_117",
            "tgt_ix": "344-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_118",
            "tgt_ix": "344-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_119",
            "tgt_ix": "344-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_120",
            "tgt_ix": "344-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_121",
            "tgt_ix": "344-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_122",
            "tgt_ix": "344-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_123",
            "tgt_ix": "344-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_124",
            "tgt_ix": "344-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_125",
            "tgt_ix": "344-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_126",
            "tgt_ix": "344-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_127",
            "tgt_ix": "344-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_128",
            "tgt_ix": "344-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_129",
            "tgt_ix": "344-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_130",
            "tgt_ix": "344-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_131",
            "tgt_ix": "344-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_132",
            "tgt_ix": "344-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_133",
            "tgt_ix": "344-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_134",
            "tgt_ix": "344-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_135",
            "tgt_ix": "344-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_136",
            "tgt_ix": "344-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_137",
            "tgt_ix": "344-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_138",
            "tgt_ix": "344-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_139",
            "tgt_ix": "344-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_140",
            "tgt_ix": "344-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "344-ARR_v1_141",
            "tgt_ix": "344-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1369,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "344-ARR",
        "version": 1
    }
}