{
    "nodes": [
        {
            "ix": "390-ARR_v2_0",
            "content": "CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_2",
            "content": "CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks. Previously, CLIP is only regarded as a powerful visual encoder. However, after being pretrained by language supervision from a large amount of image-caption pairs, CLIP itself should also have acquired some few-shot abilities for vision-language tasks. In this work, we empirically show that CLIP can be a strong vision-language few-shot learner by leveraging the power of language. We first evaluate CLIP's zero-shot performance on a typical visual question answering task and demonstrate a zero-shot cross-modality transfer capability of CLIP on the visual entailment task. Then we propose a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task. We achieve competitive zero/fewshot results on the visual question answering and visual entailment tasks without introducing any additional pre-training procedure.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "390-ARR_v2_4",
            "content": "Vision-language understanding (VLU) tasks, such as visual question answering (Antol et al., 2015) and visual entailment (Xie et al., 2019), test a system's ability to comprehensively understand the semantics of both visual world and natural language. To capture the alignment between vision and language, various efforts have been made to build the vision-language pre-trained models (Lu et al., 2019;Chen et al., 2020;Su et al., 2020;Zhang et al., 2021;. Despite their superior performances, these methods have extensively utilized human-annotated training data that are expensive or require expert knowledge, such as object detection datasets (Lin et al., 2014;Kuznetsova et al., 2020) and aligned image-text pairs (Deng et al., 2009;Sharma et al., 2018). Collecting such datasets requires heavy work on data gathering and * Contribution during internship at Microsoft Research.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_5",
            "content": "Figure 1: Examples of the two vision-language understanding tasks. For VQA, language prompts are used. For visual entailment, caption and hypothesis, i.e., texttext pairs, are used in training, while image and hypothesis, i.e., image-text pairs, are used at inference. human annotation, and thus their scales are only in the realm of tens of millions, which are much smaller than the Internet text corpora for NLP pretraining (Devlin et al., 2019;Brown et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_6",
            "content": "Recently, CLIP (Radford et al., 2021) has been proposed to learn visual concepts with natural language supervision, where its 400 million imagetext pairs are crawled from the Internet. CLIP consists of a visual encoder and a text encoder, and it learns visual representations by aligning images and texts through contrastive loss. In this way, CLIP achieves strong zero-shot performances on vision benchmarks such as ImageNet. Besides, Shen et al. (2022) prove that CLIP could be leveraged as a strong visual encoder to benefit downstream vision-language tasks. However, there are two major differences between CLIP and previous visual encoders: 1) it is trained on much larger yet noisy web data, and 2) it has a shallow interaction between vision and language. The first feature promises the generalization ability of CLIP, and the second one equips alignment ability across modalities. Could the strong zero-shot ability of CLIP be transferred to vision-language understanding tasks?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_7",
            "content": "To answer the above question, in this work, we empirically study how to transfer CLIP's zero-shot ability into VLU tasks and further turn CLIP into a few-shot learner. We carried out experiments on two VLU tasks: 1) visual question answering, where the model needs to give an answer according to the details of an image and a natural sentence question, and 2) visual entailment, where the model needs to determine the entailment relation between an image and a natural sentence. Figure 1 demonstrates the basic forms of the two studied tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_8",
            "content": "For the zero-shot visual question answering task, the key to a successful zero-shot capability transfer is to mitigate the gap between the pre-training task of CLIP and the task form of question answering. Inspired by the recent advancements of few-shot learning in NLP (Schick and Sch\u00fctze, 2021b;, we address this issue by introducing a two-step prompt generation strategy, including automatic conversions from question to statement to get masked templates, and a span-infilling with generative pre-trained T5 model (Raffel et al., 2020) to get candidate answers.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_9",
            "content": "We explore a zero-shot cross-modality (language and vision) transfer capability through the visual entailment task. Specifically, we replace the image with its captions during training and only update a small classification layer. Then at inference, as usual, we still use image-text pairs for testing. This allows us to investigate how well the language and vision representations are aligned in CLIP models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_10",
            "content": "We further leverage few-shot learning to improve CLIP's visual question answering performance based on the zero-shot transferring methods. We find that optimizing only bias and normalization (BiNor) parameters would make better use of limited examples and yield better results than the latest few-shot model Frozen (Tsimpoukelli et al., 2021). Experiments confirm that CLIP models can be good vision-language few-shot learners.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_11",
            "content": "Our contributions are summarized as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_12",
            "content": "\u2022 To the best of our knowledge, this is the first work that studies how to transfer CLIP's zeroshot capabilities into VLU tasks and confirms CLIP models can be good few-shot learners. \u2022 A zero-shot cross-modality transfer capability in CLIP is demonstrated. \u2022 A parameter-efficient fine-tuning strategy, Bi-Nor, is proposed to boost CLIP's few-shot visual question answering performance. ResNet (He et al., 2016) and ViT (Dosovitskiy et al., 2020), and a text encoder T, e.g. transformer (Vaswani et al., 2017), where they encode images and texts independently. Followed up is a dot-product between the two encoders' outputs, i.e.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_13",
            "content": "T(text) \u2022 V(image), which is used as an alignment score between the input image and text. It is pretrained to distinguish aligned image-text pairs from randomly combined ones by a contrastive loss. Instead of training on vision benchmarks, CLIP leverages abundant language supervisions from 400 million web-crawled image-text pairs and can conduct a variety of image classification tasks without specific optimizing. However, directly applying CLIP as a vision-language understanding model is still difficult (Kim et al., 2021;Shen et al., 2022).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_14",
            "content": "Vision-Language Understanding Tasks",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "390-ARR_v2_15",
            "content": "Visual question answering. The task of VQA requires the model to answer questions about the details of input images. Following previous work, we experiment on the VQAv2 (Goyal et al., 2017) dataset and formulate the task as a classification problem over 3,129 pre-defined most frequent answers. The images in VQAv2 come from Microsoft COCO (Lin et al., 2014), and there are 65 types of questions in the dataset, such as how many and what color is. For answers, there are three types, including yes/no, number, and other. Visual entailment. Similar to the natural language inference (NLI), the task of visual entailment predicts the entailment relations, including entailment, neutral, and contradiction, between a premise and a hypothesis. Under the VL setting, the premise in visual entailment is based on the details of an image rather than textual descriptions in NLI. The SNLI-VE dataset (Xie et al., 2019) is adapted from SNLI (Bowman et al., 2015) and replaces SNLI's premises with the images in the Flickr30k dataset (Young et al., 2014). Considering the above characteristics, here we leverage the SNLI-VE dataset to verify the zero-shot crossmodality (language and vision) transfer capabilities of the CLIP models. This zero-shot setting investigates how well the vision and language representations are aligned in CLIP models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_16",
            "content": "3 Zero-shot VQA",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_17",
            "content": "A Two-Step Prompt Generation Method",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "390-ARR_v2_18",
            "content": "Previous works (Kim et al., 2021;Shen et al., 2022) have found that directly applying CLIP models for zero-shot VL tasks are infeasible. For example, nearly random-chance level zero-shot performances are observed on the VQAv2 dataset by directly applying a \"question: [question text] answer:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_19",
            "content": "[answer text]\" prompt template (Shen et al., 2022).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_20",
            "content": "After rethinking the essence of prompt engineering in CLIP, we can find that the key to a successful zero-shot capability transfer for the VQA task is to mitigate the gap between natural language description and the form of question answering.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_21",
            "content": "Motivated by the above observations, we propose a two-step automatic prompt generation method to enable the zero-shot VQA capabilities in CLIP models, with the assistant of a pre-trained generative T5 model (Raffel et al., 2020). The key ideas of the two-step prompt generation method is illustrated in Figure 3: the first step is to convert the question into a masked template T , and the second step is to filter out impossible answers by language model and get a candidate answer set V F . The infilled template connects both the question and answers in a natural description way and thus could be an ideal form of prompt for the VQA task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_22",
            "content": "Step I: Automatic Template Generation This step is designed to convert the question into a template, which is a statement with a mask token. To tackle the conversion challenge, we explore two ways, including an in-context demonstration method and a dependency parsing based method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_23",
            "content": "Demonstration to T5. The idea of this conversion method is relatively simple: by demonstrating question-to-template (with [mask] token) examples to the language model, the model could implicitly capture the conversion pattern. We define a few examples for each question type and convert the questions according to their types. Figure 3 shows a conversion example. More cases could be found at appendix D. Specifically, we use T5 (Raffel et al., 2020), a large pre-trained text-to-text Transformer, for the question to template conversion. T5 is pretrained to infill the missing spans (replaced by T5 special tokens, e.g. <extra_id_0>) of a sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_24",
            "content": "We present a concatenation of examples, question, and the <extra_id_0> token to T5 for conditional generation to restore it, and the generated span is our masked template, named as T demo .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_25",
            "content": "Dependency parsing. Although the T5 conversion method works well in most situations, it still faces some out-of-coverage problems. To compensate for this shortcoming, we turn to a traditional dependency parsing based way. This method converts a question to a statement by its part-of-speech tagging and parsing results, where the wh-word, root word, auxiliary, or copula, as well as prepositions and particles that are dependents of the whword or the root, are identified, and transformations are performed according to grammar rules. We use the Stanza (Qi et al., 2020) to POS tag and parse the question and leave the answer as a mask token. Then the rules 1 in Demszky et al. (2018) are leveraged to perform the conversion. We name the template obtained in this way as T parsing .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_26",
            "content": "Step II: Answer Filtering As common sense, \"the specie of a flower\" can never be a vase. Therefore, leveraging pre-trained language models, which have well learned such concepts during pre-training, to filter out less likely answers would have a positive influence on the final question answering performance. Given a masked template T , a language model L, and the answer vocabulary V, we get the filtered answers V F as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_27",
            "content": "Top-k v\u2208V {log P L ([mask] = v|T )} ,(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_28",
            "content": "where the [mask] is the answer span in template T , and P L is the output distribution of the language model. Here we also apply the T5 to infill answers because it makes no assumption about the length and position of the span. Once we get the template T and the filtered answers V F , we replace the [mask] token in template T with every selected answer in V F to get the prompts P.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_29",
            "content": "TAP-C Method for VQA",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "390-ARR_v2_30",
            "content": "The proposed method follows a Template-Answer-Prompt then CLIP discrimination pipeline, and thus we name it as TAP-C. To make better use of template T parsing and T demo , we use an ensemble of both templates by simply setting a threshold for the T5's generation confidence. We prefer to use T demo but use T parsing if the generation confidence is low. Finally, given an image i and the generated prompts P, the TAP-C method can get a zero-shot VQA prediction by:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_31",
            "content": "max v\u2208V F , pv\u2208P {V (i) \u2022 T (p v )} ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_32",
            "content": "where V and T are the visual and text encoders in CLIP models. The p v is a prompt generated by the TAP-C method, where the masked template is infilled with answer v from the filtered answer vocabulary V F .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_33",
            "content": "Zero-shot Cross-modality Transfer",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "390-ARR_v2_34",
            "content": "Recent pre-trained multilingual language models (Wu and Dredze, 2019;Xue et al., 2021) have been shown to be successful in transferring representations across different languages. For example, they can be only fine-tuned on a source language and evaluated on various target languages without specific training, yet still achieving good performance. On the other hand, the CLIP models achieve strong zero-shot performances on both image-to-text and text-to-image retrieval tasks (Radford et al., 2021) only through a dot product between vision and language representations, which gives us an intuition that the two modalities are well aligned in the CLIP models. Is there a cross-modality capability between language and vision in the CLIP models, just like the multilingual ones across languages? To answer the above question, we utilize the visual entailment task ( \u00a7 2.2) to explore the zero-shot cross-modality performance. Figure 4 briefs the key idea. Specifically, we train an MLP classifier over the fused representations of premise and hy-pothesis, and the fusion function is:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_35",
            "content": "fuse (v 1 , v 2 ) = [v 1 , v 2 , v 1 + v 2 , v 1 \u2212 v 2 , v 1 \u2022 v 2 ] ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_36",
            "content": "where v 1 and v 2 are two input vectors. During training, text-only premise and hypothesis are used as the input of CLIP text encoder:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_37",
            "content": "MLP {fuse (T(pre t ), T(hyp t ))} , (3",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_38",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_39",
            "content": "where T is the CLIP text encoder and is not updated during training. And pre t and hyp t are the text premise and hypothesis. Then at inference, the premise is in image and is fed into the CLIP visual encoder. The trained MLP is leveraged for prediction:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_40",
            "content": "max {MLP {fuse (V(pre i ), T(hyp t ))}} ,(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_41",
            "content": "where the pre i is the image premise and V is the CLIP visual encoder.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_42",
            "content": "5 Few-shot Learning for VQA",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_43",
            "content": "In this section, We aim to investigate whether the CLIP models could benefit from few-shot learning, where we work on the visual question answering task to study it.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_44",
            "content": "Setup of Few-shot VQA",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "390-ARR_v2_45",
            "content": "Here we briefly define the terminology used in our few-shot visual question answering settings:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_46",
            "content": "\u2022 Number of ways. Originally, it is defined as the distinct classes in a task. However, rather than defining a 3,129-way task according to the answer vocabulary, we define the number of ways as question type times answer type ( \u00a7 2.2), i.e., 65\u00d73=195 ways, to ensure the model's generalization ability where it can answer a type of questions. \u2022 Number of shots. The number of distinct examples in each way. Here a shot is an image along with the question and the answer. \u2022 Support set and query set.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_47",
            "content": "Parameter-efficient Fine-tuning",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "390-ARR_v2_48",
            "content": "Under the few-shot setting, our goal is to make the CLIP models learn from N-way K-shot examples and improve the zero-shot VQA performance. Specifically, we identify only a very small set of parameters in CLIP models (about 0.3 million out of over 100 million, details in appendix B.3), including the bias term and normalization term, to be optimized. For either the BatchNorm in ResNet or the LayerNorm in Transformer, the normalization could be uniformly denoted as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_49",
            "content": "y = x \u2212 E(x) Var(x) + \u2022 \u03b3 + \u03b2,(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_50",
            "content": "where x and y are the mini-batched input and output, and the \u03b3 and \u03b2 are learned parameters. And for all the linear layers and projection layers in CLIP models, they could be denoted as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_51",
            "content": "o = w \u2022 h + bias,(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_52",
            "content": "where h and o are the input and output vectors. We define the learnable parameter set as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_53",
            "content": "P learn = {bias, \u03b3, \u03b2}.(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_54",
            "content": "We optimize the Bias and Normalization (BiNor) parameters on the few-shot examples with a standard cross-entropy loss over the dot products from each image-prompt pair (Eq.2). Besides, when there are a few examples available, we could also leverage an in-context demonstration manner to improve the performance of the answer filtering process in TAP-C ( \u00a7 3.1) by: Top-k",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_55",
            "content": "v\u2208V {log P L ([mask] = v | [D, T ])} ,(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_56",
            "content": "where the D denotes the demonstrations. D is similar to template T but has been infilled with the answers, and it is sampled from the same type of question in the available few-shot examples. The resulting filtered vocabulary is noted as V demo . We report the few-shot training procedure in appendix C.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_57",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "390-ARR_v2_58",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "390-ARR_v2_59",
            "content": "Datasets. For visual question answering and visual entailment, we carry out experiments on the VQAv2 (Goyal et al., 2017) and the SNLI-VE (Xie et al., 2019) datasets, respectively. We report the statistics of the two datasets in appendix A. For the VQA task's evaluation, we follow the Frozen model (Tsimpoukelli et al., 2021) to calculate the vqa scores on the VQAv2 validation set. For visual entailment, we calculate the accuracy on both validation and test sets through the sklearn toolkit.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_60",
            "content": "CLIP models. According to the types of visual encoders, e.g. ResNet or ViT, CLIP models have different variants, resulting in a significant difference in the number of learnable bias and normalization parameters. We report the number of learnable parameters of CLIP variants in appendix B.3. We select two best performing (and publicly available) variants from two kinds of visual encoders, including the CLIP Res50x16 and the CLIP ViT-B/16, to empirically study their zero-shot and few-shot vision-language understanding performances by applying our transferring methods ( \u00a7 \u00a7 3-5).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_61",
            "content": "Results of Zero-shot VQA",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "390-ARR_v2_62",
            "content": "As previous VL models heavily rely on object detection sub-modules, it is not feasible to directly apply them under the zero-shot setting. Here we setup zero-shot VL baselines from two latest works:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_63",
            "content": "\u2022 Frozen. Frozen (Tsimpoukelli et al., 2021) prompts a seven-billion-parameter 32-layer language model with image representations.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_64",
            "content": "It is trained on aligned image-caption data and is also the first model that shows promising zero-shot and few-shot VQA performances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_65",
            "content": "\u2022 Question irrelevant prompt. We report the zero-shot VQA results in Table 1. The experimental results verify our hypothesis ( \u00a7 3.1) that the prompts of CLIP should be used to describe the labels rather than the tasks. As we can see, the question irrelevant prompting methods simply present the task description and answers to the CLIP models and only get barely better than random guess results. In contrast, by converting questions into templates and filtering answers with pre-trained language models, our TAP-C method enables CLIP models a strong zero-shot capability on the VQA task, even compared with the sevenbillion-parameter Frozen zero-shot model.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_66",
            "content": "Zero-shot Cross-modality Transfer",
            "ntype": "title",
            "meta": {
                "section": "6.3"
            }
        },
        {
            "ix": "390-ARR_v2_67",
            "content": "We report the zero-shot cross-modality transfer results in Table 2. We first investigate the language to vision transfer capability. As introduced in \u00a7 4, we train a classifier on the text-only SNLI-VE dataset where the image is replaced by its caption. At inference, the trained classifier is evaluated by taking the image and text as inputs. As shown in the first group of results, after solely trained on text-text (caption as the premise) entailment data, different CLIP variants could successfully gain a similar discriminative ability under the image-text setting. To ensure that the above results are indeed transferring from language to vision, we made a double check by masking out the images at inference time, and the results are reported at Image Masked. As we can see, the results are similar to a random guess of three relations, indicating the images are of importance in the cross-modality evaluation. Now that we have observed the language to vision transferring capability in CLIP models, we further investigate whether there is also a vision to language transfer capability. We conduct a similar experiment but train the classifier on the original SNLI-VE dataset, i.e., image premise and text hypothesis. At inference, we evaluate the classifier with the text-only valid and test data. The results are reported in Table 2, which confirms the vision to language capability. Since text data are usually much cheaper than visual data, the first kind of transferring tends to be more promising in practice.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_68",
            "content": "Results of Few-shot VQA",
            "ntype": "title",
            "meta": {
                "section": "6.4"
            }
        },
        {
            "ix": "390-ARR_v2_69",
            "content": "We report the few-shot VQA results in Table 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_70",
            "content": "We take the Frozen model and the image blacked out Frozen blind as baselines. Under different k, our methods could always learn from limited training examples and improve over the zero-shot results, which confirms that CLIP models could be VL fewshot learners. With the increase of the number of shots, significant performance gains are observed in other category, which concurs with our intuition: as we sample examples from each question type, most answers in other category are not revealed to the model. As a result, the model could always learn to improve. Similarly, presenting examples to the T5 could also improve the answer filtering process, leading to significant performance gains over the other category. In contrast, the score of number category improves significantly when the model just begins to see some training examples while slowing down as k continues to increase.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_71",
            "content": "Analyses and Discussion",
            "ntype": "title",
            "meta": {
                "section": "6.5"
            }
        },
        {
            "ix": "390-ARR_v2_72",
            "content": "The effects of template generation methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_73",
            "content": "Our TAP-C method uses an ensemble of dependency parsing template T parsing and T5 demonstration template T demo . Here we investigate whether it is necessary to use such an ensemble. We report the ablation results of two templates in Table 4. The results show that the two templates have different effects over different questions, and the ensemble could make the best use of their advantages.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_74",
            "content": "The effects of two steps in TAP-C. The TAP-C method generates prompts through template generation (t.gen.) and answer filtering (a.filt.). Here we quantify how much each step contributes to the final zero/few-shot VQA performances. We report the ablation results in Table 5. When we remove the answer filtering step (w/o a.filt.), both the zeroshot and few-shot performances generally fall by about 20%, but the models still retain some fewshot learning capabilities. We further remove the template generation step and only use question irrelevant templates: all results are nearly cut in half, indicating the importance of considering questions in both zero-shot and few-shot scenarios. Comparisons of fine-tuning methods. We only update the bias and normalization parameters during few-shot learning ( \u00a7 5.2). To investigate whether our BiNor fine-tuning strategy works well, we compare BiNor with two fine-tuning methods: 1) Full-FT (Full fine-tuning), which updates all parameters in the model. 2) BitFit (Ben Zaken et al., 2021), which only updates the bias-terms in all model layers. We report the comparison results in Table 6. Both BiNor and BitFit significantly outperform the full fine-tuning way: millions of parameters are very easy to overfit to a few training examples. When k is small, the performance differences between BiNor and BitFit are very small. When k becomes larger, BiNor begins to outperform BitFit with a noticeable margin. Our BiNor fine-tuning strategy is similar to the BitFit but differs in that it also updates the normalization parameters, which would grant the ResNet CLIP models better flexibility to adapt to new examples due to their larger number of batch normalization parameters. For the specific number of different parameters in each CLIP variant, please refer to the appendix B.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_75",
            "content": "Limitations of TAP-C. The proposed TAP-C method explores CLIP models' potential to conduct zero/few-shot VQA tasks. However, we also found several limitations that hinder further improving the few-shot performance, which could be rooted in the CLIP models. First, CLIP models struggle with counting the number of fine-grained objects in an image, especially counting from a small area of the image. This shortcoming can hardly be improved by any kind of language knowledge. Besides, the CLIP models perform poorly in distinguishing subtle semantic differences. For example, when asked \"what is the man in the background doing?\", all the experimented CLIP models give predictions of the man \"in the foreground\". Under such cases, even if the TAP-C method perfectly converts the question into a prompt, the final results would still be wrong. Nevertheless, We believe this issue could be well addressed by enhancing CLIP models with a stronger text encoder, and we will make explorations in future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_76",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "390-ARR_v2_77",
            "content": "Vision-language few-shot learning. Leveraging aligned caption data, vision-language models pre-trained by an image-text discriminative loss have recently enabled strong zero-shot generalization on image classification and cross-modality retrieval tasks (Jia et al., 2021;Radford et al., 2021). Different from the discriminative manner, Tsimpoukelli et al. ( 2021) prompt a large frozen language model with vision prefix in a generative way, which is the first vision-language few-shot model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_78",
            "content": "Language model prompting. This work is also inspired by the line of research in language model prompting (Liu et al., 2021). Initialized by the GPT series (Radford et al., 2018(Radford et al., , 2019Brown et al., 2020), prompting has become a popular manner to mining knowledge from pre-trained language models (Petroni et al., 2019) in a zero-shot or few-shot way (Shin et al., 2020;Qin and Eisner, 2021). Besides mining knowledge from the language model, PET work (Schick and Sch\u00fctze, 2021a,b) presents a semi-supervised prompting method for improving few-shot language understanding performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_79",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "8"
            }
        },
        {
            "ix": "390-ARR_v2_80",
            "content": "In this work, we empirically studied how to transfer CLIP models into vision-language understanding tasks. We first explored the CLIP models' zero-shot VQA capability by leveraging language prompts and further proposed a parameter-efficient finetuning method to boost the few-shot performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_81",
            "content": "We also demonstrate a zero-shot cross-modality transfer capability of CLIP models on the visual entailment task. Experiments and analyses on VQAv2 and SNLI-VE confirm that the CLIP models can be good VL few-shot learners.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_82",
            "content": "In this section, we showcase several template generation examples to illustrate how the proposed method works. Since we have introduced how to convert a question into a masked template by demonstrating examples to the T5 ( \u00a7 3.1), here we directly present several examples in Table 12. These examples are sampled from five different question types and also cover the three answer types. As shown in Table 12, a single demo in the demonstration consists of a question and an answer with the [mask] token. Notice that the [mask] token is only a placeholder rather than a real mask in the pre-trained language models. Different from the <extra_id_0> in T5 that represents a corrupted span, the [mask] is used to inform the T5 where the answer words should be placed. After seeing several examples in the demonstration, the powerful T5 large model could capture the conversion pattern in each type of question and perfectly complete most conversions without ignoring the subtle grammars. Once the masked template is generated, we could infill the [mask] place with answer words and then carry out further processing. The processing for the yes/no type is a little different: as it is a binary task, we directly generate a positive prompt and a negative prompt, rather than masked templates, for the yes and no, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "390-ARR_v2_83",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Vqa: Visual question answering, 2015, Proceedings of the IEEE international conference on computer vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Stanislaw Antol",
                    "Aishwarya Agrawal",
                    "Jiasen Lu",
                    "Margaret Mitchell",
                    "Dhruv Batra",
                    "Lawrence Zitnick",
                    "Devi Parikh"
                ],
                "title": "Vqa: Visual question answering",
                "pub_date": "2015",
                "pub_title": "Proceedings of the IEEE international conference on computer vision",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_84",
            "content": "UNKNOWN, None, 2021, Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_85",
            "content": "Samuel Bowman, Gabor Angeli, Christopher Potts, Christopher D Manning, A large annotated corpus for learning natural language inference, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Samuel Bowman",
                    "Gabor Angeli",
                    "Christopher Potts",
                    "Christopher D Manning"
                ],
                "title": "A large annotated corpus for learning natural language inference",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_86",
            "content": "Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry,  Askell, Language models are few-shot learners, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Benjamin Tom B Brown",
                    "Nick Mann",
                    "Melanie Ryder",
                    "Jared Subbiah",
                    "Prafulla Kaplan",
                    "Arvind Dhariwal",
                    "Pranav Neelakantan",
                    "Girish Shyam",
                    "Amanda Sastry",
                    " Askell"
                ],
                "title": "Language models are few-shot learners",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_87",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Uniter: Universal image-text representation learning, 2020, European conference on computer vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Yen-Chun Chen",
                    "Linjie Li",
                    "Licheng Yu",
                    "Ahmed Kholy",
                    "Faisal Ahmed",
                    "Zhe Gan",
                    "Yu Cheng",
                    "Jingjing Liu"
                ],
                "title": "Uniter: Universal image-text representation learning",
                "pub_date": "2020",
                "pub_title": "European conference on computer vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "390-ARR_v2_88",
            "content": "UNKNOWN, None, 2018, Transforming question answering datasets into natural language inference datasets, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Transforming question answering datasets into natural language inference datasets",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_89",
            "content": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Imagenet: A large-scale hierarchical image database, 2009, 2009 IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jia Deng",
                    "Wei Dong",
                    "Richard Socher",
                    "Li-Jia Li",
                    "Kai Li",
                    "Li Fei-Fei"
                ],
                "title": "Imagenet: A large-scale hierarchical image database",
                "pub_date": "2009",
                "pub_title": "2009 IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_90",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_91",
            "content": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, An image is worth 16x16 words: Transformers for image recognition at scale, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Alexey Dosovitskiy",
                    "Lucas Beyer",
                    "Alexander Kolesnikov",
                    "Dirk Weissenborn",
                    "Xiaohua Zhai",
                    "Thomas Unterthiner",
                    "Mostafa Dehghani",
                    "Matthias Minderer",
                    "Georg Heigold",
                    "Sylvain Gelly"
                ],
                "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_92",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Proceedings of the ACL 2021, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Tianyu Gao",
                    "Adam Fisch",
                    "Danqi Chen"
                ],
                "title": "Making pre-trained language models better few-shot learners",
                "pub_date": "2021",
                "pub_title": "Proceedings of the ACL 2021",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "390-ARR_v2_93",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Yash Goyal",
                    "Tejas Khot",
                    "Douglas Summers-Stay",
                    "Dhruv Batra",
                    "Devi Parikh"
                ],
                "title": "Making the v in vqa matter: Elevating the role of image understanding in visual question answering",
                "pub_date": "2017",
                "pub_title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_94",
            "content": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep residual learning for image recognition, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Kaiming He",
                    "Xiangyu Zhang",
                    "Shaoqing Ren",
                    "Jian Sun"
                ],
                "title": "Deep residual learning for image recognition",
                "pub_date": "2016",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_95",
            "content": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, V Quoc, Yunhsuan Le, Zhen Sung, Tom Li,  Duerig, 2021. Scaling up visual and vision-language representation learning with noisy text supervision, , International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Chao Jia",
                    "Yinfei Yang",
                    "Ye Xia",
                    "Yi-Ting Chen",
                    "Zarana Parekh",
                    "Hieu Pham",
                    "V Quoc",
                    "Yunhsuan Le",
                    "Zhen Sung",
                    "Tom Li",
                    " Duerig"
                ],
                "title": "2021. Scaling up visual and vision-language representation learning with noisy text supervision",
                "pub_date": null,
                "pub_title": "International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_96",
            "content": "Wonjae Kim, Bokyung Son, Ildoo Kim, Vilt: Vision-and-language transformer without convolution or region supervision, 2021, International Conference on Machine Learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Wonjae Kim",
                    "Bokyung Son",
                    "Ildoo Kim"
                ],
                "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
                "pub_date": "2021",
                "pub_title": "International Conference on Machine Learning",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_97",
            "content": "Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, The open images dataset v4, 2020, International Journal of Computer Vision, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Alina Kuznetsova",
                    "Hassan Rom",
                    "Neil Alldrin",
                    "Jasper Uijlings",
                    "Ivan Krasin",
                    "Jordi Pont-Tuset",
                    "Shahab Kamali",
                    "Stefan Popov",
                    "Matteo Malloci",
                    "Alexander Kolesnikov"
                ],
                "title": "The open images dataset v4",
                "pub_date": "2020",
                "pub_title": "International Journal of Computer Vision",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_98",
            "content": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco: Common objects in context, 2014, European conference on computer vision, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Tsung-Yi Lin",
                    "Michael Maire",
                    "Serge Belongie",
                    "James Hays",
                    "Pietro Perona",
                    "Deva Ramanan",
                    "Piotr Doll\u00e1r",
                    "C Lawrence Zitnick"
                ],
                "title": "Microsoft coco: Common objects in context",
                "pub_date": "2014",
                "pub_title": "European conference on computer vision",
                "pub": "Springer"
            }
        },
        {
            "ix": "390-ARR_v2_99",
            "content": "UNKNOWN, None, 2021, Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_100",
            "content": "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer, Multilingual denoising pre-training for neural machine translation, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Yinhan Liu",
                    "Jiatao Gu",
                    "Naman Goyal",
                    "Xian Li",
                    "Sergey Edunov",
                    "Marjan Ghazvininejad",
                    "Mike Lewis",
                    "Luke Zettlemoyer"
                ],
                "title": "Multilingual denoising pre-training for neural machine translation",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_101",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze-questions for few-shot text classification and natural language inference, 2021, Proceedings of the 16th Conference of the EACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Exploiting cloze-questions for few-shot text classification and natural language inference",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 16th Conference of the EACL",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_102",
            "content": "Timo Schick, Hinrich Sch\u00fctze, It's not just size that matters: Small language models are also few-shot learners, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Timo Schick",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "It's not just size that matters: Small language models are also few-shot learners",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_103",
            "content": "Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Piyush Sharma",
                    "Nan Ding",
                    "Sebastian Goodman",
                    "Radu Soricut"
                ],
                "title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_104",
            "content": "Sheng Shen, Liunian Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, Kurt Keutzer, How much can clip benefit vision-and-language tasks, 2022, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Sheng Shen",
                    "Liunian Li",
                    "Hao Tan",
                    "Mohit Bansal",
                    "Anna Rohrbach",
                    "Kai-Wei Chang",
                    "Zhewei Yao",
                    "Kurt Keutzer"
                ],
                "title": "How much can clip benefit vision-and-language tasks",
                "pub_date": "2022",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_105",
            "content": "Taylor Shin, Yasaman Razeghi, I Robert L Logan, Eric Wallace, Sameer Singh, AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts, 2020, Proceedings of EMNLP 2020, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Taylor Shin",
                    "Yasaman Razeghi",
                    "I Robert L Logan",
                    "Eric Wallace",
                    "Sameer Singh"
                ],
                "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts",
                "pub_date": "2020",
                "pub_title": "Proceedings of EMNLP 2020",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_106",
            "content": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, Vl-bert: Pretraining of generic visual-linguistic representations, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Weijie Su",
                    "Xizhou Zhu",
                    "Yue Cao",
                    "Bin Li",
                    "Lewei Lu",
                    "Furu Wei",
                    "Jifeng Dai"
                ],
                "title": "Vl-bert: Pretraining of generic visual-linguistic representations",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_107",
            "content": "UNKNOWN, None, 2021, Multimodal few-shot learning with frozen language models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Multimodal few-shot learning with frozen language models",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_108",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_109",
            "content": "UNKNOWN, None, 2021, Vlmo: Unified vision-language pre-training with mixture-of-modality-experts, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_110",
            "content": "Shijie Wu, Mark Dredze, Beto, bentz, becas: The surprising cross-lingual effectiveness of bert, 2019, Proceedings of EMNLP 2019, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Shijie Wu",
                    "Mark Dredze"
                ],
                "title": "Beto, bentz, becas: The surprising cross-lingual effectiveness of bert",
                "pub_date": "2019",
                "pub_title": "Proceedings of EMNLP 2019",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_111",
            "content": "UNKNOWN, None, 2019, Visual entailment: A novel task for fine-grained image understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Visual entailment: A novel task for fine-grained image understanding",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_112",
            "content": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer, , Proceedings of NAACL 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Linting Xue",
                    "Noah Constant",
                    "Adam Roberts",
                    "Mihir Kale",
                    "Rami Al-Rfou",
                    "Aditya Siddhant"
                ],
                "title": "Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer",
                "pub_date": null,
                "pub_title": "Proceedings of NAACL 2021",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_113",
            "content": "Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions, 2014, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Peter Young",
                    "Alice Lai",
                    "Micah Hodosh",
                    "Julia Hockenmaier"
                ],
                "title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
                "pub_date": "2014",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_114",
            "content": "Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, Qi Tian, Deep modular co-attention networks for visual question answering, 2019, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Zhou Yu",
                    "Jun Yu",
                    "Yuhao Cui",
                    "Dacheng Tao",
                    "Qi Tian"
                ],
                "title": "Deep modular co-attention networks for visual question answering",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        },
        {
            "ix": "390-ARR_v2_115",
            "content": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, Vinvl: Revisiting visual representations in vision-language models, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Pengchuan Zhang",
                    "Xiujun Li",
                    "Xiaowei Hu",
                    "Jianwei Yang",
                    "Lei Zhang",
                    "Lijuan Wang",
                    "Yejin Choi",
                    "Jianfeng Gao"
                ],
                "title": "Vinvl: Revisiting visual representations in vision-language models",
                "pub_date": "2021",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "390-ARR_v2_0@0",
            "content": "CLIP Models are Few-shot Learners: Empirical Studies on VQA and Visual Entailment",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_0",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_2@0",
            "content": "CLIP has shown a remarkable zero-shot capability on a wide range of vision tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_2",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_2@1",
            "content": "Previously, CLIP is only regarded as a powerful visual encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_2",
            "start": 82,
            "end": 144,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_2@2",
            "content": "However, after being pretrained by language supervision from a large amount of image-caption pairs, CLIP itself should also have acquired some few-shot abilities for vision-language tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_2",
            "start": 146,
            "end": 333,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_2@3",
            "content": "In this work, we empirically show that CLIP can be a strong vision-language few-shot learner by leveraging the power of language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_2",
            "start": 335,
            "end": 463,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_2@4",
            "content": "We first evaluate CLIP's zero-shot performance on a typical visual question answering task and demonstrate a zero-shot cross-modality transfer capability of CLIP on the visual entailment task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_2",
            "start": 465,
            "end": 656,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_2@5",
            "content": "Then we propose a parameter-efficient fine-tuning strategy to boost the few-shot performance on the vqa task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_2",
            "start": 658,
            "end": 766,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_2@6",
            "content": "We achieve competitive zero/fewshot results on the visual question answering and visual entailment tasks without introducing any additional pre-training procedure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_2",
            "start": 768,
            "end": 930,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_4@0",
            "content": "Vision-language understanding (VLU) tasks, such as visual question answering (Antol et al., 2015) and visual entailment (Xie et al., 2019), test a system's ability to comprehensively understand the semantics of both visual world and natural language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_4",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_4@1",
            "content": "To capture the alignment between vision and language, various efforts have been made to build the vision-language pre-trained models (Lu et al., 2019;Chen et al., 2020;Su et al., 2020;Zhang et al., 2021;. Despite their superior performances, these methods have extensively utilized human-annotated training data that are expensive or require expert knowledge, such as object detection datasets (Lin et al., 2014;Kuznetsova et al., 2020) and aligned image-text pairs (Deng et al., 2009;Sharma et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_4",
            "start": 251,
            "end": 756,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_4@2",
            "content": "Collecting such datasets requires heavy work on data gathering and * Contribution during internship at Microsoft Research.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_4",
            "start": 758,
            "end": 879,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_5@0",
            "content": "Figure 1: Examples of the two vision-language understanding tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_5",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_5@1",
            "content": "For VQA, language prompts are used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_5",
            "start": 67,
            "end": 101,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_5@2",
            "content": "For visual entailment, caption and hypothesis, i.e., texttext pairs, are used in training, while image and hypothesis, i.e., image-text pairs, are used at inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_5",
            "start": 103,
            "end": 267,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_5@3",
            "content": "human annotation, and thus their scales are only in the realm of tens of millions, which are much smaller than the Internet text corpora for NLP pretraining (Devlin et al., 2019;Brown et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_5",
            "start": 269,
            "end": 466,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_6@0",
            "content": "Recently, CLIP (Radford et al., 2021) has been proposed to learn visual concepts with natural language supervision, where its 400 million imagetext pairs are crawled from the Internet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_6",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_6@1",
            "content": "CLIP consists of a visual encoder and a text encoder, and it learns visual representations by aligning images and texts through contrastive loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_6",
            "start": 185,
            "end": 329,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_6@2",
            "content": "In this way, CLIP achieves strong zero-shot performances on vision benchmarks such as ImageNet. Besides, Shen et al. (2022) prove that CLIP could be leveraged as a strong visual encoder to benefit downstream vision-language tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_6",
            "start": 331,
            "end": 560,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_6@3",
            "content": "However, there are two major differences between CLIP and previous visual encoders: 1) it is trained on much larger yet noisy web data, and 2) it has a shallow interaction between vision and language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_6",
            "start": 562,
            "end": 761,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_6@4",
            "content": "The first feature promises the generalization ability of CLIP, and the second one equips alignment ability across modalities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_6",
            "start": 763,
            "end": 887,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_6@5",
            "content": "Could the strong zero-shot ability of CLIP be transferred to vision-language understanding tasks?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_6",
            "start": 889,
            "end": 985,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_7@0",
            "content": "To answer the above question, in this work, we empirically study how to transfer CLIP's zero-shot ability into VLU tasks and further turn CLIP into a few-shot learner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_7",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_7@1",
            "content": "We carried out experiments on two VLU tasks: 1) visual question answering, where the model needs to give an answer according to the details of an image and a natural sentence question, and 2) visual entailment, where the model needs to determine the entailment relation between an image and a natural sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_7",
            "start": 168,
            "end": 477,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_7@2",
            "content": "Figure 1 demonstrates the basic forms of the two studied tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_7",
            "start": 479,
            "end": 541,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_8@0",
            "content": "For the zero-shot visual question answering task, the key to a successful zero-shot capability transfer is to mitigate the gap between the pre-training task of CLIP and the task form of question answering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_8",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_8@1",
            "content": "Inspired by the recent advancements of few-shot learning in NLP (Schick and Sch\u00fctze, 2021b;, we address this issue by introducing a two-step prompt generation strategy, including automatic conversions from question to statement to get masked templates, and a span-infilling with generative pre-trained T5 model (Raffel et al., 2020) to get candidate answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_8",
            "start": 206,
            "end": 563,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_9@0",
            "content": "We explore a zero-shot cross-modality (language and vision) transfer capability through the visual entailment task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_9",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_9@1",
            "content": "Specifically, we replace the image with its captions during training and only update a small classification layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_9",
            "start": 116,
            "end": 229,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_9@2",
            "content": "Then at inference, as usual, we still use image-text pairs for testing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_9",
            "start": 231,
            "end": 301,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_9@3",
            "content": "This allows us to investigate how well the language and vision representations are aligned in CLIP models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_9",
            "start": 303,
            "end": 408,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_10@0",
            "content": "We further leverage few-shot learning to improve CLIP's visual question answering performance based on the zero-shot transferring methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_10",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_10@1",
            "content": "We find that optimizing only bias and normalization (BiNor) parameters would make better use of limited examples and yield better results than the latest few-shot model Frozen (Tsimpoukelli et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_10",
            "start": 139,
            "end": 342,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_10@2",
            "content": "Experiments confirm that CLIP models can be good vision-language few-shot learners.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_10",
            "start": 344,
            "end": 426,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_11@0",
            "content": "Our contributions are summarized as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_11",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_12@0",
            "content": "\u2022 To the best of our knowledge, this is the first work that studies how to transfer CLIP's zeroshot capabilities into VLU tasks and confirms CLIP models can be good few-shot learners. \u2022 A zero-shot cross-modality transfer capability in CLIP is demonstrated. \u2022 A parameter-efficient fine-tuning strategy, Bi-Nor, is proposed to boost CLIP's few-shot visual question answering performance. ResNet (He et al., 2016) and ViT (Dosovitskiy et al., 2020), and a text encoder T, e.g. transformer (Vaswani et al., 2017), where they encode images and texts independently. Followed up is a dot-product between the two encoders' outputs, i.e.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_12",
            "start": 0,
            "end": 629,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_13@0",
            "content": "T(text) \u2022 V(image), which is used as an alignment score between the input image and text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_13",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_13@1",
            "content": "It is pretrained to distinguish aligned image-text pairs from randomly combined ones by a contrastive loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_13",
            "start": 90,
            "end": 196,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_13@2",
            "content": "Instead of training on vision benchmarks, CLIP leverages abundant language supervisions from 400 million web-crawled image-text pairs and can conduct a variety of image classification tasks without specific optimizing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_13",
            "start": 198,
            "end": 415,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_13@3",
            "content": "However, directly applying CLIP as a vision-language understanding model is still difficult (Kim et al., 2021;Shen et al., 2022).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_13",
            "start": 417,
            "end": 545,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_14@0",
            "content": "Vision-Language Understanding Tasks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_14",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@0",
            "content": "Visual question answering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@1",
            "content": "The task of VQA requires the model to answer questions about the details of input images.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 27,
            "end": 115,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@2",
            "content": "Following previous work, we experiment on the VQAv2 (Goyal et al., 2017) dataset and formulate the task as a classification problem over 3,129 pre-defined most frequent answers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 117,
            "end": 293,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@3",
            "content": "The images in VQAv2 come from Microsoft COCO (Lin et al., 2014), and there are 65 types of questions in the dataset, such as how many and what color is.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 295,
            "end": 446,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@4",
            "content": "For answers, there are three types, including yes/no, number, and other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 448,
            "end": 519,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@5",
            "content": "Visual entailment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 521,
            "end": 538,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@6",
            "content": "Similar to the natural language inference (NLI), the task of visual entailment predicts the entailment relations, including entailment, neutral, and contradiction, between a premise and a hypothesis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 540,
            "end": 738,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@7",
            "content": "Under the VL setting, the premise in visual entailment is based on the details of an image rather than textual descriptions in NLI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 740,
            "end": 870,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@8",
            "content": "The SNLI-VE dataset (Xie et al., 2019) is adapted from SNLI (Bowman et al., 2015) and replaces SNLI's premises with the images in the Flickr30k dataset (Young et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 872,
            "end": 1044,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@9",
            "content": "Considering the above characteristics, here we leverage the SNLI-VE dataset to verify the zero-shot crossmodality (language and vision) transfer capabilities of the CLIP models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 1046,
            "end": 1222,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_15@10",
            "content": "This zero-shot setting investigates how well the vision and language representations are aligned in CLIP models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_15",
            "start": 1224,
            "end": 1335,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_16@0",
            "content": "3 Zero-shot VQA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_16",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_17@0",
            "content": "A Two-Step Prompt Generation Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_17",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_18@0",
            "content": "Previous works (Kim et al., 2021;Shen et al., 2022) have found that directly applying CLIP models for zero-shot VL tasks are infeasible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_18",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_18@1",
            "content": "For example, nearly random-chance level zero-shot performances are observed on the VQAv2 dataset by directly applying a \"question: [question text] answer:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_18",
            "start": 137,
            "end": 290,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_19@0",
            "content": "[answer text]\" prompt template (Shen et al., 2022).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_19",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_20@0",
            "content": "After rethinking the essence of prompt engineering in CLIP, we can find that the key to a successful zero-shot capability transfer for the VQA task is to mitigate the gap between natural language description and the form of question answering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_20",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_21@0",
            "content": "Motivated by the above observations, we propose a two-step automatic prompt generation method to enable the zero-shot VQA capabilities in CLIP models, with the assistant of a pre-trained generative T5 model (Raffel et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_21",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_21@1",
            "content": "The key ideas of the two-step prompt generation method is illustrated in Figure 3: the first step is to convert the question into a masked template T , and the second step is to filter out impossible answers by language model and get a candidate answer set V F .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_21",
            "start": 230,
            "end": 491,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_21@2",
            "content": "The infilled template connects both the question and answers in a natural description way and thus could be an ideal form of prompt for the VQA task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_21",
            "start": 493,
            "end": 641,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_22@0",
            "content": "Step I: Automatic Template Generation This step is designed to convert the question into a template, which is a statement with a mask token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_22",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_22@1",
            "content": "To tackle the conversion challenge, we explore two ways, including an in-context demonstration method and a dependency parsing based method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_22",
            "start": 141,
            "end": 280,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_23@0",
            "content": "Demonstration to T5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_23",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_23@1",
            "content": "The idea of this conversion method is relatively simple: by demonstrating question-to-template (with [mask] token) examples to the language model, the model could implicitly capture the conversion pattern.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_23",
            "start": 21,
            "end": 225,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_23@2",
            "content": "We define a few examples for each question type and convert the questions according to their types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_23",
            "start": 227,
            "end": 325,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_23@3",
            "content": "Figure 3 shows a conversion example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_23",
            "start": 327,
            "end": 362,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_23@4",
            "content": "More cases could be found at appendix D. Specifically, we use T5 (Raffel et al., 2020), a large pre-trained text-to-text Transformer, for the question to template conversion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_23",
            "start": 364,
            "end": 537,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_23@5",
            "content": "T5 is pretrained to infill the missing spans (replaced by T5 special tokens, e.g. <extra_id_0>) of a sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_23",
            "start": 539,
            "end": 648,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_24@0",
            "content": "We present a concatenation of examples, question, and the <extra_id_0> token to T5 for conditional generation to restore it, and the generated span is our masked template, named as T demo .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_24",
            "start": 0,
            "end": 188,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_25@0",
            "content": "Dependency parsing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_25",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_25@1",
            "content": "Although the T5 conversion method works well in most situations, it still faces some out-of-coverage problems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_25",
            "start": 20,
            "end": 129,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_25@2",
            "content": "To compensate for this shortcoming, we turn to a traditional dependency parsing based way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_25",
            "start": 131,
            "end": 220,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_25@3",
            "content": "This method converts a question to a statement by its part-of-speech tagging and parsing results, where the wh-word, root word, auxiliary, or copula, as well as prepositions and particles that are dependents of the whword or the root, are identified, and transformations are performed according to grammar rules.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_25",
            "start": 222,
            "end": 533,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_25@4",
            "content": "We use the Stanza (Qi et al., 2020) to POS tag and parse the question and leave the answer as a mask token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_25",
            "start": 535,
            "end": 641,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_25@5",
            "content": "Then the rules 1 in Demszky et al. (2018) are leveraged to perform the conversion.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_25",
            "start": 643,
            "end": 724,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_25@6",
            "content": "We name the template obtained in this way as T parsing .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_25",
            "start": 726,
            "end": 781,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_26@0",
            "content": "Step II: Answer Filtering As common sense, \"the specie of a flower\" can never be a vase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_26",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_26@1",
            "content": "Therefore, leveraging pre-trained language models, which have well learned such concepts during pre-training, to filter out less likely answers would have a positive influence on the final question answering performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_26",
            "start": 89,
            "end": 308,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_26@2",
            "content": "Given a masked template T , a language model L, and the answer vocabulary V, we get the filtered answers V F as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_26",
            "start": 310,
            "end": 421,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_27@0",
            "content": "Top-k v\u2208V {log P L ([mask] = v|T )} ,(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_27",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_28@0",
            "content": "where the [mask] is the answer span in template T , and P L is the output distribution of the language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_28",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_28@1",
            "content": "Here we also apply the T5 to infill answers because it makes no assumption about the length and position of the span.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_28",
            "start": 110,
            "end": 226,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_28@2",
            "content": "Once we get the template T and the filtered answers V F , we replace the [mask] token in template T with every selected answer in V F to get the prompts P.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_28",
            "start": 228,
            "end": 382,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_29@0",
            "content": "TAP-C Method for VQA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_29",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_30@0",
            "content": "The proposed method follows a Template-Answer-Prompt then CLIP discrimination pipeline, and thus we name it as TAP-C. To make better use of template T parsing and T demo , we use an ensemble of both templates by simply setting a threshold for the T5's generation confidence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_30",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_30@1",
            "content": "We prefer to use T demo but use T parsing if the generation confidence is low.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_30",
            "start": 275,
            "end": 352,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_30@2",
            "content": "Finally, given an image i and the generated prompts P, the TAP-C method can get a zero-shot VQA prediction by:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_30",
            "start": 354,
            "end": 463,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_31@0",
            "content": "max v\u2208V F , pv\u2208P {V (i) \u2022 T (p v )} ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_31",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_32@0",
            "content": "where V and T are the visual and text encoders in CLIP models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_32",
            "start": 0,
            "end": 61,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_32@1",
            "content": "The p v is a prompt generated by the TAP-C method, where the masked template is infilled with answer v from the filtered answer vocabulary V F .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_32",
            "start": 63,
            "end": 206,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_33@0",
            "content": "Zero-shot Cross-modality Transfer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_33",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_34@0",
            "content": "Recent pre-trained multilingual language models (Wu and Dredze, 2019;Xue et al., 2021) have been shown to be successful in transferring representations across different languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_34",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_34@1",
            "content": "For example, they can be only fine-tuned on a source language and evaluated on various target languages without specific training, yet still achieving good performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_34",
            "start": 180,
            "end": 347,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_34@2",
            "content": "On the other hand, the CLIP models achieve strong zero-shot performances on both image-to-text and text-to-image retrieval tasks (Radford et al., 2021) only through a dot product between vision and language representations, which gives us an intuition that the two modalities are well aligned in the CLIP models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_34",
            "start": 349,
            "end": 660,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_34@3",
            "content": "Is there a cross-modality capability between language and vision in the CLIP models, just like the multilingual ones across languages? To answer the above question, we utilize the visual entailment task ( \u00a7 2.2) to explore the zero-shot cross-modality performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_34",
            "start": 662,
            "end": 925,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_34@4",
            "content": "Figure 4 briefs the key idea.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_34",
            "start": 927,
            "end": 955,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_34@5",
            "content": "Specifically, we train an MLP classifier over the fused representations of premise and hy-pothesis, and the fusion function is:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_34",
            "start": 957,
            "end": 1083,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_35@0",
            "content": "fuse (v 1 , v 2 ) = [v 1 , v 2 , v 1 + v 2 , v 1 \u2212 v 2 , v 1 \u2022 v 2 ] ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_35",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_36@0",
            "content": "where v 1 and v 2 are two input vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_36",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_36@1",
            "content": "During training, text-only premise and hypothesis are used as the input of CLIP text encoder:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_36",
            "start": 41,
            "end": 133,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_37@0",
            "content": "MLP {fuse (T(pre t ), T(hyp t ))} , (3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_37",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_38@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_38",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_39@0",
            "content": "where T is the CLIP text encoder and is not updated during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_39",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_39@1",
            "content": "And pre t and hyp t are the text premise and hypothesis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_39",
            "start": 69,
            "end": 124,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_39@2",
            "content": "Then at inference, the premise is in image and is fed into the CLIP visual encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_39",
            "start": 126,
            "end": 208,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_39@3",
            "content": "The trained MLP is leveraged for prediction:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_39",
            "start": 210,
            "end": 253,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_40@0",
            "content": "max {MLP {fuse (V(pre i ), T(hyp t ))}} ,(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_40",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_41@0",
            "content": "where the pre i is the image premise and V is the CLIP visual encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_41",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_42@0",
            "content": "5 Few-shot Learning for VQA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_42",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_43@0",
            "content": "In this section, We aim to investigate whether the CLIP models could benefit from few-shot learning, where we work on the visual question answering task to study it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_43",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_44@0",
            "content": "Setup of Few-shot VQA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_44",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_45@0",
            "content": "Here we briefly define the terminology used in our few-shot visual question answering settings:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_45",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_46@0",
            "content": "\u2022 Number of ways. Originally, it is defined as the distinct classes in a task. However, rather than defining a 3,129-way task according to the answer vocabulary, we define the number of ways as question type times answer type ( \u00a7 2.2), i.e., 65\u00d73=195 ways, to ensure the model's generalization ability where it can answer a type of questions. \u2022 Number of shots. The number of distinct examples in each way. Here a shot is an image along with the question and the answer. \u2022 Support set and query set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_46",
            "start": 0,
            "end": 498,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_47@0",
            "content": "Parameter-efficient Fine-tuning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_47",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_48@0",
            "content": "Under the few-shot setting, our goal is to make the CLIP models learn from N-way K-shot examples and improve the zero-shot VQA performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_48",
            "start": 0,
            "end": 138,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_48@1",
            "content": "Specifically, we identify only a very small set of parameters in CLIP models (about 0.3 million out of over 100 million, details in appendix B.3), including the bias term and normalization term, to be optimized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_48",
            "start": 140,
            "end": 350,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_48@2",
            "content": "For either the BatchNorm in ResNet or the LayerNorm in Transformer, the normalization could be uniformly denoted as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_48",
            "start": 352,
            "end": 467,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_49@0",
            "content": "y = x \u2212 E(x) Var(x) + \u2022 \u03b3 + \u03b2,(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_49",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_50@0",
            "content": "where x and y are the mini-batched input and output, and the \u03b3 and \u03b2 are learned parameters. And for all the linear layers and projection layers in CLIP models, they could be denoted as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_50",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_51@0",
            "content": "o = w \u2022 h + bias,(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_51",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_52@0",
            "content": "where h and o are the input and output vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_52",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_52@1",
            "content": "We define the learnable parameter set as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_52",
            "start": 48,
            "end": 88,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_53@0",
            "content": "P learn = {bias, \u03b3, \u03b2}.(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_53",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_54@0",
            "content": "We optimize the Bias and Normalization (BiNor) parameters on the few-shot examples with a standard cross-entropy loss over the dot products from each image-prompt pair (Eq.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_54",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_54@1",
            "content": "Besides, when there are a few examples available, we could also leverage an in-context demonstration manner to improve the performance of the answer filtering process in TAP-C ( \u00a7 3.1) by: Top-k",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_54",
            "start": 176,
            "end": 369,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_55@0",
            "content": "v\u2208V {log P L ([mask] = v | [D, T ])} ,(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_55",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_56@0",
            "content": "where the D denotes the demonstrations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_56",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_56@1",
            "content": "D is similar to template T but has been infilled with the answers, and it is sampled from the same type of question in the available few-shot examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_56",
            "start": 40,
            "end": 190,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_56@2",
            "content": "The resulting filtered vocabulary is noted as V demo .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_56",
            "start": 192,
            "end": 245,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_56@3",
            "content": "We report the few-shot training procedure in appendix C.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_56",
            "start": 247,
            "end": 302,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_57@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_57",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_58@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_58",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_59@0",
            "content": "Datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_59",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_59@1",
            "content": "For visual question answering and visual entailment, we carry out experiments on the VQAv2 (Goyal et al., 2017) and the SNLI-VE (Xie et al., 2019) datasets, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_59",
            "start": 10,
            "end": 179,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_59@2",
            "content": "We report the statistics of the two datasets in appendix A. For the VQA task's evaluation, we follow the Frozen model (Tsimpoukelli et al., 2021) to calculate the vqa scores on the VQAv2 validation set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_59",
            "start": 181,
            "end": 382,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_59@3",
            "content": "For visual entailment, we calculate the accuracy on both validation and test sets through the sklearn toolkit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_59",
            "start": 384,
            "end": 493,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_60@0",
            "content": "CLIP models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_60",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_60@1",
            "content": "According to the types of visual encoders, e.g. ResNet or ViT, CLIP models have different variants, resulting in a significant difference in the number of learnable bias and normalization parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_60",
            "start": 13,
            "end": 211,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_60@2",
            "content": "We report the number of learnable parameters of CLIP variants in appendix B.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_60",
            "start": 213,
            "end": 290,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_60@3",
            "content": "We select two best performing (and publicly available) variants from two kinds of visual encoders, including the CLIP Res50x16 and the CLIP ViT-B/16, to empirically study their zero-shot and few-shot vision-language understanding performances by applying our transferring methods ( \u00a7 \u00a7 3-5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_60",
            "start": 292,
            "end": 582,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_61@0",
            "content": "Results of Zero-shot VQA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_61",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_62@0",
            "content": "As previous VL models heavily rely on object detection sub-modules, it is not feasible to directly apply them under the zero-shot setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_62",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_62@1",
            "content": "Here we setup zero-shot VL baselines from two latest works:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_62",
            "start": 139,
            "end": 197,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_63@0",
            "content": "\u2022 Frozen. Frozen (Tsimpoukelli et al., 2021) prompts a seven-billion-parameter 32-layer language model with image representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_63",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_64@0",
            "content": "It is trained on aligned image-caption data and is also the first model that shows promising zero-shot and few-shot VQA performances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_64",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_65@0",
            "content": "\u2022 Question irrelevant prompt. We report the zero-shot VQA results in Table 1. The experimental results verify our hypothesis ( \u00a7 3.1) that the prompts of CLIP should be used to describe the labels rather than the tasks. As we can see, the question irrelevant prompting methods simply present the task description and answers to the CLIP models and only get barely better than random guess results. In contrast, by converting questions into templates and filtering answers with pre-trained language models, our TAP-C method enables CLIP models a strong zero-shot capability on the VQA task, even compared with the sevenbillion-parameter Frozen zero-shot model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_65",
            "start": 0,
            "end": 658,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_66@0",
            "content": "Zero-shot Cross-modality Transfer",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_66",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@0",
            "content": "We report the zero-shot cross-modality transfer results in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@1",
            "content": "We first investigate the language to vision transfer capability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 68,
            "end": 131,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@2",
            "content": "As introduced in \u00a7 4, we train a classifier on the text-only SNLI-VE dataset where the image is replaced by its caption.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 133,
            "end": 252,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@3",
            "content": "At inference, the trained classifier is evaluated by taking the image and text as inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 254,
            "end": 342,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@4",
            "content": "As shown in the first group of results, after solely trained on text-text (caption as the premise) entailment data, different CLIP variants could successfully gain a similar discriminative ability under the image-text setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 344,
            "end": 569,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@5",
            "content": "To ensure that the above results are indeed transferring from language to vision, we made a double check by masking out the images at inference time, and the results are reported at Image Masked.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 571,
            "end": 765,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@6",
            "content": "As we can see, the results are similar to a random guess of three relations, indicating the images are of importance in the cross-modality evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 767,
            "end": 916,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@7",
            "content": "Now that we have observed the language to vision transferring capability in CLIP models, we further investigate whether there is also a vision to language transfer capability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 918,
            "end": 1092,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@8",
            "content": "We conduct a similar experiment but train the classifier on the original SNLI-VE dataset, i.e., image premise and text hypothesis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 1094,
            "end": 1223,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@9",
            "content": "At inference, we evaluate the classifier with the text-only valid and test data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 1225,
            "end": 1304,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@10",
            "content": "The results are reported in Table 2, which confirms the vision to language capability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 1306,
            "end": 1391,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_67@11",
            "content": "Since text data are usually much cheaper than visual data, the first kind of transferring tends to be more promising in practice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_67",
            "start": 1393,
            "end": 1521,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_68@0",
            "content": "Results of Few-shot VQA",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_68",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_69@0",
            "content": "We report the few-shot VQA results in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_69",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_70@0",
            "content": "We take the Frozen model and the image blacked out Frozen blind as baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_70",
            "start": 0,
            "end": 76,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_70@1",
            "content": "Under different k, our methods could always learn from limited training examples and improve over the zero-shot results, which confirms that CLIP models could be VL fewshot learners.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_70",
            "start": 78,
            "end": 259,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_70@2",
            "content": "With the increase of the number of shots, significant performance gains are observed in other category, which concurs with our intuition: as we sample examples from each question type, most answers in other category are not revealed to the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_70",
            "start": 261,
            "end": 506,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_70@3",
            "content": "As a result, the model could always learn to improve.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_70",
            "start": 508,
            "end": 560,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_70@4",
            "content": "Similarly, presenting examples to the T5 could also improve the answer filtering process, leading to significant performance gains over the other category.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_70",
            "start": 562,
            "end": 716,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_70@5",
            "content": "In contrast, the score of number category improves significantly when the model just begins to see some training examples while slowing down as k continues to increase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_70",
            "start": 718,
            "end": 885,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_71@0",
            "content": "Analyses and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_71",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_72@0",
            "content": "The effects of template generation methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_72",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_73@0",
            "content": "Our TAP-C method uses an ensemble of dependency parsing template T parsing and T5 demonstration template T demo .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_73",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_73@1",
            "content": "Here we investigate whether it is necessary to use such an ensemble.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_73",
            "start": 114,
            "end": 181,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_73@2",
            "content": "We report the ablation results of two templates in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_73",
            "start": 183,
            "end": 241,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_73@3",
            "content": "The results show that the two templates have different effects over different questions, and the ensemble could make the best use of their advantages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_73",
            "start": 243,
            "end": 392,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@0",
            "content": "The effects of two steps in TAP-C. The TAP-C method generates prompts through template generation (t.gen.) and answer filtering (a.filt.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@1",
            "content": "Here we quantify how much each step contributes to the final zero/few-shot VQA performances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 139,
            "end": 230,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@2",
            "content": "We report the ablation results in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 232,
            "end": 273,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@3",
            "content": "When we remove the answer filtering step (w/o a.filt.), both the zeroshot and few-shot performances generally fall by about 20%, but the models still retain some fewshot learning capabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 275,
            "end": 466,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@4",
            "content": "We further remove the template generation step and only use question irrelevant templates: all results are nearly cut in half, indicating the importance of considering questions in both zero-shot and few-shot scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 468,
            "end": 686,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@5",
            "content": "Comparisons of fine-tuning methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 688,
            "end": 722,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@6",
            "content": "We only update the bias and normalization parameters during few-shot learning ( \u00a7 5.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 724,
            "end": 810,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@7",
            "content": "To investigate whether our BiNor fine-tuning strategy works well, we compare BiNor with two fine-tuning methods: 1) Full-FT (Full fine-tuning), which updates all parameters in the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 812,
            "end": 997,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@8",
            "content": "2) BitFit (Ben Zaken et al., 2021), which only updates the bias-terms in all model layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 999,
            "end": 1088,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@9",
            "content": "We report the comparison results in Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 1090,
            "end": 1133,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@10",
            "content": "Both BiNor and BitFit significantly outperform the full fine-tuning way: millions of parameters are very easy to overfit to a few training examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 1135,
            "end": 1282,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@11",
            "content": "When k is small, the performance differences between BiNor and BitFit are very small.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 1284,
            "end": 1368,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@12",
            "content": "When k becomes larger, BiNor begins to outperform BitFit with a noticeable margin.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 1370,
            "end": 1451,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@13",
            "content": "Our BiNor fine-tuning strategy is similar to the BitFit but differs in that it also updates the normalization parameters, which would grant the ResNet CLIP models better flexibility to adapt to new examples due to their larger number of batch normalization parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 1453,
            "end": 1720,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_74@14",
            "content": "For the specific number of different parameters in each CLIP variant, please refer to the appendix B.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_74",
            "start": 1722,
            "end": 1824,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_75@0",
            "content": "Limitations of TAP-C. The proposed TAP-C method explores CLIP models' potential to conduct zero/few-shot VQA tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_75",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_75@1",
            "content": "However, we also found several limitations that hinder further improving the few-shot performance, which could be rooted in the CLIP models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_75",
            "start": 116,
            "end": 255,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_75@2",
            "content": "First, CLIP models struggle with counting the number of fine-grained objects in an image, especially counting from a small area of the image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_75",
            "start": 257,
            "end": 397,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_75@3",
            "content": "This shortcoming can hardly be improved by any kind of language knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_75",
            "start": 399,
            "end": 472,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_75@4",
            "content": "Besides, the CLIP models perform poorly in distinguishing subtle semantic differences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_75",
            "start": 474,
            "end": 559,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_75@5",
            "content": "For example, when asked \"what is the man in the background doing?\", all the experimented CLIP models give predictions of the man \"in the foreground\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_75",
            "start": 561,
            "end": 709,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_75@6",
            "content": "Under such cases, even if the TAP-C method perfectly converts the question into a prompt, the final results would still be wrong.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_75",
            "start": 711,
            "end": 839,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_75@7",
            "content": "Nevertheless, We believe this issue could be well addressed by enhancing CLIP models with a stronger text encoder, and we will make explorations in future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_75",
            "start": 841,
            "end": 1000,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_76@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_76",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_77@0",
            "content": "Vision-language few-shot learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_77",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_77@1",
            "content": "Leveraging aligned caption data, vision-language models pre-trained by an image-text discriminative loss have recently enabled strong zero-shot generalization on image classification and cross-modality retrieval tasks (Jia et al., 2021;Radford et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_77",
            "start": 35,
            "end": 292,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_77@2",
            "content": "Different from the discriminative manner, Tsimpoukelli et al. ( 2021) prompt a large frozen language model with vision prefix in a generative way, which is the first vision-language few-shot model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_77",
            "start": 294,
            "end": 490,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_78@0",
            "content": "Language model prompting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_78",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_78@1",
            "content": "This work is also inspired by the line of research in language model prompting (Liu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_78",
            "start": 26,
            "end": 123,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_78@2",
            "content": "Initialized by the GPT series (Radford et al., 2018(Radford et al., , 2019Brown et al., 2020), prompting has become a popular manner to mining knowledge from pre-trained language models (Petroni et al., 2019) in a zero-shot or few-shot way (Shin et al., 2020;Qin and Eisner, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_78",
            "start": 125,
            "end": 405,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_78@3",
            "content": "Besides mining knowledge from the language model, PET work (Schick and Sch\u00fctze, 2021a,b) presents a semi-supervised prompting method for improving few-shot language understanding performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_78",
            "start": 407,
            "end": 597,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_79@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_79",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_80@0",
            "content": "In this work, we empirically studied how to transfer CLIP models into vision-language understanding tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_80",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_80@1",
            "content": "We first explored the CLIP models' zero-shot VQA capability by leveraging language prompts and further proposed a parameter-efficient finetuning method to boost the few-shot performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_80",
            "start": 107,
            "end": 292,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_81@0",
            "content": "We also demonstrate a zero-shot cross-modality transfer capability of CLIP models on the visual entailment task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_81",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_81@1",
            "content": "Experiments and analyses on VQAv2 and SNLI-VE confirm that the CLIP models can be good VL few-shot learners.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_81",
            "start": 113,
            "end": 220,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_82@0",
            "content": "In this section, we showcase several template generation examples to illustrate how the proposed method works.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_82",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_82@1",
            "content": "Since we have introduced how to convert a question into a masked template by demonstrating examples to the T5 ( \u00a7 3.1), here we directly present several examples in Table 12.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_82",
            "start": 111,
            "end": 284,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_82@2",
            "content": "These examples are sampled from five different question types and also cover the three answer types.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_82",
            "start": 286,
            "end": 385,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_82@3",
            "content": "As shown in Table 12, a single demo in the demonstration consists of a question and an answer with the [mask] token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_82",
            "start": 387,
            "end": 502,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_82@4",
            "content": "Notice that the [mask] token is only a placeholder rather than a real mask in the pre-trained language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_82",
            "start": 504,
            "end": 613,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_82@5",
            "content": "Different from the <extra_id_0> in T5 that represents a corrupted span, the [mask] is used to inform the T5 where the answer words should be placed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_82",
            "start": 615,
            "end": 762,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_82@6",
            "content": "After seeing several examples in the demonstration, the powerful T5 large model could capture the conversion pattern in each type of question and perfectly complete most conversions without ignoring the subtle grammars.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_82",
            "start": 764,
            "end": 982,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_82@7",
            "content": "Once the masked template is generated, we could infill the [mask] place with answer words and then carry out further processing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_82",
            "start": 984,
            "end": 1111,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_82@8",
            "content": "The processing for the yes/no type is a little different: as it is a binary task, we directly generate a positive prompt and a negative prompt, rather than masked templates, for the yes and no, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_82",
            "start": 1113,
            "end": 1319,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_83@0",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, Devi Parikh, Vqa: Visual question answering, 2015, Proceedings of the IEEE international conference on computer vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_83",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_84@0",
            "content": "UNKNOWN, None, 2021, Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_84",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_85@0",
            "content": "Samuel Bowman, Gabor Angeli, Christopher Potts, Christopher D Manning, A large annotated corpus for learning natural language inference, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_85",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_86@0",
            "content": "Benjamin Tom B Brown, Nick Mann, Melanie Ryder, Jared Subbiah, Prafulla Kaplan, Arvind Dhariwal, Pranav Neelakantan, Girish Shyam, Amanda Sastry,  Askell, Language models are few-shot learners, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_86",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_87@0",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, Uniter: Universal image-text representation learning, 2020, European conference on computer vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_87",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_88@0",
            "content": "UNKNOWN, None, 2018, Transforming question answering datasets into natural language inference datasets, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_88",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_89@0",
            "content": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, Imagenet: A large-scale hierarchical image database, 2009, 2009 IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_89",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_90@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_90",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_91@0",
            "content": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, An image is worth 16x16 words: Transformers for image recognition at scale, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_91",
            "start": 0,
            "end": 310,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_92@0",
            "content": "Tianyu Gao, Adam Fisch, Danqi Chen, Making pre-trained language models better few-shot learners, 2021, Proceedings of the ACL 2021, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_92",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_93@0",
            "content": "Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, Devi Parikh, Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_93",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_94@0",
            "content": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Deep residual learning for image recognition, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_94",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_95@0",
            "content": "Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, V Quoc, Yunhsuan Le, Zhen Sung, Tom Li,  Duerig, 2021. Scaling up visual and vision-language representation learning with noisy text supervision, , International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_95",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_96@0",
            "content": "Wonjae Kim, Bokyung Son, Ildoo Kim, Vilt: Vision-and-language transformer without convolution or region supervision, 2021, International Conference on Machine Learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_96",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_97@0",
            "content": "Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, The open images dataset v4, 2020, International Journal of Computer Vision, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_97",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_98@0",
            "content": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, Microsoft coco: Common objects in context, 2014, European conference on computer vision, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_98",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_99@0",
            "content": "UNKNOWN, None, 2021, Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_99",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_100@0",
            "content": "Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer, Multilingual denoising pre-training for neural machine translation, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_100",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_101@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, Exploiting cloze-questions for few-shot text classification and natural language inference, 2021, Proceedings of the 16th Conference of the EACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_101",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_102@0",
            "content": "Timo Schick, Hinrich Sch\u00fctze, It's not just size that matters: Small language models are also few-shot learners, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_102",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_103@0",
            "content": "Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_103",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_104@0",
            "content": "Sheng Shen, Liunian Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, Kurt Keutzer, How much can clip benefit vision-and-language tasks, 2022, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_104",
            "start": 0,
            "end": 216,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_105@0",
            "content": "Taylor Shin, Yasaman Razeghi, I Robert L Logan, Eric Wallace, Sameer Singh, AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts, 2020, Proceedings of EMNLP 2020, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_105",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_106@0",
            "content": "Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, Vl-bert: Pretraining of generic visual-linguistic representations, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_106",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_107@0",
            "content": "UNKNOWN, None, 2021, Multimodal few-shot learning with frozen language models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_107",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_108@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_108",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_109@0",
            "content": "UNKNOWN, None, 2021, Vlmo: Unified vision-language pre-training with mixture-of-modality-experts, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_109",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_110@0",
            "content": "Shijie Wu, Mark Dredze, Beto, bentz, becas: The surprising cross-lingual effectiveness of bert, 2019, Proceedings of EMNLP 2019, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_110",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_111@0",
            "content": "UNKNOWN, None, 2019, Visual entailment: A novel task for fine-grained image understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_111",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_112@0",
            "content": "Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer, , Proceedings of NAACL 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_112",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_113@0",
            "content": "Peter Young, Alice Lai, Micah Hodosh, Julia Hockenmaier, From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions, 2014, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_113",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_114@0",
            "content": "Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, Qi Tian, Deep modular co-attention networks for visual question answering, 2019, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_114",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "390-ARR_v2_115@0",
            "content": "Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, Vinvl: Revisiting visual representations in vision-language models, 2021, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "390-ARR_v2_115",
            "start": 0,
            "end": 261,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_1",
            "tgt_ix": "390-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_1",
            "tgt_ix": "390-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_2",
            "tgt_ix": "390-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_4",
            "tgt_ix": "390-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_5",
            "tgt_ix": "390-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_6",
            "tgt_ix": "390-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_7",
            "tgt_ix": "390-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_8",
            "tgt_ix": "390-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_9",
            "tgt_ix": "390-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_10",
            "tgt_ix": "390-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_11",
            "tgt_ix": "390-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_13",
            "tgt_ix": "390-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_14",
            "tgt_ix": "390-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_14",
            "tgt_ix": "390-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_14",
            "tgt_ix": "390-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_16",
            "tgt_ix": "390-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_18",
            "tgt_ix": "390-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_19",
            "tgt_ix": "390-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_20",
            "tgt_ix": "390-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_21",
            "tgt_ix": "390-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_22",
            "tgt_ix": "390-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_23",
            "tgt_ix": "390-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_24",
            "tgt_ix": "390-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_25",
            "tgt_ix": "390-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_26",
            "tgt_ix": "390-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_27",
            "tgt_ix": "390-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_28",
            "tgt_ix": "390-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_30",
            "tgt_ix": "390-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_31",
            "tgt_ix": "390-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_29",
            "tgt_ix": "390-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_29",
            "tgt_ix": "390-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_29",
            "tgt_ix": "390-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_29",
            "tgt_ix": "390-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_32",
            "tgt_ix": "390-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_34",
            "tgt_ix": "390-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_35",
            "tgt_ix": "390-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_36",
            "tgt_ix": "390-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_37",
            "tgt_ix": "390-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_38",
            "tgt_ix": "390-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_39",
            "tgt_ix": "390-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_40",
            "tgt_ix": "390-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_41",
            "tgt_ix": "390-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_42",
            "tgt_ix": "390-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_43",
            "tgt_ix": "390-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_45",
            "tgt_ix": "390-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_44",
            "tgt_ix": "390-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_44",
            "tgt_ix": "390-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_44",
            "tgt_ix": "390-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_48",
            "tgt_ix": "390-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_49",
            "tgt_ix": "390-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_50",
            "tgt_ix": "390-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_51",
            "tgt_ix": "390-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_52",
            "tgt_ix": "390-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_53",
            "tgt_ix": "390-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_54",
            "tgt_ix": "390-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_55",
            "tgt_ix": "390-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_56",
            "tgt_ix": "390-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_57",
            "tgt_ix": "390-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_57",
            "tgt_ix": "390-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_59",
            "tgt_ix": "390-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_58",
            "tgt_ix": "390-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_58",
            "tgt_ix": "390-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_58",
            "tgt_ix": "390-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_57",
            "tgt_ix": "390-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_60",
            "tgt_ix": "390-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_62",
            "tgt_ix": "390-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_64",
            "tgt_ix": "390-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_61",
            "tgt_ix": "390-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_61",
            "tgt_ix": "390-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_61",
            "tgt_ix": "390-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_61",
            "tgt_ix": "390-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_61",
            "tgt_ix": "390-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_57",
            "tgt_ix": "390-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_66",
            "tgt_ix": "390-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_66",
            "tgt_ix": "390-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_57",
            "tgt_ix": "390-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_69",
            "tgt_ix": "390-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_68",
            "tgt_ix": "390-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_68",
            "tgt_ix": "390-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_68",
            "tgt_ix": "390-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_57",
            "tgt_ix": "390-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_70",
            "tgt_ix": "390-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_72",
            "tgt_ix": "390-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_73",
            "tgt_ix": "390-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_71",
            "tgt_ix": "390-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_71",
            "tgt_ix": "390-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_71",
            "tgt_ix": "390-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_71",
            "tgt_ix": "390-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_71",
            "tgt_ix": "390-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_75",
            "tgt_ix": "390-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_77",
            "tgt_ix": "390-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_76",
            "tgt_ix": "390-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_76",
            "tgt_ix": "390-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_76",
            "tgt_ix": "390-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_78",
            "tgt_ix": "390-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_80",
            "tgt_ix": "390-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_79",
            "tgt_ix": "390-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_79",
            "tgt_ix": "390-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_79",
            "tgt_ix": "390-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_79",
            "tgt_ix": "390-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_81",
            "tgt_ix": "390-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "390-ARR_v2_0",
            "tgt_ix": "390-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_1",
            "tgt_ix": "390-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_2",
            "tgt_ix": "390-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_2",
            "tgt_ix": "390-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_2",
            "tgt_ix": "390-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_2",
            "tgt_ix": "390-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_2",
            "tgt_ix": "390-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_2",
            "tgt_ix": "390-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_2",
            "tgt_ix": "390-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_3",
            "tgt_ix": "390-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_4",
            "tgt_ix": "390-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_4",
            "tgt_ix": "390-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_4",
            "tgt_ix": "390-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_5",
            "tgt_ix": "390-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_5",
            "tgt_ix": "390-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_5",
            "tgt_ix": "390-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_5",
            "tgt_ix": "390-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_6",
            "tgt_ix": "390-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_6",
            "tgt_ix": "390-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_6",
            "tgt_ix": "390-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_6",
            "tgt_ix": "390-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_6",
            "tgt_ix": "390-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_6",
            "tgt_ix": "390-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_7",
            "tgt_ix": "390-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_7",
            "tgt_ix": "390-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_7",
            "tgt_ix": "390-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_8",
            "tgt_ix": "390-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_8",
            "tgt_ix": "390-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_9",
            "tgt_ix": "390-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_9",
            "tgt_ix": "390-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_9",
            "tgt_ix": "390-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_9",
            "tgt_ix": "390-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_10",
            "tgt_ix": "390-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_10",
            "tgt_ix": "390-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_10",
            "tgt_ix": "390-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_11",
            "tgt_ix": "390-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_12",
            "tgt_ix": "390-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_13",
            "tgt_ix": "390-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_13",
            "tgt_ix": "390-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_13",
            "tgt_ix": "390-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_13",
            "tgt_ix": "390-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_14",
            "tgt_ix": "390-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_15",
            "tgt_ix": "390-ARR_v2_15@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_16",
            "tgt_ix": "390-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_17",
            "tgt_ix": "390-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_18",
            "tgt_ix": "390-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_18",
            "tgt_ix": "390-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_19",
            "tgt_ix": "390-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_20",
            "tgt_ix": "390-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_21",
            "tgt_ix": "390-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_21",
            "tgt_ix": "390-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_21",
            "tgt_ix": "390-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_22",
            "tgt_ix": "390-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_22",
            "tgt_ix": "390-ARR_v2_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_23",
            "tgt_ix": "390-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_23",
            "tgt_ix": "390-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_23",
            "tgt_ix": "390-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_23",
            "tgt_ix": "390-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_23",
            "tgt_ix": "390-ARR_v2_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_23",
            "tgt_ix": "390-ARR_v2_23@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_24",
            "tgt_ix": "390-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_25",
            "tgt_ix": "390-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_25",
            "tgt_ix": "390-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_25",
            "tgt_ix": "390-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_25",
            "tgt_ix": "390-ARR_v2_25@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_25",
            "tgt_ix": "390-ARR_v2_25@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_25",
            "tgt_ix": "390-ARR_v2_25@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_25",
            "tgt_ix": "390-ARR_v2_25@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_26",
            "tgt_ix": "390-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_26",
            "tgt_ix": "390-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_26",
            "tgt_ix": "390-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_27",
            "tgt_ix": "390-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_28",
            "tgt_ix": "390-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_28",
            "tgt_ix": "390-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_28",
            "tgt_ix": "390-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_29",
            "tgt_ix": "390-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_30",
            "tgt_ix": "390-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_30",
            "tgt_ix": "390-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_30",
            "tgt_ix": "390-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_31",
            "tgt_ix": "390-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_32",
            "tgt_ix": "390-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_32",
            "tgt_ix": "390-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_33",
            "tgt_ix": "390-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_34",
            "tgt_ix": "390-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_34",
            "tgt_ix": "390-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_34",
            "tgt_ix": "390-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_34",
            "tgt_ix": "390-ARR_v2_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_34",
            "tgt_ix": "390-ARR_v2_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_34",
            "tgt_ix": "390-ARR_v2_34@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_35",
            "tgt_ix": "390-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_36",
            "tgt_ix": "390-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_36",
            "tgt_ix": "390-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_37",
            "tgt_ix": "390-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_38",
            "tgt_ix": "390-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_39",
            "tgt_ix": "390-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_39",
            "tgt_ix": "390-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_39",
            "tgt_ix": "390-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_39",
            "tgt_ix": "390-ARR_v2_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_40",
            "tgt_ix": "390-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_41",
            "tgt_ix": "390-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_42",
            "tgt_ix": "390-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_43",
            "tgt_ix": "390-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_44",
            "tgt_ix": "390-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_45",
            "tgt_ix": "390-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_46",
            "tgt_ix": "390-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_47",
            "tgt_ix": "390-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_48",
            "tgt_ix": "390-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_48",
            "tgt_ix": "390-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_48",
            "tgt_ix": "390-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_49",
            "tgt_ix": "390-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_50",
            "tgt_ix": "390-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_51",
            "tgt_ix": "390-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_52",
            "tgt_ix": "390-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_52",
            "tgt_ix": "390-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_53",
            "tgt_ix": "390-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_54",
            "tgt_ix": "390-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_54",
            "tgt_ix": "390-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_55",
            "tgt_ix": "390-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_56",
            "tgt_ix": "390-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_56",
            "tgt_ix": "390-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_56",
            "tgt_ix": "390-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_56",
            "tgt_ix": "390-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_57",
            "tgt_ix": "390-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_58",
            "tgt_ix": "390-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_59",
            "tgt_ix": "390-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_59",
            "tgt_ix": "390-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_59",
            "tgt_ix": "390-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_59",
            "tgt_ix": "390-ARR_v2_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_60",
            "tgt_ix": "390-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_60",
            "tgt_ix": "390-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_60",
            "tgt_ix": "390-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_60",
            "tgt_ix": "390-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_61",
            "tgt_ix": "390-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_62",
            "tgt_ix": "390-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_62",
            "tgt_ix": "390-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_63",
            "tgt_ix": "390-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_64",
            "tgt_ix": "390-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_65",
            "tgt_ix": "390-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_66",
            "tgt_ix": "390-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_67",
            "tgt_ix": "390-ARR_v2_67@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_68",
            "tgt_ix": "390-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_69",
            "tgt_ix": "390-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_70",
            "tgt_ix": "390-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_70",
            "tgt_ix": "390-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_70",
            "tgt_ix": "390-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_70",
            "tgt_ix": "390-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_70",
            "tgt_ix": "390-ARR_v2_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_70",
            "tgt_ix": "390-ARR_v2_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_71",
            "tgt_ix": "390-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_72",
            "tgt_ix": "390-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_73",
            "tgt_ix": "390-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_73",
            "tgt_ix": "390-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_73",
            "tgt_ix": "390-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_73",
            "tgt_ix": "390-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_74",
            "tgt_ix": "390-ARR_v2_74@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_75",
            "tgt_ix": "390-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_75",
            "tgt_ix": "390-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_75",
            "tgt_ix": "390-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_75",
            "tgt_ix": "390-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_75",
            "tgt_ix": "390-ARR_v2_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_75",
            "tgt_ix": "390-ARR_v2_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_75",
            "tgt_ix": "390-ARR_v2_75@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_75",
            "tgt_ix": "390-ARR_v2_75@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_76",
            "tgt_ix": "390-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_77",
            "tgt_ix": "390-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_77",
            "tgt_ix": "390-ARR_v2_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_77",
            "tgt_ix": "390-ARR_v2_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_78",
            "tgt_ix": "390-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_78",
            "tgt_ix": "390-ARR_v2_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_78",
            "tgt_ix": "390-ARR_v2_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_78",
            "tgt_ix": "390-ARR_v2_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_79",
            "tgt_ix": "390-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_80",
            "tgt_ix": "390-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_80",
            "tgt_ix": "390-ARR_v2_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_81",
            "tgt_ix": "390-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_81",
            "tgt_ix": "390-ARR_v2_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_82",
            "tgt_ix": "390-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_82",
            "tgt_ix": "390-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_82",
            "tgt_ix": "390-ARR_v2_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_82",
            "tgt_ix": "390-ARR_v2_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_82",
            "tgt_ix": "390-ARR_v2_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_82",
            "tgt_ix": "390-ARR_v2_82@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_82",
            "tgt_ix": "390-ARR_v2_82@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_82",
            "tgt_ix": "390-ARR_v2_82@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_82",
            "tgt_ix": "390-ARR_v2_82@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_83",
            "tgt_ix": "390-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_84",
            "tgt_ix": "390-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_85",
            "tgt_ix": "390-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_86",
            "tgt_ix": "390-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_87",
            "tgt_ix": "390-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_88",
            "tgt_ix": "390-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_89",
            "tgt_ix": "390-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_90",
            "tgt_ix": "390-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_91",
            "tgt_ix": "390-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_92",
            "tgt_ix": "390-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_93",
            "tgt_ix": "390-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_94",
            "tgt_ix": "390-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_95",
            "tgt_ix": "390-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_96",
            "tgt_ix": "390-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_97",
            "tgt_ix": "390-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_98",
            "tgt_ix": "390-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_99",
            "tgt_ix": "390-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_100",
            "tgt_ix": "390-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_101",
            "tgt_ix": "390-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_102",
            "tgt_ix": "390-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_103",
            "tgt_ix": "390-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_104",
            "tgt_ix": "390-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_105",
            "tgt_ix": "390-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_106",
            "tgt_ix": "390-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_107",
            "tgt_ix": "390-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_108",
            "tgt_ix": "390-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_109",
            "tgt_ix": "390-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_110",
            "tgt_ix": "390-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_111",
            "tgt_ix": "390-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_112",
            "tgt_ix": "390-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_113",
            "tgt_ix": "390-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_114",
            "tgt_ix": "390-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "390-ARR_v2_115",
            "tgt_ix": "390-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 805,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "390-ARR",
        "version": 2
    }
}