{
    "nodes": [
        {
            "ix": "343-ARR_v2_0",
            "content": "Interactive Symbol Grounding with Complex Referential Expressions",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_2",
            "content": "We present a procedure for learning to ground symbols from a sequence of stimuli consisting of an arbitrarily complex noun phrase (e.g. \"all but one green square above both red circles.\") and its designation in the visual scene. Our distinctive approach combines: a) lazy fewshot learning to relate open-class words like green and above to their visual percepts; and b) symbolic reasoning with closed-class word categories like quantifiers and negation. We use this combination to estimate new training examples for grounding symbols that occur within a noun phrase but aren't designated by that noun phase (e.g, red in the above example), thereby potentially gaining data efficiency. We evaluate the approach in a visual reference resolution task, in which the learner starts out unaware of concepts that are part of the domain model and how they relate to visual percepts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "343-ARR_v2_4",
            "content": "The subfield of robotics known as Interactive Task Learning (ITL, see Laird et al. (2017) for a survey) addresses scenarios where a robot must learn to adapt its behaviour to novel and unforeseen objects, relations, and attributes that are introduced into the environment after deployment. The ITL agent learns its novel task via evidence from its own actions and reactive guidance from a teacher. This paper focuses on symbol grounding (Harnad, 1999) in the context of ITL (Matuszek, 2018): the learner must use the teacher's embodied natural language utterance and its context to learn a mapping from natural language expressions to their denotations, given the visual percepts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_5",
            "content": "There are two challenges in learning symbol grounding models (grounders) in ITL. Firstly, in contrast to many grounders (Ye et al., 2019;Datta et al., 2019), ITL requires incremental learning: knowledge is acquired piecemeal via an extended interaction, and it must influence planning as and when it occurs. Secondly, previous work limits the teacher's language to bare nouns (e.g., square) or very short phrases (e.g., blue square, square above circle) (Hristov et al., 2018(Hristov et al., , 2019. But there's evidence from Dale and Reiter (1995) that speakers use complex referring expressions even when simpler ones would successfully refer. Such language creates the possibility that novel symbolsneologisms-are introduced in a context where their denotation is not designated by the teacher. In this work we study the natural language of complex referential expressions (REs) like \"a blue square behind both red circles\" which teachers can use when designating an object.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_6",
            "content": "Our aim is for the learner to extract knowledge that improves their domain representation and state estimates-a necessary condition for successful planning. Contemporary grounders miss learning opportunities that complex REs afford: the RE example above not only entails that its referents are blue and square, but also that there exists two objects that are both red and circle and that they are above the designated objects, and everything else in the domain is either not red or not a circle (thanks to the meaning of both). Thus, a complex RE and its designation can be used to gather multiple (noisy) training exemplars (both positive and negative) for several symbols at once, even if they have not been designated.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_7",
            "content": "In this work, we develop a method to integrate knowledge from interactively gathered evidence in the form of complex RE-designation pairs to aid data acquisition for a (neural) few-shot grounder. We explore the effect of such a method on data efficiency and the overall grounder's performance. A major novel component to our procedure is that we exploit the formal semantics of closed class word categories (e.g., quantifiers and negation) to boost the data efficiency of few-shot neural grounding models. Our experiments show these symbolic inductive biases are successful.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_8",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "343-ARR_v2_9",
            "content": "Symbol Grounding. Contemporary grounders extensively utilize batch learning (e.g. Shridhar and Hsu (2018)). Yet, ITL requires incremental learning because without it the teacher guidance cannot influence the learner's inferences about plans as and when the advice is given. Further, many grounders assume that the learner starts out with a complete and accurate conceptualisation of the domain using pre-defined visual features and a known vocabulary (Kennington et al., 2015;Kennington and Schlangen, 2017;Wang et al., 2017). In ITL, both of these assumptions are unrealistic; therefore in this paper we explore models for which these assumptions don't apply. Finally, in contrast to all prior grounders, we support incremental learning when the training exemplars feature REs that are linguistically complex: e.g., \"two red circles that aren't to the right of both green squares\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_10",
            "content": "Representation Learning. Models for jointly learning a representation for vision and language utilize either explicit alignment via bounding boxes or instance segmentation (Lu et al., 2019;Chen et al., 2020;Tan and Bansal, 2019;Kamath et al., 2021;Yu et al., 2021), or a large-volume of weakly labeled data in the form of image-caption pairs (Radford et al., 2021). These models rely on offline learning with large datasets. This work, on the other hand, explores how to incrementally extract knowledge from few-shot learning, using sequentially observed evidence that includes neologisms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_11",
            "content": "Visual Questions Answering (VQA). This is a task of answering free-form questions about an image (Antol et al., 2015). VQA has reached impressive performance in recent years (Fukui et al., 2016;, yet VQA models struggle with outof distribution generalization for new types of questions, requiring multi-step reasoning, with analysis revealing that they often rely on shortcuts (Jiang and Bansal, 2019;Subramanian et al., , 2020. Grounded VQA models like (Yi et al., 2018) and Bogin et al. (2021) tackle these shortcomings by grounding parts of the question and then learning to compose those parts via the question's syntax to compute the answer. They thus estimate denotations of linguistic parts that are not denoted by the answer to the question. These 'compositional' models help to achieve out-of-distribution generalization for novel questions. But they lack ITL's requirement for incremental learning: model training relies on batch learning. Furthermore, while their performance is impressive, error analysis shows that it makes mistakes when language includes logical concepts like quantifiers and negation (e.g. Bogin et al. (2021) Figure 9 shows that the determiner most incorrectly denotes an arbitrary subset of entities). Our view is that there is little benefit in trying to learn to ground logic concepts as they are domain independent and can be interpreted using formal semantics. In our experiments, we are testing the extent to which knowing and reasoning with the logical meanings of these symbols helps incremental grounding, and in particular estimating denotations of symbols within an RE that are not designated.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_12",
            "content": "Grounded Language Acquisition. This task is often realized as grounded grammar induction from image-caption pairs (Shi et al., 2019;Zhao and Titov, 2020), or as learning (neural) semantic parsers from a reward signal (Williams, 1992) in VQA Yi et al., 2018) or in planning (Azaria et al., 2016;Wang et al., 2016Wang et al., , 2017Karamcheti et al., 2020). There, the main objective is to learn to map natural language to logical forms, which in turn get associated with visual percepts during the learning process. This paper does not aim to learn a semantic parser. Instead, we obtain logical forms from an existing broad-coverage grammar which is hard to engineer, but is robust on lexical variation (Curran et al., 2007). Our focus instead is on exploiting the logical consequences of those logical forms during symbol grounding-i.e., our focus is to utilise the interpretation of logical forms, and in particular the truth functional meanings of close-class words like quantifiers and negation, to inform the learning of mappings from (open-class) symbols like red to their denotations, given the visual percepts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_13",
            "content": "Visual Reference Resolution. In previous experiments, it is often assumed that there is a unique referent in the visual scene for the given RE in the test phase (Kazemzadeh et al., 2014;Whitney et al., 2016). We aim to cope with situations where the RE has multiple referents: identifying all the referents that satisfy an RE enables efficient planning, because it affords free choice when executing certain commands-e.g., \"move a square above both red circles\" when there is more than one square affords choosing a control policy so that resources are optimized.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_14",
            "content": "Background",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "343-ARR_v2_15",
            "content": "Formal Semantics of Natural Language",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "343-ARR_v2_16",
            "content": "Predicate logic with generalized quantifiers L ( Barwise and Cooper, 1981;van Benthem, 1984) is a canonical meaning representation for natural languages. L-sentences \u03c6 are constructed recursively from predicates P , terms T (i.e., variables V and constants C), logical connectives O = {\u00ac, \u2227, \u2228, \u2192 } and quantifiers Q (see Table 1 column 1):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_17",
            "content": "\u03c6 ::= p(t 1 , . . . , t n ) \u2261 p(t n ) |(\u00ac\u03c6)|(\u03c6 1 \u2227 \u03c6 2 )|(\u03c6 1 \u2228 \u03c6 2 )|(\u03c6 1 \u2192 \u03c6 2 ) |(Qx(\u03c6 1 , \u03c6 2 ))",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_18",
            "content": "where p is an n-place predicate,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_19",
            "content": "t i \u2208 T are terms, Q \u2208 Q is a quantifier, and x \u2208 V is a variable (in Qx(\u03c6 1 , \u03c6 2 )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_20",
            "content": ", \u03c6 1 is the restrictor and \u03c6 2 the body).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_21",
            "content": "We also introduce \u03bb-expressions of the form \u03bbx.\u03c6, where x \u2208 V is free or absent in \u03c6.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_22",
            "content": "Model-theoretic Interpretation",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "343-ARR_v2_23",
            "content": "L-sentences are interpreted using a domain model M = (E, I) consisting of a set of entities E (universe of discourse), and an extension function I that maps non-logical symbols P \u222a C to denotations (tuples of entities). For convenience, we assume I : C \u2192 E is one-to-one. Variables are interpreted via an assignment function g : V \u2192 E.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_24",
            "content": "The interpretation function \u2022 M,g specifies the semantic value of well-formed expressions of L:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_25",
            "content": "a M,g = I(a) if a \u2208 P \u222a C g(a) if a \u2208 V p(t n ) M,g = 1 iff ( t 1 M,g , . . . , t n M,g ) \u2208 p M,g \u00ac\u03c6 M,g = 1 iff \u03c6 M,g = 0 \u03c6 \u2227 \u03c8 M,g = 1 iff \u03c6 M,g = 1 and \u03c8 M,g = 1 \u03c6 \u2228 \u03c8 M,g = 1 iff \u03c6 M,g = 1 or \u03c8 M,g = 1 \u03c6 \u2192 \u03c8 M,g = 1 iff \u03c6 M,g = 0 or \u03c8 M,g = 1 \u03bbx.\u03c6 M,g = {e \u2208 E : \u03c6 M,g[x/e] = 1} Qx(\u03c6 1 , \u03c6 2 ) M,g = Q( \u03bb.x\u03c6 1 M,g , \u03bb.x\u03c6 2 M,g )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_26",
            "content": "where g[x/e] is just like g, except g[x/e](x) = e and Q is a specific relation between the restrictor \u03bbx.\u03c6 1 M,g and body \u03bbx.\u03c6 2 M,g , as defined in Table 1 column 3. \u2022 M,g is directly related to satisfiability for L-sentences:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_27",
            "content": "M, g |= \u03c6 iff \u03c6 M,g = 1 M |= \u03c6 iff \u03c6 M = 1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_28",
            "content": "where \u03c6 M = 1 iff \u03c6 M,g = 1 for all g. Further, if x is the only free variable in \u03c6, then \u03bbx.\u03c6 M,g = \u03bbx.\u03c6 M,g for all g, g ; so without a loss of generality, this is expressed as \u03bbx.\u03c6 M . 1 If all variables in Qx(\u03c6, \u03c8) are bound by quantifiers, then this L-sentence is true iff Q is true for all g. Some quantifiers, like \"both\", 2 are presupposition triggers: \"exactly two blocks are blue\" is different from \"both blocks are blue\" in that the latter is true only if there are exactly two individuals that are blocks. We've adopted a Russellian interpretation (Russell, 1917) of these in Table 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_29",
            "content": "Logical Forms of Referential Expressions",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "343-ARR_v2_30",
            "content": "We now define the logical forms of REs and their interpretations with respect to a domain model M. Noun phrases like \"a block\" are represented as _a_qx.block(x) . More generally, let Qx.\u03c6 be the logical form of an RE, where Q \u2208 Q and \u03c6 is an L-sentence with x \u2208 V being the only free variable in \u03c6. The referents Qx.\u03c6 M of this logical form with respect to M are computed as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_31",
            "content": "Qx.\u03c6 M = Q \u03c0(M,\u03c6,x)(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_32",
            "content": "where \u03c0(M, \u03c6, x) is an M-projection, giving a new domain model M with entities E = E \u2229 \u03bb.x\u03c6 M and Q M is a quantifier referent-a quantifier-specific subset of the power set of E. Table 1 column 4 gives the list of quantifier referents.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_33",
            "content": "To illustrate, consider the domain model where:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_34",
            "content": "E = {a, b, c, d, f } I(cat) = {a, b} I(dog) = {c, d, f } I(bit) = {(c, a), (c, b), (d, b), (f, a), (f, b)}",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_35",
            "content": "The RE \"a dog that bit both cats\" has logical form _a_q x._both_q y(cat(y), dog(x) \u2227 bit(x, y)) . By Equation 1, its referent is:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_36",
            "content": "_a_q \u03c0(M,_both_q y(cat(y),dog(x)\u2227bit(x,y)),x)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_37",
            "content": "The semantic value of the \u03bb-expression formed from this RE is a set of entities e \u2208 E for which the following quantifier condition is true: both_q(R, B) where R = \u03bby.cat(y) M,g[x/e] and B = \u03bby.dog(x) \u2227 bit(x, y)) In interaction, the learner observes an RE, which is parsed to logical form ( \u00a75.3.2) and interpreted with respect of the extracted feature vectors for denotations ( \u00a75.3.1) to perform reference resolution ( \u00a73.3) with respect to the estimated domain model M. In case of teacher feedback, RE and its designation is observed. This is used to build the L-sentence that is added to \u2206 to update beliefs about the underlying concept vectors ( \u00a74.3.2), which in turn are used to update the support set ( \u00a73.3), used as parameters for the grounder ( \u00a74.1). Elements in blue are pre-defined elements of IGRE while elements in red are learned through interaction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_38",
            "content": ". quantifiers Q surface form condition Q(R, B) referent Q M _exactly_n_q exactly n |R \u2229 B| = n {A \u2286 E : |A| = n} _at_most_n_q at most n |R \u2229 B| \u2264 n {A \u2286 E : |A| \u2264 n} _at_least_n_q at least n |R \u2229 B| \u2265 n {A \u2286 E : |A| \u2265 n} _a_q a/an |R \u2229 B| = n {A \u2286 E : |A| \u2264 1} _every_q all/every |R \u2229 B| = |R| {A \u2286 E : |A| = |E|} _the_n_q the n |R \u2229 B| = n) \u2227 |R| = n {A \u2286 E : |A| = |E| \u2227 (|E| = n)} _both_q both |R \u2229 B| = 2) \u2227 |R| = 2 {A \u2286 E : |A| = |E| \u2227 |E| = 2} _all_but_n_q all but n |R \u2229 B| = |R| \u2212 n {A \u2286 E : |A| = |E| \u2212 n \u2227 |E| \u2265 n} _n_of_the_m_q n of the m |R \u2229 B| = n \u2227 |R| = m {A \u2286 E : |A| = n \u2227 |E| = m}",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_39",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "343-ARR_v2_40",
            "content": "Below we present the procedure of interactive grounding with referential expressions (IGRE). The overall framework is given in Figure 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_41",
            "content": "Grounder",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "343-ARR_v2_42",
            "content": "Matching networks (Vinyals et al., 2016) are an extension of the k nearest-neighbour algorithm (Fix and Hodges, 1989) and has been used as a fast fewshot grounder in the ITL setting (Cano Sant\u00edn et al., 2020). For predicates P n \u2286 P of the same arity n, a grounder \u0398 n is parameterized by a support set",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_43",
            "content": "S n = {(x n i , y n i )} K n i=1 , consisting of K n pairs of feature vectors x n i \u2208 R dn for denotations e n \u2208 E n and concept vectors y n i \u2208 [0, 1] |P n | .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_44",
            "content": "In y n , the dimension z corresponds the predicate p z \u2208 P n and its value is the probability that p z (e n ) M,g = 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_45",
            "content": "Concept vectors have a one-to-one correspondence with the domain model M.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_46",
            "content": "Given a feature vector x n for a denotation e n \u2208 E n , \u0398 n predicts the concept vector \u0177n , using the following inference rule:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_47",
            "content": "\u0398 n (x n , S n ) = k i=1 \u03b1 n (x n i , x n ; S n )y n i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_48",
            "content": "where \u03b1 n is an attention kernel:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_49",
            "content": "\u03b1 n (x n i , x n ; S n ) = exp (f n (x n i ) \u2022 h n (x n )) k j=1 exp (f n (x n j ) \u2022 h n (x n )) f n (x n ) = ReLU(w n \u2022 x n + b n ) ||ReLU(w n \u2022 x n + b n )|| 2 h n (x n ) = ReLU(v n \u2022 x n + c n ) ||ReLU(v n \u2022 x n + c n )|| 2 ReLU(a) = max (0, a)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_50",
            "content": "with learnable parameters \u03b8 n = {w n , v n , b n , c n }, and S n is k = 3 nearest exemplars to x n from S n :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_51",
            "content": "S n = {(x n i , y n i ) \u2208 S n : x n i \u2208 V(k, x n , S n )} where V(k, x n , S n ) is a set of k nearest feature vectors.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_52",
            "content": "Batch Learning",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "343-ARR_v2_53",
            "content": "Given S n , one can estimate \u0398 n either via batch learning performed offline, or-when S n is smallin real time, as outlined by Cano Sant\u00edn et al. (2020). In our scenario, we learn in real time by minimizing binary cross-entropy between the ground-truth y n and predicted \u0177n concept vectors:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_54",
            "content": "L(y n , \u0177n ) = \u2212 |P n | z=1 l(y n z , \u0177n z ) l(y n i , \u0177n i ) = y n i log(\u0177 n i ) + (1 \u2212 y n i ) log(1 \u2212 \u0177n i )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_55",
            "content": "Incremental Learning",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "343-ARR_v2_56",
            "content": "S n gets augmented whenever the teacher provides an RE-designation pair. This speech act provides two types of information: certain information C n in the form of denotation-symbol-semantic value triples (e n , p z , y n z ), corresponding to symbols and entities designated by the RE; and noisy information N n , corresponding to denotation-symbolsemantic value estimate triples (e n , p z , \u1ef9n z ), which are acquired from the symbols that are part of the RE and its referent inferred via (uncertain) reasoning. E.g., the RE \"a circle below a square.\", entails that its designation e \u2208 E is a circle and so (e, circle, 1) is added to C n . But it also entails there exists an entity which is a square that is not designated by the RE, but rather this entity is in the below relation with the designated entity. If the grounder is sufficiently confident about the referent for square, then the corresponding triple is added to N n .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_57",
            "content": "Acquiring Observations and Symbols",
            "ntype": "title",
            "meta": {
                "section": "4.3.1"
            }
        },
        {
            "ix": "343-ARR_v2_58",
            "content": "When the learner first observes its visual sceneand so the teacher has not expressed any concepts, and so the learner is currently unaware of all concepts-the noisy support set N n is populated with (e n , p z , 0.5) (0.5 is a default semantic value) for all e n in the scene and for all known n-place predicates. Whenever the teacher's REdesignation pair features a neologism p * , then this expansion to the learner's vocabulary prompts adding (e n , p * , 0.5) to N n for all e n . During interaction, each RE-designation pair uttered by the teacher adds elements to C n (for designated symbols) and triggers updates to the N n elements for all entities in the current visual scene, as we'll now describe.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_59",
            "content": "Integrating the Teacher's Feedback",
            "ntype": "title",
            "meta": {
                "section": "4.3.2"
            }
        },
        {
            "ix": "343-ARR_v2_60",
            "content": "N n elements are interactively updated using an incrementally built domain-level theory \u2206, which is the conjunction of L-sentences that are built from the logical forms of the REs that the teacher has uttered so far and their designations. To compute the beliefs about semantic values, given \u2206, we model the semantic value of L-sentences of the form p(t n ), in which t n are all constants (ground atom), as a random variable with Bernoulli's distribution B. Thus a distribution over the possible domain models can be estimated using (propositional) model counting MC (Valiant, 1979), which maps each L-sentence to the number of domain models satisfying it. In this way, the semantic value of any proposition can be estimated as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_61",
            "content": "\u1ef9n z = MC(pz(e n )\u2227\u2206) MC(\u2206)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_62",
            "content": "if MC(\u2206) = 0 0.5 otherwise MC can be computed exactly or approximately (Samer and Szeider, 2010). In our experiments we use the ADDMC (Dudek et al., 2020) weighted model counter, with weights set to 0.5.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_63",
            "content": "Building the Support Set",
            "ntype": "title",
            "meta": {
                "section": "4.3.3"
            }
        },
        {
            "ix": "343-ARR_v2_64",
            "content": "Concept vectors for S n are built using information in C n and N n : namely each denotation e n gets associated with its feature vector x n , and the zdimension of y corresponding to predicate p z \u2208 P n is computed as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_65",
            "content": "y n z = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 y n z if (e n , p z , y n z ) \u2208 C n \u1ef9n z if (e n , p z , \u1ef9n z ) \u2208 N n \u2227 H[B(\u1ef9 n z )] \u2264 \u03c4 n 0.5 otherwise",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_66",
            "content": "where H[P] is the entropy of the probability distribution P, and \u03c4 n is the confidence threshold for adding noisy exemplars: in our case, it's set to 0.6 for predicates of all arities.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_67",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "343-ARR_v2_68",
            "content": "Task: Visual Reference Resolution",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "343-ARR_v2_69",
            "content": "To evaluate IGRE, we use a task of visual reference resolution 3 : given a visual scene (an image) with localized entities (bounding boxes) and an RE, the grounder must estimate all its referents, as defined in \u00a73.3. The model learns its task by observing an image accompanied by a sequence of REs, with each RE paired with its designation in the image.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_70",
            "content": "We measure the performance of IGRE on the task after each observed RE and its designation. Performance is measured using the precision P, recall R, and F1 score F1 on the test set between: 1) estimated vs. ground-truth domain models, formed from the concept vectors (intrinsic evaluation) and 2) estimated vs. ground-truth referents for the RE (extrinsic evaluation). These metrics are calculated only for those symbols/concepts that the teacher has mentioned so far (since the system is unaware that the remaining concepts exist). To obtain reliable results, we repeat the experiment 10 times: i.e., 10 different visual scenes, with a sequence of 5 different teacher utterances in each scene. We record in \u00a76 the average precision, recall and f-scores over those 10 trials.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_71",
            "content": "Perhaps unusually, this training and testing regime uses very small data sets: that's because in ITL it is the initial portions of the learning curve that matters. The learner must achieve decent performance on its task via only a few teacher utterances: human teachers won't tolerate repeating the same REs many times and so the learner lacks the luxury of learning (and testing) symbol grounding on large data sets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_72",
            "content": "Data: ShapeWorld",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "343-ARR_v2_73",
            "content": "To generate training and test sets, we construct ShapeWorld domain models (Kuhnle and Copestake, 2017), each consisting of 3-12 entities, synthesized visual scenes X (64x64 pixels), and 5 REs. Each domain model is describable using 7 shape symbols S1 (square, circle, triangle, pentagon, cross, ellipse, semicircle), 6 colour symbols C1 (red, green, blue, yellow, magenta, cyan) and 4 spatial relationships symbols R2 (left, right, above, below). 4 In scene synthesis, the image is created from the domain model, with variation on the hue of the colour category, variation on the size, position, rotation, and distortion of the shapes, and variation on the spatial positions of the entities related by each spatial term. Note that the colour categories are not mutually exclusive-e.g., there are RGB values that count as both red and magenta.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_74",
            "content": "To generate REs, we sample Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) graph templates, processed using ACE (generation mode) 5 and the English Resource Grammar (ERG) (Flickinger, 2000). Generated REs are evaluated with respect to the domain model to guarantee an existing referent. In total we generated 30 such domain models for training and 10 for testing. The data statistics for the training set is given in Table 2 for the general categories of symbols, where certain (C n ) means that the designation is denoted by the symbol in the RE, and noisy (N n ) means that the symbol is a part of the RE but is not designated by it. Note that the first argument to the spatial relations R2 is always denoted by the designation while its second argument is not. Note also there is high variance in the frequencies among the individual symbols. For instance, blue occurs 27 and 28 times in certain vs. noisy positions respectively, while triangle occurs 7 and 12 times respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_75",
            "content": "Category C n candidates N n candidates C1 18.67 \u00b1 5.39 19.83 \u00b1 5.04 S1 14.67 \u00b1 3.98 16.50 \u00b1 5.32 R2 0 37.75 \u00b1 6.75",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_76",
            "content": "Table 2: Average symbol counts per word for colours (C1), shapes (S1), and spatial relationships (R2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_77",
            "content": "Implementation Details",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "343-ARR_v2_78",
            "content": "Feature Extraction",
            "ntype": "title",
            "meta": {
                "section": "5.3.1"
            }
        },
        {
            "ix": "343-ARR_v2_79",
            "content": "To extract visual features for individuals in the scene, we utilize bounding boxes b = [x left , x right , y top , y bottom ] for each entity e \u2208 E in the visual scene by localizing them (cropping) and extracting the visual features using a pre-trained visual feature encoder (in our case, DenseNet161 (Huang et al., 2017)). Additionally, for the feature vector, we add each entity's bounding box coordinates for spatial information, lost in the localization process:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_80",
            "content": "x n = Concat({[DenseNet161(X[b i ]], b i )} n i=1 )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_81",
            "content": "Grammar-based Semantic Parsing",
            "ntype": "title",
            "meta": {
                "section": "5.3.2"
            }
        },
        {
            "ix": "343-ARR_v2_82",
            "content": "To parse REs to their logical forms, we use the English Resource Grammar (ERG) and ACE (parsing mode) to produce a representation in minimal recursion semantics (MRS) (Copestake et al., 1997), which we then simplify via hand-written rules (e.g., removing event arguments from predicate symbols corresponding to adjectives and prepositions). Underspecification of the MRS was resolved using UTOOL (Koller and Thater, 2005) and the final logical form was selected based on the linear order of scope-bearing elements (quantifiers and negation): e.g. for the RE \"every circle above a square\", _every_q outscopes _a_q.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_83",
            "content": "Axioms for R2",
            "ntype": "title",
            "meta": {
                "section": "5.3.3"
            }
        },
        {
            "ix": "343-ARR_v2_84",
            "content": "For |E| entities, there are |E| 2 denotations to consider for each 2-place predicate-a larger search space compared to |E| denotations for 1-place predicates. Moreover, these predicates can only be acquired from the noisy component N n because the referent of the second argument to the relation is always latent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_85",
            "content": "To aid the learning process for R2, whenever a new symbol R \u2208 R2 is observed, domainlevel axioms are added to \u2206 for it, making it irrreflexive: \u2200x.\u00acR(x, x) (an entity cannot be in a spatial relationship to itself) and asymmetric: \u2200x, y.R(x, y) \u2192 \u00acR(y, x) (reflecting the fact that entities in spatial relations take different roles (Miller and Johnson-Laird, 1976)). These axioms reduce the number of possible denotations for R2 symbols from |E| 2 to |E| 2 2 \u2212 |E|.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_86",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "343-ARR_v2_87",
            "content": "To test the benefit of using noisy training exemplars N n from the oblique symbols in the REs-in other words, those symbols that are a part of the RE but not designated by it-we implemented a HEAD grounder baseline, which uses information only from C n . That HEAD uses only symbol-designation pairs that are acquired when the symbol denotes the referent (in our case, that's the head noun in the RE and its pre-head modifier, if it exists).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_88",
            "content": "To test the the benefit of using the precise formal semantic meanings of logical symbols (i.e., quantifiers and negation), we implemented an EXIST grounder baseline. This utilizes the information from the symbols in the oblique positions, but it does not utilize the precise symbolic interpretation of the logical symbols, instead simplifying the logical form of the RE by replacing all quantifiers with the existential _a_q and removing negation (e.g., \"every cross on the left of the one circle\" is equivalent to \"a cross on the left of a circle\"). This baseline preserves the basic linguistic structure of the formal semantic representation of the RE, but not its truth-functional interpretation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_89",
            "content": "Results and Discussion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "343-ARR_v2_90",
            "content": "Figure 2 shows the evolution of the performance of the IGRE grounder and the two baselines on the test set, as it gets exposed to more information (i.e., RE-designation pairs) over time. In the intrinsic evaluation (domain model prediction), there is no significant difference between the three grounders considered. Yet, for extrinsic evaluation (reference resolution), we observe that IGRE outperforms the HEAD and EXISTS baselines over time (both a steeper and a smoother curve). By the end of the interaction, a t-test shows significant differences in IGRE's performance compared with both baselines (p-value of 0.01).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_91",
            "content": "Table 3 shows the best performance that each grounder achieved over time. When analysing their performance on particular categories, we observe that C1 and S1 are equally hard to learn for grounders while R2 is easier.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_92",
            "content": "We suspect that the reason why the three models performed differently in extrinsic evaluation even though they don't with intrinsic evaluation is down to the fact that IGRE uses its complete and accurate knowledge of the meanings of closed class words like quantifiers and negation at test time as well as training time in the extrinsic evaluation, but not in the intrinsic evaluation. The IGRE model can use these meanings to constrain and correct error-prone estimates of referents for open class words at test time in the reference task (as well as using their meanings to boost the training sets). For example, the RE \"both squares\" implies there exist exactly two squares; if the symbol grounding model has an uncertain belief that there are more (or fewer) squares than this, it will select the two most probably candidates (and infer that all other entities are non-squares). These experiments suggest that this sort of correction to confident but wrong estimates of the denotations of open-class symbols happens sufficiently often at test time in the reference task to make a difference in this low-data regime we are interested in, for addressing ITL tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_93",
            "content": "Error Analysis",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "343-ARR_v2_94",
            "content": "The HEAD and EXISTS baselines never acquire negative exemplars: e.g., information that a particular individual is not red. Figure 2 shows that this severely impacts their performance, and error analysis showed that in some experiment runs it leads to model-collapse, with all denotations predicted to be in the extensions of all symbols. On the other hand, IGRE is able to acquire and use negative examples from the truth functional meanings of the logical symbols, specifically from: (a) negation (\"not\"); (b) the presupposition triggers\"the N \", \"N of the M \", and \"all but N \" where N , and M are numbers and \"both\"; and (c) the use of \"every\" when it modifies the head noun.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_95",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "343-ARR_v2_96",
            "content": "In this work, we presented IGRE-a grounder that supports incremental learning of the mapping from symbols to visual features whenever the teacher presents a linguistically complex RE and its desig-nation(s) in the visual scene. The grounder starts the learning process with no conceptualisation of the domain model, and so the learner must revise its hypothesis space of possible domain models as and when the teacher introduces new and unforeseen concepts via neologisms. We showed how exploiting the model-theoretic interpretation of the formal semantic representations of REs, and in particular the truth conditions of 'logical' words like quantifiers and negation, can inform the acquisition of noisy training exemplars that in turn guide learning-IGRE reasons about the likely denotations of symbols within an RE that aren't designated by that RE, and when sufficiently confident it exploits them to update its grounding model. We showed that: 1) this grounding approach is more data efficient then a model that omits such observations and reasoning, using only the designated symbols; and 2) it is beneficial to exploit the logical consequences of the logical symbols, to gain even more data efficiency and training stability. In both cases, there was much to be gained from such reasoning because in contrast to the baselines, it contributes to acquiring negative exemplars: in other words, objects that get associated with not being red, for example.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_97",
            "content": "Future Work",
            "ntype": "title",
            "meta": {
                "section": "7.1"
            }
        },
        {
            "ix": "343-ARR_v2_98",
            "content": "IGRE uses a single source of data augmentation by acquiring noisy exemplars from symbols in oblique positions. Further and parallel data gains may be obtained by exploring semi-supervised learning methods (Yarowsky, 1995;Delalleau et al., 2005).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_99",
            "content": "In this work, converting L-sentences to conjunctive normal form, which is an NP-hard problem, was a computational bottleneck. Future work needs to address this by either considering lifted inference methods (e.g., den Broeck et al. ( 2011)) or defining model counters that use L-sentences directly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_100",
            "content": "Finally, the purpose of IGRE is to aid ITL: i.e., the (incremental) updates to beliefs about symbol grounding should enhance learning to solve domain-level planning problems. Future work needs to address this by using IGRE to learn planning tasks where the learner has the physical ability to execute certain actions but starts out unaware of domain concepts that define the goal and are critical to task success. The learner must not only use IGRE to interpret the teacher's feedback, but also learn decision making strategies, both on what to say (or ask) the teacher in their extended dialogue and what actions to perform in the environment. Furthermore, the static formal semantics that we used here should be replaced with a dynamic semantics (e.g., Groenendijk and Stokhof (1991); van der Sandt (1992); Asher and Lascarides (2003)), to account for how contextual salience influences truth and reference in dialogue. Following Batra et al. (2020), we plan to test the benefits of IGRE within a system that learns to solve planning problems that focus on rearrangement tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v2_101",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Zitnick, Devi Parikh, VQA: visual question answering, 2015-12-07, 2015 IEEE International Conference on Computer Vision, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Stanislaw Antol",
                    "Aishwarya Agrawal",
                    "Jiasen Lu",
                    "Margaret Mitchell",
                    "Dhruv Batra",
                    "C Zitnick",
                    "Devi Parikh"
                ],
                "title": "VQA: visual question answering",
                "pub_date": "2015-12-07",
                "pub_title": "2015 IEEE International Conference on Computer Vision",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "343-ARR_v2_102",
            "content": "UNKNOWN, None, 2003, Logics of Conversation, Cambridge University Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2003",
                "pub_title": "Logics of Conversation",
                "pub": "Cambridge University Press"
            }
        },
        {
            "ix": "343-ARR_v2_103",
            "content": "Amos Azaria, Jayant Krishnamurthy, Tom Mitchell, Instructable intelligent personal agent, 2016-02-12, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Amos Azaria",
                    "Jayant Krishnamurthy",
                    "Tom Mitchell"
                ],
                "title": "Instructable intelligent personal agent",
                "pub_date": "2016-02-12",
                "pub_title": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "343-ARR_v2_104",
            "content": "Jon Barwise, Robin Cooper, Generalized quantifiers and natural language, 1981, Linguistics and Philosophy, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Jon Barwise",
                    "Robin Cooper"
                ],
                "title": "Generalized quantifiers and natural language",
                "pub_date": "1981",
                "pub_title": "Linguistics and Philosophy",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_105",
            "content": "UNKNOWN, None, 1975, Rearrangement: A challenge for embodied AI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "1975",
                "pub_title": "Rearrangement: A challenge for embodied AI",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_106",
            "content": "Ben Bogin, Sanjay Subramanian, Matt Gardner, Jonathan Berant, Latent compositional representations improve systematic generalization in grounded question answering, 2021, Transactions of the Association of Computational Linguisticsl (TACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Ben Bogin",
                    "Sanjay Subramanian",
                    "Matt Gardner",
                    "Jonathan Berant"
                ],
                "title": "Latent compositional representations improve systematic generalization in grounded question answering",
                "pub_date": "2021",
                "pub_title": "Transactions of the Association of Computational Linguisticsl (TACL)",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_107",
            "content": "Jos\u00e9 Miguel Cano Sant\u00edn, Simon Dobnik, Mehdi Ghanimifard, Fast visual grounding in interaction: bringing few-shot learning with neural networks to an interactive robot, 2020, Proceedings of the Probability and Meaning Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jos\u00e9 Miguel Cano Sant\u00edn",
                    "Simon Dobnik",
                    "Mehdi Ghanimifard"
                ],
                "title": "Fast visual grounding in interaction: bringing few-shot learning with neural networks to an interactive robot",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Probability and Meaning Conference",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_108",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, UNITER: universal image-text representation learning, 2020-08-23, Computer Vision -ECCV 2020 -16th European Conference, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Yen-Chun Chen",
                    "Linjie Li",
                    "Licheng Yu",
                    "Ahmed Kholy",
                    "Faisal Ahmed",
                    "Zhe Gan",
                    "Yu Cheng",
                    "Jingjing Liu"
                ],
                "title": "UNITER: universal image-text representation learning",
                "pub_date": "2020-08-23",
                "pub_title": "Computer Vision -ECCV 2020 -16th European Conference",
                "pub": "Springer"
            }
        },
        {
            "ix": "343-ARR_v2_109",
            "content": "Ann Copestake, Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go, 2009-03-30, EACL 2009, 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Ann Copestake"
                ],
                "title": "Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go",
                "pub_date": "2009-03-30",
                "pub_title": "EACL 2009, 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_110",
            "content": "Ann Copestake, D Flickinger, C Pollard, I Sag, Minimal recursion semantics: An introduction, 1997, Research on Language and Computation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Ann Copestake",
                    "D Flickinger",
                    "C Pollard",
                    "I Sag"
                ],
                "title": "Minimal recursion semantics: An introduction",
                "pub_date": "1997",
                "pub_title": "Research on Language and Computation",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_111",
            "content": "James Curran, Stephen Clark, Johan Bos, Linguistically motivated large-scale NLP with c&c and boxer, 2007-06-23, ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, The Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "James Curran",
                    "Stephen Clark",
                    "Johan Bos"
                ],
                "title": "Linguistically motivated large-scale NLP with c&c and boxer",
                "pub_date": "2007-06-23",
                "pub_title": "ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics",
                "pub": "The Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v2_112",
            "content": "Robert Dale, Ehud Reiter, Computational interpretations of the gricean maxims in the generation of referring expressions, 1995, Cognitive science, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Robert Dale",
                    "Ehud Reiter"
                ],
                "title": "Computational interpretations of the gricean maxims in the generation of referring expressions",
                "pub_date": "1995",
                "pub_title": "Cognitive science",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_113",
            "content": "Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh, Ajay Divakaran, Align2ground: Weakly supervised phrase grounding guided by image-caption alignment, 2019-10-27, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Samyak Datta",
                    "Karan Sikka",
                    "Anirban Roy",
                    "Karuna Ahuja",
                    "Devi Parikh",
                    "Ajay Divakaran"
                ],
                "title": "Align2ground: Weakly supervised phrase grounding guided by image-caption alignment",
                "pub_date": "2019-10-27",
                "pub_title": "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019",
                "pub": "IEEE"
            }
        },
        {
            "ix": "343-ARR_v2_114",
            "content": "Olivier Delalleau, Yoshua Bengio, Nicolas Roux, Efficient non-parametric function induction in semi-supervised learning, 2005-01-06, Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, AISTATS 2005, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Olivier Delalleau",
                    "Yoshua Bengio",
                    "Nicolas Roux"
                ],
                "title": "Efficient non-parametric function induction in semi-supervised learning",
                "pub_date": "2005-01-06",
                "pub_title": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, AISTATS 2005",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_115",
            "content": "Guy Van Den Broeck, Nima Taghipour, Wannes Meert, Jesse Davis, Luc De Raedt, Lifted probabilistic inference by first-order knowledge compilation, 2011-07-16, IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Guy Van Den Broeck",
                    "Nima Taghipour",
                    "Wannes Meert",
                    "Jesse Davis",
                    "Luc De Raedt"
                ],
                "title": "Lifted probabilistic inference by first-order knowledge compilation",
                "pub_date": "2011-07-16",
                "pub_title": "IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_116",
            "content": "Jeffrey Dudek, Vu Phan, Moshe Vardi, ADDMC: weighted model counting with algebraic decision diagrams, 2020-02-07, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Jeffrey Dudek",
                    "Vu Phan",
                    "Moshe Vardi"
                ],
                "title": "ADDMC: weighted model counting with algebraic decision diagrams",
                "pub_date": "2020-02-07",
                "pub_title": "The Thirty-Second Innovative Applications of Artificial Intelligence Conference",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "343-ARR_v2_117",
            "content": "Evelyn Fix, Joseph Hodges, Discriminatory analysis -nonparametric discrimination: Consistency properties, 1989, International Statistical Review, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Evelyn Fix",
                    "Joseph Hodges"
                ],
                "title": "Discriminatory analysis -nonparametric discrimination: Consistency properties",
                "pub_date": "1989",
                "pub_title": "International Statistical Review",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_118",
            "content": "Dan Flickinger, On building a more effcient grammar by exploiting types, 2000, Nat. Lang. Eng, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Dan Flickinger"
                ],
                "title": "On building a more effcient grammar by exploiting types",
                "pub_date": "2000",
                "pub_title": "Nat. Lang. Eng",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_119",
            "content": "Akira Fukui, Dong Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach, Multimodal compact bilinear pooling for visual question answering and visual grounding, 2016-11-01, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, The Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Akira Fukui",
                    "Dong Park",
                    "Daylen Yang",
                    "Anna Rohrbach",
                    "Trevor Darrell",
                    "Marcus Rohrbach"
                ],
                "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
                "pub_date": "2016-11-01",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "The Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v2_120",
            "content": "Jeroen Groenendijk, Martin Stokhof, Dynamic predicate logic, 1991, Linguistics and Philosophy, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Jeroen Groenendijk",
                    "Martin Stokhof"
                ],
                "title": "Dynamic predicate logic",
                "pub_date": "1991",
                "pub_title": "Linguistics and Philosophy",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_121",
            "content": "UNKNOWN, None, 1999, The symbol grounding problem. CoRR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": "1999",
                "pub_title": "The symbol grounding problem. CoRR",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_122",
            "content": "Yordan Hristov, Daniel Angelov, Michael Burke, Alex Lascarides, Subramanian Ramamoorthy, Disentangled relational representations for explaining and learning from demonstration, 2019-10-30, 3rd Annual Conference on Robot Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Yordan Hristov",
                    "Daniel Angelov",
                    "Michael Burke",
                    "Alex Lascarides",
                    "Subramanian Ramamoorthy"
                ],
                "title": "Disentangled relational representations for explaining and learning from demonstration",
                "pub_date": "2019-10-30",
                "pub_title": "3rd Annual Conference on Robot Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "343-ARR_v2_123",
            "content": "Yordan Hristov, Alex Lascarides, Subramanian Ramamoorthy, Interpretable latent spaces for learning from demonstration, 2018-10-31, of Proceedings of Machine Learning Research, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Yordan Hristov",
                    "Alex Lascarides",
                    "Subramanian Ramamoorthy"
                ],
                "title": "Interpretable latent spaces for learning from demonstration",
                "pub_date": "2018-10-31",
                "pub_title": "of Proceedings of Machine Learning Research",
                "pub": "PMLR"
            }
        },
        {
            "ix": "343-ARR_v2_124",
            "content": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Weinberger, Densely connected convolutional networks, 2017-07-21, 2017 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Gao Huang",
                    "Zhuang Liu",
                    "Laurens Van Der Maaten",
                    "Kilian Weinberger"
                ],
                "title": "Densely connected convolutional networks",
                "pub_date": "2017-07-21",
                "pub_title": "2017 IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "343-ARR_v2_125",
            "content": "Yichen Jiang, Mohit Bansal, Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Yichen Jiang",
                    "Mohit Bansal"
                ],
                "title": "Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_126",
            "content": "UNKNOWN, None, , Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. 2021. MDETR -modulated detection for end-to-end multimodal understanding, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. 2021. MDETR -modulated detection for end-to-end multimodal understanding",
                "pub": "CoRR"
            }
        },
        {
            "ix": "343-ARR_v2_127",
            "content": "UNKNOWN, None, 2010, Learning adaptive language interfaces through decomposition. CoRR, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2010",
                "pub_title": "Learning adaptive language interfaces through decomposition. CoRR, abs",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_128",
            "content": "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Referitgame: Referring to objects in photographs of natural scenes, 2014-10-25, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Sahar Kazemzadeh",
                    "Vicente Ordonez",
                    "Mark Matten",
                    "Tamara Berg"
                ],
                "title": "Referitgame: Referring to objects in photographs of natural scenes",
                "pub_date": "2014-10-25",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
                "pub": "ACL"
            }
        },
        {
            "ix": "343-ARR_v2_129",
            "content": "Casey Kennington, Livia Dia, David Schlangen, A discriminative model for perceptuallygrounded incremental reference resolution, 2015-04-17, Proceedings of the 11th International Conference on Computational Semantics, IWCS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Casey Kennington",
                    "Livia Dia",
                    "David Schlangen"
                ],
                "title": "A discriminative model for perceptuallygrounded incremental reference resolution",
                "pub_date": "2015-04-17",
                "pub_title": "Proceedings of the 11th International Conference on Computational Semantics, IWCS",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_130",
            "content": "Casey Kennington, David Schlangen, A simple generative model of incremental reference resolution for situated dialogue, 2017, Comput. Speech Lang, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Casey Kennington",
                    "David Schlangen"
                ],
                "title": "A simple generative model of incremental reference resolution for situated dialogue",
                "pub_date": "2017",
                "pub_title": "Comput. Speech Lang",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_131",
            "content": "Alexander Koller, Stefan Thater, The evolution of dominance constraint solvers, 2005, Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Alexander Koller",
                    "Stefan Thater"
                ],
                "title": "The evolution of dominance constraint solvers",
                "pub_date": "2005",
                "pub_title": "Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_132",
            "content": "UNKNOWN, None, 2017, Shapeworld -A new test methodology for multimodal language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Shapeworld -A new test methodology for multimodal language understanding",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_133",
            "content": "John Laird, Kevin Gluck, John Anderson, Kenneth Forbus,  Odest Chadwicke, Christian Jenkins, Dario Lebiere, Matthias Salvucci, Andrea Scheutz, J Thomaz, Robert Trafton, Shiwali Wray, James Mohan,  Kirk, None, 2017, teractive learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "John Laird",
                    "Kevin Gluck",
                    "John Anderson",
                    "Kenneth Forbus",
                    " Odest Chadwicke",
                    "Christian Jenkins",
                    "Dario Lebiere",
                    "Matthias Salvucci",
                    "Andrea Scheutz",
                    "J Thomaz",
                    "Robert Trafton",
                    "Shiwali Wray",
                    "James Mohan",
                    " Kirk"
                ],
                "title": null,
                "pub_date": "2017",
                "pub_title": "teractive learning",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_134",
            "content": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao, Oscar: Object-semantics aligned pre-training for vision-language tasks, 2020-08-23, Computer Vision -ECCV 2020 -16th European Conference, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Xiujun Li",
                    "Xi Yin",
                    "Chunyuan Li",
                    "Pengchuan Zhang",
                    "Xiaowei Hu",
                    "Lei Zhang",
                    "Lijuan Wang",
                    "Houdong Hu",
                    "Li Dong",
                    "Furu Wei",
                    "Yejin Choi",
                    "Jianfeng Gao"
                ],
                "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
                "pub_date": "2020-08-23",
                "pub_title": "Computer Vision -ECCV 2020 -16th European Conference",
                "pub": "Springer"
            }
        },
        {
            "ix": "343-ARR_v2_135",
            "content": "Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Jiasen Lu",
                    "Dhruv Batra",
                    "Devi Parikh",
                    "Stefan Lee"
                ],
                "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
                "pub_date": "2019-12-08",
                "pub_title": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_136",
            "content": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua Tenenbaum, Jiajun Wu, The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Jiayuan Mao",
                    "Chuang Gan",
                    "Pushmeet Kohli",
                    "Joshua Tenenbaum",
                    "Jiajun Wu"
                ],
                "title": "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
                "pub_date": "2019-05-06",
                "pub_title": "7th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_137",
            "content": "Cynthia Matuszek, Grounded language learning: Where robotics and NLP meet, 2018-07-13, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Cynthia Matuszek"
                ],
                "title": "Grounded language learning: Where robotics and NLP meet",
                "pub_date": "2018-07-13",
                "pub_title": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_138",
            "content": "UNKNOWN, None, 1976, Language and Perception, Belknap Press Imprint.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "1976",
                "pub_title": "Language and Perception",
                "pub": "Belknap Press Imprint"
            }
        },
        {
            "ix": "343-ARR_v2_139",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Learning transferable visual models from natural language supervision, 2021-07, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Alec Radford",
                    "Jong Kim",
                    "Chris Hallacy",
                    "Aditya Ramesh",
                    "Gabriel Goh",
                    "Sandhini Agarwal",
                    "Girish Sastry",
                    "Amanda Askell",
                    "Pamela Mishkin",
                    "Jack Clark",
                    "Gretchen Krueger",
                    "Ilya Sutskever"
                ],
                "title": "Learning transferable visual models from natural language supervision",
                "pub_date": "2021-07",
                "pub_title": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021",
                "pub": "PMLR"
            }
        },
        {
            "ix": "343-ARR_v2_140",
            "content": "Bertrand Russell, Knowledge by acquaintance and knowledge by description, 1917, Mysticism and Logic, Longmans Green.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Bertrand Russell"
                ],
                "title": "Knowledge by acquaintance and knowledge by description",
                "pub_date": "1917",
                "pub_title": "Mysticism and Logic",
                "pub": "Longmans Green"
            }
        },
        {
            "ix": "343-ARR_v2_141",
            "content": "Marko Samer, Stefan Szeider, Algorithms for propositional model counting, 2010, J. Discrete Algorithms, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Marko Samer",
                    "Stefan Szeider"
                ],
                "title": "Algorithms for propositional model counting",
                "pub_date": "2010",
                "pub_title": "J. Discrete Algorithms",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_142",
            "content": "Haoyue Shi, Jiayuan Mao, Kevin Gimpel, Karen Livescu, Visually grounded neural syntax acquisition, 2019-07-28, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Haoyue Shi",
                    "Jiayuan Mao",
                    "Kevin Gimpel",
                    "Karen Livescu"
                ],
                "title": "Visually grounded neural syntax acquisition",
                "pub_date": "2019-07-28",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v2_143",
            "content": "Mohit Shridhar, David Hsu, Interactive visual grounding of referring expressions for humanrobot interaction, 2018-06-26, Robotics: Science and Systems XIV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Mohit Shridhar",
                    "David Hsu"
                ],
                "title": "Interactive visual grounding of referring expressions for humanrobot interaction",
                "pub_date": "2018-06-26",
                "pub_title": "Robotics: Science and Systems XIV",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_144",
            "content": "Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner, Obtaining faithful interpretations from compositional neural networks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Sanjay Subramanian",
                    "Ben Bogin",
                    "Nitish Gupta",
                    "Tomer Wolfson",
                    "Sameer Singh",
                    "Jonathan Berant",
                    "Matt Gardner"
                ],
                "title": "Obtaining faithful interpretations from compositional neural networks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v2_145",
            "content": "UNKNOWN, None, 2019, Analyzing compositionality in visual question answering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Analyzing compositionality in visual question answering",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_146",
            "content": "Hao Tan, Mohit Bansal, LXMERT: learning cross-modality encoder representations from transformers, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": [
                    "Hao Tan",
                    "Mohit Bansal"
                ],
                "title": "LXMERT: learning cross-modality encoder representations from transformers",
                "pub_date": "2019-11-03",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_147",
            "content": "Leslie Valiant, The complexity of computing the permanent, 1979, Theor. Comput. Sci, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Leslie Valiant"
                ],
                "title": "The complexity of computing the permanent",
                "pub_date": "1979",
                "pub_title": "Theor. Comput. Sci",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_148",
            "content": "Johan Van Benthem, Questions about quantifiers, 1984, J. Symb. Log, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Johan Van Benthem"
                ],
                "title": "Questions about quantifiers",
                "pub_date": "1984",
                "pub_title": "J. Symb. Log",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_149",
            "content": "Rob Van Der,  Sandt, Presupposition projection as anaphora resolution, 1992, Journal of Semantics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Rob Van Der",
                    " Sandt"
                ],
                "title": "Presupposition projection as anaphora resolution",
                "pub_date": "1992",
                "pub_title": "Journal of Semantics",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_150",
            "content": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, Daan Wierstra, Matching networks for one shot learning, 2016-12-05, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "Oriol Vinyals",
                    "Charles Blundell",
                    "Tim Lillicrap",
                    "Koray Kavukcuoglu",
                    "Daan Wierstra"
                ],
                "title": "Matching networks for one shot learning",
                "pub_date": "2016-12-05",
                "pub_title": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_151",
            "content": "I Sida, Samuel Wang, Percy Ginn, Christopher Liang,  Manning, Naturalizing a programming language via interactive learning, 2017-07-30, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "I Sida",
                    "Samuel Wang",
                    "Percy Ginn",
                    "Christopher Liang",
                    " Manning"
                ],
                "title": "Naturalizing a programming language via interactive learning",
                "pub_date": "2017-07-30",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v2_152",
            "content": "I Sida, Percy Wang, Christopher Liang,  Manning, Learning language games through interaction, 2016-08-07, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, .",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "I Sida",
                    "Percy Wang",
                    "Christopher Liang",
                    " Manning"
                ],
                "title": "Learning language games through interaction",
                "pub_date": "2016-08-07",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_153",
            "content": "David Whitney, Miles Eldon, John Oberlin, Stefanie Tellex, Interpreting multimodal referring expressions in real time, 2016, 2016 IEEE International Conference on Robotics and Automation (ICRA), .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "David Whitney",
                    "Miles Eldon",
                    "John Oberlin",
                    "Stefanie Tellex"
                ],
                "title": "Interpreting multimodal referring expressions in real time",
                "pub_date": "2016",
                "pub_title": "2016 IEEE International Conference on Robotics and Automation (ICRA)",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_154",
            "content": "Ronald Williams, Simple statistical gradientfollowing algorithms for connectionist reinforcement learning, 1992, Mach. Learn, .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "Ronald Williams"
                ],
                "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
                "pub_date": "1992",
                "pub_title": "Mach. Learn",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_155",
            "content": "David Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, 1995, 33rd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "David Yarowsky"
                ],
                "title": "Unsupervised word sense disambiguation rivaling supervised methods",
                "pub_date": "1995",
                "pub_title": "33rd Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v2_156",
            "content": "Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang, Cross-modal self-attention network for referring image segmentation, 2019-06-16, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Computer Vision Foundation / IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "Linwei Ye",
                    "Mrigank Rochan",
                    "Zhi Liu",
                    "Yang Wang"
                ],
                "title": "Cross-modal self-attention network for referring image segmentation",
                "pub_date": "2019-06-16",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019",
                "pub": "Computer Vision Foundation / IEEE"
            }
        },
        {
            "ix": "343-ARR_v2_157",
            "content": "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum, Neural-symbolic VQA: disentangling reasoning from vision and language understanding, 2018-12-03, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Kexin Yi",
                    "Jiajun Wu",
                    "Chuang Gan",
                    "Antonio Torralba",
                    "Pushmeet Kohli",
                    "Josh Tenenbaum"
                ],
                "title": "Neural-symbolic VQA: disentangling reasoning from vision and language understanding",
                "pub_date": "2018-12-03",
                "pub_title": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v2_158",
            "content": "Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hua Hao Tian, Haifeng Wu,  Wang, Ernie-vil: Knowledge enhanced vision-language representations through scene graphs, 2021-02-02, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Fei Yu",
                    "Jiji Tang",
                    "Weichong Yin",
                    "Yu Sun",
                    "Hua Hao Tian",
                    "Haifeng Wu",
                    " Wang"
                ],
                "title": "Ernie-vil: Knowledge enhanced vision-language representations through scene graphs",
                "pub_date": "2021-02-02",
                "pub_title": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "343-ARR_v2_159",
            "content": "Yanpeng Zhao, Ivan Titov, Visually grounded compound pcfgs, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": [
                    "Yanpeng Zhao",
                    "Ivan Titov"
                ],
                "title": "Visually grounded compound pcfgs",
                "pub_date": "2020-11-16",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "343-ARR_v2_0@0",
            "content": "Interactive Symbol Grounding with Complex Referential Expressions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_0",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_2@0",
            "content": "We present a procedure for learning to ground symbols from a sequence of stimuli consisting of an arbitrarily complex noun phrase (e.g. \"all but one green square above both red circles.\") and its designation in the visual scene.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_2",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_2@1",
            "content": "Our distinctive approach combines: a) lazy fewshot learning to relate open-class words like green and above to their visual percepts; and b) symbolic reasoning with closed-class word categories like quantifiers and negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_2",
            "start": 229,
            "end": 452,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_2@2",
            "content": "We use this combination to estimate new training examples for grounding symbols that occur within a noun phrase but aren't designated by that noun phase (e.g, red in the above example), thereby potentially gaining data efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_2",
            "start": 454,
            "end": 683,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_2@3",
            "content": "We evaluate the approach in a visual reference resolution task, in which the learner starts out unaware of concepts that are part of the domain model and how they relate to visual percepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_2",
            "start": 685,
            "end": 873,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_4@0",
            "content": "The subfield of robotics known as Interactive Task Learning (ITL, see Laird et al. (2017) for a survey) addresses scenarios where a robot must learn to adapt its behaviour to novel and unforeseen objects, relations, and attributes that are introduced into the environment after deployment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_4",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_4@1",
            "content": "The ITL agent learns its novel task via evidence from its own actions and reactive guidance from a teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_4",
            "start": 290,
            "end": 396,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_4@2",
            "content": "This paper focuses on symbol grounding (Harnad, 1999) in the context of ITL (Matuszek, 2018): the learner must use the teacher's embodied natural language utterance and its context to learn a mapping from natural language expressions to their denotations, given the visual percepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_4",
            "start": 398,
            "end": 679,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_5@0",
            "content": "There are two challenges in learning symbol grounding models (grounders) in ITL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_5",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_5@1",
            "content": "Firstly, in contrast to many grounders (Ye et al., 2019;Datta et al., 2019), ITL requires incremental learning: knowledge is acquired piecemeal via an extended interaction, and it must influence planning as and when it occurs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_5",
            "start": 81,
            "end": 306,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_5@2",
            "content": "Secondly, previous work limits the teacher's language to bare nouns (e.g., square) or very short phrases (e.g., blue square, square above circle) (Hristov et al., 2018(Hristov et al., , 2019. But there's evidence from Dale and Reiter (1995) that speakers use complex referring expressions even when simpler ones would successfully refer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_5",
            "start": 308,
            "end": 644,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_5@3",
            "content": "Such language creates the possibility that novel symbolsneologisms-are introduced in a context where their denotation is not designated by the teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_5",
            "start": 646,
            "end": 796,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_5@4",
            "content": "In this work we study the natural language of complex referential expressions (REs) like \"a blue square behind both red circles\" which teachers can use when designating an object.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_5",
            "start": 798,
            "end": 976,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_6@0",
            "content": "Our aim is for the learner to extract knowledge that improves their domain representation and state estimates-a necessary condition for successful planning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_6",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_6@1",
            "content": "Contemporary grounders miss learning opportunities that complex REs afford: the RE example above not only entails that its referents are blue and square, but also that there exists two objects that are both red and circle and that they are above the designated objects, and everything else in the domain is either not red or not a circle (thanks to the meaning of both).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_6",
            "start": 157,
            "end": 526,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_6@2",
            "content": "Thus, a complex RE and its designation can be used to gather multiple (noisy) training exemplars (both positive and negative) for several symbols at once, even if they have not been designated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_6",
            "start": 528,
            "end": 720,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_7@0",
            "content": "In this work, we develop a method to integrate knowledge from interactively gathered evidence in the form of complex RE-designation pairs to aid data acquisition for a (neural) few-shot grounder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_7",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_7@1",
            "content": "We explore the effect of such a method on data efficiency and the overall grounder's performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_7",
            "start": 196,
            "end": 292,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_7@2",
            "content": "A major novel component to our procedure is that we exploit the formal semantics of closed class word categories (e.g., quantifiers and negation) to boost the data efficiency of few-shot neural grounding models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_7",
            "start": 294,
            "end": 504,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_7@3",
            "content": "Our experiments show these symbolic inductive biases are successful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_7",
            "start": 506,
            "end": 573,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_8@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_8",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_9@0",
            "content": "Symbol Grounding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_9",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_9@1",
            "content": "Contemporary grounders extensively utilize batch learning (e.g. Shridhar and Hsu (2018)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_9",
            "start": 18,
            "end": 106,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_9@2",
            "content": "Yet, ITL requires incremental learning because without it the teacher guidance cannot influence the learner's inferences about plans as and when the advice is given.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_9",
            "start": 108,
            "end": 272,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_9@3",
            "content": "Further, many grounders assume that the learner starts out with a complete and accurate conceptualisation of the domain using pre-defined visual features and a known vocabulary (Kennington et al., 2015;Kennington and Schlangen, 2017;Wang et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_9",
            "start": 274,
            "end": 525,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_9@4",
            "content": "In ITL, both of these assumptions are unrealistic; therefore in this paper we explore models for which these assumptions don't apply.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_9",
            "start": 527,
            "end": 659,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_9@5",
            "content": "Finally, in contrast to all prior grounders, we support incremental learning when the training exemplars feature REs that are linguistically complex: e.g., \"two red circles that aren't to the right of both green squares\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_9",
            "start": 661,
            "end": 881,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_10@0",
            "content": "Representation Learning. Models for jointly learning a representation for vision and language utilize either explicit alignment via bounding boxes or instance segmentation (Lu et al., 2019;Chen et al., 2020;Tan and Bansal, 2019;Kamath et al., 2021;Yu et al., 2021), or a large-volume of weakly labeled data in the form of image-caption pairs (Radford et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_10",
            "start": 0,
            "end": 364,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_10@1",
            "content": "These models rely on offline learning with large datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_10",
            "start": 366,
            "end": 423,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_10@2",
            "content": "This work, on the other hand, explores how to incrementally extract knowledge from few-shot learning, using sequentially observed evidence that includes neologisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_10",
            "start": 425,
            "end": 588,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_11@0",
            "content": "Visual Questions Answering (VQA).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_11",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_11@1",
            "content": "This is a task of answering free-form questions about an image (Antol et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_11",
            "start": 34,
            "end": 117,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_11@2",
            "content": "VQA has reached impressive performance in recent years (Fukui et al., 2016;, yet VQA models struggle with outof distribution generalization for new types of questions, requiring multi-step reasoning, with analysis revealing that they often rely on shortcuts (Jiang and Bansal, 2019;Subramanian et al., , 2020.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_11",
            "start": 119,
            "end": 427,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_11@3",
            "content": "Grounded VQA models like (Yi et al., 2018) and Bogin et al. (2021) tackle these shortcomings by grounding parts of the question and then learning to compose those parts via the question's syntax to compute the answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_11",
            "start": 429,
            "end": 645,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_11@4",
            "content": "They thus estimate denotations of linguistic parts that are not denoted by the answer to the question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_11",
            "start": 647,
            "end": 748,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_11@5",
            "content": "These 'compositional' models help to achieve out-of-distribution generalization for novel questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_11",
            "start": 750,
            "end": 849,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_11@6",
            "content": "But they lack ITL's requirement for incremental learning: model training relies on batch learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_11",
            "start": 851,
            "end": 948,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_11@7",
            "content": "Furthermore, while their performance is impressive, error analysis shows that it makes mistakes when language includes logical concepts like quantifiers and negation (e.g. Bogin et al. (2021) Figure 9 shows that the determiner most incorrectly denotes an arbitrary subset of entities).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_11",
            "start": 950,
            "end": 1234,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_11@8",
            "content": "Our view is that there is little benefit in trying to learn to ground logic concepts as they are domain independent and can be interpreted using formal semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_11",
            "start": 1236,
            "end": 1397,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_11@9",
            "content": "In our experiments, we are testing the extent to which knowing and reasoning with the logical meanings of these symbols helps incremental grounding, and in particular estimating denotations of symbols within an RE that are not designated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_11",
            "start": 1399,
            "end": 1636,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_12@0",
            "content": "Grounded Language Acquisition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_12",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_12@1",
            "content": "This task is often realized as grounded grammar induction from image-caption pairs (Shi et al., 2019;Zhao and Titov, 2020), or as learning (neural) semantic parsers from a reward signal (Williams, 1992) in VQA Yi et al., 2018) or in planning (Azaria et al., 2016;Wang et al., 2016Wang et al., , 2017Karamcheti et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_12",
            "start": 31,
            "end": 354,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_12@2",
            "content": "There, the main objective is to learn to map natural language to logical forms, which in turn get associated with visual percepts during the learning process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_12",
            "start": 356,
            "end": 513,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_12@3",
            "content": "This paper does not aim to learn a semantic parser.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_12",
            "start": 515,
            "end": 565,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_12@4",
            "content": "Instead, we obtain logical forms from an existing broad-coverage grammar which is hard to engineer, but is robust on lexical variation (Curran et al., 2007).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_12",
            "start": 567,
            "end": 723,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_12@5",
            "content": "Our focus instead is on exploiting the logical consequences of those logical forms during symbol grounding-i.e., our focus is to utilise the interpretation of logical forms, and in particular the truth functional meanings of close-class words like quantifiers and negation, to inform the learning of mappings from (open-class) symbols like red to their denotations, given the visual percepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_12",
            "start": 725,
            "end": 1116,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_13@0",
            "content": "Visual Reference Resolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_13",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_13@1",
            "content": "In previous experiments, it is often assumed that there is a unique referent in the visual scene for the given RE in the test phase (Kazemzadeh et al., 2014;Whitney et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_13",
            "start": 29,
            "end": 207,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_13@2",
            "content": "We aim to cope with situations where the RE has multiple referents: identifying all the referents that satisfy an RE enables efficient planning, because it affords free choice when executing certain commands-e.g., \"move a square above both red circles\" when there is more than one square affords choosing a control policy so that resources are optimized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_13",
            "start": 209,
            "end": 562,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_14@0",
            "content": "Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_14",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_15@0",
            "content": "Formal Semantics of Natural Language",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_15",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_16@0",
            "content": "Predicate logic with generalized quantifiers L ( Barwise and Cooper, 1981;van Benthem, 1984) is a canonical meaning representation for natural languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_16",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_16@1",
            "content": "L-sentences \u03c6 are constructed recursively from predicates P , terms T (i.e., variables V and constants C), logical connectives O = {\u00ac, \u2227, \u2228, \u2192 } and quantifiers Q (see Table 1 column 1):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_16",
            "start": 154,
            "end": 339,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_17@0",
            "content": "\u03c6 ::= p(t 1 , . . . , t n ) \u2261 p(t n ) |(\u00ac\u03c6)|(\u03c6 1 \u2227 \u03c6 2 )|(\u03c6 1 \u2228 \u03c6 2 )|(\u03c6 1 \u2192 \u03c6 2 ) |(Qx(\u03c6 1 , \u03c6 2 ))",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_17",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_18@0",
            "content": "where p is an n-place predicate,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_18",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_19@0",
            "content": "t i \u2208 T are terms, Q \u2208 Q is a quantifier, and x \u2208 V is a variable (in Qx(\u03c6 1 , \u03c6 2 )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_19",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_20@0",
            "content": ", \u03c6 1 is the restrictor and \u03c6 2 the body).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_20",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_21@0",
            "content": "We also introduce \u03bb-expressions of the form \u03bbx.\u03c6, where x \u2208 V is free or absent in \u03c6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_21",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_22@0",
            "content": "Model-theoretic Interpretation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_22",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_23@0",
            "content": "L-sentences are interpreted using a domain model M = (E, I) consisting of a set of entities E (universe of discourse), and an extension function I that maps non-logical symbols P \u222a C to denotations (tuples of entities).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_23",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_23@1",
            "content": "For convenience, we assume I : C \u2192 E is one-to-one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_23",
            "start": 220,
            "end": 270,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_23@2",
            "content": "Variables are interpreted via an assignment function g : V \u2192 E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_23",
            "start": 272,
            "end": 334,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_24@0",
            "content": "The interpretation function \u2022 M,g specifies the semantic value of well-formed expressions of L:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_24",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_25@0",
            "content": "a M,g = I(a) if a \u2208 P \u222a C g(a) if a \u2208 V p(t n ) M,g = 1 iff ( t 1 M,g , . . . , t n M,g ) \u2208 p M,g \u00ac\u03c6 M,g = 1 iff \u03c6 M,g = 0 \u03c6 \u2227 \u03c8 M,g = 1 iff \u03c6 M,g = 1 and \u03c8 M,g = 1 \u03c6 \u2228 \u03c8 M,g = 1 iff \u03c6 M,g = 1 or \u03c8 M,g = 1 \u03c6 \u2192 \u03c8 M,g = 1 iff \u03c6 M,g = 0 or \u03c8 M,g = 1 \u03bbx.\u03c6 M,g = {e \u2208 E : \u03c6 M,g[x/e] = 1} Qx(\u03c6 1 , \u03c6 2 ) M,g = Q( \u03bb.x\u03c6 1 M,g , \u03bb.x\u03c6 2 M,g )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_25",
            "start": 0,
            "end": 331,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_26@0",
            "content": "where g[x/e] is just like g, except g[x/e](x) = e and Q is a specific relation between the restrictor \u03bbx.\u03c6 1 M,g and body \u03bbx.\u03c6 2 M,g , as defined in Table 1 column 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_26",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_26@1",
            "content": "\u2022 M,g is directly related to satisfiability for L-sentences:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_26",
            "start": 167,
            "end": 226,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_27@0",
            "content": "M, g |= \u03c6 iff \u03c6 M,g = 1 M |= \u03c6 iff \u03c6 M = 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_27",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_28@0",
            "content": "where \u03c6 M = 1 iff \u03c6 M,g = 1 for all g. Further, if x is the only free variable in \u03c6, then \u03bbx.\u03c6 M,g = \u03bbx.\u03c6 M,g for all g, g ; so without a loss of generality, this is expressed as \u03bbx.\u03c6 M .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_28",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_28@1",
            "content": "1 If all variables in Qx(\u03c6, \u03c8) are bound by quantifiers, then this L-sentence is true iff Q is true for all g. Some quantifiers, like \"both\", 2 are presupposition triggers: \"exactly two blocks are blue\" is different from \"both blocks are blue\" in that the latter is true only if there are exactly two individuals that are blocks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_28",
            "start": 188,
            "end": 516,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_28@2",
            "content": "We've adopted a Russellian interpretation (Russell, 1917) of these in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_28",
            "start": 518,
            "end": 595,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_29@0",
            "content": "Logical Forms of Referential Expressions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_29",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_30@0",
            "content": "We now define the logical forms of REs and their interpretations with respect to a domain model M. Noun phrases like \"a block\" are represented as _a_qx.block(x) . More generally, let Qx.\u03c6 be the logical form of an RE, where Q \u2208 Q and \u03c6 is an L-sentence with x \u2208 V being the only free variable in \u03c6. The referents Qx.\u03c6 M of this logical form with respect to M are computed as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_30",
            "start": 0,
            "end": 382,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_31@0",
            "content": "Qx.\u03c6 M = Q \u03c0(M,\u03c6,x)(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_31",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_32@0",
            "content": "where \u03c0(M, \u03c6, x) is an M-projection, giving a new domain model M with entities E = E \u2229 \u03bb.x\u03c6 M and Q M is a quantifier referent-a quantifier-specific subset of the power set of E. Table 1 column 4 gives the list of quantifier referents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_32",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_33@0",
            "content": "To illustrate, consider the domain model where:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_33",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_34@0",
            "content": "E = {a, b, c, d, f } I(cat) = {a, b} I(dog) = {c, d, f } I(bit) = {(c, a), (c, b), (d, b), (f, a), (f, b)}",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_34",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_35@0",
            "content": "The RE \"a dog that bit both cats\" has logical form _a_q x._both_q y(cat(y), dog(x) \u2227 bit(x, y)) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_35",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_35@1",
            "content": "By Equation 1, its referent is:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_35",
            "start": 98,
            "end": 128,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_36@0",
            "content": "_a_q \u03c0(M,_both_q y(cat(y),dog(x)\u2227bit(x,y)),x)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_36",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_37@0",
            "content": "The semantic value of the \u03bb-expression formed from this RE is a set of entities e \u2208 E for which the following quantifier condition is true: both_q(R, B) where R = \u03bby.cat(y) M,g[x/e] and B = \u03bby.dog(x) \u2227 bit(x, y)) In interaction, the learner observes an RE, which is parsed to logical form ( \u00a75.3.2) and interpreted with respect of the extracted feature vectors for denotations ( \u00a75.3.1) to perform reference resolution ( \u00a73.3) with respect to the estimated domain model M. In case of teacher feedback, RE and its designation is observed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_37",
            "start": 0,
            "end": 536,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_37@1",
            "content": "This is used to build the L-sentence that is added to \u2206 to update beliefs about the underlying concept vectors ( \u00a74.3.2), which in turn are used to update the support set ( \u00a73.3), used as parameters for the grounder ( \u00a74.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_37",
            "start": 538,
            "end": 761,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_37@2",
            "content": "Elements in blue are pre-defined elements of IGRE while elements in red are learned through interaction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_37",
            "start": 763,
            "end": 866,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_38@0",
            "content": ". quantifiers Q surface form condition Q(R, B) referent Q M _exactly_n_q exactly n |R \u2229 B| = n {A \u2286 E : |A| = n} _at_most_n_q at most n |R \u2229 B| \u2264 n {A \u2286 E : |A| \u2264 n} _at_least_n_q at least n |R \u2229 B| \u2265 n {A \u2286 E : |A| \u2265 n} _a_q a/an |R \u2229 B| = n {A \u2286 E : |A| \u2264 1} _every_q all/every |R \u2229 B| = |R| {A \u2286 E : |A| = |E|} _the_n_q the n |R \u2229 B| = n) \u2227 |R| = n {A \u2286 E : |A| = |E| \u2227 (|E| = n)} _both_q both |R \u2229 B| = 2) \u2227 |R| = 2 {A \u2286 E : |A| = |E| \u2227 |E| = 2} _all_but_n_q all but n |R \u2229 B| = |R| \u2212 n {A \u2286 E : |A| = |E| \u2212 n \u2227 |E| \u2265 n} _n_of_the_m_q n of the m |R \u2229 B| = n \u2227 |R| = m {A \u2286 E : |A| = n \u2227 |E| = m}",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_38",
            "start": 0,
            "end": 598,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_39@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_39",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_40@0",
            "content": "Below we present the procedure of interactive grounding with referential expressions (IGRE).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_40",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_40@1",
            "content": "The overall framework is given in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_40",
            "start": 93,
            "end": 135,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_41@0",
            "content": "Grounder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_41",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_42@0",
            "content": "Matching networks (Vinyals et al., 2016) are an extension of the k nearest-neighbour algorithm (Fix and Hodges, 1989) and has been used as a fast fewshot grounder in the ITL setting (Cano Sant\u00edn et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_42",
            "start": 0,
            "end": 208,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_42@1",
            "content": "For predicates P n \u2286 P of the same arity n, a grounder \u0398 n is parameterized by a support set",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_42",
            "start": 210,
            "end": 301,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_43@0",
            "content": "S n = {(x n i , y n i )} K n i=1 , consisting of K n pairs of feature vectors x n i \u2208 R dn for denotations e n \u2208 E n and concept vectors y n i \u2208 [0, 1] |P n | .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_43",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_44@0",
            "content": "In y n , the dimension z corresponds the predicate p z \u2208 P n and its value is the probability that p z (e n ) M,g = 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_44",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_45@0",
            "content": "Concept vectors have a one-to-one correspondence with the domain model M.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_45",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_46@0",
            "content": "Given a feature vector x n for a denotation e n \u2208 E n , \u0398 n predicts the concept vector \u0177n , using the following inference rule:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_46",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_47@0",
            "content": "\u0398 n (x n , S n ) = k i=1 \u03b1 n (x n i , x n ; S n )y n i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_47",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_48@0",
            "content": "where \u03b1 n is an attention kernel:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_48",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_49@0",
            "content": "\u03b1 n (x n i , x n ; S n ) = exp (f n (x n i ) \u2022 h n (x n )) k j=1 exp (f n (x n j ) \u2022 h n (x n )) f n (x n ) = ReLU(w n \u2022 x n + b n ) ||ReLU(w n \u2022 x n + b n )|| 2 h n (x n ) = ReLU(v n \u2022 x n + c n ) ||ReLU(v n \u2022 x n + c n )|| 2 ReLU(a) = max (0, a)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_49",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_50@0",
            "content": "with learnable parameters \u03b8 n = {w n , v n , b n , c n }, and S n is k = 3 nearest exemplars to x n from S n :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_50",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_51@0",
            "content": "S n = {(x n i , y n i ) \u2208 S n : x n i \u2208 V(k, x n , S n )} where V(k, x n , S n ) is a set of k nearest feature vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_51",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_52@0",
            "content": "Batch Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_52",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_53@0",
            "content": "Given S n , one can estimate \u0398 n either via batch learning performed offline, or-when S n is smallin real time, as outlined by Cano Sant\u00edn et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_53",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_53@1",
            "content": "In our scenario, we learn in real time by minimizing binary cross-entropy between the ground-truth y n and predicted \u0177n concept vectors:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_53",
            "start": 154,
            "end": 289,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_54@0",
            "content": "L(y n , \u0177n ) = \u2212 |P n | z=1 l(y n z , \u0177n z ) l(y n i , \u0177n i ) = y n i log(\u0177 n i ) + (1 \u2212 y n i ) log(1 \u2212 \u0177n i )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_54",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_55@0",
            "content": "Incremental Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_55",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_56@0",
            "content": "S n gets augmented whenever the teacher provides an RE-designation pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_56",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_56@1",
            "content": "This speech act provides two types of information: certain information C n in the form of denotation-symbol-semantic value triples (e n , p z , y n z ), corresponding to symbols and entities designated by the RE; and noisy information N n , corresponding to denotation-symbolsemantic value estimate triples (e n , p z , \u1ef9n z ), which are acquired from the symbols that are part of the RE and its referent inferred via (uncertain) reasoning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_56",
            "start": 73,
            "end": 512,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_56@2",
            "content": "E.g., the RE \"a circle below a square.\", entails that its designation e \u2208 E is a circle and so (e, circle, 1) is added to C n .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_56",
            "start": 514,
            "end": 640,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_56@3",
            "content": "But it also entails there exists an entity which is a square that is not designated by the RE, but rather this entity is in the below relation with the designated entity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_56",
            "start": 642,
            "end": 811,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_56@4",
            "content": "If the grounder is sufficiently confident about the referent for square, then the corresponding triple is added to N n .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_56",
            "start": 813,
            "end": 932,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_57@0",
            "content": "Acquiring Observations and Symbols",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_57",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_58@0",
            "content": "When the learner first observes its visual sceneand so the teacher has not expressed any concepts, and so the learner is currently unaware of all concepts-the noisy support set N n is populated with (e n , p z , 0.5) (0.5 is a default semantic value) for all e n in the scene and for all known n-place predicates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_58",
            "start": 0,
            "end": 312,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_58@1",
            "content": "Whenever the teacher's REdesignation pair features a neologism p * , then this expansion to the learner's vocabulary prompts adding (e n , p * , 0.5) to N n for all e n .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_58",
            "start": 314,
            "end": 483,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_58@2",
            "content": "During interaction, each RE-designation pair uttered by the teacher adds elements to C n (for designated symbols) and triggers updates to the N n elements for all entities in the current visual scene, as we'll now describe.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_58",
            "start": 485,
            "end": 707,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_59@0",
            "content": "Integrating the Teacher's Feedback",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_59",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_60@0",
            "content": "N n elements are interactively updated using an incrementally built domain-level theory \u2206, which is the conjunction of L-sentences that are built from the logical forms of the REs that the teacher has uttered so far and their designations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_60",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_60@1",
            "content": "To compute the beliefs about semantic values, given \u2206, we model the semantic value of L-sentences of the form p(t n ), in which t n are all constants (ground atom), as a random variable with Bernoulli's distribution B. Thus a distribution over the possible domain models can be estimated using (propositional) model counting MC (Valiant, 1979), which maps each L-sentence to the number of domain models satisfying it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_60",
            "start": 240,
            "end": 656,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_60@2",
            "content": "In this way, the semantic value of any proposition can be estimated as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_60",
            "start": 658,
            "end": 736,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_61@0",
            "content": "\u1ef9n z = MC(pz(e n )\u2227\u2206) MC(\u2206)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_61",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_62@0",
            "content": "if MC(\u2206) = 0 0.5 otherwise MC can be computed exactly or approximately (Samer and Szeider, 2010).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_62",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_62@1",
            "content": "In our experiments we use the ADDMC (Dudek et al., 2020) weighted model counter, with weights set to 0.5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_62",
            "start": 98,
            "end": 202,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_63@0",
            "content": "Building the Support Set",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_63",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_64@0",
            "content": "Concept vectors for S n are built using information in C n and N n : namely each denotation e n gets associated with its feature vector x n , and the zdimension of y corresponding to predicate p z \u2208 P n is computed as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_64",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_65@0",
            "content": "y n z = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 y n z if (e n , p z , y n z ) \u2208 C n \u1ef9n z if (e n , p z , \u1ef9n z ) \u2208 N n \u2227 H[B(\u1ef9 n z )] \u2264 \u03c4 n 0.5 otherwise",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_65",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_66@0",
            "content": "where H[P] is the entropy of the probability distribution P, and \u03c4 n is the confidence threshold for adding noisy exemplars: in our case, it's set to 0.6 for predicates of all arities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_66",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_67@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_67",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_68@0",
            "content": "Task: Visual Reference Resolution",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_68",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_69@0",
            "content": "To evaluate IGRE, we use a task of visual reference resolution 3 : given a visual scene (an image) with localized entities (bounding boxes) and an RE, the grounder must estimate all its referents, as defined in \u00a73.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_69",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_69@1",
            "content": "The model learns its task by observing an image accompanied by a sequence of REs, with each RE paired with its designation in the image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_69",
            "start": 217,
            "end": 352,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_70@0",
            "content": "We measure the performance of IGRE on the task after each observed RE and its designation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_70",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_70@1",
            "content": "Performance is measured using the precision P, recall R, and F1 score F1 on the test set between: 1) estimated vs. ground-truth domain models, formed from the concept vectors (intrinsic evaluation) and 2) estimated vs. ground-truth referents for the RE (extrinsic evaluation).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_70",
            "start": 91,
            "end": 366,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_70@2",
            "content": "These metrics are calculated only for those symbols/concepts that the teacher has mentioned so far (since the system is unaware that the remaining concepts exist).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_70",
            "start": 368,
            "end": 530,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_70@3",
            "content": "To obtain reliable results, we repeat the experiment 10 times: i.e., 10 different visual scenes, with a sequence of 5 different teacher utterances in each scene.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_70",
            "start": 532,
            "end": 692,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_70@4",
            "content": "We record in \u00a76 the average precision, recall and f-scores over those 10 trials.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_70",
            "start": 694,
            "end": 773,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_71@0",
            "content": "Perhaps unusually, this training and testing regime uses very small data sets: that's because in ITL it is the initial portions of the learning curve that matters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_71",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_71@1",
            "content": "The learner must achieve decent performance on its task via only a few teacher utterances: human teachers won't tolerate repeating the same REs many times and so the learner lacks the luxury of learning (and testing) symbol grounding on large data sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_71",
            "start": 164,
            "end": 416,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_72@0",
            "content": "Data: ShapeWorld",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_72",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_73@0",
            "content": "To generate training and test sets, we construct ShapeWorld domain models (Kuhnle and Copestake, 2017), each consisting of 3-12 entities, synthesized visual scenes X (64x64 pixels), and 5 REs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_73",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_73@1",
            "content": "Each domain model is describable using 7 shape symbols S1 (square, circle, triangle, pentagon, cross, ellipse, semicircle), 6 colour symbols C1 (red, green, blue, yellow, magenta, cyan) and 4 spatial relationships symbols R2 (left, right, above, below).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_73",
            "start": 193,
            "end": 445,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_73@2",
            "content": "4 In scene synthesis, the image is created from the domain model, with variation on the hue of the colour category, variation on the size, position, rotation, and distortion of the shapes, and variation on the spatial positions of the entities related by each spatial term.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_73",
            "start": 447,
            "end": 719,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_73@3",
            "content": "Note that the colour categories are not mutually exclusive-e.g., there are RGB values that count as both red and magenta.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_73",
            "start": 721,
            "end": 841,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_74@0",
            "content": "To generate REs, we sample Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) graph templates, processed using ACE (generation mode) 5 and the English Resource Grammar (ERG) (Flickinger, 2000).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_74",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_74@1",
            "content": "Generated REs are evaluated with respect to the domain model to guarantee an existing referent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_74",
            "start": 207,
            "end": 301,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_74@2",
            "content": "In total we generated 30 such domain models for training and 10 for testing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_74",
            "start": 303,
            "end": 378,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_74@3",
            "content": "The data statistics for the training set is given in Table 2 for the general categories of symbols, where certain (C n ) means that the designation is denoted by the symbol in the RE, and noisy (N n ) means that the symbol is a part of the RE but is not designated by it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_74",
            "start": 380,
            "end": 650,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_74@4",
            "content": "Note that the first argument to the spatial relations R2 is always denoted by the designation while its second argument is not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_74",
            "start": 652,
            "end": 778,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_74@5",
            "content": "Note also there is high variance in the frequencies among the individual symbols.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_74",
            "start": 780,
            "end": 860,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_74@6",
            "content": "For instance, blue occurs 27 and 28 times in certain vs. noisy positions respectively, while triangle occurs 7 and 12 times respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_74",
            "start": 862,
            "end": 998,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_75@0",
            "content": "Category C n candidates N n candidates C1 18.67 \u00b1 5.39 19.83 \u00b1 5.04 S1 14.67 \u00b1 3.98 16.50 \u00b1 5.32 R2 0 37.75 \u00b1 6.75",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_75",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_76@0",
            "content": "Table 2: Average symbol counts per word for colours (C1), shapes (S1), and spatial relationships (R2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_76",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_77@0",
            "content": "Implementation Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_77",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_78@0",
            "content": "Feature Extraction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_78",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_79@0",
            "content": "To extract visual features for individuals in the scene, we utilize bounding boxes b = [x left , x right , y top , y bottom ] for each entity e \u2208 E in the visual scene by localizing them (cropping) and extracting the visual features using a pre-trained visual feature encoder (in our case, DenseNet161 (Huang et al., 2017)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_79",
            "start": 0,
            "end": 323,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_79@1",
            "content": "Additionally, for the feature vector, we add each entity's bounding box coordinates for spatial information, lost in the localization process:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_79",
            "start": 325,
            "end": 466,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_80@0",
            "content": "x n = Concat({[DenseNet161(X[b i ]], b i )} n i=1 )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_80",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_81@0",
            "content": "Grammar-based Semantic Parsing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_81",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_82@0",
            "content": "To parse REs to their logical forms, we use the English Resource Grammar (ERG) and ACE (parsing mode) to produce a representation in minimal recursion semantics (MRS) (Copestake et al., 1997), which we then simplify via hand-written rules (e.g., removing event arguments from predicate symbols corresponding to adjectives and prepositions).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_82",
            "start": 0,
            "end": 339,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_82@1",
            "content": "Underspecification of the MRS was resolved using UTOOL (Koller and Thater, 2005) and the final logical form was selected based on the linear order of scope-bearing elements (quantifiers and negation): e.g. for the RE \"every circle above a square\", _every_q outscopes _a_q.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_82",
            "start": 341,
            "end": 612,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_83@0",
            "content": "Axioms for R2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_83",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_84@0",
            "content": "For |E| entities, there are |E| 2 denotations to consider for each 2-place predicate-a larger search space compared to |E| denotations for 1-place predicates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_84",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_84@1",
            "content": "Moreover, these predicates can only be acquired from the noisy component N n because the referent of the second argument to the relation is always latent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_84",
            "start": 159,
            "end": 312,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_85@0",
            "content": "To aid the learning process for R2, whenever a new symbol R \u2208 R2 is observed, domainlevel axioms are added to \u2206 for it, making it irrreflexive: \u2200x.\u00acR(x, x) (an entity cannot be in a spatial relationship to itself) and asymmetric: \u2200x, y.R(x, y) \u2192 \u00acR(y, x) (reflecting the fact that entities in spatial relations take different roles (Miller and Johnson-Laird, 1976)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_85",
            "start": 0,
            "end": 365,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_85@1",
            "content": "These axioms reduce the number of possible denotations for R2 symbols from |E| 2 to |E| 2 2 \u2212 |E|.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_85",
            "start": 367,
            "end": 464,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_86@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_86",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_87@0",
            "content": "To test the benefit of using noisy training exemplars N n from the oblique symbols in the REs-in other words, those symbols that are a part of the RE but not designated by it-we implemented a HEAD grounder baseline, which uses information only from C n .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_87",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_87@1",
            "content": "That HEAD uses only symbol-designation pairs that are acquired when the symbol denotes the referent (in our case, that's the head noun in the RE and its pre-head modifier, if it exists).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_87",
            "start": 255,
            "end": 440,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_88@0",
            "content": "To test the the benefit of using the precise formal semantic meanings of logical symbols (i.e., quantifiers and negation), we implemented an EXIST grounder baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_88",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_88@1",
            "content": "This utilizes the information from the symbols in the oblique positions, but it does not utilize the precise symbolic interpretation of the logical symbols, instead simplifying the logical form of the RE by replacing all quantifiers with the existential _a_q and removing negation (e.g., \"every cross on the left of the one circle\" is equivalent to \"a cross on the left of a circle\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_88",
            "start": 166,
            "end": 549,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_88@2",
            "content": "This baseline preserves the basic linguistic structure of the formal semantic representation of the RE, but not its truth-functional interpretation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_88",
            "start": 551,
            "end": 698,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_89@0",
            "content": "Results and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_89",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_90@0",
            "content": "Figure 2 shows the evolution of the performance of the IGRE grounder and the two baselines on the test set, as it gets exposed to more information (i.e., RE-designation pairs) over time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_90",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_90@1",
            "content": "In the intrinsic evaluation (domain model prediction), there is no significant difference between the three grounders considered.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_90",
            "start": 187,
            "end": 315,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_90@2",
            "content": "Yet, for extrinsic evaluation (reference resolution), we observe that IGRE outperforms the HEAD and EXISTS baselines over time (both a steeper and a smoother curve).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_90",
            "start": 317,
            "end": 481,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_90@3",
            "content": "By the end of the interaction, a t-test shows significant differences in IGRE's performance compared with both baselines (p-value of 0.01).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_90",
            "start": 483,
            "end": 621,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_91@0",
            "content": "Table 3 shows the best performance that each grounder achieved over time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_91",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_91@1",
            "content": "When analysing their performance on particular categories, we observe that C1 and S1 are equally hard to learn for grounders while R2 is easier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_91",
            "start": 74,
            "end": 217,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_92@0",
            "content": "We suspect that the reason why the three models performed differently in extrinsic evaluation even though they don't with intrinsic evaluation is down to the fact that IGRE uses its complete and accurate knowledge of the meanings of closed class words like quantifiers and negation at test time as well as training time in the extrinsic evaluation, but not in the intrinsic evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_92",
            "start": 0,
            "end": 384,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_92@1",
            "content": "The IGRE model can use these meanings to constrain and correct error-prone estimates of referents for open class words at test time in the reference task (as well as using their meanings to boost the training sets).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_92",
            "start": 386,
            "end": 600,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_92@2",
            "content": "For example, the RE \"both squares\" implies there exist exactly two squares; if the symbol grounding model has an uncertain belief that there are more (or fewer) squares than this, it will select the two most probably candidates (and infer that all other entities are non-squares).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_92",
            "start": 602,
            "end": 881,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_92@3",
            "content": "These experiments suggest that this sort of correction to confident but wrong estimates of the denotations of open-class symbols happens sufficiently often at test time in the reference task to make a difference in this low-data regime we are interested in, for addressing ITL tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_92",
            "start": 883,
            "end": 1165,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_93@0",
            "content": "Error Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_93",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_94@0",
            "content": "The HEAD and EXISTS baselines never acquire negative exemplars: e.g., information that a particular individual is not red.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_94",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_94@1",
            "content": "Figure 2 shows that this severely impacts their performance, and error analysis showed that in some experiment runs it leads to model-collapse, with all denotations predicted to be in the extensions of all symbols.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_94",
            "start": 123,
            "end": 336,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_94@2",
            "content": "On the other hand, IGRE is able to acquire and use negative examples from the truth functional meanings of the logical symbols, specifically from: (a) negation (\"not\"); (b) the presupposition triggers\"the N \", \"N of the M \", and \"all but N \" where N , and M are numbers and \"both\"; and (c) the use of \"every\" when it modifies the head noun.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_94",
            "start": 338,
            "end": 677,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_95@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_95",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_96@0",
            "content": "In this work, we presented IGRE-a grounder that supports incremental learning of the mapping from symbols to visual features whenever the teacher presents a linguistically complex RE and its desig-nation(s) in the visual scene.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_96",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_96@1",
            "content": "The grounder starts the learning process with no conceptualisation of the domain model, and so the learner must revise its hypothesis space of possible domain models as and when the teacher introduces new and unforeseen concepts via neologisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_96",
            "start": 228,
            "end": 471,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_96@2",
            "content": "We showed how exploiting the model-theoretic interpretation of the formal semantic representations of REs, and in particular the truth conditions of 'logical' words like quantifiers and negation, can inform the acquisition of noisy training exemplars that in turn guide learning-IGRE reasons about the likely denotations of symbols within an RE that aren't designated by that RE, and when sufficiently confident it exploits them to update its grounding model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_96",
            "start": 473,
            "end": 931,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_96@3",
            "content": "We showed that: 1) this grounding approach is more data efficient then a model that omits such observations and reasoning, using only the designated symbols; and 2) it is beneficial to exploit the logical consequences of the logical symbols, to gain even more data efficiency and training stability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_96",
            "start": 933,
            "end": 1231,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_96@4",
            "content": "In both cases, there was much to be gained from such reasoning because in contrast to the baselines, it contributes to acquiring negative exemplars: in other words, objects that get associated with not being red, for example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_96",
            "start": 1233,
            "end": 1457,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_97@0",
            "content": "Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_97",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_98@0",
            "content": "IGRE uses a single source of data augmentation by acquiring noisy exemplars from symbols in oblique positions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_98",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_98@1",
            "content": "Further and parallel data gains may be obtained by exploring semi-supervised learning methods (Yarowsky, 1995;Delalleau et al., 2005).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_98",
            "start": 111,
            "end": 244,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_99@0",
            "content": "In this work, converting L-sentences to conjunctive normal form, which is an NP-hard problem, was a computational bottleneck.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_99",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_99@1",
            "content": "Future work needs to address this by either considering lifted inference methods (e.g., den Broeck et al. ( 2011)) or defining model counters that use L-sentences directly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_99",
            "start": 126,
            "end": 297,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_100@0",
            "content": "Finally, the purpose of IGRE is to aid ITL: i.e., the (incremental) updates to beliefs about symbol grounding should enhance learning to solve domain-level planning problems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_100",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_100@1",
            "content": "Future work needs to address this by using IGRE to learn planning tasks where the learner has the physical ability to execute certain actions but starts out unaware of domain concepts that define the goal and are critical to task success.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_100",
            "start": 175,
            "end": 412,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_100@2",
            "content": "The learner must not only use IGRE to interpret the teacher's feedback, but also learn decision making strategies, both on what to say (or ask) the teacher in their extended dialogue and what actions to perform in the environment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_100",
            "start": 414,
            "end": 643,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_100@3",
            "content": "Furthermore, the static formal semantics that we used here should be replaced with a dynamic semantics (e.g., Groenendijk and Stokhof (1991); van der Sandt (1992); Asher and Lascarides (2003)), to account for how contextual salience influences truth and reference in dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_100",
            "start": 645,
            "end": 920,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_100@4",
            "content": "Following Batra et al. (2020), we plan to test the benefits of IGRE within a system that learns to solve planning problems that focus on rearrangement tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_100",
            "start": 922,
            "end": 1078,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_101@0",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Zitnick, Devi Parikh, VQA: visual question answering, 2015-12-07, 2015 IEEE International Conference on Computer Vision, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_101",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_102@0",
            "content": "UNKNOWN, None, 2003, Logics of Conversation, Cambridge University Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_102",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_103@0",
            "content": "Amos Azaria, Jayant Krishnamurthy, Tom Mitchell, Instructable intelligent personal agent, 2016-02-12, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_103",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_104@0",
            "content": "Jon Barwise, Robin Cooper, Generalized quantifiers and natural language, 1981, Linguistics and Philosophy, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_104",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_105@0",
            "content": "UNKNOWN, None, 1975, Rearrangement: A challenge for embodied AI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_105",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_106@0",
            "content": "Ben Bogin, Sanjay Subramanian, Matt Gardner, Jonathan Berant, Latent compositional representations improve systematic generalization in grounded question answering, 2021, Transactions of the Association of Computational Linguisticsl (TACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_106",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_107@0",
            "content": "Jos\u00e9 Miguel Cano Sant\u00edn, Simon Dobnik, Mehdi Ghanimifard, Fast visual grounding in interaction: bringing few-shot learning with neural networks to an interactive robot, 2020, Proceedings of the Probability and Meaning Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_107",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_108@0",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, UNITER: universal image-text representation learning, 2020-08-23, Computer Vision -ECCV 2020 -16th European Conference, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_108",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_109@0",
            "content": "Ann Copestake, Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go, 2009-03-30, EACL 2009, 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_109",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_110@0",
            "content": "Ann Copestake, D Flickinger, C Pollard, I Sag, Minimal recursion semantics: An introduction, 1997, Research on Language and Computation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_110",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_111@0",
            "content": "James Curran, Stephen Clark, Johan Bos, Linguistically motivated large-scale NLP with c&c and boxer, 2007-06-23, ACL 2007, Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics, The Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_111",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_112@0",
            "content": "Robert Dale, Ehud Reiter, Computational interpretations of the gricean maxims in the generation of referring expressions, 1995, Cognitive science, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_112",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_113@0",
            "content": "Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh, Ajay Divakaran, Align2ground: Weakly supervised phrase grounding guided by image-caption alignment, 2019-10-27, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_113",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_114@0",
            "content": "Olivier Delalleau, Yoshua Bengio, Nicolas Roux, Efficient non-parametric function induction in semi-supervised learning, 2005-01-06, Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, AISTATS 2005, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_114",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_115@0",
            "content": "Guy Van Den Broeck, Nima Taghipour, Wannes Meert, Jesse Davis, Luc De Raedt, Lifted probabilistic inference by first-order knowledge compilation, 2011-07-16, IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_115",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_116@0",
            "content": "Jeffrey Dudek, Vu Phan, Moshe Vardi, ADDMC: weighted model counting with algebraic decision diagrams, 2020-02-07, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_116",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_117@0",
            "content": "Evelyn Fix, Joseph Hodges, Discriminatory analysis -nonparametric discrimination: Consistency properties, 1989, International Statistical Review, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_117",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_118@0",
            "content": "Dan Flickinger, On building a more effcient grammar by exploiting types, 2000, Nat. Lang. Eng, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_118",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_119@0",
            "content": "Akira Fukui, Dong Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach, Multimodal compact bilinear pooling for visual question answering and visual grounding, 2016-11-01, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, The Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_119",
            "start": 0,
            "end": 318,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_120@0",
            "content": "Jeroen Groenendijk, Martin Stokhof, Dynamic predicate logic, 1991, Linguistics and Philosophy, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_120",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_121@0",
            "content": "UNKNOWN, None, 1999, The symbol grounding problem. CoRR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_121",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_122@0",
            "content": "Yordan Hristov, Daniel Angelov, Michael Burke, Alex Lascarides, Subramanian Ramamoorthy, Disentangled relational representations for explaining and learning from demonstration, 2019-10-30, 3rd Annual Conference on Robot Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_122",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_123@0",
            "content": "Yordan Hristov, Alex Lascarides, Subramanian Ramamoorthy, Interpretable latent spaces for learning from demonstration, 2018-10-31, of Proceedings of Machine Learning Research, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_123",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_124@0",
            "content": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Weinberger, Densely connected convolutional networks, 2017-07-21, 2017 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_124",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_125@0",
            "content": "Yichen Jiang, Mohit Bansal, Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_125",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_126@0",
            "content": "UNKNOWN, None, , Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. 2021. MDETR -modulated detection for end-to-end multimodal understanding, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_126",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_127@0",
            "content": "UNKNOWN, None, 2010, Learning adaptive language interfaces through decomposition. CoRR, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_127",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_128@0",
            "content": "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Referitgame: Referring to objects in photographs of natural scenes, 2014-10-25, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_128",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_129@0",
            "content": "Casey Kennington, Livia Dia, David Schlangen, A discriminative model for perceptuallygrounded incremental reference resolution, 2015-04-17, Proceedings of the 11th International Conference on Computational Semantics, IWCS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_129",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_130@0",
            "content": "Casey Kennington, David Schlangen, A simple generative model of incremental reference resolution for situated dialogue, 2017, Comput. Speech Lang, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_130",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_131@0",
            "content": "Alexander Koller, Stefan Thater, The evolution of dominance constraint solvers, 2005, Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_131",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_132@0",
            "content": "UNKNOWN, None, 2017, Shapeworld -A new test methodology for multimodal language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_132",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_133@0",
            "content": "John Laird, Kevin Gluck, John Anderson, Kenneth Forbus,  Odest Chadwicke, Christian Jenkins, Dario Lebiere, Matthias Salvucci, Andrea Scheutz, J Thomaz, Robert Trafton, Shiwali Wray, James Mohan,  Kirk, None, 2017, teractive learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_133",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_134@0",
            "content": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao, Oscar: Object-semantics aligned pre-training for vision-language tasks, 2020-08-23, Computer Vision -ECCV 2020 -16th European Conference, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_134",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_135@0",
            "content": "Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_135",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_136@0",
            "content": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua Tenenbaum, Jiajun Wu, The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_136",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_137@0",
            "content": "Cynthia Matuszek, Grounded language learning: Where robotics and NLP meet, 2018-07-13, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_137",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_138@0",
            "content": "UNKNOWN, None, 1976, Language and Perception, Belknap Press Imprint.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_138",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_139@0",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Learning transferable visual models from natural language supervision, 2021-07, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_139",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_140@0",
            "content": "Bertrand Russell, Knowledge by acquaintance and knowledge by description, 1917, Mysticism and Logic, Longmans Green.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_140",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_141@0",
            "content": "Marko Samer, Stefan Szeider, Algorithms for propositional model counting, 2010, J. Discrete Algorithms, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_141",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_142@0",
            "content": "Haoyue Shi, Jiayuan Mao, Kevin Gimpel, Karen Livescu, Visually grounded neural syntax acquisition, 2019-07-28, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_142",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_143@0",
            "content": "Mohit Shridhar, David Hsu, Interactive visual grounding of referring expressions for humanrobot interaction, 2018-06-26, Robotics: Science and Systems XIV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_143",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_144@0",
            "content": "Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner, Obtaining faithful interpretations from compositional neural networks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_144",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_145@0",
            "content": "UNKNOWN, None, 2019, Analyzing compositionality in visual question answering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_145",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_146@0",
            "content": "Hao Tan, Mohit Bansal, LXMERT: learning cross-modality encoder representations from transformers, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_146",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_147@0",
            "content": "Leslie Valiant, The complexity of computing the permanent, 1979, Theor. Comput. Sci, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_147",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_148@0",
            "content": "Johan Van Benthem, Questions about quantifiers, 1984, J. Symb. Log, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_148",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_149@0",
            "content": "Rob Van Der,  Sandt, Presupposition projection as anaphora resolution, 1992, Journal of Semantics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_149",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_150@0",
            "content": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, Daan Wierstra, Matching networks for one shot learning, 2016-12-05, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_150",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_151@0",
            "content": "I Sida, Samuel Wang, Percy Ginn, Christopher Liang,  Manning, Naturalizing a programming language via interactive learning, 2017-07-30, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_151",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_152@0",
            "content": "I Sida, Percy Wang, Christopher Liang,  Manning, Learning language games through interaction, 2016-08-07, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_152",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_153@0",
            "content": "David Whitney, Miles Eldon, John Oberlin, Stefanie Tellex, Interpreting multimodal referring expressions in real time, 2016, 2016 IEEE International Conference on Robotics and Automation (ICRA), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_153",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_154@0",
            "content": "Ronald Williams, Simple statistical gradientfollowing algorithms for connectionist reinforcement learning, 1992, Mach. Learn, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_154",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_155@0",
            "content": "David Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, 1995, 33rd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_155",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_156@0",
            "content": "Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang, Cross-modal self-attention network for referring image segmentation, 2019-06-16, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Computer Vision Foundation / IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_156",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_157@0",
            "content": "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum, Neural-symbolic VQA: disentangling reasoning from vision and language understanding, 2018-12-03, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_157",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_158@0",
            "content": "Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hua Hao Tian, Haifeng Wu,  Wang, Ernie-vil: Knowledge enhanced vision-language representations through scene graphs, 2021-02-02, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_158",
            "start": 0,
            "end": 424,
            "label": {}
        },
        {
            "ix": "343-ARR_v2_159@0",
            "content": "Yanpeng Zhao, Ivan Titov, Visually grounded compound pcfgs, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v2_159",
            "start": 0,
            "end": 201,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "343-ARR_v2_0",
            "tgt_ix": "343-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_0",
            "tgt_ix": "343-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_1",
            "tgt_ix": "343-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_1",
            "tgt_ix": "343-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_0",
            "tgt_ix": "343-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_2",
            "tgt_ix": "343-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_4",
            "tgt_ix": "343-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_5",
            "tgt_ix": "343-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_6",
            "tgt_ix": "343-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_3",
            "tgt_ix": "343-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_3",
            "tgt_ix": "343-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_3",
            "tgt_ix": "343-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_3",
            "tgt_ix": "343-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_3",
            "tgt_ix": "343-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_0",
            "tgt_ix": "343-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_7",
            "tgt_ix": "343-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_9",
            "tgt_ix": "343-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_10",
            "tgt_ix": "343-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_12",
            "tgt_ix": "343-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_8",
            "tgt_ix": "343-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_8",
            "tgt_ix": "343-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_8",
            "tgt_ix": "343-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_8",
            "tgt_ix": "343-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_8",
            "tgt_ix": "343-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_8",
            "tgt_ix": "343-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_0",
            "tgt_ix": "343-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_13",
            "tgt_ix": "343-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_14",
            "tgt_ix": "343-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_14",
            "tgt_ix": "343-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_16",
            "tgt_ix": "343-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_17",
            "tgt_ix": "343-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_18",
            "tgt_ix": "343-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_19",
            "tgt_ix": "343-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_20",
            "tgt_ix": "343-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_15",
            "tgt_ix": "343-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_15",
            "tgt_ix": "343-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_15",
            "tgt_ix": "343-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_15",
            "tgt_ix": "343-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_15",
            "tgt_ix": "343-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_15",
            "tgt_ix": "343-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_15",
            "tgt_ix": "343-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_14",
            "tgt_ix": "343-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_21",
            "tgt_ix": "343-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_23",
            "tgt_ix": "343-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_24",
            "tgt_ix": "343-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_25",
            "tgt_ix": "343-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_26",
            "tgt_ix": "343-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_27",
            "tgt_ix": "343-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_22",
            "tgt_ix": "343-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_22",
            "tgt_ix": "343-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_22",
            "tgt_ix": "343-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_22",
            "tgt_ix": "343-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_22",
            "tgt_ix": "343-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_22",
            "tgt_ix": "343-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_22",
            "tgt_ix": "343-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_14",
            "tgt_ix": "343-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_28",
            "tgt_ix": "343-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_30",
            "tgt_ix": "343-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_31",
            "tgt_ix": "343-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_32",
            "tgt_ix": "343-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_33",
            "tgt_ix": "343-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_34",
            "tgt_ix": "343-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_35",
            "tgt_ix": "343-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_36",
            "tgt_ix": "343-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_37",
            "tgt_ix": "343-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_0",
            "tgt_ix": "343-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_38",
            "tgt_ix": "343-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_39",
            "tgt_ix": "343-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_39",
            "tgt_ix": "343-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_39",
            "tgt_ix": "343-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_40",
            "tgt_ix": "343-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_42",
            "tgt_ix": "343-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_43",
            "tgt_ix": "343-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_44",
            "tgt_ix": "343-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_45",
            "tgt_ix": "343-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_46",
            "tgt_ix": "343-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_47",
            "tgt_ix": "343-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_48",
            "tgt_ix": "343-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_49",
            "tgt_ix": "343-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_50",
            "tgt_ix": "343-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_39",
            "tgt_ix": "343-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_51",
            "tgt_ix": "343-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_53",
            "tgt_ix": "343-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_52",
            "tgt_ix": "343-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_52",
            "tgt_ix": "343-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_52",
            "tgt_ix": "343-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_39",
            "tgt_ix": "343-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_54",
            "tgt_ix": "343-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_55",
            "tgt_ix": "343-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_55",
            "tgt_ix": "343-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_39",
            "tgt_ix": "343-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_56",
            "tgt_ix": "343-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_57",
            "tgt_ix": "343-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_57",
            "tgt_ix": "343-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_39",
            "tgt_ix": "343-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_58",
            "tgt_ix": "343-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_60",
            "tgt_ix": "343-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_61",
            "tgt_ix": "343-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_59",
            "tgt_ix": "343-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_59",
            "tgt_ix": "343-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_59",
            "tgt_ix": "343-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_59",
            "tgt_ix": "343-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_39",
            "tgt_ix": "343-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_62",
            "tgt_ix": "343-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_64",
            "tgt_ix": "343-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_65",
            "tgt_ix": "343-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_63",
            "tgt_ix": "343-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_63",
            "tgt_ix": "343-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_63",
            "tgt_ix": "343-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_63",
            "tgt_ix": "343-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_0",
            "tgt_ix": "343-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_66",
            "tgt_ix": "343-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_67",
            "tgt_ix": "343-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_67",
            "tgt_ix": "343-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_69",
            "tgt_ix": "343-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_70",
            "tgt_ix": "343-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_68",
            "tgt_ix": "343-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_68",
            "tgt_ix": "343-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_68",
            "tgt_ix": "343-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_68",
            "tgt_ix": "343-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_67",
            "tgt_ix": "343-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_71",
            "tgt_ix": "343-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_73",
            "tgt_ix": "343-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_74",
            "tgt_ix": "343-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_75",
            "tgt_ix": "343-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_72",
            "tgt_ix": "343-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_72",
            "tgt_ix": "343-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_72",
            "tgt_ix": "343-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_72",
            "tgt_ix": "343-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_72",
            "tgt_ix": "343-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_67",
            "tgt_ix": "343-ARR_v2_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_76",
            "tgt_ix": "343-ARR_v2_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_67",
            "tgt_ix": "343-ARR_v2_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_77",
            "tgt_ix": "343-ARR_v2_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_79",
            "tgt_ix": "343-ARR_v2_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_78",
            "tgt_ix": "343-ARR_v2_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_78",
            "tgt_ix": "343-ARR_v2_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_78",
            "tgt_ix": "343-ARR_v2_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_67",
            "tgt_ix": "343-ARR_v2_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_80",
            "tgt_ix": "343-ARR_v2_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_81",
            "tgt_ix": "343-ARR_v2_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_81",
            "tgt_ix": "343-ARR_v2_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_67",
            "tgt_ix": "343-ARR_v2_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_82",
            "tgt_ix": "343-ARR_v2_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_84",
            "tgt_ix": "343-ARR_v2_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_83",
            "tgt_ix": "343-ARR_v2_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_83",
            "tgt_ix": "343-ARR_v2_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_83",
            "tgt_ix": "343-ARR_v2_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_67",
            "tgt_ix": "343-ARR_v2_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_85",
            "tgt_ix": "343-ARR_v2_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_87",
            "tgt_ix": "343-ARR_v2_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_86",
            "tgt_ix": "343-ARR_v2_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_86",
            "tgt_ix": "343-ARR_v2_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_86",
            "tgt_ix": "343-ARR_v2_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_0",
            "tgt_ix": "343-ARR_v2_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_88",
            "tgt_ix": "343-ARR_v2_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_90",
            "tgt_ix": "343-ARR_v2_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_91",
            "tgt_ix": "343-ARR_v2_92",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_89",
            "tgt_ix": "343-ARR_v2_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_89",
            "tgt_ix": "343-ARR_v2_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_89",
            "tgt_ix": "343-ARR_v2_92",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_89",
            "tgt_ix": "343-ARR_v2_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_89",
            "tgt_ix": "343-ARR_v2_93",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_92",
            "tgt_ix": "343-ARR_v2_93",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_93",
            "tgt_ix": "343-ARR_v2_94",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_93",
            "tgt_ix": "343-ARR_v2_94",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_0",
            "tgt_ix": "343-ARR_v2_95",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_94",
            "tgt_ix": "343-ARR_v2_95",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_95",
            "tgt_ix": "343-ARR_v2_96",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_95",
            "tgt_ix": "343-ARR_v2_96",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_95",
            "tgt_ix": "343-ARR_v2_97",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_96",
            "tgt_ix": "343-ARR_v2_97",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_98",
            "tgt_ix": "343-ARR_v2_99",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_99",
            "tgt_ix": "343-ARR_v2_100",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_97",
            "tgt_ix": "343-ARR_v2_98",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_97",
            "tgt_ix": "343-ARR_v2_99",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_97",
            "tgt_ix": "343-ARR_v2_100",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_97",
            "tgt_ix": "343-ARR_v2_98",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v2_0",
            "tgt_ix": "343-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_1",
            "tgt_ix": "343-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_2",
            "tgt_ix": "343-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_2",
            "tgt_ix": "343-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_2",
            "tgt_ix": "343-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_2",
            "tgt_ix": "343-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_3",
            "tgt_ix": "343-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_4",
            "tgt_ix": "343-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_4",
            "tgt_ix": "343-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_4",
            "tgt_ix": "343-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_5",
            "tgt_ix": "343-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_5",
            "tgt_ix": "343-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_5",
            "tgt_ix": "343-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_5",
            "tgt_ix": "343-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_5",
            "tgt_ix": "343-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_6",
            "tgt_ix": "343-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_6",
            "tgt_ix": "343-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_6",
            "tgt_ix": "343-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_7",
            "tgt_ix": "343-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_7",
            "tgt_ix": "343-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_7",
            "tgt_ix": "343-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_7",
            "tgt_ix": "343-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_8",
            "tgt_ix": "343-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_9",
            "tgt_ix": "343-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_9",
            "tgt_ix": "343-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_9",
            "tgt_ix": "343-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_9",
            "tgt_ix": "343-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_9",
            "tgt_ix": "343-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_9",
            "tgt_ix": "343-ARR_v2_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_10",
            "tgt_ix": "343-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_10",
            "tgt_ix": "343-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_10",
            "tgt_ix": "343-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_11@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_11",
            "tgt_ix": "343-ARR_v2_11@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_12",
            "tgt_ix": "343-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_12",
            "tgt_ix": "343-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_12",
            "tgt_ix": "343-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_12",
            "tgt_ix": "343-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_12",
            "tgt_ix": "343-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_12",
            "tgt_ix": "343-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_13",
            "tgt_ix": "343-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_13",
            "tgt_ix": "343-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_13",
            "tgt_ix": "343-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_14",
            "tgt_ix": "343-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_15",
            "tgt_ix": "343-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_16",
            "tgt_ix": "343-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_16",
            "tgt_ix": "343-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_17",
            "tgt_ix": "343-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_18",
            "tgt_ix": "343-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_19",
            "tgt_ix": "343-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_20",
            "tgt_ix": "343-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_21",
            "tgt_ix": "343-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_22",
            "tgt_ix": "343-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_23",
            "tgt_ix": "343-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_23",
            "tgt_ix": "343-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_23",
            "tgt_ix": "343-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_24",
            "tgt_ix": "343-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_25",
            "tgt_ix": "343-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_26",
            "tgt_ix": "343-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_26",
            "tgt_ix": "343-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_27",
            "tgt_ix": "343-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_28",
            "tgt_ix": "343-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_28",
            "tgt_ix": "343-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_28",
            "tgt_ix": "343-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_29",
            "tgt_ix": "343-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_30",
            "tgt_ix": "343-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_31",
            "tgt_ix": "343-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_32",
            "tgt_ix": "343-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_33",
            "tgt_ix": "343-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_34",
            "tgt_ix": "343-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_35",
            "tgt_ix": "343-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_35",
            "tgt_ix": "343-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_36",
            "tgt_ix": "343-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_37",
            "tgt_ix": "343-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_37",
            "tgt_ix": "343-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_37",
            "tgt_ix": "343-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_38",
            "tgt_ix": "343-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_39",
            "tgt_ix": "343-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_40",
            "tgt_ix": "343-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_40",
            "tgt_ix": "343-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_41",
            "tgt_ix": "343-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_42",
            "tgt_ix": "343-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_42",
            "tgt_ix": "343-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_43",
            "tgt_ix": "343-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_44",
            "tgt_ix": "343-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_45",
            "tgt_ix": "343-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_46",
            "tgt_ix": "343-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_47",
            "tgt_ix": "343-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_48",
            "tgt_ix": "343-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_49",
            "tgt_ix": "343-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_50",
            "tgt_ix": "343-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_51",
            "tgt_ix": "343-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_52",
            "tgt_ix": "343-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_53",
            "tgt_ix": "343-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_53",
            "tgt_ix": "343-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_54",
            "tgt_ix": "343-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_55",
            "tgt_ix": "343-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_56",
            "tgt_ix": "343-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_56",
            "tgt_ix": "343-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_56",
            "tgt_ix": "343-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_56",
            "tgt_ix": "343-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_56",
            "tgt_ix": "343-ARR_v2_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_57",
            "tgt_ix": "343-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_58",
            "tgt_ix": "343-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_58",
            "tgt_ix": "343-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_58",
            "tgt_ix": "343-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_59",
            "tgt_ix": "343-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_60",
            "tgt_ix": "343-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_60",
            "tgt_ix": "343-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_60",
            "tgt_ix": "343-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_61",
            "tgt_ix": "343-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_62",
            "tgt_ix": "343-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_62",
            "tgt_ix": "343-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_63",
            "tgt_ix": "343-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_64",
            "tgt_ix": "343-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_65",
            "tgt_ix": "343-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_66",
            "tgt_ix": "343-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_67",
            "tgt_ix": "343-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_68",
            "tgt_ix": "343-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_69",
            "tgt_ix": "343-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_69",
            "tgt_ix": "343-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_70",
            "tgt_ix": "343-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_70",
            "tgt_ix": "343-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_70",
            "tgt_ix": "343-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_70",
            "tgt_ix": "343-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_70",
            "tgt_ix": "343-ARR_v2_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_71",
            "tgt_ix": "343-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_71",
            "tgt_ix": "343-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_72",
            "tgt_ix": "343-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_73",
            "tgt_ix": "343-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_73",
            "tgt_ix": "343-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_73",
            "tgt_ix": "343-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_73",
            "tgt_ix": "343-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_74",
            "tgt_ix": "343-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_74",
            "tgt_ix": "343-ARR_v2_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_74",
            "tgt_ix": "343-ARR_v2_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_74",
            "tgt_ix": "343-ARR_v2_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_74",
            "tgt_ix": "343-ARR_v2_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_74",
            "tgt_ix": "343-ARR_v2_74@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_74",
            "tgt_ix": "343-ARR_v2_74@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_75",
            "tgt_ix": "343-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_76",
            "tgt_ix": "343-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_77",
            "tgt_ix": "343-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_78",
            "tgt_ix": "343-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_79",
            "tgt_ix": "343-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_79",
            "tgt_ix": "343-ARR_v2_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_80",
            "tgt_ix": "343-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_81",
            "tgt_ix": "343-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_82",
            "tgt_ix": "343-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_82",
            "tgt_ix": "343-ARR_v2_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_83",
            "tgt_ix": "343-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_84",
            "tgt_ix": "343-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_84",
            "tgt_ix": "343-ARR_v2_84@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_85",
            "tgt_ix": "343-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_85",
            "tgt_ix": "343-ARR_v2_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_86",
            "tgt_ix": "343-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_87",
            "tgt_ix": "343-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_87",
            "tgt_ix": "343-ARR_v2_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_88",
            "tgt_ix": "343-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_88",
            "tgt_ix": "343-ARR_v2_88@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_88",
            "tgt_ix": "343-ARR_v2_88@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_89",
            "tgt_ix": "343-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_90",
            "tgt_ix": "343-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_90",
            "tgt_ix": "343-ARR_v2_90@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_90",
            "tgt_ix": "343-ARR_v2_90@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_90",
            "tgt_ix": "343-ARR_v2_90@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_91",
            "tgt_ix": "343-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_91",
            "tgt_ix": "343-ARR_v2_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_92",
            "tgt_ix": "343-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_92",
            "tgt_ix": "343-ARR_v2_92@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_92",
            "tgt_ix": "343-ARR_v2_92@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_92",
            "tgt_ix": "343-ARR_v2_92@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_93",
            "tgt_ix": "343-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_94",
            "tgt_ix": "343-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_94",
            "tgt_ix": "343-ARR_v2_94@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_94",
            "tgt_ix": "343-ARR_v2_94@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_95",
            "tgt_ix": "343-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_96",
            "tgt_ix": "343-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_96",
            "tgt_ix": "343-ARR_v2_96@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_96",
            "tgt_ix": "343-ARR_v2_96@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_96",
            "tgt_ix": "343-ARR_v2_96@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_96",
            "tgt_ix": "343-ARR_v2_96@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_97",
            "tgt_ix": "343-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_98",
            "tgt_ix": "343-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_98",
            "tgt_ix": "343-ARR_v2_98@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_99",
            "tgt_ix": "343-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_99",
            "tgt_ix": "343-ARR_v2_99@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_100",
            "tgt_ix": "343-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_100",
            "tgt_ix": "343-ARR_v2_100@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_100",
            "tgt_ix": "343-ARR_v2_100@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_100",
            "tgt_ix": "343-ARR_v2_100@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_100",
            "tgt_ix": "343-ARR_v2_100@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_101",
            "tgt_ix": "343-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_102",
            "tgt_ix": "343-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_103",
            "tgt_ix": "343-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_104",
            "tgt_ix": "343-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_105",
            "tgt_ix": "343-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_106",
            "tgt_ix": "343-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_107",
            "tgt_ix": "343-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_108",
            "tgt_ix": "343-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_109",
            "tgt_ix": "343-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_110",
            "tgt_ix": "343-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_111",
            "tgt_ix": "343-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_112",
            "tgt_ix": "343-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_113",
            "tgt_ix": "343-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_114",
            "tgt_ix": "343-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_115",
            "tgt_ix": "343-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_116",
            "tgt_ix": "343-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_117",
            "tgt_ix": "343-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_118",
            "tgt_ix": "343-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_119",
            "tgt_ix": "343-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_120",
            "tgt_ix": "343-ARR_v2_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_121",
            "tgt_ix": "343-ARR_v2_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_122",
            "tgt_ix": "343-ARR_v2_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_123",
            "tgt_ix": "343-ARR_v2_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_124",
            "tgt_ix": "343-ARR_v2_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_125",
            "tgt_ix": "343-ARR_v2_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_126",
            "tgt_ix": "343-ARR_v2_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_127",
            "tgt_ix": "343-ARR_v2_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_128",
            "tgt_ix": "343-ARR_v2_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_129",
            "tgt_ix": "343-ARR_v2_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_130",
            "tgt_ix": "343-ARR_v2_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_131",
            "tgt_ix": "343-ARR_v2_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_132",
            "tgt_ix": "343-ARR_v2_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_133",
            "tgt_ix": "343-ARR_v2_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_134",
            "tgt_ix": "343-ARR_v2_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_135",
            "tgt_ix": "343-ARR_v2_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_136",
            "tgt_ix": "343-ARR_v2_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_137",
            "tgt_ix": "343-ARR_v2_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_138",
            "tgt_ix": "343-ARR_v2_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_139",
            "tgt_ix": "343-ARR_v2_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_140",
            "tgt_ix": "343-ARR_v2_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_141",
            "tgt_ix": "343-ARR_v2_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_142",
            "tgt_ix": "343-ARR_v2_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_143",
            "tgt_ix": "343-ARR_v2_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_144",
            "tgt_ix": "343-ARR_v2_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_145",
            "tgt_ix": "343-ARR_v2_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_146",
            "tgt_ix": "343-ARR_v2_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_147",
            "tgt_ix": "343-ARR_v2_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_148",
            "tgt_ix": "343-ARR_v2_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_149",
            "tgt_ix": "343-ARR_v2_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_150",
            "tgt_ix": "343-ARR_v2_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_151",
            "tgt_ix": "343-ARR_v2_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_152",
            "tgt_ix": "343-ARR_v2_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_153",
            "tgt_ix": "343-ARR_v2_153@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_154",
            "tgt_ix": "343-ARR_v2_154@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_155",
            "tgt_ix": "343-ARR_v2_155@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_156",
            "tgt_ix": "343-ARR_v2_156@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_157",
            "tgt_ix": "343-ARR_v2_157@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_158",
            "tgt_ix": "343-ARR_v2_158@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v2_159",
            "tgt_ix": "343-ARR_v2_159@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 851,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "343-ARR",
        "version": 2
    }
}