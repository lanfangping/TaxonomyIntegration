{
    "nodes": [
        {
            "ix": "343-ARR_v1_0",
            "content": "Interactive Symbol Grounding with Complex Referential Expressions",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_2",
            "content": "We present a procedure for learning to ground symbols from a sequence of stimuli consisting of an arbitrarily complex noun phrase (e.g. \"all but one green square above both red circles.\") and its designation in the visual scene. Our distinctive approach combines: a) lazy fewshot learning to relate open-class words like green and above to their visual percepts; and b) symbolic reasoning with closed-class word categories like quantifiers and negation. We use this combination to estimate new training examples for grounding symbols that occur within a noun phrase but aren't designated by that noun phase (e.g, red in the above example), thereby potentially gaining data efficiency. We evaluate the approach in a visual reference resolution task, in which the learner starts out unaware of concepts that are part of the domain model and how they relate to visual percepts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "343-ARR_v1_4",
            "content": "Robotic tasks in unstructured (e.g., Stuckler et al. (2012)) or semi-structured (e.g., Correll et al. (2018)) environments involve rearranging objects (Batra et al., 2020) to reach the goal. Such tasks can be solved using task and motion planning if the robot has a full and accurate domain model (Garrett et al., 2021). But there are many scenarios in which the robot must adapt its behaviour to novel and unforeseen objects and attributes that are introduced into the environment after deployment. These types of scenarios can be tackled using interactive task learning (ITL) (see Laird et al. (2017) for a survey): a learner solves the novel task via evidence from its own actions and reactive guidance from a teacher. This paper focuses on symbol grounding (Harnad, 1999) in the context of ITL (Matuszek, 2018): the learner must use the teacher's embodied natural language utterance and its context to learn a mapping from linguistic expressions to their denotations, given the visual percepts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_5",
            "content": "There are two challenges in learning symbol grounding models (grounders) in ITL. Firstly, in contrast to many contemporary grounders (e.g., Ye et al. (2019); Datta et al. (2019)), ITL requires incremental learning because the data samples are acquired piecemeal via an extended interaction, and these samples should influence planning as and when they occur. Secondly, previous work on learning grounders has been mainly focused on using simple expressions of bare nouns (e.g. \"block\") or very short phrases (e.g., \"blue sphere\", \"block behind cylinder\") (e.g., Hristov et al. (2018Hristov et al. ( , 2019). But it is highly natural for the teacher's language to be more complex, creating the possibility that novel symbols-neologisms-are introduced in a context where their denotation is not designated by the teacher (e.g., blue and square when a teacher points to the denotation of the noun phrase \"a red circle behind both blue squares\").",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_6",
            "content": "We tackle both of these challenges while aiming to extract knowledge that is \"actionable\": the acquired knowledge should improve both the agent's understanding of the domain and its estimates of the current state, a necessary condition to successful planning. Current grounding approaches miss many learning opportunities that complex referring expressions (REs) like \"all but one red circle above both blue crosses\" afford: e.g., this RE not only entails that its referents all satisfy the properties red and circle, but also that there are two (further) objects that both satisfy blue and star, that these are above the designated objects, and everything else in the domain is either not blue or not a star (thanks to the meaning of both). Thus, a single RE and its designation can in principle be used to gather several (noisy) training exemplars (both positive and negative) for several symbols at once, even if these symbols have not been designated by the RE. The main aim of this paper is to explore the effects of this facility on data efficiency and accuracy in learning grounders.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_7",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "343-ARR_v1_8",
            "content": "Symbol Grounding. Contemporary grounders extensively utilize batch learning (e.g. Shridhar and Hsu (2018)). Yet, ITL requires incremental learning because without it the teacher guidance cannot influence the learner's inferences about plans as and when the advice is given. Further, many grounders assume that the learner starts out with a complete and accurate conceptualisation of the domain using pre-defined visual features and a known vocabulary (Kennington et al., 2015;Kennington and Schlangen, 2017;Wang et al., 2017). In ITL, both of these assumptions are unrealistic; therefore in this paper we explore models for which these assumptions don't apply. Finally, in contrast to all prior grounders, we support incremental learning when the training exemplars feature REs that are linguistically complex: e.g., \"two red circles that aren't to the right of both green squares\".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_9",
            "content": "Representation Learning. Models for jointly learning a representation for vision and language utilize either explicit alignment via bounding boxes or instance segmentation (Lu et al., 2019;Chen et al., 2020;Tan and Bansal, 2019;Kamath et al., 2021;Yu et al., 2021), or a large-volume of weakly labeled data in the form of image-caption pairs (Radford et al., 2021). These models rely on offline learning with large datasets. This work, on the other hand, explores how to incrementally extract knowledge in a few-shot manner, using sequentially observed evidence that consists of weakly-labelled annotations, including neologisms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_10",
            "content": "Grounded Language Acquisition. This task is commonly realized as grounded grammar induction from image-caption pairs (Shi et al., 2019;Zhao and Titov, 2020), or as (neural) semantic parsers learning from a reward signal (Williams, 1992) in visual question answering Yi et al., 2018) or in planning (Azaria et al., 2016;Wang et al., 2016Wang et al., , 2017Karamcheti et al., 2020). In these scenarios, the primary objective is to learn mappings from linguistic expressions to logical forms, which in turn get associated with certain visual percepts during the learning process. This paper does not tackle learning logical forms. Instead, we obtain them from an existing broad-coverage grammar and focus on exploiting the logical consequences of those logical forms during symbol grounding-i.e., our focus is to utilise the interpretation of logical forms, and in particular the truth functional meanings of close-class words like quantifiers and negation, to inform the learning of mappings from (open-class) symbols like red to their denotations, given the visual percepts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_11",
            "content": "Visual Questions Answering (VQA) is a task of answering free-form questions about an image (Antol et al., 2015). VQA has reached impressive performance in recent years (Fukui et al., 2016;, yet VQA models struggle with out-of distribution generalization for new types of questions, requiring multi-step reasoning, with analysis revealing that they often rely on shortcuts (Jiang and Bansal, 2019;Subramanian et al., , 2020. Grounded VQA models like (Yi et al., 2018) and Bogin et al. (2021) tackle these shortcomings by learning to ground parts of the question and then learning to compose those parts via the question's syntax to compute the answer. They thus estimate denotations of linguistic parts that are not denoted by the answer to the question. By doing so, these models achieve out-of-distribution generalization for novel questions. However, these models lack the ITL's requirement for incremental learning: model training relies on batch learning. Furthermore, while their performance is impressive, error analysis shows that it tends to make mistakes on sentences containing logical concepts like quantifiers and negation (e.g. Bogin et al. (2021) Figure 9 shows that the determiner most incorrectly denotes an arbitrary subset of entities). Our view is that there is little benefit in trying to learn to ground these concepts as they are domain independent and can be interpreted using formal semantics. In our experiments, we are testing the extent to which knowing and reasoning with the logical meanings of these symbols helps incremental grounding, and in particular estimating denotations of symbols within an RE that aren't designated by that RE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_12",
            "content": "Visual Reference Resolution. In previous experimental setups, it is often assumed that there is a unique referent in the visual scene for the given RE in the test phase (e.g., Kazemzadeh et al. (2014); Whitney et al. (2016)). We aim to cope with situations where the RE has multiple referents: identifying all the referents that satisfy an RE enables efficient planning, because it affords free choice when executing certain commands-e.g., \"move a circle to the left.\" when there is more than one circle affords choosing a control policy so that resources are optimized.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_13",
            "content": "Background",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "343-ARR_v1_14",
            "content": "Formal Semantics of Natural Language",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "343-ARR_v1_15",
            "content": "The language L of predicate logic with generalized quantifiers (Barwise and Cooper, 1981;van Benthem, 1984) is a canonical formal meaning representation for natural languages. L-sentences \u03c6 are constructed recursively from predicates P , terms T (i.e., variables V and constants C), logical connectives O = {\u00ac, \u2227, \u2228, \u2192} and quantifiers Q (see Table 1 column 1):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_16",
            "content": "\u03c6 ::= p(t 1 , . . . , t n ) \u2261 p(t n ) |(\u00ac\u03c6)|(\u03c6 1 \u2227 \u03c6 2 )|(\u03c6 1 \u2228 \u03c6 2 )|(\u03c6 1 \u2192 \u03c6 2 ) |(Qx(\u03c6 1 , \u03c6 2 ))",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_17",
            "content": "where p is an n-place predicate, t i \u2208 T are terms, Q \u2208 Q is a quantifier, and x \u2208 V is a variable (in Qx(\u03c6 1 , \u03c6 2 ), \u03c6 1 is the restrictor and \u03c6 2 the body). We also introduce \u03bb-expressions of the form \u03bbx.\u03c6, where x \u2208 V is free or absent in \u03c6.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_18",
            "content": "Model-theoretic Interpretation",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "343-ARR_v1_19",
            "content": "L-sentences are interpreted using a domain model M = (E, I) consisting of a set of entities E (universe of discourse), and an extension function I that maps non-logical symbols P \u222a C to denotations (tuples of entities). For convenience, we assume I : C \u2192 E is one-to-one. Variables are interpreted via an assignment function g : V \u2192 E.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_20",
            "content": "The interpretation function \u2022 M,g specifies the semantic value of well-formed expressions of L:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_21",
            "content": "a M,g = I(a) if a \u2208 P \u222a C g(a) if a \u2208 V p(t n ) M,g = 1 iff ( t n M,g ) \u2208 p M,g \u00ac\u03c6 M,g = 1 iff \u03c6 M,g = 0 \u03c6 \u2227 \u03c8 M,g = 1 iff \u03c6 M,g = 1 and \u03c8 M,g = 1 \u03c6 \u2228 \u03c8 M,g = 1 iff \u03c6 M,g = 1 or \u03c8 M,g = 1 \u03c6 \u2192 \u03c8 M,g = 1 iff \u03c6 M,g = 0 or \u03c8 M,g = 1 \u03bbx.\u03c6 M,g = {e \u2208 E : \u03c6 M,g[x/e] = 1} Qx(\u03c6 1 , \u03c6 2 ) M,g = Q( \u03bb.x\u03c6 1 M,g , \u03bb.x\u03c6 2 M,g )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_22",
            "content": "where g[x/e] is just like g, except g[x/e](x) = e and Q is a specific relation between the restrictor \u03bbx.\u03c6 1 M,g and body \u03bbx.\u03c6 2 M,g , as defined in Table 1 column 3. \u2022 M,g is directly related to satisfiability for L-sentences:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_23",
            "content": "M, g |= \u03c6 iff \u03c6 M,g = 1 M |= \u03c6 iff \u03c6 M = 1",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_24",
            "content": "where \u03c6 M = 1 iff \u03c6 M,g = 1 for all g. Further, if x is the only free variable in \u03c6, then \u03bbx.\u03c6 M,g = \u03bbx.\u03c6 M,g for all g, g ; so without a loss of generality, this is expressed as \u03bbx.\u03c6 M . 1 If all variables in Qx(\u03c6, \u03c8) are bound by quantifiers, then this L-sentence is true iff Q is true for all g. Some quantifiers, like \"both\" in English, 2 are presupposition triggers: a statement \"two blocks are blue\" is different from \"both blocks are blue\" in that the latter is true only if there are exactly two individuals that are blocks. We've adopted a Russellian interpretation (Russell, 1917) of these in Table 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_25",
            "content": "Logical Forms of Referential Expressions",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "343-ARR_v1_26",
            "content": "We now define the logical forms of REs and their interpretations with respect to a domain model M. Noun phrases like \"a block\" are represented as _a_qx.block(x) . More generally, let Qx.\u03c6 be the logical form of an RE, where Q \u2208 Q and \u03c6 is an L-sentence with x \u2208 V being the only free variable in \u03c6. The referents Qx.\u03c6 M of this logical form with respect to M are computed as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_27",
            "content": "Qx.\u03c6 M = Q \u03c0(M,\u03c6,x)(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_28",
            "content": "where \u03c0(M, \u03c6, x) is an M-projection, giving a new domain model M with entities E = E \u2229 \u03bb.x\u03c6 M and Q M is a quantifier referent-a quantifier-specific subset of the power set of E. Table 1 column 4 gives the list of quantifier referents.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_29",
            "content": "To illustrate, consider the domain model where:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_30",
            "content": "E = {a, b, c, d, f } I(cat) = {a, b} I(dog) = {c, d, f } I(bit) = {(c, a), (c, b), (d, b), (f, a), (f, b)}",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_31",
            "content": "The RE \"a dog that bit both cats\" has logical form _a_qx._both_qy.(cat(y), dog(x) \u2227 bit(x, y)) . By Equation 1, its referent is:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_32",
            "content": "_a_q \u03c0(M,_both_qy.(cat(y),dog(x)\u2227bit(x,y)),x)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_33",
            "content": "The semantic value of the \u03bb-expression formed from this RE is a set of entities e \u2208 E for which the following quantifier condition is true: both_q(R, B) where R = \u03bb.y.cat(y) M,g[x/e] and B = \u03bb.y.dog(x) \u2227 bit(x, y))",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_34",
            "content": ". quantifiers Q surface form condition Q(R, B) referent Q M _exactly_n_q exactly n |R \u2229 B| = n {A \u2286 E : |A| = n} _at_most_n_q at most n |R \u2229 B| \u2264 n {A \u2286 E : |A| \u2264 n} _at_least_n_q at least n |R \u2229 B| \u2265 n {A \u2286 E : |A| \u2265 n} _a_q a/an |R \u2229 B| = n {A \u2286 E : |A| \u2264 1} _every_q all/every |R \u2229 B| = |R| {A \u2286 E : |A| = |E|} _the_n_q the n |R \u2229 B| = n \u2227 |R| = n {A \u2286 E : |A| = |E| \u2227 |E| = n} _both_q both |R \u2229 B| = 2 \u2227 |R| = 2 {A \u2286 E : |A| = |E| \u2227 |E| = 2} _all_but_n_q all but n |R \u2229 B| = |R| \u2212 n {A \u2286 E : |A| = |E| \u2212 n \u2227 |E| \u2265 n} _n_of_the_m_q n of the m |R \u2229 B| = n \u2227 |R| = m {A \u2286 E : |A| = n \u2227 |E| = m}",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_35",
            "content": "Table 1: Quantifiers (column 1), their surface forms (column 2), condition Q between the restrictor R and body denotations B, used to compute a semantic value for L-sentences of the form Qx(\u03c6, \u03c8) (column 3); and quantifier referents Q M used to compute references of the logical form of REs (column 4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_36",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "343-ARR_v1_37",
            "content": "Below we present the procedure of interactive grounding with referential expressions (IGRE). The overall framework is given in Figure 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_38",
            "content": "Grounder",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "343-ARR_v1_39",
            "content": "Matching networks (Vinyals et al., 2016) are an extension of the k nearest-neighbour algorithm (Fix and Hodges, 1989) and they are usable as a fast fewshot grounder in the ITL setting (Cano Sant\u00edn et al., 2020). For predicates P n \u2286 P of the same arity n, a grounder \u0398 n is parameterized by a support set S n = {(x n i , y n i )} Kn i=1 , consisting of K n pairs of feature vectors x n i \u2208 R dn for denotations e n \u2208 E n and concept vectors y n i \u2208 [0, 1] |Pn| . In y n , the dimension z corresponds the predicate p z \u2208 P n and its value is the probability that p z (e n ) M,g = 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_40",
            "content": "Concept vectors have a one-to-one correspondence with the domain model M.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_41",
            "content": "Given a feature vector x n for a denotation e n \u2208 E n , \u0398 n predicts the concept vector \u0177n , using the following inference rule:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_42",
            "content": "\u0398 n (x n , S n ) = k i=1 \u03b1 n (x n i , x n ; S n )y n i",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_43",
            "content": "where \u03b1 n is an attention kernel:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_44",
            "content": "\u03b1 n (x n i , x n ; S n ) = exp (f n (x n i ) \u2022 h n (x n )) k j=1 exp (f n (x n j ) \u2022 h n (x n )) f n (x n ) = ReLU(w n \u2022 x n + b n ) ||ReLU(w n \u2022 x n + b n )|| 2 h n (x) = ReLU(v n \u2022 x n + c n ) ||ReLU(v n \u2022 x n + c n )|| 2 ReLU(a) = max (0, a) with learnable parameters \u03b8 n = {w n , v n , b n , c n }, and S n is k = 3 nearest exemplars to x n from S n : S n = {(x n i , y n i ) \u2208 S n : x n i \u2208 V(k, x n , S n )} where V(k, x n , S n ) is a set of k nearest feature vectors.",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_45",
            "content": "Batch Learning",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "343-ARR_v1_46",
            "content": "Given S n , \u03b8 n can be estimated either using batch learning, performed offline, or-when S n is smallin real time, as outlined by Cano Sant\u00edn et al. (2020). In our scenario, we learn in real time by minimizing binary cross-entropy between the ground-truth y n and predicted \u0177n concept vectors:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_47",
            "content": "L(y n , \u0177n ) = \u2212 |Pn| z=1 l(y z , \u0177z ) l(y n i , \u0177n i ) = y n i log(\u0177 n i ) + (1 \u2212 y n i ) log(1 \u2212 \u0177n i )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_48",
            "content": "Incremental Learning",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "343-ARR_v1_49",
            "content": "S n gets augmented whenever the teacher provides feedback in the form of an RE-designation pair. This feedback provides two types of information: certain information C n in the form of denotationsymbol-semantic value triples (e n , p z , y nz ), corresponding to symbols and entities designated by the RE; and noisy information N n , corresponding to denotation-symbol-semantic value estimate triples (e n , p z , \u1ef9nz ), which are acquired from the symbols that are part of the RE and its referent inferred via (uncertain) reasoning. E.g., the RE \"a circle below a square.\", entails that its designation e \u2208 E is a circle and so (e, circle, 1) is added to C n . But it also entails there exists an entity which is a square that is not designated by the RE, but rather this entity is in the below relation with the designated entity. If the grounder is sufficiently confident about the referent for square, then the corresponding triple is added to N n .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_50",
            "content": "Acquiring Observations and Symbols",
            "ntype": "title",
            "meta": {
                "section": "4.3.1"
            }
        },
        {
            "ix": "343-ARR_v1_51",
            "content": "When the learner first observes its visual sceneand so the teacher has not expressed any concepts, and so the learner is currently unaware of all concepts-the noisy support set N n is populated with (e n , p z , 0.5) (0.5 is a default semantic value) for all e n in the scene and for all known n-place predicates. As the teacher provides feedback, if a new concept gets introduced with a neologism p * that the teacher utters, N n gets populated with (e n , p * , 0.5) for all denotations observed so far in the interaction. During interaction, each teacher's utterance, corresponding to an RE-designation pair, adds elements to C n (for designated symbols) and triggers updates to the N n elements for all entities in the current visual scene, as we'll now describe.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_52",
            "content": "Integrating Teacher's Feedback",
            "ntype": "title",
            "meta": {
                "section": "4.3.2"
            }
        },
        {
            "ix": "343-ARR_v1_53",
            "content": "N n elements are interactively updated using an incrementally built domain-level theory \u2206, which is the conjunction of L-sentences that are built from the logical forms of the REs that the teacher has uttered so far and their designations. To compute the beliefs about semantic values, given \u2206, we model the semantic value of L-sentences of the form p(t n ), in which t n are all constants, as a random variable with Bernoulli's distribution B. Thus a distribution over the possible domain models can be estimated using (propositional) model counting MC (Valiant, 1979), which maps each L-sentence to the number of domain models satisfying it. In this way, the semantic value of any proposition can be estimated as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_54",
            "content": "\u1ef9nz = MC(pz(e n )\u2227\u2206) MC(\u2206)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_55",
            "content": "if MC(\u2206) = 0 0.5 otherwise MC can be computed exactly or approximately (Samer and Szeider, 2010). (in our experiments we are using the ADDMC (Dudek et al., 2020) weighted model counter with weights set to 0.5).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_56",
            "content": "Building the Support Set",
            "ntype": "title",
            "meta": {
                "section": "4.3.3"
            }
        },
        {
            "ix": "343-ARR_v1_57",
            "content": "Concept vectors for S n are built using information in C n and N n : namely each denotation e n gets associated with its feature vector x n , and the zdimension of y corresponding to predicate p z \u2208 P n is computed as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_58",
            "content": "y z = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 y nz if (e n , p z , y nz ) \u2208 C n \u1ef9nz if (e n , p z , \u1ef9nz ) \u2208 N n \u2227 H[B(\u1ef9 nz )] \u2264 \u03c4 n 0.5 otherwise",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_59",
            "content": "where H[P] is the entropy of the probability distribution P, and \u03c4 n is a hyperparameter for the confidence threshold for adding noisy exemplars: in our case, it's set to 0.6 for predicates of all arities.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_60",
            "content": "5 Experiments",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_61",
            "content": "Task: Visual Reference Resolution",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "343-ARR_v1_62",
            "content": "To evaluate IGRE, we use a task of visual reference resolution: given a visual scene (an image) with localized entities (bounding boxes) and an RE, the grounder must estimate all its referents, as defined in \u00a73.3. The model learns its task by observing an image accompanied by a sequence of REs, with each RE paired with its designation in the image.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_63",
            "content": "We measure the performance of IGRE on the task after each observed RE and its designation. Performance is measured using the precision P, recall R, and F1 score F1 on the test set between: 1) estimated vs. ground-truth domain models, formed from the concept vectors (intrinsic evaluation) and 2) estimated vs. ground-truth referents for the RE (extrinsic evaluation). These metrics are calculated only for those symbols/concepts that the teacher has mentioned so far (since the system is unaware that the remaining concepts exist). To obtain reliable results, we repeat the experiment 10 times: i.e., 10 different visual scenes, with a sequence of 5 different teacher utterances in each scene. We record in \u00a76 the average precision, recall and f-scores over those 10 trials.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_64",
            "content": "Data: ShapeWorld",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "343-ARR_v1_65",
            "content": "To generate training and test sets, we construct ShapeWorld (Kuhnle and Copestake, 2017) domain models, each consisting of 3-12 entities, synthesized visual scenes X (64x64 pixels), and 5 REs. Each domain model is describable using 7 shape symbols S1 (square, circle, triangle, pentagon, cross, ellipse, semicircle), 6 colour symbols C1 (red, green, blue, yellow, magenta, cyan) and 4 spatial relationships symbols R2 (left, right, above, below). 3 In scene synthesis, the image is created from the domain model, with variation on the In interaction, the learner observes an RE, which is parsed to logical form ( \u00a75.3.2) and interpreted with respect of the extracted feature vectors for denotations ( \u00a75.3.1) to perform reference resolution ( \u00a73.3) respect to the estimated domain model M. In case of teacher feedback, RE and its designation is observed. This is used to build the L-sentence that is added to \u2206 to update beliefs about the underlying concept vectors ( \u00a74.3.2), which in turn are used to update the support set ( \u00a74.3.3), used as parameters for the grounder ( \u00a74.1). Elements in blue are pre-defined elements of IGRE while elements in red are learned through interaction.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_66",
            "content": "hue of the colour category, and variation on the size, position, rotation, and distortion of the shapes. Note that the colour categories are not mutually exclusive-e.g., there are RGB values that count as both red and magenta.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_67",
            "content": "To generate REs, we sample Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) graph templates, processed using ACE (generation mode) 4 and the English Resource Grammar (ERG) (Flickinger, 2000). Generated REs are evaluated with respect to the domain model to guarantee an existing referent. In total we generated 30 such domain models for training and 10 for testing. The data statistics for the training set is given in Table 2 for the general categories of symbols, where certain means that the designation is denoted by the symbol in the RE, and noisy means that the symbol is a part of the RE but is not designated by it. Note that the first argument to the spatial relations R2 is always denoted by the designation while its second argument is not. Note also there is high variance in the frequencies among the individual symbols. For instance, blue occurs 27 and 28 times in certain vs. noisy positions respectively, while triangle occurs 7 and 12 times respectively. Category C n candidates N n candidates C1 18.67 \u00b1 5.39 19.83 \u00b1 5.04 S1 14.67 \u00b1 3.98 16.50 \u00b1 5.32 R2 0 37.75 \u00b1 6.75",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_68",
            "content": "Table 2: Average symbol counts per word for colours (C1), shapes (S1), and spatial relationships (R2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_69",
            "content": "Implementation Details",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "343-ARR_v1_70",
            "content": "Feature Extraction",
            "ntype": "title",
            "meta": {
                "section": "5.3.1"
            }
        },
        {
            "ix": "343-ARR_v1_71",
            "content": "To extract visual features for denotations e n \u2208 E n , we utilize bounding boxes b = [x lef t , x right , y top , y bottom ] for each entity e \u2208 E in the visual scene by localizing them (cropping) and extracting the visual features using a pre-trained visual feature encoder (in our case, DenseNet161 (Huang et al., 2017)). Additionally, for the feature vector, we add entity's bounding box coordinates for spatial information, lost in the localization process:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_72",
            "content": "x n = Concat({[DenseNet161(X[b i ]], b i )} n i=1 )",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_73",
            "content": "Grammar-based Semantic Parsing",
            "ntype": "title",
            "meta": {
                "section": "5.3.2"
            }
        },
        {
            "ix": "343-ARR_v1_74",
            "content": "To parse REs to their logical forms, we use the English Resource Grammar (ERG) and ACE (parsing mode) to produce a representation in minimal recursion semantics (MRS) (Copestake et al., 1997), which we then simplify via hand-written rules (e.g., removing event arguments from predicate symbols corresponding to adjectives and prepositions). Underspecification of the MRS was resolved using UTOOL (Koller and Thater, 2005) and the final logical form was selected based on the linear order of scope-bearing elements (quantifiers and negation): e.g. for the RE \"every circle above a square\", _every_q outscopes _a_q.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_75",
            "content": "Axioms for R2",
            "ntype": "title",
            "meta": {
                "section": "5.3.3"
            }
        },
        {
            "ix": "343-ARR_v1_76",
            "content": "For |E| entities, there are |E| 2 denotations to consider for each 2-place predicate-a larger search space compared to |E| denotations for 1-place predicate. Moreover, these predicates can only be acquired from the noisy component N n because the second argument to the relation is always latent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_77",
            "content": "To aid the learning process for R2, whenever a new symbol R \u2208 R2 is observed, domainlevel axioms are added to \u2206 for it, making it irrreflexive: \u2200x.\u00acR(x, x) (an entity cannot be in a spatial relationship to itself) and asymmetric: \u2200x, y.R(x, y) \u2192 \u00acR(y, x) (reflecting the fact that entities in spatial relations take different roles (Miller and Johnson-Laird, 1976)). These axioms reduce the number of possible denotations for R2 symbols from |E| 2 to |E| 2 2 \u2212 |E|.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_78",
            "content": "Baselines",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "343-ARR_v1_79",
            "content": "To test the benefit of using noisy training exemplars N n from the oblique symbols in the REs-in other words, those symbols that are a part of the RE but not designated by it-we implemented a HEAD grounder baseline, which uses information only from C n . That HEAD uses only symbol-designation pairs that are acquired when the symbol denotes the referent (in our case, that's the head noun in the RE and its pre-head modifier, if it exists).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_80",
            "content": "To test the the benefit of using the precise formal semantic meanings of logical symbols (i.e., quantifiers and negation), we implemented an EXIST grounder baseline. This utilizes the information from the symbols in the oblique positions, but it does not utilize the precise symbolic interpretation of the logical symbols, instead simplifying the logical form of the RE by replacing all quantifiers with the existential _a_q and removing negation (e.g., \"every cross on the left of the one circle\" is equivalent to \"a cross on the left of a circle\"). This baseline preserves the basic linguistic structure of the formal semantic representation of the RE, but not its truth-functional interpretation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_81",
            "content": "Results and Discussion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "343-ARR_v1_82",
            "content": "Figure 2 shows the evolution of the performance of the IGRE grounder and the two baselines on the test set, as it gets exposed to more information (i.e., RE-designation pairs) over time. In the intrinsic evaluation (domain model prediction), there is no significant difference between the three grounders considered. Yet, for extrinsic evaluation (reference resolution), we observe that IGRE outperforms the HEAD and EXISTS baselines over time (both a steeper and a smoother curve). By the end of the interaction, a t-test shows significant differences in IGRE's performance compared with both baselines (p-value of 0.01).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_83",
            "content": "Table 3 shows the best performance that each grounder achieved over time. When analysing their performance on particular categories, we observe that C1 and S1 are equally hard to learn for grounders while R2 is easier.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_84",
            "content": "Error Analysis",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "343-ARR_v1_85",
            "content": "The HEAD and EXISTS baselines never acquire negative exemplars: e.g., information that a particular individual is not red. Figure 2 shows that this severely impacts their performance, and error analysis showed that in some experiment runs it leads to mode-collapse, with all denotations predicted to be the extensions of all symbols. On the other hand, IGRE is able to acquire and use negative examples from the truth functional meanings of the logical symbols, specifically from: (a) negation (\"not\"); (b) the presupposition triggers\"the N \", \"N of the M \", and \"all but N \" where N , and M are numbers and \"both\"; and (c) the use of \"every\" when it modifies the head noun.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_86",
            "content": "Conclusions",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "343-ARR_v1_87",
            "content": "In this work, we presented IGRE-a grounder that supports incremental learning of the mapping from symbols to visual features whenever the teacher presents a linguistically complex RE and its designation(s) in the visual scene. The grounder starts the learning process with no conceptualisation of the domain model, and so the learner must revise its hypothesis space of possible domain models as and when the teacher introduces new and unforeseen concepts via neologisms. We showed how exploiting the model-theoretic interpretation of the formal semantic representations of REs, and in particular the truth conditions of 'logical' words like quantifiers and negation, can inform the acquisition of noisy training exemplars that in turn guide learning-IGRE reasons about the likely denotations of symbols within an RE that aren't designated by that RE, and when sufficiently confident it exploits them to update its grounding model. We showed that: 1) this grounding approach is more data efficient then a model that omits such observations and reasoning, using only the designated symbols; and 2) it is beneficial to exploit the logical consequences of the logical symbols, to gain even more data efficiency and training stability. In both cases, there was much to be gained from such reasoning because in contrast to the baselines, it contributes to acquiring negative exemplars: in other words, objects that get associated with not being red, for example.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_88",
            "content": "Future Work",
            "ntype": "title",
            "meta": {
                "section": "7.1"
            }
        },
        {
            "ix": "343-ARR_v1_89",
            "content": "IGRE uses a single source of data augmentation by acquiring noisy exemplars from symbols in oblique positions. Further and parallel data gains may be obtained by exploring semi-supervised learning methods (Yarowsky, 1995;Delalleau et al., 2005).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_90",
            "content": "In this work, converting L-sentences to conjunctive normal form, which is an NP-hard problem, was a computational bottleneck. Future work needs to address this by either considering lifted inference methods (e.g., den Broeck et al. ( 2011)) or defining model counters that use L-sentences directly.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_91",
            "content": "Finally, the purpose of IGRE is to aid ITL: i.e., the (incremental) updates to beliefs about symbol grounding should enhance learning to solve domain-level planning problems. Future work needs to address this by using IGRE to learn planning tasks where the learner has the physical ability to execute certain actions but starts out unaware of domain concepts that define the goal and are critical to task success. The learner must not only use IGRE to interpret the teacher's feedback, but also learn decision making strategies, both on what to say (or ask) the teacher in their extended dialogue and what actions to perform in the environment. Furthermore, the static formal semantics that we used here should be replaced with a dynamic semantics (e.g., Groenendijk and Stokhof (1991); van der Sandt (1992); Asher and Lascarides (2003)), to account for how contextual salience influences truth and reference in dialogue. Following Batra et al. (2020), we plan to test the benefits of IGRE within a system that learns to solve planning problems that focus on rearrangement tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "343-ARR_v1_92",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Zitnick, Devi Parikh, VQA: visual question answering, 2015-12-07, 2015 IEEE International Conference on Computer Vision, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Stanislaw Antol",
                    "Aishwarya Agrawal",
                    "Jiasen Lu",
                    "Margaret Mitchell",
                    "Dhruv Batra",
                    "C Zitnick",
                    "Devi Parikh"
                ],
                "title": "VQA: visual question answering",
                "pub_date": "2015-12-07",
                "pub_title": "2015 IEEE International Conference on Computer Vision",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "343-ARR_v1_93",
            "content": "UNKNOWN, None, 2003, Logics of Conversation, Cambridge University Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2003",
                "pub_title": "Logics of Conversation",
                "pub": "Cambridge University Press"
            }
        },
        {
            "ix": "343-ARR_v1_94",
            "content": "Amos Azaria, Jayant Krishnamurthy, Tom Mitchell, Instructable intelligent personal agent, 2016-02-12, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Amos Azaria",
                    "Jayant Krishnamurthy",
                    "Tom Mitchell"
                ],
                "title": "Instructable intelligent personal agent",
                "pub_date": "2016-02-12",
                "pub_title": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "343-ARR_v1_95",
            "content": "Jon Barwise, Robin Cooper, Generalized quantifiers and natural language, 1981, Linguistics and Philosophy, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Jon Barwise",
                    "Robin Cooper"
                ],
                "title": "Generalized quantifiers and natural language",
                "pub_date": "1981",
                "pub_title": "Linguistics and Philosophy",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_96",
            "content": "UNKNOWN, None, 1975, Rearrangement: A challenge for embodied AI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "1975",
                "pub_title": "Rearrangement: A challenge for embodied AI",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_97",
            "content": "Ben Bogin, Sanjay Subramanian, Matt Gardner, Jonathan Berant, Latent compositional representations improve systematic generalization in grounded question answering, 2021, Transactions of the Association of Computational Linguisticsl (TACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Ben Bogin",
                    "Sanjay Subramanian",
                    "Matt Gardner",
                    "Jonathan Berant"
                ],
                "title": "Latent compositional representations improve systematic generalization in grounded question answering",
                "pub_date": "2021",
                "pub_title": "Transactions of the Association of Computational Linguisticsl (TACL)",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_98",
            "content": "Jos\u00e9 Miguel Cano Sant\u00edn, Simon Dobnik, Mehdi Ghanimifard, Fast visual grounding in interaction: bringing few-shot learning with neural networks to an interactive robot, 2020, Proceedings of the Probability and Meaning Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jos\u00e9 Miguel Cano Sant\u00edn",
                    "Simon Dobnik",
                    "Mehdi Ghanimifard"
                ],
                "title": "Fast visual grounding in interaction: bringing few-shot learning with neural networks to an interactive robot",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Probability and Meaning Conference",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_99",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, UNITER: universal image-text representation learning, 2020-08-23, Computer Vision -ECCV 2020 -16th European Conference, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Yen-Chun Chen",
                    "Linjie Li",
                    "Licheng Yu",
                    "Ahmed Kholy",
                    "Faisal Ahmed",
                    "Zhe Gan",
                    "Yu Cheng",
                    "Jingjing Liu"
                ],
                "title": "UNITER: universal image-text representation learning",
                "pub_date": "2020-08-23",
                "pub_title": "Computer Vision -ECCV 2020 -16th European Conference",
                "pub": "Springer"
            }
        },
        {
            "ix": "343-ARR_v1_100",
            "content": "Ann Copestake, Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go, 2009-03-30, EACL 2009, 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Ann Copestake"
                ],
                "title": "Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go",
                "pub_date": "2009-03-30",
                "pub_title": "EACL 2009, 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_101",
            "content": "Ann Copestake, D Flickinger, C Pollard, I Sag, Minimal recursion semantics: An introduction, 1997, Research on Language and Computation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Ann Copestake",
                    "D Flickinger",
                    "C Pollard",
                    "I Sag"
                ],
                "title": "Minimal recursion semantics: An introduction",
                "pub_date": "1997",
                "pub_title": "Research on Language and Computation",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_102",
            "content": "Nikolaus Correll, Kostas Bekris, Dmitry Berenson, Oliver Brock, Albert Causo, Kris Hauser, Kei Okada, Alberto Rodriguez, Joseph Romano, Peter Wurman, Analysis and observations from the first amazon picking challenge, 2018, IEEE Trans Autom. Sci. Eng, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Nikolaus Correll",
                    "Kostas Bekris",
                    "Dmitry Berenson",
                    "Oliver Brock",
                    "Albert Causo",
                    "Kris Hauser",
                    "Kei Okada",
                    "Alberto Rodriguez",
                    "Joseph Romano",
                    "Peter Wurman"
                ],
                "title": "Analysis and observations from the first amazon picking challenge",
                "pub_date": "2018",
                "pub_title": "IEEE Trans Autom. Sci. Eng",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_103",
            "content": "Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh, Ajay Divakaran, Align2ground: Weakly supervised phrase grounding guided by image-caption alignment, 2019-10-27, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Samyak Datta",
                    "Karan Sikka",
                    "Anirban Roy",
                    "Karuna Ahuja",
                    "Devi Parikh",
                    "Ajay Divakaran"
                ],
                "title": "Align2ground: Weakly supervised phrase grounding guided by image-caption alignment",
                "pub_date": "2019-10-27",
                "pub_title": "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019",
                "pub": "IEEE"
            }
        },
        {
            "ix": "343-ARR_v1_104",
            "content": "Olivier Delalleau, Yoshua Bengio, Nicolas Roux, Efficient non-parametric function induction in semi-supervised learning, 2005-01-06, Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, AISTATS 2005, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Olivier Delalleau",
                    "Yoshua Bengio",
                    "Nicolas Roux"
                ],
                "title": "Efficient non-parametric function induction in semi-supervised learning",
                "pub_date": "2005-01-06",
                "pub_title": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, AISTATS 2005",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_105",
            "content": "Guy Van Den Broeck, Nima Taghipour, Wannes Meert, Jesse Davis, Luc De Raedt, Lifted probabilistic inference by first-order knowledge compilation, 2011-07-16, IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Guy Van Den Broeck",
                    "Nima Taghipour",
                    "Wannes Meert",
                    "Jesse Davis",
                    "Luc De Raedt"
                ],
                "title": "Lifted probabilistic inference by first-order knowledge compilation",
                "pub_date": "2011-07-16",
                "pub_title": "IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_106",
            "content": "Jeffrey Dudek, Vu Phan, Moshe Vardi, ADDMC: weighted model counting with algebraic decision diagrams, 2020-02-07, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Jeffrey Dudek",
                    "Vu Phan",
                    "Moshe Vardi"
                ],
                "title": "ADDMC: weighted model counting with algebraic decision diagrams",
                "pub_date": "2020-02-07",
                "pub_title": "The Thirty-Second Innovative Applications of Artificial Intelligence Conference",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "343-ARR_v1_107",
            "content": "Evelyn Fix, Joseph Hodges, Discriminatory analysis -nonparametric discrimination: Consistency properties, 1989, International Statistical Review, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Evelyn Fix",
                    "Joseph Hodges"
                ],
                "title": "Discriminatory analysis -nonparametric discrimination: Consistency properties",
                "pub_date": "1989",
                "pub_title": "International Statistical Review",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_108",
            "content": "Dan Flickinger, On building a more effcient grammar by exploiting types, 2000, Nat. Lang. Eng, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Dan Flickinger"
                ],
                "title": "On building a more effcient grammar by exploiting types",
                "pub_date": "2000",
                "pub_title": "Nat. Lang. Eng",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_109",
            "content": "Akira Fukui, Dong Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach, Multimodal compact bilinear pooling for visual question answering and visual grounding, 2016-11-01, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, The Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Akira Fukui",
                    "Dong Park",
                    "Daylen Yang",
                    "Anna Rohrbach",
                    "Trevor Darrell",
                    "Marcus Rohrbach"
                ],
                "title": "Multimodal compact bilinear pooling for visual question answering and visual grounding",
                "pub_date": "2016-11-01",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
                "pub": "The Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v1_110",
            "content": "UNKNOWN, None, 2021, Annual review of control, robotics, and autonomous systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Annual review of control, robotics, and autonomous systems",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_111",
            "content": "J Groenendijk, M Stokhof, Dynamic predicate logic, 1991, Linguistics and Philosophy, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "J Groenendijk",
                    "M Stokhof"
                ],
                "title": "Dynamic predicate logic",
                "pub_date": "1991",
                "pub_title": "Linguistics and Philosophy",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_112",
            "content": "UNKNOWN, None, 1999, The symbol grounding problem. CoRR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": "1999",
                "pub_title": "The symbol grounding problem. CoRR",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_113",
            "content": "Yordan Hristov, Daniel Angelov, Michael Burke, Alex Lascarides, Subramanian Ramamoorthy, Disentangled relational representations for explaining and learning from demonstration, 2019-10-30, 3rd Annual Conference on Robot Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Yordan Hristov",
                    "Daniel Angelov",
                    "Michael Burke",
                    "Alex Lascarides",
                    "Subramanian Ramamoorthy"
                ],
                "title": "Disentangled relational representations for explaining and learning from demonstration",
                "pub_date": "2019-10-30",
                "pub_title": "3rd Annual Conference on Robot Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "343-ARR_v1_114",
            "content": "Yordan Hristov, Alex Lascarides, Subramanian Ramamoorthy, Interpretable latent spaces for learning from demonstration, 2018-10-31, of Proceedings of Machine Learning Research, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Yordan Hristov",
                    "Alex Lascarides",
                    "Subramanian Ramamoorthy"
                ],
                "title": "Interpretable latent spaces for learning from demonstration",
                "pub_date": "2018-10-31",
                "pub_title": "of Proceedings of Machine Learning Research",
                "pub": "PMLR"
            }
        },
        {
            "ix": "343-ARR_v1_115",
            "content": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Weinberger, Densely connected convolutional networks, 2017-07-21, 2017 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Gao Huang",
                    "Zhuang Liu",
                    "Laurens Van Der Maaten",
                    "Kilian Weinberger"
                ],
                "title": "Densely connected convolutional networks",
                "pub_date": "2017-07-21",
                "pub_title": "2017 IEEE Conference on Computer Vision and Pattern Recognition",
                "pub": "IEEE Computer Society"
            }
        },
        {
            "ix": "343-ARR_v1_116",
            "content": "Yichen Jiang, Mohit Bansal, Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Yichen Jiang",
                    "Mohit Bansal"
                ],
                "title": "Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_117",
            "content": "UNKNOWN, None, , Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. 2021. MDETR -modulated detection for end-to-end multimodal understanding, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. 2021. MDETR -modulated detection for end-to-end multimodal understanding",
                "pub": "CoRR"
            }
        },
        {
            "ix": "343-ARR_v1_118",
            "content": "UNKNOWN, None, 2010, Learning adaptive language interfaces through decomposition. CoRR, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": null,
                "title": null,
                "pub_date": "2010",
                "pub_title": "Learning adaptive language interfaces through decomposition. CoRR, abs",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_119",
            "content": "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Referitgame: Referring to objects in photographs of natural scenes, 2014-10-25, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Sahar Kazemzadeh",
                    "Vicente Ordonez",
                    "Mark Matten",
                    "Tamara Berg"
                ],
                "title": "Referitgame: Referring to objects in photographs of natural scenes",
                "pub_date": "2014-10-25",
                "pub_title": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing",
                "pub": "ACL"
            }
        },
        {
            "ix": "343-ARR_v1_120",
            "content": "C Kennington, David Schlangen, A simple generative model of incremental reference resolution for situated dialogue, 2017, Comput. Speech Lang, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "C Kennington",
                    "David Schlangen"
                ],
                "title": "A simple generative model of incremental reference resolution for situated dialogue",
                "pub_date": "2017",
                "pub_title": "Comput. Speech Lang",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_121",
            "content": "Casey Kennington, Livia Dia, David Schlangen, A discriminative model for perceptuallygrounded incremental reference resolution, 2015-04-17, Proceedings of the 11th International Conference on Computational Semantics, IWCS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Casey Kennington",
                    "Livia Dia",
                    "David Schlangen"
                ],
                "title": "A discriminative model for perceptuallygrounded incremental reference resolution",
                "pub_date": "2015-04-17",
                "pub_title": "Proceedings of the 11th International Conference on Computational Semantics, IWCS",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_122",
            "content": "Alexander Koller, Stefan Thater, The evolution of dominance constraint solvers, 2005, Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Alexander Koller",
                    "Stefan Thater"
                ],
                "title": "The evolution of dominance constraint solvers",
                "pub_date": "2005",
                "pub_title": "Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_123",
            "content": "UNKNOWN, None, 2017, Shapeworld -A new test methodology for multimodal language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Shapeworld -A new test methodology for multimodal language understanding",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_124",
            "content": "UNKNOWN, None, 2017, teractive task learning. IEEE Intelligent Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "teractive task learning. IEEE Intelligent Systems",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_125",
            "content": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao, Oscar: Object-semantics aligned pre-training for vision-language tasks, 2020-08-23, Computer Vision -ECCV 2020 -16th European Conference, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Xiujun Li",
                    "Xi Yin",
                    "Chunyuan Li",
                    "Pengchuan Zhang",
                    "Xiaowei Hu",
                    "Lei Zhang",
                    "Lijuan Wang",
                    "Houdong Hu",
                    "Li Dong",
                    "Furu Wei",
                    "Yejin Choi",
                    "Jianfeng Gao"
                ],
                "title": "Oscar: Object-semantics aligned pre-training for vision-language tasks",
                "pub_date": "2020-08-23",
                "pub_title": "Computer Vision -ECCV 2020 -16th European Conference",
                "pub": "Springer"
            }
        },
        {
            "ix": "343-ARR_v1_126",
            "content": "Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Jiasen Lu",
                    "Dhruv Batra",
                    "Devi Parikh",
                    "Stefan Lee"
                ],
                "title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks",
                "pub_date": "2019-12-08",
                "pub_title": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_127",
            "content": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua Tenenbaum, Jiajun Wu, The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Jiayuan Mao",
                    "Chuang Gan",
                    "Pushmeet Kohli",
                    "Joshua Tenenbaum",
                    "Jiajun Wu"
                ],
                "title": "The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",
                "pub_date": "2019-05-06",
                "pub_title": "7th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_128",
            "content": "Cynthia Matuszek, Grounded language learning: Where robotics and NLP meet, 2018-07-13, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Cynthia Matuszek"
                ],
                "title": "Grounded language learning: Where robotics and NLP meet",
                "pub_date": "2018-07-13",
                "pub_title": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_129",
            "content": "UNKNOWN, None, 1976, Language and perception, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "1976",
                "pub_title": "Language and perception",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_130",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Learning transferable visual models from natural language supervision, 2021-07, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Alec Radford",
                    "Jong Kim",
                    "Chris Hallacy",
                    "Aditya Ramesh",
                    "Gabriel Goh",
                    "Sandhini Agarwal",
                    "Girish Sastry",
                    "Amanda Askell",
                    "Pamela Mishkin",
                    "Jack Clark",
                    "Gretchen Krueger",
                    "Ilya Sutskever"
                ],
                "title": "Learning transferable visual models from natural language supervision",
                "pub_date": "2021-07",
                "pub_title": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021",
                "pub": "PMLR"
            }
        },
        {
            "ix": "343-ARR_v1_131",
            "content": "Bertrand Russell, Knowledge by acquaintance and knowledge by description, 1917, Mysticism and Logic, Longmans Green.",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Bertrand Russell"
                ],
                "title": "Knowledge by acquaintance and knowledge by description",
                "pub_date": "1917",
                "pub_title": "Mysticism and Logic",
                "pub": "Longmans Green"
            }
        },
        {
            "ix": "343-ARR_v1_132",
            "content": "Marko Samer, Stefan Szeider, Algorithms for propositional model counting, 2010, J. Discrete Algorithms, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Marko Samer",
                    "Stefan Szeider"
                ],
                "title": "Algorithms for propositional model counting",
                "pub_date": "2010",
                "pub_title": "J. Discrete Algorithms",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_133",
            "content": "Haoyue Shi, Jiayuan Mao, Kevin Gimpel, Karen Livescu, Visually grounded neural syntax acquisition, 2019-07-28, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Haoyue Shi",
                    "Jiayuan Mao",
                    "Kevin Gimpel",
                    "Karen Livescu"
                ],
                "title": "Visually grounded neural syntax acquisition",
                "pub_date": "2019-07-28",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v1_134",
            "content": "Mohit Shridhar, David Hsu, Interactive visual grounding of referring expressions for humanrobot interaction, 2018-06-26, Robotics: Science and Systems XIV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": [
                    "Mohit Shridhar",
                    "David Hsu"
                ],
                "title": "Interactive visual grounding of referring expressions for humanrobot interaction",
                "pub_date": "2018-06-26",
                "pub_title": "Robotics: Science and Systems XIV",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_135",
            "content": "J Stuckler, D Holz, S Behnke, Robocup@home: Demonstrating everyday manipulation skills in robocup@home, 2012, IEEE Robotics Automation Magazine, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "J Stuckler",
                    "D Holz",
                    "S Behnke"
                ],
                "title": "Robocup@home: Demonstrating everyday manipulation skills in robocup@home",
                "pub_date": "2012",
                "pub_title": "IEEE Robotics Automation Magazine",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_136",
            "content": "Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner, Obtaining faithful interpretations from compositional neural networks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b44",
                "authors": [
                    "Sanjay Subramanian",
                    "Ben Bogin",
                    "Nitish Gupta",
                    "Tomer Wolfson",
                    "Sameer Singh",
                    "Jonathan Berant",
                    "Matt Gardner"
                ],
                "title": "Obtaining faithful interpretations from compositional neural networks",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v1_137",
            "content": "UNKNOWN, None, 2019, Analyzing compositionality in visual question answering, .",
            "ntype": "ref",
            "meta": {
                "xid": "b45",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Analyzing compositionality in visual question answering",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_138",
            "content": "Hao Tan, Mohit Bansal, LXMERT: learning cross-modality encoder representations from transformers, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b46",
                "authors": [
                    "Hao Tan",
                    "Mohit Bansal"
                ],
                "title": "LXMERT: learning cross-modality encoder representations from transformers",
                "pub_date": "2019-11-03",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_139",
            "content": "Leslie Valiant, The complexity of computing the permanent, 1979, Theor. Comput. Sci, .",
            "ntype": "ref",
            "meta": {
                "xid": "b47",
                "authors": [
                    "Leslie Valiant"
                ],
                "title": "The complexity of computing the permanent",
                "pub_date": "1979",
                "pub_title": "Theor. Comput. Sci",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_140",
            "content": "Johan Van Benthem, Questions about quantifiers, 1984, J. Symb. Log, .",
            "ntype": "ref",
            "meta": {
                "xid": "b48",
                "authors": [
                    "Johan Van Benthem"
                ],
                "title": "Questions about quantifiers",
                "pub_date": "1984",
                "pub_title": "J. Symb. Log",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_141",
            "content": "R Van Der,  Sandt, Presupposition projection as anaphora resolution, 1992, Journal of Semantics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b49",
                "authors": [
                    "R Van Der",
                    " Sandt"
                ],
                "title": "Presupposition projection as anaphora resolution",
                "pub_date": "1992",
                "pub_title": "Journal of Semantics",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_142",
            "content": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, Daan Wierstra, Matching networks for one shot learning, 2016-12-05, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, .",
            "ntype": "ref",
            "meta": {
                "xid": "b50",
                "authors": [
                    "Oriol Vinyals",
                    "Charles Blundell",
                    "Tim Lillicrap",
                    "Koray Kavukcuoglu",
                    "Daan Wierstra"
                ],
                "title": "Matching networks for one shot learning",
                "pub_date": "2016-12-05",
                "pub_title": "Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_143",
            "content": "I Sida, Samuel Wang, Percy Ginn, Christopher Liang,  Manning, Naturalizing a programming language via interactive learning, 2017-07-30, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b51",
                "authors": [
                    "I Sida",
                    "Samuel Wang",
                    "Percy Ginn",
                    "Christopher Liang",
                    " Manning"
                ],
                "title": "Naturalizing a programming language via interactive learning",
                "pub_date": "2017-07-30",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v1_144",
            "content": "I Sida, Percy Wang, Christopher Liang,  Manning, Learning language games through interaction, 2016-08-07, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, .",
            "ntype": "ref",
            "meta": {
                "xid": "b52",
                "authors": [
                    "I Sida",
                    "Percy Wang",
                    "Christopher Liang",
                    " Manning"
                ],
                "title": "Learning language games through interaction",
                "pub_date": "2016-08-07",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_145",
            "content": "D Whitney, M Eldon, J Oberlin, S Tellex, Interpreting multimodal referring expressions in real time, 2016, Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), .",
            "ntype": "ref",
            "meta": {
                "xid": "b53",
                "authors": [
                    "D Whitney",
                    "M Eldon",
                    "J Oberlin",
                    "S Tellex"
                ],
                "title": "Interpreting multimodal referring expressions in real time",
                "pub_date": "2016",
                "pub_title": "Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_146",
            "content": "Ronald Williams, Simple statistical gradientfollowing algorithms for connectionist reinforcement learning, 1992, Mach. Learn, .",
            "ntype": "ref",
            "meta": {
                "xid": "b54",
                "authors": [
                    "Ronald Williams"
                ],
                "title": "Simple statistical gradientfollowing algorithms for connectionist reinforcement learning",
                "pub_date": "1992",
                "pub_title": "Mach. Learn",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_147",
            "content": "David Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, 1995, 33rd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b55",
                "authors": [
                    "David Yarowsky"
                ],
                "title": "Unsupervised word sense disambiguation rivaling supervised methods",
                "pub_date": "1995",
                "pub_title": "33rd Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "343-ARR_v1_148",
            "content": "Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang, Cross-modal self-attention network for referring image segmentation, 2019-06-16, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Computer Vision Foundation / IEEE.",
            "ntype": "ref",
            "meta": {
                "xid": "b56",
                "authors": [
                    "Linwei Ye",
                    "Mrigank Rochan",
                    "Zhi Liu",
                    "Yang Wang"
                ],
                "title": "Cross-modal self-attention network for referring image segmentation",
                "pub_date": "2019-06-16",
                "pub_title": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019",
                "pub": "Computer Vision Foundation / IEEE"
            }
        },
        {
            "ix": "343-ARR_v1_149",
            "content": "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum, Neural-symbolic VQA: disentangling reasoning from vision and language understanding, 2018-12-03, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b57",
                "authors": [
                    "Kexin Yi",
                    "Jiajun Wu",
                    "Chuang Gan",
                    "Antonio Torralba",
                    "Pushmeet Kohli",
                    "Josh Tenenbaum"
                ],
                "title": "Neural-symbolic VQA: disentangling reasoning from vision and language understanding",
                "pub_date": "2018-12-03",
                "pub_title": "Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_150",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b58",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "343-ARR_v1_151",
            "content": ", Knowledge enhanced vision-language representations through scene graphs, 2021, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, AAAI Press.",
            "ntype": "ref",
            "meta": {
                "xid": "b59",
                "authors": [],
                "title": "Knowledge enhanced vision-language representations through scene graphs",
                "pub_date": "2021",
                "pub_title": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event",
                "pub": "AAAI Press"
            }
        },
        {
            "ix": "343-ARR_v1_152",
            "content": "Yanpeng Zhao, Ivan Titov, Visually grounded compound pcfgs, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b60",
                "authors": [
                    "Yanpeng Zhao",
                    "Ivan Titov"
                ],
                "title": "Visually grounded compound pcfgs",
                "pub_date": "2020-11-16",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "343-ARR_v1_0@0",
            "content": "Interactive Symbol Grounding with Complex Referential Expressions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_0",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_2@0",
            "content": "We present a procedure for learning to ground symbols from a sequence of stimuli consisting of an arbitrarily complex noun phrase (e.g. \"all but one green square above both red circles.\") and its designation in the visual scene.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_2",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_2@1",
            "content": "Our distinctive approach combines: a) lazy fewshot learning to relate open-class words like green and above to their visual percepts; and b) symbolic reasoning with closed-class word categories like quantifiers and negation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_2",
            "start": 229,
            "end": 452,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_2@2",
            "content": "We use this combination to estimate new training examples for grounding symbols that occur within a noun phrase but aren't designated by that noun phase (e.g, red in the above example), thereby potentially gaining data efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_2",
            "start": 454,
            "end": 683,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_2@3",
            "content": "We evaluate the approach in a visual reference resolution task, in which the learner starts out unaware of concepts that are part of the domain model and how they relate to visual percepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_2",
            "start": 685,
            "end": 873,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_4@0",
            "content": "Robotic tasks in unstructured (e.g., Stuckler et al. (2012)) or semi-structured (e.g., Correll et al. (2018)) environments involve rearranging objects (Batra et al., 2020) to reach the goal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_4",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_4@1",
            "content": "Such tasks can be solved using task and motion planning if the robot has a full and accurate domain model (Garrett et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_4",
            "start": 191,
            "end": 319,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_4@2",
            "content": "But there are many scenarios in which the robot must adapt its behaviour to novel and unforeseen objects and attributes that are introduced into the environment after deployment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_4",
            "start": 321,
            "end": 498,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_4@3",
            "content": "These types of scenarios can be tackled using interactive task learning (ITL) (see Laird et al. (2017) for a survey): a learner solves the novel task via evidence from its own actions and reactive guidance from a teacher.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_4",
            "start": 500,
            "end": 720,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_4@4",
            "content": "This paper focuses on symbol grounding (Harnad, 1999) in the context of ITL (Matuszek, 2018): the learner must use the teacher's embodied natural language utterance and its context to learn a mapping from linguistic expressions to their denotations, given the visual percepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_4",
            "start": 722,
            "end": 997,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_5@0",
            "content": "There are two challenges in learning symbol grounding models (grounders) in ITL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_5",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_5@1",
            "content": "Firstly, in contrast to many contemporary grounders (e.g., Ye et al. (2019); Datta et al. (2019)), ITL requires incremental learning because the data samples are acquired piecemeal via an extended interaction, and these samples should influence planning as and when they occur.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_5",
            "start": 81,
            "end": 357,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_5@2",
            "content": "Secondly, previous work on learning grounders has been mainly focused on using simple expressions of bare nouns (e.g. \"block\") or very short phrases (e.g., \"blue sphere\", \"block behind cylinder\") (e.g., Hristov et al. (2018Hristov et al. ( , 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_5",
            "start": 359,
            "end": 606,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_5@3",
            "content": "But it is highly natural for the teacher's language to be more complex, creating the possibility that novel symbols-neologisms-are introduced in a context where their denotation is not designated by the teacher (e.g., blue and square when a teacher points to the denotation of the noun phrase \"a red circle behind both blue squares\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_5",
            "start": 608,
            "end": 941,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_6@0",
            "content": "We tackle both of these challenges while aiming to extract knowledge that is \"actionable\": the acquired knowledge should improve both the agent's understanding of the domain and its estimates of the current state, a necessary condition to successful planning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_6",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_6@1",
            "content": "Current grounding approaches miss many learning opportunities that complex referring expressions (REs) like \"all but one red circle above both blue crosses\" afford: e.g., this RE not only entails that its referents all satisfy the properties red and circle, but also that there are two (further) objects that both satisfy blue and star, that these are above the designated objects, and everything else in the domain is either not blue or not a star (thanks to the meaning of both).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_6",
            "start": 260,
            "end": 740,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_6@2",
            "content": "Thus, a single RE and its designation can in principle be used to gather several (noisy) training exemplars (both positive and negative) for several symbols at once, even if these symbols have not been designated by the RE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_6",
            "start": 742,
            "end": 964,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_6@3",
            "content": "The main aim of this paper is to explore the effects of this facility on data efficiency and accuracy in learning grounders.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_6",
            "start": 966,
            "end": 1089,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_7@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_7",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_8@0",
            "content": "Symbol Grounding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_8",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_8@1",
            "content": "Contemporary grounders extensively utilize batch learning (e.g. Shridhar and Hsu (2018)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_8",
            "start": 18,
            "end": 106,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_8@2",
            "content": "Yet, ITL requires incremental learning because without it the teacher guidance cannot influence the learner's inferences about plans as and when the advice is given.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_8",
            "start": 108,
            "end": 272,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_8@3",
            "content": "Further, many grounders assume that the learner starts out with a complete and accurate conceptualisation of the domain using pre-defined visual features and a known vocabulary (Kennington et al., 2015;Kennington and Schlangen, 2017;Wang et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_8",
            "start": 274,
            "end": 525,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_8@4",
            "content": "In ITL, both of these assumptions are unrealistic; therefore in this paper we explore models for which these assumptions don't apply.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_8",
            "start": 527,
            "end": 659,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_8@5",
            "content": "Finally, in contrast to all prior grounders, we support incremental learning when the training exemplars feature REs that are linguistically complex: e.g., \"two red circles that aren't to the right of both green squares\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_8",
            "start": 661,
            "end": 881,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_9@0",
            "content": "Representation Learning. Models for jointly learning a representation for vision and language utilize either explicit alignment via bounding boxes or instance segmentation (Lu et al., 2019;Chen et al., 2020;Tan and Bansal, 2019;Kamath et al., 2021;Yu et al., 2021), or a large-volume of weakly labeled data in the form of image-caption pairs (Radford et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_9",
            "start": 0,
            "end": 364,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_9@1",
            "content": "These models rely on offline learning with large datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_9",
            "start": 366,
            "end": 423,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_9@2",
            "content": "This work, on the other hand, explores how to incrementally extract knowledge in a few-shot manner, using sequentially observed evidence that consists of weakly-labelled annotations, including neologisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_9",
            "start": 425,
            "end": 628,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_10@0",
            "content": "Grounded Language Acquisition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_10",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_10@1",
            "content": "This task is commonly realized as grounded grammar induction from image-caption pairs (Shi et al., 2019;Zhao and Titov, 2020), or as (neural) semantic parsers learning from a reward signal (Williams, 1992) in visual question answering Yi et al., 2018) or in planning (Azaria et al., 2016;Wang et al., 2016Wang et al., , 2017Karamcheti et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_10",
            "start": 31,
            "end": 379,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_10@2",
            "content": "In these scenarios, the primary objective is to learn mappings from linguistic expressions to logical forms, which in turn get associated with certain visual percepts during the learning process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_10",
            "start": 381,
            "end": 575,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_10@3",
            "content": "This paper does not tackle learning logical forms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_10",
            "start": 577,
            "end": 626,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_10@4",
            "content": "Instead, we obtain them from an existing broad-coverage grammar and focus on exploiting the logical consequences of those logical forms during symbol grounding-i.e., our focus is to utilise the interpretation of logical forms, and in particular the truth functional meanings of close-class words like quantifiers and negation, to inform the learning of mappings from (open-class) symbols like red to their denotations, given the visual percepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_10",
            "start": 628,
            "end": 1072,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_11@0",
            "content": "Visual Questions Answering (VQA) is a task of answering free-form questions about an image (Antol et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_11",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_11@1",
            "content": "VQA has reached impressive performance in recent years (Fukui et al., 2016;, yet VQA models struggle with out-of distribution generalization for new types of questions, requiring multi-step reasoning, with analysis revealing that they often rely on shortcuts (Jiang and Bansal, 2019;Subramanian et al., , 2020.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_11",
            "start": 113,
            "end": 422,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_11@2",
            "content": "Grounded VQA models like (Yi et al., 2018) and Bogin et al. (2021) tackle these shortcomings by learning to ground parts of the question and then learning to compose those parts via the question's syntax to compute the answer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_11",
            "start": 424,
            "end": 649,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_11@3",
            "content": "They thus estimate denotations of linguistic parts that are not denoted by the answer to the question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_11",
            "start": 651,
            "end": 752,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_11@4",
            "content": "By doing so, these models achieve out-of-distribution generalization for novel questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_11",
            "start": 754,
            "end": 842,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_11@5",
            "content": "However, these models lack the ITL's requirement for incremental learning: model training relies on batch learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_11",
            "start": 844,
            "end": 958,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_11@6",
            "content": "Furthermore, while their performance is impressive, error analysis shows that it tends to make mistakes on sentences containing logical concepts like quantifiers and negation (e.g. Bogin et al. (2021) Figure 9 shows that the determiner most incorrectly denotes an arbitrary subset of entities).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_11",
            "start": 960,
            "end": 1253,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_11@7",
            "content": "Our view is that there is little benefit in trying to learn to ground these concepts as they are domain independent and can be interpreted using formal semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_11",
            "start": 1255,
            "end": 1416,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_11@8",
            "content": "In our experiments, we are testing the extent to which knowing and reasoning with the logical meanings of these symbols helps incremental grounding, and in particular estimating denotations of symbols within an RE that aren't designated by that RE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_11",
            "start": 1418,
            "end": 1665,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_12@0",
            "content": "Visual Reference Resolution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_12",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_12@1",
            "content": "In previous experimental setups, it is often assumed that there is a unique referent in the visual scene for the given RE in the test phase (e.g., Kazemzadeh et al. (2014); Whitney et al. (2016)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_12",
            "start": 29,
            "end": 224,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_12@2",
            "content": "We aim to cope with situations where the RE has multiple referents: identifying all the referents that satisfy an RE enables efficient planning, because it affords free choice when executing certain commands-e.g., \"move a circle to the left.\" when there is more than one circle affords choosing a control policy so that resources are optimized.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_12",
            "start": 226,
            "end": 569,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_13@0",
            "content": "Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_13",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_14@0",
            "content": "Formal Semantics of Natural Language",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_14",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_15@0",
            "content": "The language L of predicate logic with generalized quantifiers (Barwise and Cooper, 1981;van Benthem, 1984) is a canonical formal meaning representation for natural languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_15",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_15@1",
            "content": "L-sentences \u03c6 are constructed recursively from predicates P , terms T (i.e., variables V and constants C), logical connectives O = {\u00ac, \u2227, \u2228, \u2192} and quantifiers Q (see Table 1 column 1):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_15",
            "start": 176,
            "end": 360,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_16@0",
            "content": "\u03c6 ::= p(t 1 , . . . , t n ) \u2261 p(t n ) |(\u00ac\u03c6)|(\u03c6 1 \u2227 \u03c6 2 )|(\u03c6 1 \u2228 \u03c6 2 )|(\u03c6 1 \u2192 \u03c6 2 ) |(Qx(\u03c6 1 , \u03c6 2 ))",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_16",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_17@0",
            "content": "where p is an n-place predicate, t i \u2208 T are terms, Q \u2208 Q is a quantifier, and x \u2208 V is a variable (in Qx(\u03c6 1 , \u03c6 2 ), \u03c6 1 is the restrictor and \u03c6 2 the body). We also introduce \u03bb-expressions of the form \u03bbx.\u03c6, where x \u2208 V is free or absent in \u03c6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_17",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_18@0",
            "content": "Model-theoretic Interpretation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_18",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_19@0",
            "content": "L-sentences are interpreted using a domain model M = (E, I) consisting of a set of entities E (universe of discourse), and an extension function I that maps non-logical symbols P \u222a C to denotations (tuples of entities).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_19",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_19@1",
            "content": "For convenience, we assume I : C \u2192 E is one-to-one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_19",
            "start": 220,
            "end": 270,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_19@2",
            "content": "Variables are interpreted via an assignment function g : V \u2192 E.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_19",
            "start": 272,
            "end": 334,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_20@0",
            "content": "The interpretation function \u2022 M,g specifies the semantic value of well-formed expressions of L:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_20",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_21@0",
            "content": "a M,g = I(a) if a \u2208 P \u222a C g(a) if a \u2208 V p(t n ) M,g = 1 iff ( t n M,g ) \u2208 p M,g \u00ac\u03c6 M,g = 1 iff \u03c6 M,g = 0 \u03c6 \u2227 \u03c8 M,g = 1 iff \u03c6 M,g = 1 and \u03c8 M,g = 1 \u03c6 \u2228 \u03c8 M,g = 1 iff \u03c6 M,g = 1 or \u03c8 M,g = 1 \u03c6 \u2192 \u03c8 M,g = 1 iff \u03c6 M,g = 0 or \u03c8 M,g = 1 \u03bbx.\u03c6 M,g = {e \u2208 E : \u03c6 M,g[x/e] = 1} Qx(\u03c6 1 , \u03c6 2 ) M,g = Q( \u03bb.x\u03c6 1 M,g , \u03bb.x\u03c6 2 M,g )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_21",
            "start": 0,
            "end": 313,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_22@0",
            "content": "where g[x/e] is just like g, except g[x/e](x) = e and Q is a specific relation between the restrictor \u03bbx.\u03c6 1 M,g and body \u03bbx.\u03c6 2 M,g , as defined in Table 1 column 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_22",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_22@1",
            "content": "\u2022 M,g is directly related to satisfiability for L-sentences:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_22",
            "start": 167,
            "end": 226,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_23@0",
            "content": "M, g |= \u03c6 iff \u03c6 M,g = 1 M |= \u03c6 iff \u03c6 M = 1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_23",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_24@0",
            "content": "where \u03c6 M = 1 iff \u03c6 M,g = 1 for all g. Further, if x is the only free variable in \u03c6, then \u03bbx.\u03c6 M,g = \u03bbx.\u03c6 M,g for all g, g ; so without a loss of generality, this is expressed as \u03bbx.\u03c6 M .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_24",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_24@1",
            "content": "1 If all variables in Qx(\u03c6, \u03c8) are bound by quantifiers, then this L-sentence is true iff Q is true for all g. Some quantifiers, like \"both\" in English, 2 are presupposition triggers: a statement \"two blocks are blue\" is different from \"both blocks are blue\" in that the latter is true only if there are exactly two individuals that are blocks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_24",
            "start": 188,
            "end": 531,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_24@2",
            "content": "We've adopted a Russellian interpretation (Russell, 1917) of these in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_24",
            "start": 533,
            "end": 610,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_25@0",
            "content": "Logical Forms of Referential Expressions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_25",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_26@0",
            "content": "We now define the logical forms of REs and their interpretations with respect to a domain model M. Noun phrases like \"a block\" are represented as _a_qx.block(x) . More generally, let Qx.\u03c6 be the logical form of an RE, where Q \u2208 Q and \u03c6 is an L-sentence with x \u2208 V being the only free variable in \u03c6. The referents Qx.\u03c6 M of this logical form with respect to M are computed as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_26",
            "start": 0,
            "end": 382,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_27@0",
            "content": "Qx.\u03c6 M = Q \u03c0(M,\u03c6,x)(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_27",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_28@0",
            "content": "where \u03c0(M, \u03c6, x) is an M-projection, giving a new domain model M with entities E = E \u2229 \u03bb.x\u03c6 M and Q M is a quantifier referent-a quantifier-specific subset of the power set of E. Table 1 column 4 gives the list of quantifier referents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_28",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_29@0",
            "content": "To illustrate, consider the domain model where:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_29",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_30@0",
            "content": "E = {a, b, c, d, f } I(cat) = {a, b} I(dog) = {c, d, f } I(bit) = {(c, a), (c, b), (d, b), (f, a), (f, b)}",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_30",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_31@0",
            "content": "The RE \"a dog that bit both cats\" has logical form _a_qx._both_qy.(cat(y), dog(x) \u2227 bit(x, y)) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_31",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_31@1",
            "content": "By Equation 1, its referent is:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_31",
            "start": 97,
            "end": 127,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_32@0",
            "content": "_a_q \u03c0(M,_both_qy.(cat(y),dog(x)\u2227bit(x,y)),x)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_32",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_33@0",
            "content": "The semantic value of the \u03bb-expression formed from this RE is a set of entities e \u2208 E for which the following quantifier condition is true: both_q(R, B) where R = \u03bb.y.cat(y) M,g[x/e] and B = \u03bb.y.dog(x) \u2227 bit(x, y))",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_33",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_34@0",
            "content": ". quantifiers Q surface form condition Q(R, B) referent Q M _exactly_n_q exactly n |R \u2229 B| = n {A \u2286 E : |A| = n} _at_most_n_q at most n |R \u2229 B| \u2264 n {A \u2286 E : |A| \u2264 n} _at_least_n_q at least n |R \u2229 B| \u2265 n {A \u2286 E : |A| \u2265 n} _a_q a/an |R \u2229 B| = n {A \u2286 E : |A| \u2264 1} _every_q all/every |R \u2229 B| = |R| {A \u2286 E : |A| = |E|} _the_n_q the n |R \u2229 B| = n \u2227 |R| = n {A \u2286 E : |A| = |E| \u2227 |E| = n} _both_q both |R \u2229 B| = 2 \u2227 |R| = 2 {A \u2286 E : |A| = |E| \u2227 |E| = 2} _all_but_n_q all but n |R \u2229 B| = |R| \u2212 n {A \u2286 E : |A| = |E| \u2212 n \u2227 |E| \u2265 n} _n_of_the_m_q n of the m |R \u2229 B| = n \u2227 |R| = m {A \u2286 E : |A| = n \u2227 |E| = m}",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_34",
            "start": 0,
            "end": 594,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_35@0",
            "content": "Table 1: Quantifiers (column 1), their surface forms (column 2), condition Q between the restrictor R and body denotations B, used to compute a semantic value for L-sentences of the form Qx(\u03c6, \u03c8) (column 3); and quantifier referents Q M used to compute references of the logical form of REs (column 4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_35",
            "start": 0,
            "end": 301,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_36@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_36",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_37@0",
            "content": "Below we present the procedure of interactive grounding with referential expressions (IGRE).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_37",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_37@1",
            "content": "The overall framework is given in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_37",
            "start": 93,
            "end": 135,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_38@0",
            "content": "Grounder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_38",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_39@0",
            "content": "Matching networks (Vinyals et al., 2016) are an extension of the k nearest-neighbour algorithm (Fix and Hodges, 1989) and they are usable as a fast fewshot grounder in the ITL setting (Cano Sant\u00edn et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_39",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_39@1",
            "content": "For predicates P n \u2286 P of the same arity n, a grounder \u0398 n is parameterized by a support set S n = {(x n i , y n i )} Kn i=1 , consisting of K n pairs of feature vectors x n i \u2208 R dn for denotations e n \u2208 E n and concept vectors y n i \u2208 [0, 1] |Pn| .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_39",
            "start": 212,
            "end": 461,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_39@2",
            "content": "In y n , the dimension z corresponds the predicate p z \u2208 P n and its value is the probability that p z (e n ) M,g = 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_39",
            "start": 463,
            "end": 580,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_40@0",
            "content": "Concept vectors have a one-to-one correspondence with the domain model M.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_40",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_41@0",
            "content": "Given a feature vector x n for a denotation e n \u2208 E n , \u0398 n predicts the concept vector \u0177n , using the following inference rule:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_41",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_42@0",
            "content": "\u0398 n (x n , S n ) = k i=1 \u03b1 n (x n i , x n ; S n )y n i",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_42",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_43@0",
            "content": "where \u03b1 n is an attention kernel:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_43",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_44@0",
            "content": "\u03b1 n (x n i , x n ; S n ) = exp (f n (x n i ) \u2022 h n (x n )) k j=1 exp (f n (x n j ) \u2022 h n (x n )) f n (x n ) = ReLU(w n \u2022 x n + b n ) ||ReLU(w n \u2022 x n + b n )|| 2 h n (x) = ReLU(v n \u2022 x n + c n ) ||ReLU(v n \u2022 x n + c n )|| 2 ReLU(a) = max (0, a) with learnable parameters \u03b8 n = {w n , v n , b n , c n }, and S n is k = 3 nearest exemplars to x n from S n : S n = {(x n i , y n i ) \u2208 S n : x n i \u2208 V(k, x n , S n )} where V(k, x n , S n ) is a set of k nearest feature vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_44",
            "start": 0,
            "end": 474,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_45@0",
            "content": "Batch Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_45",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_46@0",
            "content": "Given S n , \u03b8 n can be estimated either using batch learning, performed offline, or-when S n is smallin real time, as outlined by Cano Sant\u00edn et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_46",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_46@1",
            "content": "In our scenario, we learn in real time by minimizing binary cross-entropy between the ground-truth y n and predicted \u0177n concept vectors:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_46",
            "start": 157,
            "end": 292,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_47@0",
            "content": "L(y n , \u0177n ) = \u2212 |Pn| z=1 l(y z , \u0177z ) l(y n i , \u0177n i ) = y n i log(\u0177 n i ) + (1 \u2212 y n i ) log(1 \u2212 \u0177n i )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_47",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_48@0",
            "content": "Incremental Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_48",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_49@0",
            "content": "S n gets augmented whenever the teacher provides feedback in the form of an RE-designation pair.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_49",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_49@1",
            "content": "This feedback provides two types of information: certain information C n in the form of denotationsymbol-semantic value triples (e n , p z , y nz ), corresponding to symbols and entities designated by the RE; and noisy information N n , corresponding to denotation-symbol-semantic value estimate triples (e n , p z , \u1ef9nz ), which are acquired from the symbols that are part of the RE and its referent inferred via (uncertain) reasoning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_49",
            "start": 97,
            "end": 532,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_49@2",
            "content": "E.g., the RE \"a circle below a square.\", entails that its designation e \u2208 E is a circle and so (e, circle, 1) is added to C n .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_49",
            "start": 534,
            "end": 660,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_49@3",
            "content": "But it also entails there exists an entity which is a square that is not designated by the RE, but rather this entity is in the below relation with the designated entity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_49",
            "start": 662,
            "end": 831,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_49@4",
            "content": "If the grounder is sufficiently confident about the referent for square, then the corresponding triple is added to N n .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_49",
            "start": 833,
            "end": 952,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_50@0",
            "content": "Acquiring Observations and Symbols",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_50",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_51@0",
            "content": "When the learner first observes its visual sceneand so the teacher has not expressed any concepts, and so the learner is currently unaware of all concepts-the noisy support set N n is populated with (e n , p z , 0.5) (0.5 is a default semantic value) for all e n in the scene and for all known n-place predicates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_51",
            "start": 0,
            "end": 312,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_51@1",
            "content": "As the teacher provides feedback, if a new concept gets introduced with a neologism p * that the teacher utters, N n gets populated with (e n , p * , 0.5) for all denotations observed so far in the interaction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_51",
            "start": 314,
            "end": 523,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_51@2",
            "content": "During interaction, each teacher's utterance, corresponding to an RE-designation pair, adds elements to C n (for designated symbols) and triggers updates to the N n elements for all entities in the current visual scene, as we'll now describe.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_51",
            "start": 525,
            "end": 766,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_52@0",
            "content": "Integrating Teacher's Feedback",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_52",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_53@0",
            "content": "N n elements are interactively updated using an incrementally built domain-level theory \u2206, which is the conjunction of L-sentences that are built from the logical forms of the REs that the teacher has uttered so far and their designations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_53",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_53@1",
            "content": "To compute the beliefs about semantic values, given \u2206, we model the semantic value of L-sentences of the form p(t n ), in which t n are all constants, as a random variable with Bernoulli's distribution B. Thus a distribution over the possible domain models can be estimated using (propositional) model counting MC (Valiant, 1979), which maps each L-sentence to the number of domain models satisfying it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_53",
            "start": 240,
            "end": 642,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_53@2",
            "content": "In this way, the semantic value of any proposition can be estimated as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_53",
            "start": 644,
            "end": 722,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_54@0",
            "content": "\u1ef9nz = MC(pz(e n )\u2227\u2206) MC(\u2206)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_54",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_55@0",
            "content": "if MC(\u2206) = 0 0.5 otherwise MC can be computed exactly or approximately (Samer and Szeider, 2010).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_55",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_55@1",
            "content": "(in our experiments we are using the ADDMC (Dudek et al., 2020) weighted model counter with weights set to 0.5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_55",
            "start": 98,
            "end": 209,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_56@0",
            "content": "Building the Support Set",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_56",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_57@0",
            "content": "Concept vectors for S n are built using information in C n and N n : namely each denotation e n gets associated with its feature vector x n , and the zdimension of y corresponding to predicate p z \u2208 P n is computed as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_57",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_58@0",
            "content": "y z = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 y nz if (e n , p z , y nz ) \u2208 C n \u1ef9nz if (e n , p z , \u1ef9nz ) \u2208 N n \u2227 H[B(\u1ef9 nz )] \u2264 \u03c4 n 0.5 otherwise",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_58",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_59@0",
            "content": "where H[P] is the entropy of the probability distribution P, and \u03c4 n is a hyperparameter for the confidence threshold for adding noisy exemplars: in our case, it's set to 0.6 for predicates of all arities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_59",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_60@0",
            "content": "5 Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_60",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_61@0",
            "content": "Task: Visual Reference Resolution",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_61",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_62@0",
            "content": "To evaluate IGRE, we use a task of visual reference resolution: given a visual scene (an image) with localized entities (bounding boxes) and an RE, the grounder must estimate all its referents, as defined in \u00a73.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_62",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_62@1",
            "content": "The model learns its task by observing an image accompanied by a sequence of REs, with each RE paired with its designation in the image.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_62",
            "start": 214,
            "end": 349,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_63@0",
            "content": "We measure the performance of IGRE on the task after each observed RE and its designation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_63",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_63@1",
            "content": "Performance is measured using the precision P, recall R, and F1 score F1 on the test set between: 1) estimated vs. ground-truth domain models, formed from the concept vectors (intrinsic evaluation) and 2) estimated vs. ground-truth referents for the RE (extrinsic evaluation).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_63",
            "start": 91,
            "end": 366,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_63@2",
            "content": "These metrics are calculated only for those symbols/concepts that the teacher has mentioned so far (since the system is unaware that the remaining concepts exist).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_63",
            "start": 368,
            "end": 530,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_63@3",
            "content": "To obtain reliable results, we repeat the experiment 10 times: i.e., 10 different visual scenes, with a sequence of 5 different teacher utterances in each scene.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_63",
            "start": 532,
            "end": 692,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_63@4",
            "content": "We record in \u00a76 the average precision, recall and f-scores over those 10 trials.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_63",
            "start": 694,
            "end": 773,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_64@0",
            "content": "Data: ShapeWorld",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_64",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_65@0",
            "content": "To generate training and test sets, we construct ShapeWorld (Kuhnle and Copestake, 2017) domain models, each consisting of 3-12 entities, synthesized visual scenes X (64x64 pixels), and 5 REs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_65",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_65@1",
            "content": "Each domain model is describable using 7 shape symbols S1 (square, circle, triangle, pentagon, cross, ellipse, semicircle), 6 colour symbols C1 (red, green, blue, yellow, magenta, cyan) and 4 spatial relationships symbols R2 (left, right, above, below).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_65",
            "start": 193,
            "end": 445,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_65@2",
            "content": "3 In scene synthesis, the image is created from the domain model, with variation on the In interaction, the learner observes an RE, which is parsed to logical form ( \u00a75.3.2) and interpreted with respect of the extracted feature vectors for denotations ( \u00a75.3.1) to perform reference resolution ( \u00a73.3) respect to the estimated domain model M. In case of teacher feedback, RE and its designation is observed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_65",
            "start": 447,
            "end": 853,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_65@3",
            "content": "This is used to build the L-sentence that is added to \u2206 to update beliefs about the underlying concept vectors ( \u00a74.3.2), which in turn are used to update the support set ( \u00a74.3.3), used as parameters for the grounder ( \u00a74.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_65",
            "start": 855,
            "end": 1080,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_65@4",
            "content": "Elements in blue are pre-defined elements of IGRE while elements in red are learned through interaction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_65",
            "start": 1082,
            "end": 1185,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_66@0",
            "content": "hue of the colour category, and variation on the size, position, rotation, and distortion of the shapes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_66",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_66@1",
            "content": "Note that the colour categories are not mutually exclusive-e.g., there are RGB values that count as both red and magenta.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_66",
            "start": 105,
            "end": 225,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_67@0",
            "content": "To generate REs, we sample Dependency Minimal Recursion Semantics (DMRS) (Copestake, 2009) graph templates, processed using ACE (generation mode) 4 and the English Resource Grammar (ERG) (Flickinger, 2000).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_67",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_67@1",
            "content": "Generated REs are evaluated with respect to the domain model to guarantee an existing referent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_67",
            "start": 207,
            "end": 301,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_67@2",
            "content": "In total we generated 30 such domain models for training and 10 for testing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_67",
            "start": 303,
            "end": 378,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_67@3",
            "content": "The data statistics for the training set is given in Table 2 for the general categories of symbols, where certain means that the designation is denoted by the symbol in the RE, and noisy means that the symbol is a part of the RE but is not designated by it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_67",
            "start": 380,
            "end": 636,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_67@4",
            "content": "Note that the first argument to the spatial relations R2 is always denoted by the designation while its second argument is not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_67",
            "start": 638,
            "end": 764,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_67@5",
            "content": "Note also there is high variance in the frequencies among the individual symbols.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_67",
            "start": 766,
            "end": 846,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_67@6",
            "content": "For instance, blue occurs 27 and 28 times in certain vs. noisy positions respectively, while triangle occurs 7 and 12 times respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_67",
            "start": 848,
            "end": 984,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_67@7",
            "content": "Category C n candidates N n candidates C1 18.67 \u00b1 5.39 19.83 \u00b1 5.04 S1 14.67 \u00b1 3.98 16.50 \u00b1 5.32 R2 0 37.75 \u00b1 6.75",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_67",
            "start": 986,
            "end": 1099,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_68@0",
            "content": "Table 2: Average symbol counts per word for colours (C1), shapes (S1), and spatial relationships (R2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_68",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_69@0",
            "content": "Implementation Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_69",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_70@0",
            "content": "Feature Extraction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_70",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_71@0",
            "content": "To extract visual features for denotations e n \u2208 E n , we utilize bounding boxes b = [x lef t , x right , y top , y bottom ] for each entity e \u2208 E in the visual scene by localizing them (cropping) and extracting the visual features using a pre-trained visual feature encoder (in our case, DenseNet161 (Huang et al., 2017)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_71",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_71@1",
            "content": "Additionally, for the feature vector, we add entity's bounding box coordinates for spatial information, lost in the localization process:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_71",
            "start": 324,
            "end": 460,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_72@0",
            "content": "x n = Concat({[DenseNet161(X[b i ]], b i )} n i=1 )",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_72",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_73@0",
            "content": "Grammar-based Semantic Parsing",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_73",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_74@0",
            "content": "To parse REs to their logical forms, we use the English Resource Grammar (ERG) and ACE (parsing mode) to produce a representation in minimal recursion semantics (MRS) (Copestake et al., 1997), which we then simplify via hand-written rules (e.g., removing event arguments from predicate symbols corresponding to adjectives and prepositions).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_74",
            "start": 0,
            "end": 339,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_74@1",
            "content": "Underspecification of the MRS was resolved using UTOOL (Koller and Thater, 2005) and the final logical form was selected based on the linear order of scope-bearing elements (quantifiers and negation): e.g. for the RE \"every circle above a square\", _every_q outscopes _a_q.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_74",
            "start": 341,
            "end": 612,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_75@0",
            "content": "Axioms for R2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_75",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_76@0",
            "content": "For |E| entities, there are |E| 2 denotations to consider for each 2-place predicate-a larger search space compared to |E| denotations for 1-place predicate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_76",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_76@1",
            "content": "Moreover, these predicates can only be acquired from the noisy component N n because the second argument to the relation is always latent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_76",
            "start": 158,
            "end": 295,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_77@0",
            "content": "To aid the learning process for R2, whenever a new symbol R \u2208 R2 is observed, domainlevel axioms are added to \u2206 for it, making it irrreflexive: \u2200x.\u00acR(x, x) (an entity cannot be in a spatial relationship to itself) and asymmetric: \u2200x, y.R(x, y) \u2192 \u00acR(y, x) (reflecting the fact that entities in spatial relations take different roles (Miller and Johnson-Laird, 1976)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_77",
            "start": 0,
            "end": 365,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_77@1",
            "content": "These axioms reduce the number of possible denotations for R2 symbols from |E| 2 to |E| 2 2 \u2212 |E|.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_77",
            "start": 367,
            "end": 464,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_78@0",
            "content": "Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_78",
            "start": 0,
            "end": 8,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_79@0",
            "content": "To test the benefit of using noisy training exemplars N n from the oblique symbols in the REs-in other words, those symbols that are a part of the RE but not designated by it-we implemented a HEAD grounder baseline, which uses information only from C n .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_79",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_79@1",
            "content": "That HEAD uses only symbol-designation pairs that are acquired when the symbol denotes the referent (in our case, that's the head noun in the RE and its pre-head modifier, if it exists).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_79",
            "start": 255,
            "end": 440,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_80@0",
            "content": "To test the the benefit of using the precise formal semantic meanings of logical symbols (i.e., quantifiers and negation), we implemented an EXIST grounder baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_80",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_80@1",
            "content": "This utilizes the information from the symbols in the oblique positions, but it does not utilize the precise symbolic interpretation of the logical symbols, instead simplifying the logical form of the RE by replacing all quantifiers with the existential _a_q and removing negation (e.g., \"every cross on the left of the one circle\" is equivalent to \"a cross on the left of a circle\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_80",
            "start": 166,
            "end": 549,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_80@2",
            "content": "This baseline preserves the basic linguistic structure of the formal semantic representation of the RE, but not its truth-functional interpretation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_80",
            "start": 551,
            "end": 698,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_81@0",
            "content": "Results and Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_81",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_82@0",
            "content": "Figure 2 shows the evolution of the performance of the IGRE grounder and the two baselines on the test set, as it gets exposed to more information (i.e., RE-designation pairs) over time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_82",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_82@1",
            "content": "In the intrinsic evaluation (domain model prediction), there is no significant difference between the three grounders considered.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_82",
            "start": 187,
            "end": 315,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_82@2",
            "content": "Yet, for extrinsic evaluation (reference resolution), we observe that IGRE outperforms the HEAD and EXISTS baselines over time (both a steeper and a smoother curve).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_82",
            "start": 317,
            "end": 481,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_82@3",
            "content": "By the end of the interaction, a t-test shows significant differences in IGRE's performance compared with both baselines (p-value of 0.01).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_82",
            "start": 483,
            "end": 621,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_83@0",
            "content": "Table 3 shows the best performance that each grounder achieved over time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_83",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_83@1",
            "content": "When analysing their performance on particular categories, we observe that C1 and S1 are equally hard to learn for grounders while R2 is easier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_83",
            "start": 74,
            "end": 217,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_84@0",
            "content": "Error Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_84",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_85@0",
            "content": "The HEAD and EXISTS baselines never acquire negative exemplars: e.g., information that a particular individual is not red.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_85",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_85@1",
            "content": "Figure 2 shows that this severely impacts their performance, and error analysis showed that in some experiment runs it leads to mode-collapse, with all denotations predicted to be the extensions of all symbols.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_85",
            "start": 123,
            "end": 332,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_85@2",
            "content": "On the other hand, IGRE is able to acquire and use negative examples from the truth functional meanings of the logical symbols, specifically from: (a) negation (\"not\"); (b) the presupposition triggers\"the N \", \"N of the M \", and \"all but N \" where N , and M are numbers and \"both\"; and (c) the use of \"every\" when it modifies the head noun.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_85",
            "start": 334,
            "end": 673,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_86@0",
            "content": "Conclusions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_86",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_87@0",
            "content": "In this work, we presented IGRE-a grounder that supports incremental learning of the mapping from symbols to visual features whenever the teacher presents a linguistically complex RE and its designation(s) in the visual scene.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_87",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_87@1",
            "content": "The grounder starts the learning process with no conceptualisation of the domain model, and so the learner must revise its hypothesis space of possible domain models as and when the teacher introduces new and unforeseen concepts via neologisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_87",
            "start": 227,
            "end": 470,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_87@2",
            "content": "We showed how exploiting the model-theoretic interpretation of the formal semantic representations of REs, and in particular the truth conditions of 'logical' words like quantifiers and negation, can inform the acquisition of noisy training exemplars that in turn guide learning-IGRE reasons about the likely denotations of symbols within an RE that aren't designated by that RE, and when sufficiently confident it exploits them to update its grounding model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_87",
            "start": 472,
            "end": 930,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_87@3",
            "content": "We showed that: 1) this grounding approach is more data efficient then a model that omits such observations and reasoning, using only the designated symbols; and 2) it is beneficial to exploit the logical consequences of the logical symbols, to gain even more data efficiency and training stability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_87",
            "start": 932,
            "end": 1230,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_87@4",
            "content": "In both cases, there was much to be gained from such reasoning because in contrast to the baselines, it contributes to acquiring negative exemplars: in other words, objects that get associated with not being red, for example.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_87",
            "start": 1232,
            "end": 1456,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_88@0",
            "content": "Future Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_88",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_89@0",
            "content": "IGRE uses a single source of data augmentation by acquiring noisy exemplars from symbols in oblique positions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_89",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_89@1",
            "content": "Further and parallel data gains may be obtained by exploring semi-supervised learning methods (Yarowsky, 1995;Delalleau et al., 2005).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_89",
            "start": 111,
            "end": 244,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_90@0",
            "content": "In this work, converting L-sentences to conjunctive normal form, which is an NP-hard problem, was a computational bottleneck.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_90",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_90@1",
            "content": "Future work needs to address this by either considering lifted inference methods (e.g., den Broeck et al. ( 2011)) or defining model counters that use L-sentences directly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_90",
            "start": 126,
            "end": 297,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_91@0",
            "content": "Finally, the purpose of IGRE is to aid ITL: i.e., the (incremental) updates to beliefs about symbol grounding should enhance learning to solve domain-level planning problems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_91",
            "start": 0,
            "end": 173,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_91@1",
            "content": "Future work needs to address this by using IGRE to learn planning tasks where the learner has the physical ability to execute certain actions but starts out unaware of domain concepts that define the goal and are critical to task success.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_91",
            "start": 175,
            "end": 412,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_91@2",
            "content": "The learner must not only use IGRE to interpret the teacher's feedback, but also learn decision making strategies, both on what to say (or ask) the teacher in their extended dialogue and what actions to perform in the environment.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_91",
            "start": 414,
            "end": 643,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_91@3",
            "content": "Furthermore, the static formal semantics that we used here should be replaced with a dynamic semantics (e.g., Groenendijk and Stokhof (1991); van der Sandt (1992); Asher and Lascarides (2003)), to account for how contextual salience influences truth and reference in dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_91",
            "start": 645,
            "end": 920,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_91@4",
            "content": "Following Batra et al. (2020), we plan to test the benefits of IGRE within a system that learns to solve planning problems that focus on rearrangement tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_91",
            "start": 922,
            "end": 1078,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_92@0",
            "content": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Zitnick, Devi Parikh, VQA: visual question answering, 2015-12-07, 2015 IEEE International Conference on Computer Vision, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_92",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_93@0",
            "content": "UNKNOWN, None, 2003, Logics of Conversation, Cambridge University Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_93",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_94@0",
            "content": "Amos Azaria, Jayant Krishnamurthy, Tom Mitchell, Instructable intelligent personal agent, 2016-02-12, Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_94",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_95@0",
            "content": "Jon Barwise, Robin Cooper, Generalized quantifiers and natural language, 1981, Linguistics and Philosophy, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_95",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_96@0",
            "content": "UNKNOWN, None, 1975, Rearrangement: A challenge for embodied AI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_96",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_97@0",
            "content": "Ben Bogin, Sanjay Subramanian, Matt Gardner, Jonathan Berant, Latent compositional representations improve systematic generalization in grounded question answering, 2021, Transactions of the Association of Computational Linguisticsl (TACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_97",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_98@0",
            "content": "Jos\u00e9 Miguel Cano Sant\u00edn, Simon Dobnik, Mehdi Ghanimifard, Fast visual grounding in interaction: bringing few-shot learning with neural networks to an interactive robot, 2020, Proceedings of the Probability and Meaning Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_98",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_99@0",
            "content": "Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, UNITER: universal image-text representation learning, 2020-08-23, Computer Vision -ECCV 2020 -16th European Conference, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_99",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_100@0",
            "content": "Ann Copestake, Slacker semantics: Why superficiality, dependency and avoidance of commitment can be the right way to go, 2009-03-30, EACL 2009, 12th Conference of the European Chapter of the Association for Computational Linguistics, Proceedings of the Conference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_100",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_101@0",
            "content": "Ann Copestake, D Flickinger, C Pollard, I Sag, Minimal recursion semantics: An introduction, 1997, Research on Language and Computation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_101",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_102@0",
            "content": "Nikolaus Correll, Kostas Bekris, Dmitry Berenson, Oliver Brock, Albert Causo, Kris Hauser, Kei Okada, Alberto Rodriguez, Joseph Romano, Peter Wurman, Analysis and observations from the first amazon picking challenge, 2018, IEEE Trans Autom. Sci. Eng, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_102",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_103@0",
            "content": "Samyak Datta, Karan Sikka, Anirban Roy, Karuna Ahuja, Devi Parikh, Ajay Divakaran, Align2ground: Weakly supervised phrase grounding guided by image-caption alignment, 2019-10-27, 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_103",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_104@0",
            "content": "Olivier Delalleau, Yoshua Bengio, Nicolas Roux, Efficient non-parametric function induction in semi-supervised learning, 2005-01-06, Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, AISTATS 2005, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_104",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_105@0",
            "content": "Guy Van Den Broeck, Nima Taghipour, Wannes Meert, Jesse Davis, Luc De Raedt, Lifted probabilistic inference by first-order knowledge compilation, 2011-07-16, IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_105",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_106@0",
            "content": "Jeffrey Dudek, Vu Phan, Moshe Vardi, ADDMC: weighted model counting with algebraic decision diagrams, 2020-02-07, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_106",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_107@0",
            "content": "Evelyn Fix, Joseph Hodges, Discriminatory analysis -nonparametric discrimination: Consistency properties, 1989, International Statistical Review, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_107",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_108@0",
            "content": "Dan Flickinger, On building a more effcient grammar by exploiting types, 2000, Nat. Lang. Eng, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_108",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_109@0",
            "content": "Akira Fukui, Dong Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, Marcus Rohrbach, Multimodal compact bilinear pooling for visual question answering and visual grounding, 2016-11-01, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, The Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_109",
            "start": 0,
            "end": 318,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_110@0",
            "content": "UNKNOWN, None, 2021, Annual review of control, robotics, and autonomous systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_110",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_111@0",
            "content": "J Groenendijk, M Stokhof, Dynamic predicate logic, 1991, Linguistics and Philosophy, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_111",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_112@0",
            "content": "UNKNOWN, None, 1999, The symbol grounding problem. CoRR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_112",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_113@0",
            "content": "Yordan Hristov, Daniel Angelov, Michael Burke, Alex Lascarides, Subramanian Ramamoorthy, Disentangled relational representations for explaining and learning from demonstration, 2019-10-30, 3rd Annual Conference on Robot Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_113",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_114@0",
            "content": "Yordan Hristov, Alex Lascarides, Subramanian Ramamoorthy, Interpretable latent spaces for learning from demonstration, 2018-10-31, of Proceedings of Machine Learning Research, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_114",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_115@0",
            "content": "Gao Huang, Zhuang Liu, Laurens Van Der Maaten, Kilian Weinberger, Densely connected convolutional networks, 2017-07-21, 2017 IEEE Conference on Computer Vision and Pattern Recognition, IEEE Computer Society.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_115",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_116@0",
            "content": "Yichen Jiang, Mohit Bansal, Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_116",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_117@0",
            "content": "UNKNOWN, None, , Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. 2021. MDETR -modulated detection for end-to-end multimodal understanding, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_117",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_118@0",
            "content": "UNKNOWN, None, 2010, Learning adaptive language interfaces through decomposition. CoRR, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_118",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_119@0",
            "content": "Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, Tamara Berg, Referitgame: Referring to objects in photographs of natural scenes, 2014-10-25, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_119",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_120@0",
            "content": "C Kennington, David Schlangen, A simple generative model of incremental reference resolution for situated dialogue, 2017, Comput. Speech Lang, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_120",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_121@0",
            "content": "Casey Kennington, Livia Dia, David Schlangen, A discriminative model for perceptuallygrounded incremental reference resolution, 2015-04-17, Proceedings of the 11th International Conference on Computational Semantics, IWCS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_121",
            "start": 0,
            "end": 223,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_122@0",
            "content": "Alexander Koller, Stefan Thater, The evolution of dominance constraint solvers, 2005, Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_122",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_123@0",
            "content": "UNKNOWN, None, 2017, Shapeworld -A new test methodology for multimodal language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_123",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_124@0",
            "content": "UNKNOWN, None, 2017, teractive task learning. IEEE Intelligent Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_124",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_125@0",
            "content": "Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, Yejin Choi, Jianfeng Gao, Oscar: Object-semantics aligned pre-training for vision-language tasks, 2020-08-23, Computer Vision -ECCV 2020 -16th European Conference, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_125",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_126@0",
            "content": "Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks, 2019-12-08, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_126",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_127@0",
            "content": "Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua Tenenbaum, Jiajun Wu, The neurosymbolic concept learner: Interpreting scenes, words, and sentences from natural supervision, 2019-05-06, 7th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_127",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_128@0",
            "content": "Cynthia Matuszek, Grounded language learning: Where robotics and NLP meet, 2018-07-13, Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_128",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_129@0",
            "content": "UNKNOWN, None, 1976, Language and perception, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_129",
            "start": 0,
            "end": 46,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_130@0",
            "content": "Alec Radford, Jong Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, Learning transferable visual models from natural language supervision, 2021-07, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_130",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_131@0",
            "content": "Bertrand Russell, Knowledge by acquaintance and knowledge by description, 1917, Mysticism and Logic, Longmans Green.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_131",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_132@0",
            "content": "Marko Samer, Stefan Szeider, Algorithms for propositional model counting, 2010, J. Discrete Algorithms, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_132",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_133@0",
            "content": "Haoyue Shi, Jiayuan Mao, Kevin Gimpel, Karen Livescu, Visually grounded neural syntax acquisition, 2019-07-28, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_133",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_134@0",
            "content": "Mohit Shridhar, David Hsu, Interactive visual grounding of referring expressions for humanrobot interaction, 2018-06-26, Robotics: Science and Systems XIV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_134",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_135@0",
            "content": "J Stuckler, D Holz, S Behnke, Robocup@home: Demonstrating everyday manipulation skills in robocup@home, 2012, IEEE Robotics Automation Magazine, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_135",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_136@0",
            "content": "Sanjay Subramanian, Ben Bogin, Nitish Gupta, Tomer Wolfson, Sameer Singh, Jonathan Berant, Matt Gardner, Obtaining faithful interpretations from compositional neural networks, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_136",
            "start": 0,
            "end": 320,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_137@0",
            "content": "UNKNOWN, None, 2019, Analyzing compositionality in visual question answering, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_137",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_138@0",
            "content": "Hao Tan, Mohit Bansal, LXMERT: learning cross-modality encoder representations from transformers, 2019-11-03, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_138",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_139@0",
            "content": "Leslie Valiant, The complexity of computing the permanent, 1979, Theor. Comput. Sci, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_139",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_140@0",
            "content": "Johan Van Benthem, Questions about quantifiers, 1984, J. Symb. Log, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_140",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_141@0",
            "content": "R Van Der,  Sandt, Presupposition projection as anaphora resolution, 1992, Journal of Semantics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_141",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_142@0",
            "content": "Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, Daan Wierstra, Matching networks for one shot learning, 2016-12-05, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_142",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_143@0",
            "content": "I Sida, Samuel Wang, Percy Ginn, Christopher Liang,  Manning, Naturalizing a programming language via interactive learning, 2017-07-30, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_143",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_144@0",
            "content": "I Sida, Percy Wang, Christopher Liang,  Manning, Learning language games through interaction, 2016-08-07, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_144",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_145@0",
            "content": "D Whitney, M Eldon, J Oberlin, S Tellex, Interpreting multimodal referring expressions in real time, 2016, Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_145",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_146@0",
            "content": "Ronald Williams, Simple statistical gradientfollowing algorithms for connectionist reinforcement learning, 1992, Mach. Learn, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_146",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_147@0",
            "content": "David Yarowsky, Unsupervised word sense disambiguation rivaling supervised methods, 1995, 33rd Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_147",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_148@0",
            "content": "Linwei Ye, Mrigank Rochan, Zhi Liu, Yang Wang, Cross-modal self-attention network for referring image segmentation, 2019-06-16, IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Computer Vision Foundation / IEEE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_148",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_149@0",
            "content": "Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, Josh Tenenbaum, Neural-symbolic VQA: disentangling reasoning from vision and language understanding, 2018-12-03, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_149",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_150@0",
            "content": "UNKNOWN, None, 2021, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_150",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_151@0",
            "content": ", Knowledge enhanced vision-language representations through scene graphs, 2021, Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, AAAI Press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_151",
            "start": 0,
            "end": 350,
            "label": {}
        },
        {
            "ix": "343-ARR_v1_152@0",
            "content": "Yanpeng Zhao, Ivan Titov, Visually grounded compound pcfgs, 2020-11-16, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "343-ARR_v1_152",
            "start": 0,
            "end": 201,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_1",
            "tgt_ix": "343-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_1",
            "tgt_ix": "343-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_2",
            "tgt_ix": "343-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_4",
            "tgt_ix": "343-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_5",
            "tgt_ix": "343-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_3",
            "tgt_ix": "343-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_3",
            "tgt_ix": "343-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_3",
            "tgt_ix": "343-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_3",
            "tgt_ix": "343-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_6",
            "tgt_ix": "343-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_8",
            "tgt_ix": "343-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_9",
            "tgt_ix": "343-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_10",
            "tgt_ix": "343-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_11",
            "tgt_ix": "343-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_7",
            "tgt_ix": "343-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_7",
            "tgt_ix": "343-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_7",
            "tgt_ix": "343-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_7",
            "tgt_ix": "343-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_7",
            "tgt_ix": "343-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_7",
            "tgt_ix": "343-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_12",
            "tgt_ix": "343-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_13",
            "tgt_ix": "343-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_13",
            "tgt_ix": "343-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_15",
            "tgt_ix": "343-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_16",
            "tgt_ix": "343-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_14",
            "tgt_ix": "343-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_14",
            "tgt_ix": "343-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_14",
            "tgt_ix": "343-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_14",
            "tgt_ix": "343-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_13",
            "tgt_ix": "343-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_17",
            "tgt_ix": "343-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_19",
            "tgt_ix": "343-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_20",
            "tgt_ix": "343-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_21",
            "tgt_ix": "343-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_22",
            "tgt_ix": "343-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_23",
            "tgt_ix": "343-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_18",
            "tgt_ix": "343-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_18",
            "tgt_ix": "343-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_18",
            "tgt_ix": "343-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_18",
            "tgt_ix": "343-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_18",
            "tgt_ix": "343-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_18",
            "tgt_ix": "343-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_18",
            "tgt_ix": "343-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_13",
            "tgt_ix": "343-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_24",
            "tgt_ix": "343-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_26",
            "tgt_ix": "343-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_27",
            "tgt_ix": "343-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_28",
            "tgt_ix": "343-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_29",
            "tgt_ix": "343-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_30",
            "tgt_ix": "343-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_31",
            "tgt_ix": "343-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_32",
            "tgt_ix": "343-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_33",
            "tgt_ix": "343-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_34",
            "tgt_ix": "343-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_35",
            "tgt_ix": "343-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_36",
            "tgt_ix": "343-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_36",
            "tgt_ix": "343-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_36",
            "tgt_ix": "343-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_37",
            "tgt_ix": "343-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_39",
            "tgt_ix": "343-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_40",
            "tgt_ix": "343-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_41",
            "tgt_ix": "343-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_42",
            "tgt_ix": "343-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_43",
            "tgt_ix": "343-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_38",
            "tgt_ix": "343-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_38",
            "tgt_ix": "343-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_38",
            "tgt_ix": "343-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_38",
            "tgt_ix": "343-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_38",
            "tgt_ix": "343-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_38",
            "tgt_ix": "343-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_38",
            "tgt_ix": "343-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_36",
            "tgt_ix": "343-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_44",
            "tgt_ix": "343-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_46",
            "tgt_ix": "343-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_45",
            "tgt_ix": "343-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_45",
            "tgt_ix": "343-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_45",
            "tgt_ix": "343-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_36",
            "tgt_ix": "343-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_47",
            "tgt_ix": "343-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_48",
            "tgt_ix": "343-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_48",
            "tgt_ix": "343-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_36",
            "tgt_ix": "343-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_49",
            "tgt_ix": "343-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_50",
            "tgt_ix": "343-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_50",
            "tgt_ix": "343-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_36",
            "tgt_ix": "343-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_51",
            "tgt_ix": "343-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_53",
            "tgt_ix": "343-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_54",
            "tgt_ix": "343-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_52",
            "tgt_ix": "343-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_52",
            "tgt_ix": "343-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_52",
            "tgt_ix": "343-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_52",
            "tgt_ix": "343-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_36",
            "tgt_ix": "343-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_55",
            "tgt_ix": "343-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_57",
            "tgt_ix": "343-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_58",
            "tgt_ix": "343-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_59",
            "tgt_ix": "343-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_56",
            "tgt_ix": "343-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_56",
            "tgt_ix": "343-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_56",
            "tgt_ix": "343-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_56",
            "tgt_ix": "343-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_56",
            "tgt_ix": "343-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_60",
            "tgt_ix": "343-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_62",
            "tgt_ix": "343-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_61",
            "tgt_ix": "343-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_61",
            "tgt_ix": "343-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_61",
            "tgt_ix": "343-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_63",
            "tgt_ix": "343-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_65",
            "tgt_ix": "343-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_66",
            "tgt_ix": "343-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_67",
            "tgt_ix": "343-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_64",
            "tgt_ix": "343-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_64",
            "tgt_ix": "343-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_64",
            "tgt_ix": "343-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_64",
            "tgt_ix": "343-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_64",
            "tgt_ix": "343-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_68",
            "tgt_ix": "343-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_69",
            "tgt_ix": "343-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_69",
            "tgt_ix": "343-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_71",
            "tgt_ix": "343-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_70",
            "tgt_ix": "343-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_70",
            "tgt_ix": "343-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_70",
            "tgt_ix": "343-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_69",
            "tgt_ix": "343-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_72",
            "tgt_ix": "343-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_73",
            "tgt_ix": "343-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_73",
            "tgt_ix": "343-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_69",
            "tgt_ix": "343-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_74",
            "tgt_ix": "343-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_76",
            "tgt_ix": "343-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_75",
            "tgt_ix": "343-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_75",
            "tgt_ix": "343-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_75",
            "tgt_ix": "343-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_77",
            "tgt_ix": "343-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_79",
            "tgt_ix": "343-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_78",
            "tgt_ix": "343-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_78",
            "tgt_ix": "343-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_78",
            "tgt_ix": "343-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_80",
            "tgt_ix": "343-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_82",
            "tgt_ix": "343-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_81",
            "tgt_ix": "343-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_81",
            "tgt_ix": "343-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_81",
            "tgt_ix": "343-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_81",
            "tgt_ix": "343-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_83",
            "tgt_ix": "343-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_84",
            "tgt_ix": "343-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_84",
            "tgt_ix": "343-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_86",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_85",
            "tgt_ix": "343-ARR_v1_86",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_86",
            "tgt_ix": "343-ARR_v1_87",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_86",
            "tgt_ix": "343-ARR_v1_87",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_86",
            "tgt_ix": "343-ARR_v1_88",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_87",
            "tgt_ix": "343-ARR_v1_88",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_89",
            "tgt_ix": "343-ARR_v1_90",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_90",
            "tgt_ix": "343-ARR_v1_91",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_88",
            "tgt_ix": "343-ARR_v1_89",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_88",
            "tgt_ix": "343-ARR_v1_90",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_88",
            "tgt_ix": "343-ARR_v1_91",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_88",
            "tgt_ix": "343-ARR_v1_89",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "343-ARR_v1_0",
            "tgt_ix": "343-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_1",
            "tgt_ix": "343-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_2",
            "tgt_ix": "343-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_2",
            "tgt_ix": "343-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_2",
            "tgt_ix": "343-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_2",
            "tgt_ix": "343-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_3",
            "tgt_ix": "343-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_4",
            "tgt_ix": "343-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_4",
            "tgt_ix": "343-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_4",
            "tgt_ix": "343-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_4",
            "tgt_ix": "343-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_4",
            "tgt_ix": "343-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_5",
            "tgt_ix": "343-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_5",
            "tgt_ix": "343-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_5",
            "tgt_ix": "343-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_5",
            "tgt_ix": "343-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_6",
            "tgt_ix": "343-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_6",
            "tgt_ix": "343-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_6",
            "tgt_ix": "343-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_6",
            "tgt_ix": "343-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_7",
            "tgt_ix": "343-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_8",
            "tgt_ix": "343-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_8",
            "tgt_ix": "343-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_8",
            "tgt_ix": "343-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_8",
            "tgt_ix": "343-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_8",
            "tgt_ix": "343-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_8",
            "tgt_ix": "343-ARR_v1_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_9",
            "tgt_ix": "343-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_9",
            "tgt_ix": "343-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_9",
            "tgt_ix": "343-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_10",
            "tgt_ix": "343-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_10",
            "tgt_ix": "343-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_10",
            "tgt_ix": "343-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_10",
            "tgt_ix": "343-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_10",
            "tgt_ix": "343-ARR_v1_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_11",
            "tgt_ix": "343-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_11",
            "tgt_ix": "343-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_11",
            "tgt_ix": "343-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_11",
            "tgt_ix": "343-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_11",
            "tgt_ix": "343-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_11",
            "tgt_ix": "343-ARR_v1_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_11",
            "tgt_ix": "343-ARR_v1_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_11",
            "tgt_ix": "343-ARR_v1_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_11",
            "tgt_ix": "343-ARR_v1_11@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_12",
            "tgt_ix": "343-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_12",
            "tgt_ix": "343-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_12",
            "tgt_ix": "343-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_13",
            "tgt_ix": "343-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_14",
            "tgt_ix": "343-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_15",
            "tgt_ix": "343-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_15",
            "tgt_ix": "343-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_16",
            "tgt_ix": "343-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_17",
            "tgt_ix": "343-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_18",
            "tgt_ix": "343-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_19",
            "tgt_ix": "343-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_19",
            "tgt_ix": "343-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_19",
            "tgt_ix": "343-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_20",
            "tgt_ix": "343-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_21",
            "tgt_ix": "343-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_22",
            "tgt_ix": "343-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_22",
            "tgt_ix": "343-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_23",
            "tgt_ix": "343-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_24",
            "tgt_ix": "343-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_24",
            "tgt_ix": "343-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_24",
            "tgt_ix": "343-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_25",
            "tgt_ix": "343-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_26",
            "tgt_ix": "343-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_27",
            "tgt_ix": "343-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_28",
            "tgt_ix": "343-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_29",
            "tgt_ix": "343-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_30",
            "tgt_ix": "343-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_31",
            "tgt_ix": "343-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_31",
            "tgt_ix": "343-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_32",
            "tgt_ix": "343-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_33",
            "tgt_ix": "343-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_34",
            "tgt_ix": "343-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_35",
            "tgt_ix": "343-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_36",
            "tgt_ix": "343-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_37",
            "tgt_ix": "343-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_37",
            "tgt_ix": "343-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_38",
            "tgt_ix": "343-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_39",
            "tgt_ix": "343-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_39",
            "tgt_ix": "343-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_39",
            "tgt_ix": "343-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_40",
            "tgt_ix": "343-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_41",
            "tgt_ix": "343-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_42",
            "tgt_ix": "343-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_43",
            "tgt_ix": "343-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_44",
            "tgt_ix": "343-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_45",
            "tgt_ix": "343-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_46",
            "tgt_ix": "343-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_46",
            "tgt_ix": "343-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_47",
            "tgt_ix": "343-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_48",
            "tgt_ix": "343-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_49",
            "tgt_ix": "343-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_49",
            "tgt_ix": "343-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_49",
            "tgt_ix": "343-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_49",
            "tgt_ix": "343-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_49",
            "tgt_ix": "343-ARR_v1_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_50",
            "tgt_ix": "343-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_51",
            "tgt_ix": "343-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_51",
            "tgt_ix": "343-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_51",
            "tgt_ix": "343-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_52",
            "tgt_ix": "343-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_53",
            "tgt_ix": "343-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_53",
            "tgt_ix": "343-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_53",
            "tgt_ix": "343-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_54",
            "tgt_ix": "343-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_55",
            "tgt_ix": "343-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_55",
            "tgt_ix": "343-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_56",
            "tgt_ix": "343-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_57",
            "tgt_ix": "343-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_58",
            "tgt_ix": "343-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_59",
            "tgt_ix": "343-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_60",
            "tgt_ix": "343-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_61",
            "tgt_ix": "343-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_62",
            "tgt_ix": "343-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_62",
            "tgt_ix": "343-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_63",
            "tgt_ix": "343-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_63",
            "tgt_ix": "343-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_63",
            "tgt_ix": "343-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_63",
            "tgt_ix": "343-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_63",
            "tgt_ix": "343-ARR_v1_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_64",
            "tgt_ix": "343-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_65",
            "tgt_ix": "343-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_65",
            "tgt_ix": "343-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_65",
            "tgt_ix": "343-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_65",
            "tgt_ix": "343-ARR_v1_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_65",
            "tgt_ix": "343-ARR_v1_65@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_66",
            "tgt_ix": "343-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_66",
            "tgt_ix": "343-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_67",
            "tgt_ix": "343-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_67",
            "tgt_ix": "343-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_67",
            "tgt_ix": "343-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_67",
            "tgt_ix": "343-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_67",
            "tgt_ix": "343-ARR_v1_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_67",
            "tgt_ix": "343-ARR_v1_67@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_67",
            "tgt_ix": "343-ARR_v1_67@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_67",
            "tgt_ix": "343-ARR_v1_67@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_68",
            "tgt_ix": "343-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_69",
            "tgt_ix": "343-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_70",
            "tgt_ix": "343-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_71",
            "tgt_ix": "343-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_71",
            "tgt_ix": "343-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_72",
            "tgt_ix": "343-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_73",
            "tgt_ix": "343-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_74",
            "tgt_ix": "343-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_74",
            "tgt_ix": "343-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_75",
            "tgt_ix": "343-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_76",
            "tgt_ix": "343-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_76",
            "tgt_ix": "343-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_77",
            "tgt_ix": "343-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_77",
            "tgt_ix": "343-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_78",
            "tgt_ix": "343-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_79",
            "tgt_ix": "343-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_79",
            "tgt_ix": "343-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_80",
            "tgt_ix": "343-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_80",
            "tgt_ix": "343-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_80",
            "tgt_ix": "343-ARR_v1_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_81",
            "tgt_ix": "343-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_82",
            "tgt_ix": "343-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_82",
            "tgt_ix": "343-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_82",
            "tgt_ix": "343-ARR_v1_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_82",
            "tgt_ix": "343-ARR_v1_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_83",
            "tgt_ix": "343-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_83",
            "tgt_ix": "343-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_84",
            "tgt_ix": "343-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_85",
            "tgt_ix": "343-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_85",
            "tgt_ix": "343-ARR_v1_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_85",
            "tgt_ix": "343-ARR_v1_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_86",
            "tgt_ix": "343-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_87",
            "tgt_ix": "343-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_87",
            "tgt_ix": "343-ARR_v1_87@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_87",
            "tgt_ix": "343-ARR_v1_87@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_87",
            "tgt_ix": "343-ARR_v1_87@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_87",
            "tgt_ix": "343-ARR_v1_87@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_88",
            "tgt_ix": "343-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_89",
            "tgt_ix": "343-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_89",
            "tgt_ix": "343-ARR_v1_89@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_90",
            "tgt_ix": "343-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_90",
            "tgt_ix": "343-ARR_v1_90@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_91",
            "tgt_ix": "343-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_91",
            "tgt_ix": "343-ARR_v1_91@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_91",
            "tgt_ix": "343-ARR_v1_91@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_91",
            "tgt_ix": "343-ARR_v1_91@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_91",
            "tgt_ix": "343-ARR_v1_91@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_92",
            "tgt_ix": "343-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_93",
            "tgt_ix": "343-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_94",
            "tgt_ix": "343-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_95",
            "tgt_ix": "343-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_96",
            "tgt_ix": "343-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_97",
            "tgt_ix": "343-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_98",
            "tgt_ix": "343-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_99",
            "tgt_ix": "343-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_100",
            "tgt_ix": "343-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_101",
            "tgt_ix": "343-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_102",
            "tgt_ix": "343-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_103",
            "tgt_ix": "343-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_104",
            "tgt_ix": "343-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_105",
            "tgt_ix": "343-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_106",
            "tgt_ix": "343-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_107",
            "tgt_ix": "343-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_108",
            "tgt_ix": "343-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_109",
            "tgt_ix": "343-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_110",
            "tgt_ix": "343-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_111",
            "tgt_ix": "343-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_112",
            "tgt_ix": "343-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_113",
            "tgt_ix": "343-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_114",
            "tgt_ix": "343-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_115",
            "tgt_ix": "343-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_116",
            "tgt_ix": "343-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_117",
            "tgt_ix": "343-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_118",
            "tgt_ix": "343-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_119",
            "tgt_ix": "343-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_120",
            "tgt_ix": "343-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_121",
            "tgt_ix": "343-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_122",
            "tgt_ix": "343-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_123",
            "tgt_ix": "343-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_124",
            "tgt_ix": "343-ARR_v1_124@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_125",
            "tgt_ix": "343-ARR_v1_125@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_126",
            "tgt_ix": "343-ARR_v1_126@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_127",
            "tgt_ix": "343-ARR_v1_127@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_128",
            "tgt_ix": "343-ARR_v1_128@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_129",
            "tgt_ix": "343-ARR_v1_129@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_130",
            "tgt_ix": "343-ARR_v1_130@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_131",
            "tgt_ix": "343-ARR_v1_131@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_132",
            "tgt_ix": "343-ARR_v1_132@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_133",
            "tgt_ix": "343-ARR_v1_133@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_134",
            "tgt_ix": "343-ARR_v1_134@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_135",
            "tgt_ix": "343-ARR_v1_135@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_136",
            "tgt_ix": "343-ARR_v1_136@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_137",
            "tgt_ix": "343-ARR_v1_137@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_138",
            "tgt_ix": "343-ARR_v1_138@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_139",
            "tgt_ix": "343-ARR_v1_139@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_140",
            "tgt_ix": "343-ARR_v1_140@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_141",
            "tgt_ix": "343-ARR_v1_141@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_142",
            "tgt_ix": "343-ARR_v1_142@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_143",
            "tgt_ix": "343-ARR_v1_143@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_144",
            "tgt_ix": "343-ARR_v1_144@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_145",
            "tgt_ix": "343-ARR_v1_145@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_146",
            "tgt_ix": "343-ARR_v1_146@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_147",
            "tgt_ix": "343-ARR_v1_147@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_148",
            "tgt_ix": "343-ARR_v1_148@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_149",
            "tgt_ix": "343-ARR_v1_149@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_150",
            "tgt_ix": "343-ARR_v1_150@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_151",
            "tgt_ix": "343-ARR_v1_151@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "343-ARR_v1_152",
            "tgt_ix": "343-ARR_v1_152@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1147,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "343-ARR",
        "version": 1
    }
}