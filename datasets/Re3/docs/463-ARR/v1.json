{
    "nodes": [
        {
            "ix": "463-ARR_v1_0",
            "content": "AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_2",
            "content": "Transformer-based pre-trained models with millions of parameters require large storage. Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters. In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed. AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer. Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias. The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pretrained models. We further find that Adapter-Bias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "463-ARR_v1_4",
            "content": "While large pre-trained language models (PLMs) reached state-of-the-art results on natural language processing (NLP) tasks, PLMs require updating all parameters and storing the fully fine-tuned model for each downstream task. These requirements have led to difficulties in real-world applications. Moreover, fine-tuning PLMs on lowresource datasets is subject to instabilities.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_5",
            "content": "To tackle these shortcomings, Adapters (Houlsby et al., 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al., 2017) has been proposed. Instead of full fine-tuning the whole model, Adapters introduces extra tunable weights and freezes the original parameters of PLM. Adapters demonstrated comparable performance with fully fine-tuning the entire model. Although Adapters solve the problem of the PLM's massive parameters, researchers are curious about how many more parameters are required Figure 1: Overview of the main concept of our work compared to BitFit (Ben Zaken et al., 2021). Left: Bit-Fit tends to add the same representation shift to different tokens. Right: Our work applies different representation shifts to tokens considering their importance to the downstream task and their characteristics. The shifts of the input words that are more task-related is more significant than that of other tokens. For example, in SST-2 (Socher et al., 2013), which is a semantic task, the representation shifts of the semantic words, such as \"kind\" and \"worse\", are larger than that of other words.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_6",
            "content": "to reach state-of-the-art performance on standard NLP tasks. The results in Houlsby et al. (2019) have shown that the performance on GLUE benchmark (Wang et al., 2018) is almost the same when removing the Adapters in the first layers, which indicates that not every adapter is useful. It leaves the question of whether adapters can be even more parameter-efficient.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_7",
            "content": "To develop practical and memory-efficient adapters, Diff pruning (Guo et al., 2020) enables parameter-efficient transfer learning that scales well with new tasks. The approach learns a taskspecific \"diff\" vector that extends the original pretrained parameters and encourages the sparsity of the vector through L 0 -norm regularization. Another approach is BitFit (Ben Zaken et al., 2021), which shows that with small-to-medium training data, fine-tuning only a subset of the bias terms of pre-trained BERT models (Devlin et al., 2018) is competitive with fine-tuning the entire model. The central concept of these approaches is to add task-specific shifts to each output representation of the PLM layers so as to adapt to different tasks. In the previous works, Ben Zaken et al. (2021); Guo et al. (2020) both add the same shifts to the output representation regardless of which token is more relevant to the task. However, considering some specific tokens might be more critical to a particular task, the representation can better adapt to the downstream task under a limited amount of parameters if these shifts are based on the input tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_8",
            "content": "Based on this concept, in this study, we add token-dependent biases to the shifts by proposing AdapterBias, which consists of a vector and a linear layer (L \u03b1 ). The vector represents the task-specific shift, and L \u03b1 produces the weights for input tokens. Thus, with the vector and the weights, AdapterBias can add a token-dependent shift to the transformer layer. Since the concept of BitFit (Ben Zaken et al., 2021) is similar to AdapterBias by adding a shift to the representation, we demonstrate the difference between BitFit and AdapterBias in Figure 1. Bit-Fit assigns identical shifts to all the tokens, while AdapterBias adds more significant shifts to the tokens related to the task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_9",
            "content": "With fewer trainable parameters required, AdapterBias achieves comparable performance on the GLUE benchmark with Houlsby et al. (2019); Pfeiffer et al. (2020a); Guo et al. (2020); Ben Zaken et al. (2021). We further decrease the parameters of AdapterBias in different ways, including partial weight-sharing in AdapterBias and adding L 0 -norm regularization. Finally, AdapterBias has better interpretability due to its simplicity. We use different tools, including word cloud and PCA (Jolliffe, 2002), to visualize what AdapterBias has learned, and we found that the proposed approach automatically learns to assign larger representation shifts to the task-related tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "463-ARR_v1_11",
            "content": "For NLP tasks, adapters are introduced for the transformer architecture. A set of adapter parameters was added at each transformer layer, which is mostly bottleneck architectures Houlsby et al. (2019). By keeping the output dimension identical, they cause no change to the structure or parameters of the original model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_12",
            "content": "Adapters quickly gained popularity in NLP with various applications. For multi-task learning (Caruana, 1997;Zhang and Yang, 2017;Liu et al., 2019b), a projected self-attention layer is proposed by Stickland and Murray (2019), while Bapna et al. (2019) proposed an additional layer norm suitable for machine translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_13",
            "content": "Besides the applications of adapters, researchers are also dedicated to improving their performance. Based on the architecture introduced by Houlsby et al. (2019), AdapterFusion (Pfeiffer et al., 2020a) leveraged knowledge from multiple tasks with a new two-stage learning algorithm. Despite the recent popularity of these methods, they still train a relatively large number of training parameters.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_14",
            "content": "Recently, studies start to focus on improving the parameter-efficiency of adapters. Diff-pruning (Guo et al., 2020) achieves parameter efficiency by adding a sparse, task-specific difference-vector to the fixed original parameters. The vector is adaptively pruned during training with a differentiable approximation to the L 0 -norm penalty to encourage sparsity. R\u00fcckl\u00e9 et al. (2020) introduced Adap-terDrop, which has been recently integrated into AdapterHub (Pfeiffer et al., 2020b) by removing adapters from lower transformer layers during training and inference, which can dynamically reduce the computational cost. Mahabadi et al. (2021) proposed Compacter, which improved the trade-off between performance and trainable parameters per task with low-rank optimization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_15",
            "content": "On the other hand, without modifying the architecture of the PLM, BitFit (Ben Zaken et al., 2021) shows that fine-tuning only the bias terms of a large PLM is also competitive with fine-tuning the entire model. Fine-tuning only the bias terms can be considered as adding a task-specific shift to the token representation. BitFit is most similar to our work. While in BitFit, the shifts added to all the representations are exactly the same for all input tokens, in our work, the shifts are token-dependent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_16",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "463-ARR_v1_17",
            "content": "In this section, we present AdapterBias, an efficient way to adapt large-scale PLMs. In order to better adapt to different downstream tasks, the adapter module should be token-specific. AdapterBias produces a suitable weight of the bias based on the input tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_18",
            "content": "Problem Formulation We consider the general problem of fine-tuning PLMs, where the training data D = (x i , y i ) N n=1 is given. Assume that given",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_19",
            "content": "AdapterBias",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "463-ARR_v1_20",
            "content": "The architecture of AdapterBias is shown in the right part of Figure 2. AdapterBias consists of two modules: a vector (v) and a linear layer (L \u03b1 ). v is a task-specific shift added to the output of each transformer layer. Since some tokens are more important to some tasks, these tokens should be assigned larger representation shifts than other tokens. The linear layer (L \u03b1 ) produces a token-dependent weight vector \u03b1 = [\u03b1 1 , \u03b1 2 . . . \u03b1 m ] T , where \u03b1 i is the weight of the ith token's representation shift. By applying the token-specific weight to the taskspecific representation shift (v), AdapterBias can focus on the tokens that are more important to the task and is able to adapt to different downstream tasks efficiently.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_21",
            "content": "We define the output of AdapterBias as the bias (B), which is the outer product of v and the learned weights vector \u03b1. When the dimension of the token's representation is r with with m input tokens, the function can be defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_22",
            "content": "B = v \u2297 \u03b1 T = \u03b1 1 v \u03b1 2 v . . . \u03b1 m v (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_23",
            "content": "where v \u2208 R r , \u03b1 \u2208 R m , and B \u2208 R r\u00d7m .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_24",
            "content": "To further elaborate on the details of Adapter-Bias, we give an example of how AdapterBias produces B and how B adds to the transformer layer. In Figure 3, we assume that there are three representation outputs (r 1 , r 2 , r 3 ) after the first layer normalization. The dimension of r 1 , r 2 and r 3 is the dimension of the 2nd feedforward layer, while the dimension of the linear layer (L \u03b1 ) is the output dimension of the first feed-forward layer with the token representation (r 1 , r 2 , r 3 ) as its inputs. The linear layer (L \u03b1 ) produces \u03b1, where \u03b1 \u2208 R 3 . The blocks in different colors represent the difference of the weights (\u03b1 1 , \u03b1 2 , \u03b1 3 ). Take BERT-base for example, after performing outer product with the weights vector \u03b1 and the vector (v), the dimension of B becomes 768 \u00d7 3. For example, b 1 , the first column of B, is the shift for the first token representation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_25",
            "content": "Further improvement on parameter-efficiency of AdapterBias",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "463-ARR_v1_26",
            "content": "In this section, we experiment on two ways to make AdapterBias more parameter efficient. One is partial weight-sharing of AdapterBias among transformer layers, another is enforcing the weights of the linear layer (L \u03b1 ) to be sparse by utilizing L 0norm penalty.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_27",
            "content": "Cross-layer parameters sharing in AdapterBias",
            "ntype": "title",
            "meta": {
                "section": "3.2.1"
            }
        },
        {
            "ix": "463-ARR_v1_28",
            "content": "Redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important. In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important. In addition, sharing parameters of the Adapter across layers leads to a comparatively small drop in performance in some tasks. In light of the above information, we further reduce the number of parameters required for each task by partially sharing the weights of the adapters across all transformer layers. The experimental results are discussed at Section 4.6.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_29",
            "content": "L 0 regularization in AdapterBias",
            "ntype": "title",
            "meta": {
                "section": "3.2.2"
            }
        },
        {
            "ix": "463-ARR_v1_30",
            "content": "Sparsity has been utilized in various parameterefficient methods. For applications in NLP tasks, Diff-pruning (Guo et al., 2020) learns a sparse vector added to the whole PLM with L 0 -norm penalty.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_31",
            "content": "Inspired by their work, we further apply L 0 -norm regularization to L \u03b1 in the AdapterBias module, aiming to encourage the sparsity of L \u03b1 . We choose to drop L \u03b1 because it contributes most of the parameters in AdapterBias. Encouraging its sparsity can further increase the parameter efficiency. Note that we specifically apply L 0 regularization in Section 4.6.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_32",
            "content": "In AdapterBias, we add L 0 -norm penalty to the linear layer (L \u03b1 ). The optimization problem can be expressed as,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_33",
            "content": "min \u03b8 L(D; \u03b8, \u03b8 ) + \u03bb \u03b8 L\u03b1 0 ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_34",
            "content": "where L(D; \u2022) represents the original loss with training data D. \u03bb is the hyperparameter for L 0norm penalty. Note that \u03b8 represents trainable parameters and \u03b8 L\u03b1 represents the parameters of L \u03b1 in AdapterBias. Following the work of Diffpruning, we utilize a relaxed mask vector (Louizos et al., 2017) with a stretched Hard-Concrete distribution (Jang et al., 2016;Maddison et al., 2016) to encourage L 0 sparsity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_35",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "463-ARR_v1_36",
            "content": "In this section, we evaluate the effectiveness of our proposed adapter module in NLP training tasks, and provide the analysis of what AdapterBias has learned in different tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_37",
            "content": "Results on GLUE",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "463-ARR_v1_38",
            "content": "In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters (Houlsby et al., 2019), Diff-pruning (Guo et al., 2020), andBitFit (Ben Zaken et al., 2021) 2019). Although Diff-pruning (Guo et al., 2020) has the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM. Thus, Diff-pruning requires 340M trainable parameters of BERT-large during the training stage, while AdapterBias only trains 0.23M parameters. Furthermore, AdapterBias achieves comparable performance with BitFit with fewer parameters needed per task. This shows that AdapterBias is a worthwhile targeted fine-tuning method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_39",
            "content": "Different base models",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "463-ARR_v1_40",
            "content": "To analyze how well this approach generalizes to different PLMs on different models of AdapterBias, as shown in Table 2, we apply AdapterBias in different transformer-based PLMs, including BERT-base (BB), BERT-large (BL), RoBERTa-base (RoB), and RoBERTa-large (RoL), on the GLUE benchmark. All results are scored by the GLUE evaluate server. Compared with BitFit, In Table 2, not only can AdapterBias perform well on BERT but also achieve competitive performance on larger PLMs such as RoBERTa.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_41",
            "content": "Size of training data",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "463-ARR_v1_42",
            "content": "In the previous experimental results, we observe that AdapterBias tends to have higher performance on tasks with a smaller amount of data (i.e. show that AdapterBias has the ability to outperform fine-tuning the whole PLM with small-to-medium data size, similarly to BitFit.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_43",
            "content": "Investigation on the effectiveness of token dependent representation shift",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "463-ARR_v1_44",
            "content": "Different from BitFit (Ben Zaken et al., 2021), where the bias terms in all transformer layers are tuned, we claim that the bias added to the representation should be token-dependent, and proposed AdapterBias based on this concept. We conduct ablation studies to verify this claim. In this experiment, the linear layer (L \u03b1 ) in AdapterBias that produces the token-dependent weights vector (\u03b1) is removed; that is, only the v is trained. All shifts added to the representation outputs are identical within the same transformer layer. The experiments are conducted with BERT-base model. We report the test scores on the GLUE benchmark in Table 3. The performance of AdapterBias without the linear layer (L \u03b1 ) dramatically decreases. Without L \u03b1 , it is hard for the vector (v) to adapt to different downstream tasks. This result demonstrates the importance of L \u03b1 . In other words, assigning different shifts to different token representations improves the performance of the method.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_45",
            "content": "Improving the parameter efficiency of AdapterBias",
            "ntype": "title",
            "meta": {
                "section": "4.6"
            }
        },
        {
            "ix": "463-ARR_v1_46",
            "content": "We further apply two additional methods to AdapterBias to enhance its parameter efficiency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_47",
            "content": "Experiments are conducted to see whether Adapter-Bias can be more parameter-efficient by sharing its components across all layers. Moreover, we experiment on adding L 0 -norm regularization during the training stage to encourage the sparsity of AdapterBias.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_48",
            "content": "Sharing components in AdapterBias",
            "ntype": "title",
            "meta": {
                "section": "4.6.1"
            }
        },
        {
            "ix": "463-ARR_v1_49",
            "content": "In this experiment, we conduct an ablation study of partial weight-sharing in the AdapterBias module. In Table 4, we share components of Adapter-Bias among different transformer layers. Share v represents sharing v across all transformer layers, while Share L \u03b1 means sharing the linear layer (L \u03b1 ). Share v+L \u03b1 denotes sharing one Adapter-Bias across all transformer layers. As can be seen in Table 4, the performance of Share L \u03b1 stands out among other partial weight-sharing methods, while Share v leads to a poor performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_50",
            "content": "From the experiments above, we conclude that the linear layer (L \u03b1 ) captures general task information by learning the weights of the bias for different tokens. Thus, sharing L \u03b1 across all layers results in better performance compared to other components. The vector module (v) in AdapterBias aims to learn local information in each transformer layer. If v among different transformer layers are shared, the performance drops dramatically. This might be due to a failure of v to learn general information which can be adapted to each individual transformer layer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_51",
            "content": "L 0 -norm regularization in AdapterBias",
            "ntype": "title",
            "meta": {
                "section": "4.6.2"
            }
        },
        {
            "ix": "463-ARR_v1_52",
            "content": "We observed that many of the trained parameters in L \u03b1 have values that are extremely close to zero after tuning on downstream tasks, which might cause redundancy of the parameters. To further encourage the sparsity of AdapterBias, we add L 0norm regularization to L \u03b1 during the training stage.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_53",
            "content": "In Table 5, we use BERT-base (BB) and BERTlarge (BL) for the PLM. We compare the performance of fine-tuning, the original AdapterBias, and the one trained with L 0 -norm regularization. The experiment shows that adding L 0 -norm regularization during the training step improves the performance on 7 out of 9 tasks in BERT-base models. However, the performance did not improve when applied to BERT-large models. As for the parameter efficiency of applying L 0 -norm penalty, the linear layer (L \u03b1 ) with L 0 -norm penalty saves about 17% parameter on average compared to the original AdapterBias. The details of the reduced parameters of each task are shown in Appendix A.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_54",
            "content": "What AdapterBias learns",
            "ntype": "title",
            "meta": {
                "section": "4.7"
            }
        },
        {
            "ix": "463-ARR_v1_55",
            "content": "AdapterBias has good interpretability due to its simplicity. Compared to our similar work Bit-Fit (Ben Zaken et al., 2021), where the shifts are identical for all tokens, AdapterBias adds tokendependent shifts to the output representation. By observing these token-dependent shifts, we analyze what AdapterBias learns when adapting to downstream tasks. In AdapterBias, the linear layer (L \u03b1 ) produces a weights vector \u03b1 for representation shifts, therefore, the average absolute value of vector \u03b1 can give us a look at the shifting amount in the transformer layers when adapting to downstream tasks. In Figure 5, the layers are ordered from lower to upper. From the experimental result, we find that the weight in each layer is considerably different in different tasks in general.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_56",
            "content": "CoLA (Warstadt et al., 2019) is a syntactic task that consists of English acceptability judgments in the GLUE benchmark. As shown in Figure 5, its average shift at the ninth layer is the highest among all layers, which is quite different from the others. We speculate that the ninth layer has the ability to extract the syntactic information, leading AdapterBias to add the largest shift in this layer. Our experiment has a similar observation with the work of Jawahar et al. (2019). Jawahar et al. (2019) also observe on a syntactic task with BShift (Conneau et al., 2018) that the ninth layer of BERT embeds a rich hierarchy of syntactic information. (Jawahar et al., 2019) Moreover, we observe similar distributions between specific tasks. For instance, RTE (Giampiccolo et al., 2007;Bentivogli et al., 2009) and MNLI (Williams et al., 2017), where both tasks recognize textual entailment, have higher values in the upper layers than those in the lower ones.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_57",
            "content": "Based on these findings, we find that Adapter-Bias assigns suitable representation shifts in different tasks. For tasks with similar objectives, AdapterBias tends to add similar representation shifts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_58",
            "content": "Which kind of word does L \u03b1 focus on",
            "ntype": "title",
            "meta": {
                "section": "4.7.2"
            }
        },
        {
            "ix": "463-ARR_v1_59",
            "content": "Since \u03b1 i represents the weight of the representation shift for ith token in a transformer layer, we can observe the significance of ith token from the summation of \u03b1 i in all the transformer layers. Special tokens, including [CLS], [SEP], and [PAD], are not included for analysis. We use the validation sets of CoLA and SST-2, and word cloud is used for visualizations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_60",
            "content": "In Figure 6, we visualize all words in the validation data of CoLA. The result shows that Adapter-Bias focuses more on reflexive pronouns, such as yourself, himself, and myself. This is because there are many incorrect sentences with misused reflexive pronouns, such as \"He washed yourself.\"",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_61",
            "content": "In Figure 7, we visualize all words in the validation data of SST-2. The result shows that Adapter-Bias focuses more on adjectives, such as \"bad\", \"awful\", and \"worst\". SST-2 is a binary sentiment analysis dataset, which classifies movie reviews into positive and negative classes. AdapterBias learns that adjectives often constitute a crucial factor in sentiment analysis during tuning, and adds larger shifts to these adjective tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_62",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "463-ARR_v1_63",
            "content": "In this study, we present AdapterBias. By adding token-dependent representation shifts to the PLM, AdapterBias shows competitive results even though it uses far fewer parameters than the existing methods. Through extensive experiments, not only does AdapterBias reaches competitive results on the GLUE benchmark, but it also obtains good performance on small-to-medium datasets. In addition, we demonstrate the robustness of AdapterBias to different PLMs. Finally, we provide analysis on what AdapterBias learns by comparing \u03b1, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information. Our study overturns previous architectures of adapters by proposing a simple adapter that can produce suitable representation shifts for different tokens.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "463-ARR_v1_64",
            "content": "UNKNOWN, None, 2019, Simple, scalable adaptation for neural machine translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Simple, scalable adaptation for neural machine translation",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_65",
            "content": "UNKNOWN, None, 2021, Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_66",
            "content": "UNKNOWN, None, 2009, The fifth pascal recognizing textual entailment challenge, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2009",
                "pub_title": "The fifth pascal recognizing textual entailment challenge",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_67",
            "content": "Rich Caruana, Multitask learning, 1997, Machine learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Rich Caruana"
                ],
                "title": "Multitask learning",
                "pub_date": "1997",
                "pub_title": "Machine learning",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_68",
            "content": "UNKNOWN, None, 2017, Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_69",
            "content": "UNKNOWN, None, 2018, What you can cram into a single vector: Probing sentence embeddings for linguistic properties, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "What you can cram into a single vector: Probing sentence embeddings for linguistic properties",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_70",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_71",
            "content": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, William B Dolan, The third pascal recognizing textual entailment challenge, 2007, Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Danilo Giampiccolo",
                    "Bernardo Magnini",
                    "Ido Dagan",
                    "William B Dolan"
                ],
                "title": "The third pascal recognizing textual entailment challenge",
                "pub_date": "2007",
                "pub_title": "Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_72",
            "content": "UNKNOWN, None, 2020, Parameter-efficient transfer learning with diff pruning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Parameter-efficient transfer learning with diff pruning",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_73",
            "content": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, Parameter-efficient transfer learning for nlp, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Neil Houlsby",
                    "Andrei Giurgiu",
                    "Stanislaw Jastrzebski",
                    "Bruna Morrone",
                    "Quentin De Laroussilhe",
                    "Andrea Gesmundo",
                    "Mona Attariyan",
                    "Sylvain Gelly"
                ],
                "title": "Parameter-efficient transfer learning for nlp",
                "pub_date": "2019",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "463-ARR_v1_74",
            "content": "UNKNOWN, None, 2016, Categorical reparameterization with gumbel-softmax, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Categorical reparameterization with gumbel-softmax",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_75",
            "content": "Ganesh Jawahar, Beno\u00eet Sagot, Djam\u00e9 Seddah, What does bert learn about the structure of language, 2019, ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Ganesh Jawahar",
                    "Beno\u00eet Sagot",
                    "Djam\u00e9 Seddah"
                ],
                "title": "What does bert learn about the structure of language",
                "pub_date": "2019",
                "pub_title": "ACL 2019-57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_76",
            "content": "UNKNOWN, None, 2002, Springer series in statistics. Principal component analysis, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2002",
                "pub_title": "Springer series in statistics. Principal component analysis",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_77",
            "content": "UNKNOWN, None, 2019, Revealing the dark secrets of bert, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Revealing the dark secrets of bert",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_78",
            "content": "UNKNOWN, None, 2019, Linguistic knowledge and transferability of contextual representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Linguistic knowledge and transferability of contextual representations",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_79",
            "content": "UNKNOWN, None, 2019, Multi-task deep neural networks for natural language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Multi-task deep neural networks for natural language understanding",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_80",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_81",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_82",
            "content": "UNKNOWN, None, 2017, Learning sparse neural networks through l_0 regularization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Learning sparse neural networks through l_0 regularization",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_83",
            "content": "UNKNOWN, None, 2016, The concrete distribution: A continuous relaxation of discrete random variables, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "The concrete distribution: A continuous relaxation of discrete random variables",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_84",
            "content": "UNKNOWN, None, 2021, Compacter: Efficient lowrank hypercomplex adapter layers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Compacter: Efficient lowrank hypercomplex adapter layers",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_85",
            "content": "UNKNOWN, None, 2020, Adapterfusion: Non-destructive task composition for transfer learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Adapterfusion: Non-destructive task composition for transfer learning",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_86",
            "content": "UNKNOWN, None, 2020, Adapterhub: A framework for adapting transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Adapterhub: A framework for adapting transformers",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_87",
            "content": "UNKNOWN, None, 2016, Squad: 100,000+ questions for machine comprehension of text, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Squad: 100,000+ questions for machine comprehension of text",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_88",
            "content": "UNKNOWN, None, , Nils Reimers, and Iryna Gurevych. 2020. Adapterdrop: On the efficiency of adapters in transformers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Nils Reimers, and Iryna Gurevych. 2020. Adapterdrop: On the efficiency of adapters in transformers",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_89",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher,  Manning, Y Andrew, Christopher Ng,  Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 conference on empirical methods in natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Richard Socher",
                    "Alex Perelygin",
                    "Jean Wu",
                    "Jason Chuang",
                    "D Christopher",
                    " Manning",
                    "Y Andrew",
                    "Christopher Ng",
                    " Potts"
                ],
                "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
                "pub_date": "2013",
                "pub_title": "Proceedings of the 2013 conference on empirical methods in natural language processing",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_90",
            "content": "Asa Stickland, Iain Murray, Bert and pals: Projected attention layers for efficient adaptation in multi-task learning, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Asa Stickland",
                    "Iain Murray"
                ],
                "title": "Bert and pals: Projected attention layers for efficient adaptation in multi-task learning",
                "pub_date": "2019",
                "pub_title": "International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "463-ARR_v1_91",
            "content": "UNKNOWN, None, 2019, Bert rediscovers the classical nlp pipeline, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Bert rediscovers the classical nlp pipeline",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_92",
            "content": "UNKNOWN, None, 2017, Attention is all you need, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Attention is all you need",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_93",
            "content": "UNKNOWN, None, 2018, Glue: A multi-task benchmark and analysis platform for natural language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_94",
            "content": "Alex Warstadt, Amanpreet Singh, Samuel R Bowman, Neural network acceptability judgments, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Alex Warstadt",
                    "Amanpreet Singh",
                    "Samuel R Bowman"
                ],
                "title": "Neural network acceptability judgments",
                "pub_date": "2019",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_95",
            "content": "UNKNOWN, None, 2017, A broad-coverage challenge corpus for understanding inference, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "A broad-coverage challenge corpus for understanding inference",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_96",
            "content": "UNKNOWN, None, 2019, Huggingface's transformers: State-of-the-art natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Huggingface's transformers: State-of-the-art natural language processing",
                "pub": null
            }
        },
        {
            "ix": "463-ARR_v1_97",
            "content": "UNKNOWN, None, 2017, A survey on multitask learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "A survey on multitask learning",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "463-ARR_v1_0@0",
            "content": "AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_0",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_2@0",
            "content": "Transformer-based pre-trained models with millions of parameters require large storage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_2",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_2@1",
            "content": "Recent approaches tackle this shortcoming by training adapters, but these approaches still require a relatively large number of parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_2",
            "start": 88,
            "end": 226,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_2@2",
            "content": "In this study, AdapterBias, a surprisingly simple yet effective adapter architecture, is proposed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_2",
            "start": 228,
            "end": 325,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_2@3",
            "content": "AdapterBias adds a token-dependent shift to the hidden output of transformer layers to adapt to downstream tasks with only a vector and a linear layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_2",
            "start": 327,
            "end": 477,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_2@4",
            "content": "Extensive experiments are conducted to demonstrate the effectiveness of AdapterBias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_2",
            "start": 479,
            "end": 562,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_2@5",
            "content": "The experiments show that our proposed method can dramatically reduce the trainable parameters compared to the previous works with a minimal decrease in task performances compared with fine-tuned pretrained models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_2",
            "start": 564,
            "end": 777,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_2@6",
            "content": "We further find that Adapter-Bias automatically learns to assign more significant representation shifts to the tokens related to the task in consideration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_2",
            "start": 779,
            "end": 933,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_4@0",
            "content": "While large pre-trained language models (PLMs) reached state-of-the-art results on natural language processing (NLP) tasks, PLMs require updating all parameters and storing the fully fine-tuned model for each downstream task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_4",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_4@1",
            "content": "These requirements have led to difficulties in real-world applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_4",
            "start": 226,
            "end": 296,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_4@2",
            "content": "Moreover, fine-tuning PLMs on lowresource datasets is subject to instabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_4",
            "start": 298,
            "end": 376,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_5@0",
            "content": "To tackle these shortcomings, Adapters (Houlsby et al., 2019), a more parameter-efficient alternative training strategy for the transformer architecture (Vaswani et al., 2017) has been proposed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_5",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_5@1",
            "content": "Instead of full fine-tuning the whole model, Adapters introduces extra tunable weights and freezes the original parameters of PLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_5",
            "start": 195,
            "end": 324,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_5@2",
            "content": "Adapters demonstrated comparable performance with fully fine-tuning the entire model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_5",
            "start": 326,
            "end": 410,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_5@3",
            "content": "Although Adapters solve the problem of the PLM's massive parameters, researchers are curious about how many more parameters are required Figure 1: Overview of the main concept of our work compared to BitFit (Ben Zaken et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_5",
            "start": 412,
            "end": 643,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_5@4",
            "content": "Left: Bit-Fit tends to add the same representation shift to different tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_5",
            "start": 645,
            "end": 721,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_5@5",
            "content": "Right: Our work applies different representation shifts to tokens considering their importance to the downstream task and their characteristics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_5",
            "start": 723,
            "end": 866,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_5@6",
            "content": "The shifts of the input words that are more task-related is more significant than that of other tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_5",
            "start": 868,
            "end": 970,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_5@7",
            "content": "For example, in SST-2 (Socher et al., 2013), which is a semantic task, the representation shifts of the semantic words, such as \"kind\" and \"worse\", are larger than that of other words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_5",
            "start": 972,
            "end": 1155,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_6@0",
            "content": "to reach state-of-the-art performance on standard NLP tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_6",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_6@1",
            "content": "The results in Houlsby et al. (2019) have shown that the performance on GLUE benchmark (Wang et al., 2018) is almost the same when removing the Adapters in the first layers, which indicates that not every adapter is useful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_6",
            "start": 61,
            "end": 283,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_6@2",
            "content": "It leaves the question of whether adapters can be even more parameter-efficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_6",
            "start": 285,
            "end": 364,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_7@0",
            "content": "To develop practical and memory-efficient adapters, Diff pruning (Guo et al., 2020) enables parameter-efficient transfer learning that scales well with new tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_7",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_7@1",
            "content": "The approach learns a taskspecific \"diff\" vector that extends the original pretrained parameters and encourages the sparsity of the vector through L 0 -norm regularization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_7",
            "start": 163,
            "end": 334,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_7@2",
            "content": "Another approach is BitFit (Ben Zaken et al., 2021), which shows that with small-to-medium training data, fine-tuning only a subset of the bias terms of pre-trained BERT models (Devlin et al., 2018) is competitive with fine-tuning the entire model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_7",
            "start": 336,
            "end": 583,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_7@3",
            "content": "The central concept of these approaches is to add task-specific shifts to each output representation of the PLM layers so as to adapt to different tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_7",
            "start": 585,
            "end": 737,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_7@4",
            "content": "In the previous works, Ben Zaken et al. (2021); Guo et al. (2020) both add the same shifts to the output representation regardless of which token is more relevant to the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_7",
            "start": 739,
            "end": 913,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_7@5",
            "content": "However, considering some specific tokens might be more critical to a particular task, the representation can better adapt to the downstream task under a limited amount of parameters if these shifts are based on the input tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_7",
            "start": 915,
            "end": 1143,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_8@0",
            "content": "Based on this concept, in this study, we add token-dependent biases to the shifts by proposing AdapterBias, which consists of a vector and a linear layer (L \u03b1 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_8",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_8@1",
            "content": "The vector represents the task-specific shift, and L \u03b1 produces the weights for input tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_8",
            "start": 162,
            "end": 254,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_8@2",
            "content": "Thus, with the vector and the weights, AdapterBias can add a token-dependent shift to the transformer layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_8",
            "start": 256,
            "end": 363,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_8@3",
            "content": "Since the concept of BitFit (Ben Zaken et al., 2021) is similar to AdapterBias by adding a shift to the representation, we demonstrate the difference between BitFit and AdapterBias in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_8",
            "start": 365,
            "end": 557,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_8@4",
            "content": "Bit-Fit assigns identical shifts to all the tokens, while AdapterBias adds more significant shifts to the tokens related to the task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_8",
            "start": 559,
            "end": 691,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_9@0",
            "content": "With fewer trainable parameters required, AdapterBias achieves comparable performance on the GLUE benchmark with Houlsby et al. (2019); Pfeiffer et al. (2020a); Guo et al. (2020); Ben Zaken et al. (2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_9",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_9@1",
            "content": "We further decrease the parameters of AdapterBias in different ways, including partial weight-sharing in AdapterBias and adding L 0 -norm regularization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_9",
            "start": 205,
            "end": 357,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_9@2",
            "content": "Finally, AdapterBias has better interpretability due to its simplicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_9",
            "start": 359,
            "end": 429,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_9@3",
            "content": "We use different tools, including word cloud and PCA (Jolliffe, 2002), to visualize what AdapterBias has learned, and we found that the proposed approach automatically learns to assign larger representation shifts to the task-related tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_9",
            "start": 431,
            "end": 671,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_11@0",
            "content": "For NLP tasks, adapters are introduced for the transformer architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_11",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_11@1",
            "content": "A set of adapter parameters was added at each transformer layer, which is mostly bottleneck architectures Houlsby et al. (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_11",
            "start": 73,
            "end": 200,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_11@2",
            "content": "By keeping the output dimension identical, they cause no change to the structure or parameters of the original model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_11",
            "start": 202,
            "end": 318,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_12@0",
            "content": "Adapters quickly gained popularity in NLP with various applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_12",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_12@1",
            "content": "For multi-task learning (Caruana, 1997;Zhang and Yang, 2017;Liu et al., 2019b), a projected self-attention layer is proposed by Stickland and Murray (2019), while Bapna et al. (2019) proposed an additional layer norm suitable for machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_12",
            "start": 69,
            "end": 318,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_13@0",
            "content": "Besides the applications of adapters, researchers are also dedicated to improving their performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_13",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_13@1",
            "content": "Based on the architecture introduced by Houlsby et al. (2019), AdapterFusion (Pfeiffer et al., 2020a) leveraged knowledge from multiple tasks with a new two-stage learning algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_13",
            "start": 101,
            "end": 282,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_13@2",
            "content": "Despite the recent popularity of these methods, they still train a relatively large number of training parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_13",
            "start": 284,
            "end": 397,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_14@0",
            "content": "Recently, studies start to focus on improving the parameter-efficiency of adapters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_14",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_14@1",
            "content": "Diff-pruning (Guo et al., 2020) achieves parameter efficiency by adding a sparse, task-specific difference-vector to the fixed original parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_14",
            "start": 84,
            "end": 230,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_14@2",
            "content": "The vector is adaptively pruned during training with a differentiable approximation to the L 0 -norm penalty to encourage sparsity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_14",
            "start": 232,
            "end": 362,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_14@3",
            "content": "R\u00fcckl\u00e9 et al. (2020) introduced Adap-terDrop, which has been recently integrated into AdapterHub (Pfeiffer et al., 2020b) by removing adapters from lower transformer layers during training and inference, which can dynamically reduce the computational cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_14",
            "start": 364,
            "end": 619,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_14@4",
            "content": "Mahabadi et al. (2021) proposed Compacter, which improved the trade-off between performance and trainable parameters per task with low-rank optimization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_14",
            "start": 621,
            "end": 773,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_15@0",
            "content": "On the other hand, without modifying the architecture of the PLM, BitFit (Ben Zaken et al., 2021) shows that fine-tuning only the bias terms of a large PLM is also competitive with fine-tuning the entire model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_15",
            "start": 0,
            "end": 209,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_15@1",
            "content": "Fine-tuning only the bias terms can be considered as adding a task-specific shift to the token representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_15",
            "start": 211,
            "end": 320,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_15@2",
            "content": "BitFit is most similar to our work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_15",
            "start": 322,
            "end": 356,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_15@3",
            "content": "While in BitFit, the shifts added to all the representations are exactly the same for all input tokens, in our work, the shifts are token-dependent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_15",
            "start": 358,
            "end": 505,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_16@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_16",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_17@0",
            "content": "In this section, we present AdapterBias, an efficient way to adapt large-scale PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_17",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_17@1",
            "content": "In order to better adapt to different downstream tasks, the adapter module should be token-specific.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_17",
            "start": 85,
            "end": 184,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_17@2",
            "content": "AdapterBias produces a suitable weight of the bias based on the input tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_17",
            "start": 186,
            "end": 262,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_18@0",
            "content": "Problem Formulation We consider the general problem of fine-tuning PLMs, where the training data D = (x i , y i ) N n=1 is given.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_18",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_18@1",
            "content": "Assume that given",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_18",
            "start": 130,
            "end": 146,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_19@0",
            "content": "AdapterBias",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_19",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_20@0",
            "content": "The architecture of AdapterBias is shown in the right part of Figure 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_20",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_20@1",
            "content": "AdapterBias consists of two modules: a vector (v) and a linear layer (L \u03b1 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_20",
            "start": 72,
            "end": 147,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_20@2",
            "content": "v is a task-specific shift added to the output of each transformer layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_20",
            "start": 149,
            "end": 221,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_20@3",
            "content": "Since some tokens are more important to some tasks, these tokens should be assigned larger representation shifts than other tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_20",
            "start": 223,
            "end": 353,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_20@4",
            "content": "The linear layer (L \u03b1 ) produces a token-dependent weight vector \u03b1 = [\u03b1 1 , \u03b1 2 . . . \u03b1 m ] T , where \u03b1 i is the weight of the ith token's representation shift.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_20",
            "start": 355,
            "end": 514,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_20@5",
            "content": "By applying the token-specific weight to the taskspecific representation shift (v), AdapterBias can focus on the tokens that are more important to the task and is able to adapt to different downstream tasks efficiently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_20",
            "start": 516,
            "end": 734,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_21@0",
            "content": "We define the output of AdapterBias as the bias (B), which is the outer product of v and the learned weights vector \u03b1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_21",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_21@1",
            "content": "When the dimension of the token's representation is r with with m input tokens, the function can be defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_21",
            "start": 119,
            "end": 237,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_22@0",
            "content": "B = v \u2297 \u03b1 T = \u03b1 1 v \u03b1 2 v . . . \u03b1 m v (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_22",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_23@0",
            "content": "where v \u2208 R r , \u03b1 \u2208 R m , and B \u2208 R r\u00d7m .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_23",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_24@0",
            "content": "To further elaborate on the details of Adapter-Bias, we give an example of how AdapterBias produces B and how B adds to the transformer layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_24",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_24@1",
            "content": "In Figure 3, we assume that there are three representation outputs (r 1 , r 2 , r 3 ) after the first layer normalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_24",
            "start": 143,
            "end": 264,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_24@2",
            "content": "The dimension of r 1 , r 2 and r 3 is the dimension of the 2nd feedforward layer, while the dimension of the linear layer (L \u03b1 ) is the output dimension of the first feed-forward layer with the token representation (r 1 , r 2 , r 3 ) as its inputs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_24",
            "start": 266,
            "end": 513,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_24@3",
            "content": "The linear layer (L \u03b1 ) produces \u03b1, where \u03b1 \u2208 R 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_24",
            "start": 515,
            "end": 565,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_24@4",
            "content": "The blocks in different colors represent the difference of the weights (\u03b1 1 , \u03b1 2 , \u03b1 3 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_24",
            "start": 567,
            "end": 656,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_24@5",
            "content": "Take BERT-base for example, after performing outer product with the weights vector \u03b1 and the vector (v), the dimension of B becomes 768 \u00d7 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_24",
            "start": 658,
            "end": 797,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_24@6",
            "content": "For example, b 1 , the first column of B, is the shift for the first token representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_24",
            "start": 799,
            "end": 888,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_25@0",
            "content": "Further improvement on parameter-efficiency of AdapterBias",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_25",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_26@0",
            "content": "In this section, we experiment on two ways to make AdapterBias more parameter efficient.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_26",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_26@1",
            "content": "One is partial weight-sharing of AdapterBias among transformer layers, another is enforcing the weights of the linear layer (L \u03b1 ) to be sparse by utilizing L 0norm penalty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_26",
            "start": 89,
            "end": 261,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_27@0",
            "content": "Cross-layer parameters sharing in AdapterBias",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_27",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_28@0",
            "content": "Redundancies have been observed in the information captured by adapters, with adapters in lower layers being less important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_28",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_28@1",
            "content": "In the work of Houlsby et al. (2019), they observed that their Adapter modules in the lower layers are less important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_28",
            "start": 125,
            "end": 242,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_28@2",
            "content": "In addition, sharing parameters of the Adapter across layers leads to a comparatively small drop in performance in some tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_28",
            "start": 244,
            "end": 369,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_28@3",
            "content": "In light of the above information, we further reduce the number of parameters required for each task by partially sharing the weights of the adapters across all transformer layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_28",
            "start": 371,
            "end": 550,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_28@4",
            "content": "The experimental results are discussed at Section 4.6.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_28",
            "start": 552,
            "end": 607,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_29@0",
            "content": "L 0 regularization in AdapterBias",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_29",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_30@0",
            "content": "Sparsity has been utilized in various parameterefficient methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_30",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_30@1",
            "content": "For applications in NLP tasks, Diff-pruning (Guo et al., 2020) learns a sparse vector added to the whole PLM with L 0 -norm penalty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_30",
            "start": 66,
            "end": 197,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_31@0",
            "content": "Inspired by their work, we further apply L 0 -norm regularization to L \u03b1 in the AdapterBias module, aiming to encourage the sparsity of L \u03b1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_31",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_31@1",
            "content": "We choose to drop L \u03b1 because it contributes most of the parameters in AdapterBias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_31",
            "start": 142,
            "end": 224,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_31@2",
            "content": "Encouraging its sparsity can further increase the parameter efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_31",
            "start": 226,
            "end": 296,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_31@3",
            "content": "Note that we specifically apply L 0 regularization in Section 4.6.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_31",
            "start": 298,
            "end": 365,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_32@0",
            "content": "In AdapterBias, we add L 0 -norm penalty to the linear layer (L \u03b1 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_32",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_32@1",
            "content": "The optimization problem can be expressed as,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_32",
            "start": 69,
            "end": 113,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_33@0",
            "content": "min \u03b8 L(D; \u03b8, \u03b8 ) + \u03bb \u03b8 L\u03b1 0 ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_33",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_34@0",
            "content": "where L(D; \u2022) represents the original loss with training data D. \u03bb is the hyperparameter for L 0norm penalty.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_34",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_34@1",
            "content": "Note that \u03b8 represents trainable parameters and \u03b8 L\u03b1 represents the parameters of L \u03b1 in AdapterBias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_34",
            "start": 110,
            "end": 210,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_34@2",
            "content": "Following the work of Diffpruning, we utilize a relaxed mask vector (Louizos et al., 2017) with a stretched Hard-Concrete distribution (Jang et al., 2016;Maddison et al., 2016) to encourage L 0 sparsity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_34",
            "start": 212,
            "end": 414,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_35@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_35",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_36@0",
            "content": "In this section, we evaluate the effectiveness of our proposed adapter module in NLP training tasks, and provide the analysis of what AdapterBias has learned in different tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_36",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_37@0",
            "content": "Results on GLUE",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_37",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_38@0",
            "content": "In this section, we compare AdapterBias to other parameter-efficient methods, including Adapters (Houlsby et al., 2019), Diff-pruning (Guo et al., 2020), andBitFit (Ben Zaken et al., 2021) 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_38",
            "start": 0,
            "end": 194,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_38@1",
            "content": "Although Diff-pruning (Guo et al., 2020) has the best average score among all parameter-efficient methods, their work trains an additional vector whose parameter count is equivalent to the parameters of the whole PLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_38",
            "start": 196,
            "end": 412,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_38@2",
            "content": "Thus, Diff-pruning requires 340M trainable parameters of BERT-large during the training stage, while AdapterBias only trains 0.23M parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_38",
            "start": 414,
            "end": 555,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_38@3",
            "content": "Furthermore, AdapterBias achieves comparable performance with BitFit with fewer parameters needed per task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_38",
            "start": 557,
            "end": 663,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_38@4",
            "content": "This shows that AdapterBias is a worthwhile targeted fine-tuning method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_38",
            "start": 665,
            "end": 736,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_39@0",
            "content": "Different base models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_39",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_40@0",
            "content": "To analyze how well this approach generalizes to different PLMs on different models of AdapterBias, as shown in Table 2, we apply AdapterBias in different transformer-based PLMs, including BERT-base (BB), BERT-large (BL), RoBERTa-base (RoB), and RoBERTa-large (RoL), on the GLUE benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_40",
            "start": 0,
            "end": 288,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_40@1",
            "content": "All results are scored by the GLUE evaluate server.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_40",
            "start": 290,
            "end": 340,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_40@2",
            "content": "Compared with BitFit, In Table 2, not only can AdapterBias perform well on BERT but also achieve competitive performance on larger PLMs such as RoBERTa.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_40",
            "start": 342,
            "end": 493,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_41@0",
            "content": "Size of training data",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_41",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_42@0",
            "content": "In the previous experimental results, we observe that AdapterBias tends to have higher performance on tasks with a smaller amount of data (i.e. show that AdapterBias has the ability to outperform fine-tuning the whole PLM with small-to-medium data size, similarly to BitFit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_42",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_43@0",
            "content": "Investigation on the effectiveness of token dependent representation shift",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_43",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_44@0",
            "content": "Different from BitFit (Ben Zaken et al., 2021), where the bias terms in all transformer layers are tuned, we claim that the bias added to the representation should be token-dependent, and proposed AdapterBias based on this concept.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_44",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_44@1",
            "content": "We conduct ablation studies to verify this claim.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_44",
            "start": 232,
            "end": 280,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_44@2",
            "content": "In this experiment, the linear layer (L \u03b1 ) in AdapterBias that produces the token-dependent weights vector (\u03b1) is removed; that is, only the v is trained.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_44",
            "start": 282,
            "end": 436,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_44@3",
            "content": "All shifts added to the representation outputs are identical within the same transformer layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_44",
            "start": 438,
            "end": 532,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_44@4",
            "content": "The experiments are conducted with BERT-base model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_44",
            "start": 534,
            "end": 584,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_44@5",
            "content": "We report the test scores on the GLUE benchmark in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_44",
            "start": 586,
            "end": 644,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_44@6",
            "content": "The performance of AdapterBias without the linear layer (L \u03b1 ) dramatically decreases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_44",
            "start": 646,
            "end": 731,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_44@7",
            "content": "Without L \u03b1 , it is hard for the vector (v) to adapt to different downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_44",
            "start": 733,
            "end": 815,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_44@8",
            "content": "This result demonstrates the importance of L \u03b1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_44",
            "start": 817,
            "end": 864,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_44@9",
            "content": "In other words, assigning different shifts to different token representations improves the performance of the method.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_44",
            "start": 866,
            "end": 982,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_45@0",
            "content": "Improving the parameter efficiency of AdapterBias",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_45",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_46@0",
            "content": "We further apply two additional methods to AdapterBias to enhance its parameter efficiency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_46",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_47@0",
            "content": "Experiments are conducted to see whether Adapter-Bias can be more parameter-efficient by sharing its components across all layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_47",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_47@1",
            "content": "Moreover, we experiment on adding L 0 -norm regularization during the training stage to encourage the sparsity of AdapterBias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_47",
            "start": 131,
            "end": 256,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_48@0",
            "content": "Sharing components in AdapterBias",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_48",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_49@0",
            "content": "In this experiment, we conduct an ablation study of partial weight-sharing in the AdapterBias module.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_49",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_49@1",
            "content": "In Table 4, we share components of Adapter-Bias among different transformer layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_49",
            "start": 102,
            "end": 184,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_49@2",
            "content": "Share v represents sharing v across all transformer layers, while Share L \u03b1 means sharing the linear layer (L \u03b1 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_49",
            "start": 186,
            "end": 299,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_49@3",
            "content": "Share v+L \u03b1 denotes sharing one Adapter-Bias across all transformer layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_49",
            "start": 301,
            "end": 375,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_49@4",
            "content": "As can be seen in Table 4, the performance of Share L \u03b1 stands out among other partial weight-sharing methods, while Share v leads to a poor performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_49",
            "start": 377,
            "end": 529,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_50@0",
            "content": "From the experiments above, we conclude that the linear layer (L \u03b1 ) captures general task information by learning the weights of the bias for different tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_50",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_50@1",
            "content": "Thus, sharing L \u03b1 across all layers results in better performance compared to other components.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_50",
            "start": 161,
            "end": 255,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_50@2",
            "content": "The vector module (v) in AdapterBias aims to learn local information in each transformer layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_50",
            "start": 257,
            "end": 351,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_50@3",
            "content": "If v among different transformer layers are shared, the performance drops dramatically.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_50",
            "start": 353,
            "end": 439,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_50@4",
            "content": "This might be due to a failure of v to learn general information which can be adapted to each individual transformer layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_50",
            "start": 441,
            "end": 563,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_51@0",
            "content": "L 0 -norm regularization in AdapterBias",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_51",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_52@0",
            "content": "We observed that many of the trained parameters in L \u03b1 have values that are extremely close to zero after tuning on downstream tasks, which might cause redundancy of the parameters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_52",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_52@1",
            "content": "To further encourage the sparsity of AdapterBias, we add L 0norm regularization to L \u03b1 during the training stage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_52",
            "start": 182,
            "end": 294,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_53@0",
            "content": "In Table 5, we use BERT-base (BB) and BERTlarge (BL) for the PLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_53",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_53@1",
            "content": "We compare the performance of fine-tuning, the original AdapterBias, and the one trained with L 0 -norm regularization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_53",
            "start": 66,
            "end": 184,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_53@2",
            "content": "The experiment shows that adding L 0 -norm regularization during the training step improves the performance on 7 out of 9 tasks in BERT-base models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_53",
            "start": 186,
            "end": 333,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_53@3",
            "content": "However, the performance did not improve when applied to BERT-large models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_53",
            "start": 335,
            "end": 409,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_53@4",
            "content": "As for the parameter efficiency of applying L 0 -norm penalty, the linear layer (L \u03b1 ) with L 0 -norm penalty saves about 17% parameter on average compared to the original AdapterBias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_53",
            "start": 411,
            "end": 594,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_53@5",
            "content": "The details of the reduced parameters of each task are shown in Appendix A.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_53",
            "start": 596,
            "end": 672,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_54@0",
            "content": "What AdapterBias learns",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_54",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_55@0",
            "content": "AdapterBias has good interpretability due to its simplicity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_55",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_55@1",
            "content": "Compared to our similar work Bit-Fit (Ben Zaken et al., 2021), where the shifts are identical for all tokens, AdapterBias adds tokendependent shifts to the output representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_55",
            "start": 61,
            "end": 238,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_55@2",
            "content": "By observing these token-dependent shifts, we analyze what AdapterBias learns when adapting to downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_55",
            "start": 240,
            "end": 351,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_55@3",
            "content": "In AdapterBias, the linear layer (L \u03b1 ) produces a weights vector \u03b1 for representation shifts, therefore, the average absolute value of vector \u03b1 can give us a look at the shifting amount in the transformer layers when adapting to downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_55",
            "start": 353,
            "end": 599,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_55@4",
            "content": "In Figure 5, the layers are ordered from lower to upper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_55",
            "start": 601,
            "end": 656,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_55@5",
            "content": "From the experimental result, we find that the weight in each layer is considerably different in different tasks in general.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_55",
            "start": 658,
            "end": 781,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_56@0",
            "content": "CoLA (Warstadt et al., 2019) is a syntactic task that consists of English acceptability judgments in the GLUE benchmark.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_56",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_56@1",
            "content": "As shown in Figure 5, its average shift at the ninth layer is the highest among all layers, which is quite different from the others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_56",
            "start": 121,
            "end": 253,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_56@2",
            "content": "We speculate that the ninth layer has the ability to extract the syntactic information, leading AdapterBias to add the largest shift in this layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_56",
            "start": 255,
            "end": 401,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_56@3",
            "content": "Our experiment has a similar observation with the work of Jawahar et al. (2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_56",
            "start": 403,
            "end": 482,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_56@4",
            "content": "Jawahar et al. (2019) also observe on a syntactic task with BShift (Conneau et al., 2018) that the ninth layer of BERT embeds a rich hierarchy of syntactic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_56",
            "start": 484,
            "end": 651,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_56@5",
            "content": "(Jawahar et al., 2019) Moreover, we observe similar distributions between specific tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_56",
            "start": 653,
            "end": 741,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_56@6",
            "content": "For instance, RTE (Giampiccolo et al., 2007;Bentivogli et al., 2009) and MNLI (Williams et al., 2017), where both tasks recognize textual entailment, have higher values in the upper layers than those in the lower ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_56",
            "start": 743,
            "end": 960,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_57@0",
            "content": "Based on these findings, we find that Adapter-Bias assigns suitable representation shifts in different tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_57",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_57@1",
            "content": "For tasks with similar objectives, AdapterBias tends to add similar representation shifts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_57",
            "start": 110,
            "end": 199,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_58@0",
            "content": "Which kind of word does L \u03b1 focus on",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_58",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_59@0",
            "content": "Since \u03b1 i represents the weight of the representation shift for ith token in a transformer layer, we can observe the significance of ith token from the summation of \u03b1 i in all the transformer layers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_59",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_59@1",
            "content": "Special tokens, including [CLS], [SEP], and [PAD], are not included for analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_59",
            "start": 200,
            "end": 280,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_59@2",
            "content": "We use the validation sets of CoLA and SST-2, and word cloud is used for visualizations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_59",
            "start": 282,
            "end": 369,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_60@0",
            "content": "In Figure 6, we visualize all words in the validation data of CoLA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_60",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_60@1",
            "content": "The result shows that Adapter-Bias focuses more on reflexive pronouns, such as yourself, himself, and myself.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_60",
            "start": 68,
            "end": 176,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_60@2",
            "content": "This is because there are many incorrect sentences with misused reflexive pronouns, such as \"He washed yourself.\"",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_60",
            "start": 178,
            "end": 290,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_61@0",
            "content": "In Figure 7, we visualize all words in the validation data of SST-2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_61",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_61@1",
            "content": "The result shows that Adapter-Bias focuses more on adjectives, such as \"bad\", \"awful\", and \"worst\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_61",
            "start": 69,
            "end": 167,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_61@2",
            "content": "SST-2 is a binary sentiment analysis dataset, which classifies movie reviews into positive and negative classes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_61",
            "start": 169,
            "end": 280,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_61@3",
            "content": "AdapterBias learns that adjectives often constitute a crucial factor in sentiment analysis during tuning, and adds larger shifts to these adjective tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_61",
            "start": 282,
            "end": 436,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_62@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_62",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_63@0",
            "content": "In this study, we present AdapterBias.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_63",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_63@1",
            "content": "By adding token-dependent representation shifts to the PLM, AdapterBias shows competitive results even though it uses far fewer parameters than the existing methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_63",
            "start": 39,
            "end": 203,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_63@2",
            "content": "Through extensive experiments, not only does AdapterBias reaches competitive results on the GLUE benchmark, but it also obtains good performance on small-to-medium datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_63",
            "start": 205,
            "end": 377,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_63@3",
            "content": "In addition, we demonstrate the robustness of AdapterBias to different PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_63",
            "start": 379,
            "end": 454,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_63@4",
            "content": "Finally, we provide analysis on what AdapterBias learns by comparing \u03b1, the weights of representation shift for different tokens, finding it has the ability to identify task-specific information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_63",
            "start": 456,
            "end": 650,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_63@5",
            "content": "Our study overturns previous architectures of adapters by proposing a simple adapter that can produce suitable representation shifts for different tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_63",
            "start": 652,
            "end": 805,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_64@0",
            "content": "UNKNOWN, None, 2019, Simple, scalable adaptation for neural machine translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_64",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_65@0",
            "content": "UNKNOWN, None, 2021, Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked languagemodels, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_65",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_66@0",
            "content": "UNKNOWN, None, 2009, The fifth pascal recognizing textual entailment challenge, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_66",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_67@0",
            "content": "Rich Caruana, Multitask learning, 1997, Machine learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_67",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_68@0",
            "content": "UNKNOWN, None, 2017, Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_68",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_69@0",
            "content": "UNKNOWN, None, 2018, What you can cram into a single vector: Probing sentence embeddings for linguistic properties, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_69",
            "start": 0,
            "end": 116,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_70@0",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_70",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_71@0",
            "content": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, William B Dolan, The third pascal recognizing textual entailment challenge, 2007, Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_71",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_72@0",
            "content": "UNKNOWN, None, 2020, Parameter-efficient transfer learning with diff pruning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_72",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_73@0",
            "content": "Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, Sylvain Gelly, Parameter-efficient transfer learning for nlp, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_73",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_74@0",
            "content": "UNKNOWN, None, 2016, Categorical reparameterization with gumbel-softmax, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_74",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_75@0",
            "content": "Ganesh Jawahar, Beno\u00eet Sagot, Djam\u00e9 Seddah, What does bert learn about the structure of language, 2019, ACL 2019-57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_75",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_76@0",
            "content": "UNKNOWN, None, 2002, Springer series in statistics. Principal component analysis, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_76",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_77@0",
            "content": "UNKNOWN, None, 2019, Revealing the dark secrets of bert, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_77",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_78@0",
            "content": "UNKNOWN, None, 2019, Linguistic knowledge and transferability of contextual representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_78",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_79@0",
            "content": "UNKNOWN, None, 2019, Multi-task deep neural networks for natural language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_79",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_80@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_80",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_81@0",
            "content": "UNKNOWN, None, 2017, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_81",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_82@0",
            "content": "UNKNOWN, None, 2017, Learning sparse neural networks through l_0 regularization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_82",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_83@0",
            "content": "UNKNOWN, None, 2016, The concrete distribution: A continuous relaxation of discrete random variables, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_83",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_84@0",
            "content": "UNKNOWN, None, 2021, Compacter: Efficient lowrank hypercomplex adapter layers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_84",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_85@0",
            "content": "UNKNOWN, None, 2020, Adapterfusion: Non-destructive task composition for transfer learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_85",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_86@0",
            "content": "UNKNOWN, None, 2020, Adapterhub: A framework for adapting transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_86",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_87@0",
            "content": "UNKNOWN, None, 2016, Squad: 100,000+ questions for machine comprehension of text, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_87",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_88@0",
            "content": "UNKNOWN, None, , Nils Reimers, and Iryna Gurevych. 2020. Adapterdrop: On the efficiency of adapters in transformers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_88",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_89@0",
            "content": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher,  Manning, Y Andrew, Christopher Ng,  Potts, Recursive deep models for semantic compositionality over a sentiment treebank, 2013, Proceedings of the 2013 conference on empirical methods in natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_89",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_90@0",
            "content": "Asa Stickland, Iain Murray, Bert and pals: Projected attention layers for efficient adaptation in multi-task learning, 2019, International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_90",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_91@0",
            "content": "UNKNOWN, None, 2019, Bert rediscovers the classical nlp pipeline, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_91",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_92@0",
            "content": "UNKNOWN, None, 2017, Attention is all you need, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_92",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_93@0",
            "content": "UNKNOWN, None, 2018, Glue: A multi-task benchmark and analysis platform for natural language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_93",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_94@0",
            "content": "Alex Warstadt, Amanpreet Singh, Samuel R Bowman, Neural network acceptability judgments, 2019, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_94",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_95@0",
            "content": "UNKNOWN, None, 2017, A broad-coverage challenge corpus for understanding inference, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_95",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_96@0",
            "content": "UNKNOWN, None, 2019, Huggingface's transformers: State-of-the-art natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_96",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "463-ARR_v1_97@0",
            "content": "UNKNOWN, None, 2017, A survey on multitask learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "463-ARR_v1_97",
            "start": 0,
            "end": 53,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "463-ARR_v1_0",
            "tgt_ix": "463-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_0",
            "tgt_ix": "463-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_1",
            "tgt_ix": "463-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_1",
            "tgt_ix": "463-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_0",
            "tgt_ix": "463-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_2",
            "tgt_ix": "463-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_4",
            "tgt_ix": "463-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_5",
            "tgt_ix": "463-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_6",
            "tgt_ix": "463-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_7",
            "tgt_ix": "463-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_8",
            "tgt_ix": "463-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_3",
            "tgt_ix": "463-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_3",
            "tgt_ix": "463-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_3",
            "tgt_ix": "463-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_3",
            "tgt_ix": "463-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_3",
            "tgt_ix": "463-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_3",
            "tgt_ix": "463-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_3",
            "tgt_ix": "463-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_0",
            "tgt_ix": "463-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_9",
            "tgt_ix": "463-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_11",
            "tgt_ix": "463-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_12",
            "tgt_ix": "463-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_13",
            "tgt_ix": "463-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_14",
            "tgt_ix": "463-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_10",
            "tgt_ix": "463-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_10",
            "tgt_ix": "463-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_10",
            "tgt_ix": "463-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_10",
            "tgt_ix": "463-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_10",
            "tgt_ix": "463-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_10",
            "tgt_ix": "463-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_0",
            "tgt_ix": "463-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_15",
            "tgt_ix": "463-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_17",
            "tgt_ix": "463-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_16",
            "tgt_ix": "463-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_16",
            "tgt_ix": "463-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_16",
            "tgt_ix": "463-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_16",
            "tgt_ix": "463-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_18",
            "tgt_ix": "463-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_20",
            "tgt_ix": "463-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_21",
            "tgt_ix": "463-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_22",
            "tgt_ix": "463-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_23",
            "tgt_ix": "463-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_19",
            "tgt_ix": "463-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_19",
            "tgt_ix": "463-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_19",
            "tgt_ix": "463-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_19",
            "tgt_ix": "463-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_19",
            "tgt_ix": "463-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_19",
            "tgt_ix": "463-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_16",
            "tgt_ix": "463-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_24",
            "tgt_ix": "463-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_25",
            "tgt_ix": "463-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_25",
            "tgt_ix": "463-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_16",
            "tgt_ix": "463-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_26",
            "tgt_ix": "463-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_27",
            "tgt_ix": "463-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_27",
            "tgt_ix": "463-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_16",
            "tgt_ix": "463-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_28",
            "tgt_ix": "463-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_30",
            "tgt_ix": "463-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_31",
            "tgt_ix": "463-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_32",
            "tgt_ix": "463-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_33",
            "tgt_ix": "463-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_29",
            "tgt_ix": "463-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_29",
            "tgt_ix": "463-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_29",
            "tgt_ix": "463-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_29",
            "tgt_ix": "463-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_29",
            "tgt_ix": "463-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_29",
            "tgt_ix": "463-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_0",
            "tgt_ix": "463-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_34",
            "tgt_ix": "463-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_36",
            "tgt_ix": "463-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_37",
            "tgt_ix": "463-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_37",
            "tgt_ix": "463-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_38",
            "tgt_ix": "463-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_39",
            "tgt_ix": "463-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_39",
            "tgt_ix": "463-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_40",
            "tgt_ix": "463-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_41",
            "tgt_ix": "463-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_41",
            "tgt_ix": "463-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_42",
            "tgt_ix": "463-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_43",
            "tgt_ix": "463-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_43",
            "tgt_ix": "463-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_46",
            "tgt_ix": "463-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_45",
            "tgt_ix": "463-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_45",
            "tgt_ix": "463-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_45",
            "tgt_ix": "463-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_47",
            "tgt_ix": "463-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_49",
            "tgt_ix": "463-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_48",
            "tgt_ix": "463-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_48",
            "tgt_ix": "463-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_48",
            "tgt_ix": "463-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_50",
            "tgt_ix": "463-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_52",
            "tgt_ix": "463-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_51",
            "tgt_ix": "463-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_51",
            "tgt_ix": "463-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_51",
            "tgt_ix": "463-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_53",
            "tgt_ix": "463-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_55",
            "tgt_ix": "463-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_56",
            "tgt_ix": "463-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_54",
            "tgt_ix": "463-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_54",
            "tgt_ix": "463-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_54",
            "tgt_ix": "463-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_54",
            "tgt_ix": "463-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_57",
            "tgt_ix": "463-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_59",
            "tgt_ix": "463-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_60",
            "tgt_ix": "463-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_58",
            "tgt_ix": "463-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_58",
            "tgt_ix": "463-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_58",
            "tgt_ix": "463-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_58",
            "tgt_ix": "463-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_0",
            "tgt_ix": "463-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_61",
            "tgt_ix": "463-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_62",
            "tgt_ix": "463-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_62",
            "tgt_ix": "463-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "463-ARR_v1_0",
            "tgt_ix": "463-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_1",
            "tgt_ix": "463-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_2",
            "tgt_ix": "463-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_2",
            "tgt_ix": "463-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_2",
            "tgt_ix": "463-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_2",
            "tgt_ix": "463-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_2",
            "tgt_ix": "463-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_2",
            "tgt_ix": "463-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_2",
            "tgt_ix": "463-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_3",
            "tgt_ix": "463-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_4",
            "tgt_ix": "463-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_4",
            "tgt_ix": "463-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_4",
            "tgt_ix": "463-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_5",
            "tgt_ix": "463-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_5",
            "tgt_ix": "463-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_5",
            "tgt_ix": "463-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_5",
            "tgt_ix": "463-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_5",
            "tgt_ix": "463-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_5",
            "tgt_ix": "463-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_5",
            "tgt_ix": "463-ARR_v1_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_5",
            "tgt_ix": "463-ARR_v1_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_6",
            "tgt_ix": "463-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_6",
            "tgt_ix": "463-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_6",
            "tgt_ix": "463-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_7",
            "tgt_ix": "463-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_7",
            "tgt_ix": "463-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_7",
            "tgt_ix": "463-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_7",
            "tgt_ix": "463-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_7",
            "tgt_ix": "463-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_7",
            "tgt_ix": "463-ARR_v1_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_8",
            "tgt_ix": "463-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_8",
            "tgt_ix": "463-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_8",
            "tgt_ix": "463-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_8",
            "tgt_ix": "463-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_8",
            "tgt_ix": "463-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_9",
            "tgt_ix": "463-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_9",
            "tgt_ix": "463-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_9",
            "tgt_ix": "463-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_9",
            "tgt_ix": "463-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_10",
            "tgt_ix": "463-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_11",
            "tgt_ix": "463-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_11",
            "tgt_ix": "463-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_11",
            "tgt_ix": "463-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_12",
            "tgt_ix": "463-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_12",
            "tgt_ix": "463-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_13",
            "tgt_ix": "463-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_13",
            "tgt_ix": "463-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_13",
            "tgt_ix": "463-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_14",
            "tgt_ix": "463-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_14",
            "tgt_ix": "463-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_14",
            "tgt_ix": "463-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_14",
            "tgt_ix": "463-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_14",
            "tgt_ix": "463-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_15",
            "tgt_ix": "463-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_15",
            "tgt_ix": "463-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_15",
            "tgt_ix": "463-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_15",
            "tgt_ix": "463-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_16",
            "tgt_ix": "463-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_17",
            "tgt_ix": "463-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_17",
            "tgt_ix": "463-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_17",
            "tgt_ix": "463-ARR_v1_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_18",
            "tgt_ix": "463-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_18",
            "tgt_ix": "463-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_19",
            "tgt_ix": "463-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_20",
            "tgt_ix": "463-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_20",
            "tgt_ix": "463-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_20",
            "tgt_ix": "463-ARR_v1_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_20",
            "tgt_ix": "463-ARR_v1_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_20",
            "tgt_ix": "463-ARR_v1_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_20",
            "tgt_ix": "463-ARR_v1_20@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_21",
            "tgt_ix": "463-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_21",
            "tgt_ix": "463-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_22",
            "tgt_ix": "463-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_23",
            "tgt_ix": "463-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_24",
            "tgt_ix": "463-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_24",
            "tgt_ix": "463-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_24",
            "tgt_ix": "463-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_24",
            "tgt_ix": "463-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_24",
            "tgt_ix": "463-ARR_v1_24@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_24",
            "tgt_ix": "463-ARR_v1_24@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_24",
            "tgt_ix": "463-ARR_v1_24@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_25",
            "tgt_ix": "463-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_26",
            "tgt_ix": "463-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_26",
            "tgt_ix": "463-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_27",
            "tgt_ix": "463-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_28",
            "tgt_ix": "463-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_28",
            "tgt_ix": "463-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_28",
            "tgt_ix": "463-ARR_v1_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_28",
            "tgt_ix": "463-ARR_v1_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_28",
            "tgt_ix": "463-ARR_v1_28@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_29",
            "tgt_ix": "463-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_30",
            "tgt_ix": "463-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_30",
            "tgt_ix": "463-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_31",
            "tgt_ix": "463-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_31",
            "tgt_ix": "463-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_31",
            "tgt_ix": "463-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_31",
            "tgt_ix": "463-ARR_v1_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_32",
            "tgt_ix": "463-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_32",
            "tgt_ix": "463-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_33",
            "tgt_ix": "463-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_34",
            "tgt_ix": "463-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_34",
            "tgt_ix": "463-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_34",
            "tgt_ix": "463-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_35",
            "tgt_ix": "463-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_36",
            "tgt_ix": "463-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_37",
            "tgt_ix": "463-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_38",
            "tgt_ix": "463-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_38",
            "tgt_ix": "463-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_38",
            "tgt_ix": "463-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_38",
            "tgt_ix": "463-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_38",
            "tgt_ix": "463-ARR_v1_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_39",
            "tgt_ix": "463-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_40",
            "tgt_ix": "463-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_40",
            "tgt_ix": "463-ARR_v1_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_40",
            "tgt_ix": "463-ARR_v1_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_41",
            "tgt_ix": "463-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_42",
            "tgt_ix": "463-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_43",
            "tgt_ix": "463-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_44@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_44@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_44@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_44@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_44",
            "tgt_ix": "463-ARR_v1_44@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_45",
            "tgt_ix": "463-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_46",
            "tgt_ix": "463-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_47",
            "tgt_ix": "463-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_47",
            "tgt_ix": "463-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_48",
            "tgt_ix": "463-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_49",
            "tgt_ix": "463-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_49",
            "tgt_ix": "463-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_49",
            "tgt_ix": "463-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_49",
            "tgt_ix": "463-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_49",
            "tgt_ix": "463-ARR_v1_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_50",
            "tgt_ix": "463-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_50",
            "tgt_ix": "463-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_50",
            "tgt_ix": "463-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_50",
            "tgt_ix": "463-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_50",
            "tgt_ix": "463-ARR_v1_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_51",
            "tgt_ix": "463-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_52",
            "tgt_ix": "463-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_52",
            "tgt_ix": "463-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_53",
            "tgt_ix": "463-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_53",
            "tgt_ix": "463-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_53",
            "tgt_ix": "463-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_53",
            "tgt_ix": "463-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_53",
            "tgt_ix": "463-ARR_v1_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_53",
            "tgt_ix": "463-ARR_v1_53@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_54",
            "tgt_ix": "463-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_55",
            "tgt_ix": "463-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_55",
            "tgt_ix": "463-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_55",
            "tgt_ix": "463-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_55",
            "tgt_ix": "463-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_55",
            "tgt_ix": "463-ARR_v1_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_55",
            "tgt_ix": "463-ARR_v1_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_56",
            "tgt_ix": "463-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_56",
            "tgt_ix": "463-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_56",
            "tgt_ix": "463-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_56",
            "tgt_ix": "463-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_56",
            "tgt_ix": "463-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_56",
            "tgt_ix": "463-ARR_v1_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_56",
            "tgt_ix": "463-ARR_v1_56@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_57",
            "tgt_ix": "463-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_57",
            "tgt_ix": "463-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_58",
            "tgt_ix": "463-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_59",
            "tgt_ix": "463-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_59",
            "tgt_ix": "463-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_59",
            "tgt_ix": "463-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_60",
            "tgt_ix": "463-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_60",
            "tgt_ix": "463-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_60",
            "tgt_ix": "463-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_61",
            "tgt_ix": "463-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_61",
            "tgt_ix": "463-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_61",
            "tgt_ix": "463-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_61",
            "tgt_ix": "463-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_62",
            "tgt_ix": "463-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_63",
            "tgt_ix": "463-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_63",
            "tgt_ix": "463-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_63",
            "tgt_ix": "463-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_63",
            "tgt_ix": "463-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_63",
            "tgt_ix": "463-ARR_v1_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_63",
            "tgt_ix": "463-ARR_v1_63@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_64",
            "tgt_ix": "463-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_65",
            "tgt_ix": "463-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_66",
            "tgt_ix": "463-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_67",
            "tgt_ix": "463-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_68",
            "tgt_ix": "463-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_69",
            "tgt_ix": "463-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_70",
            "tgt_ix": "463-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_71",
            "tgt_ix": "463-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_72",
            "tgt_ix": "463-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_73",
            "tgt_ix": "463-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_74",
            "tgt_ix": "463-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_75",
            "tgt_ix": "463-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_76",
            "tgt_ix": "463-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_77",
            "tgt_ix": "463-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_78",
            "tgt_ix": "463-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_79",
            "tgt_ix": "463-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_80",
            "tgt_ix": "463-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_81",
            "tgt_ix": "463-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_82",
            "tgt_ix": "463-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_83",
            "tgt_ix": "463-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_84",
            "tgt_ix": "463-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_85",
            "tgt_ix": "463-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_86",
            "tgt_ix": "463-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_87",
            "tgt_ix": "463-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_88",
            "tgt_ix": "463-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_89",
            "tgt_ix": "463-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_90",
            "tgt_ix": "463-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_91",
            "tgt_ix": "463-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_92",
            "tgt_ix": "463-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_93",
            "tgt_ix": "463-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_94",
            "tgt_ix": "463-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_95",
            "tgt_ix": "463-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_96",
            "tgt_ix": "463-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "463-ARR_v1_97",
            "tgt_ix": "463-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1069,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "463-ARR",
        "version": 1
    }
}