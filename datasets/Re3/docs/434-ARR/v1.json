{
    "nodes": [
        {
            "ix": "434-ARR_v1_0",
            "content": "Cross-Lingual UMLS Named Entity Linking using UMLS Dictionary Fine-Tuning",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_2",
            "content": "We study cross-lingual UMLS named entity linking, where mentions in a given source language are mapped to UMLS concepts, most of which are labeled in English. We propose a general solution that can be easily adapted to any source language and demonstrate the method on Hebrew documents. Our crosslingual framework includes an offline unsupervised construction of a bilingual UMLS dictionary and a per-document pipeline which identifies UMLS candidate mentions and uses a finetuned pretrained transformer language model to filter candidates according to context.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_3",
            "content": "Our method exploits a small dataset of manually annotated UMLS mentions in the source language and uses this supervised data in two ways: to extend the unsupervised UMLS dictionary and to fine-tune the contextual filtering of candidate mentions in full documents. Our method addresses cross-lingual UMLS NEL in a low resource setting, where the ontology is large, there is a lack of descriptive text defining most entities, and labeled data can only cover a small portion of the ontology. We demonstrate results of our approach on both Hebrew and English. We achieve new state-of-the-art results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset. We also achieve new SOTA on the English dataset MedMentions with +7.3 F1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_4",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "434-ARR_v1_5",
            "content": "Public health practices are becoming increasingly digital, with tools to explore scientific sources of information such as medical literature and online health communities rising in popularity. Such tools are essential in offering insights to researchers, providing information to patients and to their caregivers. Reliable identification of mentions of biomedical concepts in free text is a key technique to enable robust mining of such textual resources. Named-Entity Recognition (NER) is the task of classifying entities in text to high level classes (Person, Organization, Gene, Disease, Treatment, etc.). Named-Entity Linking (NEL) seeks to additionally classify entity mentions in text into specific concepts according to an existing reference list or knowledge base. We focus in this work on biomedical NEL, i.e., identifying mentions referring to biomedical concepts such as disorders and drugs and linking them to normalized concepts, for example, those listed in the Unified Medical Language System (UMLS) ontology. Biomedical NEL has been mostly studied in English. Other languages present additional challenges because terms in the ontology are described in English. We address cross-lingual NEL (xNEL) which consists of mapping mentions in a source language to concepts labeled and described in a different target language. We focus on UMLS xNEL, where mentions in the source language (we specifically test Hebrew, see Appendix A for a Hebrew tagging example) are mapped to UMLS concepts. We aim for a general solution that can be adapted to any source language. We operate in a low resource setting, where the ontology is large, text describing most entities is not available, and labeled data can only cover a small portion of the ontology. We also consider different genres of text to be annotated, ranging from consumer health medical articles in popular web sites to scientific biomedical articles.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_6",
            "content": "Our main contributions are: (1) We provide a general framework for cross-lingual UMLS NEL that can be adapted to source languages with few pre-requisites; our method includes four steps (a) offline unsupervised learning of a language-specific UMLS dictionary; for each document: (b) generation of candidate mentions, (c) high-recall matching of candidate mentions to UMLS concepts and (d) contextual relevance filtering of (candidate, concept) pairs. Steps (c) and (d) take advantage of multi-lingual pre-trained transformer language models (PLMs). (2) Our method exploits a small annotated corpus of documents in the source language and genre annotated manually for UMLS mentions (a few thousands annotated mentions). This training data is split to support (a) the extension of the unsupervised UMLS dictionary with corpus-salient entity names and (b) fine-tune the contextual ranking and filtering of (candidate mentions, concept) pairs. We find that the step of UMLS dictionary fine-tuning boosts NEL performance and identify a clear tradeoff in allocating training data between lexicon extension and contextual fine-tuning; (3) We demonstrate results of our approach on both Hebrew and English. We achieve new SOTA on the Hebrew Camoni corpus (Bitton et al., 2020) with +8.87 F1 and on the English dataset MedMentions (Mohan and Li, 2019) with +7.3 F1 1 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_7",
            "content": "Previous Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "434-ARR_v1_8",
            "content": "Biomedical NEL is challenging because the underlying ontology (most often UMLS) is extremely large and the acquisition of annotated training data requires rare and expensive expertise. Loureiro and Jorge (2020) presented MedLinker, a tool for improving biomedical NEL by predicting the semantic type of a medical concept mention and filtering out candidates of the wrong type. MedLinker was tested on the MedMentions task of concept linking (Mohan and Li, 2019), improving above TaggerOne , the baseline model for MedMentions which did not use deep learning. MedLinker splits the end to end task of entity linking into two stages -candidate recognition and linking. For candidate matching, it combines a BiLSTM-CRF model for contextual matching with an approximate dictionary matching method to increase recall. In the cross-lingual setting, dictionary matching is not applicable. We report our results on the same MedMentions dataset in 5.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_9",
            "content": "Past work has shown that using in-domain text can provide additional gains over general-domain language models (Gu et al., 2020). Therefore, recent work (BioBERT (Lee et al., 2020), SciB-ERT (Beltagy et al., 2019)) addressed biomedical NEL, focusing on pre-training models on scientific/medical text. Liu et al. (2021) developed Sap-BERT, a pre-training scheme which exploits the graph structure of the UMLS ontology and aims at learning an encoding of medical mentions that can align with synonym relations in the UMLS graph. Combining the SapBERT objective with pre-training on biomedical text of PubMedBERT (Gu et al., 2020) boosts results on NEL. Experimental results demonstrated that SapBERT outperforms many domain-specific BERT-based variants (BioBERT and SciBERT) on the BC5CDR dataset. Although our model focuses on cross-lingual NEL, it also applies to English documents. We compare our results to these approaches on BC5CDR and MedMentions (Tables 4 and 3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_10",
            "content": "Indexing of the abundant biomedical scientific literature requires precise detection of medical concepts. Mohan et al. (2021) developed a lowresource recognition and linking model of biomedical concepts called LRR aimed at generalizing to entities unseen at training time, and incorporating linking predictions into the mention segmentation decisions. This BERT-based model achieved SOTA results on the MedMentions task. In our work, we adopt the LRR bottom-up candidate generation approach (see 4.2). We address the main drawback of the approach by incorporating a UMLS dictionary fine-tuning technique which extends the list of candidate pairs (source expression, CUI) on a portion of the training data. We elaborate on the motivation for the technique in 4.5 and demonstrate its contribution in ablation experiments (see 5.4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_11",
            "content": "xNEL, the problem of grounding mentions of entities in a source language text into a different target language knowledge base (typically English), has been addressed in recent years, with a range of promising techniques. When the source and target languages operate over different alphabets and sound systems, both translation and transliteration of terms (which is a noisy process even when done by people) must be handled. Bitton et al. (2020) curated the Camoni corpus, an annotated resource of Hebrew posts from online health communities (OHCs), where noisy text (as opposed to scientific text) introduces additional challenges. Many user queries mention medical terms, which are very likely to include noisy transliterations. For example, the Hebrew query equivalent to \"How do I know I have fibromyalgia?\" does not return any results in the search engine of the Camoni online community when 'fibromialgia' is transliterated. Bitton et al. (2020) introduced MDTEL (Medical Deep Transliteration Entity Linking) for Hebrew-English NEL on noisy text in OHCs, and tested it on the Camoni corpus. MDTEL adopts a fourstep approach -consisting of an offline unsupervised Hebrew UMLS dictionary learning, candidate mention generation, high-recall matching and filtering of matching mentions. We adopt MDTEL's unsupervised UMLS dictionary matching, which uses an attention-based recurrent neural network encoder-decoder that maps UMLS from English to Hebrew (either a Hebrew translation or transliteration of the concept). We introduce new methods for candidate generation, high-recall matching and contextual relevance filtering, relying on multilingual pre-trained language model (mBERT). Our new components lead to significant performance improvement over MDTEL on the Camoni corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_12",
            "content": "Task Formulation",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "434-ARR_v1_13",
            "content": "Given input language L and target language L t , a database of medical concepts C Lt : L * t \u2192 CU I is a function from concept names in L t to concept IDs (CUIs). Using C Lt , we want to learn a function F from a span in input language L and its context to a CUI. We identify a translated dictionary, C L : L * \u2192 CU I. C L is the \"translated\" version of the medical concepts database C Lt . We learn C L by mapping the medical terms in L t to terms in L. Given mapping C L , we aim to learn:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_14",
            "content": "F : L * * L * \u2192 CU I \u222a {\u22a5}",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_15",
            "content": "where \u22a5 is a special code denoting a non-medical term. F differs from C L as it addresses the variability and ambiguity of the task by depending on the context as well as the span. Given text W = (w 1 , ..., w n ), where w i \u2208 L, for every span s i,j = (w i , ..., w j ) \u2286 W , we would like to compute F (W, s i,j ), where 0 \u2264 j \u2212 i < k (we limit the span sizes to at most k), that is, we want to predict the concept associated with a span within a text in L. Provided a dataset A L exposing a subset of F combined with linguistic knowledge and generalization capabilities of neural models, we aim at learning a larger portion of function F .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_16",
            "content": "Model Architecture",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "434-ARR_v1_17",
            "content": "Our end-to-end xNEL model (Fig. 1) consists of four consecutive stages: (1) multilingual UMLS mapping: generate UMLS dictionary C L (see 4.1) based on the method of Bitton et al. (2020); (2) candidate generation: consider all spans of up to k words as candidate mentions and compute vector representations for both mentions and concepts (see 4.2); (3) high recall matching: use a semantic similarity based score function to generate the top matching entities with high recall (see 4.3) and (4) contextual relevance modeling: encode each candidate into a context-dependent vector representation using a pre-trained transformer-based language model fine tuning process (see 4.4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_18",
            "content": "Our approach attempts to avoid three types of mistakes: (1) morphological and transliteration noise, where candidate terms in the source language might be extracted due to a transliteration or morphological error and matched with UMLS entities, (2) contextual errors, where candidate terms which are not medical terms when considering the context might be matched with UMLS entities, and (3) partial UMLS tagging, where candidate terms which are not full medical terms in the text but rather more general UMLS mentions might be tagged as UMLS concepts (e.g., in the mention \"flu vaccine\", \"flu\" should not be tagged). The first challenge is addressed by learning a highrecall C L dictionary with generalization capabilities, trained both on translation and transliteration data; the second, is addressed by an mBERT-based contextual language model; the third, by systematic consideration of all spans up to size k as candidates as part of the candidate generation and contextual relevance components.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_19",
            "content": "Multilingual UMLS Mapping",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "434-ARR_v1_20",
            "content": "The first step of our model is offline, fully unsupervised, and based on the method of (Bitton et al., 2020): we generate a mapping C L between medical concept names in source language L to their corresponding CUIs. An attention-based characterbased recurrent neural network encoder-decoder is used to create a list of \u2329UMLS term in English, term in language L\u232a so that each UMLS term in English is matched with both transliterated and translated forms in L. This is done without the need of manually annotated data and results in a noisy mapping C L of source language medical terms and their CUIs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_21",
            "content": "Candidate Generation",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "434-ARR_v1_22",
            "content": "Given a document in L where we want to identify UMLS mentions, the candidate generation step begins with pre-processing: we normalize the source text documents from annotated data A L and the target UMLS concepts from C L by transforming all string values to lower case and removing delimiters. We then generate a list of overlapping candidate mention spans, ranging in length according to the max length parameter k (i.e., 1, ..., k. See Appendix B for details). We exclude spans starting or ending with stop words. We then represent both the spans and the concepts as tf-idf character n-gram (1 to 3-gram) vectors using sklearn's implementation (Pedregosa et al., 2011). Empirical experiments showed that tf-idf encoding improved recall in candidate generation compared to bag of words encoding (see Appendix C for a comparison between the two representations using both Hebrew and English datasets).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_23",
            "content": "High Recall Matching",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "434-ARR_v1_24",
            "content": "The high recall matcher (HRM) receives the vector representations from the candidate generator and computes a similarity score between each span and all concept names in C L using cosine similarity (see Appendix C for comparison against Manhattan score function). We then select the top m matches per span with score over a threshold th (see Appendix D for hyper-parameters). This results in a high recall list of candidate matches.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_25",
            "content": "Contextual Relevance Modeling",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "434-ARR_v1_26",
            "content": "At this step, we want to predict which spans returned from the high recall matcher are true biomedical concepts. We use multilingual BERT (m-BERT) (Jacob Devlin, 2019), a 12 layer transformer that was trained on the Wikipedia pages of 104 languages (including Hebrew) with a shared word piece vocabulary. M-BERT does not use any marker denoting the input language, and does not include explicit mechanism to encourage translation equivalent pairs to have similar representations. We fine-tune m-BERT on a binary classification task on our training data: each candidate mention span returned from the HRM is centered in its context from the original doc, i.e., W s words to the right of the span and W s words to the left of the span, creating a window surrounding the candidate mention. The classifier takes as input the window, the HRM's decision on which concept is represented by the mention in the window, and the true verdict of whether the candidate mention is indeed an occurrence of the concept. We utilize m-BERT's QA format as follows: the question (medical concept c) and the reference text (window w) are packed into the input, and provide the binary label as answer of whether or not c is a medical mention in context w:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_27",
            "content": "[CLS] w [SEP ] c [SEP ].",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_28",
            "content": "This fine-tuning step consists of adding an additional output layer on top of the pre-trained m-BERT model to adapt it to the biomedical NEL task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_29",
            "content": "UMLS Dictionary Fine-Tuning",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "434-ARR_v1_30",
            "content": "We introduce a UMLS dictionary fine-tuning technique where some of the data in A L is removed from the training dataset and used to directly expand the learned dictionary C L . We reserve R% of the training data A L to fine-tune C L generating C L (see Fig. 1): from this chunk of A L , we add each mention in the tagged data as new pairs (mention in L, CUI).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_31",
            "content": "For example, suppose our training data consists of 10 tagged documents and our UMLS dictionary C L contains 100 concepts. Given R = 10%, our UMLS dictionary fine-tuning technique will require one tagged document d (10% of the 10 docs in the training set) to be used for fine-tuning C L .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_32",
            "content": "We go over every tagged pair (m, c) from doc d, where m is a mention in doc d and c is the UMLS concept the annotators tagged m. If m \u2208 C L , we add m to C L with the CUI of c. Suppose doc d contained 15 such tags, we will obtain an augmented C L containing 100 + 15 = 115 concepts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_33",
            "content": "We cannot use this portion of data for later training of our model, since after fine-tuning we are guaranteed to get a perfect match for all the spans in the documents used for fine-tuning (thus creating bias of the HRM).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_34",
            "content": "Although this process decreases the overall size of the input dataset for contextual relevance finetuning, it improves the recall of the HRM and adds more positive examples for the BERT training process. We elaborate more on this trade-off in 5.4.2. This approach allows us to improve recall on synonyms and abbreviations that were not originally in our UMLS dictionary, with genre-specific terminology observed in the training data (as evident from the experiment shown in Table 6).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_35",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "434-ARR_v1_36",
            "content": "We test our approach both on cross-lingual UMLS Linking using the Camoni dataset of Hebrew consumer health data and on English UMLS Linking using MedMentions and BC5CDR, which include scientific papers in the bio-medical field.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_37",
            "content": "Camoni Corpus",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "434-ARR_v1_38",
            "content": "The Camoni corpus was curated by Bitton et al. (2020) for the analysis of the MDTEL system. Camoni is an Israeli social network in Hebrew aimed at patients with chronic diseases and their family members (Camoni). Camoni serves about 20,000 registered members and 100,000 unique visitors per month. The digital platform is organized into 39 disease-specific communities. Bitton et al. (2020) extracted text from three communities (diabetes, sclerosis, and depression), for a total of 55,000 posts and 2.5 million tokens, and constructed an annotated dataset in which 1,000 mentions of UMLS terms were annotated. Bitton et al. (2020) proposed a high recall matcher based on a fuzzy string matching algorithm introduced in prior work to perform the matching between the spans and medical entities. and mBERT similarity matching) significantly improves the recall of the HRM (from about 74% in MDTEL to about 82% overall). We believe that the use of the tf-idf character n-gram vectorization before applying the cosine similarity function as means of comparison helped us achieve better results compared to MDTEL's method which only applied the cosine similarity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_39",
            "content": "In the end to end linking task, our model achieves much higher precision (98% vs. 77%) at the cost of slightly lower accuracy but much improved F-score 84 vs 74. Table 2 compares the performance of MDTEL with our model on the end to end linking task for each community.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_40",
            "content": "MedMentions",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "434-ARR_v1_41",
            "content": "MedMentions (Mohan and Li, 2019) is a corpus of Biomedical papers annotated with mentions of UMLS entities. The corpus consists of 4,392 papers (Titles and Abstracts) randomly selected from papers released on PubMed in 2016, that were in the biomedical field, published in the English language, and had both a Title and an Abstract available. MedMentions contains over 350,000 linked mentions, annotated by a team of professional annotators with rich experience in biomedical content curation. We focus on MedMentions ST21pv (21 Semantic Types and Preferred Vocabularies), a subset of the full annotations containing 203,282 mentions and restricting the concepts to a 2.3M large subset of the full ontology (UMLS ST21pv). Each concept in this subset is associated with one of 21 selected semantic types, or to one of their descendants in the semantic type hierarchy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_42",
            "content": "We compare our performance to other models' results on MedMentions ST21pv in Table 3. We improve on the latest SOTA LRR (Mohan et al., 2021) compared to 63. We believe this improvement can be attributed to our UMLS dictionary finetuning technique, which provides an extended list of candidates and thus more examples for the mBERT fine-tuning process for contextual relevance. Mohan et al. (2021) mention the need to improve recall for cases where the mentions are indirect or too abbreviated to generate a good lexical match from the entity knowledge base, which is exactly what our technique helps improve. For example, our process picked up in the training data that the abbreviation mrn is tagged as messenger rna (CUI C0035696), which was not originally present in the UMLS dictionary for English.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_43",
            "content": "BC5CDR",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "434-ARR_v1_44",
            "content": "The BC5CDR corpus (Li et al., 2016) consists of 1,500 PubMed articles with 4,409 annotated chemicals, 5,818 diseases and 3,116 chemicaldisease interactions. Each entity annotation includes both the mention text spans and normalized concept identifiers, using MeSH (Lipscomb, 2000) as the controlled vocabulary (MeSH is part of the UMLS ontology). Compared to MedMentions which contains annotations of general medical concepts, BC5CDR is topic-specific, containing only annotations of chemicals and diseases. BC5CDR is also much smaller, consisting of just 1,500 articles compared to the 4,392 annotated papers of Med-Mentions. BC5CDR has a total of 13,343 linked mentions compared to 203,282 in MedMentions ST21pv. We compare our model's performance to other models using BC5CDR's test set in Table 5 details our full results (additional evaluation metrics). We observe that domain-specific pre-trained transformers help improve results on BC5CDR (93.5 F-measure vs. 73.0 for our model). The subset of semantic types covered in this dataset is much more technical (chemicals and chemicaldisease interactions) than those covered in Med-Mentions, even though both BC5CDR and Med-Mentions include documents in the same genre of scientific biomedical articles. This difference is evidenced in the ablation study presented below. It explains why specialized language models trained on the biomedical domain lead to much improved performance compared to our model which uses the general mBERT. We hypothesize that using SapBERT combined with our model could enhance performance on this dataset and leave this for future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_45",
            "content": "UMLS Dictionary Fine-Tuning Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "434-ARR_v1_46",
            "content": "In this section, we test several factors impacting the contribution of UMLS dictionary fine-tuning to our tagger's performance. First, we test the technique on two different datasets and evaluate its benefits depending on the dataset size. Next, we test a range of UMLS dictionary fine-tuning percentage values (R) and discuss the trade-off between this value and the end to end performance of our linker.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_47",
            "content": "Dataset Size Impact",
            "ntype": "title",
            "meta": {
                "section": "5.4.1"
            }
        },
        {
            "ix": "434-ARR_v1_48",
            "content": "We tested the UMLS dictionary fine-tuning technique on English datasets MedMentions and BC5CDR across 5 random seeds and found that it improved recall on both, but impacting MedMentions much more than BC5CDR due to a much smaller number of added concepts in BC5CDR, 209 compared to 3,294 in MedMentions (see Table 6). The difference in the number of added concepts could be explained by the fact that BC5CDR is much smaller, thus the decrease in training data size counteracts the small number of concepts being added to the UMLS dictionary. To test this hypothesis, we took a subset of MedMentions of the same size as BC5CDR (annotation-wise: 8,575 in total), see Table 7 for results averaged across 5 random seeds. The results suggest that the size of the dataset directly affects the number of concepts added to our UMLS dictionary (227 added in the MedMentions subset, very close to the 209 added in BC5CDR), which in turn impacts the HRM's recall: the improvement in recall is very similar between the two datasets, +1.37 for BC5CDR, +1.7 for MedMentions subset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_49",
            "content": "The Recall-Accuracy Tradeoff",
            "ntype": "title",
            "meta": {
                "section": "5.4.2"
            }
        },
        {
            "ix": "434-ARR_v1_50",
            "content": "We first observe that our UMLS dictionary finetuning technique can only improve the high recall matching performance (Section 4.3) since an annotation that we do not have a good semantic match for from UMLS will be a missed match without UMLS DFT. Similarly, an annotation for which we do have a good semantic match will be found regardless of whether we utilize UMLS DFT or not. Thus, UMLS dictionary fine-tuning helps us find non-semantically similar matches that we would have otherwise missed, meaning that the higher R is -the higher the recall of the HRM should be. However, there is a trade-off between the recall gained from the annotations utilized for UMLS dictionary fine-tuning and the overall performance",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_51",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "434-ARR_v1_52",
            "content": "In this work we explored the task of cross lingual named entity linking in the biomedical field. We 2020) which takes into account both translation and transliteration but extends this dictionary with a portion of the training data mentions; empirical analysis of this dictionary augmentation method demonstrates its importance in end to end linking performance; (3) it adopts the bottom-up systematic generation of candidates from Mohan et al. (2021) and improves it by using a compact tf*idf ranking of the candidates (char n-gram) which helps reduce memory allocation; (4) it uses a multi-lingual pre-trained language model (mBERT) to fine-tune a contextual relevance model to filter a list of high-recall candidate matches. Our framework for cross-lingual UMLS NEL can easily be adapted to any source language and does not rely on any descriptive text for the entities.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_53",
            "content": "We compared our performance to baseline approaches on the Camoni dataset in Hebrew (Bitton et al., 2020), and the MedMentions (Mohan and Li, 2019) and BC5CDR English datasets. Our end-toend approach achieves SOTA results on Camoni in Hebrew and MedMentions in English with significant improvements. For BC5CDR, we observe that the small size of the dataset prevents our dictionary augmentation technique from reaching its potential and models trained on specialized biomedical text (PubMedBert with SapBert training objective) obtain better coverage. Such specialized training is, however, not available in a multi-lingual setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_54",
            "content": "For future work, we intend to test whether utilizing language-specific BERT models instead of multilingual BERT (e.g., swapping m-BERT with the recently released AlephBERT (Seker et al., 2021), a Hebrew version of BERT) could improve results on the Hebrew Camoni corpus. In addition, taking into account the SapBERT objective which exploits the UMLS graph structure as part of either fine-tuning or pre-training in Hebrew could lead to improved generalization capabilities. Finally, exploring datasets with additional source languages will help understand the capabilities of our multilingual pipeline. The CLEF eHealth challenges (N\u00e9v\u00e9ol et al., 2017(N\u00e9v\u00e9ol et al., , 2018 are good candidates for such analysis.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_55",
            "content": "We compared the performance (recall %) using two different score functions: (1) cosine similarity and (2) Manhattan distance, and two different vectorization techniques: (1) term frequency (tf) and",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_56",
            "content": "(2) tf-idf (term frequency * inverse document frequency). We used character unigram, bigram and trigram analysis in all the reported cases (Table 8).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_57",
            "content": "We hypothesize that the improvement stems from Idf penalizing frequent words by taking the log of {number of docs in the corpus divided by the number of docs in which the term appears}, where in our context, a 'doc' is either a span of text or a UMLS concept from C L . Since no stop words can appear at either the start or end of the span/concept, we increase the odds of having meaningful words comprising each 'doc'. The tf-idf method may contribute to this further because it not only focuses on the frequency of words present in the corpus (tf, bag of word) but also provides an importance weight to them.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v1_58",
            "content": "UNKNOWN, None, 2019, Scibert: Pretrained contextualized embeddings for scientific text, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Scibert: Pretrained contextualized embeddings for scientific text",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_59",
            "content": "Yonatan Bitton, Raphael Cohen, Tamar Schifter, Eitan Bachmat, Michael Elhadad, and No\u00e9mie Elhadad. 2020. Cross-lingual Unified Medical Language System entity linking in online health communities, , Journal of the American Medical Informatics Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Yonatan Bitton",
                    "Raphael Cohen",
                    "Tamar Schifter"
                ],
                "title": "Eitan Bachmat, Michael Elhadad, and No\u00e9mie Elhadad. 2020. Cross-lingual Unified Medical Language System entity linking in online health communities",
                "pub_date": null,
                "pub_title": "Journal of the American Medical Informatics Association",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_60",
            "content": "UNKNOWN, None, , The camoni global network, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "The camoni global network",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_61",
            "content": "UNKNOWN, None, , Jianfeng Gao, and Hoifung Poon. 2020. Domainspecific language model pretraining for biomedical natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Jianfeng Gao, and Hoifung Poon. 2020. Domainspecific language model pretraining for biomedical natural language processing",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_62",
            "content": "UNKNOWN, None, 2019, Bert: Pretraining of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_63",
            "content": "Robert Leaman, Zhiyong Lu, Taggerone: joint named entity recognition and normalization with semi-markov models, 2016, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Robert Leaman",
                    "Zhiyong Lu"
                ],
                "title": "Taggerone: joint named entity recognition and normalization with semi-markov models",
                "pub_date": "2016",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_64",
            "content": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Biobert: a pre-trained biomedical language representation model for biomedical text mining, 2020, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jinhyuk Lee",
                    "Wonjin Yoon",
                    "Sungdong Kim",
                    "Donghyeon Kim",
                    "Sunkyu Kim",
                    "Chan Ho So",
                    "Jaewoo Kang"
                ],
                "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
                "pub_date": "2020",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_65",
            "content": "UNKNOWN, None, 2016, Biocreative v cdr task corpus: a resource for chemical disease relation extraction, Database.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Biocreative v cdr task corpus: a resource for chemical disease relation extraction",
                "pub": "Database"
            }
        },
        {
            "ix": "434-ARR_v1_66",
            "content": "Carolyn Lipscomb, Medical subject headings (mesh), 2000, Bulletin of the Medical Library, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Carolyn Lipscomb"
                ],
                "title": "Medical subject headings (mesh)",
                "pub_date": "2000",
                "pub_title": "Bulletin of the Medical Library",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_67",
            "content": "Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, Nigel Collier, Self-alignment pretraining for biomedical entity representations, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Fangyu Liu",
                    "Ehsan Shareghi",
                    "Zaiqiao Meng",
                    "Marco Basaldella",
                    "Nigel Collier"
                ],
                "title": "Self-alignment pretraining for biomedical entity representations",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_68",
            "content": "Daniel Loureiro, Al\u00edpio M\u00e1rio Jorge , Medlinker: Medical entity linking with neural representations and dictionary matching, 2020, European Conference on Information Retrieval, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Daniel Loureiro",
                    "Al\u00edpio M\u00e1rio Jorge "
                ],
                "title": "Medlinker: Medical entity linking with neural representations and dictionary matching",
                "pub_date": "2020",
                "pub_title": "European Conference on Information Retrieval",
                "pub": "Springer"
            }
        },
        {
            "ix": "434-ARR_v1_69",
            "content": "UNKNOWN, None, 2021, Low resource recognition and linking of biomedical concepts from a large ontology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Low resource recognition and linking of biomedical concepts from a large ontology",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_70",
            "content": "UNKNOWN, None, 2019, Medmentions: a large biomedical corpus annotated with umls concepts, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Medmentions: a large biomedical corpus annotated with umls concepts",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_71",
            "content": "Aur\u00e9lie N\u00e9v\u00e9ol, Aude Robert, Robert Anderson, Kevin Cohen, Cyril Grouin, Thomas Lavergne, Gr\u00e9goire Rey, Claire Rondet, Pierre Zweigenbaum, Clef ehealth 2017 multilingual information extraction task overview: Icd10 coding of death certificates in english and french, 2017, CLEF (Working Notes), .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Aur\u00e9lie N\u00e9v\u00e9ol",
                    "Aude Robert",
                    "Robert Anderson",
                    "Kevin Cohen",
                    "Cyril Grouin",
                    "Thomas Lavergne",
                    "Gr\u00e9goire Rey",
                    "Claire Rondet",
                    "Pierre Zweigenbaum"
                ],
                "title": "Clef ehealth 2017 multilingual information extraction task overview: Icd10 coding of death certificates in english and french",
                "pub_date": "2017",
                "pub_title": "CLEF (Working Notes)",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_72",
            "content": "Aur\u00e9lie N\u00e9v\u00e9ol, Aude Robert, Francesco Grippo, Claire Morgand, Chiara Orsi, Laszlo Pelikan, Lionel Ramadier, Gr\u00e9goire Rey, Pierre Zweigenbaum, Clef ehealth 2018 multilingual information extraction task overview: Icd10 coding of death certificates in french, hungarian and italian, 2018, CLEF (Working Notes), .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Aur\u00e9lie N\u00e9v\u00e9ol",
                    "Aude Robert",
                    "Francesco Grippo",
                    "Claire Morgand",
                    "Chiara Orsi",
                    "Laszlo Pelikan",
                    "Lionel Ramadier",
                    "Gr\u00e9goire Rey",
                    "Pierre Zweigenbaum"
                ],
                "title": "Clef ehealth 2018 multilingual information extraction task overview: Icd10 coding of death certificates in french, hungarian and italian",
                "pub_date": "2018",
                "pub_title": "CLEF (Working Notes)",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_73",
            "content": "F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Scikit-learn: Machine learning in Python, 2011, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "F Pedregosa",
                    "G Varoquaux",
                    "A Gramfort",
                    "V Michel",
                    "B Thirion",
                    "O Grisel",
                    "M Blondel",
                    "P Prettenhofer",
                    "R Weiss",
                    "V Dubourg",
                    "J Vanderplas",
                    "A Passos",
                    "D Cournapeau",
                    "M Brucher",
                    "M Perrot",
                    "E Duchesnay"
                ],
                "title": "Scikit-learn: Machine learning in Python",
                "pub_date": "2011",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v1_74",
            "content": "UNKNOWN, None, 2021, Alephbert: A hebrew large pretrained language model to start-off your hebrew NLP application with, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Alephbert: A hebrew large pretrained language model to start-off your hebrew NLP application with",
                "pub": "CoRR"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "434-ARR_v1_0@0",
            "content": "Cross-Lingual UMLS Named Entity Linking using UMLS Dictionary Fine-Tuning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_0",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_2@0",
            "content": "We study cross-lingual UMLS named entity linking, where mentions in a given source language are mapped to UMLS concepts, most of which are labeled in English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_2",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_2@1",
            "content": "We propose a general solution that can be easily adapted to any source language and demonstrate the method on Hebrew documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_2",
            "start": 159,
            "end": 285,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_2@2",
            "content": "Our crosslingual framework includes an offline unsupervised construction of a bilingual UMLS dictionary and a per-document pipeline which identifies UMLS candidate mentions and uses a finetuned pretrained transformer language model to filter candidates according to context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_2",
            "start": 287,
            "end": 560,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_3@0",
            "content": "Our method exploits a small dataset of manually annotated UMLS mentions in the source language and uses this supervised data in two ways: to extend the unsupervised UMLS dictionary and to fine-tune the contextual filtering of candidate mentions in full documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_3",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_3@1",
            "content": "Our method addresses cross-lingual UMLS NEL in a low resource setting, where the ontology is large, there is a lack of descriptive text defining most entities, and labeled data can only cover a small portion of the ontology.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_3",
            "start": 264,
            "end": 487,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_3@2",
            "content": "We demonstrate results of our approach on both Hebrew and English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_3",
            "start": 489,
            "end": 554,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_3@3",
            "content": "We achieve new state-of-the-art results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_3",
            "start": 556,
            "end": 683,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_3@4",
            "content": "We also achieve new SOTA on the English dataset MedMentions with +7.3 F1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_3",
            "start": 685,
            "end": 757,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_4@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_4",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@0",
            "content": "Public health practices are becoming increasingly digital, with tools to explore scientific sources of information such as medical literature and online health communities rising in popularity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@1",
            "content": "Such tools are essential in offering insights to researchers, providing information to patients and to their caregivers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 194,
            "end": 313,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@2",
            "content": "Reliable identification of mentions of biomedical concepts in free text is a key technique to enable robust mining of such textual resources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 315,
            "end": 455,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@3",
            "content": "Named-Entity Recognition (NER) is the task of classifying entities in text to high level classes (Person, Organization, Gene, Disease, Treatment, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 457,
            "end": 608,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@4",
            "content": "Named-Entity Linking (NEL) seeks to additionally classify entity mentions in text into specific concepts according to an existing reference list or knowledge base.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 610,
            "end": 772,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@5",
            "content": "We focus in this work on biomedical NEL, i.e., identifying mentions referring to biomedical concepts such as disorders and drugs and linking them to normalized concepts, for example, those listed in the Unified Medical Language System (UMLS) ontology.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 774,
            "end": 1024,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@6",
            "content": "Biomedical NEL has been mostly studied in English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 1026,
            "end": 1075,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@7",
            "content": "Other languages present additional challenges because terms in the ontology are described in English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 1077,
            "end": 1177,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@8",
            "content": "We address cross-lingual NEL (xNEL) which consists of mapping mentions in a source language to concepts labeled and described in a different target language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 1179,
            "end": 1335,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@9",
            "content": "We focus on UMLS xNEL, where mentions in the source language (we specifically test Hebrew, see Appendix A for a Hebrew tagging example) are mapped to UMLS concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 1337,
            "end": 1500,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@10",
            "content": "We aim for a general solution that can be adapted to any source language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 1502,
            "end": 1574,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@11",
            "content": "We operate in a low resource setting, where the ontology is large, text describing most entities is not available, and labeled data can only cover a small portion of the ontology.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 1576,
            "end": 1754,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_5@12",
            "content": "We also consider different genres of text to be annotated, ranging from consumer health medical articles in popular web sites to scientific biomedical articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_5",
            "start": 1756,
            "end": 1915,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_6@0",
            "content": "Our main contributions are: (1) We provide a general framework for cross-lingual UMLS NEL that can be adapted to source languages with few pre-requisites; our method includes four steps (a) offline unsupervised learning of a language-specific UMLS dictionary; for each document: (b) generation of candidate mentions, (c) high-recall matching of candidate mentions to UMLS concepts and (d) contextual relevance filtering of (candidate, concept) pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_6",
            "start": 0,
            "end": 449,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_6@1",
            "content": "Steps (c) and (d) take advantage of multi-lingual pre-trained transformer language models (PLMs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_6",
            "start": 451,
            "end": 547,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_6@2",
            "content": "(2) Our method exploits a small annotated corpus of documents in the source language and genre annotated manually for UMLS mentions (a few thousands annotated mentions).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_6",
            "start": 549,
            "end": 717,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_6@3",
            "content": "This training data is split to support (a) the extension of the unsupervised UMLS dictionary with corpus-salient entity names and (b) fine-tune the contextual ranking and filtering of (candidate mentions, concept) pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_6",
            "start": 719,
            "end": 938,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_6@4",
            "content": "We find that the step of UMLS dictionary fine-tuning boosts NEL performance and identify a clear tradeoff in allocating training data between lexicon extension and contextual fine-tuning; (3) We demonstrate results of our approach on both Hebrew and English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_6",
            "start": 940,
            "end": 1197,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_6@5",
            "content": "We achieve new SOTA on the Hebrew Camoni corpus (Bitton et al., 2020) with +8.87 F1 and on the English dataset MedMentions (Mohan and Li, 2019) with +7.3 F1 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_6",
            "start": 1199,
            "end": 1358,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_7@0",
            "content": "Previous Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_7",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_8@0",
            "content": "Biomedical NEL is challenging because the underlying ontology (most often UMLS) is extremely large and the acquisition of annotated training data requires rare and expensive expertise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_8",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_8@1",
            "content": "Loureiro and Jorge (2020) presented MedLinker, a tool for improving biomedical NEL by predicting the semantic type of a medical concept mention and filtering out candidates of the wrong type.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_8",
            "start": 185,
            "end": 375,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_8@2",
            "content": "MedLinker was tested on the MedMentions task of concept linking (Mohan and Li, 2019), improving above TaggerOne , the baseline model for MedMentions which did not use deep learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_8",
            "start": 377,
            "end": 557,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_8@3",
            "content": "MedLinker splits the end to end task of entity linking into two stages -candidate recognition and linking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_8",
            "start": 559,
            "end": 664,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_8@4",
            "content": "For candidate matching, it combines a BiLSTM-CRF model for contextual matching with an approximate dictionary matching method to increase recall.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_8",
            "start": 666,
            "end": 810,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_8@5",
            "content": "In the cross-lingual setting, dictionary matching is not applicable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_8",
            "start": 812,
            "end": 879,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_8@6",
            "content": "We report our results on the same MedMentions dataset in 5.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_8",
            "start": 881,
            "end": 941,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_9@0",
            "content": "Past work has shown that using in-domain text can provide additional gains over general-domain language models (Gu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_9",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_9@1",
            "content": "Therefore, recent work (BioBERT (Lee et al., 2020), SciB-ERT (Beltagy et al., 2019)) addressed biomedical NEL, focusing on pre-training models on scientific/medical text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_9",
            "start": 130,
            "end": 299,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_9@2",
            "content": "Liu et al. (2021) developed Sap-BERT, a pre-training scheme which exploits the graph structure of the UMLS ontology and aims at learning an encoding of medical mentions that can align with synonym relations in the UMLS graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_9",
            "start": 301,
            "end": 525,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_9@3",
            "content": "Combining the SapBERT objective with pre-training on biomedical text of PubMedBERT (Gu et al., 2020) boosts results on NEL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_9",
            "start": 527,
            "end": 649,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_9@4",
            "content": "Experimental results demonstrated that SapBERT outperforms many domain-specific BERT-based variants (BioBERT and SciBERT) on the BC5CDR dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_9",
            "start": 651,
            "end": 794,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_9@5",
            "content": "Although our model focuses on cross-lingual NEL, it also applies to English documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_9",
            "start": 796,
            "end": 881,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_9@6",
            "content": "We compare our results to these approaches on BC5CDR and MedMentions (Tables 4 and 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_9",
            "start": 883,
            "end": 968,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_10@0",
            "content": "Indexing of the abundant biomedical scientific literature requires precise detection of medical concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_10",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_10@1",
            "content": "Mohan et al. (2021) developed a lowresource recognition and linking model of biomedical concepts called LRR aimed at generalizing to entities unseen at training time, and incorporating linking predictions into the mention segmentation decisions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_10",
            "start": 106,
            "end": 350,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_10@2",
            "content": "This BERT-based model achieved SOTA results on the MedMentions task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_10",
            "start": 352,
            "end": 419,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_10@3",
            "content": "In our work, we adopt the LRR bottom-up candidate generation approach (see 4.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_10",
            "start": 421,
            "end": 500,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_10@4",
            "content": "We address the main drawback of the approach by incorporating a UMLS dictionary fine-tuning technique which extends the list of candidate pairs (source expression, CUI) on a portion of the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_10",
            "start": 502,
            "end": 704,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_10@5",
            "content": "We elaborate on the motivation for the technique in 4.5 and demonstrate its contribution in ablation experiments (see 5.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_10",
            "start": 706,
            "end": 828,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_11@0",
            "content": "xNEL, the problem of grounding mentions of entities in a source language text into a different target language knowledge base (typically English), has been addressed in recent years, with a range of promising techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_11",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_11@1",
            "content": "When the source and target languages operate over different alphabets and sound systems, both translation and transliteration of terms (which is a noisy process even when done by people) must be handled.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_11",
            "start": 221,
            "end": 423,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_11@2",
            "content": "Bitton et al. (2020) curated the Camoni corpus, an annotated resource of Hebrew posts from online health communities (OHCs), where noisy text (as opposed to scientific text) introduces additional challenges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_11",
            "start": 425,
            "end": 631,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_11@3",
            "content": "Many user queries mention medical terms, which are very likely to include noisy transliterations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_11",
            "start": 633,
            "end": 729,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_11@4",
            "content": "For example, the Hebrew query equivalent to \"How do I know I have fibromyalgia?\" does not return any results in the search engine of the Camoni online community when 'fibromialgia' is transliterated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_11",
            "start": 731,
            "end": 929,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_11@5",
            "content": "Bitton et al. (2020) introduced MDTEL (Medical Deep Transliteration Entity Linking) for Hebrew-English NEL on noisy text in OHCs, and tested it on the Camoni corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_11",
            "start": 931,
            "end": 1095,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_11@6",
            "content": "MDTEL adopts a fourstep approach -consisting of an offline unsupervised Hebrew UMLS dictionary learning, candidate mention generation, high-recall matching and filtering of matching mentions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_11",
            "start": 1097,
            "end": 1287,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_11@7",
            "content": "We adopt MDTEL's unsupervised UMLS dictionary matching, which uses an attention-based recurrent neural network encoder-decoder that maps UMLS from English to Hebrew (either a Hebrew translation or transliteration of the concept).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_11",
            "start": 1289,
            "end": 1517,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_11@8",
            "content": "We introduce new methods for candidate generation, high-recall matching and contextual relevance filtering, relying on multilingual pre-trained language model (mBERT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_11",
            "start": 1519,
            "end": 1685,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_11@9",
            "content": "Our new components lead to significant performance improvement over MDTEL on the Camoni corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_11",
            "start": 1687,
            "end": 1781,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_12@0",
            "content": "Task Formulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_12",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_13@0",
            "content": "Given input language L and target language L t , a database of medical concepts C Lt : L * t \u2192 CU I is a function from concept names in L t to concept IDs (CUIs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_13",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_13@1",
            "content": "Using C Lt , we want to learn a function F from a span in input language L and its context to a CUI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_13",
            "start": 163,
            "end": 262,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_13@2",
            "content": "We identify a translated dictionary, C L : L * \u2192 CU I. C L is the \"translated\" version of the medical concepts database C Lt .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_13",
            "start": 264,
            "end": 389,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_13@3",
            "content": "We learn C L by mapping the medical terms in L t to terms in L. Given mapping C L , we aim to learn:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_13",
            "start": 391,
            "end": 490,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_14@0",
            "content": "F : L * * L * \u2192 CU I \u222a {\u22a5}",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_14",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_15@0",
            "content": "where \u22a5 is a special code denoting a non-medical term.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_15",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_15@1",
            "content": "F differs from C L as it addresses the variability and ambiguity of the task by depending on the context as well as the span.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_15",
            "start": 55,
            "end": 179,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_15@2",
            "content": "Given text W = (w 1 , ..., w n ), where w i \u2208 L, for every span s i,j = (w i , ..., w j ) \u2286 W , we would like to compute F (W, s i,j ), where 0 \u2264 j \u2212 i < k (we limit the span sizes to at most k), that is, we want to predict the concept associated with a span within a text in L. Provided a dataset A L exposing a subset of F combined with linguistic knowledge and generalization capabilities of neural models, we aim at learning a larger portion of function F .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_15",
            "start": 181,
            "end": 641,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_16@0",
            "content": "Model Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_16",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_17@0",
            "content": "Our end-to-end xNEL model (Fig. 1) consists of four consecutive stages: (1) multilingual UMLS mapping: generate UMLS dictionary C L (see 4.1) based on the method of Bitton et al. (2020); (2) candidate generation: consider all spans of up to k words as candidate mentions and compute vector representations for both mentions and concepts (see 4.2); (3) high recall matching: use a semantic similarity based score function to generate the top matching entities with high recall (see 4.3) and (4) contextual relevance modeling: encode each candidate into a context-dependent vector representation using a pre-trained transformer-based language model fine tuning process (see 4.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_17",
            "start": 0,
            "end": 676,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_18@0",
            "content": "Our approach attempts to avoid three types of mistakes: (1) morphological and transliteration noise, where candidate terms in the source language might be extracted due to a transliteration or morphological error and matched with UMLS entities, (2) contextual errors, where candidate terms which are not medical terms when considering the context might be matched with UMLS entities, and (3) partial UMLS tagging, where candidate terms which are not full medical terms in the text but rather more general UMLS mentions might be tagged as UMLS concepts (e.g., in the mention \"flu vaccine\", \"flu\" should not be tagged).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_18",
            "start": 0,
            "end": 616,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_18@1",
            "content": "The first challenge is addressed by learning a highrecall C L dictionary with generalization capabilities, trained both on translation and transliteration data; the second, is addressed by an mBERT-based contextual language model; the third, by systematic consideration of all spans up to size k as candidates as part of the candidate generation and contextual relevance components.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_18",
            "start": 618,
            "end": 999,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_19@0",
            "content": "Multilingual UMLS Mapping",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_19",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_20@0",
            "content": "The first step of our model is offline, fully unsupervised, and based on the method of (Bitton et al., 2020): we generate a mapping C L between medical concept names in source language L to their corresponding CUIs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_20",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_20@1",
            "content": "An attention-based characterbased recurrent neural network encoder-decoder is used to create a list of \u2329UMLS term in English, term in language L\u232a so that each UMLS term in English is matched with both transliterated and translated forms in L. This is done without the need of manually annotated data and results in a noisy mapping C L of source language medical terms and their CUIs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_20",
            "start": 216,
            "end": 598,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_21@0",
            "content": "Candidate Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_21",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_22@0",
            "content": "Given a document in L where we want to identify UMLS mentions, the candidate generation step begins with pre-processing: we normalize the source text documents from annotated data A L and the target UMLS concepts from C L by transforming all string values to lower case and removing delimiters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_22",
            "start": 0,
            "end": 293,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_22@1",
            "content": "We then generate a list of overlapping candidate mention spans, ranging in length according to the max length parameter k (i.e., 1, ..., k. See Appendix B for details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_22",
            "start": 295,
            "end": 462,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_22@2",
            "content": "We exclude spans starting or ending with stop words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_22",
            "start": 464,
            "end": 515,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_22@3",
            "content": "We then represent both the spans and the concepts as tf-idf character n-gram (1 to 3-gram) vectors using sklearn's implementation (Pedregosa et al., 2011).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_22",
            "start": 517,
            "end": 671,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_22@4",
            "content": "Empirical experiments showed that tf-idf encoding improved recall in candidate generation compared to bag of words encoding (see Appendix C for a comparison between the two representations using both Hebrew and English datasets).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_22",
            "start": 673,
            "end": 901,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_23@0",
            "content": "High Recall Matching",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_23",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_24@0",
            "content": "The high recall matcher (HRM) receives the vector representations from the candidate generator and computes a similarity score between each span and all concept names in C L using cosine similarity (see Appendix C for comparison against Manhattan score function).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_24",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_24@1",
            "content": "We then select the top m matches per span with score over a threshold th (see Appendix D for hyper-parameters).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_24",
            "start": 264,
            "end": 374,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_24@2",
            "content": "This results in a high recall list of candidate matches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_24",
            "start": 376,
            "end": 431,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_25@0",
            "content": "Contextual Relevance Modeling",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_25",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_26@0",
            "content": "At this step, we want to predict which spans returned from the high recall matcher are true biomedical concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_26",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_26@1",
            "content": "We use multilingual BERT (m-BERT) (Jacob Devlin, 2019), a 12 layer transformer that was trained on the Wikipedia pages of 104 languages (including Hebrew) with a shared word piece vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_26",
            "start": 113,
            "end": 303,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_26@2",
            "content": "M-BERT does not use any marker denoting the input language, and does not include explicit mechanism to encourage translation equivalent pairs to have similar representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_26",
            "start": 305,
            "end": 478,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_26@3",
            "content": "We fine-tune m-BERT on a binary classification task on our training data: each candidate mention span returned from the HRM is centered in its context from the original doc, i.e., W s words to the right of the span and W s words to the left of the span, creating a window surrounding the candidate mention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_26",
            "start": 480,
            "end": 785,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_26@4",
            "content": "The classifier takes as input the window, the HRM's decision on which concept is represented by the mention in the window, and the true verdict of whether the candidate mention is indeed an occurrence of the concept.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_26",
            "start": 787,
            "end": 1002,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_26@5",
            "content": "We utilize m-BERT's QA format as follows: the question (medical concept c) and the reference text (window w) are packed into the input, and provide the binary label as answer of whether or not c is a medical mention in context w:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_26",
            "start": 1004,
            "end": 1232,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_27@0",
            "content": "[CLS] w [SEP ] c [SEP ].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_27",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_28@0",
            "content": "This fine-tuning step consists of adding an additional output layer on top of the pre-trained m-BERT model to adapt it to the biomedical NEL task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_28",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_29@0",
            "content": "UMLS Dictionary Fine-Tuning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_29",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_30@0",
            "content": "We introduce a UMLS dictionary fine-tuning technique where some of the data in A L is removed from the training dataset and used to directly expand the learned dictionary C L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_30",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_30@1",
            "content": "We reserve R% of the training data A L to fine-tune C L generating C L (see Fig. 1): from this chunk of A L , we add each mention in the tagged data as new pairs (mention in L, CUI).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_30",
            "start": 177,
            "end": 358,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_31@0",
            "content": "For example, suppose our training data consists of 10 tagged documents and our UMLS dictionary C L contains 100 concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_31",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_31@1",
            "content": "Given R = 10%, our UMLS dictionary fine-tuning technique will require one tagged document d (10% of the 10 docs in the training set) to be used for fine-tuning C L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_31",
            "start": 122,
            "end": 286,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_32@0",
            "content": "We go over every tagged pair (m, c) from doc d, where m is a mention in doc d and c is the UMLS concept the annotators tagged m. If m \u2208 C L , we add m to C L with the CUI of c. Suppose doc d contained 15 such tags, we will obtain an augmented C L containing 100 + 15 = 115 concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_32",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_33@0",
            "content": "We cannot use this portion of data for later training of our model, since after fine-tuning we are guaranteed to get a perfect match for all the spans in the documents used for fine-tuning (thus creating bias of the HRM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_33",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_34@0",
            "content": "Although this process decreases the overall size of the input dataset for contextual relevance finetuning, it improves the recall of the HRM and adds more positive examples for the BERT training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_34",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_34@1",
            "content": "We elaborate more on this trade-off in 5.4.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_34",
            "start": 204,
            "end": 248,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_34@2",
            "content": "This approach allows us to improve recall on synonyms and abbreviations that were not originally in our UMLS dictionary, with genre-specific terminology observed in the training data (as evident from the experiment shown in Table 6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_34",
            "start": 250,
            "end": 482,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_35@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_35",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_36@0",
            "content": "We test our approach both on cross-lingual UMLS Linking using the Camoni dataset of Hebrew consumer health data and on English UMLS Linking using MedMentions and BC5CDR, which include scientific papers in the bio-medical field.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_36",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_37@0",
            "content": "Camoni Corpus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_37",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_38@0",
            "content": "The Camoni corpus was curated by Bitton et al. (2020) for the analysis of the MDTEL system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_38",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_38@1",
            "content": "Camoni is an Israeli social network in Hebrew aimed at patients with chronic diseases and their family members (Camoni).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_38",
            "start": 92,
            "end": 211,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_38@2",
            "content": "Camoni serves about 20,000 registered members and 100,000 unique visitors per month.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_38",
            "start": 213,
            "end": 296,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_38@3",
            "content": "The digital platform is organized into 39 disease-specific communities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_38",
            "start": 298,
            "end": 368,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_38@4",
            "content": "Bitton et al. (2020) extracted text from three communities (diabetes, sclerosis, and depression), for a total of 55,000 posts and 2.5 million tokens, and constructed an annotated dataset in which 1,000 mentions of UMLS terms were annotated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_38",
            "start": 370,
            "end": 609,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_38@5",
            "content": "Bitton et al. (2020) proposed a high recall matcher based on a fuzzy string matching algorithm introduced in prior work to perform the matching between the spans and medical entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_38",
            "start": 611,
            "end": 793,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_38@6",
            "content": "and mBERT similarity matching) significantly improves the recall of the HRM (from about 74% in MDTEL to about 82% overall).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_38",
            "start": 795,
            "end": 917,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_38@7",
            "content": "We believe that the use of the tf-idf character n-gram vectorization before applying the cosine similarity function as means of comparison helped us achieve better results compared to MDTEL's method which only applied the cosine similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_38",
            "start": 919,
            "end": 1158,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_39@0",
            "content": "In the end to end linking task, our model achieves much higher precision (98% vs. 77%) at the cost of slightly lower accuracy but much improved F-score 84 vs 74.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_39",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_39@1",
            "content": "Table 2 compares the performance of MDTEL with our model on the end to end linking task for each community.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_39",
            "start": 162,
            "end": 268,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_40@0",
            "content": "MedMentions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_40",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_41@0",
            "content": "MedMentions (Mohan and Li, 2019) is a corpus of Biomedical papers annotated with mentions of UMLS entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_41",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_41@1",
            "content": "The corpus consists of 4,392 papers (Titles and Abstracts) randomly selected from papers released on PubMed in 2016, that were in the biomedical field, published in the English language, and had both a Title and an Abstract available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_41",
            "start": 108,
            "end": 341,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_41@2",
            "content": "MedMentions contains over 350,000 linked mentions, annotated by a team of professional annotators with rich experience in biomedical content curation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_41",
            "start": 343,
            "end": 492,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_41@3",
            "content": "We focus on MedMentions ST21pv (21 Semantic Types and Preferred Vocabularies), a subset of the full annotations containing 203,282 mentions and restricting the concepts to a 2.3M large subset of the full ontology (UMLS ST21pv).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_41",
            "start": 494,
            "end": 720,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_41@4",
            "content": "Each concept in this subset is associated with one of 21 selected semantic types, or to one of their descendants in the semantic type hierarchy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_41",
            "start": 722,
            "end": 865,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_42@0",
            "content": "We compare our performance to other models' results on MedMentions ST21pv in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_42",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_42@1",
            "content": "We improve on the latest SOTA LRR (Mohan et al., 2021) compared to 63.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_42",
            "start": 86,
            "end": 155,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_42@2",
            "content": "We believe this improvement can be attributed to our UMLS dictionary finetuning technique, which provides an extended list of candidates and thus more examples for the mBERT fine-tuning process for contextual relevance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_42",
            "start": 157,
            "end": 375,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_42@3",
            "content": "Mohan et al. (2021) mention the need to improve recall for cases where the mentions are indirect or too abbreviated to generate a good lexical match from the entity knowledge base, which is exactly what our technique helps improve.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_42",
            "start": 377,
            "end": 607,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_42@4",
            "content": "For example, our process picked up in the training data that the abbreviation mrn is tagged as messenger rna (CUI C0035696), which was not originally present in the UMLS dictionary for English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_42",
            "start": 609,
            "end": 801,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_43@0",
            "content": "BC5CDR",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_43",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@0",
            "content": "The BC5CDR corpus (Li et al., 2016) consists of 1,500 PubMed articles with 4,409 annotated chemicals, 5,818 diseases and 3,116 chemicaldisease interactions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@1",
            "content": "Each entity annotation includes both the mention text spans and normalized concept identifiers, using MeSH (Lipscomb, 2000) as the controlled vocabulary (MeSH is part of the UMLS ontology).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 157,
            "end": 345,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@2",
            "content": "Compared to MedMentions which contains annotations of general medical concepts, BC5CDR is topic-specific, containing only annotations of chemicals and diseases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 347,
            "end": 506,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@3",
            "content": "BC5CDR is also much smaller, consisting of just 1,500 articles compared to the 4,392 annotated papers of Med-Mentions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 508,
            "end": 625,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@4",
            "content": "BC5CDR has a total of 13,343 linked mentions compared to 203,282 in MedMentions ST21pv.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 627,
            "end": 713,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@5",
            "content": "We compare our model's performance to other models using BC5CDR's test set in Table 5 details our full results (additional evaluation metrics).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 715,
            "end": 857,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@6",
            "content": "We observe that domain-specific pre-trained transformers help improve results on BC5CDR (93.5 F-measure vs. 73.0 for our model).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 859,
            "end": 986,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@7",
            "content": "The subset of semantic types covered in this dataset is much more technical (chemicals and chemicaldisease interactions) than those covered in Med-Mentions, even though both BC5CDR and Med-Mentions include documents in the same genre of scientific biomedical articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 988,
            "end": 1255,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@8",
            "content": "This difference is evidenced in the ablation study presented below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 1257,
            "end": 1323,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@9",
            "content": "It explains why specialized language models trained on the biomedical domain lead to much improved performance compared to our model which uses the general mBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 1325,
            "end": 1486,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_44@10",
            "content": "We hypothesize that using SapBERT combined with our model could enhance performance on this dataset and leave this for future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_44",
            "start": 1488,
            "end": 1618,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_45@0",
            "content": "UMLS Dictionary Fine-Tuning Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_45",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_46@0",
            "content": "In this section, we test several factors impacting the contribution of UMLS dictionary fine-tuning to our tagger's performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_46",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_46@1",
            "content": "First, we test the technique on two different datasets and evaluate its benefits depending on the dataset size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_46",
            "start": 128,
            "end": 238,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_46@2",
            "content": "Next, we test a range of UMLS dictionary fine-tuning percentage values (R) and discuss the trade-off between this value and the end to end performance of our linker.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_46",
            "start": 240,
            "end": 404,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_47@0",
            "content": "Dataset Size Impact",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_47",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_48@0",
            "content": "We tested the UMLS dictionary fine-tuning technique on English datasets MedMentions and BC5CDR across 5 random seeds and found that it improved recall on both, but impacting MedMentions much more than BC5CDR due to a much smaller number of added concepts in BC5CDR, 209 compared to 3,294 in MedMentions (see Table 6).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_48",
            "start": 0,
            "end": 316,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_48@1",
            "content": "The difference in the number of added concepts could be explained by the fact that BC5CDR is much smaller, thus the decrease in training data size counteracts the small number of concepts being added to the UMLS dictionary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_48",
            "start": 318,
            "end": 540,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_48@2",
            "content": "To test this hypothesis, we took a subset of MedMentions of the same size as BC5CDR (annotation-wise: 8,575 in total), see Table 7 for results averaged across 5 random seeds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_48",
            "start": 542,
            "end": 715,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_48@3",
            "content": "The results suggest that the size of the dataset directly affects the number of concepts added to our UMLS dictionary (227 added in the MedMentions subset, very close to the 209 added in BC5CDR), which in turn impacts the HRM's recall: the improvement in recall is very similar between the two datasets, +1.37 for BC5CDR, +1.7 for MedMentions subset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_48",
            "start": 717,
            "end": 1066,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_49@0",
            "content": "The Recall-Accuracy Tradeoff",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_49",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_50@0",
            "content": "We first observe that our UMLS dictionary finetuning technique can only improve the high recall matching performance (Section 4.3) since an annotation that we do not have a good semantic match for from UMLS will be a missed match without UMLS DFT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_50",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_50@1",
            "content": "Similarly, an annotation for which we do have a good semantic match will be found regardless of whether we utilize UMLS DFT or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_50",
            "start": 248,
            "end": 378,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_50@2",
            "content": "Thus, UMLS dictionary fine-tuning helps us find non-semantically similar matches that we would have otherwise missed, meaning that the higher R is -the higher the recall of the HRM should be.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_50",
            "start": 380,
            "end": 570,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_50@3",
            "content": "However, there is a trade-off between the recall gained from the annotations utilized for UMLS dictionary fine-tuning and the overall performance",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_50",
            "start": 572,
            "end": 716,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_51@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_51",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_52@0",
            "content": "In this work we explored the task of cross lingual named entity linking in the biomedical field.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_52",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_52@1",
            "content": "We 2020) which takes into account both translation and transliteration but extends this dictionary with a portion of the training data mentions; empirical analysis of this dictionary augmentation method demonstrates its importance in end to end linking performance; (3) it adopts the bottom-up systematic generation of candidates from Mohan et al. (2021) and improves it by using a compact tf*idf ranking of the candidates (char n-gram) which helps reduce memory allocation; (4) it uses a multi-lingual pre-trained language model (mBERT) to fine-tune a contextual relevance model to filter a list of high-recall candidate matches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_52",
            "start": 97,
            "end": 726,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_52@2",
            "content": "Our framework for cross-lingual UMLS NEL can easily be adapted to any source language and does not rely on any descriptive text for the entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_52",
            "start": 728,
            "end": 872,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_53@0",
            "content": "We compared our performance to baseline approaches on the Camoni dataset in Hebrew (Bitton et al., 2020), and the MedMentions (Mohan and Li, 2019) and BC5CDR English datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_53",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_53@1",
            "content": "Our end-toend approach achieves SOTA results on Camoni in Hebrew and MedMentions in English with significant improvements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_53",
            "start": 176,
            "end": 297,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_53@2",
            "content": "For BC5CDR, we observe that the small size of the dataset prevents our dictionary augmentation technique from reaching its potential and models trained on specialized biomedical text (PubMedBert with SapBert training objective) obtain better coverage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_53",
            "start": 299,
            "end": 549,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_53@3",
            "content": "Such specialized training is, however, not available in a multi-lingual setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_53",
            "start": 551,
            "end": 630,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_54@0",
            "content": "For future work, we intend to test whether utilizing language-specific BERT models instead of multilingual BERT (e.g., swapping m-BERT with the recently released AlephBERT (Seker et al., 2021), a Hebrew version of BERT) could improve results on the Hebrew Camoni corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_54",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_54@1",
            "content": "In addition, taking into account the SapBERT objective which exploits the UMLS graph structure as part of either fine-tuning or pre-training in Hebrew could lead to improved generalization capabilities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_54",
            "start": 271,
            "end": 472,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_54@2",
            "content": "Finally, exploring datasets with additional source languages will help understand the capabilities of our multilingual pipeline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_54",
            "start": 474,
            "end": 601,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_54@3",
            "content": "The CLEF eHealth challenges (N\u00e9v\u00e9ol et al., 2017(N\u00e9v\u00e9ol et al., , 2018 are good candidates for such analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_54",
            "start": 603,
            "end": 711,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_55@0",
            "content": "We compared the performance (recall %) using two different score functions: (1) cosine similarity and (2) Manhattan distance, and two different vectorization techniques: (1) term frequency (tf) and",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_55",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_56@0",
            "content": "(2) tf-idf (term frequency * inverse document frequency).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_56",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_56@1",
            "content": "We used character unigram, bigram and trigram analysis in all the reported cases (Table 8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_56",
            "start": 58,
            "end": 148,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_57@0",
            "content": "We hypothesize that the improvement stems from Idf penalizing frequent words by taking the log of {number of docs in the corpus divided by the number of docs in which the term appears}, where in our context, a 'doc' is either a span of text or a UMLS concept from C L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_57",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_57@1",
            "content": "Since no stop words can appear at either the start or end of the span/concept, we increase the odds of having meaningful words comprising each 'doc'.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_57",
            "start": 270,
            "end": 418,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_57@2",
            "content": "The tf-idf method may contribute to this further because it not only focuses on the frequency of words present in the corpus (tf, bag of word) but also provides an importance weight to them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_57",
            "start": 420,
            "end": 609,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_58@0",
            "content": "UNKNOWN, None, 2019, Scibert: Pretrained contextualized embeddings for scientific text, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_58",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_59@0",
            "content": "Yonatan Bitton, Raphael Cohen, Tamar Schifter, Eitan Bachmat, Michael Elhadad, and No\u00e9mie Elhadad. 2020. Cross-lingual Unified Medical Language System entity linking in online health communities, , Journal of the American Medical Informatics Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_59",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_60@0",
            "content": "UNKNOWN, None, , The camoni global network, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_60",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_61@0",
            "content": "UNKNOWN, None, , Jianfeng Gao, and Hoifung Poon. 2020. Domainspecific language model pretraining for biomedical natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_61",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_62@0",
            "content": "UNKNOWN, None, 2019, Bert: Pretraining of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_62",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_63@0",
            "content": "Robert Leaman, Zhiyong Lu, Taggerone: joint named entity recognition and normalization with semi-markov models, 2016, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_63",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_64@0",
            "content": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Biobert: a pre-trained biomedical language representation model for biomedical text mining, 2020, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_64",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_65@0",
            "content": "UNKNOWN, None, 2016, Biocreative v cdr task corpus: a resource for chemical disease relation extraction, Database.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_65",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_66@0",
            "content": "Carolyn Lipscomb, Medical subject headings (mesh), 2000, Bulletin of the Medical Library, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_66",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_67@0",
            "content": "Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, Nigel Collier, Self-alignment pretraining for biomedical entity representations, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_67",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_68@0",
            "content": "Daniel Loureiro, Al\u00edpio M\u00e1rio Jorge , Medlinker: Medical entity linking with neural representations and dictionary matching, 2020, European Conference on Information Retrieval, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_68",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_69@0",
            "content": "UNKNOWN, None, 2021, Low resource recognition and linking of biomedical concepts from a large ontology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_69",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_70@0",
            "content": "UNKNOWN, None, 2019, Medmentions: a large biomedical corpus annotated with umls concepts, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_70",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_71@0",
            "content": "Aur\u00e9lie N\u00e9v\u00e9ol, Aude Robert, Robert Anderson, Kevin Cohen, Cyril Grouin, Thomas Lavergne, Gr\u00e9goire Rey, Claire Rondet, Pierre Zweigenbaum, Clef ehealth 2017 multilingual information extraction task overview: Icd10 coding of death certificates in english and french, 2017, CLEF (Working Notes), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_71",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_72@0",
            "content": "Aur\u00e9lie N\u00e9v\u00e9ol, Aude Robert, Francesco Grippo, Claire Morgand, Chiara Orsi, Laszlo Pelikan, Lionel Ramadier, Gr\u00e9goire Rey, Pierre Zweigenbaum, Clef ehealth 2018 multilingual information extraction task overview: Icd10 coding of death certificates in french, hungarian and italian, 2018, CLEF (Working Notes), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_72",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_73@0",
            "content": "F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Scikit-learn: Machine learning in Python, 2011, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_73",
            "start": 0,
            "end": 274,
            "label": {}
        },
        {
            "ix": "434-ARR_v1_74@0",
            "content": "UNKNOWN, None, 2021, Alephbert: A hebrew large pretrained language model to start-off your hebrew NLP application with, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v1_74",
            "start": 0,
            "end": 124,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "434-ARR_v1_0",
            "tgt_ix": "434-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_0",
            "tgt_ix": "434-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_1",
            "tgt_ix": "434-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_1",
            "tgt_ix": "434-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_1",
            "tgt_ix": "434-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_2",
            "tgt_ix": "434-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_0",
            "tgt_ix": "434-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_3",
            "tgt_ix": "434-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_4",
            "tgt_ix": "434-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_4",
            "tgt_ix": "434-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_4",
            "tgt_ix": "434-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_0",
            "tgt_ix": "434-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_6",
            "tgt_ix": "434-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_8",
            "tgt_ix": "434-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_9",
            "tgt_ix": "434-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_10",
            "tgt_ix": "434-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_7",
            "tgt_ix": "434-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_7",
            "tgt_ix": "434-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_7",
            "tgt_ix": "434-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_7",
            "tgt_ix": "434-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_7",
            "tgt_ix": "434-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_0",
            "tgt_ix": "434-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_13",
            "tgt_ix": "434-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_14",
            "tgt_ix": "434-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_12",
            "tgt_ix": "434-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_12",
            "tgt_ix": "434-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_12",
            "tgt_ix": "434-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_12",
            "tgt_ix": "434-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_0",
            "tgt_ix": "434-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_15",
            "tgt_ix": "434-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_17",
            "tgt_ix": "434-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_16",
            "tgt_ix": "434-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_16",
            "tgt_ix": "434-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_16",
            "tgt_ix": "434-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_16",
            "tgt_ix": "434-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_18",
            "tgt_ix": "434-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_19",
            "tgt_ix": "434-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_19",
            "tgt_ix": "434-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_16",
            "tgt_ix": "434-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_20",
            "tgt_ix": "434-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_21",
            "tgt_ix": "434-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_21",
            "tgt_ix": "434-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_16",
            "tgt_ix": "434-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_22",
            "tgt_ix": "434-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_23",
            "tgt_ix": "434-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_23",
            "tgt_ix": "434-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_16",
            "tgt_ix": "434-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_24",
            "tgt_ix": "434-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_26",
            "tgt_ix": "434-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_27",
            "tgt_ix": "434-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_25",
            "tgt_ix": "434-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_25",
            "tgt_ix": "434-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_25",
            "tgt_ix": "434-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_25",
            "tgt_ix": "434-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_16",
            "tgt_ix": "434-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_28",
            "tgt_ix": "434-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_30",
            "tgt_ix": "434-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_31",
            "tgt_ix": "434-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_32",
            "tgt_ix": "434-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_33",
            "tgt_ix": "434-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_29",
            "tgt_ix": "434-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_29",
            "tgt_ix": "434-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_29",
            "tgt_ix": "434-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_29",
            "tgt_ix": "434-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_29",
            "tgt_ix": "434-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_29",
            "tgt_ix": "434-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_0",
            "tgt_ix": "434-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_34",
            "tgt_ix": "434-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_35",
            "tgt_ix": "434-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_35",
            "tgt_ix": "434-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_35",
            "tgt_ix": "434-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_36",
            "tgt_ix": "434-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_38",
            "tgt_ix": "434-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_37",
            "tgt_ix": "434-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_37",
            "tgt_ix": "434-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_37",
            "tgt_ix": "434-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_35",
            "tgt_ix": "434-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_39",
            "tgt_ix": "434-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_41",
            "tgt_ix": "434-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_40",
            "tgt_ix": "434-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_40",
            "tgt_ix": "434-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_40",
            "tgt_ix": "434-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_35",
            "tgt_ix": "434-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_42",
            "tgt_ix": "434-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_43",
            "tgt_ix": "434-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_43",
            "tgt_ix": "434-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_35",
            "tgt_ix": "434-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_45",
            "tgt_ix": "434-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_45",
            "tgt_ix": "434-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_35",
            "tgt_ix": "434-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_46",
            "tgt_ix": "434-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_47",
            "tgt_ix": "434-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_47",
            "tgt_ix": "434-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_35",
            "tgt_ix": "434-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_48",
            "tgt_ix": "434-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_49",
            "tgt_ix": "434-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_49",
            "tgt_ix": "434-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_0",
            "tgt_ix": "434-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_50",
            "tgt_ix": "434-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_52",
            "tgt_ix": "434-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_53",
            "tgt_ix": "434-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_51",
            "tgt_ix": "434-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_51",
            "tgt_ix": "434-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_51",
            "tgt_ix": "434-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_51",
            "tgt_ix": "434-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_55",
            "tgt_ix": "434-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_56",
            "tgt_ix": "434-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_51",
            "tgt_ix": "434-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_51",
            "tgt_ix": "434-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_51",
            "tgt_ix": "434-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_54",
            "tgt_ix": "434-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v1_0",
            "tgt_ix": "434-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_1",
            "tgt_ix": "434-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_2",
            "tgt_ix": "434-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_2",
            "tgt_ix": "434-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_2",
            "tgt_ix": "434-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_3",
            "tgt_ix": "434-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_3",
            "tgt_ix": "434-ARR_v1_3@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_3",
            "tgt_ix": "434-ARR_v1_3@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_3",
            "tgt_ix": "434-ARR_v1_3@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_3",
            "tgt_ix": "434-ARR_v1_3@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_4",
            "tgt_ix": "434-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_5",
            "tgt_ix": "434-ARR_v1_5@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_6",
            "tgt_ix": "434-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_6",
            "tgt_ix": "434-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_6",
            "tgt_ix": "434-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_6",
            "tgt_ix": "434-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_6",
            "tgt_ix": "434-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_6",
            "tgt_ix": "434-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_7",
            "tgt_ix": "434-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_8",
            "tgt_ix": "434-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_8",
            "tgt_ix": "434-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_8",
            "tgt_ix": "434-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_8",
            "tgt_ix": "434-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_8",
            "tgt_ix": "434-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_8",
            "tgt_ix": "434-ARR_v1_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_8",
            "tgt_ix": "434-ARR_v1_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_9",
            "tgt_ix": "434-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_9",
            "tgt_ix": "434-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_9",
            "tgt_ix": "434-ARR_v1_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_9",
            "tgt_ix": "434-ARR_v1_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_9",
            "tgt_ix": "434-ARR_v1_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_9",
            "tgt_ix": "434-ARR_v1_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_9",
            "tgt_ix": "434-ARR_v1_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_10",
            "tgt_ix": "434-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_10",
            "tgt_ix": "434-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_10",
            "tgt_ix": "434-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_10",
            "tgt_ix": "434-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_10",
            "tgt_ix": "434-ARR_v1_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_10",
            "tgt_ix": "434-ARR_v1_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_11@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_11",
            "tgt_ix": "434-ARR_v1_11@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_12",
            "tgt_ix": "434-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_13",
            "tgt_ix": "434-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_13",
            "tgt_ix": "434-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_13",
            "tgt_ix": "434-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_13",
            "tgt_ix": "434-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_14",
            "tgt_ix": "434-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_15",
            "tgt_ix": "434-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_15",
            "tgt_ix": "434-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_15",
            "tgt_ix": "434-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_16",
            "tgt_ix": "434-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_17",
            "tgt_ix": "434-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_18",
            "tgt_ix": "434-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_18",
            "tgt_ix": "434-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_19",
            "tgt_ix": "434-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_20",
            "tgt_ix": "434-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_20",
            "tgt_ix": "434-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_21",
            "tgt_ix": "434-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_22",
            "tgt_ix": "434-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_22",
            "tgt_ix": "434-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_22",
            "tgt_ix": "434-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_22",
            "tgt_ix": "434-ARR_v1_22@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_22",
            "tgt_ix": "434-ARR_v1_22@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_23",
            "tgt_ix": "434-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_24",
            "tgt_ix": "434-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_24",
            "tgt_ix": "434-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_24",
            "tgt_ix": "434-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_25",
            "tgt_ix": "434-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_26",
            "tgt_ix": "434-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_26",
            "tgt_ix": "434-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_26",
            "tgt_ix": "434-ARR_v1_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_26",
            "tgt_ix": "434-ARR_v1_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_26",
            "tgt_ix": "434-ARR_v1_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_26",
            "tgt_ix": "434-ARR_v1_26@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_27",
            "tgt_ix": "434-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_28",
            "tgt_ix": "434-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_29",
            "tgt_ix": "434-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_30",
            "tgt_ix": "434-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_30",
            "tgt_ix": "434-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_31",
            "tgt_ix": "434-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_31",
            "tgt_ix": "434-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_32",
            "tgt_ix": "434-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_33",
            "tgt_ix": "434-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_34",
            "tgt_ix": "434-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_34",
            "tgt_ix": "434-ARR_v1_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_34",
            "tgt_ix": "434-ARR_v1_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_35",
            "tgt_ix": "434-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_36",
            "tgt_ix": "434-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_37",
            "tgt_ix": "434-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_38",
            "tgt_ix": "434-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_38",
            "tgt_ix": "434-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_38",
            "tgt_ix": "434-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_38",
            "tgt_ix": "434-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_38",
            "tgt_ix": "434-ARR_v1_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_38",
            "tgt_ix": "434-ARR_v1_38@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_38",
            "tgt_ix": "434-ARR_v1_38@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_38",
            "tgt_ix": "434-ARR_v1_38@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_39",
            "tgt_ix": "434-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_39",
            "tgt_ix": "434-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_40",
            "tgt_ix": "434-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_41",
            "tgt_ix": "434-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_41",
            "tgt_ix": "434-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_41",
            "tgt_ix": "434-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_41",
            "tgt_ix": "434-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_41",
            "tgt_ix": "434-ARR_v1_41@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_42",
            "tgt_ix": "434-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_42",
            "tgt_ix": "434-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_42",
            "tgt_ix": "434-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_42",
            "tgt_ix": "434-ARR_v1_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_42",
            "tgt_ix": "434-ARR_v1_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_43",
            "tgt_ix": "434-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_44",
            "tgt_ix": "434-ARR_v1_44@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_45",
            "tgt_ix": "434-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_46",
            "tgt_ix": "434-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_46",
            "tgt_ix": "434-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_46",
            "tgt_ix": "434-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_47",
            "tgt_ix": "434-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_48",
            "tgt_ix": "434-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_48",
            "tgt_ix": "434-ARR_v1_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_48",
            "tgt_ix": "434-ARR_v1_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_48",
            "tgt_ix": "434-ARR_v1_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_49",
            "tgt_ix": "434-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_50",
            "tgt_ix": "434-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_50",
            "tgt_ix": "434-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_50",
            "tgt_ix": "434-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_50",
            "tgt_ix": "434-ARR_v1_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_51",
            "tgt_ix": "434-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_52",
            "tgt_ix": "434-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_52",
            "tgt_ix": "434-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_52",
            "tgt_ix": "434-ARR_v1_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_53",
            "tgt_ix": "434-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_53",
            "tgt_ix": "434-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_53",
            "tgt_ix": "434-ARR_v1_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_53",
            "tgt_ix": "434-ARR_v1_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_54",
            "tgt_ix": "434-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_54",
            "tgt_ix": "434-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_54",
            "tgt_ix": "434-ARR_v1_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_54",
            "tgt_ix": "434-ARR_v1_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_55",
            "tgt_ix": "434-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_56",
            "tgt_ix": "434-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_56",
            "tgt_ix": "434-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_57",
            "tgt_ix": "434-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_57",
            "tgt_ix": "434-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_57",
            "tgt_ix": "434-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_58",
            "tgt_ix": "434-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_59",
            "tgt_ix": "434-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_60",
            "tgt_ix": "434-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_61",
            "tgt_ix": "434-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_62",
            "tgt_ix": "434-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_63",
            "tgt_ix": "434-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_64",
            "tgt_ix": "434-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_65",
            "tgt_ix": "434-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_66",
            "tgt_ix": "434-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_67",
            "tgt_ix": "434-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_68",
            "tgt_ix": "434-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_69",
            "tgt_ix": "434-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_70",
            "tgt_ix": "434-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_71",
            "tgt_ix": "434-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_72",
            "tgt_ix": "434-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_73",
            "tgt_ix": "434-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v1_74",
            "tgt_ix": "434-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1081,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "434-ARR",
        "version": 1
    }
}