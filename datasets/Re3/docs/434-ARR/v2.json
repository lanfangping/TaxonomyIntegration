{
    "nodes": [
        {
            "ix": "434-ARR_v2_0",
            "content": "Cross-Lingual UMLS Named Entity Linking using UMLS Dictionary Fine-Tuning",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_2",
            "content": "We study cross-lingual UMLS named entity linking, where mentions in a given source language are mapped to UMLS concepts, most of which are labeled in English. Our crosslingual framework includes an offline unsupervised construction of a translated UMLS dictionary and a per-document pipeline which identifies UMLS candidate mentions and uses a finetuned pretrained transformer language model to filter candidates according to context. Our method exploits a small dataset of manually annotated UMLS mentions in the source language and uses this supervised data in two ways: to extend the unsupervised UMLS dictionary and to fine-tune the contextual filtering of candidate mentions in full documents. We demonstrate results of our approach on both Hebrew and English. We achieve new state-of-the-art (SOTA) results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset. We also achieve new SOTA on the English dataset MedMentions with +7.3 F1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "434-ARR_v2_4",
            "content": "Public health practices are becoming increasingly digital, with tools to explore scientific sources of information such as medical literature and online health communities rising in popularity. Such tools are essential in offering insights to researchers, providing information to patients and to their caregivers. Reliable identification of mentions of biomedical concepts in free text is a key technique to enable robust mining of such textual resources. Named-Entity Recognition (NER) is the task of classifying entities in text to high level classes (Person, Organization, Gene, Disease, Treatment, etc.).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_5",
            "content": "Named-Entity Linking (NEL) seeks to additionally classify entity mentions in text into specific concepts according to an existing reference list or knowledge base. We focus in this work on biomedical NEL, i.e., identifying mentions referring to biomedical concepts such as disorders and drugs and linking them to normalized concepts, for example, concept unique identifiers (CUIs) listed in the Unified Medical Language System (UMLS) controlled vocabulary. Biomedical NEL has been mostly studied in English. Other languages present additional challenges because terms in the ontology are described in English. We address crosslingual NEL which consists of mapping mentions in a source language to concepts labeled and described in a different target language. We focus on cross-lingual UMLS NEL, where mentions in the source language (we specifically test Hebrew, see Figure 1 for a Hebrew tagging example) are mapped to UMLS concepts. We aim for a general solution that can be adapted to any source language. We operate in a low resource setting, where the ontology is large, text describing most entities is not available, and labeled data can only cover a small portion of the ontology. We also consider different genres of text to be annotated, ranging from consumer health medical articles in popular web sites to scientific biomedical articles.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_6",
            "content": "Our main contributions are: (1) We provide a general framework for cross-lingual UMLS NEL that can be adapted to source languages with few pre-requisites; (2) Our method exploits a small annotated corpus of documents in the source language and genre annotated manually for UMLS mentions (a few thousands annotated mentions). This training data is split to support (a) the extension of the unsupervised UMLS dictionary with corpus-salient entity names and (b) fine-tune the contextual ranking and filtering of (candidate mentions, concept) pairs. We find that the step of UMLS dictionary fine-tuning boosts NEL performance and identify a clear tradeoff in allocating training data between lexicon extension and contextual fine-tuning; (3) We demonstrate results of our approach on both Hebrew and English. We achieve new SOTA on the Hebrew Camoni corpus (Bitton et al., 2020) with +8.87 F1 and on the English dataset MedMentions (Mohan and Li, 2019) with +7.3 F1 1 .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_7",
            "content": "Previous Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "434-ARR_v2_8",
            "content": "Biomedical NEL is challenging because the underlying ontology (most often UMLS) is extremely large and the acquisition of annotated training data requires rare and expensive expertise. Loureiro and Jorge (2020) presented MedLinker, a tool for improving biomedical NEL by predicting the semantic type of a medical concept mention and filtering out candidates of the wrong type. MedLinker was tested on the MedMentions task of concept linking (Mohan and Li, 2019), improving above TaggerOne , the baseline model for MedMentions which did not use deep learning. MedLinker splits the end to end task of entity linking into two stages -candidate recognition and linking. For candidate matching, it combines a BiLSTM-CRF model for contextual matching with an approximate dictionary matching method to increase recall. In the cross-lingual setting, dictionary matching is not applicable. We report our results on the same MedMentions dataset in Section 5.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_9",
            "content": "Past work has shown that using in-domain text can provide additional gains over general-domain language models (Gu et al., 2020). Therefore, recent work (BioBERT (Lee et al., 2020), SciB-ERT (Beltagy et al., 2019)) addressed biomedical NEL, focusing on pre-training models on scientific/medical text. Liu et al. (2021) developed Sap-BERT, a pre-training scheme which exploits the graph structure of the UMLS controlled vocabulary and aims at learning an encoding of medical mentions that can align with synonym relations in the UMLS graph. Combining the SapBERT objective with pre-training on biomedical text of PubMed-BERT (Gu et al., 2020) boosts results on NEL. Experimental results demonstrated that SapBERT outperforms many domain-specific BERT-based variants (BioBERT and SciBERT) on the BC5CDR (BioCreative V CDR) corpus. Although our model focuses on cross-lingual NEL, it also applies to English documents. We compare our results to 1 Our source code is publicly available on GitHub https://github.com/rinagalperin/biomedical_nel these approaches on BC5CDR and MedMentions (Tables 4 and 3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_10",
            "content": "Indexing of the abundant biomedical scientific literature requires precise detection of medical concepts. Mohan et al. (2021) developed a lowresource recognition and linking model of biomedical concepts (henceforth referred to as LRR) aimed at generalizing to entities unseen at training time, and incorporating linking predictions into the mention segmentation decisions. This BERT-based model achieved SOTA results on the MedMentions task. In our work, we adopt the LRR bottom-up candidate generation approach (see Section 4.2). We address the main drawback of the approach by incorporating a UMLS dictionary fine-tuning technique which extends the list of candidate pairs (source expression, CUI) on a portion of the training data. We elaborate on the motivation for the technique in Section 4.5 and demonstrate its contribution in ablation experiments (see Section 5.4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_11",
            "content": "Cross-lingual NEL, the problem of grounding mentions of entities in a source language text into a different target language knowledge base (typically English), has been addressed in recent years, with a range of promising techniques. When the source and target languages operate over different alphabets and sound systems, both translation and transliteration of terms (which is a noisy process even when done by people) must be handled. Bitton et al. (2020) curated the Camoni corpus, an annotated resource of Hebrew posts from online health communities (OHCs), where noisy text (as opposed to scientific text) introduces additional challenges. Many user queries mention medical terms, which are very likely to include noisy transliterations. For example, the Hebrew query equivalent to \"How do I know I have fibromyalgia?\" does not return any results in the search engine of the Camoni online community when 'fibromialgia' is transliterated. Bitton et al. (2020) introduced MDTEL (Medical Deep Transliteration Entity Linking) for Hebrew-English NEL on noisy text in OHCs, and tested it on the Camoni corpus. MDTEL adopts a fourstep approach -consisting of an offline unsupervised Hebrew UMLS dictionary learning, candidate mention generation, high-recall matching and filtering of matching mentions. We adopt MDTEL's unsupervised UMLS dictionary matching, which uses an attention-based recurrent neural network encoder-decoder that maps UMLS from English to Hebrew (either a Hebrew translation or translit-Figure 1: A forum post from the Camoni sclerosis community that translates to: \"Hello, recently, my gait has deteriorated and I was suggested to begin Botox treatment to release the muscles and prevent spasticity. Has anyone here undergone such treatment? does it help? is there a risk that such treatment will greatly weaken the muscle, causing the exact opposite action?\". The post contains 37 words and 6 spans that link to 4 different CUIs of UMLS concepts. Notice that a span can consist of more than 1 word (such as the term matched with \"gait abnormality\") and a single CUI can be referenced from several places in the same post (such as the CUI of \"General Treatment\").",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_12",
            "content": "eration of the concept). We introduce new methods for candidate generation, high-recall matching and contextual relevance filtering, relying on multilingual pre-trained language model (mBERT). Our new components lead to significant performance improvement over MDTEL on the Camoni corpus (see Table 2).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_13",
            "content": "Task Formulation",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "434-ARR_v2_14",
            "content": "Given input language L and target language L t , a database of medical concepts C Lt : L * t \u2192 CU I is a function from concept names in L t to concept IDs (CUIs). Using C Lt , we want to learn a function F from a span in input language L and its context to a CUI. We identify dictionary C L : L * \u2192 CU I. C L is the translated version of the medical concepts database C Lt . We learn C L by mapping the medical terms in L t to terms in L. Given mapping C L , we aim to learn:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_15",
            "content": "F : L * \u00d7 L * \u2192 CU I \u222a {\u22a5}",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_16",
            "content": "where \u22a5 is a special code denoting a non-medical term. F differs from C L as it addresses the variability and ambiguity of the task by depending on the context as well as the span. Given text W = (w 1 , ..., w n ), where w i \u2208 L, for every span s i,j = (w i , ..., w j ) \u2286 W , we would like to compute F (W, s i,j ), where 0 \u2264 j \u2212 i < k (we limit the span sizes to at most k), that is, we want to predict the concept associated with a span under context W in language L. Provided a dataset A L exposing a subset of F combined with linguistic knowledge and generalization capabilities of neural models, we aim at learning a larger portion of function F .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_17",
            "content": "Model Architecture",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "434-ARR_v2_18",
            "content": "Our end-to-end cross-lingual UMLS NEL model (Figure 2) consists of four consecutive stages: (1) multilingual UMLS mapping: generate UMLS dictionary C L (see Section 4.1) based on the method of Bitton et al. (2020), and fine-tune it using our UMLS dictionary fine-tuning technique (see Section 4.5); (2) candidate generation: consider all spans of up to k words as candidate mentions and compute vector representations for both mentions and concepts (see Section 4.2); (3) high recall matching: use a semantic similarity based score function to generate the top matching entities with high recall (see Section 4.3) and ( 4) contextual relevance modeling: encode each candidate into a context-dependent vector representation using a pre-trained transformer-based language model fine tuning process (see Section 4.4).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_19",
            "content": "Our approach attempts to avoid three types of mistakes: (1) morphological and transliteration noise, where candidate terms in the source language might be extracted due to a transliteration or morphological error and matched with UMLS entities, (2) contextual errors, where candidate terms which are not medical terms when considering the context might be matched with UMLS entities, and (3) partial UMLS tagging, where candidate terms which are not the full medical terms in the text but rather more general UMLS mentions might be tagged instead of the full term (e.g., in the mention \"flu vaccine\", \"flu\" should not be tagged). The first challenge is addressed by learning a high-recall C L dictionary with generalization capabilities, trained both on translation and transliteration data; the second, is addressed by an mBERT-based contextual language model; the third, by systematic consideration of all spans up to size k as candidates as part of the candidate generation and contextual relevance components.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_20",
            "content": "Multilingual UMLS Mapping",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "434-ARR_v2_21",
            "content": "The first step of our model is offline, fully unsupervised, and based on the method of (Bitton et al., 2020): we generate a mapping C L between medical concept names in source language L to their corresponding CUIs. An attention-based characterbased recurrent neural network encoder-decoder is used to create a list of \u2329UMLS term in English, term in language L\u232a so that each UMLS term in English is matched with both transliterated and translated forms in L. This is done without the need of manually annotated data and results in a noisy mapping C L of source language medical terms and their CUIs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_22",
            "content": "Candidate Generation",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "434-ARR_v2_23",
            "content": "Given a document in L where we want to identify UMLS mentions, the candidate generation step begins with pre-processing: we normalize the source text documents from annotated data A L and the target UMLS concepts from C L by transforming all string values to lower case and removing delimiters. We then generate a list of overlapping candidate mention spans, ranging in length according to the max length parameter k (i.e., 1, ..., k. See Appendix A for details). We exclude spans starting or ending with stop words. We then represent both the spans and the concepts as tf-idf character n-gram (1 to 3-gram) vectors using sklearn's implementation (Pedregosa et al., 2011). Empirical experiments showed that tf-idf encoding improved recall in candidate generation compared to bag of words encoding (see Appendix B for a comparison between the two representations using both Hebrew and English datasets).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_24",
            "content": "High Recall Matching",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "434-ARR_v2_25",
            "content": "The high recall matcher (HRM) receives the vector representations from the candidate generator and computes a similarity score between each span and all concept names in C L using cosine similarity (see Appendix B for comparison against Manhattan score function). We then select the top m matches per span with score over a threshold th (see Appendix C for hyper-parameters). This results in a high recall list of candidate matches.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_26",
            "content": "Contextual Relevance Modeling",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "434-ARR_v2_27",
            "content": "At this step, we want to predict which spans returned from the high recall matcher are true biomedical concepts. We use multilingual BERT (m-BERT) (Jacob Devlin, 2019), a 12 layer transformer that was trained on the Wikipedia pages of 104 languages (including Hebrew) with a shared word piece vocabulary. M-BERT does not use any marker denoting the input language, and does not include explicit mechanism to encourage translation equivalent pairs to have similar representations. We fine-tune m-BERT on a binary classification task on our training data: each candidate mention span returned from the HRM is centered in its context from the original doc, i.e., W s words to the right of the span and W s words to the left of the span, creating a window surrounding the candidate mention. The classifier takes as input the window, the HRM's decision on which concept is represented by the mention in the window, and the true verdict of whether the candidate mention is indeed an occurrence of the concept. We utilize m-BERT's QA format as follows: the question (medical concept c) and the reference text (window w) are packed into the input, and provide the binary label as answer of whether or not c is a medical mention in context w:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_28",
            "content": "[CLS] w [SEP ] c [SEP ].",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_29",
            "content": "This fine-tuning step consists of adding an additional output layer on top of the pre-trained m-BERT model to adapt it to the biomedical NEL task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_30",
            "content": "UMLS Dictionary Fine-Tuning",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "434-ARR_v2_31",
            "content": "We introduce a UMLS dictionary finetuning (UMLS DFT) technique where some of the data in A L is removed from the training dataset and used to directly expand the learned dictionary C L . We reserve R% of the training data A L to fine-tune C L generating C \u2032 L (see Figure 2): from this chunk of A L , we add each mention in the tagged data as new pairs (mention in L, CUI).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_32",
            "content": "For example, suppose our training data consists of 10 tagged documents and our UMLS dictionary C L contains 100 concepts. Given R = 10%, our UMLS dictionary fine-tuning technique will require one tagged document d (10% of the 10 docs in the training set) to be used for fine-tuning C L . We go over every tagged pair (m, c) from doc d, where m is a mention in doc d and c is the UMLS concept the annotators tagged m. If m \u0338 \u2208 C L , we add m to C L with the CUI of c. Suppose doc d contained 15 such tags, we will obtain an augmented C \u2032 L containing 100 + 15 = 115 concepts. We cannot use this portion of data for later training of our model, since after fine-tuning we are guaranteed to get a perfect match for all the spans in the documents used for fine-tuning (thus creating bias of the HRM). Although this process decreases the overall size of the input dataset for contextual relevance fine-tuning, it improves the recall of the HRM and adds more positive examples for the BERT training process. We elaborate more on this trade-off in Section 5.4.2. This approach allows us to improve recall on synonyms and abbreviations that were not originally in our UMLS dictionary, with genrespecific terminology observed in the training data (as evident from the experiment shown in Table 5).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_33",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "434-ARR_v2_34",
            "content": "We test our approach both on cross-lingual UMLS Linking using the Camoni dataset of Hebrew consumer health data and on English UMLS Linking using MedMentions and BC5CDR, which include scientific papers in the bio-medical field.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_35",
            "content": "Camoni Corpus",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "434-ARR_v2_36",
            "content": "The Camoni corpus was curated by Bitton et al. (2020) for the analysis of the MDTEL system. Camoni is an Israeli social network in Hebrew aimed at patients with chronic diseases and their family members (Camoni). Camoni serves about 20,000 registered members and 100,000 unique visitors per month. The digital platform is organized into 39 disease-specific communities. Bitton et al. (2020) extracted text from three communities (diabetes, sclerosis, and depression), for a total of 55,000 posts and 2.5 million tokens, and constructed an annotated dataset in which 1,000 mentions of UMLS terms were annotated. Bitton et al. (2020) proposed a high recall matcher based on a fuzzy string matching algorithm introduced in prior work to perform the matching between the spans and medical entities. Table 1 compares our HRM results (recall) with MDTEL for each community (diabetes, depression, sclerosis).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_37",
            "content": "We observe that our candidate generation method (adopting the LRR bottom-up approach and mBERT similarity matching) significantly improves the recall of the HRM (average of 74% using MDTEL's approach vs. average of 82% using our method). We believe that the use of the tf-idf character n-gram vectorization before applying the cosine similarity function as means of comparison helped us achieve better results compared to MDTEL's method which only applied the cosine similarity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_38",
            "content": "In the end to end linking task, our model achieves much higher precision (98% vs. 77%) without affecting the recall (73%), resulting in much improved F-score (84% vs 74%). model on the end to end linking task for each community.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_39",
            "content": "MedMentions",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "434-ARR_v2_40",
            "content": "MedMentions (Mohan and Li, 2019) is a corpus of Biomedical papers annotated with mentions of UMLS entities. The corpus consists of 4,392 papers (Titles and Abstracts) randomly selected from papers released on PubMed in 2016, that were in the biomedical field, published in the English language, and had both a Title and an Abstract available. MedMentions contains over 350,000 linked mentions, annotated by a team of professional annotators with rich experience in biomedical content curation. We focus on MedMentions ST21pv (21 Semantic Types and Preferred Vocabularies), a subset of the full annotations containing 203,282 mentions and restricting the concepts to a 2.3M large subset of the full ontology (UMLS ST21pv). Each concept in this subset is associated with one of 21 selected semantic types, or to one of their descendants in the semantic type hierarchy. We compare our performance to other models' results on MedMentions ST21pv in Table 3. We improve on the latest SOTA LRR (Mohan et al., 2021), achieving +7.3 F1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_41",
            "content": "Our recall was similar to LRR, however our model achieved highly improved precision, 76.4 compared to 63. We believe this improvement can be attributed to our UMLS dictionary finetuning technique, which provides an extended list of candidates and thus more examples for the mBERT fine-tuning process for contextual relevance. Mohan et al. (2021) mention the need to improve recall for cases where the mentions are indirect or too abbreviated to generate a good lexical match from the entity knowledge base, which is exactly what our technique helps improve. For example, our process picked up in the training data that the abbreviation mrn is tagged as messenger rna (CUI C0035696), which was not originally present in the UMLS dictionary for English.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_42",
            "content": "BC5CDR",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "434-ARR_v2_43",
            "content": "The BC5CDR corpus (Li et al., 2016) consists of 1,500 PubMed articles with 4,409 annotated chemicals, 5,818 diseases and 3,116 chemical-disease interactions. Each entity annotation includes both the mention text spans and normalized concept identifiers, using MeSH (Medical Subject Headings) (Lipscomb, 2000) as the controlled vocabulary (MeSH is part of the UMLS controlled vocabulary). Compared to MedMentions which contains annotations of general medical concepts, BC5CDR is topic-specific, containing only annotations of chemicals and diseases. BC5CDR is also much smaller, consisting of just 1,500 articles compared to the 4,392 annotated papers of MedMentions. BC5CDR has a total of 13,343 linked mentions compared to 203,282 in MedMentions ST21pv.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_44",
            "content": "We compare our model's performance to other models using BC5CDR's test set in Table 4. We observe that domain-specific pre-trained transformers help improve results on BC5CDR (93.5 F-measure vs. 73 for our model). The subset of semantic types covered in this dataset is much more technical (chemicals and chemical-disease interactions) than those covered in MedMentions, even though both BC5CDR and MedMentions include documents in the same genre of scientific biomedical articles. This difference is evidenced in the ablation study presented below. It explains why specialized language models trained on the biomedical domain lead to much improved performance compared to our model which uses the general mBERT. We hypothesize that using SapBERT combined with our model could enhance performance on this dataset and leave this for future work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_45",
            "content": "UMLS Dictionary Fine-Tuning Ablation Study",
            "ntype": "title",
            "meta": {
                "section": "5.4"
            }
        },
        {
            "ix": "434-ARR_v2_46",
            "content": "In this section, we test several factors impacting the contribution of UMLS dictionary fine-tuning to our tagger's performance. First, we test the technique on two different datasets and evaluate its benefits depending on the dataset size. Next, we test a range of UMLS dictionary fine-tuning percentage values (R) and discuss the trade-off between this value and the end to end performance of our linker.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_47",
            "content": "Dataset Size Impact",
            "ntype": "title",
            "meta": {
                "section": "5.4.1"
            }
        },
        {
            "ix": "434-ARR_v2_48",
            "content": "We tested the UMLS dictionary fine-tuning technique on English datasets MedMentions and BC5CDR across 5 random seeds and found that it improved recall on both, but impacting MedMentions much more than BC5CDR due to a much smaller number of added concepts in BC5CDR, 209 compared to 3,294 in MedMentions (see Table 5). The difference in the number of added concepts could be explained by the fact that BC5CDR is much smaller, thus the decrease in training data size counteracts the small number of concepts being added to the UMLS dictionary. To test this hypothesis, we took a subset of MedMentions of the same size as BC5CDR (annotation-wise: 8,575 in total), see Table 6 for results averaged across 5 random seeds. The results suggest that the size of the dataset directly affects the number of concepts added to our UMLS dictionary (227 added in the MedMentions subset, very close to the 209 added in BC5CDR), which in turn impacts the HRM's recall: the improvement in recall is very similar between the two datasets, +1.37 for BC5CDR, +1.7 for MedMentions subset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_49",
            "content": "The Recall-Accuracy Tradeoff",
            "ntype": "title",
            "meta": {
                "section": "5.4.2"
            }
        },
        {
            "ix": "434-ARR_v2_50",
            "content": "We first observe that our UMLS dictionary finetuning (DFT) technique can only improve the high recall matching performance (Section 4.3) since an annotation that we do not have a good semantic match for from UMLS will be a missed match without UMLS DFT. Similarly, an annotation for which we do have a good semantic match will be found regardless of whether we utilize UMLS DFT or not.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_51",
            "content": "Thus, UMLS dictionary fine-tuning helps us find non-semantically similar matches that we would have otherwise missed, meaning that the higher R is -the higher the recall of the HRM should be. However, there is a trade-off between the recall gained from the annotations utilized for UMLS dictionary fine-tuning and the overall performance of the linker, since the annotations used for finetuning are examples that the contextual model will be missing during fine-tuning. We explore this trade-off and compare the performance of the high recall matching component with the final tagging results of our model using different values of R on the MedMentions dataset. Figure 3 shows that there is a clear trend of increased recall of the HRM as R increases. However, Figure 4 shows the complexity of the trade-off since the tagger's performance reaches a peak and then begins to drop as R increases. The contextual model fine-tuning improvement plateaus after a certain amount of training examples, demonstrating the benefit of multi-task adaptation of pre-trained models which converge rapidly. The data efficiency of the contextual relevance fine-tuning process allows the UMLS dictionary fine-tuning technique to help improve end to end linking results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_52",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "434-ARR_v2_53",
            "content": "In this work we explored the task of cross lingual named entity linking in the biomedical field. We describe a pipeline to detect and link mentions of UMLS concepts in documents in Hebrew or in English, which improves upon existing methods. Table 5: Number of added concepts per dataset and the average performance of the HRM with and without UMLS dictionary fine-tuning, across 5 random seeds. \"\u2717\": UMLS DFT not used, \"\u2713\": UMLS DFT used. 2020) which takes into account both translation and transliteration but extends this dictionary with a portion of the training data mentions; empirical analysis of this dictionary augmentation method demonstrates its importance in end to end linking performance; (3) it adopts the bottom-up systematic generation of candidates from Mohan et al. (2021) and improves it by using a compact tf*idf ranking of the candidates (char n-gram) which helps reduce memory allocation; (4) it uses a multi-lingual pre-trained language model (mBERT) to fine-tune a contextual relevance model to filter a list of high-recall candidate matches. Our framework for cross-lingual UMLS NEL can easily be adapted to any source language and does not rely on any descriptive text for the entities.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_54",
            "content": "We compared our performance to baseline approaches on the Camoni dataset in Hebrew (Bitton et al., 2020), and the MedMentions (Mohan and Li, 2019) and BC5CDR English datasets. Our end-toend approach achieves SOTA results on Camoni in Hebrew and MedMentions in English with significant improvements. For BC5CDR, we observe that the small size of the dataset prevents our dictionary augmentation technique from reaching its potential and models trained on specialized biomedical text (PubMedBert with SapBert training objective) obtain better coverage. Such specialized training is, however, not available in a multi-lingual setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "434-ARR_v2_55",
            "content": "UNKNOWN, None, 2019, Scibert: Pretrained contextualized embeddings for scientific text, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Scibert: Pretrained contextualized embeddings for scientific text",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_56",
            "content": "Yonatan Bitton, Raphael Cohen, Tamar Schifter, Eitan Bachmat, Michael Elhadad, and No\u00e9mie Elhadad. 2020. Cross-lingual Unified Medical Language System entity linking in online health communities, , Journal of the American Medical Informatics Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Yonatan Bitton",
                    "Raphael Cohen",
                    "Tamar Schifter"
                ],
                "title": "Eitan Bachmat, Michael Elhadad, and No\u00e9mie Elhadad. 2020. Cross-lingual Unified Medical Language System entity linking in online health communities",
                "pub_date": null,
                "pub_title": "Journal of the American Medical Informatics Association",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_57",
            "content": "UNKNOWN, None, , The camoni global network, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "The camoni global network",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_58",
            "content": "UNKNOWN, None, , Jianfeng Gao, and Hoifung Poon. 2020. Domain-specific language model pretraining for biomedical natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Jianfeng Gao, and Hoifung Poon. 2020. Domain-specific language model pretraining for biomedical natural language processing",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_59",
            "content": "UNKNOWN, None, 2019, Bert: Pretraining of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_60",
            "content": "Robert Leaman, Zhiyong Lu, Taggerone: joint named entity recognition and normalization with semi-markov models, 2016, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Robert Leaman",
                    "Zhiyong Lu"
                ],
                "title": "Taggerone: joint named entity recognition and normalization with semi-markov models",
                "pub_date": "2016",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_61",
            "content": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Biobert: a pre-trained biomedical language representation model for biomedical text mining, 2020, Bioinformatics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Jinhyuk Lee",
                    "Wonjin Yoon",
                    "Sungdong Kim",
                    "Donghyeon Kim",
                    "Sunkyu Kim",
                    "Chan Ho So",
                    "Jaewoo Kang"
                ],
                "title": "Biobert: a pre-trained biomedical language representation model for biomedical text mining",
                "pub_date": "2020",
                "pub_title": "Bioinformatics",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_62",
            "content": "UNKNOWN, None, 2016, Biocreative v cdr task corpus: a resource for chemical disease relation extraction, Database.",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": "Biocreative v cdr task corpus: a resource for chemical disease relation extraction",
                "pub": "Database"
            }
        },
        {
            "ix": "434-ARR_v2_63",
            "content": "Carolyn Lipscomb, Medical subject headings (mesh), 2000, Bulletin of the Medical Library Association, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Carolyn Lipscomb"
                ],
                "title": "Medical subject headings (mesh)",
                "pub_date": "2000",
                "pub_title": "Bulletin of the Medical Library Association",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_64",
            "content": "Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, Nigel Collier, Self-alignment pretraining for biomedical entity representations, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Fangyu Liu",
                    "Ehsan Shareghi",
                    "Zaiqiao Meng",
                    "Marco Basaldella",
                    "Nigel Collier"
                ],
                "title": "Self-alignment pretraining for biomedical entity representations",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_65",
            "content": "Daniel Loureiro, Al\u00edpio M\u00e1rio Jorge , Medlinker: Medical entity linking with neural representations and dictionary matching, 2020, European Conference on Information Retrieval, Springer.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Daniel Loureiro",
                    "Al\u00edpio M\u00e1rio Jorge "
                ],
                "title": "Medlinker: Medical entity linking with neural representations and dictionary matching",
                "pub_date": "2020",
                "pub_title": "European Conference on Information Retrieval",
                "pub": "Springer"
            }
        },
        {
            "ix": "434-ARR_v2_66",
            "content": "UNKNOWN, None, 2021, Low resource recognition and linking of biomedical concepts from a large ontology, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Low resource recognition and linking of biomedical concepts from a large ontology",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_67",
            "content": "UNKNOWN, None, 2019, Medmentions: a large biomedical corpus annotated with umls concepts, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Medmentions: a large biomedical corpus annotated with umls concepts",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_68",
            "content": "Aur\u00e9lie N\u00e9v\u00e9ol, Aude Robert, Robert Anderson, Kevin Cohen, Cyril Grouin, Thomas Lavergne, Gr\u00e9goire Rey, Claire Rondet, Pierre Zweigenbaum, Clef ehealth 2017 multilingual information extraction task overview: Icd10 coding of death certificates in english and french, 2017, CLEF (Working Notes), .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Aur\u00e9lie N\u00e9v\u00e9ol",
                    "Aude Robert",
                    "Robert Anderson",
                    "Kevin Cohen",
                    "Cyril Grouin",
                    "Thomas Lavergne",
                    "Gr\u00e9goire Rey",
                    "Claire Rondet",
                    "Pierre Zweigenbaum"
                ],
                "title": "Clef ehealth 2017 multilingual information extraction task overview: Icd10 coding of death certificates in english and french",
                "pub_date": "2017",
                "pub_title": "CLEF (Working Notes)",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_69",
            "content": "Aur\u00e9lie N\u00e9v\u00e9ol, Aude Robert, Francesco Grippo, Claire Morgand, Chiara Orsi, Laszlo Pelikan, Lionel Ramadier, Gr\u00e9goire Rey, Pierre Zweigenbaum, Clef ehealth 2018 multilingual information extraction task overview: Icd10 coding of death certificates in french, hungarian and italian, 2018, CLEF (Working Notes), .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Aur\u00e9lie N\u00e9v\u00e9ol",
                    "Aude Robert",
                    "Francesco Grippo",
                    "Claire Morgand",
                    "Chiara Orsi",
                    "Laszlo Pelikan",
                    "Lionel Ramadier",
                    "Gr\u00e9goire Rey",
                    "Pierre Zweigenbaum"
                ],
                "title": "Clef ehealth 2018 multilingual information extraction task overview: Icd10 coding of death certificates in french, hungarian and italian",
                "pub_date": "2018",
                "pub_title": "CLEF (Working Notes)",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_70",
            "content": "F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Scikit-learn: Machine learning in Python, 2011, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "F Pedregosa",
                    "G Varoquaux",
                    "A Gramfort",
                    "V Michel",
                    "B Thirion",
                    "O Grisel",
                    "M Blondel",
                    "P Prettenhofer",
                    "R Weiss",
                    "V Dubourg",
                    "J Vanderplas",
                    "A Passos",
                    "D Cournapeau",
                    "M Brucher",
                    "M Perrot",
                    "E Duchesnay"
                ],
                "title": "Scikit-learn: Machine learning in Python",
                "pub_date": "2011",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "434-ARR_v2_71",
            "content": "UNKNOWN, None, 2021, Alephbert: A hebrew large pre-trained language model to start-off your hebrew NLP application with, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Alephbert: A hebrew large pre-trained language model to start-off your hebrew NLP application with",
                "pub": "CoRR"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "434-ARR_v2_0@0",
            "content": "Cross-Lingual UMLS Named Entity Linking using UMLS Dictionary Fine-Tuning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_0",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_2@0",
            "content": "We study cross-lingual UMLS named entity linking, where mentions in a given source language are mapped to UMLS concepts, most of which are labeled in English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_2",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_2@1",
            "content": "Our crosslingual framework includes an offline unsupervised construction of a translated UMLS dictionary and a per-document pipeline which identifies UMLS candidate mentions and uses a finetuned pretrained transformer language model to filter candidates according to context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_2",
            "start": 159,
            "end": 433,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_2@2",
            "content": "Our method exploits a small dataset of manually annotated UMLS mentions in the source language and uses this supervised data in two ways: to extend the unsupervised UMLS dictionary and to fine-tune the contextual filtering of candidate mentions in full documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_2",
            "start": 435,
            "end": 697,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_2@3",
            "content": "We demonstrate results of our approach on both Hebrew and English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_2",
            "start": 699,
            "end": 764,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_2@4",
            "content": "We achieve new state-of-the-art (SOTA) results on the Hebrew Camoni corpus, +8.9 F1 on average across three communities in the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_2",
            "start": 766,
            "end": 900,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_2@5",
            "content": "We also achieve new SOTA on the English dataset MedMentions with +7.3 F1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_2",
            "start": 902,
            "end": 974,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_4@0",
            "content": "Public health practices are becoming increasingly digital, with tools to explore scientific sources of information such as medical literature and online health communities rising in popularity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_4",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_4@1",
            "content": "Such tools are essential in offering insights to researchers, providing information to patients and to their caregivers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_4",
            "start": 194,
            "end": 313,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_4@2",
            "content": "Reliable identification of mentions of biomedical concepts in free text is a key technique to enable robust mining of such textual resources.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_4",
            "start": 315,
            "end": 455,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_4@3",
            "content": "Named-Entity Recognition (NER) is the task of classifying entities in text to high level classes (Person, Organization, Gene, Disease, Treatment, etc.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_4",
            "start": 457,
            "end": 608,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_5@0",
            "content": "Named-Entity Linking (NEL) seeks to additionally classify entity mentions in text into specific concepts according to an existing reference list or knowledge base.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_5",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_5@1",
            "content": "We focus in this work on biomedical NEL, i.e., identifying mentions referring to biomedical concepts such as disorders and drugs and linking them to normalized concepts, for example, concept unique identifiers (CUIs) listed in the Unified Medical Language System (UMLS) controlled vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_5",
            "start": 164,
            "end": 455,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_5@2",
            "content": "Biomedical NEL has been mostly studied in English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_5",
            "start": 457,
            "end": 506,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_5@3",
            "content": "Other languages present additional challenges because terms in the ontology are described in English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_5",
            "start": 508,
            "end": 608,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_5@4",
            "content": "We address crosslingual NEL which consists of mapping mentions in a source language to concepts labeled and described in a different target language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_5",
            "start": 610,
            "end": 758,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_5@5",
            "content": "We focus on cross-lingual UMLS NEL, where mentions in the source language (we specifically test Hebrew, see Figure 1 for a Hebrew tagging example) are mapped to UMLS concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_5",
            "start": 760,
            "end": 934,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_5@6",
            "content": "We aim for a general solution that can be adapted to any source language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_5",
            "start": 936,
            "end": 1008,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_5@7",
            "content": "We operate in a low resource setting, where the ontology is large, text describing most entities is not available, and labeled data can only cover a small portion of the ontology.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_5",
            "start": 1010,
            "end": 1188,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_5@8",
            "content": "We also consider different genres of text to be annotated, ranging from consumer health medical articles in popular web sites to scientific biomedical articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_5",
            "start": 1190,
            "end": 1349,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_6@0",
            "content": "Our main contributions are: (1) We provide a general framework for cross-lingual UMLS NEL that can be adapted to source languages with few pre-requisites; (2) Our method exploits a small annotated corpus of documents in the source language and genre annotated manually for UMLS mentions (a few thousands annotated mentions).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_6",
            "start": 0,
            "end": 323,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_6@1",
            "content": "This training data is split to support (a) the extension of the unsupervised UMLS dictionary with corpus-salient entity names and (b) fine-tune the contextual ranking and filtering of (candidate mentions, concept) pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_6",
            "start": 325,
            "end": 544,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_6@2",
            "content": "We find that the step of UMLS dictionary fine-tuning boosts NEL performance and identify a clear tradeoff in allocating training data between lexicon extension and contextual fine-tuning; (3) We demonstrate results of our approach on both Hebrew and English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_6",
            "start": 546,
            "end": 803,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_6@3",
            "content": "We achieve new SOTA on the Hebrew Camoni corpus (Bitton et al., 2020) with +8.87 F1 and on the English dataset MedMentions (Mohan and Li, 2019) with +7.3 F1 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_6",
            "start": 805,
            "end": 964,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_7@0",
            "content": "Previous Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_7",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_8@0",
            "content": "Biomedical NEL is challenging because the underlying ontology (most often UMLS) is extremely large and the acquisition of annotated training data requires rare and expensive expertise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_8",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_8@1",
            "content": "Loureiro and Jorge (2020) presented MedLinker, a tool for improving biomedical NEL by predicting the semantic type of a medical concept mention and filtering out candidates of the wrong type.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_8",
            "start": 185,
            "end": 375,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_8@2",
            "content": "MedLinker was tested on the MedMentions task of concept linking (Mohan and Li, 2019), improving above TaggerOne , the baseline model for MedMentions which did not use deep learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_8",
            "start": 377,
            "end": 557,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_8@3",
            "content": "MedLinker splits the end to end task of entity linking into two stages -candidate recognition and linking.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_8",
            "start": 559,
            "end": 664,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_8@4",
            "content": "For candidate matching, it combines a BiLSTM-CRF model for contextual matching with an approximate dictionary matching method to increase recall.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_8",
            "start": 666,
            "end": 810,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_8@5",
            "content": "In the cross-lingual setting, dictionary matching is not applicable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_8",
            "start": 812,
            "end": 879,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_8@6",
            "content": "We report our results on the same MedMentions dataset in Section 5.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_8",
            "start": 881,
            "end": 949,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_9@0",
            "content": "Past work has shown that using in-domain text can provide additional gains over general-domain language models (Gu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_9",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_9@1",
            "content": "Therefore, recent work (BioBERT (Lee et al., 2020), SciB-ERT (Beltagy et al., 2019)) addressed biomedical NEL, focusing on pre-training models on scientific/medical text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_9",
            "start": 130,
            "end": 299,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_9@2",
            "content": "Liu et al. (2021) developed Sap-BERT, a pre-training scheme which exploits the graph structure of the UMLS controlled vocabulary and aims at learning an encoding of medical mentions that can align with synonym relations in the UMLS graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_9",
            "start": 301,
            "end": 538,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_9@3",
            "content": "Combining the SapBERT objective with pre-training on biomedical text of PubMed-BERT (Gu et al., 2020) boosts results on NEL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_9",
            "start": 540,
            "end": 663,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_9@4",
            "content": "Experimental results demonstrated that SapBERT outperforms many domain-specific BERT-based variants (BioBERT and SciBERT) on the BC5CDR (BioCreative V CDR) corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_9",
            "start": 665,
            "end": 827,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_9@5",
            "content": "Although our model focuses on cross-lingual NEL, it also applies to English documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_9",
            "start": 829,
            "end": 914,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_9@6",
            "content": "We compare our results to 1 Our source code is publicly available on GitHub https://github.com/rinagalperin/biomedical_nel these approaches on BC5CDR and MedMentions (Tables 4 and 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_9",
            "start": 916,
            "end": 1098,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_10@0",
            "content": "Indexing of the abundant biomedical scientific literature requires precise detection of medical concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_10",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_10@1",
            "content": "Mohan et al. (2021) developed a lowresource recognition and linking model of biomedical concepts (henceforth referred to as LRR) aimed at generalizing to entities unseen at training time, and incorporating linking predictions into the mention segmentation decisions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_10",
            "start": 106,
            "end": 371,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_10@2",
            "content": "This BERT-based model achieved SOTA results on the MedMentions task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_10",
            "start": 373,
            "end": 440,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_10@3",
            "content": "In our work, we adopt the LRR bottom-up candidate generation approach (see Section 4.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_10",
            "start": 442,
            "end": 529,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_10@4",
            "content": "We address the main drawback of the approach by incorporating a UMLS dictionary fine-tuning technique which extends the list of candidate pairs (source expression, CUI) on a portion of the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_10",
            "start": 531,
            "end": 733,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_10@5",
            "content": "We elaborate on the motivation for the technique in Section 4.5 and demonstrate its contribution in ablation experiments (see Section 5.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_10",
            "start": 735,
            "end": 873,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_11@0",
            "content": "Cross-lingual NEL, the problem of grounding mentions of entities in a source language text into a different target language knowledge base (typically English), has been addressed in recent years, with a range of promising techniques.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_11",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_11@1",
            "content": "When the source and target languages operate over different alphabets and sound systems, both translation and transliteration of terms (which is a noisy process even when done by people) must be handled.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_11",
            "start": 234,
            "end": 436,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_11@2",
            "content": "Bitton et al. (2020) curated the Camoni corpus, an annotated resource of Hebrew posts from online health communities (OHCs), where noisy text (as opposed to scientific text) introduces additional challenges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_11",
            "start": 438,
            "end": 644,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_11@3",
            "content": "Many user queries mention medical terms, which are very likely to include noisy transliterations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_11",
            "start": 646,
            "end": 742,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_11@4",
            "content": "For example, the Hebrew query equivalent to \"How do I know I have fibromyalgia?\" does not return any results in the search engine of the Camoni online community when 'fibromialgia' is transliterated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_11",
            "start": 744,
            "end": 942,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_11@5",
            "content": "Bitton et al. (2020) introduced MDTEL (Medical Deep Transliteration Entity Linking) for Hebrew-English NEL on noisy text in OHCs, and tested it on the Camoni corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_11",
            "start": 944,
            "end": 1108,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_11@6",
            "content": "MDTEL adopts a fourstep approach -consisting of an offline unsupervised Hebrew UMLS dictionary learning, candidate mention generation, high-recall matching and filtering of matching mentions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_11",
            "start": 1110,
            "end": 1300,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_11@7",
            "content": "We adopt MDTEL's unsupervised UMLS dictionary matching, which uses an attention-based recurrent neural network encoder-decoder that maps UMLS from English to Hebrew (either a Hebrew translation or translit-Figure 1: A forum post from the Camoni sclerosis community that translates to: \"Hello, recently, my gait has deteriorated and I was suggested to begin Botox treatment to release the muscles and prevent spasticity. Has anyone here undergone such treatment? does it help? is there a risk that such treatment will greatly weaken the muscle, causing the exact opposite action?\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_11",
            "start": 1302,
            "end": 1881,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_11@8",
            "content": "The post contains 37 words and 6 spans that link to 4 different CUIs of UMLS concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_11",
            "start": 1883,
            "end": 1968,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_11@9",
            "content": "Notice that a span can consist of more than 1 word (such as the term matched with \"gait abnormality\") and a single CUI can be referenced from several places in the same post (such as the CUI of \"General Treatment\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_11",
            "start": 1970,
            "end": 2184,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_12@0",
            "content": "eration of the concept).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_12",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_12@1",
            "content": "We introduce new methods for candidate generation, high-recall matching and contextual relevance filtering, relying on multilingual pre-trained language model (mBERT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_12",
            "start": 25,
            "end": 191,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_12@2",
            "content": "Our new components lead to significant performance improvement over MDTEL on the Camoni corpus (see Table 2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_12",
            "start": 193,
            "end": 301,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_13@0",
            "content": "Task Formulation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_13",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_14@0",
            "content": "Given input language L and target language L t , a database of medical concepts C Lt : L * t \u2192 CU I is a function from concept names in L t to concept IDs (CUIs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_14",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_14@1",
            "content": "Using C Lt , we want to learn a function F from a span in input language L and its context to a CUI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_14",
            "start": 163,
            "end": 262,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_14@2",
            "content": "We identify dictionary C L : L * \u2192 CU I. C L is the translated version of the medical concepts database C Lt .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_14",
            "start": 264,
            "end": 373,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_14@3",
            "content": "We learn C L by mapping the medical terms in L t to terms in L. Given mapping C L , we aim to learn:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_14",
            "start": 375,
            "end": 474,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_15@0",
            "content": "F : L * \u00d7 L * \u2192 CU I \u222a {\u22a5}",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_15",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_16@0",
            "content": "where \u22a5 is a special code denoting a non-medical term.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_16",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_16@1",
            "content": "F differs from C L as it addresses the variability and ambiguity of the task by depending on the context as well as the span.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_16",
            "start": 55,
            "end": 179,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_16@2",
            "content": "Given text W = (w 1 , ..., w n ), where w i \u2208 L, for every span s i,j = (w i , ..., w j ) \u2286 W , we would like to compute F (W, s i,j ), where 0 \u2264 j \u2212 i < k (we limit the span sizes to at most k), that is, we want to predict the concept associated with a span under context W in language L. Provided a dataset A L exposing a subset of F combined with linguistic knowledge and generalization capabilities of neural models, we aim at learning a larger portion of function F .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_16",
            "start": 181,
            "end": 652,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_17@0",
            "content": "Model Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_17",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_18@0",
            "content": "Our end-to-end cross-lingual UMLS NEL model (Figure 2) consists of four consecutive stages: (1) multilingual UMLS mapping: generate UMLS dictionary C L (see Section 4.1) based on the method of Bitton et al. (2020), and fine-tune it using our UMLS dictionary fine-tuning technique (see Section 4.5); (2) candidate generation: consider all spans of up to k words as candidate mentions and compute vector representations for both mentions and concepts (see Section 4.2); (3) high recall matching: use a semantic similarity based score function to generate the top matching entities with high recall (see Section 4.3) and ( 4) contextual relevance modeling: encode each candidate into a context-dependent vector representation using a pre-trained transformer-based language model fine tuning process (see Section 4.4).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_18",
            "start": 0,
            "end": 813,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_19@0",
            "content": "Our approach attempts to avoid three types of mistakes: (1) morphological and transliteration noise, where candidate terms in the source language might be extracted due to a transliteration or morphological error and matched with UMLS entities, (2) contextual errors, where candidate terms which are not medical terms when considering the context might be matched with UMLS entities, and (3) partial UMLS tagging, where candidate terms which are not the full medical terms in the text but rather more general UMLS mentions might be tagged instead of the full term (e.g., in the mention \"flu vaccine\", \"flu\" should not be tagged).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_19",
            "start": 0,
            "end": 628,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_19@1",
            "content": "The first challenge is addressed by learning a high-recall C L dictionary with generalization capabilities, trained both on translation and transliteration data; the second, is addressed by an mBERT-based contextual language model; the third, by systematic consideration of all spans up to size k as candidates as part of the candidate generation and contextual relevance components.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_19",
            "start": 630,
            "end": 1012,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_20@0",
            "content": "Multilingual UMLS Mapping",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_20",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_21@0",
            "content": "The first step of our model is offline, fully unsupervised, and based on the method of (Bitton et al., 2020): we generate a mapping C L between medical concept names in source language L to their corresponding CUIs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_21",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_21@1",
            "content": "An attention-based characterbased recurrent neural network encoder-decoder is used to create a list of \u2329UMLS term in English, term in language L\u232a so that each UMLS term in English is matched with both transliterated and translated forms in L. This is done without the need of manually annotated data and results in a noisy mapping C L of source language medical terms and their CUIs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_21",
            "start": 216,
            "end": 598,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_22@0",
            "content": "Candidate Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_22",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_23@0",
            "content": "Given a document in L where we want to identify UMLS mentions, the candidate generation step begins with pre-processing: we normalize the source text documents from annotated data A L and the target UMLS concepts from C L by transforming all string values to lower case and removing delimiters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_23",
            "start": 0,
            "end": 293,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_23@1",
            "content": "We then generate a list of overlapping candidate mention spans, ranging in length according to the max length parameter k (i.e., 1, ..., k. See Appendix A for details).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_23",
            "start": 295,
            "end": 462,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_23@2",
            "content": "We exclude spans starting or ending with stop words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_23",
            "start": 464,
            "end": 515,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_23@3",
            "content": "We then represent both the spans and the concepts as tf-idf character n-gram (1 to 3-gram) vectors using sklearn's implementation (Pedregosa et al., 2011).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_23",
            "start": 517,
            "end": 671,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_23@4",
            "content": "Empirical experiments showed that tf-idf encoding improved recall in candidate generation compared to bag of words encoding (see Appendix B for a comparison between the two representations using both Hebrew and English datasets).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_23",
            "start": 673,
            "end": 901,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_24@0",
            "content": "High Recall Matching",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_24",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_25@0",
            "content": "The high recall matcher (HRM) receives the vector representations from the candidate generator and computes a similarity score between each span and all concept names in C L using cosine similarity (see Appendix B for comparison against Manhattan score function).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_25",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_25@1",
            "content": "We then select the top m matches per span with score over a threshold th (see Appendix C for hyper-parameters).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_25",
            "start": 264,
            "end": 374,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_25@2",
            "content": "This results in a high recall list of candidate matches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_25",
            "start": 376,
            "end": 431,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_26@0",
            "content": "Contextual Relevance Modeling",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_26",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_27@0",
            "content": "At this step, we want to predict which spans returned from the high recall matcher are true biomedical concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_27",
            "start": 0,
            "end": 111,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_27@1",
            "content": "We use multilingual BERT (m-BERT) (Jacob Devlin, 2019), a 12 layer transformer that was trained on the Wikipedia pages of 104 languages (including Hebrew) with a shared word piece vocabulary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_27",
            "start": 113,
            "end": 303,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_27@2",
            "content": "M-BERT does not use any marker denoting the input language, and does not include explicit mechanism to encourage translation equivalent pairs to have similar representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_27",
            "start": 305,
            "end": 478,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_27@3",
            "content": "We fine-tune m-BERT on a binary classification task on our training data: each candidate mention span returned from the HRM is centered in its context from the original doc, i.e., W s words to the right of the span and W s words to the left of the span, creating a window surrounding the candidate mention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_27",
            "start": 480,
            "end": 785,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_27@4",
            "content": "The classifier takes as input the window, the HRM's decision on which concept is represented by the mention in the window, and the true verdict of whether the candidate mention is indeed an occurrence of the concept.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_27",
            "start": 787,
            "end": 1002,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_27@5",
            "content": "We utilize m-BERT's QA format as follows: the question (medical concept c) and the reference text (window w) are packed into the input, and provide the binary label as answer of whether or not c is a medical mention in context w:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_27",
            "start": 1004,
            "end": 1232,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_28@0",
            "content": "[CLS] w [SEP ] c [SEP ].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_28",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_29@0",
            "content": "This fine-tuning step consists of adding an additional output layer on top of the pre-trained m-BERT model to adapt it to the biomedical NEL task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_29",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_30@0",
            "content": "UMLS Dictionary Fine-Tuning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_30",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_31@0",
            "content": "We introduce a UMLS dictionary finetuning (UMLS DFT) technique where some of the data in A L is removed from the training dataset and used to directly expand the learned dictionary C L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_31",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_31@1",
            "content": "We reserve R% of the training data A L to fine-tune C L generating C \u2032 L (see Figure 2): from this chunk of A L , we add each mention in the tagged data as new pairs (mention in L, CUI).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_31",
            "start": 187,
            "end": 372,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_32@0",
            "content": "For example, suppose our training data consists of 10 tagged documents and our UMLS dictionary C L contains 100 concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_32",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_32@1",
            "content": "Given R = 10%, our UMLS dictionary fine-tuning technique will require one tagged document d (10% of the 10 docs in the training set) to be used for fine-tuning C L .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_32",
            "start": 122,
            "end": 286,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_32@2",
            "content": "We go over every tagged pair (m, c) from doc d, where m is a mention in doc d and c is the UMLS concept the annotators tagged m. If m \u0338 \u2208 C L , we add m to C L with the CUI of c. Suppose doc d contained 15 such tags, we will obtain an augmented C \u2032 L containing 100 + 15 = 115 concepts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_32",
            "start": 288,
            "end": 573,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_32@3",
            "content": "We cannot use this portion of data for later training of our model, since after fine-tuning we are guaranteed to get a perfect match for all the spans in the documents used for fine-tuning (thus creating bias of the HRM).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_32",
            "start": 575,
            "end": 795,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_32@4",
            "content": "Although this process decreases the overall size of the input dataset for contextual relevance fine-tuning, it improves the recall of the HRM and adds more positive examples for the BERT training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_32",
            "start": 797,
            "end": 1000,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_32@5",
            "content": "We elaborate more on this trade-off in Section 5.4.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_32",
            "start": 1002,
            "end": 1054,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_32@6",
            "content": "This approach allows us to improve recall on synonyms and abbreviations that were not originally in our UMLS dictionary, with genrespecific terminology observed in the training data (as evident from the experiment shown in Table 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_32",
            "start": 1056,
            "end": 1287,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_33@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_33",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_34@0",
            "content": "We test our approach both on cross-lingual UMLS Linking using the Camoni dataset of Hebrew consumer health data and on English UMLS Linking using MedMentions and BC5CDR, which include scientific papers in the bio-medical field.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_34",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_35@0",
            "content": "Camoni Corpus",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_35",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_36@0",
            "content": "The Camoni corpus was curated by Bitton et al. (2020) for the analysis of the MDTEL system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_36",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_36@1",
            "content": "Camoni is an Israeli social network in Hebrew aimed at patients with chronic diseases and their family members (Camoni).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_36",
            "start": 92,
            "end": 211,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_36@2",
            "content": "Camoni serves about 20,000 registered members and 100,000 unique visitors per month.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_36",
            "start": 213,
            "end": 296,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_36@3",
            "content": "The digital platform is organized into 39 disease-specific communities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_36",
            "start": 298,
            "end": 368,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_36@4",
            "content": "Bitton et al. (2020) extracted text from three communities (diabetes, sclerosis, and depression), for a total of 55,000 posts and 2.5 million tokens, and constructed an annotated dataset in which 1,000 mentions of UMLS terms were annotated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_36",
            "start": 370,
            "end": 609,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_36@5",
            "content": "Bitton et al. (2020) proposed a high recall matcher based on a fuzzy string matching algorithm introduced in prior work to perform the matching between the spans and medical entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_36",
            "start": 611,
            "end": 793,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_36@6",
            "content": "Table 1 compares our HRM results (recall) with MDTEL for each community (diabetes, depression, sclerosis).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_36",
            "start": 795,
            "end": 900,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_37@0",
            "content": "We observe that our candidate generation method (adopting the LRR bottom-up approach and mBERT similarity matching) significantly improves the recall of the HRM (average of 74% using MDTEL's approach vs. average of 82% using our method).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_37",
            "start": 0,
            "end": 236,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_37@1",
            "content": "We believe that the use of the tf-idf character n-gram vectorization before applying the cosine similarity function as means of comparison helped us achieve better results compared to MDTEL's method which only applied the cosine similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_37",
            "start": 238,
            "end": 477,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_38@0",
            "content": "In the end to end linking task, our model achieves much higher precision (98% vs. 77%) without affecting the recall (73%), resulting in much improved F-score (84% vs 74%).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_38",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_38@1",
            "content": "model on the end to end linking task for each community.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_38",
            "start": 172,
            "end": 227,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_39@0",
            "content": "MedMentions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_39",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_40@0",
            "content": "MedMentions (Mohan and Li, 2019) is a corpus of Biomedical papers annotated with mentions of UMLS entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_40",
            "start": 0,
            "end": 106,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_40@1",
            "content": "The corpus consists of 4,392 papers (Titles and Abstracts) randomly selected from papers released on PubMed in 2016, that were in the biomedical field, published in the English language, and had both a Title and an Abstract available.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_40",
            "start": 108,
            "end": 341,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_40@2",
            "content": "MedMentions contains over 350,000 linked mentions, annotated by a team of professional annotators with rich experience in biomedical content curation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_40",
            "start": 343,
            "end": 492,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_40@3",
            "content": "We focus on MedMentions ST21pv (21 Semantic Types and Preferred Vocabularies), a subset of the full annotations containing 203,282 mentions and restricting the concepts to a 2.3M large subset of the full ontology (UMLS ST21pv).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_40",
            "start": 494,
            "end": 720,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_40@4",
            "content": "Each concept in this subset is associated with one of 21 selected semantic types, or to one of their descendants in the semantic type hierarchy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_40",
            "start": 722,
            "end": 865,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_40@5",
            "content": "We compare our performance to other models' results on MedMentions ST21pv in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_40",
            "start": 867,
            "end": 951,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_40@6",
            "content": "We improve on the latest SOTA LRR (Mohan et al., 2021), achieving +7.3 F1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_40",
            "start": 953,
            "end": 1026,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_41@0",
            "content": "Our recall was similar to LRR, however our model achieved highly improved precision, 76.4 compared to 63.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_41",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_41@1",
            "content": "We believe this improvement can be attributed to our UMLS dictionary finetuning technique, which provides an extended list of candidates and thus more examples for the mBERT fine-tuning process for contextual relevance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_41",
            "start": 106,
            "end": 324,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_41@2",
            "content": "Mohan et al. (2021) mention the need to improve recall for cases where the mentions are indirect or too abbreviated to generate a good lexical match from the entity knowledge base, which is exactly what our technique helps improve.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_41",
            "start": 326,
            "end": 556,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_41@3",
            "content": "For example, our process picked up in the training data that the abbreviation mrn is tagged as messenger rna (CUI C0035696), which was not originally present in the UMLS dictionary for English.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_41",
            "start": 558,
            "end": 750,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_42@0",
            "content": "BC5CDR",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_42",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_43@0",
            "content": "The BC5CDR corpus (Li et al., 2016) consists of 1,500 PubMed articles with 4,409 annotated chemicals, 5,818 diseases and 3,116 chemical-disease interactions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_43",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_43@1",
            "content": "Each entity annotation includes both the mention text spans and normalized concept identifiers, using MeSH (Medical Subject Headings) (Lipscomb, 2000) as the controlled vocabulary (MeSH is part of the UMLS controlled vocabulary).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_43",
            "start": 158,
            "end": 386,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_43@2",
            "content": "Compared to MedMentions which contains annotations of general medical concepts, BC5CDR is topic-specific, containing only annotations of chemicals and diseases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_43",
            "start": 388,
            "end": 547,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_43@3",
            "content": "BC5CDR is also much smaller, consisting of just 1,500 articles compared to the 4,392 annotated papers of MedMentions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_43",
            "start": 549,
            "end": 665,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_43@4",
            "content": "BC5CDR has a total of 13,343 linked mentions compared to 203,282 in MedMentions ST21pv.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_43",
            "start": 667,
            "end": 753,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_44@0",
            "content": "We compare our model's performance to other models using BC5CDR's test set in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_44",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_44@1",
            "content": "We observe that domain-specific pre-trained transformers help improve results on BC5CDR (93.5 F-measure vs. 73 for our model).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_44",
            "start": 87,
            "end": 212,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_44@2",
            "content": "The subset of semantic types covered in this dataset is much more technical (chemicals and chemical-disease interactions) than those covered in MedMentions, even though both BC5CDR and MedMentions include documents in the same genre of scientific biomedical articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_44",
            "start": 214,
            "end": 480,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_44@3",
            "content": "This difference is evidenced in the ablation study presented below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_44",
            "start": 482,
            "end": 548,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_44@4",
            "content": "It explains why specialized language models trained on the biomedical domain lead to much improved performance compared to our model which uses the general mBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_44",
            "start": 550,
            "end": 711,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_44@5",
            "content": "We hypothesize that using SapBERT combined with our model could enhance performance on this dataset and leave this for future work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_44",
            "start": 713,
            "end": 843,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_45@0",
            "content": "UMLS Dictionary Fine-Tuning Ablation Study",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_45",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_46@0",
            "content": "In this section, we test several factors impacting the contribution of UMLS dictionary fine-tuning to our tagger's performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_46",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_46@1",
            "content": "First, we test the technique on two different datasets and evaluate its benefits depending on the dataset size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_46",
            "start": 128,
            "end": 238,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_46@2",
            "content": "Next, we test a range of UMLS dictionary fine-tuning percentage values (R) and discuss the trade-off between this value and the end to end performance of our linker.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_46",
            "start": 240,
            "end": 404,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_47@0",
            "content": "Dataset Size Impact",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_47",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_48@0",
            "content": "We tested the UMLS dictionary fine-tuning technique on English datasets MedMentions and BC5CDR across 5 random seeds and found that it improved recall on both, but impacting MedMentions much more than BC5CDR due to a much smaller number of added concepts in BC5CDR, 209 compared to 3,294 in MedMentions (see Table 5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_48",
            "start": 0,
            "end": 316,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_48@1",
            "content": "The difference in the number of added concepts could be explained by the fact that BC5CDR is much smaller, thus the decrease in training data size counteracts the small number of concepts being added to the UMLS dictionary.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_48",
            "start": 318,
            "end": 540,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_48@2",
            "content": "To test this hypothesis, we took a subset of MedMentions of the same size as BC5CDR (annotation-wise: 8,575 in total), see Table 6 for results averaged across 5 random seeds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_48",
            "start": 542,
            "end": 715,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_48@3",
            "content": "The results suggest that the size of the dataset directly affects the number of concepts added to our UMLS dictionary (227 added in the MedMentions subset, very close to the 209 added in BC5CDR), which in turn impacts the HRM's recall: the improvement in recall is very similar between the two datasets, +1.37 for BC5CDR, +1.7 for MedMentions subset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_48",
            "start": 717,
            "end": 1066,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_49@0",
            "content": "The Recall-Accuracy Tradeoff",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_49",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_50@0",
            "content": "We first observe that our UMLS dictionary finetuning (DFT) technique can only improve the high recall matching performance (Section 4.3) since an annotation that we do not have a good semantic match for from UMLS will be a missed match without UMLS DFT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_50",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_50@1",
            "content": "Similarly, an annotation for which we do have a good semantic match will be found regardless of whether we utilize UMLS DFT or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_50",
            "start": 254,
            "end": 384,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_51@0",
            "content": "Thus, UMLS dictionary fine-tuning helps us find non-semantically similar matches that we would have otherwise missed, meaning that the higher R is -the higher the recall of the HRM should be.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_51",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_51@1",
            "content": "However, there is a trade-off between the recall gained from the annotations utilized for UMLS dictionary fine-tuning and the overall performance of the linker, since the annotations used for finetuning are examples that the contextual model will be missing during fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_51",
            "start": 192,
            "end": 468,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_51@2",
            "content": "We explore this trade-off and compare the performance of the high recall matching component with the final tagging results of our model using different values of R on the MedMentions dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_51",
            "start": 470,
            "end": 660,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_51@3",
            "content": "Figure 3 shows that there is a clear trend of increased recall of the HRM as R increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_51",
            "start": 662,
            "end": 750,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_51@4",
            "content": "However, Figure 4 shows the complexity of the trade-off since the tagger's performance reaches a peak and then begins to drop as R increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_51",
            "start": 752,
            "end": 892,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_51@5",
            "content": "The contextual model fine-tuning improvement plateaus after a certain amount of training examples, demonstrating the benefit of multi-task adaptation of pre-trained models which converge rapidly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_51",
            "start": 894,
            "end": 1088,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_51@6",
            "content": "The data efficiency of the contextual relevance fine-tuning process allows the UMLS dictionary fine-tuning technique to help improve end to end linking results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_51",
            "start": 1090,
            "end": 1249,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_52@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_52",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_53@0",
            "content": "In this work we explored the task of cross lingual named entity linking in the biomedical field.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_53",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_53@1",
            "content": "We describe a pipeline to detect and link mentions of UMLS concepts in documents in Hebrew or in English, which improves upon existing methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_53",
            "start": 97,
            "end": 239,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_53@2",
            "content": "Table 5: Number of added concepts per dataset and the average performance of the HRM with and without UMLS dictionary fine-tuning, across 5 random seeds. \"\u2717\": UMLS DFT not used, \"\u2713\": UMLS DFT used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_53",
            "start": 241,
            "end": 437,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_53@3",
            "content": "2020) which takes into account both translation and transliteration but extends this dictionary with a portion of the training data mentions; empirical analysis of this dictionary augmentation method demonstrates its importance in end to end linking performance; (3) it adopts the bottom-up systematic generation of candidates from Mohan et al. (2021) and improves it by using a compact tf*idf ranking of the candidates (char n-gram) which helps reduce memory allocation; (4) it uses a multi-lingual pre-trained language model (mBERT) to fine-tune a contextual relevance model to filter a list of high-recall candidate matches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_53",
            "start": 439,
            "end": 1065,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_53@4",
            "content": "Our framework for cross-lingual UMLS NEL can easily be adapted to any source language and does not rely on any descriptive text for the entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_53",
            "start": 1067,
            "end": 1211,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_54@0",
            "content": "We compared our performance to baseline approaches on the Camoni dataset in Hebrew (Bitton et al., 2020), and the MedMentions (Mohan and Li, 2019) and BC5CDR English datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_54",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_54@1",
            "content": "Our end-toend approach achieves SOTA results on Camoni in Hebrew and MedMentions in English with significant improvements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_54",
            "start": 176,
            "end": 297,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_54@2",
            "content": "For BC5CDR, we observe that the small size of the dataset prevents our dictionary augmentation technique from reaching its potential and models trained on specialized biomedical text (PubMedBert with SapBert training objective) obtain better coverage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_54",
            "start": 299,
            "end": 549,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_54@3",
            "content": "Such specialized training is, however, not available in a multi-lingual setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_54",
            "start": 551,
            "end": 630,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_55@0",
            "content": "UNKNOWN, None, 2019, Scibert: Pretrained contextualized embeddings for scientific text, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_55",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_56@0",
            "content": "Yonatan Bitton, Raphael Cohen, Tamar Schifter, Eitan Bachmat, Michael Elhadad, and No\u00e9mie Elhadad. 2020. Cross-lingual Unified Medical Language System entity linking in online health communities, , Journal of the American Medical Informatics Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_56",
            "start": 0,
            "end": 255,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_57@0",
            "content": "UNKNOWN, None, , The camoni global network, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_57",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_58@0",
            "content": "UNKNOWN, None, , Jianfeng Gao, and Hoifung Poon. 2020. Domain-specific language model pretraining for biomedical natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_58",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_59@0",
            "content": "UNKNOWN, None, 2019, Bert: Pretraining of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_59",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_60@0",
            "content": "Robert Leaman, Zhiyong Lu, Taggerone: joint named entity recognition and normalization with semi-markov models, 2016, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_60",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_61@0",
            "content": "Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, Biobert: a pre-trained biomedical language representation model for biomedical text mining, 2020, Bioinformatics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_61",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_62@0",
            "content": "UNKNOWN, None, 2016, Biocreative v cdr task corpus: a resource for chemical disease relation extraction, Database.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_62",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_63@0",
            "content": "Carolyn Lipscomb, Medical subject headings (mesh), 2000, Bulletin of the Medical Library Association, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_63",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_64@0",
            "content": "Fangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco Basaldella, Nigel Collier, Self-alignment pretraining for biomedical entity representations, 2021, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_64",
            "start": 0,
            "end": 291,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_65@0",
            "content": "Daniel Loureiro, Al\u00edpio M\u00e1rio Jorge , Medlinker: Medical entity linking with neural representations and dictionary matching, 2020, European Conference on Information Retrieval, Springer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_65",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_66@0",
            "content": "UNKNOWN, None, 2021, Low resource recognition and linking of biomedical concepts from a large ontology, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_66",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_67@0",
            "content": "UNKNOWN, None, 2019, Medmentions: a large biomedical corpus annotated with umls concepts, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_67",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_68@0",
            "content": "Aur\u00e9lie N\u00e9v\u00e9ol, Aude Robert, Robert Anderson, Kevin Cohen, Cyril Grouin, Thomas Lavergne, Gr\u00e9goire Rey, Claire Rondet, Pierre Zweigenbaum, Clef ehealth 2017 multilingual information extraction task overview: Icd10 coding of death certificates in english and french, 2017, CLEF (Working Notes), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_68",
            "start": 0,
            "end": 294,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_69@0",
            "content": "Aur\u00e9lie N\u00e9v\u00e9ol, Aude Robert, Francesco Grippo, Claire Morgand, Chiara Orsi, Laszlo Pelikan, Lionel Ramadier, Gr\u00e9goire Rey, Pierre Zweigenbaum, Clef ehealth 2018 multilingual information extraction task overview: Icd10 coding of death certificates in french, hungarian and italian, 2018, CLEF (Working Notes), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_69",
            "start": 0,
            "end": 309,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_70@0",
            "content": "F Pedregosa, G Varoquaux, A Gramfort, V Michel, B Thirion, O Grisel, M Blondel, P Prettenhofer, R Weiss, V Dubourg, J Vanderplas, A Passos, D Cournapeau, M Brucher, M Perrot, E Duchesnay, Scikit-learn: Machine learning in Python, 2011, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_70",
            "start": 0,
            "end": 274,
            "label": {}
        },
        {
            "ix": "434-ARR_v2_71@0",
            "content": "UNKNOWN, None, 2021, Alephbert: A hebrew large pre-trained language model to start-off your hebrew NLP application with, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "434-ARR_v2_71",
            "start": 0,
            "end": 125,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "434-ARR_v2_0",
            "tgt_ix": "434-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_0",
            "tgt_ix": "434-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_1",
            "tgt_ix": "434-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_1",
            "tgt_ix": "434-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_0",
            "tgt_ix": "434-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_2",
            "tgt_ix": "434-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_4",
            "tgt_ix": "434-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_5",
            "tgt_ix": "434-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_3",
            "tgt_ix": "434-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_3",
            "tgt_ix": "434-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_3",
            "tgt_ix": "434-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_3",
            "tgt_ix": "434-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_0",
            "tgt_ix": "434-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_6",
            "tgt_ix": "434-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_8",
            "tgt_ix": "434-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_9",
            "tgt_ix": "434-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_10",
            "tgt_ix": "434-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_7",
            "tgt_ix": "434-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_7",
            "tgt_ix": "434-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_7",
            "tgt_ix": "434-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_7",
            "tgt_ix": "434-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_7",
            "tgt_ix": "434-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_7",
            "tgt_ix": "434-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_0",
            "tgt_ix": "434-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_12",
            "tgt_ix": "434-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_14",
            "tgt_ix": "434-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_15",
            "tgt_ix": "434-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_13",
            "tgt_ix": "434-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_13",
            "tgt_ix": "434-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_13",
            "tgt_ix": "434-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_13",
            "tgt_ix": "434-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_0",
            "tgt_ix": "434-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_16",
            "tgt_ix": "434-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_18",
            "tgt_ix": "434-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_17",
            "tgt_ix": "434-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_17",
            "tgt_ix": "434-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_17",
            "tgt_ix": "434-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_17",
            "tgt_ix": "434-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_19",
            "tgt_ix": "434-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_20",
            "tgt_ix": "434-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_20",
            "tgt_ix": "434-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_17",
            "tgt_ix": "434-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_21",
            "tgt_ix": "434-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_22",
            "tgt_ix": "434-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_22",
            "tgt_ix": "434-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_17",
            "tgt_ix": "434-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_23",
            "tgt_ix": "434-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_24",
            "tgt_ix": "434-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_24",
            "tgt_ix": "434-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_17",
            "tgt_ix": "434-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_25",
            "tgt_ix": "434-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_27",
            "tgt_ix": "434-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_28",
            "tgt_ix": "434-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_26",
            "tgt_ix": "434-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_26",
            "tgt_ix": "434-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_26",
            "tgt_ix": "434-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_26",
            "tgt_ix": "434-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_17",
            "tgt_ix": "434-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_29",
            "tgt_ix": "434-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_31",
            "tgt_ix": "434-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_30",
            "tgt_ix": "434-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_30",
            "tgt_ix": "434-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_30",
            "tgt_ix": "434-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_0",
            "tgt_ix": "434-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_32",
            "tgt_ix": "434-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_33",
            "tgt_ix": "434-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_33",
            "tgt_ix": "434-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_33",
            "tgt_ix": "434-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_34",
            "tgt_ix": "434-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_36",
            "tgt_ix": "434-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_37",
            "tgt_ix": "434-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_35",
            "tgt_ix": "434-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_35",
            "tgt_ix": "434-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_35",
            "tgt_ix": "434-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_35",
            "tgt_ix": "434-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_33",
            "tgt_ix": "434-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_38",
            "tgt_ix": "434-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_40",
            "tgt_ix": "434-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_39",
            "tgt_ix": "434-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_39",
            "tgt_ix": "434-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_39",
            "tgt_ix": "434-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_33",
            "tgt_ix": "434-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_41",
            "tgt_ix": "434-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_43",
            "tgt_ix": "434-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_42",
            "tgt_ix": "434-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_42",
            "tgt_ix": "434-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_42",
            "tgt_ix": "434-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_33",
            "tgt_ix": "434-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_44",
            "tgt_ix": "434-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_45",
            "tgt_ix": "434-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_45",
            "tgt_ix": "434-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_33",
            "tgt_ix": "434-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_46",
            "tgt_ix": "434-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_47",
            "tgt_ix": "434-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_47",
            "tgt_ix": "434-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_33",
            "tgt_ix": "434-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_48",
            "tgt_ix": "434-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_50",
            "tgt_ix": "434-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_49",
            "tgt_ix": "434-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_49",
            "tgt_ix": "434-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_49",
            "tgt_ix": "434-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_0",
            "tgt_ix": "434-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_51",
            "tgt_ix": "434-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_53",
            "tgt_ix": "434-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_52",
            "tgt_ix": "434-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_52",
            "tgt_ix": "434-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_52",
            "tgt_ix": "434-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "434-ARR_v2_0",
            "tgt_ix": "434-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_1",
            "tgt_ix": "434-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_2",
            "tgt_ix": "434-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_2",
            "tgt_ix": "434-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_2",
            "tgt_ix": "434-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_2",
            "tgt_ix": "434-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_2",
            "tgt_ix": "434-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_2",
            "tgt_ix": "434-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_3",
            "tgt_ix": "434-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_4",
            "tgt_ix": "434-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_4",
            "tgt_ix": "434-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_4",
            "tgt_ix": "434-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_4",
            "tgt_ix": "434-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_5",
            "tgt_ix": "434-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_5",
            "tgt_ix": "434-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_5",
            "tgt_ix": "434-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_5",
            "tgt_ix": "434-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_5",
            "tgt_ix": "434-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_5",
            "tgt_ix": "434-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_5",
            "tgt_ix": "434-ARR_v2_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_5",
            "tgt_ix": "434-ARR_v2_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_5",
            "tgt_ix": "434-ARR_v2_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_6",
            "tgt_ix": "434-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_6",
            "tgt_ix": "434-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_6",
            "tgt_ix": "434-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_6",
            "tgt_ix": "434-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_7",
            "tgt_ix": "434-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_8",
            "tgt_ix": "434-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_8",
            "tgt_ix": "434-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_8",
            "tgt_ix": "434-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_8",
            "tgt_ix": "434-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_8",
            "tgt_ix": "434-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_8",
            "tgt_ix": "434-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_8",
            "tgt_ix": "434-ARR_v2_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_9",
            "tgt_ix": "434-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_9",
            "tgt_ix": "434-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_9",
            "tgt_ix": "434-ARR_v2_9@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_9",
            "tgt_ix": "434-ARR_v2_9@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_9",
            "tgt_ix": "434-ARR_v2_9@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_9",
            "tgt_ix": "434-ARR_v2_9@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_9",
            "tgt_ix": "434-ARR_v2_9@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_10",
            "tgt_ix": "434-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_10",
            "tgt_ix": "434-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_10",
            "tgt_ix": "434-ARR_v2_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_10",
            "tgt_ix": "434-ARR_v2_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_10",
            "tgt_ix": "434-ARR_v2_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_10",
            "tgt_ix": "434-ARR_v2_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_11@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_11@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_11@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_11",
            "tgt_ix": "434-ARR_v2_11@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_12",
            "tgt_ix": "434-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_12",
            "tgt_ix": "434-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_12",
            "tgt_ix": "434-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_13",
            "tgt_ix": "434-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_14",
            "tgt_ix": "434-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_14",
            "tgt_ix": "434-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_14",
            "tgt_ix": "434-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_14",
            "tgt_ix": "434-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_15",
            "tgt_ix": "434-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_16",
            "tgt_ix": "434-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_16",
            "tgt_ix": "434-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_16",
            "tgt_ix": "434-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_17",
            "tgt_ix": "434-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_18",
            "tgt_ix": "434-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_19",
            "tgt_ix": "434-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_19",
            "tgt_ix": "434-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_20",
            "tgt_ix": "434-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_21",
            "tgt_ix": "434-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_21",
            "tgt_ix": "434-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_22",
            "tgt_ix": "434-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_23",
            "tgt_ix": "434-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_23",
            "tgt_ix": "434-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_23",
            "tgt_ix": "434-ARR_v2_23@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_23",
            "tgt_ix": "434-ARR_v2_23@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_23",
            "tgt_ix": "434-ARR_v2_23@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_24",
            "tgt_ix": "434-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_25",
            "tgt_ix": "434-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_25",
            "tgt_ix": "434-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_25",
            "tgt_ix": "434-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_26",
            "tgt_ix": "434-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_27",
            "tgt_ix": "434-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_27",
            "tgt_ix": "434-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_27",
            "tgt_ix": "434-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_27",
            "tgt_ix": "434-ARR_v2_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_27",
            "tgt_ix": "434-ARR_v2_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_27",
            "tgt_ix": "434-ARR_v2_27@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_28",
            "tgt_ix": "434-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_29",
            "tgt_ix": "434-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_30",
            "tgt_ix": "434-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_31",
            "tgt_ix": "434-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_31",
            "tgt_ix": "434-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_32",
            "tgt_ix": "434-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_32",
            "tgt_ix": "434-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_32",
            "tgt_ix": "434-ARR_v2_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_32",
            "tgt_ix": "434-ARR_v2_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_32",
            "tgt_ix": "434-ARR_v2_32@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_32",
            "tgt_ix": "434-ARR_v2_32@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_32",
            "tgt_ix": "434-ARR_v2_32@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_33",
            "tgt_ix": "434-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_34",
            "tgt_ix": "434-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_35",
            "tgt_ix": "434-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_36",
            "tgt_ix": "434-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_36",
            "tgt_ix": "434-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_36",
            "tgt_ix": "434-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_36",
            "tgt_ix": "434-ARR_v2_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_36",
            "tgt_ix": "434-ARR_v2_36@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_36",
            "tgt_ix": "434-ARR_v2_36@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_36",
            "tgt_ix": "434-ARR_v2_36@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_37",
            "tgt_ix": "434-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_37",
            "tgt_ix": "434-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_38",
            "tgt_ix": "434-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_38",
            "tgt_ix": "434-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_39",
            "tgt_ix": "434-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_40",
            "tgt_ix": "434-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_40",
            "tgt_ix": "434-ARR_v2_40@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_40",
            "tgt_ix": "434-ARR_v2_40@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_40",
            "tgt_ix": "434-ARR_v2_40@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_40",
            "tgt_ix": "434-ARR_v2_40@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_40",
            "tgt_ix": "434-ARR_v2_40@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_40",
            "tgt_ix": "434-ARR_v2_40@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_41",
            "tgt_ix": "434-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_41",
            "tgt_ix": "434-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_41",
            "tgt_ix": "434-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_41",
            "tgt_ix": "434-ARR_v2_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_42",
            "tgt_ix": "434-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_43",
            "tgt_ix": "434-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_43",
            "tgt_ix": "434-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_43",
            "tgt_ix": "434-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_43",
            "tgt_ix": "434-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_43",
            "tgt_ix": "434-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_44",
            "tgt_ix": "434-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_44",
            "tgt_ix": "434-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_44",
            "tgt_ix": "434-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_44",
            "tgt_ix": "434-ARR_v2_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_44",
            "tgt_ix": "434-ARR_v2_44@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_44",
            "tgt_ix": "434-ARR_v2_44@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_45",
            "tgt_ix": "434-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_46",
            "tgt_ix": "434-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_46",
            "tgt_ix": "434-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_46",
            "tgt_ix": "434-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_47",
            "tgt_ix": "434-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_48",
            "tgt_ix": "434-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_48",
            "tgt_ix": "434-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_48",
            "tgt_ix": "434-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_48",
            "tgt_ix": "434-ARR_v2_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_49",
            "tgt_ix": "434-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_50",
            "tgt_ix": "434-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_50",
            "tgt_ix": "434-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_51",
            "tgt_ix": "434-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_51",
            "tgt_ix": "434-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_51",
            "tgt_ix": "434-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_51",
            "tgt_ix": "434-ARR_v2_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_51",
            "tgt_ix": "434-ARR_v2_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_51",
            "tgt_ix": "434-ARR_v2_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_51",
            "tgt_ix": "434-ARR_v2_51@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_52",
            "tgt_ix": "434-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_53",
            "tgt_ix": "434-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_53",
            "tgt_ix": "434-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_53",
            "tgt_ix": "434-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_53",
            "tgt_ix": "434-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_53",
            "tgt_ix": "434-ARR_v2_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_54",
            "tgt_ix": "434-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_54",
            "tgt_ix": "434-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_54",
            "tgt_ix": "434-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_54",
            "tgt_ix": "434-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_55",
            "tgt_ix": "434-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_56",
            "tgt_ix": "434-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_57",
            "tgt_ix": "434-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_58",
            "tgt_ix": "434-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_59",
            "tgt_ix": "434-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_60",
            "tgt_ix": "434-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_61",
            "tgt_ix": "434-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_62",
            "tgt_ix": "434-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_63",
            "tgt_ix": "434-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_64",
            "tgt_ix": "434-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_65",
            "tgt_ix": "434-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_66",
            "tgt_ix": "434-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_67",
            "tgt_ix": "434-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_68",
            "tgt_ix": "434-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_69",
            "tgt_ix": "434-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_70",
            "tgt_ix": "434-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "434-ARR_v2_71",
            "tgt_ix": "434-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 565,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "434-ARR",
        "version": 2
    }
}