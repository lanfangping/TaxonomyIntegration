{
    "nodes": [
        {
            "ix": "10-ARR_v2_0",
            "content": "Knowledge Enhanced Reflection Generation for Counseling Dialogues",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_2",
            "content": "In this paper, we study the effect of commonsense and domain knowledge while generating responses in counseling conversations using retrieval and generative methods for knowledge integration. We propose a pipeline that collects domain knowledge through web mining, and show that retrieval from both domainspecific and commonsense knowledge bases improves the quality of generated responses. We also present a model that incorporates knowledge generated by COMET using soft positional encoding and masked self-attention. We show that both retrieved and COMETgenerated knowledge improve the system's performance as measured by automatic metrics and by human evaluation. Lastly, we present a comparative study on the types of knowledge encoded by our system, showing that causal and intentional relationships benefit the generation task more than other types of commonsense relations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "10-ARR_v2_4",
            "content": "Mental health care has been of great importance as the ongoing COVID-19 pandemic poses a serious negative impact on people's mental wellbeing (Paredes et al., 2021). Not only there is a larger unmet need for counseling services, the health care workers are also in tremendous physical and mental strain (Huffman et al., 2021). With this in mind, it is natural to consider how the advancement in natural language processing can be leveraged to help counseling.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_5",
            "content": "Across different counseling styles, reflective listening has always been a fundamental procedure underlying effective counseling practices (Katz and McNulty, 1994). Reflective listening asks the counselor not only to listen to the client carefully, but also to actively make a guess of what the client means. If carried out the right way, it gives the client a sense of being understood and facilitates further self-exploration. However, people do not always say what they mean, which is especially the case for patients seeking mental support. Reflection, as the response made based on reflective listening, sometimes needs to decode the client's meaning not explicitly expressed in words. On the other hand, pressing the client to clarify the missing part may hinder them from expressing their own experience (Miller and Rollnick, 2012). Thus, counseling frequently calls for counselors to make inferences based on their prior knowledge. For example, when the client says I had a really hard time sticking to my diet this week, a plausible reflection may be You're wondering whether you'll be able to lose weight this way, which relates diet with losing weight as an inference based on commonsense knowledge. Moreover, making a good reflection may sometime require domain knowledge. For example, to understand the client in Figure 1, the counselor needs to know that smoking can be a possible cause of emphysema, and Chantix is a medication for smoke cessation. All these cases pose challenges to state-of-the-art language models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_6",
            "content": "In this paper, we propose the task of knowledge enhanced counseling reflection generation, which utilizes the dialogue context as well as commonsense and domain knowledge. This extra knowledge is needed since existing pre-trained language models struggle to produce coherent and informative responses that capture relevant knowledge, even if they have acquired some knowledge during the pre-training phase (Petroni et al., 2019a). A system that generates accurate counseling reflections can serve as a tool to aid counseling training or assist counselors during a session by providing alternative reflections in response to client's statements.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_7",
            "content": "We experiment with two main strategies to incorporate knowledge. The first is retrieval, which acquires sentences containing relevant knowledge based on the vector representations of sentences from the dialogue and assertions in the knowledge base using a BERT-based model (Reimers and Gurevych, 2019a). The second strategy is generative, where we first extract key phrases from the dialogue, and query a COMET model for plausible knowledge triplets with a predefined set of relations (Bosselut et al., 2019). We propose a knowledge-grounded BART (Lewis et al., 2020) model using soft positional encoding and masked self-attention representations to indicate the knowledge position and make the introduced knowledge only visible to the key phrase it relates to.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_8",
            "content": "In addition, we explore the effect of different knowledge sources on the counseling responses generation task. Although commonsense knowledge bases usually have high coverage for general domain concepts, they contain a limited amount of domain-specific knowledge. This applies particularly to medical terminology. For instance, when querying ConceptNet (Speer et al., 2017), a wellknown knowledge base, for the word Chantix (a prescription smoking cessation aid) we are only able to retrieve three relationships, including synonyms, related terms, and type-of, whereas with a common word daughter ConceptNet provides a total of eleven relationships. For the Chantix example in Figure 1, ConceptNet is also missing important causal relationships regarding side effects or suggested usage, which are especially relevant during a counseling conversation about smoking cessation. To address this challenge, we collect a dataset of counseling domain knowledge using web mining with queries constructed with the medical concepts extracted from the dialogue as well as manually defined templates. We compare this Web-collected data with a public commonsense knowledge base, and show that this data collected with no human annotation can serve as a complementary knowledge resource. We also conduct an ablation study on different categories of commonsense knowledge, and show that intentional or causal relationships are more useful for counseling response generation, a finding consistent with related medical literature. (Miller and Rollnick, 2012).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_9",
            "content": "Contributions. The main contributions of this work are as follows: 1) We collect a counseling knowledge base and use it along with commonsense knowledge bases for the task of reflection generation using different retrieval-based methods.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_10",
            "content": "2) We adopt the encoding scheme from K-BERT on BART to incorporate knowledge generated from COMET. 3) We analyze different types of commonsense and domain knowledge, and their effect on the generation task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_11",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "10-ARR_v2_12",
            "content": "Previous research has addressed the task of automating response generation in health care and counseling settings. Greer et al. (2019) used a decision tree to deliver pre-written scripts and guide the user to learn a set of positive emotion skills. V et al. (2019) identified medical entities and the client's intent to fetch an answer for cancer related questions. Almusharraf et al. (2020) classified client's responses to choose which question to ask next for smoking cessation. There are also commercial systems like Woebot (Fitzpatrick et al., 2017) that detect mental health issues mentioned by the user and direct them to relevant information. However, there is a limited amount of work on free-form generation as compared to the template-based approaches described above. Shen et al. (2020) focused on generating counseling reflections with GPT-2 based on the dialogue context and responses retrieved from similar counseling sessions. We address a similar task but enhance the generation process by infusing commonsense and domain specific knowledge to better emulate what counselors do in practice. To the best of our knowledge, the effect of knowledge in counseling response generation is not yet well studied.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_13",
            "content": "Large-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b). However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020). Thus, recent works have also explored enhancing pretrained models with external knowledge. Introducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019). External knowledge resources have been found useful for enhancing language models. For example, large-scale commonsense knowledge graphs (CSKG) that store structured commonsense knowledge in the form of knowledge triplets. The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020). There are also medical related knowledge bases such UMLS (Bodenreider, 2004) and OHAMA. 1 We use Con-ceptNet for commonsense and decide to collect a counseling knowledge base as general domain medical knowledge bases have a limited amount of knowledge aligning with our needs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_14",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "10-ARR_v2_15",
            "content": "We present a model that leverages a combination of existing commonsense knowledge resources and domain-specific knowledge derived from the target domain. The workflow is illustrated in Figure 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_16",
            "content": "Task definition",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "10-ARR_v2_17",
            "content": "We focus on the task of generating dialog responses r using the dialogue context c and an external knowledge base K. The dialogue context consists of a sequence of sentences c = (x 1 , x 2 , ..., x M ), which are M consecutive utterances in the dialogue. The knowledge base K is a collection of triplets. A triplet is denoted as i = (e 1 , r, e 2 ) and its surface text form as s i , where e 1 and e 2 are entities and r is the relationship between them. During the generation process, a set of knowledge k c relevant to c are provided to the model with parameters \u03b8 as additional input. The task generate response \u0177 maximizing the conditional probability P (r|c, k c ; \u03b8).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_18",
            "content": "In the following section, we describe the method to obtain relevant knowledge k c and the approach we use to incorporate knowledge into the language model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_19",
            "content": "Domain Knowledge Collection",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "10-ARR_v2_20",
            "content": "Despite their large size, existing commonsense knowledge bases contain a limited amount of information on domain-specific concepts, especially for causal relationships such as the reason to take a medicine or its side effects. In order to further investigate the effect of domain-specific knowledge in counseling response generation, we propose a pipeline to collect domain knowledge which requires no significant human labor involved. The main steps are as follows.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_21",
            "content": "Medical Concept Extraction. We start by identifying medical concepts occurring in a dataset of counseling conversations (P\u00e9rez-Rosas et al., 2016). We process each conversation utterance using Amazon Comprehend Medical to extract medical entities, along with their detection confidence scores, ranging between 0 to 1. 2 An example of entities extracted from a counseling dialogue is illustrated in Figure 1. Given the distribution of the five medical entity categories in the dataset, shown in Figure 2, we decide to keep medical conditions, medications, tests and treatment procedures entities occurring at least two times, and experimentally set 0.6 as the threshold of confidence scores. Additionally, we manually inspect the resulting entities and remove false positives and misspelled names. After this process we obtain a set of 452 medical entities, distributed as 345 medical conditions, 44 references to medications, and 63 to tests and treatment procedures.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_22",
            "content": "Knowledge Collection with Web Queries.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_23",
            "content": "Next, we collect domain-specific knowledge relevant to the medical entities through web mining.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_24",
            "content": "We compose a set of query templates around causal and intentional relationships frequently observed in the counseling conversations. Each entity types identified during the extraction has a set of eleven distinct query templates as shown in Table 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_25",
            "content": "Web search queries are constructed based on the templates, and searched on Google via the Zenserp API. 3 We keep only the top 100 matching websites for which we extract their text and parse it into sentences using the Spacy toolkit. 4 The resulting sentences with medical concepts are then considered as knowledge candidates during our next step.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_26",
            "content": "Causal Relationship Classification. In order to identify causal knowledge in our set of knowledge candidates, we set up a binary classification task where we seek to determine whether a given sentence contains a causal relationship. The positive samples used for this classifier consist of 1,331 sentences with cause-effect relationships (e.g., He had chest pains and headaches from mold in the bedrooms) from the SemEval10 Task 8 dataset (Hendrickx et al., 2010) and an equal amount of negative samples randomly selected from sentences containing other types of semantic relationships in the same dataset. The classifier is initialized with weights from the pretrained BERT-large model and later fine-tuned using the training set. We run this classifier on our set of knowledge candidate sentences and keep sentences for which the classifier achieves confidence scores higher than 0.7, determined empirically through inspection on a small subset of samples. The resulting set consist of 22,980 sentences containing medical concepts relevant to the counseling domain and their causal relationships.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_27",
            "content": "Retrieved Knowledge Setup",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "10-ARR_v2_28",
            "content": "To get external knowledge that provides useful information based on the dialogue context c, we assume that k c is semantically close to c. We use embedding distance to model the semantic similarity between the context and knowledge in natural language. More specifically, we use sentence-BERT (Reimers and Gurevych, 2019b) to get an embedding F (x i ) for each of input sentence x i . The pre-trained weights are obtained from the paraphrase-distilroberta model in the Sentence-Transformers library 5 . We then select s j as relevant knowledge k c based on its cosine similarity to",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_29",
            "content": "k c = argmax s j \u2208K Sim(F (c), F (s j )) (1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_30",
            "content": "We test three sentence retrieval methods to select the most relevant sentences. The first, retrievaleach consists of obtaining an k x i for each x i . The second, retrieval-average, matches knowledge sentences based on the document embedding obtained by averaging all sentence embeddings F (x i ) M . We also test an oracle retrieval (retrieval-diff ) that uses the difference between the input embedding in retrieval-average and output embeddings F (y) as the document embedding.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_31",
            "content": "Since the sentence-BERT model is trained on natural language instead of structured data such as knowledge triplets, we convert all the triplets in ConceptNet into their surface text form. We use templates built manually to replace the relation with a phrase, for example, triplet (knif e, CapableOf, cut) becomes Knife is capable of cut.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_32",
            "content": "We follow the practice in (Wolf et al., 2019) and incorporate the knowledge k c retrieved in the previous step by appending sentences in k c to the beginning of the context c. They are separated with the special token </s> as BART use the RoBERTa tokenizer for its pre-training. We use BART-large as our baseline in the experiments.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_33",
            "content": "Generated Knowledge Setup",
            "ntype": "title",
            "meta": {
                "section": "3.4"
            }
        },
        {
            "ix": "10-ARR_v2_34",
            "content": "To bypass the difficulty of matching text spans in the context to the knowledge base, we use a generative method to predict an entity e 2 in a knowledge triplet, based on the entity e 1 extracted from context c and a specified relationship r. Compared with the retrieval method described in the previous section, this method has the benefit of being able to specify the type of relation in the knowledge triplet.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_35",
            "content": "We can thus locate the knowledge relevant to specific tokens rather than the whole sentence. To complete the knowledge triplet, we use COMET, a framework for automatic knowledge base construction. This is a GPT model (Radford et al., 2018) finetuned on knowledge triplets from commonsense knowledge bases such as ConceptNet (Speer et al., 2017) and ATOMIC . The model takes j = (e 1 , r, * ) as input and predicts e 2 to complete the knowledge triplet. We use the original implementation 6 and the pretrained weights on ConceptNet.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_36",
            "content": "For each utterance x i in the dialogue context, we use constituency parsing (Kitaev and Klein, 2018) to find the verb phrase and the noun phrase at depth one in the dependency tree, and use them as the input to the COMET model. Following the categorization in (Hwang et al., 2021), we limit the relationships to the commonsense subset to reduce noise and to limit the number of generated knowledge triplets. For noun phrases, the relations are mostly about their physical properties, such as UsedFor and CapableOf. For verb phrases, we focus on the social-interaction or eventcentered aspects, which include relations such as Causes and MotivatedByGoal. For example, for the triplet (loseweight, HasP rerequisite, * ) the model predicts e 2 to be Eat less or Eat healthier.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_37",
            "content": "A potential drawback of appending the knowledge at the beginning of the input is that we are not able to include information about knowledge locality as we can not tell the model which piece of the context the knowledge is corresponding to. Therefore, we take inspiration from K-BERT (Liu et al., 2020) and adopt their representation method into our BART-based model, which is referred as K-BART. We experiment with two ways to keep the structure information.We use BART-large as the baseline, and test inserting r and e 2 without modifying the attention and positional embedding noted as inplace.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_38",
            "content": "Soft Positional Encoding. As BART's transformer layers follow the implementation of RoBERTa, it uses a learned positional embedding, which assigns a unique embedding vector to each location in the input and captures the sequential nature of the input. For COMET generated knowledge, we plug in r and e 2 next to its corresponding e 1 in the original context. Note that the input sentence is no longer a natural sentence, which is different from instances in pretraining. Consider the following sentence with corresponding knowledge in brackets: \"I've been smoking [causes cancer] too much,\". This is usually regarded as two sentences: the original input \"I've been smoking too much\" and the introduced knowledge \"smoking causes cancer.\" However, plain positional encoding scheme is not enough to represent this information. Hence, we treat the input sequence as a tree structure, where the r and e 2 are treated as a branch to the original input at the location next to e 1 . In this case, \"causes\" and \"too\" are both considered as the fourth token right after \"smoking.\" With this approach, the main body of the sentence will have the same index as a sentence without additional knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_39",
            "content": "Mask-Self-Attention. The information introduced by a COMET generated knowledge triplet is only relevant to the first argument e 1 from the original context. Therefore, we use attention mask to modify the visibility of each part in the input sequence, and hide the introduced knowledge from other irrelevant parts of the input. The tokens in the dialogue context can see each other as usual, but the introduced knowledge r and e 2 are only visible to their corresponding e 1 , which means their attention weights are always 0 for other parts of the input. In this way, unrelated tokens will not be affected by the semantics of introduced knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_40",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "10-ARR_v2_41",
            "content": "We choose BART as the backbone network for our generation model. It is a standard seq2seq style transformer which achieved SoTA on multiple down stream tasks with a bidirectional encoder and a left-to-right decoder, which generalizes both GPT2 and BERT. Each model is trained with three random seeds.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_42",
            "content": "Dataset",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "10-ARR_v2_43",
            "content": "We use the dataset from (P\u00e9rez-Rosas et al., 2016) on Motivational Interviewing for language model fine-tuning. The dataset consists of 277 counseling sessions, covering different topics on behavior change, including smoking cessation and weight management. It has annotations on counselor verbal behaviors, such as asking a question, making a reflective response, or seeking collaboration. In the experiments, we form data samples with a reflective response as the target text y and use five former utterances within the counseling dialog as the context c. That leaves us over 3000 samples after filtering.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_44",
            "content": "We use ConceptNet as the knowledge base providing commonsense knowledge. It has over 21 million knowledge triplets with a set of 34 relations covering a wide variety of knowledge, including attributional relationships, causal relationships, etc. We only keep triplets that are in English and from a selected subset of relationships based on their semantic meanings, refer to the appendix for details. This leaves us with a collection of about 3.4 million triplets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_45",
            "content": "Evaluation",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "10-ARR_v2_46",
            "content": "We evaluate our model with several common metrics. We measure the word-overlapping based relevance using BLEU-1/2 (Papineni et al., 2002), ROUGE-1/2 (Lin, 2004), and METEOR (Banerjee and Lavie, 2005). We measure the contextual embedding similarity using BertScore . We measure the diversity with the ratio of unique unigrams or bigrams among generated sentences (Li et al., 2016).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_47",
            "content": "Results of Retrieval Methods",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "10-ARR_v2_48",
            "content": "We first examine how the knowledge from different retrieval methods benefits the system. All the experiments use domain-specific knowledge as the data source. Table 2 shows our experimental results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_49",
            "content": "The retrieval-each method using sentence-level embeddings exceeds the baseline on Rouge-1 and METEOR, while the retrieval-average method, using context-level embeddings of less granularity, outperforms other methods in BLEU-2, Rouge-2, and BertScore. Meanwhile, the oracle method retrieval-diff unsurprisingly gets the highest score in all metrics by a large margin except Dist-1. Overall, results indicate that it is feasible to find relevant information from a domain-specific knowledge base to improve generation given the ground truth.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_50",
            "content": "Result on K-BART Model Architecture",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "10-ARR_v2_51",
            "content": "Next, we investigate whether knowledge from COMET, a generative approach, can provide additional context to the generation task. We also evaluate whether masked attention Att or soft positional encoding Pos are better strategies to infuse knowledge by providing locality information of what tokens the knowledge is related to. We show the results in Table 3. The inplace method, which inserts the relation r and the generated e 2 next to e 1 , shows a significant improvement over the baseline. More specifically, the improvement in Dist-1/2 suggests that commonsense stored in COMET can also be leveraged to introduce new words and concepts into the response. Using masked attention provides further improvements in several automatic metrics, except for a slightly lower BLEU score. Interestingly, the soft positional encoding worsens the performance regardless being used by itself or when combined with masked attention. One potential explanation for this is that BART is more robust to masked attention as its effects are similar to attention dropout, while the soft positional encoding causes more position collision and requires more training samples to be effective.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_52",
            "content": "Experiments Varying Knowledge Source",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "10-ARR_v2_53",
            "content": "After showing that both retrieved and generated knowledge helps to improve the generation of counseling responses, a natural question that follows is: how does the knowledge resource itself affect the overall performance?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_54",
            "content": "To explore this question, we conducted a set of comparative experiments on using domain-specific and commonsense knowledge. During our experiments, we use the retrieval-diff method, which can be seen as an upper-bound of performance using the actual ground truth response. The knowledge candidates are obtained from either ConceptNet triplets in their surface text form or domain-related knowledge collected from the Internet as described in \u00a73.2.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_55",
            "content": "Knowledge. As shown in Table 4 both domainspecific knowledge and commonsense knowledge serve as useful sources of knowledge resources for our generation task. However, the model using ConceptNet performs significantly better than the model using domain-specific knowledge in all metrics except Dist-2. One potential reason for this is that the sheer amount of commonsense knowledge is much larger than the amount we collect and has better coverage for what is mentioned in the dialogue context. However, our experiments show that aggregating both types of knowledge further improves the system's performance. This suggests the domain-specific knowledge provides complementary information relevant to the counseling domain, such as the side effect for a medication, that is not captured by the commonsense knowledge base. Note that more than 20% of the retrieved sentences are from the domain-specific knowledge base, while the commonsense knowledge base is more than 30 times larger in size. This further shows that our data collection pipeline is able to provide knowledge that is more relevant to the dialogue context, with the added benefit of no human annotation involved.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_56",
            "content": "The Role of Different Types of Commonsense Knowledge. We evaluate the role of different types of knowledge by conducting an ablation study based on the main categories in Conceptnet, including attribution, causal, comparison, conditional, intentional, spatial, and temporal categories.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_57",
            "content": "We build separate models by removing a commonsense knowledge category at a time. Results in Table 5 show that removing the intentional relationships harms the performance the most on Rouge-1/2 and METEOR, and removing the causal relationships leads to the lowest score on BLEU-1 and BertScore. Interestingly, these relations are important for counseling conversations where the counselor usually infer the intention or causes behind their clients statements. For instance, in smoke cessation counseling, counselors might be aware that the main reasons to quit are related to well-being or personal relationships.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_58",
            "content": "Removing a few sets of relationships, such as Attribution or Temporal, causes minimal performance drop or even an improvement. These results suggest that those relationships are not salient or introduce noise during the retrieval process.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_59",
            "content": "Human Evaluation",
            "ntype": "title",
            "meta": {
                "section": "4.6"
            }
        },
        {
            "ix": "10-ARR_v2_60",
            "content": "We conduct a human evaluation where we ask annotators to indicate their preferences between our best performing models from both the retrieval and the generative settings, and a model without knowledge enhancement. We evaluated each each model response using three metrics: Fluency indicating whether the sentence is grammatically correct and natural; Coherence indicating whether the response is on topic and relevant to the dialogue history; Reflectiveness indicating if the response summarizes what the client has said or interprets what the client means. All these metrics are scored with a three-point Likert scale.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_61",
            "content": "We also ask the annotators if the retrieved knowledge is helpful for generating a better response, where the knowledge is triplets for the generative setup and sentences for the retrieval setup. In addi-tion, we ask the annotators to pick the best response between our models and the ground truth.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_62",
            "content": "We randomly choose 50 samples for each model to be annotated. The annotation was conducted by two annotators using Qualtrics. 7 The annotators had no information on which model generated the the response being annotated.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_63",
            "content": "Figure 4 shows the average score for each metric and the percentage of times each system was chosen as the best response. Results show that the ground truth responses have the highest score in terms of reflectiveness and coherence. A potential reason for this is that the ground truth responses are generally longer, thus containing more information from the dialogue context. As for the best response, the ground truth was also the most picked one and our models using knowledge have not outperformed the baseline in this regard.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_64",
            "content": "The model using generated knowledge triplets outperforms the baseline in all three metrics, suggesting the motivation and cause relationships generated by COMET brought useful context to the dialog. However, only 22% of the triplets sampled from the test set are considered helpful by our annotators. This calls for closer inspection on the difference between how the models take advantage of commonsense knowledge and how humans perceive it. The model using retrieved knowledge assertions outperforms the baseline on fluency and reflectiveness but has a low coherence score. Among the knowledge assertions, 38% of retrieved sentences are relevant to the dialog when using domain knowledge, and 48% for commonsense knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_65",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "10-ARR_v2_66",
            "content": "In this paper, we proposed the task of knowledge enhanced counseling reflection generation, and experimented with different ways to introduce knowledge into the reflection generation model using both retrieval and generative settings. We found that both strategies benefit the generation task on various automatic metrics, which is further consolidated by the human evaluation. In addition, we showed that counseling domain knowledge serves as good complementary knowledge source to Con-ceptNet. Through an ablation study, we found that commonsense related to intentional and causal relationships is essential for the counseling domain.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "10-ARR_v2_67",
            "content": "Fahad Almusharraf, Jonathan Rose, Peter Selby, Engaging unmotivated smokers to move toward quitting: Design of motivational interviewing-based chatbot through iterative interactions, 2020, Journal of Medical Internet Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Fahad Almusharraf",
                    "Jonathan Rose",
                    "Peter Selby"
                ],
                "title": "Engaging unmotivated smokers to move toward quitting: Design of motivational interviewing-based chatbot through iterative interactions",
                "pub_date": "2020",
                "pub_title": "Journal of Medical Internet Research",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_68",
            "content": "Satanjeev Banerjee, Alon Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, 2005, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Satanjeev Banerjee",
                    "Alon Lavie"
                ],
                "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments",
                "pub_date": "2005",
                "pub_title": "Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_69",
            "content": "Olivier Bodenreider, The unified medical language system (umls): integrating biomedical terminology, 2004, Nucleic acids research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Olivier Bodenreider"
                ],
                "title": "The unified medical language system (umls): integrating biomedical terminology",
                "pub_date": "2004",
                "pub_title": "Nucleic acids research",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_70",
            "content": "Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, COMET: Commonsense transformers for automatic knowledge graph construction, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Antoine Bosselut",
                    "Hannah Rashkin",
                    "Maarten Sap",
                    "Chaitanya Malaviya",
                    "Asli Celikyilmaz",
                    "Yejin Choi"
                ],
                "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_71",
            "content": "Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, Nanyun Peng, R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Tuhin Chakrabarty",
                    "Debanjan Ghosh",
                    "Smaranda Muresan",
                    "Nanyun Peng"
                ],
                "title": "R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_72",
            "content": "Nam Do, Ellie Pavlick, Are rotten apples edible? challenging commonsense inference ability with exceptions, 2021, FINDINGS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Nam Do",
                    "Ellie Pavlick"
                ],
                "title": "Are rotten apples edible? challenging commonsense inference ability with exceptions",
                "pub_date": "2021",
                "pub_title": "FINDINGS",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_73",
            "content": "Kathleen Fitzpatrick, Alison Darcy, Molly Vierhile, Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (woebot): A randomized controlled trial, 2017, JMIR Mental Health, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Kathleen Fitzpatrick",
                    "Alison Darcy",
                    "Molly Vierhile"
                ],
                "title": "Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (woebot): A randomized controlled trial",
                "pub_date": "2017",
                "pub_title": "JMIR Mental Health",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_74",
            "content": "Stephanie Greer, Danielle Ramo, Yin-Juei Chang, Michael Fu, Use of the chatbot \"vivibot\" to deliver positive psychology skills and promote well-being among young people after cancer treatment: Randomized controlled feasibility trial, 2019, JMIR mHealth and uHealth, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Stephanie Greer",
                    "Danielle Ramo",
                    "Yin-Juei Chang",
                    "Michael Fu"
                ],
                "title": "Use of the chatbot \"vivibot\" to deliver positive psychology skills and promote well-being among young people after cancer treatment: Randomized controlled feasibility trial",
                "pub_date": "2019",
                "pub_title": "JMIR mHealth and uHealth",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_75",
            "content": "Iris Hendrickx, Su Kim, Zornitsa Kozareva, Preslav Nakov, ' Diarmuid, Sebastian O S'eaghdha, Marco Pad'o, Lorenza Pennacchiotti, Stan Romano,  Szpakowicz, SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals, 2010, Proceedings of the 5th International Workshop on Semantic Evaluation, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Iris Hendrickx",
                    "Su Kim",
                    "Zornitsa Kozareva",
                    "Preslav Nakov",
                    "' Diarmuid",
                    "Sebastian O S'eaghdha",
                    "Marco Pad'o",
                    "Lorenza Pennacchiotti",
                    "Stan Romano",
                    " Szpakowicz"
                ],
                "title": "SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals",
                "pub_date": "2010",
                "pub_title": "Proceedings of the 5th International Workshop on Semantic Evaluation",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "10-ARR_v2_76",
            "content": "Dimitrios Elizabeth M Huffman, Nicholas Athanasiadis,  Anton, A Lindsay, Dominique Haskett, Dimitrios Doster, Nicole Stefanidis,  Lee, How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic, 2021, The American Journal of Surgery, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Dimitrios Elizabeth M Huffman",
                    "Nicholas Athanasiadis",
                    " Anton",
                    "A Lindsay",
                    "Dominique Haskett",
                    "Dimitrios Doster",
                    "Nicole Stefanidis",
                    " Lee"
                ],
                "title": "How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic",
                "pub_date": "2021",
                "pub_title": "The American Journal of Surgery",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_77",
            "content": "Jena Hwang, Chandra Bhagavatula, Jeff Ronan Le Bras, Keisuke Da, Antoine Sakaguchi, Yejin Bosselut,  Choi, Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs, 2021, AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Jena Hwang",
                    "Chandra Bhagavatula",
                    "Jeff Ronan Le Bras",
                    "Keisuke Da",
                    "Antoine Sakaguchi",
                    "Yejin Bosselut",
                    " Choi"
                ],
                "title": "Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs",
                "pub_date": "2021",
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_78",
            "content": "Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang, Language generation with multi-hop reasoning on commonsense knowledge graph, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Haozhe Ji",
                    "Pei Ke",
                    "Shaohan Huang",
                    "Furu Wei",
                    "Xiaoyan Zhu",
                    "Minlie Huang"
                ],
                "title": "Language generation with multi-hop reasoning on commonsense knowledge graph",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "10-ARR_v2_79",
            "content": "Nora Kassner, Hinrich Sch\u00fctze, Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly, 2020, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Nora Kassner",
                    "Hinrich Sch\u00fctze"
                ],
                "title": "Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly",
                "pub_date": "2020",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_80",
            "content": "UNKNOWN, None, 1994, Reflective listening, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": null,
                "title": null,
                "pub_date": "1994",
                "pub_title": "Reflective listening",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_81",
            "content": "Nikita Kitaev, Dan Klein, Constituency parsing with a self-attentive encoder, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Nikita Kitaev",
                    "Dan Klein"
                ],
                "title": "Constituency parsing with a self-attentive encoder",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "10-ARR_v2_82",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Mike Lewis",
                    "Yinhan Liu",
                    "Naman Goyal ; Abdelrahman Mohamed",
                    "Omer Levy",
                    "Veselin Stoyanov",
                    "Luke Zettlemoyer"
                ],
                "title": "BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_83",
            "content": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, A diversity-promoting objective function for neural conversation models, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Jiwei Li",
                    "Michel Galley",
                    "Chris Brockett",
                    "Jianfeng Gao",
                    "Bill Dolan"
                ],
                "title": "A diversity-promoting objective function for neural conversation models",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "10-ARR_v2_84",
            "content": "Chin-Yew Lin, ROUGE: A package for automatic evaluation of summaries, 2004, Text Summarization Branches Out, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Chin-Yew Lin"
                ],
                "title": "ROUGE: A package for automatic evaluation of summaries",
                "pub_date": "2004",
                "pub_title": "Text Summarization Branches Out",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "10-ARR_v2_85",
            "content": "Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020. K-bert: Enabling language representation with knowledge graph, , AAAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Weijie Liu",
                    "Peng Zhou",
                    "Zhe Zhao",
                    "Zhiruo Wang",
                    "Qi Ju"
                ],
                "title": "Haotang Deng, and Ping Wang. 2020. K-bert: Enabling language representation with knowledge graph",
                "pub_date": null,
                "pub_title": "AAAI",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_86",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_87",
            "content": "Bodhisattwa Huanru Henry Mao, Julian Majumder, Garrison Mcauley,  Cottrell, Improving neural story generation by targeted common sense grounding, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Bodhisattwa Huanru Henry Mao",
                    "Julian Majumder",
                    "Garrison Mcauley",
                    " Cottrell"
                ],
                "title": "Improving neural story generation by targeted common sense grounding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_88",
            "content": "UNKNOWN, None, 2012, Motivational interviewing: Helping people change, Guilford press.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2012",
                "pub_title": "Motivational interviewing: Helping people change",
                "pub": "Guilford press"
            }
        },
        {
            "ix": "10-ARR_v2_89",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "10-ARR_v2_90",
            "content": "Vanessa Mario R Paredes, Crist\u00f3bal Apaolaza, Patrick Fernandez-Robin, Diego Hartmann,  Ya\u00f1ez-Martinez, The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience, 2021, Personality and Individual Differences, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Vanessa Mario R Paredes",
                    "Crist\u00f3bal Apaolaza",
                    "Patrick Fernandez-Robin",
                    "Diego Hartmann",
                    " Ya\u00f1ez-Martinez"
                ],
                "title": "The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience",
                "pub_date": "2021",
                "pub_title": "Personality and Individual Differences",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_91",
            "content": "Ver\u00f3nica P\u00e9rez-Rosas, Rada Mihalcea, Kenneth Resnicow, Satinder Singh, Lawrence An, Building a motivational interviewing dataset, 2016, Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Ver\u00f3nica P\u00e9rez-Rosas",
                    "Rada Mihalcea",
                    "Kenneth Resnicow",
                    "Satinder Singh",
                    "Lawrence An"
                ],
                "title": "Building a motivational interviewing dataset",
                "pub_date": "2016",
                "pub_title": "Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "10-ARR_v2_92",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Association for Computational Linguistics, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Fabio Petroni",
                    "Tim Rockt\u00e4schel",
                    "Sebastian Riedel",
                    "Patrick Lewis",
                    "Anton Bakhtin",
                    "Yuxiang Wu",
                    "Alexander Miller"
                ],
                "title": "Association for Computational Linguistics",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_93",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Association for Computational Linguistics, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Fabio Petroni",
                    "Tim Rockt\u00e4schel",
                    "Sebastian Riedel",
                    "Patrick Lewis",
                    "Anton Bakhtin",
                    "Yuxiang Wu",
                    "Alexander Miller"
                ],
                "title": "Association for Computational Linguistics",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_94",
            "content": "UNKNOWN, None, 2018, Improving language understanding by generative pre-training, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Improving language understanding by generative pre-training",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_95",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-BERT: Sentence embeddings using Siamese BERTnetworks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Nils Reimers",
                    "Iryna Gurevych"
                ],
                "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "10-ARR_v2_96",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-BERT: Sentence embeddings using Siamese BERTnetworks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Nils Reimers",
                    "Iryna Gurevych"
                ],
                "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "10-ARR_v2_97",
            "content": "Pengjie Ren, Zhumin Chen, Christof Monz, Jun Ma, Maarten De Rijke, Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Pengjie Ren",
                    "Zhumin Chen",
                    "Christof Monz",
                    "Jun Ma",
                    "Maarten De Rijke"
                ],
                "title": "Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_98",
            "content": "Maarten Sap, Emily Ronan Le Bras, Chandra Allaway, Nicholas Bhagavatula, Hannah Lourie, Brendan Rashkin,  Roof, A Noah, Yejin Smith,  Choi, Atomic: An atlas of machine commonsense for ifthen reasoning, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Maarten Sap",
                    "Emily Ronan Le Bras",
                    "Chandra Allaway",
                    "Nicholas Bhagavatula",
                    "Hannah Lourie",
                    "Brendan Rashkin",
                    " Roof",
                    "A Noah",
                    "Yejin Smith",
                    " Choi"
                ],
                "title": "Atomic: An atlas of machine commonsense for ifthen reasoning",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_99",
            "content": "Siqi Shen, F Charles, Rada Welch, Ver\u00f3nica Mihalcea,  P\u00e9rez-Rosas, Counseling-style reflection generation using generative pretrained transformers with augmented context, 2020, SIGDIAL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Siqi Shen",
                    "F Charles",
                    "Rada Welch",
                    "Ver\u00f3nica Mihalcea",
                    " P\u00e9rez-Rosas"
                ],
                "title": "Counseling-style reflection generation using generative pretrained transformers with augmented context",
                "pub_date": "2020",
                "pub_title": "SIGDIAL",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_100",
            "content": "Vered Shwartz, Peter West, Le Ronan, Chandra Bras, Yejin Bhagavatula,  Choi, Unsupervised commonsense question answering with self-talk, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Vered Shwartz",
                    "Peter West",
                    "Le Ronan",
                    "Chandra Bras",
                    "Yejin Bhagavatula",
                    " Choi"
                ],
                "title": "Unsupervised commonsense question answering with self-talk",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_101",
            "content": "Haoyu Song, Weinan Zhang, Yiming Cui, Dong Wang, Ting Liu, Exploiting persona information for diverse generation of conversational responses, 2019, IJCAI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Haoyu Song",
                    "Weinan Zhang",
                    "Yiming Cui",
                    "Dong Wang",
                    "Ting Liu"
                ],
                "title": "Exploiting persona information for diverse generation of conversational responses",
                "pub_date": "2019",
                "pub_title": "IJCAI",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_102",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, Conceptnet 5.5: An open multilingual graph of general knowledge, 2017, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Robyn Speer",
                    "Joshua Chin",
                    "Catherine Havasi"
                ],
                "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
                "pub_date": "2017",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_103",
            "content": "R V Belfin, A J Shobana, Megha Manilal, Ashly Mathew, Blessy Babu, A graph based chatbot for cancer patients, 2019, 5th International Conference on Advanced Computing & Communication Systems (ICACCS), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "R V Belfin",
                    "A J Shobana",
                    "Megha Manilal",
                    "Ashly Mathew",
                    "Blessy Babu"
                ],
                "title": "A graph based chatbot for cancer patients",
                "pub_date": "2019",
                "pub_title": "5th International Conference on Advanced Computing & Communication Systems (ICACCS)",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_104",
            "content": "UNKNOWN, None, 2019, Transfertransfo: A transfer learning approach for neural network based conversational agents, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Transfertransfo: A transfer learning approach for neural network based conversational agents",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_105",
            "content": "Hongming Zhang, Daniel Khashabi, Yangqiu Song, Dan Roth, Transomcs: From linguistic graphs to commonsense knowledge, 2020, Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Hongming Zhang",
                    "Daniel Khashabi",
                    "Yangqiu Song",
                    "Dan Roth"
                ],
                "title": "Transomcs: From linguistic graphs to commonsense knowledge",
                "pub_date": "2020",
                "pub_title": "Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_106",
            "content": "UNKNOWN, None, 2019, Bertscore: Evaluating text generation with bert, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Bertscore: Evaluating text generation with bert",
                "pub": null
            }
        },
        {
            "ix": "10-ARR_v2_107",
            "content": "Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, Rui Yan, Knowledgegrounded dialogue generation with pre-trained language models, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Xueliang Zhao",
                    "Wei Wu",
                    "Can Xu",
                    "Chongyang Tao",
                    "Dongyan Zhao",
                    "Rui Yan"
                ],
                "title": "Knowledgegrounded dialogue generation with pre-trained language models",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "10-ARR_v2_0@0",
            "content": "Knowledge Enhanced Reflection Generation for Counseling Dialogues",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_0",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_2@0",
            "content": "In this paper, we study the effect of commonsense and domain knowledge while generating responses in counseling conversations using retrieval and generative methods for knowledge integration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_2",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_2@1",
            "content": "We propose a pipeline that collects domain knowledge through web mining, and show that retrieval from both domainspecific and commonsense knowledge bases improves the quality of generated responses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_2",
            "start": 192,
            "end": 389,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_2@2",
            "content": "We also present a model that incorporates knowledge generated by COMET using soft positional encoding and masked self-attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_2",
            "start": 391,
            "end": 518,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_2@3",
            "content": "We show that both retrieved and COMETgenerated knowledge improve the system's performance as measured by automatic metrics and by human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_2",
            "start": 520,
            "end": 666,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_2@4",
            "content": "Lastly, we present a comparative study on the types of knowledge encoded by our system, showing that causal and intentional relationships benefit the generation task more than other types of commonsense relations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_2",
            "start": 668,
            "end": 880,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_4@0",
            "content": "Mental health care has been of great importance as the ongoing COVID-19 pandemic poses a serious negative impact on people's mental wellbeing (Paredes et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_4",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_4@1",
            "content": "Not only there is a larger unmet need for counseling services, the health care workers are also in tremendous physical and mental strain (Huffman et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_4",
            "start": 166,
            "end": 325,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_4@2",
            "content": "With this in mind, it is natural to consider how the advancement in natural language processing can be leveraged to help counseling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_4",
            "start": 327,
            "end": 458,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@0",
            "content": "Across different counseling styles, reflective listening has always been a fundamental procedure underlying effective counseling practices (Katz and McNulty, 1994).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@1",
            "content": "Reflective listening asks the counselor not only to listen to the client carefully, but also to actively make a guess of what the client means.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 165,
            "end": 307,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@2",
            "content": "If carried out the right way, it gives the client a sense of being understood and facilitates further self-exploration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 309,
            "end": 427,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@3",
            "content": "However, people do not always say what they mean, which is especially the case for patients seeking mental support.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 429,
            "end": 543,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@4",
            "content": "Reflection, as the response made based on reflective listening, sometimes needs to decode the client's meaning not explicitly expressed in words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 545,
            "end": 689,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@5",
            "content": "On the other hand, pressing the client to clarify the missing part may hinder them from expressing their own experience (Miller and Rollnick, 2012).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 691,
            "end": 838,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@6",
            "content": "Thus, counseling frequently calls for counselors to make inferences based on their prior knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 840,
            "end": 938,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@7",
            "content": "For example, when the client says I had a really hard time sticking to my diet this week, a plausible reflection may be You're wondering whether you'll be able to lose weight this way, which relates diet with losing weight as an inference based on commonsense knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 940,
            "end": 1209,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@8",
            "content": "Moreover, making a good reflection may sometime require domain knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 1211,
            "end": 1283,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@9",
            "content": "For example, to understand the client in Figure 1, the counselor needs to know that smoking can be a possible cause of emphysema, and Chantix is a medication for smoke cessation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 1285,
            "end": 1462,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_5@10",
            "content": "All these cases pose challenges to state-of-the-art language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_5",
            "start": 1464,
            "end": 1531,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_6@0",
            "content": "In this paper, we propose the task of knowledge enhanced counseling reflection generation, which utilizes the dialogue context as well as commonsense and domain knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_6",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_6@1",
            "content": "This extra knowledge is needed since existing pre-trained language models struggle to produce coherent and informative responses that capture relevant knowledge, even if they have acquired some knowledge during the pre-training phase (Petroni et al., 2019a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_6",
            "start": 172,
            "end": 429,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_6@2",
            "content": "A system that generates accurate counseling reflections can serve as a tool to aid counseling training or assist counselors during a session by providing alternative reflections in response to client's statements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_6",
            "start": 431,
            "end": 643,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_7@0",
            "content": "We experiment with two main strategies to incorporate knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_7",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_7@1",
            "content": "The first is retrieval, which acquires sentences containing relevant knowledge based on the vector representations of sentences from the dialogue and assertions in the knowledge base using a BERT-based model (Reimers and Gurevych, 2019a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_7",
            "start": 65,
            "end": 302,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_7@2",
            "content": "The second strategy is generative, where we first extract key phrases from the dialogue, and query a COMET model for plausible knowledge triplets with a predefined set of relations (Bosselut et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_7",
            "start": 304,
            "end": 508,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_7@3",
            "content": "We propose a knowledge-grounded BART (Lewis et al., 2020) model using soft positional encoding and masked self-attention representations to indicate the knowledge position and make the introduced knowledge only visible to the key phrase it relates to.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_7",
            "start": 510,
            "end": 760,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_8@0",
            "content": "In addition, we explore the effect of different knowledge sources on the counseling responses generation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_8",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_8@1",
            "content": "Although commonsense knowledge bases usually have high coverage for general domain concepts, they contain a limited amount of domain-specific knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_8",
            "start": 111,
            "end": 262,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_8@2",
            "content": "This applies particularly to medical terminology.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_8",
            "start": 264,
            "end": 312,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_8@3",
            "content": "For instance, when querying ConceptNet (Speer et al., 2017), a wellknown knowledge base, for the word Chantix (a prescription smoking cessation aid) we are only able to retrieve three relationships, including synonyms, related terms, and type-of, whereas with a common word daughter ConceptNet provides a total of eleven relationships.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_8",
            "start": 314,
            "end": 648,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_8@4",
            "content": "For the Chantix example in Figure 1, ConceptNet is also missing important causal relationships regarding side effects or suggested usage, which are especially relevant during a counseling conversation about smoking cessation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_8",
            "start": 650,
            "end": 874,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_8@5",
            "content": "To address this challenge, we collect a dataset of counseling domain knowledge using web mining with queries constructed with the medical concepts extracted from the dialogue as well as manually defined templates.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_8",
            "start": 876,
            "end": 1088,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_8@6",
            "content": "We compare this Web-collected data with a public commonsense knowledge base, and show that this data collected with no human annotation can serve as a complementary knowledge resource.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_8",
            "start": 1090,
            "end": 1273,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_8@7",
            "content": "We also conduct an ablation study on different categories of commonsense knowledge, and show that intentional or causal relationships are more useful for counseling response generation, a finding consistent with related medical literature. (Miller and Rollnick, 2012).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_8",
            "start": 1275,
            "end": 1542,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_9@0",
            "content": "Contributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_9",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_9@1",
            "content": "The main contributions of this work are as follows: 1) We collect a counseling knowledge base and use it along with commonsense knowledge bases for the task of reflection generation using different retrieval-based methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_9",
            "start": 15,
            "end": 236,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_10@0",
            "content": "2) We adopt the encoding scheme from K-BERT on BART to incorporate knowledge generated from COMET.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_10",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_10@1",
            "content": "3) We analyze different types of commonsense and domain knowledge, and their effect on the generation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_10",
            "start": 99,
            "end": 205,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_11@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_11",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_12@0",
            "content": "Previous research has addressed the task of automating response generation in health care and counseling settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_12",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_12@1",
            "content": "Greer et al. (2019) used a decision tree to deliver pre-written scripts and guide the user to learn a set of positive emotion skills.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_12",
            "start": 115,
            "end": 247,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_12@2",
            "content": "V et al. (2019) identified medical entities and the client's intent to fetch an answer for cancer related questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_12",
            "start": 249,
            "end": 364,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_12@3",
            "content": "Almusharraf et al. (2020) classified client's responses to choose which question to ask next for smoking cessation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_12",
            "start": 366,
            "end": 480,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_12@4",
            "content": "There are also commercial systems like Woebot (Fitzpatrick et al., 2017) that detect mental health issues mentioned by the user and direct them to relevant information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_12",
            "start": 482,
            "end": 649,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_12@5",
            "content": "However, there is a limited amount of work on free-form generation as compared to the template-based approaches described above.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_12",
            "start": 651,
            "end": 778,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_12@6",
            "content": "Shen et al. (2020) focused on generating counseling reflections with GPT-2 based on the dialogue context and responses retrieved from similar counseling sessions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_12",
            "start": 780,
            "end": 941,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_12@7",
            "content": "We address a similar task but enhance the generation process by infusing commonsense and domain specific knowledge to better emulate what counselors do in practice.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_12",
            "start": 943,
            "end": 1106,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_12@8",
            "content": "To the best of our knowledge, the effect of knowledge in counseling response generation is not yet well studied.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_12",
            "start": 1108,
            "end": 1219,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_13@0",
            "content": "Large-scale pretrained language models have been shown to encode some knowledge implicitly through their pretraining objectives (Petroni et al., 2019a), including both commonsense (Shwartz et al., 2020) and factual knowledge (Petroni et al., 2019b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_13",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_13@1",
            "content": "However, pretrained language models still struggle with some downstream applications, especially when the model needs to make inference based on context (Do and Pavlick, 2021;Kassner and Sch\u00fctze, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_13",
            "start": 250,
            "end": 451,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_13@2",
            "content": "Thus, recent works have also explored enhancing pretrained models with external knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_13",
            "start": 453,
            "end": 542,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_13@3",
            "content": "Introducing knowledge into language models has been shown to be successful on various downstream tasks and model architecture (Ren et al., 2020;Zhao et al., 2020;Song et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_13",
            "start": 544,
            "end": 724,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_13@4",
            "content": "External knowledge resources have been found useful for enhancing language models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_13",
            "start": 726,
            "end": 807,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_13@5",
            "content": "For example, large-scale commonsense knowledge graphs (CSKG) that store structured commonsense knowledge in the form of knowledge triplets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_13",
            "start": 809,
            "end": 947,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_13@6",
            "content": "The most widely used CSKG resources include Concept-Net (Speer et al., 2017), ATOMIC TransOMCS (Zhang et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_13",
            "start": 949,
            "end": 1064,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_13@7",
            "content": "There are also medical related knowledge bases such UMLS (Bodenreider, 2004) and OHAMA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_13",
            "start": 1066,
            "end": 1152,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_13@8",
            "content": "1 We use Con-ceptNet for commonsense and decide to collect a counseling knowledge base as general domain medical knowledge bases have a limited amount of knowledge aligning with our needs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_13",
            "start": 1154,
            "end": 1341,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_14@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_14",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_15@0",
            "content": "We present a model that leverages a combination of existing commonsense knowledge resources and domain-specific knowledge derived from the target domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_15",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_15@1",
            "content": "The workflow is illustrated in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_15",
            "start": 154,
            "end": 193,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_16@0",
            "content": "Task definition",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_16",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_17@0",
            "content": "We focus on the task of generating dialog responses r using the dialogue context c and an external knowledge base K. The dialogue context consists of a sequence of sentences c = (x 1 , x 2 , ..., x M ), which are M consecutive utterances in the dialogue.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_17",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_17@1",
            "content": "The knowledge base K is a collection of triplets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_17",
            "start": 255,
            "end": 303,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_17@2",
            "content": "A triplet is denoted as i = (e 1 , r, e 2 ) and its surface text form as s i , where e 1 and e 2 are entities and r is the relationship between them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_17",
            "start": 305,
            "end": 453,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_17@3",
            "content": "During the generation process, a set of knowledge k c relevant to c are provided to the model with parameters \u03b8 as additional input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_17",
            "start": 455,
            "end": 586,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_17@4",
            "content": "The task generate response \u0177 maximizing the conditional probability P (r|c, k c ; \u03b8).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_17",
            "start": 588,
            "end": 672,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_18@0",
            "content": "In the following section, we describe the method to obtain relevant knowledge k c and the approach we use to incorporate knowledge into the language model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_18",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_19@0",
            "content": "Domain Knowledge Collection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_19",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_20@0",
            "content": "Despite their large size, existing commonsense knowledge bases contain a limited amount of information on domain-specific concepts, especially for causal relationships such as the reason to take a medicine or its side effects.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_20",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_20@1",
            "content": "In order to further investigate the effect of domain-specific knowledge in counseling response generation, we propose a pipeline to collect domain knowledge which requires no significant human labor involved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_20",
            "start": 227,
            "end": 434,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_20@2",
            "content": "The main steps are as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_20",
            "start": 436,
            "end": 465,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_21@0",
            "content": "Medical Concept Extraction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_21",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_21@1",
            "content": "We start by identifying medical concepts occurring in a dataset of counseling conversations (P\u00e9rez-Rosas et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_21",
            "start": 28,
            "end": 146,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_21@2",
            "content": "We process each conversation utterance using Amazon Comprehend Medical to extract medical entities, along with their detection confidence scores, ranging between 0 to 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_21",
            "start": 148,
            "end": 316,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_21@3",
            "content": "2 An example of entities extracted from a counseling dialogue is illustrated in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_21",
            "start": 318,
            "end": 406,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_21@4",
            "content": "Given the distribution of the five medical entity categories in the dataset, shown in Figure 2, we decide to keep medical conditions, medications, tests and treatment procedures entities occurring at least two times, and experimentally set 0.6 as the threshold of confidence scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_21",
            "start": 408,
            "end": 689,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_21@5",
            "content": "Additionally, we manually inspect the resulting entities and remove false positives and misspelled names.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_21",
            "start": 691,
            "end": 795,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_21@6",
            "content": "After this process we obtain a set of 452 medical entities, distributed as 345 medical conditions, 44 references to medications, and 63 to tests and treatment procedures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_21",
            "start": 797,
            "end": 966,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_22@0",
            "content": "Knowledge Collection with Web Queries.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_22",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_23@0",
            "content": "Next, we collect domain-specific knowledge relevant to the medical entities through web mining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_23",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_24@0",
            "content": "We compose a set of query templates around causal and intentional relationships frequently observed in the counseling conversations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_24",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_24@1",
            "content": "Each entity types identified during the extraction has a set of eleven distinct query templates as shown in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_24",
            "start": 133,
            "end": 248,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_25@0",
            "content": "Web search queries are constructed based on the templates, and searched on Google via the Zenserp API.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_25",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_25@1",
            "content": "3 We keep only the top 100 matching websites for which we extract their text and parse it into sentences using the Spacy toolkit.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_25",
            "start": 103,
            "end": 231,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_25@2",
            "content": "4 The resulting sentences with medical concepts are then considered as knowledge candidates during our next step.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_25",
            "start": 233,
            "end": 345,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_26@0",
            "content": "Causal Relationship Classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_26",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_26@1",
            "content": "In order to identify causal knowledge in our set of knowledge candidates, we set up a binary classification task where we seek to determine whether a given sentence contains a causal relationship.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_26",
            "start": 36,
            "end": 231,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_26@2",
            "content": "The positive samples used for this classifier consist of 1,331 sentences with cause-effect relationships (e.g., He had chest pains and headaches from mold in the bedrooms) from the SemEval10 Task 8 dataset (Hendrickx et al., 2010) and an equal amount of negative samples randomly selected from sentences containing other types of semantic relationships in the same dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_26",
            "start": 233,
            "end": 605,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_26@3",
            "content": "The classifier is initialized with weights from the pretrained BERT-large model and later fine-tuned using the training set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_26",
            "start": 607,
            "end": 730,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_26@4",
            "content": "We run this classifier on our set of knowledge candidate sentences and keep sentences for which the classifier achieves confidence scores higher than 0.7, determined empirically through inspection on a small subset of samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_26",
            "start": 732,
            "end": 957,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_26@5",
            "content": "The resulting set consist of 22,980 sentences containing medical concepts relevant to the counseling domain and their causal relationships.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_26",
            "start": 959,
            "end": 1097,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_27@0",
            "content": "Retrieved Knowledge Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_27",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_28@0",
            "content": "To get external knowledge that provides useful information based on the dialogue context c, we assume that k c is semantically close to c. We use embedding distance to model the semantic similarity between the context and knowledge in natural language.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_28",
            "start": 0,
            "end": 251,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_28@1",
            "content": "More specifically, we use sentence-BERT (Reimers and Gurevych, 2019b) to get an embedding F (x i ) for each of input sentence x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_28",
            "start": 253,
            "end": 383,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_28@2",
            "content": "The pre-trained weights are obtained from the paraphrase-distilroberta model in the Sentence-Transformers library 5 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_28",
            "start": 385,
            "end": 501,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_28@3",
            "content": "We then select s j as relevant knowledge k c based on its cosine similarity to",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_28",
            "start": 503,
            "end": 580,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_29@0",
            "content": "k c = argmax s j \u2208K Sim(F (c), F (s j )) (1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_29",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_30@0",
            "content": "We test three sentence retrieval methods to select the most relevant sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_30",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_30@1",
            "content": "The first, retrievaleach consists of obtaining an k x i for each x i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_30",
            "start": 80,
            "end": 149,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_30@2",
            "content": "The second, retrieval-average, matches knowledge sentences based on the document embedding obtained by averaging all sentence embeddings F (x i ) M .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_30",
            "start": 151,
            "end": 299,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_30@3",
            "content": "We also test an oracle retrieval (retrieval-diff ) that uses the difference between the input embedding in retrieval-average and output embeddings F (y) as the document embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_30",
            "start": 301,
            "end": 479,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_31@0",
            "content": "Since the sentence-BERT model is trained on natural language instead of structured data such as knowledge triplets, we convert all the triplets in ConceptNet into their surface text form.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_31",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_31@1",
            "content": "We use templates built manually to replace the relation with a phrase, for example, triplet (knif e, CapableOf, cut) becomes Knife is capable of cut.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_31",
            "start": 188,
            "end": 336,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_32@0",
            "content": "We follow the practice in (Wolf et al., 2019) and incorporate the knowledge k c retrieved in the previous step by appending sentences in k c to the beginning of the context c. They are separated with the special token </s> as BART use the RoBERTa tokenizer for its pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_32",
            "start": 0,
            "end": 277,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_32@1",
            "content": "We use BART-large as our baseline in the experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_32",
            "start": 279,
            "end": 331,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_33@0",
            "content": "Generated Knowledge Setup",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_33",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_34@0",
            "content": "To bypass the difficulty of matching text spans in the context to the knowledge base, we use a generative method to predict an entity e 2 in a knowledge triplet, based on the entity e 1 extracted from context c and a specified relationship r. Compared with the retrieval method described in the previous section, this method has the benefit of being able to specify the type of relation in the knowledge triplet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_34",
            "start": 0,
            "end": 411,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_35@0",
            "content": "We can thus locate the knowledge relevant to specific tokens rather than the whole sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_35",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_35@1",
            "content": "To complete the knowledge triplet, we use COMET, a framework for automatic knowledge base construction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_35",
            "start": 93,
            "end": 195,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_35@2",
            "content": "This is a GPT model (Radford et al., 2018) finetuned on knowledge triplets from commonsense knowledge bases such as ConceptNet (Speer et al., 2017) and ATOMIC .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_35",
            "start": 197,
            "end": 356,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_35@3",
            "content": "The model takes j = (e 1 , r, * ) as input and predicts e 2 to complete the knowledge triplet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_35",
            "start": 358,
            "end": 451,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_35@4",
            "content": "We use the original implementation 6 and the pretrained weights on ConceptNet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_35",
            "start": 453,
            "end": 530,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_36@0",
            "content": "For each utterance x i in the dialogue context, we use constituency parsing (Kitaev and Klein, 2018) to find the verb phrase and the noun phrase at depth one in the dependency tree, and use them as the input to the COMET model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_36",
            "start": 0,
            "end": 226,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_36@1",
            "content": "Following the categorization in (Hwang et al., 2021), we limit the relationships to the commonsense subset to reduce noise and to limit the number of generated knowledge triplets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_36",
            "start": 228,
            "end": 406,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_36@2",
            "content": "For noun phrases, the relations are mostly about their physical properties, such as UsedFor and CapableOf.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_36",
            "start": 408,
            "end": 513,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_36@3",
            "content": "For verb phrases, we focus on the social-interaction or eventcentered aspects, which include relations such as Causes and MotivatedByGoal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_36",
            "start": 515,
            "end": 652,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_36@4",
            "content": "For example, for the triplet (loseweight, HasP rerequisite, * ) the model predicts e 2 to be Eat less or Eat healthier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_36",
            "start": 654,
            "end": 772,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_37@0",
            "content": "A potential drawback of appending the knowledge at the beginning of the input is that we are not able to include information about knowledge locality as we can not tell the model which piece of the context the knowledge is corresponding to.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_37",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_37@1",
            "content": "Therefore, we take inspiration from K-BERT (Liu et al., 2020) and adopt their representation method into our BART-based model, which is referred as K-BART.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_37",
            "start": 241,
            "end": 395,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_37@2",
            "content": "We experiment with two ways to keep the structure information.We use BART-large as the baseline, and test inserting r and e 2 without modifying the attention and positional embedding noted as inplace.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_37",
            "start": 397,
            "end": 596,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_38@0",
            "content": "Soft Positional Encoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_38",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_38@1",
            "content": "As BART's transformer layers follow the implementation of RoBERTa, it uses a learned positional embedding, which assigns a unique embedding vector to each location in the input and captures the sequential nature of the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_38",
            "start": 26,
            "end": 250,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_38@2",
            "content": "For COMET generated knowledge, we plug in r and e 2 next to its corresponding e 1 in the original context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_38",
            "start": 252,
            "end": 357,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_38@3",
            "content": "Note that the input sentence is no longer a natural sentence, which is different from instances in pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_38",
            "start": 359,
            "end": 469,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_38@4",
            "content": "Consider the following sentence with corresponding knowledge in brackets: \"I've been smoking [causes cancer] too much,\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_38",
            "start": 471,
            "end": 590,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_38@5",
            "content": "This is usually regarded as two sentences: the original input \"I've been smoking too much\" and the introduced knowledge \"smoking causes cancer.\" However, plain positional encoding scheme is not enough to represent this information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_38",
            "start": 592,
            "end": 822,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_38@6",
            "content": "Hence, we treat the input sequence as a tree structure, where the r and e 2 are treated as a branch to the original input at the location next to e 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_38",
            "start": 824,
            "end": 974,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_38@7",
            "content": "In this case, \"causes\" and \"too\" are both considered as the fourth token right after \"smoking.\" With this approach, the main body of the sentence will have the same index as a sentence without additional knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_38",
            "start": 976,
            "end": 1189,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_39@0",
            "content": "Mask-Self-Attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_39",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_39@1",
            "content": "The information introduced by a COMET generated knowledge triplet is only relevant to the first argument e 1 from the original context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_39",
            "start": 21,
            "end": 155,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_39@2",
            "content": "Therefore, we use attention mask to modify the visibility of each part in the input sequence, and hide the introduced knowledge from other irrelevant parts of the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_39",
            "start": 157,
            "end": 325,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_39@3",
            "content": "The tokens in the dialogue context can see each other as usual, but the introduced knowledge r and e 2 are only visible to their corresponding e 1 , which means their attention weights are always 0 for other parts of the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_39",
            "start": 327,
            "end": 553,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_39@4",
            "content": "In this way, unrelated tokens will not be affected by the semantics of introduced knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_39",
            "start": 555,
            "end": 646,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_40@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_40",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_41@0",
            "content": "We choose BART as the backbone network for our generation model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_41",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_41@1",
            "content": "It is a standard seq2seq style transformer which achieved SoTA on multiple down stream tasks with a bidirectional encoder and a left-to-right decoder, which generalizes both GPT2 and BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_41",
            "start": 65,
            "end": 252,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_41@2",
            "content": "Each model is trained with three random seeds.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_41",
            "start": 254,
            "end": 299,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_42@0",
            "content": "Dataset",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_42",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_43@0",
            "content": "We use the dataset from (P\u00e9rez-Rosas et al., 2016) on Motivational Interviewing for language model fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_43",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_43@1",
            "content": "The dataset consists of 277 counseling sessions, covering different topics on behavior change, including smoking cessation and weight management.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_43",
            "start": 112,
            "end": 256,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_43@2",
            "content": "It has annotations on counselor verbal behaviors, such as asking a question, making a reflective response, or seeking collaboration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_43",
            "start": 258,
            "end": 389,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_43@3",
            "content": "In the experiments, we form data samples with a reflective response as the target text y and use five former utterances within the counseling dialog as the context c. That leaves us over 3000 samples after filtering.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_43",
            "start": 391,
            "end": 606,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_44@0",
            "content": "We use ConceptNet as the knowledge base providing commonsense knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_44",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_44@1",
            "content": "It has over 21 million knowledge triplets with a set of 34 relations covering a wide variety of knowledge, including attributional relationships, causal relationships, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_44",
            "start": 73,
            "end": 244,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_44@2",
            "content": "We only keep triplets that are in English and from a selected subset of relationships based on their semantic meanings, refer to the appendix for details.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_44",
            "start": 246,
            "end": 399,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_44@3",
            "content": "This leaves us with a collection of about 3.4 million triplets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_44",
            "start": 401,
            "end": 463,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_45@0",
            "content": "Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_45",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_46@0",
            "content": "We evaluate our model with several common metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_46",
            "start": 0,
            "end": 49,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_46@1",
            "content": "We measure the word-overlapping based relevance using BLEU-1/2 (Papineni et al., 2002), ROUGE-1/2 (Lin, 2004), and METEOR (Banerjee and Lavie, 2005).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_46",
            "start": 51,
            "end": 199,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_46@2",
            "content": "We measure the contextual embedding similarity using BertScore .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_46",
            "start": 201,
            "end": 264,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_46@3",
            "content": "We measure the diversity with the ratio of unique unigrams or bigrams among generated sentences (Li et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_46",
            "start": 266,
            "end": 379,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_47@0",
            "content": "Results of Retrieval Methods",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_47",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_48@0",
            "content": "We first examine how the knowledge from different retrieval methods benefits the system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_48",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_48@1",
            "content": "All the experiments use domain-specific knowledge as the data source.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_48",
            "start": 89,
            "end": 157,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_48@2",
            "content": "Table 2 shows our experimental results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_48",
            "start": 159,
            "end": 197,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_49@0",
            "content": "The retrieval-each method using sentence-level embeddings exceeds the baseline on Rouge-1 and METEOR, while the retrieval-average method, using context-level embeddings of less granularity, outperforms other methods in BLEU-2, Rouge-2, and BertScore.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_49",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_49@1",
            "content": "Meanwhile, the oracle method retrieval-diff unsurprisingly gets the highest score in all metrics by a large margin except Dist-1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_49",
            "start": 251,
            "end": 379,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_49@2",
            "content": "Overall, results indicate that it is feasible to find relevant information from a domain-specific knowledge base to improve generation given the ground truth.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_49",
            "start": 381,
            "end": 538,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_50@0",
            "content": "Result on K-BART Model Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_50",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_51@0",
            "content": "Next, we investigate whether knowledge from COMET, a generative approach, can provide additional context to the generation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_51",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_51@1",
            "content": "We also evaluate whether masked attention Att or soft positional encoding Pos are better strategies to infuse knowledge by providing locality information of what tokens the knowledge is related to.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_51",
            "start": 129,
            "end": 325,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_51@2",
            "content": "We show the results in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_51",
            "start": 327,
            "end": 357,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_51@3",
            "content": "The inplace method, which inserts the relation r and the generated e 2 next to e 1 , shows a significant improvement over the baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_51",
            "start": 359,
            "end": 493,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_51@4",
            "content": "More specifically, the improvement in Dist-1/2 suggests that commonsense stored in COMET can also be leveraged to introduce new words and concepts into the response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_51",
            "start": 495,
            "end": 659,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_51@5",
            "content": "Using masked attention provides further improvements in several automatic metrics, except for a slightly lower BLEU score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_51",
            "start": 661,
            "end": 782,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_51@6",
            "content": "Interestingly, the soft positional encoding worsens the performance regardless being used by itself or when combined with masked attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_51",
            "start": 784,
            "end": 922,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_51@7",
            "content": "One potential explanation for this is that BART is more robust to masked attention as its effects are similar to attention dropout, while the soft positional encoding causes more position collision and requires more training samples to be effective.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_51",
            "start": 924,
            "end": 1172,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_52@0",
            "content": "Experiments Varying Knowledge Source",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_52",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_53@0",
            "content": "After showing that both retrieved and generated knowledge helps to improve the generation of counseling responses, a natural question that follows is: how does the knowledge resource itself affect the overall performance?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_53",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_54@0",
            "content": "To explore this question, we conducted a set of comparative experiments on using domain-specific and commonsense knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_54",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_54@1",
            "content": "During our experiments, we use the retrieval-diff method, which can be seen as an upper-bound of performance using the actual ground truth response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_54",
            "start": 124,
            "end": 271,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_54@2",
            "content": "The knowledge candidates are obtained from either ConceptNet triplets in their surface text form or domain-related knowledge collected from the Internet as described in \u00a73.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_54",
            "start": 273,
            "end": 446,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_55@0",
            "content": "Knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_55",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_55@1",
            "content": "As shown in Table 4 both domainspecific knowledge and commonsense knowledge serve as useful sources of knowledge resources for our generation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_55",
            "start": 11,
            "end": 157,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_55@2",
            "content": "However, the model using ConceptNet performs significantly better than the model using domain-specific knowledge in all metrics except Dist-2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_55",
            "start": 159,
            "end": 300,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_55@3",
            "content": "One potential reason for this is that the sheer amount of commonsense knowledge is much larger than the amount we collect and has better coverage for what is mentioned in the dialogue context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_55",
            "start": 302,
            "end": 493,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_55@4",
            "content": "However, our experiments show that aggregating both types of knowledge further improves the system's performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_55",
            "start": 495,
            "end": 607,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_55@5",
            "content": "This suggests the domain-specific knowledge provides complementary information relevant to the counseling domain, such as the side effect for a medication, that is not captured by the commonsense knowledge base.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_55",
            "start": 609,
            "end": 819,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_55@6",
            "content": "Note that more than 20% of the retrieved sentences are from the domain-specific knowledge base, while the commonsense knowledge base is more than 30 times larger in size.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_55",
            "start": 821,
            "end": 990,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_55@7",
            "content": "This further shows that our data collection pipeline is able to provide knowledge that is more relevant to the dialogue context, with the added benefit of no human annotation involved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_55",
            "start": 992,
            "end": 1175,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_56@0",
            "content": "The Role of Different Types of Commonsense Knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_56",
            "start": 0,
            "end": 52,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_56@1",
            "content": "We evaluate the role of different types of knowledge by conducting an ablation study based on the main categories in Conceptnet, including attribution, causal, comparison, conditional, intentional, spatial, and temporal categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_56",
            "start": 54,
            "end": 284,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_57@0",
            "content": "We build separate models by removing a commonsense knowledge category at a time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_57",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_57@1",
            "content": "Results in Table 5 show that removing the intentional relationships harms the performance the most on Rouge-1/2 and METEOR, and removing the causal relationships leads to the lowest score on BLEU-1 and BertScore.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_57",
            "start": 81,
            "end": 292,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_57@2",
            "content": "Interestingly, these relations are important for counseling conversations where the counselor usually infer the intention or causes behind their clients statements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_57",
            "start": 294,
            "end": 457,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_57@3",
            "content": "For instance, in smoke cessation counseling, counselors might be aware that the main reasons to quit are related to well-being or personal relationships.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_57",
            "start": 459,
            "end": 611,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_58@0",
            "content": "Removing a few sets of relationships, such as Attribution or Temporal, causes minimal performance drop or even an improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_58",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_58@1",
            "content": "These results suggest that those relationships are not salient or introduce noise during the retrieval process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_58",
            "start": 127,
            "end": 237,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_59@0",
            "content": "Human Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_59",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_60@0",
            "content": "We conduct a human evaluation where we ask annotators to indicate their preferences between our best performing models from both the retrieval and the generative settings, and a model without knowledge enhancement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_60",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_60@1",
            "content": "We evaluated each each model response using three metrics: Fluency indicating whether the sentence is grammatically correct and natural; Coherence indicating whether the response is on topic and relevant to the dialogue history; Reflectiveness indicating if the response summarizes what the client has said or interprets what the client means.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_60",
            "start": 215,
            "end": 557,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_60@2",
            "content": "All these metrics are scored with a three-point Likert scale.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_60",
            "start": 559,
            "end": 619,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_61@0",
            "content": "We also ask the annotators if the retrieved knowledge is helpful for generating a better response, where the knowledge is triplets for the generative setup and sentences for the retrieval setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_61",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_61@1",
            "content": "In addi-tion, we ask the annotators to pick the best response between our models and the ground truth.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_61",
            "start": 195,
            "end": 296,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_62@0",
            "content": "We randomly choose 50 samples for each model to be annotated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_62",
            "start": 0,
            "end": 60,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_62@1",
            "content": "The annotation was conducted by two annotators using Qualtrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_62",
            "start": 62,
            "end": 124,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_62@2",
            "content": "7 The annotators had no information on which model generated the the response being annotated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_62",
            "start": 126,
            "end": 219,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_63@0",
            "content": "Figure 4 shows the average score for each metric and the percentage of times each system was chosen as the best response.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_63",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_63@1",
            "content": "Results show that the ground truth responses have the highest score in terms of reflectiveness and coherence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_63",
            "start": 122,
            "end": 230,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_63@2",
            "content": "A potential reason for this is that the ground truth responses are generally longer, thus containing more information from the dialogue context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_63",
            "start": 232,
            "end": 375,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_63@3",
            "content": "As for the best response, the ground truth was also the most picked one and our models using knowledge have not outperformed the baseline in this regard.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_63",
            "start": 377,
            "end": 529,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_64@0",
            "content": "The model using generated knowledge triplets outperforms the baseline in all three metrics, suggesting the motivation and cause relationships generated by COMET brought useful context to the dialog.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_64",
            "start": 0,
            "end": 197,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_64@1",
            "content": "However, only 22% of the triplets sampled from the test set are considered helpful by our annotators.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_64",
            "start": 199,
            "end": 299,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_64@2",
            "content": "This calls for closer inspection on the difference between how the models take advantage of commonsense knowledge and how humans perceive it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_64",
            "start": 301,
            "end": 441,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_64@3",
            "content": "The model using retrieved knowledge assertions outperforms the baseline on fluency and reflectiveness but has a low coherence score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_64",
            "start": 443,
            "end": 574,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_64@4",
            "content": "Among the knowledge assertions, 38% of retrieved sentences are relevant to the dialog when using domain knowledge, and 48% for commonsense knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_64",
            "start": 576,
            "end": 724,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_65@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_65",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_66@0",
            "content": "In this paper, we proposed the task of knowledge enhanced counseling reflection generation, and experimented with different ways to introduce knowledge into the reflection generation model using both retrieval and generative settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_66",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_66@1",
            "content": "We found that both strategies benefit the generation task on various automatic metrics, which is further consolidated by the human evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_66",
            "start": 235,
            "end": 376,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_66@2",
            "content": "In addition, we showed that counseling domain knowledge serves as good complementary knowledge source to Con-ceptNet.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_66",
            "start": 378,
            "end": 494,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_66@3",
            "content": "Through an ablation study, we found that commonsense related to intentional and causal relationships is essential for the counseling domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_66",
            "start": 496,
            "end": 635,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_67@0",
            "content": "Fahad Almusharraf, Jonathan Rose, Peter Selby, Engaging unmotivated smokers to move toward quitting: Design of motivational interviewing-based chatbot through iterative interactions, 2020, Journal of Medical Internet Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_67",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_68@0",
            "content": "Satanjeev Banerjee, Alon Lavie, METEOR: An automatic metric for MT evaluation with improved correlation with human judgments, 2005, Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_68",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_69@0",
            "content": "Olivier Bodenreider, The unified medical language system (umls): integrating biomedical terminology, 2004, Nucleic acids research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_69",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_70@0",
            "content": "Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, COMET: Commonsense transformers for automatic knowledge graph construction, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_70",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_71@0",
            "content": "Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, Nanyun Peng, R\u02c63: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_71",
            "start": 0,
            "end": 245,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_72@0",
            "content": "Nam Do, Ellie Pavlick, Are rotten apples edible? challenging commonsense inference ability with exceptions, 2021, FINDINGS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_72",
            "start": 0,
            "end": 124,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_73@0",
            "content": "Kathleen Fitzpatrick, Alison Darcy, Molly Vierhile, Delivering cognitive behavior therapy to young adults with symptoms of depression and anxiety using a fully automated conversational agent (woebot): A randomized controlled trial, 2017, JMIR Mental Health, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_73",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_74@0",
            "content": "Stephanie Greer, Danielle Ramo, Yin-Juei Chang, Michael Fu, Use of the chatbot \"vivibot\" to deliver positive psychology skills and promote well-being among young people after cancer treatment: Randomized controlled feasibility trial, 2019, JMIR mHealth and uHealth, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_74",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_75@0",
            "content": "Iris Hendrickx, Su Kim, Zornitsa Kozareva, Preslav Nakov, ' Diarmuid, Sebastian O S'eaghdha, Marco Pad'o, Lorenza Pennacchiotti, Stan Romano,  Szpakowicz, SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals, 2010, Proceedings of the 5th International Workshop on Semantic Evaluation, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_75",
            "start": 0,
            "end": 367,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_76@0",
            "content": "Dimitrios Elizabeth M Huffman, Nicholas Athanasiadis,  Anton, A Lindsay, Dominique Haskett, Dimitrios Doster, Nicole Stefanidis,  Lee, How resilient is your team? exploring healthcare providers' well-being during the covid-19 pandemic, 2021, The American Journal of Surgery, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_76",
            "start": 0,
            "end": 275,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_77@0",
            "content": "Jena Hwang, Chandra Bhagavatula, Jeff Ronan Le Bras, Keisuke Da, Antoine Sakaguchi, Yejin Bosselut,  Choi, Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs, 2021, AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_77",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_78@0",
            "content": "Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang, Language generation with multi-hop reasoning on commonsense knowledge graph, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_78",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_79@0",
            "content": "Nora Kassner, Hinrich Sch\u00fctze, Negated and misprimed probes for pretrained language models: Birds can talk, but cannot fly, 2020, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_79",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_80@0",
            "content": "UNKNOWN, None, 1994, Reflective listening, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_80",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_81@0",
            "content": "Nikita Kitaev, Dan Klein, Constituency parsing with a self-attentive encoder, 2018, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_81",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_82@0",
            "content": "Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, BART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_82",
            "start": 0,
            "end": 315,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_83@0",
            "content": "Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, Bill Dolan, A diversity-promoting objective function for neural conversation models, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_83",
            "start": 0,
            "end": 331,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_84@0",
            "content": "Chin-Yew Lin, ROUGE: A package for automatic evaluation of summaries, 2004, Text Summarization Branches Out, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_84",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_85@0",
            "content": "Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020. K-bert: Enabling language representation with knowledge graph, , AAAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_85",
            "start": 0,
            "end": 159,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_86@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_86",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_87@0",
            "content": "Bodhisattwa Huanru Henry Mao, Julian Majumder, Garrison Mcauley,  Cottrell, Improving neural story generation by targeted common sense grounding, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_87",
            "start": 0,
            "end": 329,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_88@0",
            "content": "UNKNOWN, None, 2012, Motivational interviewing: Helping people change, Guilford press.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_88",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_89@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_89",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_90@0",
            "content": "Vanessa Mario R Paredes, Crist\u00f3bal Apaolaza, Patrick Fernandez-Robin, Diego Hartmann,  Ya\u00f1ez-Martinez, The impact of the covid-19 pandemic on subjective mental well-being: The interplay of perceived threat, future anxiety and resilience, 2021, Personality and Individual Differences, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_90",
            "start": 0,
            "end": 284,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_91@0",
            "content": "Ver\u00f3nica P\u00e9rez-Rosas, Rada Mihalcea, Kenneth Resnicow, Satinder Singh, Lawrence An, Building a motivational interviewing dataset, 2016, Proceedings of the Third Workshop on Computational Linguistics and Clinical Psychology, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_91",
            "start": 0,
            "end": 265,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_92@0",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Association for Computational Linguistics, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_92",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_93@0",
            "content": "Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander Miller, Association for Computational Linguistics, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_93",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_94@0",
            "content": "UNKNOWN, None, 2018, Improving language understanding by generative pre-training, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_94",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_95@0",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-BERT: Sentence embeddings using Siamese BERTnetworks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_95",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_96@0",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-BERT: Sentence embeddings using Siamese BERTnetworks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_96",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_97@0",
            "content": "Pengjie Ren, Zhumin Chen, Christof Monz, Jun Ma, Maarten De Rijke, Thinking globally, acting locally: Distantly supervised global-to-local knowledge selection for background based conversation, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_97",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_98@0",
            "content": "Maarten Sap, Emily Ronan Le Bras, Chandra Allaway, Nicholas Bhagavatula, Hannah Lourie, Brendan Rashkin,  Roof, A Noah, Yejin Smith,  Choi, Atomic: An atlas of machine commonsense for ifthen reasoning, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_98",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_99@0",
            "content": "Siqi Shen, F Charles, Rada Welch, Ver\u00f3nica Mihalcea,  P\u00e9rez-Rosas, Counseling-style reflection generation using generative pretrained transformers with augmented context, 2020, SIGDIAL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_99",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_100@0",
            "content": "Vered Shwartz, Peter West, Le Ronan, Chandra Bras, Yejin Bhagavatula,  Choi, Unsupervised commonsense question answering with self-talk, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_100",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_101@0",
            "content": "Haoyu Song, Weinan Zhang, Yiming Cui, Dong Wang, Ting Liu, Exploiting persona information for diverse generation of conversational responses, 2019, IJCAI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_101",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_102@0",
            "content": "Robyn Speer, Joshua Chin, Catherine Havasi, Conceptnet 5.5: An open multilingual graph of general knowledge, 2017, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_102",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_103@0",
            "content": "R V Belfin, A J Shobana, Megha Manilal, Ashly Mathew, Blessy Babu, A graph based chatbot for cancer patients, 2019, 5th International Conference on Advanced Computing & Communication Systems (ICACCS), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_103",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_104@0",
            "content": "UNKNOWN, None, 2019, Transfertransfo: A transfer learning approach for neural network based conversational agents, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_104",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_105@0",
            "content": "Hongming Zhang, Daniel Khashabi, Yangqiu Song, Dan Roth, Transomcs: From linguistic graphs to commonsense knowledge, 2020, Proceedings of International Joint Conference on Artificial Intelligence (IJCAI), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_105",
            "start": 0,
            "end": 205,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_106@0",
            "content": "UNKNOWN, None, 2019, Bertscore: Evaluating text generation with bert, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_106",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "10-ARR_v2_107@0",
            "content": "Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao, Dongyan Zhao, Rui Yan, Knowledgegrounded dialogue generation with pre-trained language models, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "10-ARR_v2_107",
            "start": 0,
            "end": 243,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "10-ARR_v2_0",
            "tgt_ix": "10-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_0",
            "tgt_ix": "10-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_1",
            "tgt_ix": "10-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_1",
            "tgt_ix": "10-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_0",
            "tgt_ix": "10-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_2",
            "tgt_ix": "10-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_4",
            "tgt_ix": "10-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_6",
            "tgt_ix": "10-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_7",
            "tgt_ix": "10-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_8",
            "tgt_ix": "10-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_9",
            "tgt_ix": "10-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_3",
            "tgt_ix": "10-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_3",
            "tgt_ix": "10-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_3",
            "tgt_ix": "10-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_3",
            "tgt_ix": "10-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_3",
            "tgt_ix": "10-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_3",
            "tgt_ix": "10-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_3",
            "tgt_ix": "10-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_3",
            "tgt_ix": "10-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_0",
            "tgt_ix": "10-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_10",
            "tgt_ix": "10-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_12",
            "tgt_ix": "10-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_11",
            "tgt_ix": "10-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_11",
            "tgt_ix": "10-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_11",
            "tgt_ix": "10-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_0",
            "tgt_ix": "10-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_13",
            "tgt_ix": "10-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_14",
            "tgt_ix": "10-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_14",
            "tgt_ix": "10-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_14",
            "tgt_ix": "10-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_15",
            "tgt_ix": "10-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_17",
            "tgt_ix": "10-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_16",
            "tgt_ix": "10-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_16",
            "tgt_ix": "10-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_16",
            "tgt_ix": "10-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_14",
            "tgt_ix": "10-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_18",
            "tgt_ix": "10-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_20",
            "tgt_ix": "10-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_21",
            "tgt_ix": "10-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_22",
            "tgt_ix": "10-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_23",
            "tgt_ix": "10-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_24",
            "tgt_ix": "10-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_25",
            "tgt_ix": "10-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_19",
            "tgt_ix": "10-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_19",
            "tgt_ix": "10-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_19",
            "tgt_ix": "10-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_19",
            "tgt_ix": "10-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_19",
            "tgt_ix": "10-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_19",
            "tgt_ix": "10-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_19",
            "tgt_ix": "10-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_19",
            "tgt_ix": "10-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_14",
            "tgt_ix": "10-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_26",
            "tgt_ix": "10-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_28",
            "tgt_ix": "10-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_29",
            "tgt_ix": "10-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_30",
            "tgt_ix": "10-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_31",
            "tgt_ix": "10-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_27",
            "tgt_ix": "10-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_27",
            "tgt_ix": "10-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_27",
            "tgt_ix": "10-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_27",
            "tgt_ix": "10-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_27",
            "tgt_ix": "10-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_27",
            "tgt_ix": "10-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_14",
            "tgt_ix": "10-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_32",
            "tgt_ix": "10-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_34",
            "tgt_ix": "10-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_35",
            "tgt_ix": "10-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_36",
            "tgt_ix": "10-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_37",
            "tgt_ix": "10-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_38",
            "tgt_ix": "10-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_33",
            "tgt_ix": "10-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_33",
            "tgt_ix": "10-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_33",
            "tgt_ix": "10-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_33",
            "tgt_ix": "10-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_33",
            "tgt_ix": "10-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_33",
            "tgt_ix": "10-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_33",
            "tgt_ix": "10-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_0",
            "tgt_ix": "10-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_39",
            "tgt_ix": "10-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_40",
            "tgt_ix": "10-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_40",
            "tgt_ix": "10-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_40",
            "tgt_ix": "10-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_41",
            "tgt_ix": "10-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_43",
            "tgt_ix": "10-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_42",
            "tgt_ix": "10-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_42",
            "tgt_ix": "10-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_42",
            "tgt_ix": "10-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_40",
            "tgt_ix": "10-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_44",
            "tgt_ix": "10-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_45",
            "tgt_ix": "10-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_45",
            "tgt_ix": "10-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_40",
            "tgt_ix": "10-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_46",
            "tgt_ix": "10-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_48",
            "tgt_ix": "10-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_47",
            "tgt_ix": "10-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_47",
            "tgt_ix": "10-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_47",
            "tgt_ix": "10-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_40",
            "tgt_ix": "10-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_49",
            "tgt_ix": "10-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_50",
            "tgt_ix": "10-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_50",
            "tgt_ix": "10-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_40",
            "tgt_ix": "10-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_51",
            "tgt_ix": "10-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_53",
            "tgt_ix": "10-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_52",
            "tgt_ix": "10-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_52",
            "tgt_ix": "10-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_52",
            "tgt_ix": "10-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_55",
            "tgt_ix": "10-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_56",
            "tgt_ix": "10-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_57",
            "tgt_ix": "10-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_52",
            "tgt_ix": "10-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_52",
            "tgt_ix": "10-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_52",
            "tgt_ix": "10-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_52",
            "tgt_ix": "10-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_54",
            "tgt_ix": "10-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_40",
            "tgt_ix": "10-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_58",
            "tgt_ix": "10-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_60",
            "tgt_ix": "10-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_61",
            "tgt_ix": "10-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_62",
            "tgt_ix": "10-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_63",
            "tgt_ix": "10-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_59",
            "tgt_ix": "10-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_59",
            "tgt_ix": "10-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_59",
            "tgt_ix": "10-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_59",
            "tgt_ix": "10-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_59",
            "tgt_ix": "10-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_59",
            "tgt_ix": "10-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_0",
            "tgt_ix": "10-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_64",
            "tgt_ix": "10-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_65",
            "tgt_ix": "10-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_65",
            "tgt_ix": "10-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "10-ARR_v2_0",
            "tgt_ix": "10-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_1",
            "tgt_ix": "10-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_2",
            "tgt_ix": "10-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_2",
            "tgt_ix": "10-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_2",
            "tgt_ix": "10-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_2",
            "tgt_ix": "10-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_2",
            "tgt_ix": "10-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_3",
            "tgt_ix": "10-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_4",
            "tgt_ix": "10-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_4",
            "tgt_ix": "10-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_4",
            "tgt_ix": "10-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_5",
            "tgt_ix": "10-ARR_v2_5@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_6",
            "tgt_ix": "10-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_6",
            "tgt_ix": "10-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_6",
            "tgt_ix": "10-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_7",
            "tgt_ix": "10-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_7",
            "tgt_ix": "10-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_7",
            "tgt_ix": "10-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_7",
            "tgt_ix": "10-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_8",
            "tgt_ix": "10-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_8",
            "tgt_ix": "10-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_8",
            "tgt_ix": "10-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_8",
            "tgt_ix": "10-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_8",
            "tgt_ix": "10-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_8",
            "tgt_ix": "10-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_8",
            "tgt_ix": "10-ARR_v2_8@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_8",
            "tgt_ix": "10-ARR_v2_8@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_9",
            "tgt_ix": "10-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_9",
            "tgt_ix": "10-ARR_v2_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_10",
            "tgt_ix": "10-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_10",
            "tgt_ix": "10-ARR_v2_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_11",
            "tgt_ix": "10-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_12",
            "tgt_ix": "10-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_12",
            "tgt_ix": "10-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_12",
            "tgt_ix": "10-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_12",
            "tgt_ix": "10-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_12",
            "tgt_ix": "10-ARR_v2_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_12",
            "tgt_ix": "10-ARR_v2_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_12",
            "tgt_ix": "10-ARR_v2_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_12",
            "tgt_ix": "10-ARR_v2_12@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_12",
            "tgt_ix": "10-ARR_v2_12@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_13",
            "tgt_ix": "10-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_13",
            "tgt_ix": "10-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_13",
            "tgt_ix": "10-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_13",
            "tgt_ix": "10-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_13",
            "tgt_ix": "10-ARR_v2_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_13",
            "tgt_ix": "10-ARR_v2_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_13",
            "tgt_ix": "10-ARR_v2_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_13",
            "tgt_ix": "10-ARR_v2_13@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_13",
            "tgt_ix": "10-ARR_v2_13@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_14",
            "tgt_ix": "10-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_15",
            "tgt_ix": "10-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_15",
            "tgt_ix": "10-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_16",
            "tgt_ix": "10-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_17",
            "tgt_ix": "10-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_17",
            "tgt_ix": "10-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_17",
            "tgt_ix": "10-ARR_v2_17@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_17",
            "tgt_ix": "10-ARR_v2_17@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_17",
            "tgt_ix": "10-ARR_v2_17@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_18",
            "tgt_ix": "10-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_19",
            "tgt_ix": "10-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_20",
            "tgt_ix": "10-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_20",
            "tgt_ix": "10-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_20",
            "tgt_ix": "10-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_21",
            "tgt_ix": "10-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_21",
            "tgt_ix": "10-ARR_v2_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_21",
            "tgt_ix": "10-ARR_v2_21@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_21",
            "tgt_ix": "10-ARR_v2_21@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_21",
            "tgt_ix": "10-ARR_v2_21@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_21",
            "tgt_ix": "10-ARR_v2_21@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_21",
            "tgt_ix": "10-ARR_v2_21@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_22",
            "tgt_ix": "10-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_23",
            "tgt_ix": "10-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_24",
            "tgt_ix": "10-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_24",
            "tgt_ix": "10-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_25",
            "tgt_ix": "10-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_25",
            "tgt_ix": "10-ARR_v2_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_25",
            "tgt_ix": "10-ARR_v2_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_26",
            "tgt_ix": "10-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_26",
            "tgt_ix": "10-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_26",
            "tgt_ix": "10-ARR_v2_26@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_26",
            "tgt_ix": "10-ARR_v2_26@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_26",
            "tgt_ix": "10-ARR_v2_26@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_26",
            "tgt_ix": "10-ARR_v2_26@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_27",
            "tgt_ix": "10-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_28",
            "tgt_ix": "10-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_28",
            "tgt_ix": "10-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_28",
            "tgt_ix": "10-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_28",
            "tgt_ix": "10-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_29",
            "tgt_ix": "10-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_30",
            "tgt_ix": "10-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_30",
            "tgt_ix": "10-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_30",
            "tgt_ix": "10-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_30",
            "tgt_ix": "10-ARR_v2_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_31",
            "tgt_ix": "10-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_31",
            "tgt_ix": "10-ARR_v2_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_32",
            "tgt_ix": "10-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_32",
            "tgt_ix": "10-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_33",
            "tgt_ix": "10-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_34",
            "tgt_ix": "10-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_35",
            "tgt_ix": "10-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_35",
            "tgt_ix": "10-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_35",
            "tgt_ix": "10-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_35",
            "tgt_ix": "10-ARR_v2_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_35",
            "tgt_ix": "10-ARR_v2_35@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_36",
            "tgt_ix": "10-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_36",
            "tgt_ix": "10-ARR_v2_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_36",
            "tgt_ix": "10-ARR_v2_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_36",
            "tgt_ix": "10-ARR_v2_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_36",
            "tgt_ix": "10-ARR_v2_36@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_37",
            "tgt_ix": "10-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_37",
            "tgt_ix": "10-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_37",
            "tgt_ix": "10-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_38",
            "tgt_ix": "10-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_38",
            "tgt_ix": "10-ARR_v2_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_38",
            "tgt_ix": "10-ARR_v2_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_38",
            "tgt_ix": "10-ARR_v2_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_38",
            "tgt_ix": "10-ARR_v2_38@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_38",
            "tgt_ix": "10-ARR_v2_38@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_38",
            "tgt_ix": "10-ARR_v2_38@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_38",
            "tgt_ix": "10-ARR_v2_38@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_39",
            "tgt_ix": "10-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_39",
            "tgt_ix": "10-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_39",
            "tgt_ix": "10-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_39",
            "tgt_ix": "10-ARR_v2_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_39",
            "tgt_ix": "10-ARR_v2_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_40",
            "tgt_ix": "10-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_41",
            "tgt_ix": "10-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_41",
            "tgt_ix": "10-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_41",
            "tgt_ix": "10-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_42",
            "tgt_ix": "10-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_43",
            "tgt_ix": "10-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_43",
            "tgt_ix": "10-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_43",
            "tgt_ix": "10-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_43",
            "tgt_ix": "10-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_44",
            "tgt_ix": "10-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_44",
            "tgt_ix": "10-ARR_v2_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_44",
            "tgt_ix": "10-ARR_v2_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_44",
            "tgt_ix": "10-ARR_v2_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_45",
            "tgt_ix": "10-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_46",
            "tgt_ix": "10-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_46",
            "tgt_ix": "10-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_46",
            "tgt_ix": "10-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_46",
            "tgt_ix": "10-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_47",
            "tgt_ix": "10-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_48",
            "tgt_ix": "10-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_48",
            "tgt_ix": "10-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_48",
            "tgt_ix": "10-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_49",
            "tgt_ix": "10-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_49",
            "tgt_ix": "10-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_49",
            "tgt_ix": "10-ARR_v2_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_50",
            "tgt_ix": "10-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_51",
            "tgt_ix": "10-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_51",
            "tgt_ix": "10-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_51",
            "tgt_ix": "10-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_51",
            "tgt_ix": "10-ARR_v2_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_51",
            "tgt_ix": "10-ARR_v2_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_51",
            "tgt_ix": "10-ARR_v2_51@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_51",
            "tgt_ix": "10-ARR_v2_51@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_51",
            "tgt_ix": "10-ARR_v2_51@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_52",
            "tgt_ix": "10-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_53",
            "tgt_ix": "10-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_54",
            "tgt_ix": "10-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_54",
            "tgt_ix": "10-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_54",
            "tgt_ix": "10-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_55",
            "tgt_ix": "10-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_55",
            "tgt_ix": "10-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_55",
            "tgt_ix": "10-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_55",
            "tgt_ix": "10-ARR_v2_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_55",
            "tgt_ix": "10-ARR_v2_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_55",
            "tgt_ix": "10-ARR_v2_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_55",
            "tgt_ix": "10-ARR_v2_55@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_55",
            "tgt_ix": "10-ARR_v2_55@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_56",
            "tgt_ix": "10-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_56",
            "tgt_ix": "10-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_57",
            "tgt_ix": "10-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_57",
            "tgt_ix": "10-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_57",
            "tgt_ix": "10-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_57",
            "tgt_ix": "10-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_58",
            "tgt_ix": "10-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_58",
            "tgt_ix": "10-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_59",
            "tgt_ix": "10-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_60",
            "tgt_ix": "10-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_60",
            "tgt_ix": "10-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_60",
            "tgt_ix": "10-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_61",
            "tgt_ix": "10-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_61",
            "tgt_ix": "10-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_62",
            "tgt_ix": "10-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_62",
            "tgt_ix": "10-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_62",
            "tgt_ix": "10-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_63",
            "tgt_ix": "10-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_63",
            "tgt_ix": "10-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_63",
            "tgt_ix": "10-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_63",
            "tgt_ix": "10-ARR_v2_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_64",
            "tgt_ix": "10-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_64",
            "tgt_ix": "10-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_64",
            "tgt_ix": "10-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_64",
            "tgt_ix": "10-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_64",
            "tgt_ix": "10-ARR_v2_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_65",
            "tgt_ix": "10-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_66",
            "tgt_ix": "10-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_66",
            "tgt_ix": "10-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_66",
            "tgt_ix": "10-ARR_v2_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_66",
            "tgt_ix": "10-ARR_v2_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_67",
            "tgt_ix": "10-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_68",
            "tgt_ix": "10-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_69",
            "tgt_ix": "10-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_70",
            "tgt_ix": "10-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_71",
            "tgt_ix": "10-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_72",
            "tgt_ix": "10-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_73",
            "tgt_ix": "10-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_74",
            "tgt_ix": "10-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_75",
            "tgt_ix": "10-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_76",
            "tgt_ix": "10-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_77",
            "tgt_ix": "10-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_78",
            "tgt_ix": "10-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_79",
            "tgt_ix": "10-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_80",
            "tgt_ix": "10-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_81",
            "tgt_ix": "10-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_82",
            "tgt_ix": "10-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_83",
            "tgt_ix": "10-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_84",
            "tgt_ix": "10-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_85",
            "tgt_ix": "10-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_86",
            "tgt_ix": "10-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_87",
            "tgt_ix": "10-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_88",
            "tgt_ix": "10-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_89",
            "tgt_ix": "10-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_90",
            "tgt_ix": "10-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_91",
            "tgt_ix": "10-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_92",
            "tgt_ix": "10-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_93",
            "tgt_ix": "10-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_94",
            "tgt_ix": "10-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_95",
            "tgt_ix": "10-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_96",
            "tgt_ix": "10-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_97",
            "tgt_ix": "10-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_98",
            "tgt_ix": "10-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_99",
            "tgt_ix": "10-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_100",
            "tgt_ix": "10-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_101",
            "tgt_ix": "10-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_102",
            "tgt_ix": "10-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_103",
            "tgt_ix": "10-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_104",
            "tgt_ix": "10-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_105",
            "tgt_ix": "10-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_106",
            "tgt_ix": "10-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "10-ARR_v2_107",
            "tgt_ix": "10-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 772,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "10-ARR",
        "version": 2
    }
}