{
    "nodes": [
        {
            "ix": "56-ARR_v2_0",
            "content": "UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_2",
            "content": "High-quality phrase representations are essential to finding topics and related terms in documents (a.k.a. topic mining). Existing phrase representation learning methods either simply combine unigram representations in a contextfree manner or rely on extensive annotations to learn context-aware knowledge. In this paper, we propose UCTOPIC, a novel unsupervised contrastive learning framework for context-aware phrase representations and topic mining. UCTOPIC is pretrained in a large scale to distinguish if the contexts of two phrase mentions have the same semantics. The key to pretraining is positive pair construction from our phrase-oriented assumptions. However, we find traditional in-batch negatives cause performance decay when finetuning on a dataset with small topic numbers. Hence, we propose cluster-assisted contrastive learning (CCL) which largely reduces noisy negatives by selecting negatives from clusters and further improves phrase representations for topics accordingly. UCTOPIC outperforms the state-of-the-art phrase representation model by 38.2% NMI in average on four entity clustering tasks. Comprehensive evaluation on topic mining shows that UCTOPIC can extract coherent and diverse topical phrases.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "56-ARR_v2_4",
            "content": "Topic modeling discovers abstract 'topics' in a collection of documents. A topic is typically modeled as a distribution over terms. Highquality phrase representations help topic models understand phrase semantics in order to find well-separated topics and extract coherent phrases. Some phrase representation methods (Wang et al., 2021;Yu and Dredze, 2015;Zhou et al., 2017) learn context-free representations by unigram embedding combination. Context-free representations tend to extract similar phrases mentions (e.g. \"great food\" and \"good food\", see Section 4.3). Contextaware methods such as DensePhrase (Lee et al., The United States is a federation of 50 individual states. Irving Washington's book was popular in the United States. 2021) and LUKE (Yamada et al., 2020) need supervision from task-specific datasets or distant annotations with knowledge bases. Manual or distant supervision limits the ability to represent out-ofvocabulary phrases especially for domain-specific datasets. Recently, contrastive learning has shown effectiveness for unsupervised representation learning in visual (Chen et al., 2020) and textual (Gao et al., 2021) domains.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_5",
            "content": "In this work, we seek to advance state-of-theart phrase representation methods and demonstrate that a contrastive objective can be extremely effective at learning phrase semantics in sentences. We present UCTOPIC, an Unsupervised Contrastive learning framework for phrase representations and TOPIC mining, which can produce superior phrase embeddings and have topic-specific finetuning for topic mining. To conduct contrastive learning for phrase representations, we first seek to produce contrastive pairs. Existing data augmentation methods for natural language processing (NLP) such as back translation (Xie et al., 2020), synonym replacement (Zhang et al., 2015) and text mix up (Zhang et al., 2018) are not designed for phrase-oriented noise, and thus cannot produce training pairs for phrase representation learning. In UCTOPIC, we propose two assumptions about phrase semantics to obtain contrastive pairs:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_6",
            "content": "1. The phrase semantics are determined by their context. 2. Phrases that have the same mentions have the same semantics. As shown in Figure 1, given two sentences that contain the same phrase mentions (e.g., United States), we can mask the phrase mentions and the phrase semantics should stay the same based on assumption (1). Then, the phrase semantics from the two sentences are same as each other given assumption (2). Therefore, we can use the two masked sentences as positive pairs in contrastive learning. The intuition behind the two assumptions is that we expect the phrase representations from different sentences describing the same phrase should group together in the latent space. Masking the phrase mentions forces the model to learn representations from context which prevents overfitting and representation collapse (Gao et al., 2021). Based on the two assumptions, our context-aware phrase representations can be pre-trained on a large corpus via a contrastive objective without supervision.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_7",
            "content": "For large-scale pre-training, we follow previous works (Chen et al., 2017;Henderson et al., 2017;Gao et al., 2021) and adopt in-batch negatives for training. However, we find in-batch negatives undermine the representation performance as finetuning (see Table 1). Because the number of topics is usually small in the finetuning dataset, examples in the same batch are likely to have the same topic. Hence, we cannot use in-batch negatives for data-specific finetuning. To solve this problem, we propose cluster-assisted contrastive learning (CCL) which leverages clustering results as pseudo-labels and sample negatives from highly confident examples in clusters. Cluster-assisted negative sampling has two advantages: (1) reducing potential positives from negative sampling compared to in-batch negatives; (2) the clusters are viewed as topics in documents, thus, cluster-assisted contrastive learning is a topic-specific finetuning process which pushes away instances from different topics in the latent space.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_8",
            "content": "Based on the two assumptions and clusterassisted negative sampling introduced in this paper, we pre-train phrase representations on a large-scale dataset and then finetune on a specific dataset for topic mining in an unsupervised way. In our experiments, we select LUKE (Yamada et al., 2020) as our backbone phrase representation model and pre-train it on Wikipedia 1 English corpus. To evaluate the quality of phrase representations, we conduct entity clustering on four datasets and find that pre-trained UCTOPIC achieves 53.1% (NMI) improvement compared to LUKE. After learning data-specific features with CCL, UCTOPIC outperforms LUKE by 73.2% (NMI) in average. We perform topical phrase mining on three datasets and comprehensive evaluation indicates UCTOPIC extracts coherent and diverse topical phrases. Overall, our contributions are three-fold:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_9",
            "content": "Background",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "56-ARR_v2_10",
            "content": "In this section, we introduce background knowledge about contrastive learning and our phrase encoder LUKE (Yamada et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_11",
            "content": "Contrastive Learning",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "56-ARR_v2_12",
            "content": "Contrastive learning aims to learn effective representations by pulling semantically close neighbors together and pushing apart non-neighbors in the latent space (Hadsell et al., 2006). Assume that we have a contrastive instance {x, x + , x \u2212 1 , . . . , x \u2212 N \u22121 } including one positive and N \u22121 negative instances and their representations {h, h + , h \u2212 1 , . . . , h \u2212 N \u22121 } from the encoder, we follow the contrastive learning framework (Sohn, 2016;Chen et al., 2020;Gao et al., 2021) and take cross-entropy as our objective function:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_13",
            "content": "l = \u2212 log e sim(h,h + )/\u03c4 e sim(h,h + )/\u03c4 + N \u22121 i=1 e sim(h,h \u2212 i )/\u03c4",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_14",
            "content": "(1) where \u03c4 is a temperature hyperparameter and sim(h 1 , h 2 ) is the cosine similarity",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_15",
            "content": "h 1 h 2 h 1 \u2022 h 2 . 6160",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_16",
            "content": "The first printed edition appeared in [MASK].",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_17",
            "content": "His brother Robert was senior sheriff of [MASK].",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_18",
            "content": "James Gunn",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_19",
            "content": "[MASK] is an American film director, actor.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_20",
            "content": "[MASK] is an edible fruit produced by a tree.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_21",
            "content": "Phrase Encoder",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "56-ARR_v2_22",
            "content": "In this paper, our phrase encoder E is transformerbased model LUKE (Yamada et al., 2020)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_23",
            "content": "UCTopic",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "56-ARR_v2_24",
            "content": "UCTOPIC is an unsupervised contrastive learning method for phrase representations and topic mining. Our goal is to learn a phrase encoder as well as topic representations, so we can represent phrases effectively for general settings and find topics from documents in an unsupervised way. In this section, we introduce UCTOPIC from two aspects:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_25",
            "content": "(1) constructing positive pairs for phrases;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_26",
            "content": "(2) cluster-assisted contrastive learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_27",
            "content": "Positive Instances",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "56-ARR_v2_28",
            "content": "One critical problem in constrastive learning is to how to construct positive pairs (x, x + ). Previous works (Wu et al., 2020;Meng et al., 2021) apply augmentation techniques such as word dele-tion, reordering, and paraphrasing. However, these methods are not suitable for phrase representation learning. In this paper, we utilize the proposed assumptions introduced in Section 1 to construct positive instances for contrastive learning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_29",
            "content": "Consider an example to understand our positive instance generation process: In Figure 2 (a), phrase United States appears in two different sentences \"He lived on the east coast of the United States\" and \"How much does it cost to fly to the United States\". We expect the phrase (United States) representations from the two sentences to be similar to reflect phrase semantics. To encourage the model to learn phrase semantics from context and prevent the model from comparing phrase mentions in contrastive learning, we mask the phrase mentions with [MASK] token. The two masked sentences are used as positive instances. To decrease the inconsistency caused by masking between training and evaluation, in a positive pair, we keep one phrase mention unchanged in probability p.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_30",
            "content": "Formally, suppose we have phrase instance x = (s, [l, r]) and its positive instance x + = (s , [l , r ]) where s denotes the sentence and [l, r] are left and right boundaries of a phrase in s, we obtain the phrase representations h and h + by encoder E and apply in-batch negatives for pre-training. The training objective of UCTOPIC becomes:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_31",
            "content": "l = \u2212 log e sim(h,h + )/\u03c4 N i=1 e sim(h,h i )/\u03c4 ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_32",
            "content": "for a mini-batch of N instances, where h i is an instance in a batch.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_33",
            "content": "Cluster-Assisted Contrastive Learning",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "56-ARR_v2_34",
            "content": "We find that contrastive learning with in-batch negatives on small datasets can undermine the phrase representations (see Section 4.2). Different from pre-training on a large corpus, in-batch negatives usually contain instances that have similar semantics as positives. For example, one document has three topics and our batch size is 32. Thus, some instances in one batch are from the same topic but in-batch method views these instances as negatives with each other. In this case, contrastive learning has noisy training signals and then results in decreasing performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_35",
            "content": "To reduce the noise in negatives while optimizing phrase representations according to topics in documents, we propose cluster-assisted contrastive learning (CCL). The basic idea is to utilize prior knowledge from pre-trained representations and clustering to reduce the noise existing in the negatives. Specifically, we first find the topics in documents with a clustering algorithm based on pre-trained phrase representations from UCTOPIC. The centroids of clusters are considered as topic representations for phrases. After computing the cosine distance between phrase instances and centroids, we select t percent of instances that are close to centroids and assign pseudo labels to them. Then, the label of a phrase mention p m 2 is determined by the majority vote of instances {x m 0 , x m 1 , . . . , x m n } that contain p m , where n is the number of sentences assigned pseudo labels. In this way, we get some prior knowledge of phrase mentions for the following contrastive learning. See Figure 2 (b); three phrase mentions (London, James Gunn and Apple) which belong to three different clusters are labeled by different topic categories.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_36",
            "content": "Suppose we have a topic set C in our documents, with phrases and their pseudo labels, we construct positive pairs (x c i , x + c i ) by method introduced in Section 3.1 for topic c i where c i \u2208 C.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_37",
            "content": "To have contrastive instances, we randomly select phrases p m c j and instances x m c j from topic c j as negative instances x \u2212 c j in contrastive learning, where c j \u2208 C \u2227c j = c i . As shown in Figure 2 (b), we construct positive pairs for phrase London, and use two phrases James Gunn and Apple from the other two clusters to randomly select negative instances. With pseudo labels, our method can avoid instances that have similar semantics as London.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_38",
            "content": "2 phrase mentions are extracted from sentence s, i.e., p m = s[l : r]",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_39",
            "content": "The training objective of finetuning is:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_40",
            "content": "l = \u2212 log e sim(hc i ,h + c i )/\u03c4 e sim(hc i ,h + c i )/\u03c4 + c j \u2208C e sim(hc i ,h \u2212 c j )/\u03c4",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_41",
            "content": ".",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_42",
            "content": "(3) As for the masking strategy in pre-training, we conduct masking for all training instances but keep x + c i and x \u2212 c j unchanged in probability p. To infer the topic y of phrase instance x, we compute the cosine similarity between phrase representation h and topic representations hc i , c i \u2208 C. The nearest neighbor topic of x is used as phrase topic. Formally,",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_43",
            "content": "y = argmax c i \u2208C (sim(h, hc i )) (4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_44",
            "content": "4 Experiments",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_45",
            "content": "In this section, we evaluate the effectiveness of UCTOPIC pre-training by contrastive learning. We start with entity clustering to compare the phrase representations from different methods. For topic modeling, we evaluate the topical phrases from three aspects and compare UCTOPIC to other topic modeling baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_46",
            "content": "Implementation Details",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "56-ARR_v2_47",
            "content": "To generate the training corpus, we use English Wikipedia 3 and extract text with hyper links as phrases. Phrases have the same entity ids from Wikidata 4 or have the same mentions are considered as the same phrases (i.e., phrases have the same semantics). We enumerate all sentence pairs containing the same phrase as positive pairs in contrastive learning. After processing, the pre-training dataset has 11.6 million sentences and 108.8 million training instances.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_48",
            "content": "For pre-training, we start from a pretrained LUKE-BASE model (Yamada et al., 2020). We follow previous works (Gao et al., 2021;Soares et al., 2019) and two losses are used concurrently: the masked language model loss and the contrastive learning loss with in-batch negatives. Our pretraining learning rate is 5e-5, batch size is 100 and our model is optimized by AdamW in 1 epoch. The probability p of keeping phrase mentions unchanged is 0.5 and the temperature \u03c4 in the contrastive loss is set to 0.05.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_49",
            "content": "Entity Clustering",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "56-ARR_v2_50",
            "content": "To test the performance of phrase representations under objective tasks and metrics, we first apply UCTOPIC on entity clustering and compare to other representation learning methods. Datasets. We conduct entity clustering on four datasets with annotated entities and their semantic categories are from general, review and biomedical domains: (1) CoNLL2003 (Sang and Meulder, 2003) consists of 20,744 sentences extracted from Reuters news articles. We use Person, Location, and Organization entities in our experiments. 5 (2) BC5CDR (Li et al., 2016) is the BioCreative V CDR task corpus. It contains 18,307 sentences from PubMed articles, with 15,953 chemical and 13,318 disease entities. (3) MIT Movie (MIT-M) (Liu et al., 2013) contains 12,218 sentences with Title and Person entities. (4) W-NUT 2017 (Derczynski et al., 2017) focuses on identifying unusual entities in the context of emerging discussions and contains 5,690 sentences and six kinds of entities 6 . Finetuning Setup. The learning rate for finetuning is 1e-5. We select t (percent of instances) from {5, 10, 20, 50}. The probability p of keeping phrase mentions unchanged and temperature \u03c4 in contrastive loss are the same as in pre-training settings. We apply K-Means to get pseudo labels for all experiments. Because UCTOPIC is an unsupervised method, we use all data to finetune and evaluate. All results for finetuning are the best results during training process. We follow previous clustering works (Xu et al., 2017;Zhang et al., 2021) and adopt Accuracy (ACC) and Normalized Mutual Information (NMI) to evaluate different approaches. Compared Baseline Methods. To demonstrate the effectiveness of our pre-training method and finetuning with cluster-assisted contrastive learning (CCL), we compare baseline methods from two aspects:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_51",
            "content": "(1) Pre-trained token or phrase representations:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_52",
            "content": "\u2022 Glove (Pennington et al., 2014). Pre-trained word embeddings on 6B tokens and dimension is 300. We use averaging word embeddings as the representations of phrases. \u2022 BERT (Devlin et al., 2019). Obtains phrase representations by averaging token representations (BERT-Ave.) or following CGExpan (Zhang et al., 2020) to substitute phrases with the [MASK] token, and use [MASK] representations as phrase embeddings (BERT-MASK). \u2022 LUKE (Yamada et al., 2020). Use as backbone model to show the effectiveness of our contrastive learning for pre-training and finetuning. \u2022 DensePhrase (Lee et al., 2021). Pre-trained phrase representation learning in a supervised way for question answering problem. We use a pre-trained model released from the authors to get phrase representations. \u2022 Phrase-BERT (Wang et al., 2021). Contextagnostic phrase representations from pretraining. We use a pre-trained model from the authors and get representations by phrase mentions. \u2022 Ours w/o CCL. Pre-trained phrase representations of UCTOPIC without cluster-assisted contrastive finetuning.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_53",
            "content": "(2) Fine-tuning methods based on pre-trained representations of UCTOPIC.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_54",
            "content": "\u2022 Classifier. We use pseudo labels as supervision to train a MLP layer and obtain a classifier of phrase categories. \u2022 In-Batch Contrastive Learning. Same as contrastive learning for pre-training which uses in-batch negatives. \u2022 Autoencoder. Widely used in previous neural topic and aspect extraction models Iyyer et al., 2016;Tulkens and van Cranenburgh, 2020). We follow ABAE to implement our autoencoder model for phrases.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_55",
            "content": "Experimental Results. We report evaluation results of entity clustering in When we compare different pre-trained representations, we find that our method (Ours w/o CCL) outperforms the other baselines on three datasets except MIT-M. There are two reasons: (1) All words in MIT-M dataset are lower case which is inconsistent with our pretraining dataset. The inconsistency between training and test causes performance to decay. (2) Sentences from MIT-M are usually short (10.16 words in average) compared to other datasets (e.g., 17.9 words in W-NUT2017). Hence, UCTOPIC can obtain limited contextual information with short sentences. However, the performance decay caused by the two reasons can be eliminated by our CCL finetuning on datasets since on MIT-M UCTOPIC achieves better results (0.661 NMI) than Phrase-BERT (0.575 NMI) after CCL.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_56",
            "content": "On the other hand, compared to other finetuning methods, our CCL finetuning can further improve the pre-trained phrase representations by capturing data-specific features. The improvement is up to 50% NMI on the MIT-M dataset. Ours w/ Class. performs worse than our pre-trained UCTOPIC in most cases which indicates that pseudo labels from clustering are noisy and cannot directly be used as supervision for representation learning. Ours w/ In-B. is similar as Ours w/ Class. which verifies our motivation on using CCL instead of in-batch negatives. An autoencoder can improve pre-trained representations on three datasets but the margins are limited and the performance even drops on W-NUT2017. Compared to other finetuning methods, our CCL finetuning consistently improves pre-trained phrase representations on different domains. Context or Mentions. To investigate the source of UCTOPIC phrase semantics (i.e., phrase mentions or context), we conduct an ablation study on the type of input and compare UCTOPIC to LUKE. To eliminate the influence of repeated phrase mentions on clustering results, we use only one phrase instance (i.e., sentence and position of a phrase) for each phrase mention. As shown in Table 2, there are three types of inputs: (1) Context+Mention: The same input as experiments in Table 1 including the whole sentence that contains the phrase.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_57",
            "content": "(2) Mention: Use only phrase mentions as inputs of the two models. (3) Context: We mask the phrase mentions in sentences and models can only get information from the context. We can see that UCTOPIC gets more information from context (0.43 ACC, 0.16 NMI) than mentions (0.32 ACC, 0.15 NMI). Compared to LUKE, UCTOPIC is more robust to phrase mentions (when predicting on only context, UCTOPIC \u22123% ACC and \u221244% NMI vs. LUKE \u221231% ACC and \u221267% NMI).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_58",
            "content": "Topical Phrase Mining",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "56-ARR_v2_59",
            "content": "In this section, we apply UCTOPIC on topical phrase mining and conduct human evaluation to show our model outperforms previous topic model baselines. Experiment Setup. To find topical phrases in documents, we first extract noun phrases by spaCy 7 noun chunks and remove single pronoun words. Before CCL finetuning, we obtain the number of topics for each dataset by computing the Silhouette Coefficient (Rousseeuw, 1987).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_60",
            "content": "Specifically, we randomly sample 10K phrases from the dataset and apply K-Means clustering on pre-trained UCTOPIC phrase representations with different numbers of cluster. We compute Silhouette Coefficient scores for different topic numbers; the number with the largest score will be used as the topic number in a dataset. Then, we conduct CCL on the dataset with the same settings as described in Section 4.2. Finally, after obtaining topic distribution z x \u2208 R |C| for a phrase instance x in a sentence, we get context-agnostic phrase topics by using averaged topic distribution z p m = 1 n 1\u2264i\u2264n z x m i , where phrase instances {x m i } in different sentences have the same phrase mention p m . The topic of a phrase mention has the highest probability in z p m . Dataset. We conduct topical phrase mining on three datasets from news, review and computer science domains.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_61",
            "content": "\u2022 Gest. We collect restaurant reviews from Google Local 8 and use 100K reviews containing 143,969 sentences for topical phrase mining. \u2022 KP20k (Meng et al., 2017) is a collection of titles and abstracts from computer science papers. 500K sentences are used in our experiments. \u2022 KPTimes (Gallina et al., 2019) Table 3: The numbers of topics in three datasets. Times. We use 500K sentences for topical phrase mining.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_62",
            "content": "The number of topics determined by Silhouette Coefficient is shown in Table 3. Compared Baseline Methods. We compare UCTOPIC against three topic baselines:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_63",
            "content": "\u2022 Phrase-LDA (Mimno, 2015). LDA model incorporates phrases by simply converting phrases into unigrams (e.g., \"city view\" to \"city_view\"). \u2022 TopMine (El- Kishky et al., 2014). A scalable pipeline that partitions a document into phrases, then uses phrases as constraints to ensure all words are placed under the same topic. \u2022 PNTM (Wang et al., 2021). A topic model with Phrase-BERT by using an autoencoder that reconstructs a document representation. The model is viewed as the state-of-the-art topic model.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_64",
            "content": "We do not include topic models such as LDA (Blei et al., 2003), PD-LDA (Lindsey et al., 2012), TNG (Wang et al., 2007), KERT (Danilevsky et al., 2014) as baselines, because these models are compared in TopMine and PNTM. For Phrase-LDA and PNTM, we use the same phrase list produced by UCTOPIC. TopMine uses phrases produced by itself. Topical Phrase Evaluation. We evaluate the quality of topical phrases from three aspects: (1) topical separation;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_65",
            "content": "(2) phrase coherence;",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_66",
            "content": "(3) phrase informativeness and diversity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_67",
            "content": "To evaluate topical separation, we perform the phrase intrusion task following previous work (El-Kishky et al., 2014;Chang et al., 2009). The phrase intrusion task involves a set of questions asking humans to discover the 'intruder' phrase from other phrases. In our experiments, each question has 6 phrases and 5 of them are randomly sampled from the top 50 phrases of one topic and the remaining phrase is randomly chosen from another topic (top 50 phrases). Annotators are asked to select the intruder phrase. We sample 50 questions for each method and each dataset (600 questions in total) and shuffle all questions. Because these questions are sampled independently, we asked 4 annotators to answer these questions and each annotator answers 150 questions on average. Results of the task evaluate how well the phrases are separated by topics. The evaluation results are shown in Figure 3. UCTOPIC outperforms other baselines on three datasets, which means our model can find well-separated topics in documents.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_68",
            "content": "To evaluate phrase coherence in one topic, we follow ABAE and ask annotators to evaluate if the top 50 phrases from one topic are coherent (i.e., most phrases represent the same topic). 3 annotators evaluate four models on Gest and KP20k datasets. Numbers of coherent topics are shown in Table 4. We can see that UCTOPIC, PNTM and TopMine can recognize similar numbers of coherent topics, but the numbers of Phrase-LDA are less than the other three models. For a coherent topic, each of the top phrases will be labeled as correct if the phrase reflects the related topic. Same as ABAE, we adopt precision@n to evaluate the results. Figure 4 shows the results; we can see that UCTOPIC substantially outperforms other models. UCTOPIC can maintain high precision with a large n when the precision of other models decreases. Finally, to evaluate phrase informativeness and diversity, we use tf-idf and word diversity (worddiv.) to evaluate the top topical phrases. Basically, informative phrases cannot be very common phrases in a corpus (e.g., \"good food\" in Gest) and we use tf-idf to evaluate the \"importance\" of a phrase. To eliminate the influence of phrase length, we use averaged word tf-idf in a phrase as the phrase tf-idf. Specifically, tf-idf(p, d) = 1 m 1\u2264i\u2264m tf-idf(w p i ), where d denotes the document and p is the phrase. In our experiments, a document is a sentence in a review.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_69",
            "content": "In addition, we hope that our phrases are diverse enough in a topic instead of expressing the same meaning (e.g., \"good food\" and \"great food\"). To evaluate the diversity of the top phrases, we calculate the ratio of distinct words among all words. Formally, given a list of phrases [p 1 , p 2 , . . . , p n ], we tokenize the phrases into a word list w = [w p 1 1 , w p 1 2 , . . . , w pn m ]; w is the set of unique words in w. The word diversity is computed by |w | |w| . We only evaluate coherent topics labeled in phrase coherence; the coherent topic numbers of Phrase-LDA are smaller than others, hence we evaluate the other three models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_70",
            "content": "We compute the tf-idf and word-div. on the top 10 phrases and use the averaged value on topics as final scores. Results are shown in table 5. PNTM and UCTOPIC achieve similar tf-idf scores, because the two methods use the same phrase lists extracted from spaCy. UCTOPIC extracts the most diverse phrases in a topic, because our phrase representations are more context-aware. In contrast, since PNTM gets representations dependent on phrase mentions, the phrases from PNTM contain From examples, we can see the phrases are consistent with our user study and diversity evaluation. Although the phrases from PNTM are coherent, the diversity of phrases is less than others (e.g., \"drinks\", \"bar drink\", \"just drink\" from Gest) because context-agnostic representations let similar phrase mentions group together. The phrases from TopMine are diverse but are not coherent in some cases (e.g., \"machine learning\" and \"support vector machine\" in the programming topic). In contrast, UCTOPIC can extract coherent and diverse topical phrases from documents.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_71",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "56-ARR_v2_72",
            "content": "Many attempts have been made to extract topical phrases via LDA (Blei et al., 2003). Wallach (2006) incorporated a bigram language model into LDA by a hierarchical dirichlet generative probabilistic model to share the topic across each word within a bigram. TNG (Wang et al., 2007) applied additional latent variables and word-specific multinomials to model bi-grams and combined bi-grams to form n-gram phrases. PD-LDA (Lindsey et al., 2012) used a hierarchical Pitman-Yor process to share the same topic among all words in a given n-gram. Danilevsky et al. (2014) ranked the resultant phrases based on four heuristic metrics. TOP-Mine (El-Kishky et al., 2014) proposed to restrict all constituent terms within a phrase to share the same latent topic and assign a phrase to the topic of its constituent words. Compared to previous topic mining methods, UCTOPIC builds on the success of pre-trained language models and unsupervised contrastive learning on a large-scale dataset. Therefore, UCTOPIC provides high-quality pre-trained phrase representations and state-of-the-art finetuning for topic mining.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_73",
            "content": "Early works in phrase representation build upon a composition function that combines component word embeddings together into simple phrase embedding. Yu and Dredze (2015) implemented the function by rule-based composition over word vectors. Zhou et al. (2017) applied a pair-wise GRU model and datasets such as PPDB (Pavlick et al., 2015) to learn phrase representations. Phrase-BERT (Wang et al., 2021) composed token embeddings from BERT and pretrained on positive instances produced by GPT-2-based diverse paraphrasing model (Krishna et al., 2020). Lee et al. (2021) learned phrase representations from the supervision of reading comprehension tasks and applied representations on open-domain QA. Other works learned phrase embeddings for specific tasks such as semantic parsing (Socher et al., 2011) and machine translation (Bing et al., 2015). In this paper, we present unsupervised contrastive learning method for pre-training phrase representations of general purposes and for finetuning to topicspecific phrase representations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_74",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "56-ARR_v2_75",
            "content": "In this paper, we propose UCTOPIC, a contrastive learning framework that can effectively learn phrase representations without supervision. To finetune on topic mining datasets, we propose clusterassisted contrastive learning which reduces noise by selecting negatives from clusters. During finetuning, our phrase representations are optimized for topics in the document hence the representations are further improved. We conduct comprehensive experiments on entity clustering and topical phrase mining. Results show that UCTOPIC largely improves phrase representations. Objective metrics and a user study indicate UCTOPIC can extract coherent and diverse topical phrases.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "56-ARR_v2_76",
            "content": "Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo, Rebecca Passonneau, Abstractive multidocument summarization via phrase selection and merging, 2015, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Lidong Bing",
                    "Piji Li",
                    "Yi Liao",
                    "Wai Lam",
                    "Weiwei Guo",
                    "Rebecca Passonneau"
                ],
                "title": "Abstractive multidocument summarization via phrase selection and merging",
                "pub_date": "2015",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_77",
            "content": "David Blei, A Ng, Michael Jordan, Latent dirichlet allocation, 2003, J. Mach. Learn. Res, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "David Blei",
                    "A Ng",
                    "Michael Jordan"
                ],
                "title": "Latent dirichlet allocation",
                "pub_date": "2003",
                "pub_title": "J. Mach. Learn. Res",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_78",
            "content": "Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, David Blei, Reading tea leaves: How humans interpret topic models, 2009, NIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jonathan Chang",
                    "Jordan Boyd-Graber",
                    "Sean Gerrish",
                    "Chong Wang",
                    "David Blei"
                ],
                "title": "Reading tea leaves: How humans interpret topic models",
                "pub_date": "2009",
                "pub_title": "NIPS",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_79",
            "content": "UNKNOWN, None, 2002, A simple framework for contrastive learning of visual representations. ArXiv, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2002",
                "pub_title": "A simple framework for contrastive learning of visual representations. ArXiv, abs",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_80",
            "content": "Ting Chen, Yizhou Sun, Yue Shi, Liangjie Hong, On sampling strategies for neural networkbased collaborative filtering, 2017, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Ting Chen",
                    "Yizhou Sun",
                    "Yue Shi",
                    "Liangjie Hong"
                ],
                "title": "On sampling strategies for neural networkbased collaborative filtering",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_81",
            "content": "Marina Danilevsky, Chi Wang, Nihit Desai, Xiang Ren, Jingyi Guo, Jiawei Han, Automatic construction and ranking of topical keyphrases on collections of short documents, 2014, SDM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Marina Danilevsky",
                    "Chi Wang",
                    "Nihit Desai",
                    "Xiang Ren",
                    "Jingyi Guo",
                    "Jiawei Han"
                ],
                "title": "Automatic construction and ranking of topical keyphrases on collections of short documents",
                "pub_date": "2014",
                "pub_title": "SDM",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_82",
            "content": "Leon Derczynski, Eric Nichols, Marieke Van Erp, Nut Limsopatham, Results of the WNUT2017 shared task on novel and emerging entity recognition, 2017, Proceedings of the 3rd Workshop on Noisy User-generated Text, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Leon Derczynski",
                    "Eric Nichols",
                    "Marieke Van Erp",
                    "Nut Limsopatham"
                ],
                "title": "Results of the WNUT2017 shared task on novel and emerging entity recognition",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 3rd Workshop on Noisy User-generated Text",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_83",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_84",
            "content": "Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare Voss, Jiawei Han, Scalable topical phrase mining from text corpora, 2014, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Ahmed El-Kishky",
                    "Yanglei Song",
                    "Chi Wang",
                    "Clare Voss",
                    "Jiawei Han"
                ],
                "title": "Scalable topical phrase mining from text corpora",
                "pub_date": "2014",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_85",
            "content": "Ygor Gallina, Florian Boudin, B\u00e9atrice Daille, Kptimes: A large-scale dataset for keyphrase generation on news documents, 2019, INLG, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Ygor Gallina",
                    "Florian Boudin",
                    "B\u00e9atrice Daille"
                ],
                "title": "Kptimes: A large-scale dataset for keyphrase generation on news documents",
                "pub_date": "2019",
                "pub_title": "INLG",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_86",
            "content": "Tianyu Gao, Xingcheng Yao, Danqi Chen, Simcse: Simple contrastive learning of sentence embeddings, 2021, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Tianyu Gao",
                    "Xingcheng Yao",
                    "Danqi Chen"
                ],
                "title": "Simcse: Simple contrastive learning of sentence embeddings",
                "pub_date": "2021",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_87",
            "content": "Raia Hadsell, Sumit Chopra, Yann Lecun, Dimensionality reduction by learning an invariant mapping, 2006, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Raia Hadsell",
                    "Sumit Chopra",
                    "Yann Lecun"
                ],
                "title": "Dimensionality reduction by learning an invariant mapping",
                "pub_date": "2006",
                "pub_title": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_88",
            "content": "Ruidan He, Hwee Tou Wee Sun Lee, Daniel Ng,  Dahlmeier, An unsupervised neural attention model for aspect extraction, 2017, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Ruidan He",
                    "Hwee Tou Wee Sun Lee",
                    "Daniel Ng",
                    " Dahlmeier"
                ],
                "title": "An unsupervised neural attention model for aspect extraction",
                "pub_date": "2017",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_89",
            "content": "Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-Hsuan Sung, L\u00e1szl\u00f3 Luk\u00e1cs, Ruiqi Guo, Efficient natural language response suggestion for smart reply, 2017, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Matthew Henderson",
                    "Rami Al-Rfou",
                    "Brian Strope",
                    "Yun-Hsuan Sung",
                    "L\u00e1szl\u00f3 Luk\u00e1cs",
                    "Ruiqi Guo"
                ],
                "title": "Efficient natural language response suggestion for smart reply",
                "pub_date": "2017",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_90",
            "content": "Mohit Iyyer, Anupam Guha, Snigdha Chaturvedi, Jordan Boyd-Graber, Hal Daum\u00e9, Feuding families and former friends: Unsupervised learning for dynamic fictional relationships, 2016, NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Mohit Iyyer",
                    "Anupam Guha",
                    "Snigdha Chaturvedi",
                    "Jordan Boyd-Graber",
                    "Hal Daum\u00e9"
                ],
                "title": "Feuding families and former friends: Unsupervised learning for dynamic fictional relationships",
                "pub_date": "2016",
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_91",
            "content": "Kalpesh Krishna, John Wieting, Mohit Iyyer, Reformulating unsupervised style transfer as paraphrase generation, 2020, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Kalpesh Krishna",
                    "John Wieting",
                    "Mohit Iyyer"
                ],
                "title": "Reformulating unsupervised style transfer as paraphrase generation",
                "pub_date": "2020",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_92",
            "content": "Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen, Learning dense representations of phrases at scale, 2021, ACL/IJCNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Jinhyuk Lee",
                    "Mujeen Sung",
                    "Jaewoo Kang",
                    "Danqi Chen"
                ],
                "title": "Learning dense representations of phrases at scale",
                "pub_date": "2021",
                "pub_title": "ACL/IJCNLP",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_93",
            "content": "J Li, Yueping Sun, Robin Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, A Davis, C Mattingly, Thomas Wiegers, Zhiyong Lu, Biocreative v cdr task corpus: a resource for chemical disease relation extraction, 2016, Database: The Journal of Biological Databases and Curation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "J Li",
                    "Yueping Sun",
                    "Robin Johnson",
                    "Daniela Sciaky",
                    "Chih-Hsuan Wei",
                    "Robert Leaman",
                    "A Davis",
                    "C Mattingly",
                    "Thomas Wiegers",
                    "Zhiyong Lu"
                ],
                "title": "Biocreative v cdr task corpus: a resource for chemical disease relation extraction",
                "pub_date": "2016",
                "pub_title": "Database: The Journal of Biological Databases and Curation",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_94",
            "content": "Robert Lindsey, Will Headden, Michael Stipicevic, A phrase-discovering topic model using hierarchical pitman-yor processes, 2012, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Robert Lindsey",
                    "Will Headden",
                    "Michael Stipicevic"
                ],
                "title": "A phrase-discovering topic model using hierarchical pitman-yor processes",
                "pub_date": "2012",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_95",
            "content": "Jingjing Liu, Panupong Pasupat, Yining Wang, D Scott Cyphers, James Glass, Query understanding enhanced by hierarchical parsing structures, 2013, IEEE Workshop on Automatic Speech Recognition and Understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Jingjing Liu",
                    "Panupong Pasupat",
                    "Yining Wang",
                    "D Scott Cyphers",
                    "James Glass"
                ],
                "title": "Query understanding enhanced by hierarchical parsing structures",
                "pub_date": "2013",
                "pub_title": "IEEE Workshop on Automatic Speech Recognition and Understanding",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_96",
            "content": "Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, Yu Chi, Deep keyphrase generation, 2017, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Rui Meng",
                    "Sanqiang Zhao",
                    "Shuguang Han",
                    "Daqing He",
                    "Peter Brusilovsky",
                    "Yu Chi"
                ],
                "title": "Deep keyphrase generation",
                "pub_date": "2017",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_97",
            "content": "Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, Xia Song, Coco-lm: Correcting and contrasting text sequences for language model pretraining, 2021, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Yu Meng",
                    "Chenyan Xiong",
                    "Payal Bajaj",
                    "Saurabh Tiwary",
                    "Paul Bennett",
                    "Jiawei Han",
                    "Xia Song"
                ],
                "title": "Coco-lm: Correcting and contrasting text sequences for language model pretraining",
                "pub_date": "2021",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_98",
            "content": "UNKNOWN, None, 2015, Using phrases in mallet topic models, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Using phrases in mallet topic models",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_99",
            "content": "Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch, Benjamin Van Durme, Chris Callison-Burch, Ppdb 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification, 2015, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Ellie Pavlick",
                    "Pushpendre Rastogi",
                    "Juri Ganitkevitch",
                    "Benjamin Van Durme",
                    "Chris Callison-Burch"
                ],
                "title": "Ppdb 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification",
                "pub_date": "2015",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_100",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, Glove: Global vectors for word representation, 2014, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Jeffrey Pennington",
                    "Richard Socher",
                    "Christopher Manning"
                ],
                "title": "Glove: Global vectors for word representation",
                "pub_date": "2014",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_101",
            "content": "Peter Rousseeuw, Silhouettes: a graphical aid to the interpretation and validation of cluster analysis, 1987, Journal of Computational and Applied Mathematics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Peter Rousseeuw"
                ],
                "title": "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis",
                "pub_date": "1987",
                "pub_title": "Journal of Computational and Applied Mathematics",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_102",
            "content": "E Sang, F Meulder, Introduction to the conll-2003 shared task: Language-independent named entity recognition, 2003, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "E Sang",
                    "F Meulder"
                ],
                "title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition",
                "pub_date": "2003",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_103",
            "content": "UNKNOWN, None, 1906, Matching the blanks: Distributional similarity for relation learning. ArXiv, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": null,
                "title": null,
                "pub_date": "1906",
                "pub_title": "Matching the blanks: Distributional similarity for relation learning. ArXiv, abs",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_104",
            "content": "UNKNOWN, None, 2011, Parsing natural scenes and natural language with recursive neural networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": null,
                "title": null,
                "pub_date": "2011",
                "pub_title": "Parsing natural scenes and natural language with recursive neural networks",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_105",
            "content": "Kihyuk Sohn, Improved deep metric learning with multi-class n-pair loss objective, 2016, NIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Kihyuk Sohn"
                ],
                "title": "Improved deep metric learning with multi-class n-pair loss objective",
                "pub_date": "2016",
                "pub_title": "NIPS",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_106",
            "content": "St\u00e9phan Tulkens, Andreas Van Cranenburgh, Embarrassingly simple unsupervised aspect extraction, 2020, ACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "St\u00e9phan Tulkens",
                    "Andreas Van Cranenburgh"
                ],
                "title": "Embarrassingly simple unsupervised aspect extraction",
                "pub_date": "2020",
                "pub_title": "ACL",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_107",
            "content": "M Hanna,  Wallach, Topic modeling: beyond bag-of-words, 2006, Proceedings of the 23rd international conference on Machine learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "M Hanna",
                    " Wallach"
                ],
                "title": "Topic modeling: beyond bag-of-words",
                "pub_date": "2006",
                "pub_title": "Proceedings of the 23rd international conference on Machine learning",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_108",
            "content": "Shufan Wang, Laure Thompson, Mohit Iyyer, Phrase-bert: Improved phrase embeddings from bert with an application to corpus exploration, 2021, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Shufan Wang",
                    "Laure Thompson",
                    "Mohit Iyyer"
                ],
                "title": "Phrase-bert: Improved phrase embeddings from bert with an application to corpus exploration",
                "pub_date": "2021",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_109",
            "content": "Xuerui Wang, Andrew Mccallum, Xing Wei, Topical n-grams: Phrase and topic discovery, with an application to information retrieval, 2007, Seventh IEEE International Conference on Data Mining (ICDM 2007), .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Xuerui Wang",
                    "Andrew Mccallum",
                    "Xing Wei"
                ],
                "title": "Topical n-grams: Phrase and topic discovery, with an application to information retrieval",
                "pub_date": "2007",
                "pub_title": "Seventh IEEE International Conference on Data Mining (ICDM 2007)",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_110",
            "content": "Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, Hao Ma, Clear: Contrastive learning for sentence representation, 2012, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Zhuofeng Wu",
                    "Sinong Wang",
                    "Jiatao Gu",
                    "Madian Khabsa",
                    "Fei Sun",
                    "Hao Ma"
                ],
                "title": "Clear: Contrastive learning for sentence representation",
                "pub_date": "2012",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_111",
            "content": "UNKNOWN, None, 2020, Unsupervised data augmentation for consistency training, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Unsupervised data augmentation for consistency training",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_112",
            "content": "Jiaming Xu, Bo Xu, Peng Wang, Suncong Zheng, Guanhua Tian, Self-taught convolutional neural networks for short text clustering, 2017, Neural networks : the official journal of the International Neural Network Society, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Jiaming Xu",
                    "Bo Xu",
                    "Peng Wang",
                    "Suncong Zheng",
                    "Guanhua Tian"
                ],
                "title": "Self-taught convolutional neural networks for short text clustering",
                "pub_date": "2017",
                "pub_title": "Neural networks : the official journal of the International Neural Network Society",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_113",
            "content": "Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto, Luke: Deep contextualized entity representations with entityaware self-attention, 2020, EMNLP, .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Ikuya Yamada",
                    "Akari Asai",
                    "Hiroyuki Shindo",
                    "Hideaki Takeda",
                    "Yuji Matsumoto"
                ],
                "title": "Luke: Deep contextualized entity representations with entityaware self-attention",
                "pub_date": "2020",
                "pub_title": "EMNLP",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_114",
            "content": "Mo Yu, Mark Dredze, Learning composition models for phrase embeddings, 2015, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Mo Yu",
                    "Mark Dredze"
                ],
                "title": "Learning composition models for phrase embeddings",
                "pub_date": "2015",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_115",
            "content": "Dejiao Zhang, Feng Nan, Xiaokai Wei, Shang-Wen Li, Henghui Zhu, Kathleen Mckeown, Ramesh Nallapati, Andrew O. Arnold, and Bing Xiang. 2021. Supporting clustering with contrastive learning, , NAACL, .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Dejiao Zhang",
                    "Feng Nan",
                    "Xiaokai Wei",
                    "Shang-Wen Li",
                    "Henghui Zhu",
                    "Kathleen Mckeown"
                ],
                "title": "Ramesh Nallapati, Andrew O. Arnold, and Bing Xiang. 2021. Supporting clustering with contrastive learning",
                "pub_date": null,
                "pub_title": "NAACL",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_116",
            "content": "Hongyi Zhang, Moustapha Ciss\u00e9, Yann Dauphin, David Lopez-Paz, None, 2018, mixup: Beyond empirical risk minimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Hongyi Zhang",
                    "Moustapha Ciss\u00e9",
                    "Yann Dauphin",
                    "David Lopez-Paz"
                ],
                "title": null,
                "pub_date": "2018",
                "pub_title": "mixup: Beyond empirical risk minimization",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_117",
            "content": "Xiang Zhang, Junbo Zhao, Yann Andr\u00e9 Le-Cun, Character-level convolutional networks for text classification, 2015, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b41",
                "authors": [
                    "Xiang Zhang",
                    "Junbo Zhao",
                    "Yann Andr\u00e9 Le-Cun"
                ],
                "title": "Character-level convolutional networks for text classification",
                "pub_date": "2015",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_118",
            "content": "UNKNOWN, None, 2004, Empower entity set expansion via language model probing. ArXiv, abs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b42",
                "authors": null,
                "title": null,
                "pub_date": "2004",
                "pub_title": "Empower entity set expansion via language model probing. ArXiv, abs",
                "pub": null
            }
        },
        {
            "ix": "56-ARR_v2_119",
            "content": "Zhihao Zhou, Lifu Huang, Heng Ji, Learning phrase embeddings from paraphrases with grus, 2017, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b43",
                "authors": [
                    "Zhihao Zhou",
                    "Lifu Huang",
                    "Heng Ji"
                ],
                "title": "Learning phrase embeddings from paraphrases with grus",
                "pub_date": "2017",
                "pub_title": "ArXiv",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "56-ARR_v2_0@0",
            "content": "UCTopic: Unsupervised Contrastive Learning for Phrase Representations and Topic Mining",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_0",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_2@0",
            "content": "High-quality phrase representations are essential to finding topics and related terms in documents (a.k.a. topic mining).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_2",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_2@1",
            "content": "Existing phrase representation learning methods either simply combine unigram representations in a contextfree manner or rely on extensive annotations to learn context-aware knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_2",
            "start": 122,
            "end": 305,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_2@2",
            "content": "In this paper, we propose UCTOPIC, a novel unsupervised contrastive learning framework for context-aware phrase representations and topic mining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_2",
            "start": 307,
            "end": 451,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_2@3",
            "content": "UCTOPIC is pretrained in a large scale to distinguish if the contexts of two phrase mentions have the same semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_2",
            "start": 453,
            "end": 569,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_2@4",
            "content": "The key to pretraining is positive pair construction from our phrase-oriented assumptions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_2",
            "start": 571,
            "end": 660,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_2@5",
            "content": "However, we find traditional in-batch negatives cause performance decay when finetuning on a dataset with small topic numbers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_2",
            "start": 662,
            "end": 787,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_2@6",
            "content": "Hence, we propose cluster-assisted contrastive learning (CCL) which largely reduces noisy negatives by selecting negatives from clusters and further improves phrase representations for topics accordingly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_2",
            "start": 789,
            "end": 992,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_2@7",
            "content": "UCTOPIC outperforms the state-of-the-art phrase representation model by 38.2% NMI in average on four entity clustering tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_2",
            "start": 994,
            "end": 1118,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_2@8",
            "content": "Comprehensive evaluation on topic mining shows that UCTOPIC can extract coherent and diverse topical phrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_2",
            "start": 1120,
            "end": 1228,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_4@0",
            "content": "Topic modeling discovers abstract 'topics' in a collection of documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_4",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_4@1",
            "content": "A topic is typically modeled as a distribution over terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_4",
            "start": 73,
            "end": 130,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_4@2",
            "content": "Highquality phrase representations help topic models understand phrase semantics in order to find well-separated topics and extract coherent phrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_4",
            "start": 132,
            "end": 280,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_4@3",
            "content": "Some phrase representation methods (Wang et al., 2021;Yu and Dredze, 2015;Zhou et al., 2017) learn context-free representations by unigram embedding combination.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_4",
            "start": 282,
            "end": 442,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_4@4",
            "content": "Context-free representations tend to extract similar phrases mentions (e.g. \"great food\" and \"good food\", see Section 4.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_4",
            "start": 444,
            "end": 566,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_4@5",
            "content": "Contextaware methods such as DensePhrase (Lee et al., The United States is a federation of 50 individual states. Irving Washington's book was popular in the United States. 2021) and LUKE (Yamada et al., 2020) need supervision from task-specific datasets or distant annotations with knowledge bases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_4",
            "start": 568,
            "end": 865,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_4@6",
            "content": "Manual or distant supervision limits the ability to represent out-ofvocabulary phrases especially for domain-specific datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_4",
            "start": 867,
            "end": 993,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_4@7",
            "content": "Recently, contrastive learning has shown effectiveness for unsupervised representation learning in visual (Chen et al., 2020) and textual (Gao et al., 2021) domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_4",
            "start": 995,
            "end": 1159,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_5@0",
            "content": "In this work, we seek to advance state-of-theart phrase representation methods and demonstrate that a contrastive objective can be extremely effective at learning phrase semantics in sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_5",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_5@1",
            "content": "We present UCTOPIC, an Unsupervised Contrastive learning framework for phrase representations and TOPIC mining, which can produce superior phrase embeddings and have topic-specific finetuning for topic mining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_5",
            "start": 194,
            "end": 402,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_5@2",
            "content": "To conduct contrastive learning for phrase representations, we first seek to produce contrastive pairs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_5",
            "start": 404,
            "end": 506,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_5@3",
            "content": "Existing data augmentation methods for natural language processing (NLP) such as back translation (Xie et al., 2020), synonym replacement (Zhang et al., 2015) and text mix up (Zhang et al., 2018) are not designed for phrase-oriented noise, and thus cannot produce training pairs for phrase representation learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_5",
            "start": 508,
            "end": 821,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_5@4",
            "content": "In UCTOPIC, we propose two assumptions about phrase semantics to obtain contrastive pairs:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_5",
            "start": 823,
            "end": 912,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_6@0",
            "content": "1. The phrase semantics are determined by their context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_6",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_6@1",
            "content": "2. Phrases that have the same mentions have the same semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_6",
            "start": 57,
            "end": 119,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_6@2",
            "content": "As shown in Figure 1, given two sentences that contain the same phrase mentions (e.g., United States), we can mask the phrase mentions and the phrase semantics should stay the same based on assumption (1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_6",
            "start": 121,
            "end": 325,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_6@3",
            "content": "Then, the phrase semantics from the two sentences are same as each other given assumption (2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_6",
            "start": 327,
            "end": 420,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_6@4",
            "content": "Therefore, we can use the two masked sentences as positive pairs in contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_6",
            "start": 422,
            "end": 510,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_6@5",
            "content": "The intuition behind the two assumptions is that we expect the phrase representations from different sentences describing the same phrase should group together in the latent space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_6",
            "start": 512,
            "end": 691,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_6@6",
            "content": "Masking the phrase mentions forces the model to learn representations from context which prevents overfitting and representation collapse (Gao et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_6",
            "start": 693,
            "end": 849,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_6@7",
            "content": "Based on the two assumptions, our context-aware phrase representations can be pre-trained on a large corpus via a contrastive objective without supervision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_6",
            "start": 851,
            "end": 1006,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_7@0",
            "content": "For large-scale pre-training, we follow previous works (Chen et al., 2017;Henderson et al., 2017;Gao et al., 2021) and adopt in-batch negatives for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_7",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_7@1",
            "content": "However, we find in-batch negatives undermine the representation performance as finetuning (see Table 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_7",
            "start": 158,
            "end": 262,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_7@2",
            "content": "Because the number of topics is usually small in the finetuning dataset, examples in the same batch are likely to have the same topic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_7",
            "start": 264,
            "end": 397,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_7@3",
            "content": "Hence, we cannot use in-batch negatives for data-specific finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_7",
            "start": 399,
            "end": 467,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_7@4",
            "content": "To solve this problem, we propose cluster-assisted contrastive learning (CCL) which leverages clustering results as pseudo-labels and sample negatives from highly confident examples in clusters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_7",
            "start": 469,
            "end": 662,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_7@5",
            "content": "Cluster-assisted negative sampling has two advantages: (1) reducing potential positives from negative sampling compared to in-batch negatives; (2) the clusters are viewed as topics in documents, thus, cluster-assisted contrastive learning is a topic-specific finetuning process which pushes away instances from different topics in the latent space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_7",
            "start": 664,
            "end": 1011,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_8@0",
            "content": "Based on the two assumptions and clusterassisted negative sampling introduced in this paper, we pre-train phrase representations on a large-scale dataset and then finetune on a specific dataset for topic mining in an unsupervised way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_8",
            "start": 0,
            "end": 233,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_8@1",
            "content": "In our experiments, we select LUKE (Yamada et al., 2020) as our backbone phrase representation model and pre-train it on Wikipedia 1 English corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_8",
            "start": 235,
            "end": 382,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_8@2",
            "content": "To evaluate the quality of phrase representations, we conduct entity clustering on four datasets and find that pre-trained UCTOPIC achieves 53.1% (NMI) improvement compared to LUKE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_8",
            "start": 384,
            "end": 564,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_8@3",
            "content": "After learning data-specific features with CCL, UCTOPIC outperforms LUKE by 73.2% (NMI) in average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_8",
            "start": 566,
            "end": 664,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_8@4",
            "content": "We perform topical phrase mining on three datasets and comprehensive evaluation indicates UCTOPIC extracts coherent and diverse topical phrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_8",
            "start": 666,
            "end": 809,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_8@5",
            "content": "Overall, our contributions are three-fold:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_8",
            "start": 811,
            "end": 852,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_9@0",
            "content": "Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_9",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_10@0",
            "content": "In this section, we introduce background knowledge about contrastive learning and our phrase encoder LUKE (Yamada et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_10",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_11@0",
            "content": "Contrastive Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_11",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_12@0",
            "content": "Contrastive learning aims to learn effective representations by pulling semantically close neighbors together and pushing apart non-neighbors in the latent space (Hadsell et al., 2006).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_12",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_12@1",
            "content": "Assume that we have a contrastive instance {x, x + , x \u2212 1 , . . . , x \u2212 N \u22121 } including one positive and N \u22121 negative instances and their representations {h, h + , h \u2212 1 , . . . , h \u2212 N \u22121 } from the encoder, we follow the contrastive learning framework (Sohn, 2016;Chen et al., 2020;Gao et al., 2021) and take cross-entropy as our objective function:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_12",
            "start": 186,
            "end": 539,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_13@0",
            "content": "l = \u2212 log e sim(h,h + )/\u03c4 e sim(h,h + )/\u03c4 + N \u22121 i=1 e sim(h,h \u2212 i )/\u03c4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_13",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_14@0",
            "content": "(1) where \u03c4 is a temperature hyperparameter and sim(h 1 , h 2 ) is the cosine similarity",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_14",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_15@0",
            "content": "h 1 h 2 h 1 \u2022 h 2 . 6160",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_15",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_16@0",
            "content": "The first printed edition appeared in [MASK].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_16",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_17@0",
            "content": "His brother Robert was senior sheriff of [MASK].",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_17",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_18@0",
            "content": "James Gunn",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_18",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_19@0",
            "content": "[MASK] is an American film director, actor.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_19",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_20@0",
            "content": "[MASK] is an edible fruit produced by a tree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_20",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_21@0",
            "content": "Phrase Encoder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_21",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_22@0",
            "content": "In this paper, our phrase encoder E is transformerbased model LUKE (Yamada et al., 2020)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_22",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_23@0",
            "content": "UCTopic",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_23",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_24@0",
            "content": "UCTOPIC is an unsupervised contrastive learning method for phrase representations and topic mining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_24",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_24@1",
            "content": "Our goal is to learn a phrase encoder as well as topic representations, so we can represent phrases effectively for general settings and find topics from documents in an unsupervised way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_24",
            "start": 100,
            "end": 286,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_24@2",
            "content": "In this section, we introduce UCTOPIC from two aspects:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_24",
            "start": 288,
            "end": 342,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_25@0",
            "content": "(1) constructing positive pairs for phrases;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_25",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_26@0",
            "content": "(2) cluster-assisted contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_26",
            "start": 0,
            "end": 41,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_27@0",
            "content": "Positive Instances",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_27",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_28@0",
            "content": "One critical problem in constrastive learning is to how to construct positive pairs (x, x + ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_28",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_28@1",
            "content": "Previous works (Wu et al., 2020;Meng et al., 2021) apply augmentation techniques such as word dele-tion, reordering, and paraphrasing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_28",
            "start": 95,
            "end": 228,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_28@2",
            "content": "However, these methods are not suitable for phrase representation learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_28",
            "start": 230,
            "end": 304,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_28@3",
            "content": "In this paper, we utilize the proposed assumptions introduced in Section 1 to construct positive instances for contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_28",
            "start": 306,
            "end": 437,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_29@0",
            "content": "Consider an example to understand our positive instance generation process: In Figure 2 (a), phrase United States appears in two different sentences \"He lived on the east coast of the United States\" and \"How much does it cost to fly to the United States\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_29",
            "start": 0,
            "end": 254,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_29@1",
            "content": "We expect the phrase (United States) representations from the two sentences to be similar to reflect phrase semantics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_29",
            "start": 256,
            "end": 373,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_29@2",
            "content": "To encourage the model to learn phrase semantics from context and prevent the model from comparing phrase mentions in contrastive learning, we mask the phrase mentions with [MASK] token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_29",
            "start": 375,
            "end": 560,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_29@3",
            "content": "The two masked sentences are used as positive instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_29",
            "start": 562,
            "end": 617,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_29@4",
            "content": "To decrease the inconsistency caused by masking between training and evaluation, in a positive pair, we keep one phrase mention unchanged in probability p.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_29",
            "start": 619,
            "end": 773,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_30@0",
            "content": "Formally, suppose we have phrase instance x = (s, [l, r]) and its positive instance x + = (s , [l , r ]) where s denotes the sentence and [l, r] are left and right boundaries of a phrase in s, we obtain the phrase representations h and h + by encoder E and apply in-batch negatives for pre-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_30",
            "start": 0,
            "end": 298,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_30@1",
            "content": "The training objective of UCTOPIC becomes:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_30",
            "start": 300,
            "end": 341,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_31@0",
            "content": "l = \u2212 log e sim(h,h + )/\u03c4 N i=1 e sim(h,h i )/\u03c4 ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_31",
            "start": 0,
            "end": 51,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_32@0",
            "content": "for a mini-batch of N instances, where h i is an instance in a batch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_32",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_33@0",
            "content": "Cluster-Assisted Contrastive Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_33",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_34@0",
            "content": "We find that contrastive learning with in-batch negatives on small datasets can undermine the phrase representations (see Section 4.2).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_34",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_34@1",
            "content": "Different from pre-training on a large corpus, in-batch negatives usually contain instances that have similar semantics as positives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_34",
            "start": 136,
            "end": 268,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_34@2",
            "content": "For example, one document has three topics and our batch size is 32.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_34",
            "start": 270,
            "end": 337,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_34@3",
            "content": "Thus, some instances in one batch are from the same topic but in-batch method views these instances as negatives with each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_34",
            "start": 339,
            "end": 467,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_34@4",
            "content": "In this case, contrastive learning has noisy training signals and then results in decreasing performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_34",
            "start": 469,
            "end": 573,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_35@0",
            "content": "To reduce the noise in negatives while optimizing phrase representations according to topics in documents, we propose cluster-assisted contrastive learning (CCL).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_35",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_35@1",
            "content": "The basic idea is to utilize prior knowledge from pre-trained representations and clustering to reduce the noise existing in the negatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_35",
            "start": 163,
            "end": 301,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_35@2",
            "content": "Specifically, we first find the topics in documents with a clustering algorithm based on pre-trained phrase representations from UCTOPIC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_35",
            "start": 303,
            "end": 439,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_35@3",
            "content": "The centroids of clusters are considered as topic representations for phrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_35",
            "start": 441,
            "end": 518,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_35@4",
            "content": "After computing the cosine distance between phrase instances and centroids, we select t percent of instances that are close to centroids and assign pseudo labels to them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_35",
            "start": 520,
            "end": 689,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_35@5",
            "content": "Then, the label of a phrase mention p m 2 is determined by the majority vote of instances {x m 0 , x m 1 , . . . , x m n } that contain p m , where n is the number of sentences assigned pseudo labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_35",
            "start": 691,
            "end": 890,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_35@6",
            "content": "In this way, we get some prior knowledge of phrase mentions for the following contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_35",
            "start": 892,
            "end": 990,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_35@7",
            "content": "See Figure 2 (b); three phrase mentions (London, James Gunn and Apple) which belong to three different clusters are labeled by different topic categories.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_35",
            "start": 992,
            "end": 1145,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_36@0",
            "content": "Suppose we have a topic set C in our documents, with phrases and their pseudo labels, we construct positive pairs (x c i , x + c i ) by method introduced in Section 3.1 for topic c i where c i \u2208 C.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_36",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_37@0",
            "content": "To have contrastive instances, we randomly select phrases p m c j and instances x m c j from topic c j as negative instances x \u2212 c j in contrastive learning, where c j \u2208 C \u2227c j = c i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_37",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_37@1",
            "content": "As shown in Figure 2 (b), we construct positive pairs for phrase London, and use two phrases James Gunn and Apple from the other two clusters to randomly select negative instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_37",
            "start": 185,
            "end": 364,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_37@2",
            "content": "With pseudo labels, our method can avoid instances that have similar semantics as London.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_37",
            "start": 366,
            "end": 454,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_38@0",
            "content": "2 phrase mentions are extracted from sentence s, i.e., p m = s[l : r]",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_38",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_39@0",
            "content": "The training objective of finetuning is:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_39",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_40@0",
            "content": "l = \u2212 log e sim(hc i ,h + c i )/\u03c4 e sim(hc i ,h + c i )/\u03c4 + c j \u2208C e sim(hc i ,h \u2212 c j )/\u03c4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_40",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_41@0",
            "content": ".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_41",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_42@0",
            "content": "(3) As for the masking strategy in pre-training, we conduct masking for all training instances but keep x + c i and x \u2212 c j unchanged in probability p. To infer the topic y of phrase instance x, we compute the cosine similarity between phrase representation h and topic representations hc i , c i \u2208 C. The nearest neighbor topic of x is used as phrase topic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_42",
            "start": 0,
            "end": 357,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_42@1",
            "content": "Formally,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_42",
            "start": 359,
            "end": 367,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_43@0",
            "content": "y = argmax c i \u2208C (sim(h, hc i )) (4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_43",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_44@0",
            "content": "4 Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_44",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_45@0",
            "content": "In this section, we evaluate the effectiveness of UCTOPIC pre-training by contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_45",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_45@1",
            "content": "We start with entity clustering to compare the phrase representations from different methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_45",
            "start": 96,
            "end": 188,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_45@2",
            "content": "For topic modeling, we evaluate the topical phrases from three aspects and compare UCTOPIC to other topic modeling baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_45",
            "start": 190,
            "end": 314,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_46@0",
            "content": "Implementation Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_46",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_47@0",
            "content": "To generate the training corpus, we use English Wikipedia 3 and extract text with hyper links as phrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_47",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_47@1",
            "content": "Phrases have the same entity ids from Wikidata 4 or have the same mentions are considered as the same phrases (i.e., phrases have the same semantics).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_47",
            "start": 106,
            "end": 255,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_47@2",
            "content": "We enumerate all sentence pairs containing the same phrase as positive pairs in contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_47",
            "start": 257,
            "end": 357,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_47@3",
            "content": "After processing, the pre-training dataset has 11.6 million sentences and 108.8 million training instances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_47",
            "start": 359,
            "end": 465,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_48@0",
            "content": "For pre-training, we start from a pretrained LUKE-BASE model (Yamada et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_48",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_48@1",
            "content": "We follow previous works (Gao et al., 2021;Soares et al., 2019) and two losses are used concurrently: the masked language model loss and the contrastive learning loss with in-batch negatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_48",
            "start": 84,
            "end": 274,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_48@2",
            "content": "Our pretraining learning rate is 5e-5, batch size is 100 and our model is optimized by AdamW in 1 epoch.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_48",
            "start": 276,
            "end": 379,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_48@3",
            "content": "The probability p of keeping phrase mentions unchanged is 0.5 and the temperature \u03c4 in the contrastive loss is set to 0.05.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_48",
            "start": 381,
            "end": 503,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_49@0",
            "content": "Entity Clustering",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_49",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@0",
            "content": "To test the performance of phrase representations under objective tasks and metrics, we first apply UCTOPIC on entity clustering and compare to other representation learning methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@1",
            "content": "Datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 183,
            "end": 191,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@2",
            "content": "We conduct entity clustering on four datasets with annotated entities and their semantic categories are from general, review and biomedical domains: (1) CoNLL2003 (Sang and Meulder, 2003) consists of 20,744 sentences extracted from Reuters news articles.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 193,
            "end": 446,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@3",
            "content": "We use Person, Location, and Organization entities in our experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 448,
            "end": 517,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@4",
            "content": "5 (2) BC5CDR (Li et al., 2016) is the BioCreative V CDR task corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 519,
            "end": 586,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@5",
            "content": "It contains 18,307 sentences from PubMed articles, with 15,953 chemical and 13,318 disease entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 588,
            "end": 687,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@6",
            "content": "(3) MIT Movie (MIT-M) (Liu et al., 2013) contains 12,218 sentences with Title and Person entities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 689,
            "end": 786,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@7",
            "content": "(4) W-NUT 2017 (Derczynski et al., 2017) focuses on identifying unusual entities in the context of emerging discussions and contains 5,690 sentences and six kinds of entities 6 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 788,
            "end": 965,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@8",
            "content": "Finetuning Setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 967,
            "end": 983,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@9",
            "content": "The learning rate for finetuning is 1e-5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 985,
            "end": 1025,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@10",
            "content": "We select t (percent of instances) from {5, 10, 20, 50}.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 1027,
            "end": 1082,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@11",
            "content": "The probability p of keeping phrase mentions unchanged and temperature \u03c4 in contrastive loss are the same as in pre-training settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 1084,
            "end": 1217,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@12",
            "content": "We apply K-Means to get pseudo labels for all experiments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 1219,
            "end": 1276,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@13",
            "content": "Because UCTOPIC is an unsupervised method, we use all data to finetune and evaluate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 1278,
            "end": 1361,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@14",
            "content": "All results for finetuning are the best results during training process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 1363,
            "end": 1434,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@15",
            "content": "We follow previous clustering works (Xu et al., 2017;Zhang et al., 2021) and adopt Accuracy (ACC) and Normalized Mutual Information (NMI) to evaluate different approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 1436,
            "end": 1606,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@16",
            "content": "Compared Baseline Methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 1608,
            "end": 1633,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_50@17",
            "content": "To demonstrate the effectiveness of our pre-training method and finetuning with cluster-assisted contrastive learning (CCL), we compare baseline methods from two aspects:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_50",
            "start": 1635,
            "end": 1804,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_51@0",
            "content": "(1) Pre-trained token or phrase representations:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_51",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_52@0",
            "content": "\u2022 Glove (Pennington et al., 2014). Pre-trained word embeddings on 6B tokens and dimension is 300. We use averaging word embeddings as the representations of phrases. \u2022 BERT (Devlin et al., 2019). Obtains phrase representations by averaging token representations (BERT-Ave.) or following CGExpan (Zhang et al., 2020) to substitute phrases with the [MASK] token, and use [MASK] representations as phrase embeddings (BERT-MASK). \u2022 LUKE (Yamada et al., 2020). Use as backbone model to show the effectiveness of our contrastive learning for pre-training and finetuning. \u2022 DensePhrase (Lee et al., 2021). Pre-trained phrase representation learning in a supervised way for question answering problem. We use a pre-trained model released from the authors to get phrase representations. \u2022 Phrase-BERT (Wang et al., 2021). Contextagnostic phrase representations from pretraining. We use a pre-trained model from the authors and get representations by phrase mentions. \u2022 Ours w/o CCL. Pre-trained phrase representations of UCTOPIC without cluster-assisted contrastive finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_52",
            "start": 0,
            "end": 1067,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_53@0",
            "content": "(2) Fine-tuning methods based on pre-trained representations of UCTOPIC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_53",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_54@0",
            "content": "\u2022 Classifier. We use pseudo labels as supervision to train a MLP layer and obtain a classifier of phrase categories. \u2022 In-Batch Contrastive Learning. Same as contrastive learning for pre-training which uses in-batch negatives. \u2022 Autoencoder. Widely used in previous neural topic and aspect extraction models Iyyer et al., 2016;Tulkens and van Cranenburgh, 2020). We follow ABAE to implement our autoencoder model for phrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_54",
            "start": 0,
            "end": 424,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_55@0",
            "content": "Experimental Results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_55",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_55@1",
            "content": "We report evaluation results of entity clustering in When we compare different pre-trained representations, we find that our method (Ours w/o CCL) outperforms the other baselines on three datasets except MIT-M.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_55",
            "start": 22,
            "end": 231,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_55@2",
            "content": "There are two reasons: (1) All words in MIT-M dataset are lower case which is inconsistent with our pretraining dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_55",
            "start": 233,
            "end": 352,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_55@3",
            "content": "The inconsistency between training and test causes performance to decay.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_55",
            "start": 354,
            "end": 425,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_55@4",
            "content": "(2) Sentences from MIT-M are usually short (10.16 words in average) compared to other datasets (e.g., 17.9 words in W-NUT2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_55",
            "start": 427,
            "end": 553,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_55@5",
            "content": "Hence, UCTOPIC can obtain limited contextual information with short sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_55",
            "start": 555,
            "end": 632,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_55@6",
            "content": "However, the performance decay caused by the two reasons can be eliminated by our CCL finetuning on datasets since on MIT-M UCTOPIC achieves better results (0.661 NMI) than Phrase-BERT (0.575 NMI) after CCL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_55",
            "start": 634,
            "end": 840,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@0",
            "content": "On the other hand, compared to other finetuning methods, our CCL finetuning can further improve the pre-trained phrase representations by capturing data-specific features.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@1",
            "content": "The improvement is up to 50% NMI on the MIT-M dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 172,
            "end": 225,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@2",
            "content": "Ours w/ Class.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 227,
            "end": 240,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@3",
            "content": "performs worse than our pre-trained UCTOPIC in most cases which indicates that pseudo labels from clustering are noisy and cannot directly be used as supervision for representation learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 242,
            "end": 431,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@4",
            "content": "Ours w/ In-B. is similar as Ours w/ Class. which verifies our motivation on using CCL instead of in-batch negatives.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 433,
            "end": 548,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@5",
            "content": "An autoencoder can improve pre-trained representations on three datasets but the margins are limited and the performance even drops on W-NUT2017.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 550,
            "end": 694,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@6",
            "content": "Compared to other finetuning methods, our CCL finetuning consistently improves pre-trained phrase representations on different domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 696,
            "end": 830,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@7",
            "content": "Context or Mentions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 832,
            "end": 851,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@8",
            "content": "To investigate the source of UCTOPIC phrase semantics (i.e., phrase mentions or context), we conduct an ablation study on the type of input and compare UCTOPIC to LUKE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 853,
            "end": 1020,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@9",
            "content": "To eliminate the influence of repeated phrase mentions on clustering results, we use only one phrase instance (i.e., sentence and position of a phrase) for each phrase mention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 1022,
            "end": 1197,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_56@10",
            "content": "As shown in Table 2, there are three types of inputs: (1) Context+Mention: The same input as experiments in Table 1 including the whole sentence that contains the phrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_56",
            "start": 1199,
            "end": 1368,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_57@0",
            "content": "(2) Mention: Use only phrase mentions as inputs of the two models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_57",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_57@1",
            "content": "(3) Context: We mask the phrase mentions in sentences and models can only get information from the context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_57",
            "start": 67,
            "end": 173,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_57@2",
            "content": "We can see that UCTOPIC gets more information from context (0.43 ACC, 0.16 NMI) than mentions (0.32 ACC, 0.15 NMI).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_57",
            "start": 175,
            "end": 289,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_57@3",
            "content": "Compared to LUKE, UCTOPIC is more robust to phrase mentions (when predicting on only context, UCTOPIC \u22123% ACC and \u221244% NMI vs. LUKE \u221231% ACC and \u221267% NMI).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_57",
            "start": 291,
            "end": 445,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_58@0",
            "content": "Topical Phrase Mining",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_58",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_59@0",
            "content": "In this section, we apply UCTOPIC on topical phrase mining and conduct human evaluation to show our model outperforms previous topic model baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_59",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_59@1",
            "content": "Experiment Setup.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_59",
            "start": 150,
            "end": 166,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_59@2",
            "content": "To find topical phrases in documents, we first extract noun phrases by spaCy 7 noun chunks and remove single pronoun words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_59",
            "start": 168,
            "end": 290,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_59@3",
            "content": "Before CCL finetuning, we obtain the number of topics for each dataset by computing the Silhouette Coefficient (Rousseeuw, 1987).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_59",
            "start": 292,
            "end": 420,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_60@0",
            "content": "Specifically, we randomly sample 10K phrases from the dataset and apply K-Means clustering on pre-trained UCTOPIC phrase representations with different numbers of cluster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_60",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_60@1",
            "content": "We compute Silhouette Coefficient scores for different topic numbers; the number with the largest score will be used as the topic number in a dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_60",
            "start": 172,
            "end": 321,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_60@2",
            "content": "Then, we conduct CCL on the dataset with the same settings as described in Section 4.2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_60",
            "start": 323,
            "end": 409,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_60@3",
            "content": "Finally, after obtaining topic distribution z x \u2208 R |C| for a phrase instance x in a sentence, we get context-agnostic phrase topics by using averaged topic distribution z p m = 1 n 1\u2264i\u2264n z x m i , where phrase instances {x m i } in different sentences have the same phrase mention p m .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_60",
            "start": 411,
            "end": 697,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_60@4",
            "content": "The topic of a phrase mention has the highest probability in z p m .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_60",
            "start": 699,
            "end": 766,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_60@5",
            "content": "Dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_60",
            "start": 768,
            "end": 775,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_60@6",
            "content": "We conduct topical phrase mining on three datasets from news, review and computer science domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_60",
            "start": 777,
            "end": 874,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_61@0",
            "content": "\u2022 Gest. We collect restaurant reviews from Google Local 8 and use 100K reviews containing 143,969 sentences for topical phrase mining. \u2022 KP20k (Meng et al., 2017) is a collection of titles and abstracts from computer science papers. 500K sentences are used in our experiments. \u2022 KPTimes (Gallina et al., 2019) Table 3: The numbers of topics in three datasets. Times. We use 500K sentences for topical phrase mining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_61",
            "start": 0,
            "end": 414,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_62@0",
            "content": "The number of topics determined by Silhouette Coefficient is shown in Table 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_62",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_62@1",
            "content": "Compared Baseline Methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_62",
            "start": 79,
            "end": 104,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_62@2",
            "content": "We compare UCTOPIC against three topic baselines:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_62",
            "start": 106,
            "end": 154,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_63@0",
            "content": "\u2022 Phrase-LDA (Mimno, 2015). LDA model incorporates phrases by simply converting phrases into unigrams (e.g., \"city view\" to \"city_view\"). \u2022 TopMine (El- Kishky et al., 2014). A scalable pipeline that partitions a document into phrases, then uses phrases as constraints to ensure all words are placed under the same topic. \u2022 PNTM (Wang et al., 2021). A topic model with Phrase-BERT by using an autoencoder that reconstructs a document representation. The model is viewed as the state-of-the-art topic model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_63",
            "start": 0,
            "end": 505,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_64@0",
            "content": "We do not include topic models such as LDA (Blei et al., 2003), PD-LDA (Lindsey et al., 2012), TNG (Wang et al., 2007), KERT (Danilevsky et al., 2014) as baselines, because these models are compared in TopMine and PNTM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_64",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_64@1",
            "content": "For Phrase-LDA and PNTM, we use the same phrase list produced by UCTOPIC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_64",
            "start": 220,
            "end": 292,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_64@2",
            "content": "TopMine uses phrases produced by itself.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_64",
            "start": 294,
            "end": 333,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_64@3",
            "content": "Topical Phrase Evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_64",
            "start": 335,
            "end": 360,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_64@4",
            "content": "We evaluate the quality of topical phrases from three aspects: (1) topical separation;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_64",
            "start": 362,
            "end": 447,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_65@0",
            "content": "(2) phrase coherence;",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_65",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_66@0",
            "content": "(3) phrase informativeness and diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_66",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_67@0",
            "content": "To evaluate topical separation, we perform the phrase intrusion task following previous work (El-Kishky et al., 2014;Chang et al., 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_67",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_67@1",
            "content": "The phrase intrusion task involves a set of questions asking humans to discover the 'intruder' phrase from other phrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_67",
            "start": 138,
            "end": 258,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_67@2",
            "content": "In our experiments, each question has 6 phrases and 5 of them are randomly sampled from the top 50 phrases of one topic and the remaining phrase is randomly chosen from another topic (top 50 phrases).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_67",
            "start": 260,
            "end": 459,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_67@3",
            "content": "Annotators are asked to select the intruder phrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_67",
            "start": 461,
            "end": 511,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_67@4",
            "content": "We sample 50 questions for each method and each dataset (600 questions in total) and shuffle all questions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_67",
            "start": 513,
            "end": 619,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_67@5",
            "content": "Because these questions are sampled independently, we asked 4 annotators to answer these questions and each annotator answers 150 questions on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_67",
            "start": 621,
            "end": 771,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_67@6",
            "content": "Results of the task evaluate how well the phrases are separated by topics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_67",
            "start": 773,
            "end": 846,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_67@7",
            "content": "The evaluation results are shown in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_67",
            "start": 848,
            "end": 892,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_67@8",
            "content": "UCTOPIC outperforms other baselines on three datasets, which means our model can find well-separated topics in documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_67",
            "start": 894,
            "end": 1014,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@0",
            "content": "To evaluate phrase coherence in one topic, we follow ABAE and ask annotators to evaluate if the top 50 phrases from one topic are coherent (i.e., most phrases represent the same topic).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@1",
            "content": "3 annotators evaluate four models on Gest and KP20k datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 186,
            "end": 246,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@2",
            "content": "Numbers of coherent topics are shown in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 248,
            "end": 295,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@3",
            "content": "We can see that UCTOPIC, PNTM and TopMine can recognize similar numbers of coherent topics, but the numbers of Phrase-LDA are less than the other three models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 297,
            "end": 455,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@4",
            "content": "For a coherent topic, each of the top phrases will be labeled as correct if the phrase reflects the related topic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 457,
            "end": 570,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@5",
            "content": "Same as ABAE, we adopt precision@n to evaluate the results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 572,
            "end": 630,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@6",
            "content": "Figure 4 shows the results; we can see that UCTOPIC substantially outperforms other models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 632,
            "end": 722,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@7",
            "content": "UCTOPIC can maintain high precision with a large n when the precision of other models decreases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 724,
            "end": 819,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@8",
            "content": "Finally, to evaluate phrase informativeness and diversity, we use tf-idf and word diversity (worddiv.) to evaluate the top topical phrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 821,
            "end": 959,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@9",
            "content": "Basically, informative phrases cannot be very common phrases in a corpus (e.g., \"good food\" in Gest) and we use tf-idf to evaluate the \"importance\" of a phrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 961,
            "end": 1120,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@10",
            "content": "To eliminate the influence of phrase length, we use averaged word tf-idf in a phrase as the phrase tf-idf.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 1122,
            "end": 1227,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@11",
            "content": "Specifically, tf-idf(p, d) = 1 m 1\u2264i\u2264m tf-idf(w p i ), where d denotes the document and p is the phrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 1229,
            "end": 1332,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_68@12",
            "content": "In our experiments, a document is a sentence in a review.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_68",
            "start": 1334,
            "end": 1390,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_69@0",
            "content": "In addition, we hope that our phrases are diverse enough in a topic instead of expressing the same meaning (e.g., \"good food\" and \"great food\").",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_69",
            "start": 0,
            "end": 143,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_69@1",
            "content": "To evaluate the diversity of the top phrases, we calculate the ratio of distinct words among all words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_69",
            "start": 145,
            "end": 247,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_69@2",
            "content": "Formally, given a list of phrases [p 1 , p 2 , . . . , p n ], we tokenize the phrases into a word list w = [w p 1 1 , w p 1 2 , . . . , w pn m ]; w is the set of unique words in w.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_69",
            "start": 249,
            "end": 428,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_69@3",
            "content": "The word diversity is computed by |w | |w| .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_69",
            "start": 430,
            "end": 473,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_69@4",
            "content": "We only evaluate coherent topics labeled in phrase coherence; the coherent topic numbers of Phrase-LDA are smaller than others, hence we evaluate the other three models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_69",
            "start": 475,
            "end": 643,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_70@0",
            "content": "We compute the tf-idf and word-div.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_70",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_70@1",
            "content": "on the top 10 phrases and use the averaged value on topics as final scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_70",
            "start": 36,
            "end": 110,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_70@2",
            "content": "Results are shown in table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_70",
            "start": 112,
            "end": 140,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_70@3",
            "content": "PNTM and UCTOPIC achieve similar tf-idf scores, because the two methods use the same phrase lists extracted from spaCy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_70",
            "start": 142,
            "end": 260,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_70@4",
            "content": "UCTOPIC extracts the most diverse phrases in a topic, because our phrase representations are more context-aware.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_70",
            "start": 262,
            "end": 373,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_70@5",
            "content": "In contrast, since PNTM gets representations dependent on phrase mentions, the phrases from PNTM contain From examples, we can see the phrases are consistent with our user study and diversity evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_70",
            "start": 375,
            "end": 577,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_70@6",
            "content": "Although the phrases from PNTM are coherent, the diversity of phrases is less than others (e.g., \"drinks\", \"bar drink\", \"just drink\" from Gest) because context-agnostic representations let similar phrase mentions group together.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_70",
            "start": 579,
            "end": 806,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_70@7",
            "content": "The phrases from TopMine are diverse but are not coherent in some cases (e.g., \"machine learning\" and \"support vector machine\" in the programming topic).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_70",
            "start": 808,
            "end": 960,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_70@8",
            "content": "In contrast, UCTOPIC can extract coherent and diverse topical phrases from documents.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_70",
            "start": 962,
            "end": 1046,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_71@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_71",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_72@0",
            "content": "Many attempts have been made to extract topical phrases via LDA (Blei et al., 2003).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_72",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_72@1",
            "content": "Wallach (2006) incorporated a bigram language model into LDA by a hierarchical dirichlet generative probabilistic model to share the topic across each word within a bigram.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_72",
            "start": 85,
            "end": 256,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_72@2",
            "content": "TNG (Wang et al., 2007) applied additional latent variables and word-specific multinomials to model bi-grams and combined bi-grams to form n-gram phrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_72",
            "start": 258,
            "end": 411,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_72@3",
            "content": "PD-LDA (Lindsey et al., 2012) used a hierarchical Pitman-Yor process to share the same topic among all words in a given n-gram.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_72",
            "start": 413,
            "end": 539,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_72@4",
            "content": "Danilevsky et al. (2014) ranked the resultant phrases based on four heuristic metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_72",
            "start": 541,
            "end": 626,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_72@5",
            "content": "TOP-Mine (El-Kishky et al., 2014) proposed to restrict all constituent terms within a phrase to share the same latent topic and assign a phrase to the topic of its constituent words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_72",
            "start": 628,
            "end": 809,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_72@6",
            "content": "Compared to previous topic mining methods, UCTOPIC builds on the success of pre-trained language models and unsupervised contrastive learning on a large-scale dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_72",
            "start": 811,
            "end": 977,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_72@7",
            "content": "Therefore, UCTOPIC provides high-quality pre-trained phrase representations and state-of-the-art finetuning for topic mining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_72",
            "start": 979,
            "end": 1103,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_73@0",
            "content": "Early works in phrase representation build upon a composition function that combines component word embeddings together into simple phrase embedding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_73",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_73@1",
            "content": "Yu and Dredze (2015) implemented the function by rule-based composition over word vectors.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_73",
            "start": 150,
            "end": 239,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_73@2",
            "content": "Zhou et al. (2017) applied a pair-wise GRU model and datasets such as PPDB (Pavlick et al., 2015) to learn phrase representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_73",
            "start": 241,
            "end": 370,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_73@3",
            "content": "Phrase-BERT (Wang et al., 2021) composed token embeddings from BERT and pretrained on positive instances produced by GPT-2-based diverse paraphrasing model (Krishna et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_73",
            "start": 372,
            "end": 550,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_73@4",
            "content": "Lee et al. (2021) learned phrase representations from the supervision of reading comprehension tasks and applied representations on open-domain QA.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_73",
            "start": 552,
            "end": 698,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_73@5",
            "content": "Other works learned phrase embeddings for specific tasks such as semantic parsing (Socher et al., 2011) and machine translation (Bing et al., 2015).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_73",
            "start": 700,
            "end": 847,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_73@6",
            "content": "In this paper, we present unsupervised contrastive learning method for pre-training phrase representations of general purposes and for finetuning to topicspecific phrase representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_73",
            "start": 849,
            "end": 1034,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_74@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_74",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_75@0",
            "content": "In this paper, we propose UCTOPIC, a contrastive learning framework that can effectively learn phrase representations without supervision.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_75",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_75@1",
            "content": "To finetune on topic mining datasets, we propose clusterassisted contrastive learning which reduces noise by selecting negatives from clusters.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_75",
            "start": 139,
            "end": 281,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_75@2",
            "content": "During finetuning, our phrase representations are optimized for topics in the document hence the representations are further improved.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_75",
            "start": 283,
            "end": 416,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_75@3",
            "content": "We conduct comprehensive experiments on entity clustering and topical phrase mining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_75",
            "start": 418,
            "end": 501,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_75@4",
            "content": "Results show that UCTOPIC largely improves phrase representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_75",
            "start": 503,
            "end": 568,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_75@5",
            "content": "Objective metrics and a user study indicate UCTOPIC can extract coherent and diverse topical phrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_75",
            "start": 570,
            "end": 670,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_76@0",
            "content": "Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo, Rebecca Passonneau, Abstractive multidocument summarization via phrase selection and merging, 2015, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_76",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_77@0",
            "content": "David Blei, A Ng, Michael Jordan, Latent dirichlet allocation, 2003, J. Mach. Learn. Res, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_77",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_78@0",
            "content": "Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish, Chong Wang, David Blei, Reading tea leaves: How humans interpret topic models, 2009, NIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_78",
            "start": 0,
            "end": 141,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_79@0",
            "content": "UNKNOWN, None, 2002, A simple framework for contrastive learning of visual representations. ArXiv, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_79",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_80@0",
            "content": "Ting Chen, Yizhou Sun, Yue Shi, Liangjie Hong, On sampling strategies for neural networkbased collaborative filtering, 2017, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_80",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_81@0",
            "content": "Marina Danilevsky, Chi Wang, Nihit Desai, Xiang Ren, Jingyi Guo, Jiawei Han, Automatic construction and ranking of topical keyphrases on collections of short documents, 2014, SDM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_81",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_82@0",
            "content": "Leon Derczynski, Eric Nichols, Marieke Van Erp, Nut Limsopatham, Results of the WNUT2017 shared task on novel and emerging entity recognition, 2017, Proceedings of the 3rd Workshop on Noisy User-generated Text, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_82",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_83@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_83",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_84@0",
            "content": "Ahmed El-Kishky, Yanglei Song, Chi Wang, Clare Voss, Jiawei Han, Scalable topical phrase mining from text corpora, 2014, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_84",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_85@0",
            "content": "Ygor Gallina, Florian Boudin, B\u00e9atrice Daille, Kptimes: A large-scale dataset for keyphrase generation on news documents, 2019, INLG, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_85",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_86@0",
            "content": "Tianyu Gao, Xingcheng Yao, Danqi Chen, Simcse: Simple contrastive learning of sentence embeddings, 2021, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_86",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_87@0",
            "content": "Raia Hadsell, Sumit Chopra, Yann Lecun, Dimensionality reduction by learning an invariant mapping, 2006, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_87",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_88@0",
            "content": "Ruidan He, Hwee Tou Wee Sun Lee, Daniel Ng,  Dahlmeier, An unsupervised neural attention model for aspect extraction, 2017, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_88",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_89@0",
            "content": "Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-Hsuan Sung, L\u00e1szl\u00f3 Luk\u00e1cs, Ruiqi Guo, Efficient natural language response suggestion for smart reply, 2017, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_89",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_90@0",
            "content": "Mohit Iyyer, Anupam Guha, Snigdha Chaturvedi, Jordan Boyd-Graber, Hal Daum\u00e9, Feuding families and former friends: Unsupervised learning for dynamic fictional relationships, 2016, NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_90",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_91@0",
            "content": "Kalpesh Krishna, John Wieting, Mohit Iyyer, Reformulating unsupervised style transfer as paraphrase generation, 2020, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_91",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_92@0",
            "content": "Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, Danqi Chen, Learning dense representations of phrases at scale, 2021, ACL/IJCNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_92",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_93@0",
            "content": "J Li, Yueping Sun, Robin Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, A Davis, C Mattingly, Thomas Wiegers, Zhiyong Lu, Biocreative v cdr task corpus: a resource for chemical disease relation extraction, 2016, Database: The Journal of Biological Databases and Curation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_93",
            "start": 0,
            "end": 281,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_94@0",
            "content": "Robert Lindsey, Will Headden, Michael Stipicevic, A phrase-discovering topic model using hierarchical pitman-yor processes, 2012, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_94",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_95@0",
            "content": "Jingjing Liu, Panupong Pasupat, Yining Wang, D Scott Cyphers, James Glass, Query understanding enhanced by hierarchical parsing structures, 2013, IEEE Workshop on Automatic Speech Recognition and Understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_95",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_96@0",
            "content": "Rui Meng, Sanqiang Zhao, Shuguang Han, Daqing He, Peter Brusilovsky, Yu Chi, Deep keyphrase generation, 2017, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_96",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_97@0",
            "content": "Yu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Tiwary, Paul Bennett, Jiawei Han, Xia Song, Coco-lm: Correcting and contrasting text sequences for language model pretraining, 2021, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_97",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_98@0",
            "content": "UNKNOWN, None, 2015, Using phrases in mallet topic models, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_98",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_99@0",
            "content": "Ellie Pavlick, Pushpendre Rastogi, Juri Ganitkevitch, Benjamin Van Durme, Chris Callison-Burch, Ppdb 2.0: Better paraphrase ranking, finegrained entailment relations, word embeddings, and style classification, 2015, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_99",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_100@0",
            "content": "Jeffrey Pennington, Richard Socher, Christopher Manning, Glove: Global vectors for word representation, 2014, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_100",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_101@0",
            "content": "Peter Rousseeuw, Silhouettes: a graphical aid to the interpretation and validation of cluster analysis, 1987, Journal of Computational and Applied Mathematics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_101",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_102@0",
            "content": "E Sang, F Meulder, Introduction to the conll-2003 shared task: Language-independent named entity recognition, 2003, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_102",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_103@0",
            "content": "UNKNOWN, None, 1906, Matching the blanks: Distributional similarity for relation learning. ArXiv, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_103",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_104@0",
            "content": "UNKNOWN, None, 2011, Parsing natural scenes and natural language with recursive neural networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_104",
            "start": 0,
            "end": 97,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_105@0",
            "content": "Kihyuk Sohn, Improved deep metric learning with multi-class n-pair loss objective, 2016, NIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_105",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_106@0",
            "content": "St\u00e9phan Tulkens, Andreas Van Cranenburgh, Embarrassingly simple unsupervised aspect extraction, 2020, ACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_106",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_107@0",
            "content": "M Hanna,  Wallach, Topic modeling: beyond bag-of-words, 2006, Proceedings of the 23rd international conference on Machine learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_107",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_108@0",
            "content": "Shufan Wang, Laure Thompson, Mohit Iyyer, Phrase-bert: Improved phrase embeddings from bert with an application to corpus exploration, 2021, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_108",
            "start": 0,
            "end": 148,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_109@0",
            "content": "Xuerui Wang, Andrew Mccallum, Xing Wei, Topical n-grams: Phrase and topic discovery, with an application to information retrieval, 2007, Seventh IEEE International Conference on Data Mining (ICDM 2007), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_109",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_110@0",
            "content": "Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, Hao Ma, Clear: Contrastive learning for sentence representation, 2012, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_110",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_111@0",
            "content": "UNKNOWN, None, 2020, Unsupervised data augmentation for consistency training, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_111",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_112@0",
            "content": "Jiaming Xu, Bo Xu, Peng Wang, Suncong Zheng, Guanhua Tian, Self-taught convolutional neural networks for short text clustering, 2017, Neural networks : the official journal of the International Neural Network Society, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_112",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_113@0",
            "content": "Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, Yuji Matsumoto, Luke: Deep contextualized entity representations with entityaware self-attention, 2020, EMNLP, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_113",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_114@0",
            "content": "Mo Yu, Mark Dredze, Learning composition models for phrase embeddings, 2015, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_114",
            "start": 0,
            "end": 140,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_115@0",
            "content": "Dejiao Zhang, Feng Nan, Xiaokai Wei, Shang-Wen Li, Henghui Zhu, Kathleen Mckeown, Ramesh Nallapati, Andrew O. Arnold, and Bing Xiang. 2021. Supporting clustering with contrastive learning, , NAACL, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_115",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_116@0",
            "content": "Hongyi Zhang, Moustapha Ciss\u00e9, Yann Dauphin, David Lopez-Paz, None, 2018, mixup: Beyond empirical risk minimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_116",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_117@0",
            "content": "Xiang Zhang, Junbo Zhao, Yann Andr\u00e9 Le-Cun, Character-level convolutional networks for text classification, 2015, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_117",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_118@0",
            "content": "UNKNOWN, None, 2004, Empower entity set expansion via language model probing. ArXiv, abs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_118",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "56-ARR_v2_119@0",
            "content": "Zhihao Zhou, Lifu Huang, Heng Ji, Learning phrase embeddings from paraphrases with grus, 2017, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "56-ARR_v2_119",
            "start": 0,
            "end": 102,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_1",
            "tgt_ix": "56-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_1",
            "tgt_ix": "56-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_2",
            "tgt_ix": "56-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_4",
            "tgt_ix": "56-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_5",
            "tgt_ix": "56-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_6",
            "tgt_ix": "56-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_7",
            "tgt_ix": "56-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_3",
            "tgt_ix": "56-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_3",
            "tgt_ix": "56-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_3",
            "tgt_ix": "56-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_3",
            "tgt_ix": "56-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_3",
            "tgt_ix": "56-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_3",
            "tgt_ix": "56-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_8",
            "tgt_ix": "56-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_9",
            "tgt_ix": "56-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_9",
            "tgt_ix": "56-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_9",
            "tgt_ix": "56-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_10",
            "tgt_ix": "56-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_12",
            "tgt_ix": "56-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_13",
            "tgt_ix": "56-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_14",
            "tgt_ix": "56-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_16",
            "tgt_ix": "56-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_15",
            "tgt_ix": "56-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_17",
            "tgt_ix": "56-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_19",
            "tgt_ix": "56-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_18",
            "tgt_ix": "56-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_9",
            "tgt_ix": "56-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_20",
            "tgt_ix": "56-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_21",
            "tgt_ix": "56-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_21",
            "tgt_ix": "56-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_22",
            "tgt_ix": "56-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_24",
            "tgt_ix": "56-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_25",
            "tgt_ix": "56-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_23",
            "tgt_ix": "56-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_23",
            "tgt_ix": "56-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_23",
            "tgt_ix": "56-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_23",
            "tgt_ix": "56-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_23",
            "tgt_ix": "56-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_26",
            "tgt_ix": "56-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_28",
            "tgt_ix": "56-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_29",
            "tgt_ix": "56-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_30",
            "tgt_ix": "56-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_31",
            "tgt_ix": "56-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_27",
            "tgt_ix": "56-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_27",
            "tgt_ix": "56-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_27",
            "tgt_ix": "56-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_27",
            "tgt_ix": "56-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_27",
            "tgt_ix": "56-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_27",
            "tgt_ix": "56-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_23",
            "tgt_ix": "56-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_32",
            "tgt_ix": "56-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_34",
            "tgt_ix": "56-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_35",
            "tgt_ix": "56-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_36",
            "tgt_ix": "56-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_37",
            "tgt_ix": "56-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_38",
            "tgt_ix": "56-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_39",
            "tgt_ix": "56-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_40",
            "tgt_ix": "56-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_41",
            "tgt_ix": "56-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_42",
            "tgt_ix": "56-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_43",
            "tgt_ix": "56-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_44",
            "tgt_ix": "56-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_45",
            "tgt_ix": "56-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_47",
            "tgt_ix": "56-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_46",
            "tgt_ix": "56-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_46",
            "tgt_ix": "56-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_46",
            "tgt_ix": "56-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_48",
            "tgt_ix": "56-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_51",
            "tgt_ix": "56-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_53",
            "tgt_ix": "56-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_55",
            "tgt_ix": "56-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_49",
            "tgt_ix": "56-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_49",
            "tgt_ix": "56-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_49",
            "tgt_ix": "56-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_49",
            "tgt_ix": "56-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_49",
            "tgt_ix": "56-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_49",
            "tgt_ix": "56-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_49",
            "tgt_ix": "56-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_49",
            "tgt_ix": "56-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_49",
            "tgt_ix": "56-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_57",
            "tgt_ix": "56-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_59",
            "tgt_ix": "56-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_60",
            "tgt_ix": "56-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_62",
            "tgt_ix": "56-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_64",
            "tgt_ix": "56-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_65",
            "tgt_ix": "56-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_66",
            "tgt_ix": "56-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_67",
            "tgt_ix": "56-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_69",
            "tgt_ix": "56-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_70",
            "tgt_ix": "56-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_72",
            "tgt_ix": "56-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_71",
            "tgt_ix": "56-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_71",
            "tgt_ix": "56-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_71",
            "tgt_ix": "56-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_73",
            "tgt_ix": "56-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_74",
            "tgt_ix": "56-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_74",
            "tgt_ix": "56-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "56-ARR_v2_0",
            "tgt_ix": "56-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_1",
            "tgt_ix": "56-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_2",
            "tgt_ix": "56-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_2",
            "tgt_ix": "56-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_2",
            "tgt_ix": "56-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_2",
            "tgt_ix": "56-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_2",
            "tgt_ix": "56-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_2",
            "tgt_ix": "56-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_2",
            "tgt_ix": "56-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_2",
            "tgt_ix": "56-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_2",
            "tgt_ix": "56-ARR_v2_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_3",
            "tgt_ix": "56-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_4",
            "tgt_ix": "56-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_4",
            "tgt_ix": "56-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_4",
            "tgt_ix": "56-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_4",
            "tgt_ix": "56-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_4",
            "tgt_ix": "56-ARR_v2_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_4",
            "tgt_ix": "56-ARR_v2_4@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_4",
            "tgt_ix": "56-ARR_v2_4@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_4",
            "tgt_ix": "56-ARR_v2_4@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_5",
            "tgt_ix": "56-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_5",
            "tgt_ix": "56-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_5",
            "tgt_ix": "56-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_5",
            "tgt_ix": "56-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_5",
            "tgt_ix": "56-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_6",
            "tgt_ix": "56-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_6",
            "tgt_ix": "56-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_6",
            "tgt_ix": "56-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_6",
            "tgt_ix": "56-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_6",
            "tgt_ix": "56-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_6",
            "tgt_ix": "56-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_6",
            "tgt_ix": "56-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_6",
            "tgt_ix": "56-ARR_v2_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_7",
            "tgt_ix": "56-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_7",
            "tgt_ix": "56-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_7",
            "tgt_ix": "56-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_7",
            "tgt_ix": "56-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_7",
            "tgt_ix": "56-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_7",
            "tgt_ix": "56-ARR_v2_7@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_8",
            "tgt_ix": "56-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_8",
            "tgt_ix": "56-ARR_v2_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_8",
            "tgt_ix": "56-ARR_v2_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_8",
            "tgt_ix": "56-ARR_v2_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_8",
            "tgt_ix": "56-ARR_v2_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_8",
            "tgt_ix": "56-ARR_v2_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_9",
            "tgt_ix": "56-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_10",
            "tgt_ix": "56-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_11",
            "tgt_ix": "56-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_12",
            "tgt_ix": "56-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_12",
            "tgt_ix": "56-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_13",
            "tgt_ix": "56-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_14",
            "tgt_ix": "56-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_15",
            "tgt_ix": "56-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_16",
            "tgt_ix": "56-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_17",
            "tgt_ix": "56-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_18",
            "tgt_ix": "56-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_19",
            "tgt_ix": "56-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_20",
            "tgt_ix": "56-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_21",
            "tgt_ix": "56-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_22",
            "tgt_ix": "56-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_23",
            "tgt_ix": "56-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_24",
            "tgt_ix": "56-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_24",
            "tgt_ix": "56-ARR_v2_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_24",
            "tgt_ix": "56-ARR_v2_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_25",
            "tgt_ix": "56-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_26",
            "tgt_ix": "56-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_27",
            "tgt_ix": "56-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_28",
            "tgt_ix": "56-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_28",
            "tgt_ix": "56-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_28",
            "tgt_ix": "56-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_28",
            "tgt_ix": "56-ARR_v2_28@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_29",
            "tgt_ix": "56-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_29",
            "tgt_ix": "56-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_29",
            "tgt_ix": "56-ARR_v2_29@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_29",
            "tgt_ix": "56-ARR_v2_29@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_29",
            "tgt_ix": "56-ARR_v2_29@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_30",
            "tgt_ix": "56-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_30",
            "tgt_ix": "56-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_31",
            "tgt_ix": "56-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_32",
            "tgt_ix": "56-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_33",
            "tgt_ix": "56-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_34",
            "tgt_ix": "56-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_34",
            "tgt_ix": "56-ARR_v2_34@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_34",
            "tgt_ix": "56-ARR_v2_34@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_34",
            "tgt_ix": "56-ARR_v2_34@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_34",
            "tgt_ix": "56-ARR_v2_34@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_35",
            "tgt_ix": "56-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_35",
            "tgt_ix": "56-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_35",
            "tgt_ix": "56-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_35",
            "tgt_ix": "56-ARR_v2_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_35",
            "tgt_ix": "56-ARR_v2_35@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_35",
            "tgt_ix": "56-ARR_v2_35@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_35",
            "tgt_ix": "56-ARR_v2_35@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_35",
            "tgt_ix": "56-ARR_v2_35@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_36",
            "tgt_ix": "56-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_37",
            "tgt_ix": "56-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_37",
            "tgt_ix": "56-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_37",
            "tgt_ix": "56-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_38",
            "tgt_ix": "56-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_39",
            "tgt_ix": "56-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_40",
            "tgt_ix": "56-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_41",
            "tgt_ix": "56-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_42",
            "tgt_ix": "56-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_42",
            "tgt_ix": "56-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_43",
            "tgt_ix": "56-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_44",
            "tgt_ix": "56-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_45",
            "tgt_ix": "56-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_45",
            "tgt_ix": "56-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_45",
            "tgt_ix": "56-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_46",
            "tgt_ix": "56-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_47",
            "tgt_ix": "56-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_47",
            "tgt_ix": "56-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_47",
            "tgt_ix": "56-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_47",
            "tgt_ix": "56-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_48",
            "tgt_ix": "56-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_48",
            "tgt_ix": "56-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_48",
            "tgt_ix": "56-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_48",
            "tgt_ix": "56-ARR_v2_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_49",
            "tgt_ix": "56-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@13",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@14",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@15",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@16",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_50",
            "tgt_ix": "56-ARR_v2_50@17",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_51",
            "tgt_ix": "56-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_52",
            "tgt_ix": "56-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_53",
            "tgt_ix": "56-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_54",
            "tgt_ix": "56-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_55",
            "tgt_ix": "56-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_55",
            "tgt_ix": "56-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_55",
            "tgt_ix": "56-ARR_v2_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_55",
            "tgt_ix": "56-ARR_v2_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_55",
            "tgt_ix": "56-ARR_v2_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_55",
            "tgt_ix": "56-ARR_v2_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_55",
            "tgt_ix": "56-ARR_v2_55@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_56",
            "tgt_ix": "56-ARR_v2_56@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_57",
            "tgt_ix": "56-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_57",
            "tgt_ix": "56-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_57",
            "tgt_ix": "56-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_57",
            "tgt_ix": "56-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_58",
            "tgt_ix": "56-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_59",
            "tgt_ix": "56-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_59",
            "tgt_ix": "56-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_59",
            "tgt_ix": "56-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_59",
            "tgt_ix": "56-ARR_v2_59@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_60",
            "tgt_ix": "56-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_60",
            "tgt_ix": "56-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_60",
            "tgt_ix": "56-ARR_v2_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_60",
            "tgt_ix": "56-ARR_v2_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_60",
            "tgt_ix": "56-ARR_v2_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_60",
            "tgt_ix": "56-ARR_v2_60@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_60",
            "tgt_ix": "56-ARR_v2_60@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_61",
            "tgt_ix": "56-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_62",
            "tgt_ix": "56-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_62",
            "tgt_ix": "56-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_62",
            "tgt_ix": "56-ARR_v2_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_63",
            "tgt_ix": "56-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_64",
            "tgt_ix": "56-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_64",
            "tgt_ix": "56-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_64",
            "tgt_ix": "56-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_64",
            "tgt_ix": "56-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_64",
            "tgt_ix": "56-ARR_v2_64@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_65",
            "tgt_ix": "56-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_66",
            "tgt_ix": "56-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_67",
            "tgt_ix": "56-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_67",
            "tgt_ix": "56-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_67",
            "tgt_ix": "56-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_67",
            "tgt_ix": "56-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_67",
            "tgt_ix": "56-ARR_v2_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_67",
            "tgt_ix": "56-ARR_v2_67@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_67",
            "tgt_ix": "56-ARR_v2_67@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_67",
            "tgt_ix": "56-ARR_v2_67@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_67",
            "tgt_ix": "56-ARR_v2_67@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_68",
            "tgt_ix": "56-ARR_v2_68@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_69",
            "tgt_ix": "56-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_69",
            "tgt_ix": "56-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_69",
            "tgt_ix": "56-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_69",
            "tgt_ix": "56-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_69",
            "tgt_ix": "56-ARR_v2_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_70",
            "tgt_ix": "56-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_70",
            "tgt_ix": "56-ARR_v2_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_70",
            "tgt_ix": "56-ARR_v2_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_70",
            "tgt_ix": "56-ARR_v2_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_70",
            "tgt_ix": "56-ARR_v2_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_70",
            "tgt_ix": "56-ARR_v2_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_70",
            "tgt_ix": "56-ARR_v2_70@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_70",
            "tgt_ix": "56-ARR_v2_70@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_70",
            "tgt_ix": "56-ARR_v2_70@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_71",
            "tgt_ix": "56-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_72",
            "tgt_ix": "56-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_72",
            "tgt_ix": "56-ARR_v2_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_72",
            "tgt_ix": "56-ARR_v2_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_72",
            "tgt_ix": "56-ARR_v2_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_72",
            "tgt_ix": "56-ARR_v2_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_72",
            "tgt_ix": "56-ARR_v2_72@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_72",
            "tgt_ix": "56-ARR_v2_72@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_72",
            "tgt_ix": "56-ARR_v2_72@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_73",
            "tgt_ix": "56-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_73",
            "tgt_ix": "56-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_73",
            "tgt_ix": "56-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_73",
            "tgt_ix": "56-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_73",
            "tgt_ix": "56-ARR_v2_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_73",
            "tgt_ix": "56-ARR_v2_73@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_73",
            "tgt_ix": "56-ARR_v2_73@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_74",
            "tgt_ix": "56-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_75",
            "tgt_ix": "56-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_75",
            "tgt_ix": "56-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_75",
            "tgt_ix": "56-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_75",
            "tgt_ix": "56-ARR_v2_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_75",
            "tgt_ix": "56-ARR_v2_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_75",
            "tgt_ix": "56-ARR_v2_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_76",
            "tgt_ix": "56-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_77",
            "tgt_ix": "56-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_78",
            "tgt_ix": "56-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_79",
            "tgt_ix": "56-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_80",
            "tgt_ix": "56-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_81",
            "tgt_ix": "56-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_82",
            "tgt_ix": "56-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_83",
            "tgt_ix": "56-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_84",
            "tgt_ix": "56-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_85",
            "tgt_ix": "56-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_86",
            "tgt_ix": "56-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_87",
            "tgt_ix": "56-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_88",
            "tgt_ix": "56-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_89",
            "tgt_ix": "56-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_90",
            "tgt_ix": "56-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_91",
            "tgt_ix": "56-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_92",
            "tgt_ix": "56-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_93",
            "tgt_ix": "56-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_94",
            "tgt_ix": "56-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_95",
            "tgt_ix": "56-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_96",
            "tgt_ix": "56-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_97",
            "tgt_ix": "56-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_98",
            "tgt_ix": "56-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_99",
            "tgt_ix": "56-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_100",
            "tgt_ix": "56-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_101",
            "tgt_ix": "56-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_102",
            "tgt_ix": "56-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_103",
            "tgt_ix": "56-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_104",
            "tgt_ix": "56-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_105",
            "tgt_ix": "56-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_106",
            "tgt_ix": "56-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_107",
            "tgt_ix": "56-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_108",
            "tgt_ix": "56-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_109",
            "tgt_ix": "56-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_110",
            "tgt_ix": "56-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_111",
            "tgt_ix": "56-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_112",
            "tgt_ix": "56-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_113",
            "tgt_ix": "56-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_114",
            "tgt_ix": "56-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_115",
            "tgt_ix": "56-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_116",
            "tgt_ix": "56-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_117",
            "tgt_ix": "56-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_118",
            "tgt_ix": "56-ARR_v2_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "56-ARR_v2_119",
            "tgt_ix": "56-ARR_v2_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 928,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "56-ARR",
        "version": 2
    }
}