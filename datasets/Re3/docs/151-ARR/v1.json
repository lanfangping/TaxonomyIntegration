{
    "nodes": [
        {
            "ix": "151-ARR_v1_0",
            "content": "UniTE: Unified Translation Evaluation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_2",
            "content": "Translation quality evaluation plays a crucial role in machine translation, and is mainly separated into three tasks according to different input formats, i.e., reference-only, sourceonly and source-reference-combined. Recent methods, despite their promising results, are specifically designed and optimized on one of these three tasks. This limits the convenience of these methods and overlooks commonalities among tasks. In this paper, we propose UniTE, which is the first unified framework engaged with abilities to handle all three evaluation tasks. Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task training. We empirically testify our framework on WMT 2019 Metrics and WMT 20 Quality Estimation benchmarks. Extensive analyses show that our single model can universally surpass various state-of-the-art or winner methods across tasks. Both source code and associated models will be released upon the acceptance of this paper.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "151-ARR_v1_4",
            "content": "Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020;Mathur et al., 2020;Zhao et al., 2020). According to the input contexts, translation evaluation can be mainly categorized into three classes: 1) reference-only evaluation ( REF ) approaches like BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi-2 (Lo, 2019) and TransQuest (Ranasinghe et al., 2020b), which are also referred as quality estimation (QE). These methods estimate the quality of the hypothesis based on the source sentence without using references; 3) source-reference-combined evaluation ( SRC+REF ) works like COMET (Rei et al., 2020), where the model exploits information from both source and reference for the evaluation. With the help of powerful pretrained language models (PLMs, Devlin et al., 2019;Conneau et al., 2020), model-based approaches (e.g., BLEURT, TransQuest, and COMET) have shown promising results in recent WMT competitions (Ma et al., 2019;Barrault et al., 2020;Kocmi et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_5",
            "content": "Nevertheless, each existing MT evaluation work is usually designed for one specific task, e.g., BLEURT is only used for REF task and can not support SRC and SRC+REF tasks. Moreover, those approaches preserve the same core -evaluating the quality of translation by referring to the given segments. Therefore, we believe that it is valuable to unify the capabilities of all MT evaluation tasks ( REF , SRC and SRC+REF ) into one model. Among the promising advantages are ease of use and improved robustness through knowledge transfer across evaluation tasks. To achieve this idea, the following two important challenges need to be addressed: 1) how to design a model framework that can unify all translation evaluation tasks? 2) Considering the powerful capabilities of the PLM, how to make the PLM better adapt to the unified evaluation model?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_6",
            "content": "In this paper, we propose UniTE -Unified Translation Evaluation, a novel approach which unifies the functionalities of REF , SRC and SRC+REF tasks into one model. To solve the first challenge as mentioned above, based on the multilingual PLM (Conneau et al., 2020), we utilize layerwise coordination which concatenates all input segments into one sequence as the unified input form. To further unify the modeling of three evaluation tasks, we propose a novel Monotonic Regional Attention (MRA) strategy, which allows partial semantic flows for a specific evaluation task. For the second challenge, a multi-task learning-xbased unified pretraining is proposed. To be concrete, we collect the high-quality translations and degrade low-quality translations of NMT models as synthetic data. Then we propose a novel ranking-based data labeling strategy to provide the training signal. Finally, The multilingual PLM is continuously pretrained on synthetic dataset with multi-task learning. Besides, our proposed models, named as UniTE-MRA and UniTE-UP respectively, can benefit from finetuning with human-annotated data over three tasks at once, not requiring extra taskspecific training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_7",
            "content": "Experimental results demonstrate the superiority of UniTE. Compared to different strong baseline systems on each task, UniTE, which unifies REF , SRC and SRC+REF tasks into one single model, achieves consistently absolute improvements of Kendall's \u03c4 correlations at 1.1, 2.3 and 1.1 scores on English-targeted translation directions of WMT 2019 Metric Shared task (Fonseca et al., 2019), respectively. Meanwhile, after introducing multilingual-targeted support for our unified pretraining strategy, a single model named UniTE-MUP also gives dominant results against existing methods on non-English-targeted translation evaluation tasks. Furthermore, our method can also achieve competitive results over WMT 2020 QE task compared with the winner submission (Ranasinghe et al., 2020b). Ablation studies reveal that, the proposed MRA and unified pretraining strategies are both important for model performance, making the model preserve the outstanding performance and multi-task transferability concurrently.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_8",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "151-ARR_v1_9",
            "content": "In this section, we briefly introduce the three directions of translation evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_10",
            "content": "Reference-Only Evaluation",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "151-ARR_v1_11",
            "content": "REF assesses the translation quality via comparing the translation candidate and the given reference. In this setting, the two inputs are written in the same language, thus being easily applied in most of the metric tasks. In the early stages, statistical methods are dominant solutions due to their strengths in wide language support and intuitive design. These methods measure the surface text similarity for a range of linguistic features, including n-gram (BLEU, Papineni et al., 2002), token (TER, Snover et al., 2006), and character (ChrF & ChrF++, Popovic, 2015, 2017. However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020;Rei et al., 2020;Mathur et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_12",
            "content": "Consequently, with the rapid development of PLMs, researchers have been paying their attention in model-based approaches. The basic idea of these studies is to collect sentences representations for similarity calculation (BERTScore, Zhang et al., 2020) or evaluating probabilistic confidence (PRISM-ref, Thompson and Post, 2020;BARTScore, Yuan et al., 2021). To further improve the model, Sellam et al. (2020a) pretrained a specific PLM for the translation evaluation (BLEURT), while Lo (2019) combined statistical and representative features (YiSi-1). Both these methods achieve higher correlations with human judgments than statistical counterparts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_13",
            "content": "Source-Only Evaluation",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "151-ARR_v1_14",
            "content": "SRC , which also refers to quality estimation 1 , is an important translation evaluation task especially for the scenario where the ground-truth reference is unavailable. It takes the source-side sentence and the translation candidate as inputs for the quality estimation. To achieve this, the methods are required to model cross-lingual semantic alignments. Similar to reference-only evaluation, statistical-based (Ranasinghe et al., 2020b), model-based (TransQuest, Ranasinghe et al., 2020bPRISM-src, Thompson and Post, 2020), and feature combination (YiSi-2, Lo, 2019) are typical and advanced methods in this tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_15",
            "content": "Source-Reference-Combined Evaluation",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "151-ARR_v1_16",
            "content": "Aside from the above tasks that only consider either source or target side at one time, SRC+REF takes both source and reference sentences into account. In this way, methods in this context can evaluate the translation candidate via utilizing the features from both sides. As a rising paradigm among translation evaluation tasks, SRC+REF also profits from the development of cross-lingual PLMs. For example, finetuning PLMs over human-annotated datasets (COMET, Rei et al., 2020) achieves new state-of-the-art results among all evaluation approaches in WMT 2020 (Barrault et al., 2020). For SRC+REF input format, we show the hard design for monotonic regional attention. Circles marked with denotes the masked attention logits.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_17",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "151-ARR_v1_18",
            "content": "As mentioned above, massive methods are proposed for different automatic evaluation tasks. On the one hand, it is inconvenient and expensive to develop and employ different metrics for different evaluation scenarios. On the other hand, separate models absolutely overlook the commonalities among these evaluation tasks, of which knowledge potentially benefits all three tasks. In order to fulfill the aim of unifying the functionalities on REF , SRC , and SRC+REF into one model, in this section, we introduce UniTE (Figure 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_19",
            "content": "Model Architecture",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "151-ARR_v1_20",
            "content": "By receiving a data example composing of hypothesis, source, and reference segment, UniTE first modifies it into concatenated sequence following the given setting as REF , SRC , or SRC+REF . The input can be formalized as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_21",
            "content": "x REF = Concat(h, r) \u2208 R (l h +lr) , x SRC = Concat(h, s) \u2208 R (l h +ls) ,(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_22",
            "content": "x SRC+REF = Concat(h, s, r) \u2208 R (l h +ls+lr) ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_23",
            "content": "where h, s and r represents hypothesis, source and reference segments, with sequence lengths being l h , l s and l r , respectively. The input sequence is then fed to PLM to derive representations H. Take REF as example:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_24",
            "content": "HREF = PLM(x REF ) \u2208 R (l h +ls)\u00d7d , (2",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_25",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_26",
            "content": "where d is the model size of PLM. According to common practice (Ranasinghe et al., 2020b), we use the first output representation as the input of feedforward layer (see Appendix B).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_27",
            "content": "Compared to existing methods (Rei et al., 2020;Zhang et al., 2020) which only take the outputted representations of the topmost layer for evaluation, the advantages of our architecture design are as follows. First, our UniTE model can benefit from layer-coordinated semantical interactions inside every one layer of PLM, which is proven effective on capturing diverse linguistic features (He et al., 2018;Lin et al., 2019;Jawahar et al., 2019;Tenney et al., 2019;Rogers et al., 2020). Second, for the unified approach of our model, the concatenation provides the unifying format for all task inputs, turning our model into a more general architecture. When conducting different evaluation tasks, our model requires no further modification inside. Note here, to keep the consistency across all evaluation tasks, as well as ease the unified learning, h is always located at the beginning of the input sequence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_28",
            "content": "After deriving HREF , a pooling block is arranged after PLM which gives sequence-level representations H REF . Finally, a feedforward network takes H REF as input, and gives a scalar p as prediction:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_29",
            "content": "H REF = Pool( HREF ) \u2208 R d ,(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_30",
            "content": "p REF = FeedForward(H REF ) \u2208 R 1 .(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_31",
            "content": "During training, we encourage the model to reduce the mean squared error between model prediction and given score q:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_32",
            "content": "L REF = (p REF \u2212 q) 2 . (5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_33",
            "content": "However, the input patterns for most multilingual PLMs (e,g., XLM-R, Conneau et al., 2020) are designed to receive two segments at most during pretraining. Thus there exists a gap between the pretraining of PLM and the joint training of UniTE where the concatenation of three fragments is used as input. Moreover, previous studies (Takahashi et al., 2020) show that directly training over SRC+REF by following such design leads to slightly worse performance than REF scenarios. To alleviate this issue, we propose two strategies: Monotonic Regional Attention as described in \u00a73.2 and Unified Pretraining in \u00a73.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_34",
            "content": "Monotonic Regional Attention",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "151-ARR_v1_35",
            "content": "In order to fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks. Following this, we propose to modify the attention mask of SRC+REF to simulate the modeling of two segments in SRC and REF . Specifically, when calculating the attention logits, semantics from a specific segment are only allowed to derive information from two segments at most. Therefore, we propose monotonic regional attention (MRA).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_36",
            "content": "The conventional attention module can be expressed as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_37",
            "content": "A = Softmax( QK \u221a d ) \u2208 R L\u00d7L , (6",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_38",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_39",
            "content": "where L is the sequential length for input, Q, K \u2208 R L\u00d7d are query and key representations, respectively. 2 As to MRA, we simply add a mask M to the softmax logits to control attention flows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_40",
            "content": "A = Softmax( QK \u221a d + M) \u2208 R L\u00d7L , (7) M ij = \u2212\u221e (i, j) \u2208 U, 0 otherwise, (8",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_41",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_42",
            "content": "where U stores the index pairs of all masked areas. Following this idea, the key of MRA is how to design the matrix U. For the cases where interactions inside each segment, we believe that these self-interactions are beneficial to the modeling. For other cases where interactions are arranged across segments, three patterns are included: hypothesisreference, source-reference, and hypothesis-source.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_43",
            "content": "Intuitively, the former two parts are beneficial for model training, since they might contribute the monolingual signals and cross-lingual disambiguation to evaluation, respectively. This leaves the only case, where our experimental analysis also verifies (see \u00a75.1), that interaction between hypothesis and source leads to the performance decrease for SRC+REF task, thus troubling the unifying.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_44",
            "content": "To give more fine-grained designs, we propose two approaches for UniTE-MRA, which apply the MRA mechanism into UniTE model:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_45",
            "content": "\u2022 Hard MRA. Only monotonic attention flows are allowed. Interactions between any two segments are strictly unidirectional through the entire PLM, where U stores the index pairs of unidirectional interactions of h \u2192 r, s \u2192 r and h \u2192 s, where \"\u2192\" denotes the direction of attention flows. \u2022 Soft MRA. Specific attention flows are forbidden inside each attention module. Following this design, the involved two segments may interact inside a higher layer. In practice, index pairs which denoting h \u2192 s or s \u2192 h between source and hypothesis are stored in U.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_46",
            "content": "Note here that, although the processing in source and reference segments may be affected because their position embeddings are not indexed from the start, related studies on positional embeddings reveal that PLM models can well capture relative positional information (Wang and Chen, 2020), which dispels this concern.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_47",
            "content": "Unified Pretraining",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "151-ARR_v1_48",
            "content": "To further bridge the modeling gap between PLM and the joint training of UniTE as we mentioned in \u00a73.1, we propose a unified pretraining strategy including the following main stages: 1) collecting and downgrading synthetic data; 2) labeling examples with a novel ranking-based strategy; 3) multi-task learning for unified pretraining and finetuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_49",
            "content": "Synthetic Data Collection As our approach aims at evaluating the quality of translations, generated hypotheses with NMT models are ideal synthetic data. In order to further improve the diversity of synthetic data quality, we follow Sellam et al. (2020a) to apply the word and span dropping strategy to downgrade a portion of hypotheses. The collected data totally contains N triplets composing of hypothesis, source and reference segments, which is formed as D = { h i , s i , r i } N i=1 . Data Labeling After obtaining the synthetic data, the next step is to augment each data pair with a label which serves as the signal of unified pretraining. To stabilize the model training, as well as normalize the distributions across all score systems and languages, we propose a novel rankingbased approach. This method is based on the idea of Borda count (Ho et al., 1994;Emerson, 2013), which provides more precise and well-distributed synthetic data labels than Z-score normalization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_50",
            "content": "To be concrete, for each data item, we first use existing evaluation approach to give prediction qi for each item, yielding labeled synthetic quadruple examples formed as D = { h i , s i , r i , qi } N i=1 . After that, we descendingly tag each example with their rank index qi referring to qi :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_51",
            "content": "qi = IndexOf(q i , Q), (9",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_52",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_53",
            "content": "where Q is the list storing all the sorted qi descendingly. Then, we use the conventional Z-score strategy to normalize the scores:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_54",
            "content": "q i = qi \u2212 \u00b5 \u03c3 ,(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_55",
            "content": "where \u00b5 and \u03c3 are the mean and the standard deviation of values in Q, respectively. The dataset thus updates its format to D = { h i , s i , r i , q i } N i=1 . Note here that, an example with higher qi is assigned with higher qi , thus a larger value of q i . Compared to related approaches which apply Zscore normalization (Bojar et al., 2018), or leave the conventional labeled scores as signals for learning (i.e., knowledge distillation, Kim and Rush, 2016;Phuong and Lampert, 2019), our approach can alleviate the bias of chosen model for labeling and prior distributional disagreement of scores. For example, different existing methods may output scores with different distributions. Especially for translation directions of low-resource, scores may follow skewed distribution (Sellam et al., 2020a), which has a disagreement with rich-resource scenarios. Our method can unify the distribution of all labeling data into the same scale, which can also be easily applied by the ensembling strategy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_56",
            "content": "To unify all evaluation scenarios into one model, we apply multi-task learning for both pretraining and finetuning. For each step, we arrange three substeps for all input formats, yielding L REF , L SRC , and L SRC+REF , respectively. The final learning objective is to reduce the summation of all losses:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_57",
            "content": "L = L REF + L SRC + L SRC+REF . (11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_58",
            "content": "4 Experiments",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_59",
            "content": "Experimental Settings",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "151-ARR_v1_60",
            "content": "Benchmarks Following previous work (Rei et al., 2020;Yuan et al., 2021), we examine the effectiveness of the propose method on WMT 2019 Metrics tasks (Ma et al., 2019). We follow the common practice in COMET 3 (Rei et al., 2020) to collect and preprocess the dataset. The official variant of Kendall's Tau correlation (Ma et al., 2019) is used for evaluation. We evaluate our methods on all of REF , SRC and SRC+REF scenarios. For SRC scenario, we further evaluate our method on WMT 2020 QE tasks (Specia et al., 2020), where we follow Ranasinghe et al. (2020a) for data collection and preprocessing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_61",
            "content": "Pretraining Data As mentioned in \u00a73.3, we continuously pretrain PLMs using synthetic data. The data is constructed from WMT 2021 Cz-En, De-En, Ja-En, Ru-En, and Zh-En machine translation training sets. Specifically, we follow Sellam et al. (2020a) to use Transformer-base (Vaswani et al., 2017) model to generate translation candidates, and use the checkpoints trained via UniTE-MRA approach for synthetic data labeling. We pretrain two kinds of models, one is the English version which is pretrained on English-targeted language directions, the other is a multilingual version trained using bidirectional data. Note that, for a fair comparison, we filter out all pretraining examples that are involved in benchmarks. 4 Model Setting We implement our approach upon COMET (Rei et al., 2020) and Unified Pretraining ( \u00a7 3.3), we also extend the latter with multilingual-targeted unified pretraining, thus obtaining UniTE-MUP model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_62",
            "content": "Baselines For REF approaches, we select BLEU (Papineni et al., 2002), ChrF (Popovic, 2015), YiSi-1 (Lo, 2019), BERTScore (Zhang et al., 2020), BLEURT (Sellam et al., 2020a), PRISM-ref (Thompson and Post, 2020), XLM-R+Concat (Takahashi et al., 2020), RoBERTa+Concat (Takahashi et al., 2020), and BARTScore (Yuan et al., 2021) for comparison. As to SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al., 2020), PRISMsrc (Thompson and Post, 2020) and multilingual-tomultilingual version of MTransQuest (Ranasinghe et al., 2020b). For SRC+REF scenario, we use XLM-R+Concat (Takahashi et al., 2020) and COMET (Rei et al., 2020) as strong baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_63",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "151-ARR_v1_64",
            "content": "English-Targeted Experimental results on metric task are conducted in Table 1. For REF methods, BARTScore (Yuan et al., 2021) performs better than other statistical and model-based metrics. MTransQuest (Ranasinghe et al., 2020b) gives dominant performance on SRC scenario, and COMET (Rei et al., 2020) performs better than XLM-R+Concat on SRC+REF scenario.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_65",
            "content": "As for our methods, UniTE-MRA approach achieves better results on all tasks, demonstrating the effectiveness of monotonic attention flow for cross-lingual interactions. Moreover, our proposed model UniTE-UP, which unifies both pretraining and finetuning, can yield better results following all evaluation settings. Most importantly, UniTE-UP is a single model which surpasses all the different state-of-the-art models on three tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_66",
            "content": "En-Cs En-De En-Ru En-Zh En-Fi En-Gu En-Kk En-Lt De-Cs De-Fr Fr-De Avg.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_67",
            "content": "Reference-only Evaluation \u2665 BLEU (Papineni et al., 2002) 36",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_68",
            "content": "As seen in Table 2, our multilingual-targeted version model, a single model UniTE-MUP, still gives dominant performance than all different strong baselines on REF , SRC and SRC+REF . These results further demonstrate the transferability and effectiveness of our approach. Besides, it is encouraging to see that the UniTE-UP can also give dominant results, revealing an improvement of 0.6, 0.3 and 0.9 averaged Kendall's \u03c4 correlation scores, respectively. Our further comparison indicates that UniTE-MUP also outperforms previous strong baselines but slightly worse than UniTE-UP on English-targeted translation directions. 5 We think the reason lies in the curse of multilingualism and vocabulary dilution (Conneau et al., 2020). Moreover, we also testify the performance of UniTE-MUP on WMT 2020 QE tasks via finetuning. Results 6 demonstrate that our method can also achieve competitive results over WMT 2020 QE task compared with the winner submission (Ranasinghe et al., 2020b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_69",
            "content": "Ablation Studies",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "151-ARR_v1_70",
            "content": "In this section, we conduct ablation studies to investigate the effectiveness of regional attention patterns ( \u00a75.1), unified training ( \u00a75.2), and rankingbased data labeling ( \u00a75.3). All experiments follow English-targeted setting on SRC+REF task.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_71",
            "content": "Regional Attention Patterns",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "151-ARR_v1_72",
            "content": "To investigate the effectiveness of MRA, we further collect experiments for comparison. As seen in Table 3, MRA can give performance improvements than full attention, and preventing the interactions between hypothesis and source segment can improve the performance most. We think the reasons behind are twofold. First, the source side is formed with a different language, whose semantic information is rather weak than the reference side. Second, by preventing direct interactions between source and hypothesis, semantics inside the former must be passed through reference, which is helpful for disambiguation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_73",
            "content": "Besides, not allowing the source to derive information from the hypothesis is better than the opposite direction. As Wang and Chen (2020) found that the positional embeddings in PLM are engaged with strong adjacent information. We think the reason why S\u2192H performs worse than H\u2192S lies in the skipping of indexes, which corrupts positional similarities in alignment calculation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_74",
            "content": "Additionally, when we combined two methods together, i.e., unified pretraining and finetuning with SRC+REF UniTE-MRA setting, model performance drops to 34.9 over English-targeted tasks on average. We think that both methods all intend to solve the problem of unseen SRC+REF input format, and MRA may not be necessary if massive data examples can be obtained for pretraining. Nevertheless, UniTE-MRA has its advantage on wide application without requiring pseduo labeled data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_75",
            "content": "Unified Training",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "151-ARR_v1_76",
            "content": "Experiments for comparing unified and taskspecific training are concluded in Table 4. As seen, when using the unified pretraining checkpoint to finetune over the specific task, performance over three models reveals performance drop consistently, indicating the unified finetuning is helpful for model learning, and SRC , REF and SRC+REF tasks are complementary to each other during learning. Also, utilizing task-specific pretraining instead of unified one reveals worse performance. To sum up, unifying both pretraining and finetuning only reveals one model, showing its advantage on the generalization on all tasks, where one united model can cover all functionalities of REF , SRC and SRC+REF tasks concurrently.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_77",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "151-ARR_v1_78",
            "content": "In the past decades, automatic translation evaluation is mainly divided into REF , SRC and SRC+REF tasks, each of which develops independently and is tackled by various task-specific methods. We suggest that the three tasks are possibly handled by a unified framework, thus being ease of use and facilitating the knowledge transferring. Contributions of our work are mainly in three folds: (a) We propose a flexible and unified translation evaluation model UniTE, which can be adopted into the three tasks at once; (b) Through in-depth analyses, we point out that the main challenge of unifying three tasks stems from the discrepancy between vanilla pretraining and multi-tasks finetuning. We further fill this gap via introducing MRA and Unified Pretraining; (c) Our single model consistently outperforms a variety of state-of-the-art or winner systems across tasks in WMT 2019 Metrics and WMT 2020 QE benchmarks. We hope our new insights can contribute to subsequent studies in the translation evaluation community.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_79",
            "content": "Considering the English-targeted model, we select Czech (Cz), German (De), Japanese (Ja), Russian (Ru), and Chinese (Zh) as source languages, and English (En) as target. For each translation direction, we collect 1 million samples, finally yielding 5 million examples in total for unified pretraining. As to the multilingual-targeted model, we further collect 1 million synthetic data for each language direction of En-Cz, En-De, En-Ja, En-Ru, and En-Zh. Finally, we construct 10 million examples for the pretraining of the multilingual version by adding the data of the English-targeted model. Note that, for a fair comparison, we filter out all pretraining examples that are involved in benchmarks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_80",
            "content": "After trying several pooling methods which derive sequence-level representations, we use the representations located at the start of sequence as the input of feedforward network (Ranasinghe et al., 2020b). The feedforward network consists of 3 linear transitions, where the dimensionalities of corresponding outputs are 3,072, 1,024, and 1, respectively. Between any two adjacent linear modules in feedforward, hyperbolic tangent function is arranged as activation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_81",
            "content": "For give more details for comparison, we collect the results of UniTE-MUP over English-targeted experiments in Table 6.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_82",
            "content": "The results for UniTE approach on WMT 2020 QE task are concluded in Table 7. As seen, our approach can give better performance than strong QE baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_83",
            "content": "All the models reported in this paper were finetuned on a single Nvidia V100 (32GB) GPU. Specifically for UniTE-UP and UniTE-MUP, the pretraining is arranged on 4 Nvidia V100 (32GB) GPUs. Our framework is built upon COMET repository (Rei et al., 2020). For the contribution to the research community, we will release both the source code of UniTE framework and the well-trained evaluation models including UniTE-UP and UniTE-MUP checkpoints as described in this paper upon the acceptance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v1_84",
            "content": "UNKNOWN, None, , Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 Conference on Machine Translation WMT20. In Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 Conference on Machine Translation WMT20. In Proceedings of the Fifth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_85",
            "content": "Ondrej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Philipp Koehn, Christof Monz, Findings of the 2018 Conference on Machine Translation, 2018, WMT, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Ondrej Bojar",
                    "Christian Federmann",
                    "Mark Fishel",
                    "Yvette Graham",
                    "Barry Haddow",
                    "Philipp Koehn",
                    "Christof Monz"
                ],
                "title": "Findings of the 2018 Conference on Machine Translation",
                "pub_date": "2018",
                "pub_title": "WMT",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_86",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised Cross-lingual Representation Learning at Scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Alexis Conneau",
                    "Kartikay Khandelwal",
                    "Naman Goyal",
                    "Vishrav Chaudhary",
                    "Guillaume Wenzek",
                    "Francisco Guzm\u00e1n",
                    "Edouard Grave",
                    "Myle Ott",
                    "Luke Zettlemoyer",
                    "Veselin Stoyanov"
                ],
                "title": "Unsupervised Cross-lingual Representation Learning at Scale",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_87",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL:HLT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL:HLT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_88",
            "content": "Peter Emerson, The Original Borda Count and Partial Voting, 2013, Social Choice and Welfare, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Peter Emerson"
                ],
                "title": "The Original Borda Count and Partial Voting",
                "pub_date": "2013",
                "pub_title": "Social Choice and Welfare",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_89",
            "content": "Erick Fonseca, Lisa Yankovskaya, F Andr\u00e9, Mark Martins, Christian Fishel,  Federmann, Findings of the WMT 2019 Shared Tasks on Quality Estimation, 2019, Proceedings of the Fourth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Erick Fonseca",
                    "Lisa Yankovskaya",
                    "F Andr\u00e9",
                    "Mark Martins",
                    "Christian Fishel",
                    " Federmann"
                ],
                "title": "Findings of the WMT 2019 Shared Tasks on Quality Estimation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_90",
            "content": "Markus Freitag, David Grangier, Isaac Caswell, BLEU might be guilty but references are not innocent, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Markus Freitag",
                    "David Grangier",
                    "Isaac Caswell"
                ],
                "title": "BLEU might be guilty but references are not innocent",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_91",
            "content": "Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, Tie-Yan Liu, Layer-wise coordination between encoder and decoder for neural machine translation, 2018, Proceedings of the 32nd International Conference on Neural Information Processing Systems (NeurIPS), .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Tianyu He",
                    "Xu Tan",
                    "Yingce Xia",
                    "Di He",
                    "Tao Qin",
                    "Zhibo Chen",
                    "Tie-Yan Liu"
                ],
                "title": "Layer-wise coordination between encoder and decoder for neural machine translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 32nd International Conference on Neural Information Processing Systems (NeurIPS)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_92",
            "content": "Kam Tin, Jonathan Ho, Sargur Hull,  Srihari, Decision Combination in Multiple Classifier Systems, 1994, IEEE transactions on Pattern Analysis and Machine Intelligence (TPAMI), .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Kam Tin",
                    "Jonathan Ho",
                    "Sargur Hull",
                    " Srihari"
                ],
                "title": "Decision Combination in Multiple Classifier Systems",
                "pub_date": "1994",
                "pub_title": "IEEE transactions on Pattern Analysis and Machine Intelligence (TPAMI)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_93",
            "content": "Ganesh Jawahar, Beno\u00eet Sagot, Djam\u00e9 Seddah, What Does BERT Learn about the Structure of Language?, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Ganesh Jawahar",
                    "Beno\u00eet Sagot",
                    "Djam\u00e9 Seddah"
                ],
                "title": "What Does BERT Learn about the Structure of Language?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_94",
            "content": "Fabio Kepler, Jonay Tr\u00e9nous, Marcos Treviso, Miguel Vera, Andr\u00e9 Martins, OpenKiwi: An Open Source Framework for Quality Estimation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL Demo), .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Fabio Kepler",
                    "Jonay Tr\u00e9nous",
                    "Marcos Treviso",
                    "Miguel Vera",
                    "Andr\u00e9 Martins"
                ],
                "title": "OpenKiwi: An Open Source Framework for Quality Estimation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL Demo)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_95",
            "content": "Yoon Kim, Alexander M Rush, Sequence-Level Knowledge Distillation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Yoon Kim",
                    "Alexander M Rush"
                ],
                "title": "Sequence-Level Knowledge Distillation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_96",
            "content": "Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation, 2021, Proceedings of the Seventh Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Tom Kocmi",
                    "Christian Federmann",
                    "Roman Grundkiewicz",
                    "Marcin Junczys-Dowmunt"
                ],
                "title": "To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Seventh Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_97",
            "content": "Yongjie Lin, Yi Chern Tan, Robert Frank, Open Sesame: Getting inside BERT's Linguistic Knowledge, 2019, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP@ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Yongjie Lin",
                    "Yi Chern Tan",
                    "Robert Frank"
                ],
                "title": "Open Sesame: Getting inside BERT's Linguistic Knowledge",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP@ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_98",
            "content": "Chi-Kiu Lo, YiSi -a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources, 2019, Proceedings of the Fourth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Chi-Kiu Lo"
                ],
                "title": "YiSi -a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_99",
            "content": "Qingsong Ma, Johnny Wei, Ond\u0159ej Bojar, Yvette Graham, Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges, 2019, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Qingsong Ma",
                    "Johnny Wei",
                    "Ond\u0159ej Bojar",
                    "Yvette Graham"
                ],
                "title": "Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_100",
            "content": "Nitika Mathur, Timothy Baldwin, Trevor Cohn, Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Nitika Mathur",
                    "Timothy Baldwin",
                    "Trevor Cohn"
                ],
                "title": "Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_101",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a Method for Automatic Evaluation of Machine Translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_102",
            "content": "Mary Phuong, Christoph Lampert, Towards Understanding Knowledge Distillation, 2019, Proceedings of the 36th International Conference on Machine Learning (ICML), .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Mary Phuong",
                    "Christoph Lampert"
                ],
                "title": "Towards Understanding Knowledge Distillation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 36th International Conference on Machine Learning (ICML)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_103",
            "content": "Maja Popovic, chrF: character n-gram F-score for automatic MT evaluation, 2015, Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Maja Popovic"
                ],
                "title": "chrF: character n-gram F-score for automatic MT evaluation",
                "pub_date": "2015",
                "pub_title": "Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_104",
            "content": "Maja Popovic, chrF++: words helping character n-grams, 2017, Proceedings of the Second Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Maja Popovic"
                ],
                "title": "chrF++: words helping character n-grams",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_105",
            "content": "UNKNOWN, None, 2020, Intelligent Translation Memory Matching and Retrieval with Sentence Encoders, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Intelligent Translation Memory Matching and Retrieval with Sentence Encoders",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_106",
            "content": "Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov, TransQuest: Translation Quality Estimation with Cross-lingual Transformers, 2020, Proceedings of the 28th International Conference on Computational Linguistics (COLING), .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Tharindu Ranasinghe",
                    "Constantin Orasan",
                    "Ruslan Mitkov"
                ],
                "title": "TransQuest: Translation Quality Estimation with Cross-lingual Transformers",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics (COLING)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_107",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, COMET: A neural framework for MT evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Ricardo Rei",
                    "Craig Stewart",
                    "Ana Farinha",
                    "Alon Lavie"
                ],
                "title": "COMET: A neural framework for MT evaluation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_108",
            "content": "Anna Rogers, Olga Kovaleva, Anna Rumshisky, A Primer in BERTology: What We Know About How BERT Works, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Anna Rogers",
                    "Olga Kovaleva",
                    "Anna Rumshisky"
                ],
                "title": "A Primer in BERTology: What We Know About How BERT Works",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_109",
            "content": "Thibault Sellam, Dipanjan Das, Ankur Parikh, BLEURT: learning robust metrics for text generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Thibault Sellam",
                    "Dipanjan Das",
                    "Ankur Parikh"
                ],
                "title": "BLEURT: learning robust metrics for text generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_110",
            "content": "Thibault Sellam, Amy Pu,  Hyung Won, Sebastian Chung, Qijun Gehrmann, Markus Tan, Dipanjan Freitag, Ankur Das,  Parikh, Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task, 2020, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Thibault Sellam",
                    "Amy Pu",
                    " Hyung Won",
                    "Sebastian Chung",
                    "Qijun Gehrmann",
                    "Markus Tan",
                    "Dipanjan Freitag",
                    "Ankur Das",
                    " Parikh"
                ],
                "title": "Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_111",
            "content": "Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, John Makhoul, A Study of Translation Edit Rate with Targeted Human Annotation, 2006, Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers (ATMA), .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Matthew Snover",
                    "Bonnie Dorr",
                    "Richard Schwartz",
                    "Linnea Micciulla",
                    "John Makhoul"
                ],
                "title": "A Study of Translation Edit Rate with Targeted Human Annotation",
                "pub_date": "2006",
                "pub_title": "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers (ATMA)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_112",
            "content": "Lucia Specia, Fr\u00e9d\u00e9ric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzm\u00e1n, Andr\u00e9 Martins, Findings of the WMT 2020 Shared Task on Quality Estimation, 2020, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Lucia Specia",
                    "Fr\u00e9d\u00e9ric Blain",
                    "Marina Fomicheva",
                    "Erick Fonseca",
                    "Vishrav Chaudhary",
                    "Francisco Guzm\u00e1n",
                    "Andr\u00e9 Martins"
                ],
                "title": "Findings of the WMT 2020 Shared Task on Quality Estimation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_113",
            "content": "Kosuke Takahashi, Katsuhito Sudoh, Satoshi Nakamura, Automatic Machine Translation Evaluation using Source Language Inputs and Crosslingual Language Model, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Kosuke Takahashi",
                    "Katsuhito Sudoh",
                    "Satoshi Nakamura"
                ],
                "title": "Automatic Machine Translation Evaluation using Source Language Inputs and Crosslingual Language Model",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_114",
            "content": "Ian Tenney, Dipanjan Das, Ellie Pavlick, BERT Rediscovers the Classical NLP Pipeline, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Ian Tenney",
                    "Dipanjan Das",
                    "Ellie Pavlick"
                ],
                "title": "BERT Rediscovers the Classical NLP Pipeline",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_115",
            "content": "Brian Thompson, Matt Post, Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Brian Thompson",
                    "Matt Post"
                ],
                "title": "Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "151-ARR_v1_116",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention Is All You Need, 2017, Advances in Neural Information Processing Systems (NIPS), .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention Is All You Need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems (NIPS)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_117",
            "content": "Yu-An Wang, Yun-Nung Chen, What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Yu-An Wang",
                    "Yun-Nung Chen"
                ],
                "title": "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "151-ARR_v1_118",
            "content": "Weizhe Yuan, Graham Neubig, Pengfei Liu, BARTScore: Evaluating Generated Text as Text Generation, 2021, Advances in Neural Information Processing Systems (NeurIPS), .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Weizhe Yuan",
                    "Graham Neubig",
                    "Pengfei Liu"
                ],
                "title": "BARTScore: Evaluating Generated Text as Text Generation",
                "pub_date": "2021",
                "pub_title": "Advances in Neural Information Processing Systems (NeurIPS)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_119",
            "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, Yoav Artzi, BERTScore: Evaluating Text Generation with BERT, 2020, 8th International Conference on Learning Representations (ICLR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Tianyi Zhang",
                    "Varsha Kishore",
                    "Felix Wu",
                    "Kilian Weinberger",
                    "Yoav Artzi"
                ],
                "title": "BERTScore: Evaluating Text Generation with BERT",
                "pub_date": "2020",
                "pub_title": "8th International Conference on Learning Representations (ICLR)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v1_120",
            "content": "Wei Zhao, Goran Glava\u0161, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger, On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Wei Zhao",
                    "Goran Glava\u0161",
                    "Maxime Peyrard",
                    "Yang Gao",
                    "Robert West",
                    "Steffen Eger"
                ],
                "title": "On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "151-ARR_v1_0@0",
            "content": "UniTE: Unified Translation Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_0",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_2@0",
            "content": "Translation quality evaluation plays a crucial role in machine translation, and is mainly separated into three tasks according to different input formats, i.e., reference-only, sourceonly and source-reference-combined.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_2",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_2@1",
            "content": "Recent methods, despite their promising results, are specifically designed and optimized on one of these three tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_2",
            "start": 219,
            "end": 335,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_2@2",
            "content": "This limits the convenience of these methods and overlooks commonalities among tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_2",
            "start": 337,
            "end": 421,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_2@3",
            "content": "In this paper, we propose UniTE, which is the first unified framework engaged with abilities to handle all three evaluation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_2",
            "start": 423,
            "end": 552,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_2@4",
            "content": "Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_2",
            "start": 554,
            "end": 714,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_2@5",
            "content": "We empirically testify our framework on WMT 2019 Metrics and WMT 20 Quality Estimation benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_2",
            "start": 716,
            "end": 813,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_2@6",
            "content": "Extensive analyses show that our single model can universally surpass various state-of-the-art or winner methods across tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_2",
            "start": 815,
            "end": 940,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_2@7",
            "content": "Both source code and associated models will be released upon the acceptance of this paper.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_2",
            "start": 942,
            "end": 1031,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_4@0",
            "content": "Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020;Mathur et al., 2020;Zhao et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_4",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_4@1",
            "content": "According to the input contexts, translation evaluation can be mainly categorized into three classes: 1) reference-only evaluation ( REF ) approaches like BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi-2 (Lo, 2019) and TransQuest (Ranasinghe et al., 2020b), which are also referred as quality estimation (QE).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_4",
            "start": 234,
            "end": 691,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_4@2",
            "content": "These methods estimate the quality of the hypothesis based on the source sentence without using references; 3) source-reference-combined evaluation ( SRC+REF ) works like COMET (Rei et al., 2020), where the model exploits information from both source and reference for the evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_4",
            "start": 693,
            "end": 976,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_4@3",
            "content": "With the help of powerful pretrained language models (PLMs, Devlin et al., 2019;Conneau et al., 2020), model-based approaches (e.g., BLEURT, TransQuest, and COMET) have shown promising results in recent WMT competitions (Ma et al., 2019;Barrault et al., 2020;Kocmi et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_4",
            "start": 978,
            "end": 1256,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_5@0",
            "content": "Nevertheless, each existing MT evaluation work is usually designed for one specific task, e.g., BLEURT is only used for REF task and can not support SRC and SRC+REF tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_5",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_5@1",
            "content": "Moreover, those approaches preserve the same core -evaluating the quality of translation by referring to the given segments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_5",
            "start": 172,
            "end": 295,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_5@2",
            "content": "Therefore, we believe that it is valuable to unify the capabilities of all MT evaluation tasks ( REF , SRC and SRC+REF ) into one model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_5",
            "start": 297,
            "end": 432,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_5@3",
            "content": "Among the promising advantages are ease of use and improved robustness through knowledge transfer across evaluation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_5",
            "start": 434,
            "end": 555,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_5@4",
            "content": "To achieve this idea, the following two important challenges need to be addressed: 1) how to design a model framework that can unify all translation evaluation tasks? 2) Considering the powerful capabilities of the PLM, how to make the PLM better adapt to the unified evaluation model?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_5",
            "start": 557,
            "end": 841,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_6@0",
            "content": "In this paper, we propose UniTE -Unified Translation Evaluation, a novel approach which unifies the functionalities of REF , SRC and SRC+REF tasks into one model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_6",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_6@1",
            "content": "To solve the first challenge as mentioned above, based on the multilingual PLM (Conneau et al., 2020), we utilize layerwise coordination which concatenates all input segments into one sequence as the unified input form.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_6",
            "start": 163,
            "end": 381,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_6@2",
            "content": "To further unify the modeling of three evaluation tasks, we propose a novel Monotonic Regional Attention (MRA) strategy, which allows partial semantic flows for a specific evaluation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_6",
            "start": 383,
            "end": 570,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_6@3",
            "content": "For the second challenge, a multi-task learning-xbased unified pretraining is proposed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_6",
            "start": 572,
            "end": 658,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_6@4",
            "content": "To be concrete, we collect the high-quality translations and degrade low-quality translations of NMT models as synthetic data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_6",
            "start": 660,
            "end": 785,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_6@5",
            "content": "Then we propose a novel ranking-based data labeling strategy to provide the training signal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_6",
            "start": 787,
            "end": 878,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_6@6",
            "content": "Finally, The multilingual PLM is continuously pretrained on synthetic dataset with multi-task learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_6",
            "start": 880,
            "end": 982,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_6@7",
            "content": "Besides, our proposed models, named as UniTE-MRA and UniTE-UP respectively, can benefit from finetuning with human-annotated data over three tasks at once, not requiring extra taskspecific training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_6",
            "start": 984,
            "end": 1181,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_7@0",
            "content": "Experimental results demonstrate the superiority of UniTE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_7",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_7@1",
            "content": "Compared to different strong baseline systems on each task, UniTE, which unifies REF , SRC and SRC+REF tasks into one single model, achieves consistently absolute improvements of Kendall's \u03c4 correlations at 1.1, 2.3 and 1.1 scores on English-targeted translation directions of WMT 2019 Metric Shared task (Fonseca et al., 2019), respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_7",
            "start": 59,
            "end": 400,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_7@2",
            "content": "Meanwhile, after introducing multilingual-targeted support for our unified pretraining strategy, a single model named UniTE-MUP also gives dominant results against existing methods on non-English-targeted translation evaluation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_7",
            "start": 402,
            "end": 635,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_7@3",
            "content": "Furthermore, our method can also achieve competitive results over WMT 2020 QE task compared with the winner submission (Ranasinghe et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_7",
            "start": 637,
            "end": 782,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_7@4",
            "content": "Ablation studies reveal that, the proposed MRA and unified pretraining strategies are both important for model performance, making the model preserve the outstanding performance and multi-task transferability concurrently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_7",
            "start": 784,
            "end": 1005,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_8@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_8",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_9@0",
            "content": "In this section, we briefly introduce the three directions of translation evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_9",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_10@0",
            "content": "Reference-Only Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_10",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_11@0",
            "content": "REF assesses the translation quality via comparing the translation candidate and the given reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_11",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_11@1",
            "content": "In this setting, the two inputs are written in the same language, thus being easily applied in most of the metric tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_11",
            "start": 102,
            "end": 221,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_11@2",
            "content": "In the early stages, statistical methods are dominant solutions due to their strengths in wide language support and intuitive design.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_11",
            "start": 223,
            "end": 355,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_11@3",
            "content": "These methods measure the surface text similarity for a range of linguistic features, including n-gram (BLEU, Papineni et al., 2002), token (TER, Snover et al., 2006), and character (ChrF & ChrF++, Popovic, 2015, 2017.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_11",
            "start": 357,
            "end": 574,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_11@4",
            "content": "However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020;Rei et al., 2020;Mathur et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_11",
            "start": 576,
            "end": 786,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_12@0",
            "content": "Consequently, with the rapid development of PLMs, researchers have been paying their attention in model-based approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_12",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_12@1",
            "content": "The basic idea of these studies is to collect sentences representations for similarity calculation (BERTScore, Zhang et al., 2020) or evaluating probabilistic confidence (PRISM-ref, Thompson and Post, 2020;BARTScore, Yuan et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_12",
            "start": 122,
            "end": 357,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_12@2",
            "content": "To further improve the model, Sellam et al. (2020a) pretrained a specific PLM for the translation evaluation (BLEURT), while Lo (2019) combined statistical and representative features (YiSi-1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_12",
            "start": 359,
            "end": 551,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_12@3",
            "content": "Both these methods achieve higher correlations with human judgments than statistical counterparts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_12",
            "start": 553,
            "end": 650,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_13@0",
            "content": "Source-Only Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_13",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_14@0",
            "content": "SRC , which also refers to quality estimation 1 , is an important translation evaluation task especially for the scenario where the ground-truth reference is unavailable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_14",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_14@1",
            "content": "It takes the source-side sentence and the translation candidate as inputs for the quality estimation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_14",
            "start": 171,
            "end": 271,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_14@2",
            "content": "To achieve this, the methods are required to model cross-lingual semantic alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_14",
            "start": 273,
            "end": 357,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_14@3",
            "content": "Similar to reference-only evaluation, statistical-based (Ranasinghe et al., 2020b), model-based (TransQuest, Ranasinghe et al., 2020bPRISM-src, Thompson and Post, 2020), and feature combination (YiSi-2, Lo, 2019) are typical and advanced methods in this tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_14",
            "start": 359,
            "end": 618,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_15@0",
            "content": "Source-Reference-Combined Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_15",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_16@0",
            "content": "Aside from the above tasks that only consider either source or target side at one time, SRC+REF takes both source and reference sentences into account.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_16",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_16@1",
            "content": "In this way, methods in this context can evaluate the translation candidate via utilizing the features from both sides.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_16",
            "start": 152,
            "end": 270,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_16@2",
            "content": "As a rising paradigm among translation evaluation tasks, SRC+REF also profits from the development of cross-lingual PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_16",
            "start": 272,
            "end": 392,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_16@3",
            "content": "For example, finetuning PLMs over human-annotated datasets (COMET, Rei et al., 2020) achieves new state-of-the-art results among all evaluation approaches in WMT 2020 (Barrault et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_16",
            "start": 394,
            "end": 584,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_16@4",
            "content": "For SRC+REF input format, we show the hard design for monotonic regional attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_16",
            "start": 586,
            "end": 668,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_16@5",
            "content": "Circles marked with denotes the masked attention logits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_16",
            "start": 670,
            "end": 725,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_17@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_17",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_18@0",
            "content": "As mentioned above, massive methods are proposed for different automatic evaluation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_18",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_18@1",
            "content": "On the one hand, it is inconvenient and expensive to develop and employ different metrics for different evaluation scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_18",
            "start": 91,
            "end": 215,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_18@2",
            "content": "On the other hand, separate models absolutely overlook the commonalities among these evaluation tasks, of which knowledge potentially benefits all three tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_18",
            "start": 217,
            "end": 375,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_18@3",
            "content": "In order to fulfill the aim of unifying the functionalities on REF , SRC , and SRC+REF into one model, in this section, we introduce UniTE (Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_18",
            "start": 377,
            "end": 526,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_19@0",
            "content": "Model Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_19",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_20@0",
            "content": "By receiving a data example composing of hypothesis, source, and reference segment, UniTE first modifies it into concatenated sequence following the given setting as REF , SRC , or SRC+REF .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_20",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_20@1",
            "content": "The input can be formalized as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_20",
            "start": 191,
            "end": 221,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_21@0",
            "content": "x REF = Concat(h, r) \u2208 R (l h +lr) , x SRC = Concat(h, s) \u2208 R (l h +ls) ,(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_21",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_22@0",
            "content": "x SRC+REF = Concat(h, s, r) \u2208 R (l h +ls+lr) ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_22",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_23@0",
            "content": "where h, s and r represents hypothesis, source and reference segments, with sequence lengths being l h , l s and l r , respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_23",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_23@1",
            "content": "The input sequence is then fed to PLM to derive representations H. Take REF as example:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_23",
            "start": 133,
            "end": 219,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_24@0",
            "content": "HREF = PLM(x REF ) \u2208 R (l h +ls)\u00d7d , (2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_24",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_25@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_25",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_26@0",
            "content": "where d is the model size of PLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_26",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_26@1",
            "content": "According to common practice (Ranasinghe et al., 2020b), we use the first output representation as the input of feedforward layer (see Appendix B).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_26",
            "start": 34,
            "end": 180,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_27@0",
            "content": "Compared to existing methods (Rei et al., 2020;Zhang et al., 2020) which only take the outputted representations of the topmost layer for evaluation, the advantages of our architecture design are as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_27",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_27@1",
            "content": "First, our UniTE model can benefit from layer-coordinated semantical interactions inside every one layer of PLM, which is proven effective on capturing diverse linguistic features (He et al., 2018;Lin et al., 2019;Jawahar et al., 2019;Tenney et al., 2019;Rogers et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_27",
            "start": 208,
            "end": 483,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_27@2",
            "content": "Second, for the unified approach of our model, the concatenation provides the unifying format for all task inputs, turning our model into a more general architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_27",
            "start": 485,
            "end": 650,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_27@3",
            "content": "When conducting different evaluation tasks, our model requires no further modification inside.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_27",
            "start": 652,
            "end": 745,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_27@4",
            "content": "Note here, to keep the consistency across all evaluation tasks, as well as ease the unified learning, h is always located at the beginning of the input sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_27",
            "start": 747,
            "end": 907,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_28@0",
            "content": "After deriving HREF , a pooling block is arranged after PLM which gives sequence-level representations H REF .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_28",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_28@1",
            "content": "Finally, a feedforward network takes H REF as input, and gives a scalar p as prediction:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_28",
            "start": 111,
            "end": 198,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_29@0",
            "content": "H REF = Pool( HREF ) \u2208 R d ,(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_29",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_30@0",
            "content": "p REF = FeedForward(H REF ) \u2208 R 1 .(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_30",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_31@0",
            "content": "During training, we encourage the model to reduce the mean squared error between model prediction and given score q:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_31",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_32@0",
            "content": "L REF = (p REF \u2212 q) 2 . (5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_32",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_33@0",
            "content": "However, the input patterns for most multilingual PLMs (e,g., XLM-R, Conneau et al., 2020) are designed to receive two segments at most during pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_33",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_33@1",
            "content": "Thus there exists a gap between the pretraining of PLM and the joint training of UniTE where the concatenation of three fragments is used as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_33",
            "start": 156,
            "end": 302,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_33@2",
            "content": "Moreover, previous studies (Takahashi et al., 2020) show that directly training over SRC+REF by following such design leads to slightly worse performance than REF scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_33",
            "start": 304,
            "end": 476,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_33@3",
            "content": "To alleviate this issue, we propose two strategies: Monotonic Regional Attention as described in \u00a73.2 and Unified Pretraining in \u00a73.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_33",
            "start": 478,
            "end": 611,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_34@0",
            "content": "Monotonic Regional Attention",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_34",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_35@0",
            "content": "In order to fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_35",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_35@1",
            "content": "Following this, we propose to modify the attention mask of SRC+REF to simulate the modeling of two segments in SRC and REF .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_35",
            "start": 231,
            "end": 354,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_35@2",
            "content": "Specifically, when calculating the attention logits, semantics from a specific segment are only allowed to derive information from two segments at most.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_35",
            "start": 356,
            "end": 507,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_35@3",
            "content": "Therefore, we propose monotonic regional attention (MRA).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_35",
            "start": 509,
            "end": 565,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_36@0",
            "content": "The conventional attention module can be expressed as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_36",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_37@0",
            "content": "A = Softmax( QK \u221a d ) \u2208 R L\u00d7L , (6",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_37",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_38@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_38",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_39@0",
            "content": "where L is the sequential length for input, Q, K \u2208 R L\u00d7d are query and key representations, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_39",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_39@1",
            "content": "2 As to MRA, we simply add a mask M to the softmax logits to control attention flows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_39",
            "start": 106,
            "end": 190,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_40@0",
            "content": "A = Softmax( QK \u221a d + M) \u2208 R L\u00d7L , (7) M ij = \u2212\u221e (i, j) \u2208 U, 0 otherwise, (8",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_40",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_41@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_41",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_42@0",
            "content": "where U stores the index pairs of all masked areas.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_42",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_42@1",
            "content": "Following this idea, the key of MRA is how to design the matrix U. For the cases where interactions inside each segment, we believe that these self-interactions are beneficial to the modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_42",
            "start": 52,
            "end": 243,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_42@2",
            "content": "For other cases where interactions are arranged across segments, three patterns are included: hypothesisreference, source-reference, and hypothesis-source.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_42",
            "start": 245,
            "end": 399,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_43@0",
            "content": "Intuitively, the former two parts are beneficial for model training, since they might contribute the monolingual signals and cross-lingual disambiguation to evaluation, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_43",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_43@1",
            "content": "This leaves the only case, where our experimental analysis also verifies (see \u00a75.1), that interaction between hypothesis and source leads to the performance decrease for SRC+REF task, thus troubling the unifying.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_43",
            "start": 183,
            "end": 394,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_44@0",
            "content": "To give more fine-grained designs, we propose two approaches for UniTE-MRA, which apply the MRA mechanism into UniTE model:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_44",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_45@0",
            "content": "\u2022 Hard MRA. Only monotonic attention flows are allowed. Interactions between any two segments are strictly unidirectional through the entire PLM, where U stores the index pairs of unidirectional interactions of h \u2192 r, s \u2192 r and h \u2192 s, where \"\u2192\" denotes the direction of attention flows. \u2022 Soft MRA. Specific attention flows are forbidden inside each attention module. Following this design, the involved two segments may interact inside a higher layer. In practice, index pairs which denoting h \u2192 s or s \u2192 h between source and hypothesis are stored in U.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_45",
            "start": 0,
            "end": 553,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_46@0",
            "content": "Note here that, although the processing in source and reference segments may be affected because their position embeddings are not indexed from the start, related studies on positional embeddings reveal that PLM models can well capture relative positional information (Wang and Chen, 2020), which dispels this concern.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_46",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_47@0",
            "content": "Unified Pretraining",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_47",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_48@0",
            "content": "To further bridge the modeling gap between PLM and the joint training of UniTE as we mentioned in \u00a73.1, we propose a unified pretraining strategy including the following main stages: 1) collecting and downgrading synthetic data; 2) labeling examples with a novel ranking-based strategy; 3) multi-task learning for unified pretraining and finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_48",
            "start": 0,
            "end": 348,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_49@0",
            "content": "Synthetic Data Collection As our approach aims at evaluating the quality of translations, generated hypotheses with NMT models are ideal synthetic data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_49",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_49@1",
            "content": "In order to further improve the diversity of synthetic data quality, we follow Sellam et al. (2020a) to apply the word and span dropping strategy to downgrade a portion of hypotheses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_49",
            "start": 153,
            "end": 335,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_49@2",
            "content": "The collected data totally contains N triplets composing of hypothesis, source and reference segments, which is formed as D = { h i , s i , r i } N i=1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_49",
            "start": 337,
            "end": 489,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_49@3",
            "content": "Data Labeling After obtaining the synthetic data, the next step is to augment each data pair with a label which serves as the signal of unified pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_49",
            "start": 491,
            "end": 646,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_49@4",
            "content": "To stabilize the model training, as well as normalize the distributions across all score systems and languages, we propose a novel rankingbased approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_49",
            "start": 648,
            "end": 800,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_49@5",
            "content": "This method is based on the idea of Borda count (Ho et al., 1994;Emerson, 2013), which provides more precise and well-distributed synthetic data labels than Z-score normalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_49",
            "start": 802,
            "end": 980,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_50@0",
            "content": "To be concrete, for each data item, we first use existing evaluation approach to give prediction qi for each item, yielding labeled synthetic quadruple examples formed as D = { h i , s i , r i , qi } N i=1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_50",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_50@1",
            "content": "After that, we descendingly tag each example with their rank index qi referring to qi :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_50",
            "start": 208,
            "end": 294,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_51@0",
            "content": "qi = IndexOf(q i , Q), (9",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_51",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_52@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_52",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_53@0",
            "content": "where Q is the list storing all the sorted qi descendingly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_53",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_53@1",
            "content": "Then, we use the conventional Z-score strategy to normalize the scores:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_53",
            "start": 60,
            "end": 130,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_54@0",
            "content": "q i = qi \u2212 \u00b5 \u03c3 ,(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_54",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_55@0",
            "content": "where \u00b5 and \u03c3 are the mean and the standard deviation of values in Q, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_55",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_55@1",
            "content": "The dataset thus updates its format to D = { h i , s i , r i , q i } N i=1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_55",
            "start": 84,
            "end": 159,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_55@2",
            "content": "Note here that, an example with higher qi is assigned with higher qi , thus a larger value of q i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_55",
            "start": 161,
            "end": 259,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_55@3",
            "content": "Compared to related approaches which apply Zscore normalization (Bojar et al., 2018), or leave the conventional labeled scores as signals for learning (i.e., knowledge distillation, Kim and Rush, 2016;Phuong and Lampert, 2019), our approach can alleviate the bias of chosen model for labeling and prior distributional disagreement of scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_55",
            "start": 261,
            "end": 601,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_55@4",
            "content": "For example, different existing methods may output scores with different distributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_55",
            "start": 603,
            "end": 689,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_55@5",
            "content": "Especially for translation directions of low-resource, scores may follow skewed distribution (Sellam et al., 2020a), which has a disagreement with rich-resource scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_55",
            "start": 691,
            "end": 861,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_55@6",
            "content": "Our method can unify the distribution of all labeling data into the same scale, which can also be easily applied by the ensembling strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_55",
            "start": 863,
            "end": 1002,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_56@0",
            "content": "To unify all evaluation scenarios into one model, we apply multi-task learning for both pretraining and finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_56",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_56@1",
            "content": "For each step, we arrange three substeps for all input formats, yielding L REF , L SRC , and L SRC+REF , respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_56",
            "start": 116,
            "end": 233,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_56@2",
            "content": "The final learning objective is to reduce the summation of all losses:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_56",
            "start": 235,
            "end": 304,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_57@0",
            "content": "L = L REF + L SRC + L SRC+REF . (11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_57",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_58@0",
            "content": "4 Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_58",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_59@0",
            "content": "Experimental Settings",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_59",
            "start": 0,
            "end": 20,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_60@0",
            "content": "Benchmarks Following previous work (Rei et al., 2020;Yuan et al., 2021), we examine the effectiveness of the propose method on WMT 2019 Metrics tasks (Ma et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_60",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_60@1",
            "content": "We follow the common practice in COMET 3 (Rei et al., 2020) to collect and preprocess the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_60",
            "start": 169,
            "end": 266,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_60@2",
            "content": "The official variant of Kendall's Tau correlation (Ma et al., 2019) is used for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_60",
            "start": 268,
            "end": 358,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_60@3",
            "content": "We evaluate our methods on all of REF , SRC and SRC+REF scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_60",
            "start": 360,
            "end": 425,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_60@4",
            "content": "For SRC scenario, we further evaluate our method on WMT 2020 QE tasks (Specia et al., 2020), where we follow Ranasinghe et al. (2020a) for data collection and preprocessing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_60",
            "start": 427,
            "end": 599,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_61@0",
            "content": "Pretraining Data As mentioned in \u00a73.3, we continuously pretrain PLMs using synthetic data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_61",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_61@1",
            "content": "The data is constructed from WMT 2021 Cz-En, De-En, Ja-En, Ru-En, and Zh-En machine translation training sets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_61",
            "start": 91,
            "end": 200,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_61@2",
            "content": "Specifically, we follow Sellam et al. (2020a) to use Transformer-base (Vaswani et al., 2017) model to generate translation candidates, and use the checkpoints trained via UniTE-MRA approach for synthetic data labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_61",
            "start": 202,
            "end": 419,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_61@3",
            "content": "We pretrain two kinds of models, one is the English version which is pretrained on English-targeted language directions, the other is a multilingual version trained using bidirectional data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_61",
            "start": 421,
            "end": 610,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_61@4",
            "content": "Note that, for a fair comparison, we filter out all pretraining examples that are involved in benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_61",
            "start": 612,
            "end": 716,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_61@5",
            "content": "4 Model Setting We implement our approach upon COMET (Rei et al., 2020) and Unified Pretraining ( \u00a7 3.3), we also extend the latter with multilingual-targeted unified pretraining, thus obtaining UniTE-MUP model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_61",
            "start": 718,
            "end": 928,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_62@0",
            "content": "Baselines For REF approaches, we select BLEU (Papineni et al., 2002), ChrF (Popovic, 2015), YiSi-1 (Lo, 2019), BERTScore (Zhang et al., 2020), BLEURT (Sellam et al., 2020a), PRISM-ref (Thompson and Post, 2020), XLM-R+Concat (Takahashi et al., 2020), RoBERTa+Concat (Takahashi et al., 2020), and BARTScore (Yuan et al., 2021) for comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_62",
            "start": 0,
            "end": 339,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_62@1",
            "content": "As to SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al., 2020), PRISMsrc (Thompson and Post, 2020) and multilingual-tomultilingual version of MTransQuest (Ranasinghe et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_62",
            "start": 341,
            "end": 591,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_62@2",
            "content": "For SRC+REF scenario, we use XLM-R+Concat (Takahashi et al., 2020) and COMET (Rei et al., 2020) as strong baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_62",
            "start": 593,
            "end": 708,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_63@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_63",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_64@0",
            "content": "English-Targeted Experimental results on metric task are conducted in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_64",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_64@1",
            "content": "For REF methods, BARTScore (Yuan et al., 2021) performs better than other statistical and model-based metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_64",
            "start": 79,
            "end": 188,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_64@2",
            "content": "MTransQuest (Ranasinghe et al., 2020b) gives dominant performance on SRC scenario, and COMET (Rei et al., 2020) performs better than XLM-R+Concat on SRC+REF scenario.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_64",
            "start": 190,
            "end": 355,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_65@0",
            "content": "As for our methods, UniTE-MRA approach achieves better results on all tasks, demonstrating the effectiveness of monotonic attention flow for cross-lingual interactions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_65",
            "start": 0,
            "end": 167,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_65@1",
            "content": "Moreover, our proposed model UniTE-UP, which unifies both pretraining and finetuning, can yield better results following all evaluation settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_65",
            "start": 169,
            "end": 313,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_65@2",
            "content": "Most importantly, UniTE-UP is a single model which surpasses all the different state-of-the-art models on three tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_65",
            "start": 315,
            "end": 432,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_66@0",
            "content": "En-Cs En-De En-Ru En-Zh En-Fi En-Gu En-Kk En-Lt De-Cs De-Fr Fr-De Avg.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_66",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_67@0",
            "content": "Reference-only Evaluation \u2665 BLEU (Papineni et al., 2002) 36",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_67",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_68@0",
            "content": "As seen in Table 2, our multilingual-targeted version model, a single model UniTE-MUP, still gives dominant performance than all different strong baselines on REF , SRC and SRC+REF .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_68",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_68@1",
            "content": "These results further demonstrate the transferability and effectiveness of our approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_68",
            "start": 183,
            "end": 270,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_68@2",
            "content": "Besides, it is encouraging to see that the UniTE-UP can also give dominant results, revealing an improvement of 0.6, 0.3 and 0.9 averaged Kendall's \u03c4 correlation scores, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_68",
            "start": 272,
            "end": 454,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_68@3",
            "content": "Our further comparison indicates that UniTE-MUP also outperforms previous strong baselines but slightly worse than UniTE-UP on English-targeted translation directions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_68",
            "start": 456,
            "end": 622,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_68@4",
            "content": "5 We think the reason lies in the curse of multilingualism and vocabulary dilution (Conneau et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_68",
            "start": 624,
            "end": 729,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_68@5",
            "content": "Moreover, we also testify the performance of UniTE-MUP on WMT 2020 QE tasks via finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_68",
            "start": 731,
            "end": 821,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_68@6",
            "content": "Results 6 demonstrate that our method can also achieve competitive results over WMT 2020 QE task compared with the winner submission (Ranasinghe et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_68",
            "start": 823,
            "end": 982,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_69@0",
            "content": "Ablation Studies",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_69",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_70@0",
            "content": "In this section, we conduct ablation studies to investigate the effectiveness of regional attention patterns ( \u00a75.1), unified training ( \u00a75.2), and rankingbased data labeling ( \u00a75.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_70",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_70@1",
            "content": "All experiments follow English-targeted setting on SRC+REF task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_70",
            "start": 184,
            "end": 247,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_71@0",
            "content": "Regional Attention Patterns",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_71",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_72@0",
            "content": "To investigate the effectiveness of MRA, we further collect experiments for comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_72",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_72@1",
            "content": "As seen in Table 3, MRA can give performance improvements than full attention, and preventing the interactions between hypothesis and source segment can improve the performance most.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_72",
            "start": 88,
            "end": 269,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_72@2",
            "content": "We think the reasons behind are twofold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_72",
            "start": 271,
            "end": 310,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_72@3",
            "content": "First, the source side is formed with a different language, whose semantic information is rather weak than the reference side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_72",
            "start": 312,
            "end": 437,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_72@4",
            "content": "Second, by preventing direct interactions between source and hypothesis, semantics inside the former must be passed through reference, which is helpful for disambiguation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_72",
            "start": 439,
            "end": 609,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_73@0",
            "content": "Besides, not allowing the source to derive information from the hypothesis is better than the opposite direction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_73",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_73@1",
            "content": "As Wang and Chen (2020) found that the positional embeddings in PLM are engaged with strong adjacent information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_73",
            "start": 114,
            "end": 226,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_73@2",
            "content": "We think the reason why S\u2192H performs worse than H\u2192S lies in the skipping of indexes, which corrupts positional similarities in alignment calculation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_73",
            "start": 228,
            "end": 376,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_74@0",
            "content": "Additionally, when we combined two methods together, i.e., unified pretraining and finetuning with SRC+REF UniTE-MRA setting, model performance drops to 34.9 over English-targeted tasks on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_74",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_74@1",
            "content": "We think that both methods all intend to solve the problem of unseen SRC+REF input format, and MRA may not be necessary if massive data examples can be obtained for pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_74",
            "start": 198,
            "end": 374,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_74@2",
            "content": "Nevertheless, UniTE-MRA has its advantage on wide application without requiring pseduo labeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_74",
            "start": 376,
            "end": 475,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_75@0",
            "content": "Unified Training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_75",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_76@0",
            "content": "Experiments for comparing unified and taskspecific training are concluded in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_76",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_76@1",
            "content": "As seen, when using the unified pretraining checkpoint to finetune over the specific task, performance over three models reveals performance drop consistently, indicating the unified finetuning is helpful for model learning, and SRC , REF and SRC+REF tasks are complementary to each other during learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_76",
            "start": 86,
            "end": 390,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_76@2",
            "content": "Also, utilizing task-specific pretraining instead of unified one reveals worse performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_76",
            "start": 392,
            "end": 482,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_76@3",
            "content": "To sum up, unifying both pretraining and finetuning only reveals one model, showing its advantage on the generalization on all tasks, where one united model can cover all functionalities of REF , SRC and SRC+REF tasks concurrently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_76",
            "start": 484,
            "end": 714,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_77@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_77",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_78@0",
            "content": "In the past decades, automatic translation evaluation is mainly divided into REF , SRC and SRC+REF tasks, each of which develops independently and is tackled by various task-specific methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_78",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_78@1",
            "content": "We suggest that the three tasks are possibly handled by a unified framework, thus being ease of use and facilitating the knowledge transferring.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_78",
            "start": 192,
            "end": 335,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_78@2",
            "content": "Contributions of our work are mainly in three folds: (a) We propose a flexible and unified translation evaluation model UniTE, which can be adopted into the three tasks at once; (b) Through in-depth analyses, we point out that the main challenge of unifying three tasks stems from the discrepancy between vanilla pretraining and multi-tasks finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_78",
            "start": 337,
            "end": 688,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_78@3",
            "content": "We further fill this gap via introducing MRA and Unified Pretraining; (c) Our single model consistently outperforms a variety of state-of-the-art or winner systems across tasks in WMT 2019 Metrics and WMT 2020 QE benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_78",
            "start": 690,
            "end": 913,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_78@4",
            "content": "We hope our new insights can contribute to subsequent studies in the translation evaluation community.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_78",
            "start": 915,
            "end": 1016,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_79@0",
            "content": "Considering the English-targeted model, we select Czech (Cz), German (De), Japanese (Ja), Russian (Ru), and Chinese (Zh) as source languages, and English (En) as target.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_79",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_79@1",
            "content": "For each translation direction, we collect 1 million samples, finally yielding 5 million examples in total for unified pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_79",
            "start": 170,
            "end": 300,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_79@2",
            "content": "As to the multilingual-targeted model, we further collect 1 million synthetic data for each language direction of En-Cz, En-De, En-Ja, En-Ru, and En-Zh.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_79",
            "start": 302,
            "end": 453,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_79@3",
            "content": "Finally, we construct 10 million examples for the pretraining of the multilingual version by adding the data of the English-targeted model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_79",
            "start": 455,
            "end": 593,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_79@4",
            "content": "Note that, for a fair comparison, we filter out all pretraining examples that are involved in benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_79",
            "start": 595,
            "end": 699,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_80@0",
            "content": "After trying several pooling methods which derive sequence-level representations, we use the representations located at the start of sequence as the input of feedforward network (Ranasinghe et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_80",
            "start": 0,
            "end": 204,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_80@1",
            "content": "The feedforward network consists of 3 linear transitions, where the dimensionalities of corresponding outputs are 3,072, 1,024, and 1, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_80",
            "start": 206,
            "end": 353,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_80@2",
            "content": "Between any two adjacent linear modules in feedforward, hyperbolic tangent function is arranged as activation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_80",
            "start": 355,
            "end": 464,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_81@0",
            "content": "For give more details for comparison, we collect the results of UniTE-MUP over English-targeted experiments in Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_81",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_82@0",
            "content": "The results for UniTE approach on WMT 2020 QE task are concluded in Table 7.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_82",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_82@1",
            "content": "As seen, our approach can give better performance than strong QE baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_82",
            "start": 77,
            "end": 151,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_83@0",
            "content": "All the models reported in this paper were finetuned on a single Nvidia V100 (32GB) GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_83",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_83@1",
            "content": "Specifically for UniTE-UP and UniTE-MUP, the pretraining is arranged on 4 Nvidia V100 (32GB) GPUs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_83",
            "start": 89,
            "end": 186,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_83@2",
            "content": "Our framework is built upon COMET repository (Rei et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_83",
            "start": 188,
            "end": 251,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_83@3",
            "content": "For the contribution to the research community, we will release both the source code of UniTE framework and the well-trained evaluation models including UniTE-UP and UniTE-MUP checkpoints as described in this paper upon the acceptance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_83",
            "start": 253,
            "end": 487,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_84@0",
            "content": "UNKNOWN, None, , Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 Conference on Machine Translation WMT20. In Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_84",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_85@0",
            "content": "Ondrej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Philipp Koehn, Christof Monz, Findings of the 2018 Conference on Machine Translation, 2018, WMT, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_85",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_86@0",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised Cross-lingual Representation Learning at Scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_86",
            "start": 0,
            "end": 328,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_87@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL:HLT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_87",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_88@0",
            "content": "Peter Emerson, The Original Borda Count and Partial Voting, 2013, Social Choice and Welfare, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_88",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_89@0",
            "content": "Erick Fonseca, Lisa Yankovskaya, F Andr\u00e9, Mark Martins, Christian Fishel,  Federmann, Findings of the WMT 2019 Shared Tasks on Quality Estimation, 2019, Proceedings of the Fourth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_89",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_90@0",
            "content": "Markus Freitag, David Grangier, Isaac Caswell, BLEU might be guilty but references are not innocent, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_90",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_91@0",
            "content": "Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, Tie-Yan Liu, Layer-wise coordination between encoder and decoder for neural machine translation, 2018, Proceedings of the 32nd International Conference on Neural Information Processing Systems (NeurIPS), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_91",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_92@0",
            "content": "Kam Tin, Jonathan Ho, Sargur Hull,  Srihari, Decision Combination in Multiple Classifier Systems, 1994, IEEE transactions on Pattern Analysis and Machine Intelligence (TPAMI), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_92",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_93@0",
            "content": "Ganesh Jawahar, Beno\u00eet Sagot, Djam\u00e9 Seddah, What Does BERT Learn about the Structure of Language?, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_93",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_94@0",
            "content": "Fabio Kepler, Jonay Tr\u00e9nous, Marcos Treviso, Miguel Vera, Andr\u00e9 Martins, OpenKiwi: An Open Source Framework for Quality Estimation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL Demo), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_94",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_95@0",
            "content": "Yoon Kim, Alexander M Rush, Sequence-Level Knowledge Distillation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_95",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_96@0",
            "content": "Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation, 2021, Proceedings of the Seventh Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_96",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_97@0",
            "content": "Yongjie Lin, Yi Chern Tan, Robert Frank, Open Sesame: Getting inside BERT's Linguistic Knowledge, 2019, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP@ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_97",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_98@0",
            "content": "Chi-Kiu Lo, YiSi -a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources, 2019, Proceedings of the Fourth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_98",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_99@0",
            "content": "Qingsong Ma, Johnny Wei, Ond\u0159ej Bojar, Yvette Graham, Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges, 2019, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_99",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_100@0",
            "content": "Nitika Mathur, Timothy Baldwin, Trevor Cohn, Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_100",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_101@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a Method for Automatic Evaluation of Machine Translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_101",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_102@0",
            "content": "Mary Phuong, Christoph Lampert, Towards Understanding Knowledge Distillation, 2019, Proceedings of the 36th International Conference on Machine Learning (ICML), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_102",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_103@0",
            "content": "Maja Popovic, chrF: character n-gram F-score for automatic MT evaluation, 2015, Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_103",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_104@0",
            "content": "Maja Popovic, chrF++: words helping character n-grams, 2017, Proceedings of the Second Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_104",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_105@0",
            "content": "UNKNOWN, None, 2020, Intelligent Translation Memory Matching and Retrieval with Sentence Encoders, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_105",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_106@0",
            "content": "Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov, TransQuest: Translation Quality Estimation with Cross-lingual Transformers, 2020, Proceedings of the 28th International Conference on Computational Linguistics (COLING), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_106",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_107@0",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, COMET: A neural framework for MT evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_107",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_108@0",
            "content": "Anna Rogers, Olga Kovaleva, Anna Rumshisky, A Primer in BERTology: What We Know About How BERT Works, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_108",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_109@0",
            "content": "Thibault Sellam, Dipanjan Das, Ankur Parikh, BLEURT: learning robust metrics for text generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_109",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_110@0",
            "content": "Thibault Sellam, Amy Pu,  Hyung Won, Sebastian Chung, Qijun Gehrmann, Markus Tan, Dipanjan Freitag, Ankur Das,  Parikh, Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task, 2020, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_110",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_111@0",
            "content": "Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, John Makhoul, A Study of Translation Edit Rate with Targeted Human Annotation, 2006, Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers (ATMA), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_111",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_112@0",
            "content": "Lucia Specia, Fr\u00e9d\u00e9ric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzm\u00e1n, Andr\u00e9 Martins, Findings of the WMT 2020 Shared Task on Quality Estimation, 2020, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_112",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_113@0",
            "content": "Kosuke Takahashi, Katsuhito Sudoh, Satoshi Nakamura, Automatic Machine Translation Evaluation using Source Language Inputs and Crosslingual Language Model, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_113",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_114@0",
            "content": "Ian Tenney, Dipanjan Das, Ellie Pavlick, BERT Rediscovers the Classical NLP Pipeline, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_114",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_115@0",
            "content": "Brian Thompson, Matt Post, Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_115",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_116@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention Is All You Need, 2017, Advances in Neural Information Processing Systems (NIPS), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_116",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_117@0",
            "content": "Yu-An Wang, Yun-Nung Chen, What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_117",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_118@0",
            "content": "Weizhe Yuan, Graham Neubig, Pengfei Liu, BARTScore: Evaluating Generated Text as Text Generation, 2021, Advances in Neural Information Processing Systems (NeurIPS), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_118",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_119@0",
            "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, Yoav Artzi, BERTScore: Evaluating Text Generation with BERT, 2020, 8th International Conference on Learning Representations (ICLR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_119",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "151-ARR_v1_120@0",
            "content": "Wei Zhao, Goran Glava\u0161, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger, On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v1_120",
            "start": 0,
            "end": 284,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "151-ARR_v1_0",
            "tgt_ix": "151-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_0",
            "tgt_ix": "151-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_1",
            "tgt_ix": "151-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_1",
            "tgt_ix": "151-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_0",
            "tgt_ix": "151-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_2",
            "tgt_ix": "151-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_4",
            "tgt_ix": "151-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_5",
            "tgt_ix": "151-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_6",
            "tgt_ix": "151-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_3",
            "tgt_ix": "151-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_3",
            "tgt_ix": "151-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_3",
            "tgt_ix": "151-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_3",
            "tgt_ix": "151-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_3",
            "tgt_ix": "151-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_0",
            "tgt_ix": "151-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_7",
            "tgt_ix": "151-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_8",
            "tgt_ix": "151-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_8",
            "tgt_ix": "151-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_8",
            "tgt_ix": "151-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_9",
            "tgt_ix": "151-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_11",
            "tgt_ix": "151-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_10",
            "tgt_ix": "151-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_10",
            "tgt_ix": "151-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_10",
            "tgt_ix": "151-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_8",
            "tgt_ix": "151-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_12",
            "tgt_ix": "151-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_13",
            "tgt_ix": "151-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_13",
            "tgt_ix": "151-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_8",
            "tgt_ix": "151-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_14",
            "tgt_ix": "151-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_15",
            "tgt_ix": "151-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_15",
            "tgt_ix": "151-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_0",
            "tgt_ix": "151-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_16",
            "tgt_ix": "151-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_17",
            "tgt_ix": "151-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_17",
            "tgt_ix": "151-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_17",
            "tgt_ix": "151-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_18",
            "tgt_ix": "151-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_20",
            "tgt_ix": "151-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_21",
            "tgt_ix": "151-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_22",
            "tgt_ix": "151-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_23",
            "tgt_ix": "151-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_24",
            "tgt_ix": "151-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_25",
            "tgt_ix": "151-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_26",
            "tgt_ix": "151-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_27",
            "tgt_ix": "151-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_28",
            "tgt_ix": "151-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_29",
            "tgt_ix": "151-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_30",
            "tgt_ix": "151-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_31",
            "tgt_ix": "151-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_32",
            "tgt_ix": "151-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_17",
            "tgt_ix": "151-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_33",
            "tgt_ix": "151-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_35",
            "tgt_ix": "151-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_36",
            "tgt_ix": "151-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_37",
            "tgt_ix": "151-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_38",
            "tgt_ix": "151-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_39",
            "tgt_ix": "151-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_40",
            "tgt_ix": "151-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_41",
            "tgt_ix": "151-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_42",
            "tgt_ix": "151-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_43",
            "tgt_ix": "151-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_44",
            "tgt_ix": "151-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_17",
            "tgt_ix": "151-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_46",
            "tgt_ix": "151-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_48",
            "tgt_ix": "151-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_49",
            "tgt_ix": "151-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_50",
            "tgt_ix": "151-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_51",
            "tgt_ix": "151-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_52",
            "tgt_ix": "151-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_53",
            "tgt_ix": "151-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_54",
            "tgt_ix": "151-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_56",
            "tgt_ix": "151-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_57",
            "tgt_ix": "151-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_55",
            "tgt_ix": "151-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_0",
            "tgt_ix": "151-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_58",
            "tgt_ix": "151-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_60",
            "tgt_ix": "151-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_61",
            "tgt_ix": "151-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_59",
            "tgt_ix": "151-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_59",
            "tgt_ix": "151-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_59",
            "tgt_ix": "151-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_59",
            "tgt_ix": "151-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_0",
            "tgt_ix": "151-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_62",
            "tgt_ix": "151-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_64",
            "tgt_ix": "151-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_63",
            "tgt_ix": "151-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_63",
            "tgt_ix": "151-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_63",
            "tgt_ix": "151-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_66",
            "tgt_ix": "151-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_63",
            "tgt_ix": "151-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_63",
            "tgt_ix": "151-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_65",
            "tgt_ix": "151-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_63",
            "tgt_ix": "151-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_67",
            "tgt_ix": "151-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_0",
            "tgt_ix": "151-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_68",
            "tgt_ix": "151-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_69",
            "tgt_ix": "151-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_69",
            "tgt_ix": "151-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_69",
            "tgt_ix": "151-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_70",
            "tgt_ix": "151-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_72",
            "tgt_ix": "151-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_73",
            "tgt_ix": "151-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_71",
            "tgt_ix": "151-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_71",
            "tgt_ix": "151-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_71",
            "tgt_ix": "151-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_71",
            "tgt_ix": "151-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_69",
            "tgt_ix": "151-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_74",
            "tgt_ix": "151-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_75",
            "tgt_ix": "151-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_75",
            "tgt_ix": "151-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_0",
            "tgt_ix": "151-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_76",
            "tgt_ix": "151-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_77",
            "tgt_ix": "151-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_77",
            "tgt_ix": "151-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_77",
            "tgt_ix": "151-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_78",
            "tgt_ix": "151-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_77",
            "tgt_ix": "151-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_79",
            "tgt_ix": "151-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_77",
            "tgt_ix": "151-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_80",
            "tgt_ix": "151-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_77",
            "tgt_ix": "151-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_81",
            "tgt_ix": "151-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_77",
            "tgt_ix": "151-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_82",
            "tgt_ix": "151-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v1_0",
            "tgt_ix": "151-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_1",
            "tgt_ix": "151-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_2",
            "tgt_ix": "151-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_2",
            "tgt_ix": "151-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_2",
            "tgt_ix": "151-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_2",
            "tgt_ix": "151-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_2",
            "tgt_ix": "151-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_2",
            "tgt_ix": "151-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_2",
            "tgt_ix": "151-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_2",
            "tgt_ix": "151-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_3",
            "tgt_ix": "151-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_4",
            "tgt_ix": "151-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_4",
            "tgt_ix": "151-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_4",
            "tgt_ix": "151-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_4",
            "tgt_ix": "151-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_5",
            "tgt_ix": "151-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_5",
            "tgt_ix": "151-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_5",
            "tgt_ix": "151-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_5",
            "tgt_ix": "151-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_5",
            "tgt_ix": "151-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_6",
            "tgt_ix": "151-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_6",
            "tgt_ix": "151-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_6",
            "tgt_ix": "151-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_6",
            "tgt_ix": "151-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_6",
            "tgt_ix": "151-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_6",
            "tgt_ix": "151-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_6",
            "tgt_ix": "151-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_6",
            "tgt_ix": "151-ARR_v1_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_7",
            "tgt_ix": "151-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_7",
            "tgt_ix": "151-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_7",
            "tgt_ix": "151-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_7",
            "tgt_ix": "151-ARR_v1_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_7",
            "tgt_ix": "151-ARR_v1_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_8",
            "tgt_ix": "151-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_9",
            "tgt_ix": "151-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_10",
            "tgt_ix": "151-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_11",
            "tgt_ix": "151-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_11",
            "tgt_ix": "151-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_11",
            "tgt_ix": "151-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_11",
            "tgt_ix": "151-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_11",
            "tgt_ix": "151-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_12",
            "tgt_ix": "151-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_12",
            "tgt_ix": "151-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_12",
            "tgt_ix": "151-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_12",
            "tgt_ix": "151-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_13",
            "tgt_ix": "151-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_14",
            "tgt_ix": "151-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_14",
            "tgt_ix": "151-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_14",
            "tgt_ix": "151-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_14",
            "tgt_ix": "151-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_15",
            "tgt_ix": "151-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_16",
            "tgt_ix": "151-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_16",
            "tgt_ix": "151-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_16",
            "tgt_ix": "151-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_16",
            "tgt_ix": "151-ARR_v1_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_16",
            "tgt_ix": "151-ARR_v1_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_16",
            "tgt_ix": "151-ARR_v1_16@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_17",
            "tgt_ix": "151-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_18",
            "tgt_ix": "151-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_18",
            "tgt_ix": "151-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_18",
            "tgt_ix": "151-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_18",
            "tgt_ix": "151-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_19",
            "tgt_ix": "151-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_20",
            "tgt_ix": "151-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_20",
            "tgt_ix": "151-ARR_v1_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_21",
            "tgt_ix": "151-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_22",
            "tgt_ix": "151-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_23",
            "tgt_ix": "151-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_23",
            "tgt_ix": "151-ARR_v1_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_24",
            "tgt_ix": "151-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_25",
            "tgt_ix": "151-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_26",
            "tgt_ix": "151-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_26",
            "tgt_ix": "151-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_27",
            "tgt_ix": "151-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_27",
            "tgt_ix": "151-ARR_v1_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_27",
            "tgt_ix": "151-ARR_v1_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_27",
            "tgt_ix": "151-ARR_v1_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_27",
            "tgt_ix": "151-ARR_v1_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_28",
            "tgt_ix": "151-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_28",
            "tgt_ix": "151-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_29",
            "tgt_ix": "151-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_30",
            "tgt_ix": "151-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_31",
            "tgt_ix": "151-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_32",
            "tgt_ix": "151-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_33",
            "tgt_ix": "151-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_33",
            "tgt_ix": "151-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_33",
            "tgt_ix": "151-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_33",
            "tgt_ix": "151-ARR_v1_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_34",
            "tgt_ix": "151-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_35",
            "tgt_ix": "151-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_35",
            "tgt_ix": "151-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_35",
            "tgt_ix": "151-ARR_v1_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_35",
            "tgt_ix": "151-ARR_v1_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_36",
            "tgt_ix": "151-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_37",
            "tgt_ix": "151-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_38",
            "tgt_ix": "151-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_39",
            "tgt_ix": "151-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_39",
            "tgt_ix": "151-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_40",
            "tgt_ix": "151-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_41",
            "tgt_ix": "151-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_42",
            "tgt_ix": "151-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_42",
            "tgt_ix": "151-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_42",
            "tgt_ix": "151-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_43",
            "tgt_ix": "151-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_43",
            "tgt_ix": "151-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_44",
            "tgt_ix": "151-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_45",
            "tgt_ix": "151-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_46",
            "tgt_ix": "151-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_47",
            "tgt_ix": "151-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_48",
            "tgt_ix": "151-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_49",
            "tgt_ix": "151-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_49",
            "tgt_ix": "151-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_49",
            "tgt_ix": "151-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_49",
            "tgt_ix": "151-ARR_v1_49@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_49",
            "tgt_ix": "151-ARR_v1_49@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_49",
            "tgt_ix": "151-ARR_v1_49@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_50",
            "tgt_ix": "151-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_50",
            "tgt_ix": "151-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_51",
            "tgt_ix": "151-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_52",
            "tgt_ix": "151-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_53",
            "tgt_ix": "151-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_53",
            "tgt_ix": "151-ARR_v1_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_54",
            "tgt_ix": "151-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_55",
            "tgt_ix": "151-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_55",
            "tgt_ix": "151-ARR_v1_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_55",
            "tgt_ix": "151-ARR_v1_55@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_55",
            "tgt_ix": "151-ARR_v1_55@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_55",
            "tgt_ix": "151-ARR_v1_55@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_55",
            "tgt_ix": "151-ARR_v1_55@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_55",
            "tgt_ix": "151-ARR_v1_55@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_56",
            "tgt_ix": "151-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_56",
            "tgt_ix": "151-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_56",
            "tgt_ix": "151-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_57",
            "tgt_ix": "151-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_58",
            "tgt_ix": "151-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_59",
            "tgt_ix": "151-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_60",
            "tgt_ix": "151-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_60",
            "tgt_ix": "151-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_60",
            "tgt_ix": "151-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_60",
            "tgt_ix": "151-ARR_v1_60@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_60",
            "tgt_ix": "151-ARR_v1_60@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_61",
            "tgt_ix": "151-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_61",
            "tgt_ix": "151-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_61",
            "tgt_ix": "151-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_61",
            "tgt_ix": "151-ARR_v1_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_61",
            "tgt_ix": "151-ARR_v1_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_61",
            "tgt_ix": "151-ARR_v1_61@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_62",
            "tgt_ix": "151-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_62",
            "tgt_ix": "151-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_62",
            "tgt_ix": "151-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_63",
            "tgt_ix": "151-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_64",
            "tgt_ix": "151-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_64",
            "tgt_ix": "151-ARR_v1_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_64",
            "tgt_ix": "151-ARR_v1_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_65",
            "tgt_ix": "151-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_65",
            "tgt_ix": "151-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_65",
            "tgt_ix": "151-ARR_v1_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_66",
            "tgt_ix": "151-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_67",
            "tgt_ix": "151-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_68",
            "tgt_ix": "151-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_68",
            "tgt_ix": "151-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_68",
            "tgt_ix": "151-ARR_v1_68@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_68",
            "tgt_ix": "151-ARR_v1_68@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_68",
            "tgt_ix": "151-ARR_v1_68@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_68",
            "tgt_ix": "151-ARR_v1_68@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_68",
            "tgt_ix": "151-ARR_v1_68@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_69",
            "tgt_ix": "151-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_70",
            "tgt_ix": "151-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_70",
            "tgt_ix": "151-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_71",
            "tgt_ix": "151-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_72",
            "tgt_ix": "151-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_72",
            "tgt_ix": "151-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_72",
            "tgt_ix": "151-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_72",
            "tgt_ix": "151-ARR_v1_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_72",
            "tgt_ix": "151-ARR_v1_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_73",
            "tgt_ix": "151-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_73",
            "tgt_ix": "151-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_73",
            "tgt_ix": "151-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_74",
            "tgt_ix": "151-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_74",
            "tgt_ix": "151-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_74",
            "tgt_ix": "151-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_75",
            "tgt_ix": "151-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_76",
            "tgt_ix": "151-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_76",
            "tgt_ix": "151-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_76",
            "tgt_ix": "151-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_76",
            "tgt_ix": "151-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_77",
            "tgt_ix": "151-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_78",
            "tgt_ix": "151-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_78",
            "tgt_ix": "151-ARR_v1_78@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_78",
            "tgt_ix": "151-ARR_v1_78@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_78",
            "tgt_ix": "151-ARR_v1_78@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_78",
            "tgt_ix": "151-ARR_v1_78@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_79",
            "tgt_ix": "151-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_79",
            "tgt_ix": "151-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_79",
            "tgt_ix": "151-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_79",
            "tgt_ix": "151-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_79",
            "tgt_ix": "151-ARR_v1_79@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_80",
            "tgt_ix": "151-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_80",
            "tgt_ix": "151-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_80",
            "tgt_ix": "151-ARR_v1_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_81",
            "tgt_ix": "151-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_82",
            "tgt_ix": "151-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_82",
            "tgt_ix": "151-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_83",
            "tgt_ix": "151-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_83",
            "tgt_ix": "151-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_83",
            "tgt_ix": "151-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_83",
            "tgt_ix": "151-ARR_v1_83@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_84",
            "tgt_ix": "151-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_85",
            "tgt_ix": "151-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_86",
            "tgt_ix": "151-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_87",
            "tgt_ix": "151-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_88",
            "tgt_ix": "151-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_89",
            "tgt_ix": "151-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_90",
            "tgt_ix": "151-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_91",
            "tgt_ix": "151-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_92",
            "tgt_ix": "151-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_93",
            "tgt_ix": "151-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_94",
            "tgt_ix": "151-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_95",
            "tgt_ix": "151-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_96",
            "tgt_ix": "151-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_97",
            "tgt_ix": "151-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_98",
            "tgt_ix": "151-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_99",
            "tgt_ix": "151-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_100",
            "tgt_ix": "151-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_101",
            "tgt_ix": "151-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_102",
            "tgt_ix": "151-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_103",
            "tgt_ix": "151-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_104",
            "tgt_ix": "151-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_105",
            "tgt_ix": "151-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_106",
            "tgt_ix": "151-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_107",
            "tgt_ix": "151-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_108",
            "tgt_ix": "151-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_109",
            "tgt_ix": "151-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_110",
            "tgt_ix": "151-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_111",
            "tgt_ix": "151-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_112",
            "tgt_ix": "151-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_113",
            "tgt_ix": "151-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_114",
            "tgt_ix": "151-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_115",
            "tgt_ix": "151-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_116",
            "tgt_ix": "151-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_117",
            "tgt_ix": "151-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_118",
            "tgt_ix": "151-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_119",
            "tgt_ix": "151-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v1_120",
            "tgt_ix": "151-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1243,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "151-ARR",
        "version": 1
    }
}