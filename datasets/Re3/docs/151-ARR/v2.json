{
    "nodes": [
        {
            "ix": "151-ARR_v2_0",
            "content": "UniTE: Unified Translation Evaluation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_2",
            "content": "Translation quality evaluation plays a crucial role in machine translation. According to the input format, it is mainly separated into three tasks, i.e., reference-only, source-only and source-reference-combined. Recent methods, despite their promising results, are specifically designed and optimized on one of them. This limits the convenience of these methods, and overlooks the commonalities among tasks. In this paper, we propose UniTE, which is the first unified framework engaged with abilities to handle all three evaluation tasks. Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task learning. We testify our framework on WMT 2019 Metrics and WMT 2020 Quality Estimation benchmarks. Extensive analyses show that our single model can universally surpass various state-of-the-art or winner methods across tasks. Both source code and associated models are available at https://github.com/NLP2CT/UniTE.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "151-ARR_v2_4",
            "content": "Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020;Mathur et al., 2020a;Zhao et al., 2020;Kocmi et al., 2021). Based on the input contexts, translation evaluation can be mainly categorized into three classes: 1) reference-only evaluation ( REF ) approaches like BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi-2 (Lo, 2019) and TransQuest (Ranasinghe et al., 2020b), which are also referred as quality estimation (QE). These methods estimate the quality of the hypothesis based on the source sentence without using references; 3) source-reference-combined evaluation ( SRC+REF ) works like COMET (Rei et al., 2020), where the evaluation exploits information from both source and reference. With the help of powerful pretrained language models (PLMs, Devlin et al., 2019;Conneau et al., 2020), modelbased approaches (e.g., BLEURT, TransQuest, and COMET) have shown promising results in recent WMT competitions (Ma et al., 2019;Mathur et al., 2020b;Freitag et al., 2021;Fonseca et al., 2019;Specia et al., 2020Specia et al., , 2021.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_5",
            "content": "Nevertheless, each existing MT evaluation work is usually designed for one specific task, e.g., BLEURT is only used for REF task and can not support SRC and SRC+REF tasks. Moreover, those approaches preserve the same core -evaluating the quality of translation by referring to the given segments. We believe that it is valuable, as well as feasible, to unify the capabilities of all MT evaluation tasks ( REF , SRC and SRC+REF ) into one model. Among the promising advantages are ease of use and improved robustness through knowledge transfer across evaluation tasks. To achieve this, two important challenges need to be addressed: 1) How to design a model framework that can unify all translation evaluation tasks? 2) How to make the powerful PLMs better adapt to the unified evaluation model?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_6",
            "content": "In this paper, we propose UniTE -Unified Translation Evaluation, a novel approach which unifies the functionalities of REF , SRC and SRC+REF tasks into one model. To solve the first challenge as mentioned above, based on the multilingual PLM, we utilize layerwise coordination which concatenates all input segments into one sequence as the unified input form. To further unify the modeling of three evaluation tasks, we propose a novel Monotonic Regional Attention (MRA) strat-egy, which allows partial semantic flows for a specific evaluation task. For the second challenge, a multi-task learning-based unified pretraining is proposed. To be concrete, we collect the high-quality translations and degrade low-quality translations of NMT models as synthetic data. Then we propose a novel ranking-based data labeling strategy to provide the training signal. Finally, the multilingual PLM is continuously pretrained on synthetic dataset with multi-task learning manner. Besides, our proposed models, named UniTE-MRA and UniTE-UP respectively, can benefit from finetuning with human-annotated data over three tasks at once, not requiring extra task-specific training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_7",
            "content": "Experimental results demonstrate the superiority of UniTE. Compared to various strong baseline systems on each task, UniTE, which unifies REF , SRC and SRC+REF tasks into one single model, achieves consistently absolute improvements of Kendall's \u03c4 correlations at 1.1, 2.3 and 1.1 scores on English-targeted translation directions of WMT 2019 Metric Shared task (Fonseca et al., 2019), respectively. Meanwhile, after introducing multilingual-targeted support for our unified pretraining strategy, a single model named UniTE-MUP also gives dominant results against existing methods on non-English-targeted translation evaluation tasks. Furthermore, our method can also achieve competitive results over WMT 2020 QE task compared with the winner submission (Ranasinghe et al., 2020b). Ablation studies reveal that, the proposed MRA and unified pretraining strategies are both important for model performance, making the model preserve the outstanding performance and multi-task transferability concurrently.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_8",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "151-ARR_v2_9",
            "content": "In this section, we briefly introduce the three directions of translation evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_10",
            "content": "Reference-Only Evaluation",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "151-ARR_v2_11",
            "content": "REF assesses the translation quality via comparing the translation candidate and the given reference. In this setting, the two inputs are written in the same language, thus being easily applied in most of the metric tasks. In the early stages, statistical methods are dominant solutions due to their strengths in wide language support and intuitive design. These methods measure the surface text similarity for a range of linguistic features, including n-gram (BLEU, Papineni et al., 2002), token (TER, Snover et al., 2006), and character (ChrF & ChrF++, Popovic, 2015, 2017. However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020;Rei et al., 2020;Mathur et al., 2020a).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_12",
            "content": "Consequently, with the rapid development of PLMs, researchers have been paying their attention to model-based approaches. The basic idea of these studies is to collect sentence representations for similarity calculation (BERTScore, Zhang et al., 2020) or evaluating probabilistic confidence (PRISM-ref, Thompson and Post, 2020;BARTScore, Yuan et al., 2021). To further improve the model, Sellam et al. (2020a) pretrained a specific PLM for the translation evaluation (BLEURT), while Lo (2019) combined statistical and representative features (YiSi-1). Both these methods achieve higher correlations with human judgments than statistical counterparts.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_13",
            "content": "Source-Only Evaluation",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "151-ARR_v2_14",
            "content": "SRC , which also refers to quality estimation 1 , is an important translation evaluation task especially for the scenario where the ground-truth reference is unavailable. It takes the source-side sentence and the translation candidate as inputs for the quality estimation. To achieve this, the methods are required to model cross-lingual semantic alignments. Similar to reference-only evaluation, statistical-based (Ranasinghe et al., 2020b), model-based (TransQuest, Ranasinghe et al., 2020bPRISM-src, Thompson and Post, 2020), and feature combination (YiSi-2, Lo, 2019) are typical and advanced methods in this tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_15",
            "content": "Source-Reference-Combined Evaluation",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "151-ARR_v2_16",
            "content": "Aside from the above tasks that only consider either source or target side at one time, SRC+REF takes both source and reference sentences into account. In this way, methods in this context can evaluate the translation candidate via utilizing the features from both sides. As a rising paradigm among translation evaluation tasks, SRC+REF also profits from the development of cross-lingual PLMs. For example, finetuning PLMs over human-annotated datasets (COMET, Rei et al., 2020) achieves new state-of-the-art results among all evaluation approaches in WMT 2020 (Mathur et al., 2020b For SRC+REF , we show the hard design for monotonic regional attention. denotes the masked attention logits.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_17",
            "content": "Methodology",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "151-ARR_v2_18",
            "content": "As mentioned above, massive methods are proposed for different automatic evaluation tasks. On the one hand, it is inconvenient and expensive to develop and employ different metrics for different evaluation scenarios. On the other hand, separate models absolutely overlook the commonalities among these evaluation tasks, of which knowledge potentially benefits all three tasks. In order to fulfill the aim of unifying the functionalities on REF , SRC , and SRC+REF into one model, in this section, we introduce UniTE (Figure 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_19",
            "content": "Model Architecture",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "151-ARR_v2_20",
            "content": "By receiving a data example composing of hypothesis, source, and reference segment, UniTE first modifies it into concatenated sequence following the given setting as REF , SRC , or SRC+REF :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_21",
            "content": "x REF = Concat(h, r) \u2208 R (l h +lr) , x SRC = Concat(h, s) \u2208 R (l h +ls) ,(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_22",
            "content": "x SRC+REF = Concat(h, s, r) \u2208 R (l h +ls+lr) ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_23",
            "content": "where h, s and r are hypothesis, source and reference segments, with the corresponding sequence lengths being l h , l s and l r , respectively. The input sequence is then fed to PLM to derive representations H. Take REF as an example:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_24",
            "content": "HREF = PLM(x REF ) \u2208 R (l h +lr)\u00d7d , (2",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_25",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_26",
            "content": "where d is the model size of PLM. According to Ranasinghe et al. (2020b), we use the first output representation as the input of feedforward layer.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_27",
            "content": "Compared to existing methods (Zhang et al., 2020;Rei et al., 2020) which take sentence-level representations for evaluation, the advantages of our architecture design are as follows. First, our UniTE model can benefit from layer-coordinated semantical interactions inside every one of PLM layers, which is proven effective on capturing diverse linguistic features (He et al., 2018;Lin et al., 2019;Jawahar et al., 2019;Tenney et al., 2019;Rogers et al., 2020). Second, for the unified approach of our model, the concatenation provides the unifying format for all task inputs, turning our model into a more general architecture. When conducting different evaluation tasks, our model requires no further modification inside. Note here, to keep the consistency across all evaluation tasks, as well as ease the unified learning, h is always located at the beginning of the input sequence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_28",
            "content": "After deriving HREF , a pooling block is arranged after PLM which gives sequence-level representations H REF . Finally, a feedforward network takes H REF as input, and gives a scalar p as prediction:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_29",
            "content": "H REF = Pool( HREF ) \u2208 R d ,(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_30",
            "content": "p REF = FeedForward(H REF ) \u2208 R 1 . (4",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_31",
            "content": ")",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_32",
            "content": "For training, we encourage the model to reduce the mean squared error with respect to given score q:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_33",
            "content": "L REF = (p REF \u2212 q) 2 .",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_34",
            "content": "(5)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_35",
            "content": "However, for the pretraining of most PLMs (e,g., XLM-R, Conneau et al., 2020), the input patterns are designed to receive two segments at most. Thus there exists a gap between the pretraining of PLM and the joint training of UniTE where the concatenation of three fragments is used as input. Moreover, previous study (Takahashi et al., 2020) shows that directly training over SRC+REF by following such design leads to worse performance than REF scenario. To alleviate this issue, we propose two strategies: Monotonic Regional Attention as described in \u00a73.2 and Unified Pretraining in \u00a73.3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_36",
            "content": "Monotonic Regional Attention",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "151-ARR_v2_37",
            "content": "To fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks. Following this, we propose to modify the attention mask of SRC+REF to simulate the modeling of two segments in SRC and REF . Specifically, when calculating the attention logits, semantics from a specific segment are only allowed to derive information from two segments at most. Considering the conventional attention module:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_38",
            "content": "A = Softmax( QK \u221a d ) \u2208 R L\u00d7L ,(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_39",
            "content": "where L is the sequential length for input, Q, K \u2208 R L\u00d7d are query and key representations, respectively. 2 As to monotonic regional attention (MRA), we simply add a mask M to the softmax logits to control attention flows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_40",
            "content": "A = Softmax( QK \u221a d + M) \u2208 R L\u00d7L , (7) M ij = \u2212\u221e (i, j) \u2208 U, 0 otherwise,(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_41",
            "content": "where U stores the index pairs of all masked areas. Following this idea, the key of MRA is how to design the matrix U. For the cases where interactions inside each segment, we believe that these self-interactions are beneficial to the modeling. For other cases where interactions are arranged across segments, three patterns are included: hypothesisreference, source-reference, and hypothesis-source. Intuitively, the former two parts are beneficial for model training, since they might contribute the monolingual signals and cross-lingual disambiguation to evaluation, respectively. This leaves the only case, where our experimental analysis also verifies (see \u00a75.1), that interaction between hypothesis and source leads to the performance decrease for SRC+REF task, thus troubling the unifying.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_42",
            "content": "2 For simplicity, we omit the multi-head mechanism. Figure 2: Attention flows in monotonic regional attention. h, s and r are hypothesis, source and reference, respectively. We prevent specified interactions in SRC+REF training via modifying the attention mask with regional properties. We show the hard (left) and soft design (right, no h \u2192 s) in this figure.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_43",
            "content": "To give more fine-grained designs, we propose two approaches for UniTE-MRA, which apply the MRA mechanism into UniTE model (Figure 2):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_44",
            "content": "\u2022 Hard MRA. Only monotonic attention flows are allowed. Interactions between any two segments are strictly unidirectional through the entire PLM, where U stores the index pairs of unidirectional interactions of h \u2192 r, s \u2192 r and h \u2192 s, where \"\u2192\" denotes the direction of attention flows. \u2022 Soft MRA. Specific attention flows are forbidden inside each attention module. The involved two segments may interact inside a higher layer. In practice, index pairs which denoting h \u2192 s or s \u2192 h between source and hypothesis are stored in U.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_45",
            "content": "Note that, although the processing in source and reference may be affected because their positions are not indexed from the start, related studies on positional embeddings reveal that, PLM can well capture relative positional information (Wang and Chen, 2020), which dispels this concern.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_46",
            "content": "Unified Pretraining",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "151-ARR_v2_47",
            "content": "To further bridge the modeling gap between PLM and the joint training of UniTE mentioned in \u00a73.1, we propose a unified pretraining strategy including the following main stages: 1) collecting and downgrading synthetic data; 2) labeling examples with a novel ranking-based strategy; 3) multi-task learning for unified pretraining and finetuning.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_48",
            "content": "Synthetic Data Collection As our approach aims at evaluating the quality of translations, generated hypotheses with NMT models are ideal synthetic data. To further improve the diversity of synthetic data quality, we follow existing experiences (Sellam et al., 2020a;Wan et al., 2021) to apply the word and span dropping strategy to downgrade a portion of hypotheses. The collected data totally contains N triplets composing of hypothesis, source and reference segments, which is formed as D = { h i , s i , r i } N i=1 . Data Labeling After obtaining the synthetic data, the next step is to augment each data pair with a label which serves as the signal of unified pretraining. To stabilize the model training, as well as normalize the distributions across all score systems and languages, we propose a novel rankingbased approach. This method is based on the idea of Borda count (Ho et al., 1994;Emerson, 2013), which provides more precise and well-distributed synthetic data labels than Z-score normalization.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_49",
            "content": "Specifically, we first use available approaches to derive the predicted score qi for each item, yielding labeled synthetic quadruple examples formed as D = { h i , s i , r i , qi } N i=1 . Then, we tag each example with its rank index qi referring to qi :",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_50",
            "content": "qi = IndexOf(q i , Q),(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_51",
            "content": "where Q is the list storing all the sorted qi descendingly. Then, we use the conventional Z-score strategy to normalize the scores:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_52",
            "content": "q i = qi \u2212 \u00b5 \u03c3 ,(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_53",
            "content": "where \u00b5 and \u03c3 are the mean and the standard deviation of values in Q, respectively. The dataset thus updates its format to D = { h i , s i , r i , q i } N i=1 . Note here that, an example with higher qi is assigned with higher qi , thus a larger value of q i . Compared to related approaches which apply Zscore normalization (Bojar et al., 2018), or leave the conventional labeled scores as signals for learning (i.e., knowledge distillation, Kim and Rush, 2016;Phuong and Lampert, 2019), our approach can alleviate the bias of chosen model for labeling and prior distributional disagreement of scores. For example, different methods may give scores with different distributions. Especially for translation directions of low-resource, scores may follow skewed distribution (Sellam et al., 2020a), which has a disagreement with rich-resource scenarios. Our method can unify the distribution of all labeling data into the same scale, which can also be easily applied by the ensembling strategy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_54",
            "content": "To unify all evaluation scenarios into one model, we apply multi-task learning for both pretraining and finetuning. For each step, we arrange three substeps for all input formats, yielding L REF , L SRC , and L SRC+REF , respectively. The final learning objective is to reduce the summation of all losses: (Ma et al., 2019). For the former, we follow the common practice in COMET 3 (Rei et al., 2020) to collect and preprocess the dataset. The official variant of Kendall's Tau correlation (Ma et al., 2019) is used for evaluation. We evaluate our methods on all of REF , SRC and SRC+REF scenarios.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_55",
            "content": "L = L REF + L SRC + L SRC+REF .(11",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_56",
            "content": "For SRC scenario, we further conduct results on WMT 2020 QE task (Specia et al., 2020) referring to Ranasinghe et al. (2020a) for data collection and preprocessing. Following the official report, the Pearson's correlation is used for evaluation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_57",
            "content": "Model Pretraining As mentioned in \u00a73.3, we continuously pretrain PLMs using synthetic data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_58",
            "content": "The data is constructed from WMT 2021 News Translation task, where we collect the training sets from five translation tasks. Among those tasks, the target sentences are all in English (En), and the source languages are Czech (Cs), German (De), Japanese (Ja), Russian (Ru), and Chinese (Zh). Specifically, we follow Sellam et al. (2020a) to use TRANSFORMER-base (Vaswani et al., 2017) MT models to generate translation candidates, and use the checkpoints trained via UniTE-MRA approach for synthetic data labeling. We pretrain two kinds of models, one is pretrained on English-targeted language directions, and the other is a multilingual version trained using bidirectional data. Note that, for a fair comparison, we filter out all pretraining examples that are involved in benchmarks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_59",
            "content": "Model Setting We implement our approach upon COMET (Rei et al., 2020) Baselines As to REF approaches, we select BLEU (Papineni et al., 2002), ChrF (Popovic, 2015), YiSi-1 (Lo, 2019), BERTScore (Zhang et al., 2020), BLEURT (Sellam et al., 2020a), PRISM-ref (Thompson and Post, 2020), BARTScore (Yuan et al., 2021), XLM-R+Concat (Takahashi et al., 2020), and RoBERTa+Concat (Takahashi et al., 2020) for comparison. For SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al., 2020), PRISM-src (Thompson and Post, 2020) and multilingual-to-multilingual MTran-sQuest (Ranasinghe et al., 2020b). For SRC+REF , we use XLM-R+Concat (Takahashi et al., 2020) and COMET (Rei et al., 2020) as strong baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_60",
            "content": "Data Collection for UniTE-UP",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_61",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "151-ARR_v2_62",
            "content": "English-Targeted Results on English-targeted metric task are conducted in Table 1. Among all involved baselines, for REF methods, BARTScore (Yuan et al., 2021) performs better than other statistical and model-based metrics.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_63",
            "content": "High-resource Zero-shot Avg. En-Cs En-De En-Ru En-Zh En-Fi En-Gu En-Kk En-Lt De-Cs De-Fr Fr-De Reference-only Evaluation \u2665 BLEU (Papineni et al., 2002) 36 As to SRC scenario, MTransQuest (Ranasinghe et al., 2020b) gives dominant performance. Further, COMET (Rei et al., 2020) performs better than XLM-R+Concat (Takahashi et al., 2020) on SRC+REF scenario. As for our methods, we can see that, UniTE-MRA achieves better results on all tasks, demonstrating the effectiveness of monotonic attention flows for cross-lingual interactions. Moreover, the proposed model UniTE-UP, which unifies REF , SRC , and SRC+REF learning on both pretraining and finetuning, yields better results on all evaluation settings. Most importantly, UniTE-UP is a single model which surpasses all the different state-of-the-art models on three tasks, showing its dominance on both convenience and effectiveness.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_64",
            "content": "Multilingual-Targeted As seen in Table 2, the multilingual-targeted UniTE-MUP gives dominant performance than all strong baselines on REF , SRC and SRC+REF , demonstrating the transferability and effectiveness of our approach. Besides, the UniTE-UP also gives dominant results, revealing an improvement of 0.6, 0.3 and 0.9 averaged Kendall's \u03c4 correlation scores, respectively. However, we find that UniTE-MUP outperforms strong baselines but slightly worse than UniTE-UP on English-targeted translation directions (see Table 3). We think the reason lies in the curse of multilingualism and vocabulary dilution (Conneau et al., 2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_65",
            "content": "The results for UniTE approach on WMT 2020 QE task are concluded in Table 4. As seen, it achieves competitive results on QE task compared with the winner submission (Ranasinghe et al., 2020b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_66",
            "content": "Ablation Studies",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "151-ARR_v2_67",
            "content": "In this section, we conduct ablation studies to investigate the effectiveness of regional attention patterns ( \u00a75.1), unified training ( \u00a75.2), and rankingbased data labeling ( \u00a75.3). All experiments are conducted by following English-targeted setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_68",
            "content": "Regional Attention Patterns",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "151-ARR_v2_69",
            "content": "To investigate the effectiveness of MRA, we further collect experiments in Table 5. As seen, MRA can give performance improvements than full attention, and preventing the interactions between hypothesis and source segment can improve the performance most. We think the reasons behind are twofold. First, the source side is formed with a different language, whose semantic information is rather weak than the reference side. Second, by preventing direct interactions between source and hypothe- sis, semantics inside the former must be passed through reference, which is helpful for disambiguation. Besides, not allowing the source to derive information from the hypothesis is better than the opposite direction. Wang and Chen (2020) found that the positional embeddings in PLM are engaged with strong adjacent information. We think the reason why S\u2192H performs worse than H\u2192S lies in the skipping of indexes, which corrupts positional similarities in alignment calculation. Additionally, when we combined two methods together, i.e., unified pretraining and finetuning with SRC+REF UniTE-MRA setting, model performance drops to 34.9 over English-targeted tasks on average. We think that both methods all intend to solve the problem of unseen SRC+REF input format, and MRA may not be necessary if massive data examples can be obtained for pretraining. Nevertheless, UniTE-MRA has its advantage on wide application without requiring pseduo labeled data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_70",
            "content": "Unified Training",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "151-ARR_v2_71",
            "content": "Experiments for comparing unified and taskspecific training are concluded in Table 6. As seen, when using the unified pretraining checkpoint to finetune over the specific task, performance over three models reveals performance drop con- sistently, indicating that the unified finetuning is helpful for model learning. This also verifies our hypothesis, that the cores of REF , SRC , and SRC+REF tasks are identical to each other. Moreover, unified pretraining and finetuning are complementary to each other. Also, utilizing task-specific pretraining instead of unified one reveals worse performance. To sum up, unifying both pretraining and finetuning only reveals one model, showing its advantage on the generalization on all tasks, where one united model can cover all functionalities of REF , SRC and SRC+REF tasks concurrently.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_72",
            "content": "Ranking-based Data Labeling",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "151-ARR_v2_73",
            "content": "To verify the effectiveness of ranking-based labeling, we collect the results of models applying different pseudo labeling strategies. After deriving the original scores from the well-trained UniTE-MRA checkpoint, we use Z-score and proposed ranking-based normalization methods to label synthetic data. For both methods, we also apply an ensembling strategy to assign training examples with averaged scores deriving from 3 UniTE-MRA checkpoints. Results show that, Z-score normalization reveals a performance drop when applying score ensembling with multiple models. Our proposed ranking-based normalization can boost the UniTE-UP model training, and its ensembling approach can further improve the performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_74",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "151-ARR_v2_75",
            "content": "In the past decades, automatic translation evaluation is mainly divided into REF , SRC and SRC+REF tasks, each of which develops independently and is tackled by various task-specific methods. We suggest that the three tasks are possibly handled by a unified framework, thus being ease of use and facilitating the knowledge transferring. Contributions of our work are mainly in three folds:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_76",
            "content": "(a) We propose a flexible and unified translation evaluation model UniTE, which can be adopted into the three tasks at once; (b) Through in-depth analyses, we point out that the main challenge of unifying three tasks stems from the discrepancy between vanilla pretraining and multi-tasks finetuning, and fill this gap via monotonic regional attention (MRA) and unified pretraining (UP); (c) Our single model consistently outperforms a variety of state-of-the-art or winner systems across highresource and zero-shot evaluation in WMT 2019 Metrics and WMT 2020 QE benchmarks, showing its advantage of flexibility and convincingness. We hope our new insights can contribute to subsequent studies in the translation evaluation community.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "151-ARR_v2_77",
            "content": "Ondrej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Philipp Koehn, Christof Monz, Findings of the 2018 Conference on Machine Translation, 2018, Proceedings of the Third Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Ondrej Bojar",
                    "Christian Federmann",
                    "Mark Fishel",
                    "Yvette Graham",
                    "Barry Haddow",
                    "Philipp Koehn",
                    "Christof Monz"
                ],
                "title": "Findings of the 2018 Conference on Machine Translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_78",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised Cross-lingual Representation Learning at Scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Alexis Conneau",
                    "Kartikay Khandelwal",
                    "Naman Goyal",
                    "Vishrav Chaudhary",
                    "Guillaume Wenzek",
                    "Francisco Guzm\u00e1n",
                    "Edouard Grave",
                    "Myle Ott",
                    "Luke Zettlemoyer",
                    "Veselin Stoyanov"
                ],
                "title": "Unsupervised Cross-lingual Representation Learning at Scale",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_79",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_80",
            "content": "Peter Emerson, The Original Borda Count and Partial Voting, 2013, Social Choice and Welfare, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": [
                    "Peter Emerson"
                ],
                "title": "The Original Borda Count and Partial Voting",
                "pub_date": "2013",
                "pub_title": "Social Choice and Welfare",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_81",
            "content": "Erick Fonseca, Lisa Yankovskaya, F Andr\u00e9, Mark Martins, Christian Fishel,  Federmann, Findings of the WMT 2019 Shared Tasks on Quality Estimation, 2019, Proceedings of the Fourth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Erick Fonseca",
                    "Lisa Yankovskaya",
                    "F Andr\u00e9",
                    "Mark Martins",
                    "Christian Fishel",
                    " Federmann"
                ],
                "title": "Findings of the WMT 2019 Shared Tasks on Quality Estimation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_82",
            "content": "Markus Freitag, David Grangier, Isaac Caswell, BLEU might be guilty but references are not innocent, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Markus Freitag",
                    "David Grangier",
                    "Isaac Caswell"
                ],
                "title": "BLEU might be guilty but references are not innocent",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_83",
            "content": "Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ond\u0159ej Bojar. 2021. Results of the WMT21 Metrics Shared Task: Evaluating Metrics with Expertbased Human Evaluations on TED and News Domain, , Proceedings of the Sixth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Markus Freitag",
                    "Ricardo Rei",
                    "Nitika Mathur",
                    "Chi-Kiu Lo",
                    "Craig Stewart",
                    "George Foster"
                ],
                "title": "Alon Lavie, and Ond\u0159ej Bojar. 2021. Results of the WMT21 Metrics Shared Task: Evaluating Metrics with Expertbased Human Evaluations on TED and News Domain",
                "pub_date": null,
                "pub_title": "Proceedings of the Sixth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_84",
            "content": "Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, Tie-Yan Liu, Layer-wise coordination between encoder and decoder for neural machine translation, 2018, Proceedings of the 32nd International Conference on Neural Information Process-Systems (NeurIPS), .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Tianyu He",
                    "Xu Tan",
                    "Yingce Xia",
                    "Di He",
                    "Tao Qin",
                    "Zhibo Chen",
                    "Tie-Yan Liu"
                ],
                "title": "Layer-wise coordination between encoder and decoder for neural machine translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 32nd International Conference on Neural Information Process-Systems (NeurIPS)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_85",
            "content": "Kam Tin, Jonathan Ho, Sargur Hull,  Srihari, Decision Combination in Multiple Classifier Systems, 1994, IEEE transactions on Pattern Analysis and Machine Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": [
                    "Kam Tin",
                    "Jonathan Ho",
                    "Sargur Hull",
                    " Srihari"
                ],
                "title": "Decision Combination in Multiple Classifier Systems",
                "pub_date": "1994",
                "pub_title": "IEEE transactions on Pattern Analysis and Machine Intelligence",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_86",
            "content": "Ganesh Jawahar, Beno\u00eet Sagot, Djam\u00e9 Seddah, What Does BERT Learn about the Structure of Language?, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Ganesh Jawahar",
                    "Beno\u00eet Sagot",
                    "Djam\u00e9 Seddah"
                ],
                "title": "What Does BERT Learn about the Structure of Language?",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_87",
            "content": "Fabio Kepler, Jonay Tr\u00e9nous, Marcos Treviso, Miguel Vera, Andr\u00e9 Martins, OpenKiwi: An Open Source Framework for Quality Estimation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL Demo), .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Fabio Kepler",
                    "Jonay Tr\u00e9nous",
                    "Marcos Treviso",
                    "Miguel Vera",
                    "Andr\u00e9 Martins"
                ],
                "title": "OpenKiwi: An Open Source Framework for Quality Estimation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL Demo)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_88",
            "content": "Yoon Kim, Alexander M Rush, Sequence-Level Knowledge Distillation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Yoon Kim",
                    "Alexander M Rush"
                ],
                "title": "Sequence-Level Knowledge Distillation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_89",
            "content": "Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation, 2021, Proceedings of the Seventh Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Tom Kocmi",
                    "Christian Federmann",
                    "Roman Grundkiewicz",
                    "Marcin Junczys-Dowmunt"
                ],
                "title": "To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Seventh Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_90",
            "content": "Yongjie Lin, Yi Chern Tan, Robert Frank, Open Sesame: Getting inside BERT's Linguistic Knowledge, 2019, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP@ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Yongjie Lin",
                    "Yi Chern Tan",
                    "Robert Frank"
                ],
                "title": "Open Sesame: Getting inside BERT's Linguistic Knowledge",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP@ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_91",
            "content": "Chi-Kiu Lo, YiSi -a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources, 2019, Proceedings of the Fourth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Chi-Kiu Lo"
                ],
                "title": "YiSi -a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fourth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_92",
            "content": "Qingsong Ma, Johnny Wei, Ond\u0159ej Bojar, Yvette Graham, Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges, 2019, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Qingsong Ma",
                    "Johnny Wei",
                    "Ond\u0159ej Bojar",
                    "Yvette Graham"
                ],
                "title": "Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges",
                "pub_date": "2019",
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_93",
            "content": "Nitika Mathur, Timothy Baldwin, Trevor Cohn, Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Nitika Mathur",
                    "Timothy Baldwin",
                    "Trevor Cohn"
                ],
                "title": "Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_94",
            "content": "Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong Ma, and Ond\u0159ej Bojar. 2020b. Results of the WMT20 Metrics Shared Task, , Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Nitika Mathur",
                    "Johnny Wei",
                    "Markus Freitag"
                ],
                "title": "Qingsong Ma, and Ond\u0159ej Bojar. 2020b. Results of the WMT20 Metrics Shared Task",
                "pub_date": null,
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_95",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a Method for Automatic Evaluation of Machine Translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a Method for Automatic Evaluation of Machine Translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_96",
            "content": "Mary Phuong, Christoph Lampert, Towards Understanding Knowledge Distillation, 2019, Proceedings of the 36th International Conference on Machine Learning (ICML), .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Mary Phuong",
                    "Christoph Lampert"
                ],
                "title": "Towards Understanding Knowledge Distillation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 36th International Conference on Machine Learning (ICML)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_97",
            "content": "Maja Popovic, chrF: character n-gram F-score for automatic MT evaluation, 2015, Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Maja Popovic"
                ],
                "title": "chrF: character n-gram F-score for automatic MT evaluation",
                "pub_date": "2015",
                "pub_title": "Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_98",
            "content": "Maja Popovic, chrF++: words helping character n-grams, 2017, Proceedings of the Second Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Maja Popovic"
                ],
                "title": "chrF++: words helping character n-grams",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_99",
            "content": "UNKNOWN, None, 2020, Intelligent Translation Memory Matching and Retrieval with Sentence Encoders, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Intelligent Translation Memory Matching and Retrieval with Sentence Encoders",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_100",
            "content": "Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov, TransQuest: Translation Quality Estimation with Cross-lingual Transformers, 2020, Proceedings of the 28th International Conference on Computational Linguistics (COLING), .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Tharindu Ranasinghe",
                    "Constantin Orasan",
                    "Ruslan Mitkov"
                ],
                "title": "TransQuest: Translation Quality Estimation with Cross-lingual Transformers",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 28th International Conference on Computational Linguistics (COLING)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_101",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, COMET: A neural framework for MT evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Ricardo Rei",
                    "Craig Stewart",
                    "Ana Farinha",
                    "Alon Lavie"
                ],
                "title": "COMET: A neural framework for MT evaluation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_102",
            "content": "Anna Rogers, Olga Kovaleva, Anna Rumshisky, A Primer in BERTology: What We Know About How BERT Works, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Anna Rogers",
                    "Olga Kovaleva",
                    "Anna Rumshisky"
                ],
                "title": "A Primer in BERTology: What We Know About How BERT Works",
                "pub_date": "2020",
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_103",
            "content": "Thibault Sellam, Dipanjan Das, Ankur Parikh, BLEURT: learning robust metrics for text generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Thibault Sellam",
                    "Dipanjan Das",
                    "Ankur Parikh"
                ],
                "title": "BLEURT: learning robust metrics for text generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_104",
            "content": "Thibault Sellam, Amy Pu,  Hyung Won, Sebastian Chung, Qijun Gehrmann, Markus Tan, Dipanjan Freitag, Ankur Das,  Parikh, Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task, 2020, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Thibault Sellam",
                    "Amy Pu",
                    " Hyung Won",
                    "Sebastian Chung",
                    "Qijun Gehrmann",
                    "Markus Tan",
                    "Dipanjan Freitag",
                    "Ankur Das",
                    " Parikh"
                ],
                "title": "Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_105",
            "content": "Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, John Makhoul, A Study of Translation Edit Rate with Targeted Human Annotation, 2006, Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers (ATMA), .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Matthew Snover",
                    "Bonnie Dorr",
                    "Richard Schwartz",
                    "Linnea Micciulla",
                    "John Makhoul"
                ],
                "title": "A Study of Translation Edit Rate with Targeted Human Annotation",
                "pub_date": "2006",
                "pub_title": "Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers (ATMA)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_106",
            "content": "Lucia Specia, Fr\u00e9d\u00e9ric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzm\u00e1n, Andr\u00e9 Martins, Findings of the WMT 2020 Shared Task on Quality Estimation, 2020, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Lucia Specia",
                    "Fr\u00e9d\u00e9ric Blain",
                    "Marina Fomicheva",
                    "Erick Fonseca",
                    "Vishrav Chaudhary",
                    "Francisco Guzm\u00e1n",
                    "Andr\u00e9 Martins"
                ],
                "title": "Findings of the WMT 2020 Shared Task on Quality Estimation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Fifth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_107",
            "content": "Lucia Specia, Fr\u00e9d\u00e9ric Blain, Marina Fomicheva, Chrysoula Zerva, Zhenhao Li, Vishrav Chaudhary, Andr\u00e9 Martins, 2021. Findings of the WMT 2021 Shared Task on Quality Estimation, , Proceedings of the Sixth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Lucia Specia",
                    "Fr\u00e9d\u00e9ric Blain",
                    "Marina Fomicheva",
                    "Chrysoula Zerva",
                    "Zhenhao Li",
                    "Vishrav Chaudhary",
                    "Andr\u00e9 Martins"
                ],
                "title": "2021. Findings of the WMT 2021 Shared Task on Quality Estimation",
                "pub_date": null,
                "pub_title": "Proceedings of the Sixth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_108",
            "content": "Kosuke Takahashi, Katsuhito Sudoh, Satoshi Nakamura, Automatic Machine Translation Evaluation using Source Language Inputs and Crosslingual Language Model, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Kosuke Takahashi",
                    "Katsuhito Sudoh",
                    "Satoshi Nakamura"
                ],
                "title": "Automatic Machine Translation Evaluation using Source Language Inputs and Crosslingual Language Model",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_109",
            "content": "Ian Tenney, Dipanjan Das, Ellie Pavlick, BERT Rediscovers the Classical NLP Pipeline, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Ian Tenney",
                    "Dipanjan Das",
                    "Ellie Pavlick"
                ],
                "title": "BERT Rediscovers the Classical NLP Pipeline",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_110",
            "content": "Brian Thompson, Matt Post, Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Brian Thompson",
                    "Matt Post"
                ],
                "title": "Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "151-ARR_v2_111",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention Is All You Need, 2017, Advances in Neural Information Processing Systems (NIPS), .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "\u0141ukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention Is All You Need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems (NIPS)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_112",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_113",
            "content": "Lidia Wong,  Chao, RoBLEURT Submission for WMT2021 Metrics Task, 2021, Proceedings of the Sixth Conference on Machine Translation (WMT), .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Lidia Wong",
                    " Chao"
                ],
                "title": "RoBLEURT Submission for WMT2021 Metrics Task",
                "pub_date": "2021",
                "pub_title": "Proceedings of the Sixth Conference on Machine Translation (WMT)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_114",
            "content": "Yu-An Wang, Yun-Nung Chen, What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Yu-An Wang",
                    "Yun-Nung Chen"
                ],
                "title": "What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "pub": "EMNLP"
            }
        },
        {
            "ix": "151-ARR_v2_115",
            "content": "Weizhe Yuan, Graham Neubig, Pengfei Liu, BARTScore: Evaluating Generated Text as Text Generation, 2021, Advances in Neural Information Processing Systems (NeurIPS), .",
            "ntype": "ref",
            "meta": {
                "xid": "b38",
                "authors": [
                    "Weizhe Yuan",
                    "Graham Neubig",
                    "Pengfei Liu"
                ],
                "title": "BARTScore: Evaluating Generated Text as Text Generation",
                "pub_date": "2021",
                "pub_title": "Advances in Neural Information Processing Systems (NeurIPS)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_116",
            "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, Yoav Artzi, BERTScore: Evaluating Text Generation with BERT, 2020, 8th International Conference on Learning Representations (ICLR), .",
            "ntype": "ref",
            "meta": {
                "xid": "b39",
                "authors": [
                    "Tianyi Zhang",
                    "Varsha Kishore",
                    "Felix Wu",
                    "Kilian Weinberger",
                    "Yoav Artzi"
                ],
                "title": "BERTScore: Evaluating Text Generation with BERT",
                "pub_date": "2020",
                "pub_title": "8th International Conference on Learning Representations (ICLR)",
                "pub": null
            }
        },
        {
            "ix": "151-ARR_v2_117",
            "content": "Wei Zhao, Goran Glava\u0161, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger, On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "ref",
            "meta": {
                "xid": "b40",
                "authors": [
                    "Wei Zhao",
                    "Goran Glava\u0161",
                    "Maxime Peyrard",
                    "Yang Gao",
                    "Robert West",
                    "Steffen Eger"
                ],
                "title": "On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "151-ARR_v2_0@0",
            "content": "UniTE: Unified Translation Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_0",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_2@0",
            "content": "Translation quality evaluation plays a crucial role in machine translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_2",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_2@1",
            "content": "According to the input format, it is mainly separated into three tasks, i.e., reference-only, source-only and source-reference-combined.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_2",
            "start": 76,
            "end": 211,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_2@2",
            "content": "Recent methods, despite their promising results, are specifically designed and optimized on one of them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_2",
            "start": 213,
            "end": 316,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_2@3",
            "content": "This limits the convenience of these methods, and overlooks the commonalities among tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_2",
            "start": 318,
            "end": 407,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_2@4",
            "content": "In this paper, we propose UniTE, which is the first unified framework engaged with abilities to handle all three evaluation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_2",
            "start": 409,
            "end": 538,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_2@5",
            "content": "Concretely, we propose monotonic regional attention to control the interaction among input segments, and unified pretraining to better adapt multi-task learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_2",
            "start": 540,
            "end": 700,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_2@6",
            "content": "We testify our framework on WMT 2019 Metrics and WMT 2020 Quality Estimation benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_2",
            "start": 702,
            "end": 789,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_2@7",
            "content": "Extensive analyses show that our single model can universally surpass various state-of-the-art or winner methods across tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_2",
            "start": 791,
            "end": 916,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_2@8",
            "content": "Both source code and associated models are available at https://github.com/NLP2CT/UniTE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_2",
            "start": 918,
            "end": 1005,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_4@0",
            "content": "Automatically evaluating the translation quality with the given reference segment(s), is of vital importance to identify the performance of Machine Translation (MT) models (Freitag et al., 2020;Mathur et al., 2020a;Zhao et al., 2020;Kocmi et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_4",
            "start": 0,
            "end": 252,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_4@1",
            "content": "Based on the input contexts, translation evaluation can be mainly categorized into three classes: 1) reference-only evaluation ( REF ) approaches like BLEU (Papineni et al., 2002) and BLEURT (Sellam et al., 2020a), which evaluate the hypothesis by referring the golden reference at target side; 2) source-only evaluation ( SRC ) methods like YiSi-2 (Lo, 2019) and TransQuest (Ranasinghe et al., 2020b), which are also referred as quality estimation (QE).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_4",
            "start": 254,
            "end": 707,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_4@2",
            "content": "These methods estimate the quality of the hypothesis based on the source sentence without using references; 3) source-reference-combined evaluation ( SRC+REF ) works like COMET (Rei et al., 2020), where the evaluation exploits information from both source and reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_4",
            "start": 709,
            "end": 978,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_4@3",
            "content": "With the help of powerful pretrained language models (PLMs, Devlin et al., 2019;Conneau et al., 2020), modelbased approaches (e.g., BLEURT, TransQuest, and COMET) have shown promising results in recent WMT competitions (Ma et al., 2019;Mathur et al., 2020b;Freitag et al., 2021;Fonseca et al., 2019;Specia et al., 2020Specia et al., , 2021.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_4",
            "start": 980,
            "end": 1319,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_5@0",
            "content": "Nevertheless, each existing MT evaluation work is usually designed for one specific task, e.g., BLEURT is only used for REF task and can not support SRC and SRC+REF tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_5",
            "start": 0,
            "end": 170,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_5@1",
            "content": "Moreover, those approaches preserve the same core -evaluating the quality of translation by referring to the given segments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_5",
            "start": 172,
            "end": 295,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_5@2",
            "content": "We believe that it is valuable, as well as feasible, to unify the capabilities of all MT evaluation tasks ( REF , SRC and SRC+REF ) into one model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_5",
            "start": 297,
            "end": 443,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_5@3",
            "content": "Among the promising advantages are ease of use and improved robustness through knowledge transfer across evaluation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_5",
            "start": 445,
            "end": 566,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_5@4",
            "content": "To achieve this, two important challenges need to be addressed: 1) How to design a model framework that can unify all translation evaluation tasks? 2) How to make the powerful PLMs better adapt to the unified evaluation model?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_5",
            "start": 568,
            "end": 793,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_6@0",
            "content": "In this paper, we propose UniTE -Unified Translation Evaluation, a novel approach which unifies the functionalities of REF , SRC and SRC+REF tasks into one model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_6",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_6@1",
            "content": "To solve the first challenge as mentioned above, based on the multilingual PLM, we utilize layerwise coordination which concatenates all input segments into one sequence as the unified input form.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_6",
            "start": 163,
            "end": 358,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_6@2",
            "content": "To further unify the modeling of three evaluation tasks, we propose a novel Monotonic Regional Attention (MRA) strat-egy, which allows partial semantic flows for a specific evaluation task.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_6",
            "start": 360,
            "end": 548,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_6@3",
            "content": "For the second challenge, a multi-task learning-based unified pretraining is proposed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_6",
            "start": 550,
            "end": 635,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_6@4",
            "content": "To be concrete, we collect the high-quality translations and degrade low-quality translations of NMT models as synthetic data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_6",
            "start": 637,
            "end": 762,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_6@5",
            "content": "Then we propose a novel ranking-based data labeling strategy to provide the training signal.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_6",
            "start": 764,
            "end": 855,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_6@6",
            "content": "Finally, the multilingual PLM is continuously pretrained on synthetic dataset with multi-task learning manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_6",
            "start": 857,
            "end": 966,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_6@7",
            "content": "Besides, our proposed models, named UniTE-MRA and UniTE-UP respectively, can benefit from finetuning with human-annotated data over three tasks at once, not requiring extra task-specific training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_6",
            "start": 968,
            "end": 1163,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_7@0",
            "content": "Experimental results demonstrate the superiority of UniTE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_7",
            "start": 0,
            "end": 57,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_7@1",
            "content": "Compared to various strong baseline systems on each task, UniTE, which unifies REF , SRC and SRC+REF tasks into one single model, achieves consistently absolute improvements of Kendall's \u03c4 correlations at 1.1, 2.3 and 1.1 scores on English-targeted translation directions of WMT 2019 Metric Shared task (Fonseca et al., 2019), respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_7",
            "start": 59,
            "end": 398,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_7@2",
            "content": "Meanwhile, after introducing multilingual-targeted support for our unified pretraining strategy, a single model named UniTE-MUP also gives dominant results against existing methods on non-English-targeted translation evaluation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_7",
            "start": 400,
            "end": 633,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_7@3",
            "content": "Furthermore, our method can also achieve competitive results over WMT 2020 QE task compared with the winner submission (Ranasinghe et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_7",
            "start": 635,
            "end": 780,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_7@4",
            "content": "Ablation studies reveal that, the proposed MRA and unified pretraining strategies are both important for model performance, making the model preserve the outstanding performance and multi-task transferability concurrently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_7",
            "start": 782,
            "end": 1003,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_8@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_8",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_9@0",
            "content": "In this section, we briefly introduce the three directions of translation evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_9",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_10@0",
            "content": "Reference-Only Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_10",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_11@0",
            "content": "REF assesses the translation quality via comparing the translation candidate and the given reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_11",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_11@1",
            "content": "In this setting, the two inputs are written in the same language, thus being easily applied in most of the metric tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_11",
            "start": 102,
            "end": 221,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_11@2",
            "content": "In the early stages, statistical methods are dominant solutions due to their strengths in wide language support and intuitive design.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_11",
            "start": 223,
            "end": 355,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_11@3",
            "content": "These methods measure the surface text similarity for a range of linguistic features, including n-gram (BLEU, Papineni et al., 2002), token (TER, Snover et al., 2006), and character (ChrF & ChrF++, Popovic, 2015, 2017.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_11",
            "start": 357,
            "end": 574,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_11@4",
            "content": "However, recent studies pointed out that these metrics have low consistency with human judgments and insufficiently evaluate high-qualified MT systems (Freitag et al., 2020;Rei et al., 2020;Mathur et al., 2020a).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_11",
            "start": 576,
            "end": 787,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_12@0",
            "content": "Consequently, with the rapid development of PLMs, researchers have been paying their attention to model-based approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_12",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_12@1",
            "content": "The basic idea of these studies is to collect sentence representations for similarity calculation (BERTScore, Zhang et al., 2020) or evaluating probabilistic confidence (PRISM-ref, Thompson and Post, 2020;BARTScore, Yuan et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_12",
            "start": 122,
            "end": 356,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_12@2",
            "content": "To further improve the model, Sellam et al. (2020a) pretrained a specific PLM for the translation evaluation (BLEURT), while Lo (2019) combined statistical and representative features (YiSi-1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_12",
            "start": 358,
            "end": 550,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_12@3",
            "content": "Both these methods achieve higher correlations with human judgments than statistical counterparts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_12",
            "start": 552,
            "end": 649,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_13@0",
            "content": "Source-Only Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_13",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_14@0",
            "content": "SRC , which also refers to quality estimation 1 , is an important translation evaluation task especially for the scenario where the ground-truth reference is unavailable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_14",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_14@1",
            "content": "It takes the source-side sentence and the translation candidate as inputs for the quality estimation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_14",
            "start": 171,
            "end": 271,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_14@2",
            "content": "To achieve this, the methods are required to model cross-lingual semantic alignments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_14",
            "start": 273,
            "end": 357,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_14@3",
            "content": "Similar to reference-only evaluation, statistical-based (Ranasinghe et al., 2020b), model-based (TransQuest, Ranasinghe et al., 2020bPRISM-src, Thompson and Post, 2020), and feature combination (YiSi-2, Lo, 2019) are typical and advanced methods in this tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_14",
            "start": 359,
            "end": 618,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_15@0",
            "content": "Source-Reference-Combined Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_15",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_16@0",
            "content": "Aside from the above tasks that only consider either source or target side at one time, SRC+REF takes both source and reference sentences into account.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_16",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_16@1",
            "content": "In this way, methods in this context can evaluate the translation candidate via utilizing the features from both sides.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_16",
            "start": 152,
            "end": 270,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_16@2",
            "content": "As a rising paradigm among translation evaluation tasks, SRC+REF also profits from the development of cross-lingual PLMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_16",
            "start": 272,
            "end": 392,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_16@3",
            "content": "For example, finetuning PLMs over human-annotated datasets (COMET, Rei et al., 2020) achieves new state-of-the-art results among all evaluation approaches in WMT 2020 (Mathur et al., 2020b For SRC+REF , we show the hard design for monotonic regional attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_16",
            "start": 394,
            "end": 653,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_16@4",
            "content": "denotes the masked attention logits.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_16",
            "start": 655,
            "end": 690,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_17@0",
            "content": "Methodology",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_17",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_18@0",
            "content": "As mentioned above, massive methods are proposed for different automatic evaluation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_18",
            "start": 0,
            "end": 89,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_18@1",
            "content": "On the one hand, it is inconvenient and expensive to develop and employ different metrics for different evaluation scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_18",
            "start": 91,
            "end": 215,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_18@2",
            "content": "On the other hand, separate models absolutely overlook the commonalities among these evaluation tasks, of which knowledge potentially benefits all three tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_18",
            "start": 217,
            "end": 375,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_18@3",
            "content": "In order to fulfill the aim of unifying the functionalities on REF , SRC , and SRC+REF into one model, in this section, we introduce UniTE (Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_18",
            "start": 377,
            "end": 526,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_19@0",
            "content": "Model Architecture",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_19",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_20@0",
            "content": "By receiving a data example composing of hypothesis, source, and reference segment, UniTE first modifies it into concatenated sequence following the given setting as REF , SRC , or SRC+REF :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_20",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_21@0",
            "content": "x REF = Concat(h, r) \u2208 R (l h +lr) , x SRC = Concat(h, s) \u2208 R (l h +ls) ,(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_21",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_22@0",
            "content": "x SRC+REF = Concat(h, s, r) \u2208 R (l h +ls+lr) ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_22",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_23@0",
            "content": "where h, s and r are hypothesis, source and reference segments, with the corresponding sequence lengths being l h , l s and l r , respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_23",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_23@1",
            "content": "The input sequence is then fed to PLM to derive representations H. Take REF as an example:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_23",
            "start": 144,
            "end": 233,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_24@0",
            "content": "HREF = PLM(x REF ) \u2208 R (l h +lr)\u00d7d , (2",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_24",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_25@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_25",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_26@0",
            "content": "where d is the model size of PLM.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_26",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_26@1",
            "content": "According to Ranasinghe et al. (2020b), we use the first output representation as the input of feedforward layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_26",
            "start": 34,
            "end": 146,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_27@0",
            "content": "Compared to existing methods (Zhang et al., 2020;Rei et al., 2020) which take sentence-level representations for evaluation, the advantages of our architecture design are as follows.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_27",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_27@1",
            "content": "First, our UniTE model can benefit from layer-coordinated semantical interactions inside every one of PLM layers, which is proven effective on capturing diverse linguistic features (He et al., 2018;Lin et al., 2019;Jawahar et al., 2019;Tenney et al., 2019;Rogers et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_27",
            "start": 183,
            "end": 459,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_27@2",
            "content": "Second, for the unified approach of our model, the concatenation provides the unifying format for all task inputs, turning our model into a more general architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_27",
            "start": 461,
            "end": 626,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_27@3",
            "content": "When conducting different evaluation tasks, our model requires no further modification inside.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_27",
            "start": 628,
            "end": 721,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_27@4",
            "content": "Note here, to keep the consistency across all evaluation tasks, as well as ease the unified learning, h is always located at the beginning of the input sequence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_27",
            "start": 723,
            "end": 883,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_28@0",
            "content": "After deriving HREF , a pooling block is arranged after PLM which gives sequence-level representations H REF .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_28",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_28@1",
            "content": "Finally, a feedforward network takes H REF as input, and gives a scalar p as prediction:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_28",
            "start": 111,
            "end": 198,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_29@0",
            "content": "H REF = Pool( HREF ) \u2208 R d ,(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_29",
            "start": 0,
            "end": 30,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_30@0",
            "content": "p REF = FeedForward(H REF ) \u2208 R 1 . (4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_30",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_31@0",
            "content": ")",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_31",
            "start": 0,
            "end": 0,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_32@0",
            "content": "For training, we encourage the model to reduce the mean squared error with respect to given score q:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_32",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_33@0",
            "content": "L REF = (p REF \u2212 q) 2 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_33",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_34@0",
            "content": "(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_34",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_35@0",
            "content": "However, for the pretraining of most PLMs (e,g., XLM-R, Conneau et al., 2020), the input patterns are designed to receive two segments at most.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_35",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_35@1",
            "content": "Thus there exists a gap between the pretraining of PLM and the joint training of UniTE where the concatenation of three fragments is used as input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_35",
            "start": 144,
            "end": 290,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_35@2",
            "content": "Moreover, previous study (Takahashi et al., 2020) shows that directly training over SRC+REF by following such design leads to worse performance than REF scenario.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_35",
            "start": 292,
            "end": 453,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_35@3",
            "content": "To alleviate this issue, we propose two strategies: Monotonic Regional Attention as described in \u00a73.2 and Unified Pretraining in \u00a73.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_35",
            "start": 455,
            "end": 588,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_36@0",
            "content": "Monotonic Regional Attention",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_36",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_37@0",
            "content": "To fill the modeling gap between the pretraining of PLM and the joint training of three downstream tasks, a natural idea is to unify the number of involved segments when modeling semantics for SRC , REF and SRC+REF tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_37",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_37@1",
            "content": "Following this, we propose to modify the attention mask of SRC+REF to simulate the modeling of two segments in SRC and REF .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_37",
            "start": 222,
            "end": 345,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_37@2",
            "content": "Specifically, when calculating the attention logits, semantics from a specific segment are only allowed to derive information from two segments at most.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_37",
            "start": 347,
            "end": 498,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_37@3",
            "content": "Considering the conventional attention module:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_37",
            "start": 500,
            "end": 545,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_38@0",
            "content": "A = Softmax( QK \u221a d ) \u2208 R L\u00d7L ,(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_38",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_39@0",
            "content": "where L is the sequential length for input, Q, K \u2208 R L\u00d7d are query and key representations, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_39",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_39@1",
            "content": "2 As to monotonic regional attention (MRA), we simply add a mask M to the softmax logits to control attention flows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_39",
            "start": 106,
            "end": 221,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_40@0",
            "content": "A = Softmax( QK \u221a d + M) \u2208 R L\u00d7L , (7) M ij = \u2212\u221e (i, j) \u2208 U, 0 otherwise,(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_40",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_41@0",
            "content": "where U stores the index pairs of all masked areas.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_41",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_41@1",
            "content": "Following this idea, the key of MRA is how to design the matrix U. For the cases where interactions inside each segment, we believe that these self-interactions are beneficial to the modeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_41",
            "start": 52,
            "end": 243,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_41@2",
            "content": "For other cases where interactions are arranged across segments, three patterns are included: hypothesisreference, source-reference, and hypothesis-source.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_41",
            "start": 245,
            "end": 399,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_41@3",
            "content": "Intuitively, the former two parts are beneficial for model training, since they might contribute the monolingual signals and cross-lingual disambiguation to evaluation, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_41",
            "start": 401,
            "end": 582,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_41@4",
            "content": "This leaves the only case, where our experimental analysis also verifies (see \u00a75.1), that interaction between hypothesis and source leads to the performance decrease for SRC+REF task, thus troubling the unifying.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_41",
            "start": 584,
            "end": 795,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_42@0",
            "content": "2 For simplicity, we omit the multi-head mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_42",
            "start": 0,
            "end": 50,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_42@1",
            "content": "Figure 2: Attention flows in monotonic regional attention.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_42",
            "start": 52,
            "end": 109,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_42@2",
            "content": "h, s and r are hypothesis, source and reference, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_42",
            "start": 111,
            "end": 172,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_42@3",
            "content": "We prevent specified interactions in SRC+REF training via modifying the attention mask with regional properties.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_42",
            "start": 174,
            "end": 285,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_42@4",
            "content": "We show the hard (left) and soft design (right, no h \u2192 s) in this figure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_42",
            "start": 287,
            "end": 359,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_43@0",
            "content": "To give more fine-grained designs, we propose two approaches for UniTE-MRA, which apply the MRA mechanism into UniTE model (Figure 2):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_43",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_44@0",
            "content": "\u2022 Hard MRA. Only monotonic attention flows are allowed. Interactions between any two segments are strictly unidirectional through the entire PLM, where U stores the index pairs of unidirectional interactions of h \u2192 r, s \u2192 r and h \u2192 s, where \"\u2192\" denotes the direction of attention flows. \u2022 Soft MRA. Specific attention flows are forbidden inside each attention module. The involved two segments may interact inside a higher layer. In practice, index pairs which denoting h \u2192 s or s \u2192 h between source and hypothesis are stored in U.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_44",
            "start": 0,
            "end": 530,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_45@0",
            "content": "Note that, although the processing in source and reference may be affected because their positions are not indexed from the start, related studies on positional embeddings reveal that, PLM can well capture relative positional information (Wang and Chen, 2020), which dispels this concern.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_45",
            "start": 0,
            "end": 287,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_46@0",
            "content": "Unified Pretraining",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_46",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_47@0",
            "content": "To further bridge the modeling gap between PLM and the joint training of UniTE mentioned in \u00a73.1, we propose a unified pretraining strategy including the following main stages: 1) collecting and downgrading synthetic data; 2) labeling examples with a novel ranking-based strategy; 3) multi-task learning for unified pretraining and finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_47",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_48@0",
            "content": "Synthetic Data Collection As our approach aims at evaluating the quality of translations, generated hypotheses with NMT models are ideal synthetic data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_48",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_48@1",
            "content": "To further improve the diversity of synthetic data quality, we follow existing experiences (Sellam et al., 2020a;Wan et al., 2021) to apply the word and span dropping strategy to downgrade a portion of hypotheses.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_48",
            "start": 153,
            "end": 365,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_48@2",
            "content": "The collected data totally contains N triplets composing of hypothesis, source and reference segments, which is formed as D = { h i , s i , r i } N i=1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_48",
            "start": 367,
            "end": 519,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_48@3",
            "content": "Data Labeling After obtaining the synthetic data, the next step is to augment each data pair with a label which serves as the signal of unified pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_48",
            "start": 521,
            "end": 676,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_48@4",
            "content": "To stabilize the model training, as well as normalize the distributions across all score systems and languages, we propose a novel rankingbased approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_48",
            "start": 678,
            "end": 830,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_48@5",
            "content": "This method is based on the idea of Borda count (Ho et al., 1994;Emerson, 2013), which provides more precise and well-distributed synthetic data labels than Z-score normalization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_48",
            "start": 832,
            "end": 1010,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_49@0",
            "content": "Specifically, we first use available approaches to derive the predicted score qi for each item, yielding labeled synthetic quadruple examples formed as D = { h i , s i , r i , qi } N i=1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_49",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_49@1",
            "content": "Then, we tag each example with its rank index qi referring to qi :",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_49",
            "start": 189,
            "end": 254,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_50@0",
            "content": "qi = IndexOf(q i , Q),(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_50",
            "start": 0,
            "end": 24,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_51@0",
            "content": "where Q is the list storing all the sorted qi descendingly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_51",
            "start": 0,
            "end": 58,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_51@1",
            "content": "Then, we use the conventional Z-score strategy to normalize the scores:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_51",
            "start": 60,
            "end": 130,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_52@0",
            "content": "q i = qi \u2212 \u00b5 \u03c3 ,(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_52",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_53@0",
            "content": "where \u00b5 and \u03c3 are the mean and the standard deviation of values in Q, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_53",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_53@1",
            "content": "The dataset thus updates its format to D = { h i , s i , r i , q i } N i=1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_53",
            "start": 84,
            "end": 159,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_53@2",
            "content": "Note here that, an example with higher qi is assigned with higher qi , thus a larger value of q i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_53",
            "start": 161,
            "end": 259,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_53@3",
            "content": "Compared to related approaches which apply Zscore normalization (Bojar et al., 2018), or leave the conventional labeled scores as signals for learning (i.e., knowledge distillation, Kim and Rush, 2016;Phuong and Lampert, 2019), our approach can alleviate the bias of chosen model for labeling and prior distributional disagreement of scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_53",
            "start": 261,
            "end": 601,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_53@4",
            "content": "For example, different methods may give scores with different distributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_53",
            "start": 603,
            "end": 678,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_53@5",
            "content": "Especially for translation directions of low-resource, scores may follow skewed distribution (Sellam et al., 2020a), which has a disagreement with rich-resource scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_53",
            "start": 680,
            "end": 850,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_53@6",
            "content": "Our method can unify the distribution of all labeling data into the same scale, which can also be easily applied by the ensembling strategy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_53",
            "start": 852,
            "end": 991,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_54@0",
            "content": "To unify all evaluation scenarios into one model, we apply multi-task learning for both pretraining and finetuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_54",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_54@1",
            "content": "For each step, we arrange three substeps for all input formats, yielding L REF , L SRC , and L SRC+REF , respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_54",
            "start": 116,
            "end": 233,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_54@2",
            "content": "The final learning objective is to reduce the summation of all losses: (Ma et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_54",
            "start": 235,
            "end": 323,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_54@3",
            "content": "For the former, we follow the common practice in COMET 3 (Rei et al., 2020) to collect and preprocess the dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_54",
            "start": 325,
            "end": 438,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_54@4",
            "content": "The official variant of Kendall's Tau correlation (Ma et al., 2019) is used for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_54",
            "start": 440,
            "end": 530,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_54@5",
            "content": "We evaluate our methods on all of REF , SRC and SRC+REF scenarios.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_54",
            "start": 532,
            "end": 597,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_55@0",
            "content": "L = L REF + L SRC + L SRC+REF .(11",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_55",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_56@0",
            "content": "For SRC scenario, we further conduct results on WMT 2020 QE task (Specia et al., 2020) referring to Ranasinghe et al. (2020a) for data collection and preprocessing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_56",
            "start": 0,
            "end": 163,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_56@1",
            "content": "Following the official report, the Pearson's correlation is used for evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_56",
            "start": 165,
            "end": 244,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_57@0",
            "content": "Model Pretraining As mentioned in \u00a73.3, we continuously pretrain PLMs using synthetic data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_57",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_58@0",
            "content": "The data is constructed from WMT 2021 News Translation task, where we collect the training sets from five translation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_58",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_58@1",
            "content": "Among those tasks, the target sentences are all in English (En), and the source languages are Czech (Cs), German (De), Japanese (Ja), Russian (Ru), and Chinese (Zh).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_58",
            "start": 125,
            "end": 289,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_58@2",
            "content": "Specifically, we follow Sellam et al. (2020a) to use TRANSFORMER-base (Vaswani et al., 2017) MT models to generate translation candidates, and use the checkpoints trained via UniTE-MRA approach for synthetic data labeling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_58",
            "start": 291,
            "end": 512,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_58@3",
            "content": "We pretrain two kinds of models, one is pretrained on English-targeted language directions, and the other is a multilingual version trained using bidirectional data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_58",
            "start": 514,
            "end": 678,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_58@4",
            "content": "Note that, for a fair comparison, we filter out all pretraining examples that are involved in benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_58",
            "start": 680,
            "end": 784,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_59@0",
            "content": "Model Setting We implement our approach upon COMET (Rei et al., 2020) Baselines As to REF approaches, we select BLEU (Papineni et al., 2002), ChrF (Popovic, 2015), YiSi-1 (Lo, 2019), BERTScore (Zhang et al., 2020), BLEURT (Sellam et al., 2020a), PRISM-ref (Thompson and Post, 2020), BARTScore (Yuan et al., 2021), XLM-R+Concat (Takahashi et al., 2020), and RoBERTa+Concat (Takahashi et al., 2020) for comparison.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_59",
            "start": 0,
            "end": 411,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_59@1",
            "content": "For SRC methods, we post results of both metric and QE methods, including YiSi-2 (Lo, 2019), XLM-R+Concat (Takahashi et al., 2020), PRISM-src (Thompson and Post, 2020) and multilingual-to-multilingual MTran-sQuest (Ranasinghe et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_59",
            "start": 413,
            "end": 653,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_59@2",
            "content": "For SRC+REF , we use XLM-R+Concat (Takahashi et al., 2020) and COMET (Rei et al., 2020) as strong baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_59",
            "start": 655,
            "end": 762,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_60@0",
            "content": "Data Collection for UniTE-UP",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_60",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_61@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_61",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_62@0",
            "content": "English-Targeted Results on English-targeted metric task are conducted in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_62",
            "start": 0,
            "end": 81,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_62@1",
            "content": "Among all involved baselines, for REF methods, BARTScore (Yuan et al., 2021) performs better than other statistical and model-based metrics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_62",
            "start": 83,
            "end": 222,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_63@0",
            "content": "High-resource Zero-shot Avg.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_63",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_63@1",
            "content": "En-Cs En-De En-Ru En-Zh En-Fi En-Gu En-Kk En-Lt De-Cs De-Fr Fr-De Reference-only Evaluation \u2665 BLEU (Papineni et al., 2002) 36 As to SRC scenario, MTransQuest (Ranasinghe et al., 2020b) gives dominant performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_63",
            "start": 29,
            "end": 240,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_63@2",
            "content": "Further, COMET (Rei et al., 2020) performs better than XLM-R+Concat (Takahashi et al., 2020) on SRC+REF scenario.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_63",
            "start": 242,
            "end": 354,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_63@3",
            "content": "As for our methods, we can see that, UniTE-MRA achieves better results on all tasks, demonstrating the effectiveness of monotonic attention flows for cross-lingual interactions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_63",
            "start": 356,
            "end": 532,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_63@4",
            "content": "Moreover, the proposed model UniTE-UP, which unifies REF , SRC , and SRC+REF learning on both pretraining and finetuning, yields better results on all evaluation settings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_63",
            "start": 534,
            "end": 704,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_63@5",
            "content": "Most importantly, UniTE-UP is a single model which surpasses all the different state-of-the-art models on three tasks, showing its dominance on both convenience and effectiveness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_63",
            "start": 706,
            "end": 884,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_64@0",
            "content": "Multilingual-Targeted As seen in Table 2, the multilingual-targeted UniTE-MUP gives dominant performance than all strong baselines on REF , SRC and SRC+REF , demonstrating the transferability and effectiveness of our approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_64",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_64@1",
            "content": "Besides, the UniTE-UP also gives dominant results, revealing an improvement of 0.6, 0.3 and 0.9 averaged Kendall's \u03c4 correlation scores, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_64",
            "start": 227,
            "end": 376,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_64@2",
            "content": "However, we find that UniTE-MUP outperforms strong baselines but slightly worse than UniTE-UP on English-targeted translation directions (see Table 3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_64",
            "start": 378,
            "end": 528,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_64@3",
            "content": "We think the reason lies in the curse of multilingualism and vocabulary dilution (Conneau et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_64",
            "start": 530,
            "end": 633,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_65@0",
            "content": "The results for UniTE approach on WMT 2020 QE task are concluded in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_65",
            "start": 0,
            "end": 75,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_65@1",
            "content": "As seen, it achieves competitive results on QE task compared with the winner submission (Ranasinghe et al., 2020b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_65",
            "start": 77,
            "end": 191,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_66@0",
            "content": "Ablation Studies",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_66",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_67@0",
            "content": "In this section, we conduct ablation studies to investigate the effectiveness of regional attention patterns ( \u00a75.1), unified training ( \u00a75.2), and rankingbased data labeling ( \u00a75.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_67",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_67@1",
            "content": "All experiments are conducted by following English-targeted setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_67",
            "start": 184,
            "end": 251,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_68@0",
            "content": "Regional Attention Patterns",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_68",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@0",
            "content": "To investigate the effectiveness of MRA, we further collect experiments in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@1",
            "content": "As seen, MRA can give performance improvements than full attention, and preventing the interactions between hypothesis and source segment can improve the performance most.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 84,
            "end": 254,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@2",
            "content": "We think the reasons behind are twofold.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 256,
            "end": 295,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@3",
            "content": "First, the source side is formed with a different language, whose semantic information is rather weak than the reference side.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 297,
            "end": 422,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@4",
            "content": "Second, by preventing direct interactions between source and hypothe- sis, semantics inside the former must be passed through reference, which is helpful for disambiguation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 424,
            "end": 596,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@5",
            "content": "Besides, not allowing the source to derive information from the hypothesis is better than the opposite direction.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 598,
            "end": 710,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@6",
            "content": "Wang and Chen (2020) found that the positional embeddings in PLM are engaged with strong adjacent information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 712,
            "end": 821,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@7",
            "content": "We think the reason why S\u2192H performs worse than H\u2192S lies in the skipping of indexes, which corrupts positional similarities in alignment calculation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 823,
            "end": 971,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@8",
            "content": "Additionally, when we combined two methods together, i.e., unified pretraining and finetuning with SRC+REF UniTE-MRA setting, model performance drops to 34.9 over English-targeted tasks on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 973,
            "end": 1169,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@9",
            "content": "We think that both methods all intend to solve the problem of unseen SRC+REF input format, and MRA may not be necessary if massive data examples can be obtained for pretraining.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 1171,
            "end": 1347,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_69@10",
            "content": "Nevertheless, UniTE-MRA has its advantage on wide application without requiring pseduo labeled data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_69",
            "start": 1349,
            "end": 1448,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_70@0",
            "content": "Unified Training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_70",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_71@0",
            "content": "Experiments for comparing unified and taskspecific training are concluded in Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_71",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_71@1",
            "content": "As seen, when using the unified pretraining checkpoint to finetune over the specific task, performance over three models reveals performance drop con- sistently, indicating that the unified finetuning is helpful for model learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_71",
            "start": 86,
            "end": 316,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_71@2",
            "content": "This also verifies our hypothesis, that the cores of REF , SRC , and SRC+REF tasks are identical to each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_71",
            "start": 318,
            "end": 428,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_71@3",
            "content": "Moreover, unified pretraining and finetuning are complementary to each other.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_71",
            "start": 430,
            "end": 506,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_71@4",
            "content": "Also, utilizing task-specific pretraining instead of unified one reveals worse performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_71",
            "start": 508,
            "end": 598,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_71@5",
            "content": "To sum up, unifying both pretraining and finetuning only reveals one model, showing its advantage on the generalization on all tasks, where one united model can cover all functionalities of REF , SRC and SRC+REF tasks concurrently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_71",
            "start": 600,
            "end": 830,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_72@0",
            "content": "Ranking-based Data Labeling",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_72",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_73@0",
            "content": "To verify the effectiveness of ranking-based labeling, we collect the results of models applying different pseudo labeling strategies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_73",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_73@1",
            "content": "After deriving the original scores from the well-trained UniTE-MRA checkpoint, we use Z-score and proposed ranking-based normalization methods to label synthetic data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_73",
            "start": 135,
            "end": 301,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_73@2",
            "content": "For both methods, we also apply an ensembling strategy to assign training examples with averaged scores deriving from 3 UniTE-MRA checkpoints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_73",
            "start": 303,
            "end": 444,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_73@3",
            "content": "Results show that, Z-score normalization reveals a performance drop when applying score ensembling with multiple models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_73",
            "start": 446,
            "end": 565,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_73@4",
            "content": "Our proposed ranking-based normalization can boost the UniTE-UP model training, and its ensembling approach can further improve the performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_73",
            "start": 567,
            "end": 710,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_74@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_74",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_75@0",
            "content": "In the past decades, automatic translation evaluation is mainly divided into REF , SRC and SRC+REF tasks, each of which develops independently and is tackled by various task-specific methods.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_75",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_75@1",
            "content": "We suggest that the three tasks are possibly handled by a unified framework, thus being ease of use and facilitating the knowledge transferring.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_75",
            "start": 192,
            "end": 335,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_75@2",
            "content": "Contributions of our work are mainly in three folds:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_75",
            "start": 337,
            "end": 388,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_76@0",
            "content": "(a) We propose a flexible and unified translation evaluation model UniTE, which can be adopted into the three tasks at once; (b) Through in-depth analyses, we point out that the main challenge of unifying three tasks stems from the discrepancy between vanilla pretraining and multi-tasks finetuning, and fill this gap via monotonic regional attention (MRA) and unified pretraining (UP); (c) Our single model consistently outperforms a variety of state-of-the-art or winner systems across highresource and zero-shot evaluation in WMT 2019 Metrics and WMT 2020 QE benchmarks, showing its advantage of flexibility and convincingness.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_76",
            "start": 0,
            "end": 629,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_76@1",
            "content": "We hope our new insights can contribute to subsequent studies in the translation evaluation community.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_76",
            "start": 631,
            "end": 732,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_77@0",
            "content": "Ondrej Bojar, Christian Federmann, Mark Fishel, Yvette Graham, Barry Haddow, Philipp Koehn, Christof Monz, Findings of the 2018 Conference on Machine Translation, 2018, Proceedings of the Third Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_77",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_78@0",
            "content": "Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, Veselin Stoyanov, Unsupervised Cross-lingual Representation Learning at Scale, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_78",
            "start": 0,
            "end": 328,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_79@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_79",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_80@0",
            "content": "Peter Emerson, The Original Borda Count and Partial Voting, 2013, Social Choice and Welfare, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_80",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_81@0",
            "content": "Erick Fonseca, Lisa Yankovskaya, F Andr\u00e9, Mark Martins, Christian Fishel,  Federmann, Findings of the WMT 2019 Shared Tasks on Quality Estimation, 2019, Proceedings of the Fourth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_81",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_82@0",
            "content": "Markus Freitag, David Grangier, Isaac Caswell, BLEU might be guilty but references are not innocent, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_82",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_83@0",
            "content": "Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-Kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ond\u0159ej Bojar. 2021. Results of the WMT21 Metrics Shared Task: Evaluating Metrics with Expertbased Human Evaluations on TED and News Domain, , Proceedings of the Sixth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_83",
            "start": 0,
            "end": 310,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_84@0",
            "content": "Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, Tie-Yan Liu, Layer-wise coordination between encoder and decoder for neural machine translation, 2018, Proceedings of the 32nd International Conference on Neural Information Process-Systems (NeurIPS), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_84",
            "start": 0,
            "end": 260,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_85@0",
            "content": "Kam Tin, Jonathan Ho, Sargur Hull,  Srihari, Decision Combination in Multiple Classifier Systems, 1994, IEEE transactions on Pattern Analysis and Machine Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_85",
            "start": 0,
            "end": 168,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_86@0",
            "content": "Ganesh Jawahar, Beno\u00eet Sagot, Djam\u00e9 Seddah, What Does BERT Learn about the Structure of Language?, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_86",
            "start": 0,
            "end": 196,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_87@0",
            "content": "Fabio Kepler, Jonay Tr\u00e9nous, Marcos Treviso, Miguel Vera, Andr\u00e9 Martins, OpenKiwi: An Open Source Framework for Quality Estimation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations (ACL Demo), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_87",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_88@0",
            "content": "Yoon Kim, Alexander M Rush, Sequence-Level Knowledge Distillation, 2016, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_88",
            "start": 0,
            "end": 169,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_89@0",
            "content": "Tom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation, 2021, Proceedings of the Seventh Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_89",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_90@0",
            "content": "Yongjie Lin, Yi Chern Tan, Robert Frank, Open Sesame: Getting inside BERT's Linguistic Knowledge, 2019, Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP (BlackBoxNLP@ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_90",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_91@0",
            "content": "Chi-Kiu Lo, YiSi -a Unified Semantic MT Quality Evaluation and Estimation Metric for Languages with Different Levels of Available Resources, 2019, Proceedings of the Fourth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_91",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_92@0",
            "content": "Qingsong Ma, Johnny Wei, Ond\u0159ej Bojar, Yvette Graham, Results of the WMT19 Metrics Shared Task: Segment-Level and Strong MT Systems Pose Big Challenges, 2019, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_92",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_93@0",
            "content": "Nitika Mathur, Timothy Baldwin, Trevor Cohn, Tangled up in BLEU: Reevaluating the Evaluation of Automatic Machine Translation Evaluation Metrics, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_93",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_94@0",
            "content": "Nitika Mathur, Johnny Wei, Markus Freitag, Qingsong Ma, and Ond\u0159ej Bojar. 2020b. Results of the WMT20 Metrics Shared Task, , Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_94",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_95@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a Method for Automatic Evaluation of Machine Translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_95",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_96@0",
            "content": "Mary Phuong, Christoph Lampert, Towards Understanding Knowledge Distillation, 2019, Proceedings of the 36th International Conference on Machine Learning (ICML), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_96",
            "start": 0,
            "end": 161,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_97@0",
            "content": "Maja Popovic, chrF: character n-gram F-score for automatic MT evaluation, 2015, Proceedings of the Tenth Workshop on Statistical Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_97",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_98@0",
            "content": "Maja Popovic, chrF++: words helping character n-grams, 2017, Proceedings of the Second Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_98",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_99@0",
            "content": "UNKNOWN, None, 2020, Intelligent Translation Memory Matching and Retrieval with Sentence Encoders, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_99",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_100@0",
            "content": "Tharindu Ranasinghe, Constantin Orasan, Ruslan Mitkov, TransQuest: Translation Quality Estimation with Cross-lingual Transformers, 2020, Proceedings of the 28th International Conference on Computational Linguistics (COLING), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_100",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_101@0",
            "content": "Ricardo Rei, Craig Stewart, Ana Farinha, Alon Lavie, COMET: A neural framework for MT evaluation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_101",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_102@0",
            "content": "Anna Rogers, Olga Kovaleva, Anna Rumshisky, A Primer in BERTology: What We Know About How BERT Works, 2020, Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_102",
            "start": 0,
            "end": 171,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_103@0",
            "content": "Thibault Sellam, Dipanjan Das, Ankur Parikh, BLEURT: learning robust metrics for text generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_103",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_104@0",
            "content": "Thibault Sellam, Amy Pu,  Hyung Won, Sebastian Chung, Qijun Gehrmann, Markus Tan, Dipanjan Freitag, Ankur Das,  Parikh, Learning to Evaluate Translation Beyond English: BLEURT Submissions to the WMT Metrics 2020 Shared Task, 2020, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_104",
            "start": 0,
            "end": 297,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_105@0",
            "content": "Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, John Makhoul, A Study of Translation Edit Rate with Targeted Human Annotation, 2006, Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers (ATMA), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_105",
            "start": 0,
            "end": 269,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_106@0",
            "content": "Lucia Specia, Fr\u00e9d\u00e9ric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzm\u00e1n, Andr\u00e9 Martins, Findings of the WMT 2020 Shared Task on Quality Estimation, 2020, Proceedings of the Fifth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_106",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_107@0",
            "content": "Lucia Specia, Fr\u00e9d\u00e9ric Blain, Marina Fomicheva, Chrysoula Zerva, Zhenhao Li, Vishrav Chaudhary, Andr\u00e9 Martins, 2021. Findings of the WMT 2021 Shared Task on Quality Estimation, , Proceedings of the Sixth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_107",
            "start": 0,
            "end": 245,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_108@0",
            "content": "Kosuke Takahashi, Katsuhito Sudoh, Satoshi Nakamura, Automatic Machine Translation Evaluation using Source Language Inputs and Crosslingual Language Model, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_108",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_109@0",
            "content": "Ian Tenney, Dipanjan Das, Ellie Pavlick, BERT Rediscovers the Classical NLP Pipeline, 2019, Proceedings of the 57th Conference of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_109",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_110@0",
            "content": "Brian Thompson, Matt Post, Automatic Machine Translation Evaluation in Many Languages via Zero-Shot Paraphrasing, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_110",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_111@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Attention Is All You Need, 2017, Advances in Neural Information Processing Systems (NIPS), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_111",
            "start": 0,
            "end": 210,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_112@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_112",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_113@0",
            "content": "Lidia Wong,  Chao, RoBLEURT Submission for WMT2021 Metrics Task, 2021, Proceedings of the Sixth Conference on Machine Translation (WMT), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_113",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_114@0",
            "content": "Yu-An Wang, Yun-Nung Chen, What Do Position Embeddings Learn? An Empirical Study of Pre-Trained Language Model Positional Encoding, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_114",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_115@0",
            "content": "Weizhe Yuan, Graham Neubig, Pengfei Liu, BARTScore: Evaluating Generated Text as Text Generation, 2021, Advances in Neural Information Processing Systems (NeurIPS), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_115",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_116@0",
            "content": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, Yoav Artzi, BERTScore: Evaluating Text Generation with BERT, 2020, 8th International Conference on Learning Representations (ICLR), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_116",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "151-ARR_v2_117@0",
            "content": "Wei Zhao, Goran Glava\u0161, Maxime Peyrard, Yang Gao, Robert West, Steffen Eger, On the limitations of cross-lingual encoders as exposed by reference-free machine translation evaluation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "151-ARR_v2_117",
            "start": 0,
            "end": 284,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "151-ARR_v2_0",
            "tgt_ix": "151-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_0",
            "tgt_ix": "151-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_1",
            "tgt_ix": "151-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_1",
            "tgt_ix": "151-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_0",
            "tgt_ix": "151-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_2",
            "tgt_ix": "151-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_4",
            "tgt_ix": "151-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_5",
            "tgt_ix": "151-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_6",
            "tgt_ix": "151-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_3",
            "tgt_ix": "151-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_3",
            "tgt_ix": "151-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_3",
            "tgt_ix": "151-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_3",
            "tgt_ix": "151-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_3",
            "tgt_ix": "151-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_0",
            "tgt_ix": "151-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_7",
            "tgt_ix": "151-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_8",
            "tgt_ix": "151-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_8",
            "tgt_ix": "151-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_8",
            "tgt_ix": "151-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_9",
            "tgt_ix": "151-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_11",
            "tgt_ix": "151-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_10",
            "tgt_ix": "151-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_10",
            "tgt_ix": "151-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_10",
            "tgt_ix": "151-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_8",
            "tgt_ix": "151-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_12",
            "tgt_ix": "151-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_13",
            "tgt_ix": "151-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_13",
            "tgt_ix": "151-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_8",
            "tgt_ix": "151-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_14",
            "tgt_ix": "151-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_15",
            "tgt_ix": "151-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_15",
            "tgt_ix": "151-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_0",
            "tgt_ix": "151-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_16",
            "tgt_ix": "151-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_17",
            "tgt_ix": "151-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_17",
            "tgt_ix": "151-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_17",
            "tgt_ix": "151-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_18",
            "tgt_ix": "151-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_20",
            "tgt_ix": "151-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_21",
            "tgt_ix": "151-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_22",
            "tgt_ix": "151-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_23",
            "tgt_ix": "151-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_24",
            "tgt_ix": "151-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_25",
            "tgt_ix": "151-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_26",
            "tgt_ix": "151-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_27",
            "tgt_ix": "151-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_28",
            "tgt_ix": "151-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_29",
            "tgt_ix": "151-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_30",
            "tgt_ix": "151-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_31",
            "tgt_ix": "151-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_32",
            "tgt_ix": "151-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_33",
            "tgt_ix": "151-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_34",
            "tgt_ix": "151-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_17",
            "tgt_ix": "151-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_35",
            "tgt_ix": "151-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_37",
            "tgt_ix": "151-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_38",
            "tgt_ix": "151-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_39",
            "tgt_ix": "151-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_40",
            "tgt_ix": "151-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_41",
            "tgt_ix": "151-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_42",
            "tgt_ix": "151-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_43",
            "tgt_ix": "151-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_17",
            "tgt_ix": "151-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_45",
            "tgt_ix": "151-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_47",
            "tgt_ix": "151-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_48",
            "tgt_ix": "151-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_49",
            "tgt_ix": "151-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_50",
            "tgt_ix": "151-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_51",
            "tgt_ix": "151-ARR_v2_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_52",
            "tgt_ix": "151-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_54",
            "tgt_ix": "151-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_55",
            "tgt_ix": "151-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_56",
            "tgt_ix": "151-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_57",
            "tgt_ix": "151-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_58",
            "tgt_ix": "151-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_59",
            "tgt_ix": "151-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_53",
            "tgt_ix": "151-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_0",
            "tgt_ix": "151-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_60",
            "tgt_ix": "151-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_61",
            "tgt_ix": "151-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_61",
            "tgt_ix": "151-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_63",
            "tgt_ix": "151-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_61",
            "tgt_ix": "151-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_61",
            "tgt_ix": "151-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_62",
            "tgt_ix": "151-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_61",
            "tgt_ix": "151-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_64",
            "tgt_ix": "151-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_0",
            "tgt_ix": "151-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_65",
            "tgt_ix": "151-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_66",
            "tgt_ix": "151-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_66",
            "tgt_ix": "151-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_66",
            "tgt_ix": "151-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_67",
            "tgt_ix": "151-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_68",
            "tgt_ix": "151-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_68",
            "tgt_ix": "151-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_66",
            "tgt_ix": "151-ARR_v2_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_70",
            "tgt_ix": "151-ARR_v2_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_70",
            "tgt_ix": "151-ARR_v2_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_66",
            "tgt_ix": "151-ARR_v2_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_71",
            "tgt_ix": "151-ARR_v2_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_72",
            "tgt_ix": "151-ARR_v2_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_72",
            "tgt_ix": "151-ARR_v2_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_0",
            "tgt_ix": "151-ARR_v2_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_73",
            "tgt_ix": "151-ARR_v2_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_75",
            "tgt_ix": "151-ARR_v2_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_74",
            "tgt_ix": "151-ARR_v2_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_74",
            "tgt_ix": "151-ARR_v2_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_74",
            "tgt_ix": "151-ARR_v2_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "151-ARR_v2_0",
            "tgt_ix": "151-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_1",
            "tgt_ix": "151-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_2",
            "tgt_ix": "151-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_2",
            "tgt_ix": "151-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_2",
            "tgt_ix": "151-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_2",
            "tgt_ix": "151-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_2",
            "tgt_ix": "151-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_2",
            "tgt_ix": "151-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_2",
            "tgt_ix": "151-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_2",
            "tgt_ix": "151-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_2",
            "tgt_ix": "151-ARR_v2_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_3",
            "tgt_ix": "151-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_4",
            "tgt_ix": "151-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_4",
            "tgt_ix": "151-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_4",
            "tgt_ix": "151-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_4",
            "tgt_ix": "151-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_5",
            "tgt_ix": "151-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_5",
            "tgt_ix": "151-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_5",
            "tgt_ix": "151-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_5",
            "tgt_ix": "151-ARR_v2_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_5",
            "tgt_ix": "151-ARR_v2_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_6",
            "tgt_ix": "151-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_6",
            "tgt_ix": "151-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_6",
            "tgt_ix": "151-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_6",
            "tgt_ix": "151-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_6",
            "tgt_ix": "151-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_6",
            "tgt_ix": "151-ARR_v2_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_6",
            "tgt_ix": "151-ARR_v2_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_6",
            "tgt_ix": "151-ARR_v2_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_7",
            "tgt_ix": "151-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_7",
            "tgt_ix": "151-ARR_v2_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_7",
            "tgt_ix": "151-ARR_v2_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_7",
            "tgt_ix": "151-ARR_v2_7@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_7",
            "tgt_ix": "151-ARR_v2_7@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_8",
            "tgt_ix": "151-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_9",
            "tgt_ix": "151-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_10",
            "tgt_ix": "151-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_11",
            "tgt_ix": "151-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_11",
            "tgt_ix": "151-ARR_v2_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_11",
            "tgt_ix": "151-ARR_v2_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_11",
            "tgt_ix": "151-ARR_v2_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_11",
            "tgt_ix": "151-ARR_v2_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_12",
            "tgt_ix": "151-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_12",
            "tgt_ix": "151-ARR_v2_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_12",
            "tgt_ix": "151-ARR_v2_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_12",
            "tgt_ix": "151-ARR_v2_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_13",
            "tgt_ix": "151-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_14",
            "tgt_ix": "151-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_14",
            "tgt_ix": "151-ARR_v2_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_14",
            "tgt_ix": "151-ARR_v2_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_14",
            "tgt_ix": "151-ARR_v2_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_15",
            "tgt_ix": "151-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_16",
            "tgt_ix": "151-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_16",
            "tgt_ix": "151-ARR_v2_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_16",
            "tgt_ix": "151-ARR_v2_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_16",
            "tgt_ix": "151-ARR_v2_16@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_16",
            "tgt_ix": "151-ARR_v2_16@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_17",
            "tgt_ix": "151-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_18",
            "tgt_ix": "151-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_18",
            "tgt_ix": "151-ARR_v2_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_18",
            "tgt_ix": "151-ARR_v2_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_18",
            "tgt_ix": "151-ARR_v2_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_19",
            "tgt_ix": "151-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_20",
            "tgt_ix": "151-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_21",
            "tgt_ix": "151-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_22",
            "tgt_ix": "151-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_23",
            "tgt_ix": "151-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_23",
            "tgt_ix": "151-ARR_v2_23@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_24",
            "tgt_ix": "151-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_25",
            "tgt_ix": "151-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_26",
            "tgt_ix": "151-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_26",
            "tgt_ix": "151-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_27",
            "tgt_ix": "151-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_27",
            "tgt_ix": "151-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_27",
            "tgt_ix": "151-ARR_v2_27@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_27",
            "tgt_ix": "151-ARR_v2_27@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_27",
            "tgt_ix": "151-ARR_v2_27@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_28",
            "tgt_ix": "151-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_28",
            "tgt_ix": "151-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_29",
            "tgt_ix": "151-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_30",
            "tgt_ix": "151-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_31",
            "tgt_ix": "151-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_32",
            "tgt_ix": "151-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_33",
            "tgt_ix": "151-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_34",
            "tgt_ix": "151-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_35",
            "tgt_ix": "151-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_35",
            "tgt_ix": "151-ARR_v2_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_35",
            "tgt_ix": "151-ARR_v2_35@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_35",
            "tgt_ix": "151-ARR_v2_35@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_36",
            "tgt_ix": "151-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_37",
            "tgt_ix": "151-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_37",
            "tgt_ix": "151-ARR_v2_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_37",
            "tgt_ix": "151-ARR_v2_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_37",
            "tgt_ix": "151-ARR_v2_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_38",
            "tgt_ix": "151-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_39",
            "tgt_ix": "151-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_39",
            "tgt_ix": "151-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_40",
            "tgt_ix": "151-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_41",
            "tgt_ix": "151-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_41",
            "tgt_ix": "151-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_41",
            "tgt_ix": "151-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_41",
            "tgt_ix": "151-ARR_v2_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_41",
            "tgt_ix": "151-ARR_v2_41@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_42",
            "tgt_ix": "151-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_42",
            "tgt_ix": "151-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_42",
            "tgt_ix": "151-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_42",
            "tgt_ix": "151-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_42",
            "tgt_ix": "151-ARR_v2_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_43",
            "tgt_ix": "151-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_44",
            "tgt_ix": "151-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_45",
            "tgt_ix": "151-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_46",
            "tgt_ix": "151-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_47",
            "tgt_ix": "151-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_48",
            "tgt_ix": "151-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_48",
            "tgt_ix": "151-ARR_v2_48@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_48",
            "tgt_ix": "151-ARR_v2_48@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_48",
            "tgt_ix": "151-ARR_v2_48@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_48",
            "tgt_ix": "151-ARR_v2_48@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_48",
            "tgt_ix": "151-ARR_v2_48@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_49",
            "tgt_ix": "151-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_49",
            "tgt_ix": "151-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_50",
            "tgt_ix": "151-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_51",
            "tgt_ix": "151-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_51",
            "tgt_ix": "151-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_52",
            "tgt_ix": "151-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_53",
            "tgt_ix": "151-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_53",
            "tgt_ix": "151-ARR_v2_53@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_53",
            "tgt_ix": "151-ARR_v2_53@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_53",
            "tgt_ix": "151-ARR_v2_53@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_53",
            "tgt_ix": "151-ARR_v2_53@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_53",
            "tgt_ix": "151-ARR_v2_53@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_53",
            "tgt_ix": "151-ARR_v2_53@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_54",
            "tgt_ix": "151-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_54",
            "tgt_ix": "151-ARR_v2_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_54",
            "tgt_ix": "151-ARR_v2_54@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_54",
            "tgt_ix": "151-ARR_v2_54@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_54",
            "tgt_ix": "151-ARR_v2_54@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_54",
            "tgt_ix": "151-ARR_v2_54@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_55",
            "tgt_ix": "151-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_56",
            "tgt_ix": "151-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_56",
            "tgt_ix": "151-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_57",
            "tgt_ix": "151-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_58",
            "tgt_ix": "151-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_58",
            "tgt_ix": "151-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_58",
            "tgt_ix": "151-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_58",
            "tgt_ix": "151-ARR_v2_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_58",
            "tgt_ix": "151-ARR_v2_58@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_59",
            "tgt_ix": "151-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_59",
            "tgt_ix": "151-ARR_v2_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_59",
            "tgt_ix": "151-ARR_v2_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_60",
            "tgt_ix": "151-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_61",
            "tgt_ix": "151-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_62",
            "tgt_ix": "151-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_62",
            "tgt_ix": "151-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_63",
            "tgt_ix": "151-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_63",
            "tgt_ix": "151-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_63",
            "tgt_ix": "151-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_63",
            "tgt_ix": "151-ARR_v2_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_63",
            "tgt_ix": "151-ARR_v2_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_63",
            "tgt_ix": "151-ARR_v2_63@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_64",
            "tgt_ix": "151-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_64",
            "tgt_ix": "151-ARR_v2_64@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_64",
            "tgt_ix": "151-ARR_v2_64@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_64",
            "tgt_ix": "151-ARR_v2_64@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_65",
            "tgt_ix": "151-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_65",
            "tgt_ix": "151-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_66",
            "tgt_ix": "151-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_67",
            "tgt_ix": "151-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_67",
            "tgt_ix": "151-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_68",
            "tgt_ix": "151-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_69",
            "tgt_ix": "151-ARR_v2_69@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_70",
            "tgt_ix": "151-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_71",
            "tgt_ix": "151-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_71",
            "tgt_ix": "151-ARR_v2_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_71",
            "tgt_ix": "151-ARR_v2_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_71",
            "tgt_ix": "151-ARR_v2_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_71",
            "tgt_ix": "151-ARR_v2_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_71",
            "tgt_ix": "151-ARR_v2_71@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_72",
            "tgt_ix": "151-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_73",
            "tgt_ix": "151-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_73",
            "tgt_ix": "151-ARR_v2_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_73",
            "tgt_ix": "151-ARR_v2_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_73",
            "tgt_ix": "151-ARR_v2_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_73",
            "tgt_ix": "151-ARR_v2_73@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_74",
            "tgt_ix": "151-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_75",
            "tgt_ix": "151-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_75",
            "tgt_ix": "151-ARR_v2_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_75",
            "tgt_ix": "151-ARR_v2_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_76",
            "tgt_ix": "151-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_76",
            "tgt_ix": "151-ARR_v2_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_77",
            "tgt_ix": "151-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_78",
            "tgt_ix": "151-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_79",
            "tgt_ix": "151-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_80",
            "tgt_ix": "151-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_81",
            "tgt_ix": "151-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_82",
            "tgt_ix": "151-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_83",
            "tgt_ix": "151-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_84",
            "tgt_ix": "151-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_85",
            "tgt_ix": "151-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_86",
            "tgt_ix": "151-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_87",
            "tgt_ix": "151-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_88",
            "tgt_ix": "151-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_89",
            "tgt_ix": "151-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_90",
            "tgt_ix": "151-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_91",
            "tgt_ix": "151-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_92",
            "tgt_ix": "151-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_93",
            "tgt_ix": "151-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_94",
            "tgt_ix": "151-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_95",
            "tgt_ix": "151-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_96",
            "tgt_ix": "151-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_97",
            "tgt_ix": "151-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_98",
            "tgt_ix": "151-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_99",
            "tgt_ix": "151-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_100",
            "tgt_ix": "151-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_101",
            "tgt_ix": "151-ARR_v2_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_102",
            "tgt_ix": "151-ARR_v2_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_103",
            "tgt_ix": "151-ARR_v2_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_104",
            "tgt_ix": "151-ARR_v2_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_105",
            "tgt_ix": "151-ARR_v2_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_106",
            "tgt_ix": "151-ARR_v2_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_107",
            "tgt_ix": "151-ARR_v2_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_108",
            "tgt_ix": "151-ARR_v2_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_109",
            "tgt_ix": "151-ARR_v2_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_110",
            "tgt_ix": "151-ARR_v2_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_111",
            "tgt_ix": "151-ARR_v2_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_112",
            "tgt_ix": "151-ARR_v2_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_113",
            "tgt_ix": "151-ARR_v2_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_114",
            "tgt_ix": "151-ARR_v2_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_115",
            "tgt_ix": "151-ARR_v2_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_116",
            "tgt_ix": "151-ARR_v2_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "151-ARR_v2_117",
            "tgt_ix": "151-ARR_v2_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 784,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "151-ARR",
        "version": 2
    }
}