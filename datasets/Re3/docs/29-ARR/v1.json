{
    "nodes": [
        {
            "ix": "29-ARR_v1_0",
            "content": "E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_2",
            "content": "Building huge and highly capable language models has been a trend in the past years. Despite their great performance, they incur high computational cost. A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression. This paper proposes an effective dynamic inference approach, called E-LANG, which distributes the inference between large accurate Supermodels and light-weight Swift models. To this end, a decision making module routes the inputs to Super or Swift models based on the energy characteristics of the representations in the latent space. This method is easily adoptable and architecture agnostic. As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, reassembling of modules, or re-training. Unlike existing methods that are only applicable to encoder-only backbones and classification tasks, our method also works for encoderdecoder structures and sequence-to-sequence tasks such as translation. The E-LANG performance is verified through a set of experiments with T5 and BERT backbones on GLUE, Su-perGLUE, and WMT. In particular, we outperform T5-11B with an average computations speed-up of 3.3\u00d7 on GLUE and 2.9\u00d7 on SuperGLUE. We also achieve BERT-based SOTA on GLUE with 3.2\u00d7 less computations. Code is available in supplementary materials.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "29-ARR_v1_4",
            "content": "With the introduction of influential language models such as BERT (Devlin et al., 2019), a trend in natural language processing (NLP) research has been to develop high capacity models and push their performance to new levels. Consequently, state-of-the-art (SOTA) results were achieved on various benchmarks using these models; GPT-3 (Brown et al., 2020), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020), ELECTRA (Clark et al., 2020), and DeBERTa (He et al., 2021) to name a few. A potential down-side, however, is that the number of parameters or floating point operations (FLOPs) for these models can get extremely large. For example, Gshard (Lepikhin et al., 2021) comes with 600B parameters with an enormous amount of computation. This in turn results in a higher inference latency, which is not desirable for latency-sensitive applications.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_5",
            "content": "A common solution to speed-up the large language models is to apply model compression . Although generally successful, compression does come with a trade-off on accuracy, and may lose performance if compression is heavy. In addition, these methods usually compress a model to a fixed smaller size, where a separate model is required for each possible computational budget. An alternative approach explored in the literature is to leverage dynamic inferencing in a way that examples may be routed to different (potentially lower cost) paths throughout the network. For example, a temporal early-exit model (Shen et al., 2017;Yu et al., 2018) terminates the procedure of reading the input sequence when sufficient evidence has been found for accurate predictions. Instance-wise early-exiting is another technique, which allows a sample to adaptively choose from multiple available exit nodes if some conditions are met. Consequently, earlier exists require less computation and lead to a lower latency. Adjusting the size of the model at the inference time by choosing adaptive width and depth is also another approach employed for dynamic inference (Kim and Cho, 2021;. There is a variety of adaptive/dynamic inference approaches proposed, however, a general down-side for many of these methods is that often times they require a careful architecture design, manipulation of network modules, or even re-training.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_6",
            "content": "In this paper, we propose a simple but rather effective approach of dynamically distributing the inference between the original large model (called the Super model) and a light-weight (e.g., compressed) model referred to as the Swift model. To this end, we design an energy-based decision making module that routes examples to the appropriate model based on the negative free energy of the latent space representations, such that the Swift model attains a high accuracy on the examples sent to it. The remaining samples are then forwarded to the Super model that is supposed to have a good performance on all examples. Since the Swift model can make highly accurate predictions over the majority of the samples, E-LANG significantly reduces the overall computational cost, while maintains the high accuracy of the Super model. Although simple, this strategy achieves SOTA results on multiple structures (e.g., T5 and BERT) and benchmarks (e.g., GLUE and SuperGLUE). Due to its desirable practical characteristics, this method is a strong candidate for the practical application of Super models. The main contributions of the paper are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_7",
            "content": "\u2022 Combining Super models with high accuracy and latency and Swift models with lower accuracy and latency, to achieve high accuracy and low latency. In other words, by employing our method, we can achieve the high levels of accuracy provided by Super models, but at a lower computational cost. Our method is easily adoptable, architecture agnostic, and orthogonal to many other existing methods. It can be applied to black-box pre-trained models without a need for architectural manipulations, careful reassembling of modules, or re-training. \u2022 An energy-based routing mechanism for directing examples to the Super or Swift. This provides a dynamic trade-off between the accuracy and computational cost that outperforms the previous works in both fixed-size and dynamic inference (with zero overhead for real-time adjustment of speed/accuracy). As such, E-LANG acts like a knob for adjusting the accuracy-latency trade-off in real-time during model serving. \u2022 To the best of our knowledge, our method is the first generic approach to apply dynamic inference on both encoder-only and encoderdecoder architectures (e.g., T5) and also can extend the usage beyond classification tasks, to sequence-to-sequence tasks such as translation.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_8",
            "content": "Related Works",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "29-ARR_v1_9",
            "content": "As mentioned, compression is a widely used strategy to speed-up the large language models Gupta and Agrawal, 2020). This involves incorporating techniques such as quantization of weights and activations (Bai et al., 2020;Shen et al., 2020;Kim et al., 2021;Jin et al., 2021), knowledge distillation (KD) (Hinton et al., 2015;Jiao et al., 2020;Sanh et al., 2019), pruning/sharing (Gordon et al., 2020;, multi-device distribution (Banitalebi-Dehkordi et al., 2021), or a combination of these techniques (Cheng et al., 2017;Polino et al., 2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_10",
            "content": "Among all the compression techniques, creating a fixed-size small version of large models along with distillation has been popular in the recent years. Sanh et al. (2019) introduced DistillBERT, which was a smaller version of BERT trained with distillation for general purposes. Another compact variant of BERT was proposed by Mobile-BERT (Sun et al., 2020) in which inverted bottleneck structures and progressive knowledge transfer were used. TinyBERT (Jiao et al., 2020) also presented a novel two-stage transformer distillation for both pre-training and task-specific fine-tuning. In (Iandola et al., 2020), the usage of grouped convolutions was studied to design SqueezeBERT. ELM (Jiao et al., 2021), a layer mapping search framework, was also proposed for improving downstream BERT distillation. A recent method, Ghost-BERT , employed softmaxnormalized 1D convolutions as ghost modules to generate more features with cheap operations.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_11",
            "content": "Although compression techniques in general are effective, they come with a trade-off on accuracy, and may lose performance in case of high ratio compression. In addition, an individual fixed-size model is required for each possible computational budget. As stated in the introduction, the alternative solution is dynamic inference, which can be achieved with either early-exit or length/depthadaptive models. One of the first temporal earlyexit strategies was proposed by ReasoNet (Shen et al., 2017), which stops its reading procedure when sufficient evidence has been found for answering a question. Similarly, in (Yu et al., 2018), an early stopping method applicable to classification tasks was presented. DeeBERT also proposed an instance-wise multi-exit method via the entropy of the output probability distribution to speed-up BERT inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_12",
            "content": "As a length-adaptive method, Kim and Cho (2021) introduced a dynamic inference framework with one-shot training of transformers for both sequence-and token-level classification. Also, in , an architecture named Dyn-aBERT was proposed for adaptively adjusting the computations by choosing sub-networks of different widths and depths. Both Length-Adaptive and DynaBERT utilized knowledge distillation and data augmentation to improve their performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_13",
            "content": "Although early-exit and adaptive methods have made significant progress and work well in practice, they often require architectural manipulation and re-training. In addition, they are only applicable to encoder-only backbones and classification tasks. In contrast, our method can work with out-of-the-box pre-trained models without a need for re-training and are also applicable for encoder-decoder structures and sequence-to-sequence tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_14",
            "content": "Proposed Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "29-ARR_v1_15",
            "content": "We propose a new energy-based joint inference method called E-LANG, where a large/accurate language model (Super) is jointly employed with a small/fast one (Swift) to achieve efficient inference without sacrificing the accuracy. To this end, a routing mechanism empowered by energy-based models (EBM) is introduced to dynamically distribute the input samples between the Super and Swift models. Similar to the out-of-distribution (OOD) detection problem, our goal is to identify the OOD samples that are hard to handle for the Swift and forward them to the Super model. On the other hand, we have the in-distribution data for which the Swift can make highly reliable and accurate predictions. In other words, the routing mechanism needs to detect whether or not the input data fits in the Swift's distribution (i.e., the one the Swift has been trained with). Inspired by the success of EBMs in dealing with OOD detection problems , the energy characteristics of data samples for an efficient and effective routing are investigated in our work. The overall framework of E-LANG is shown in Figure 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_16",
            "content": "Energy-Based Models",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "29-ARR_v1_17",
            "content": "The goal of EBM is to build an energy function denoted by E(x) : R D \u2192 R that maps an input data x \u2208 R D to a non-probabilistic energy value y \u2208 R. To turn a collection of arbitrary energies for all possible outputs (denoted by Y ) into a normalized probability distribution, Gibbs distribution can be used as follows (LeCun et al., 2006):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_18",
            "content": "p(y|x) = e \u2212E(x,y) y \u2208Y e \u2212E(x,y ) ,(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_19",
            "content": "where the negative log of the denominator expresses the Helmholtz free energy (LeCun et al., 2006) defined as F (x) = \u2212log y \u2208Y e \u2212E(x,y ) . In machine learning, there is a deep relationship between the EBMs and discriminative models, which can be seen by connecting the Gibbs distribution in Equation ( 1) and the categorical distribution derived for a discriminative model. A discriminative classifier is defined as a function for mapping the input x to C real-valued logits (i.e., for C number of class labels): f (x) : R D \u2192 R C . In order to derive a categorical distribution over C possible outputs, the softmax function is utilized:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_20",
            "content": "p(y|x) = e fy(x) C i e f i (x) ,(2)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_21",
            "content": "where f y (x) denotes the logit (probability) of the yth class label. Based on the inherent connection between the Gibbs and categorical distributions defined in ( 1) and ( 2), the energy function for a given input (x, y) can be defined as E(x, y) = \u2212f y (x).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_22",
            "content": "The free energy function F (x; f ) can then be obtained by taking the negative log of the categorical distribution denominator as: x) .",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_23",
            "content": "F (x; f ) = \u2212log C i e f i (",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_24",
            "content": "(3)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_25",
            "content": "Energy-Based Joint Inference",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "29-ARR_v1_26",
            "content": "Our goal is to detect the easy samples suitable for the Swift, which are indeed the ones with high likelihood in the density function. The energy-based density function for Swift is then defined as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_27",
            "content": "p(x) = e \u2212F (x;f ) x e \u2212F (x;f ) ,(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_28",
            "content": "where the denominator is the normalized densities, which can be intractable to compute or estimate. By taking the logarithm of both sides, we obtain:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_29",
            "content": "log p(x) = \u2212F (x; f ) \u2212 log( x e \u2212F (x;f ) ). (5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_30",
            "content": "The log( x e \u2212F (x;f ) ) term has no effect on the distribution of the overall energy values because it is constant for all x. As a result, \u2212F (x; f ), i.e., the negative free energy, has a linear alignment with the log likelihood function, which makes it a well-suited solution to the easy vs. hard detection problem in our framework. To this end, lower energy values indicate higher likelihood and represent easier (more fit) samples for the Swift model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_31",
            "content": "More precisely, for a threshold \u03b4 on the density function such that p(x) < \u03b4, then a threshold t on the negative free energy can be calculated according to (5) as \u2212F (x; f ) < t = log(\u03b4 x e \u2212F (x;f ) ).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_32",
            "content": "In practice, for a given input, an energy function is applied to the outputs of the Swift model during inference time to calculate the energy score. Then, if the negative energy value is smaller than a threshold, the input is identified as a bad sample for the Swift, and is sent to the Super model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_33",
            "content": "Given the energy threshold t, the Swift classifier f (x), and the Super classifier defined as g(x) : R D \u2192 R C , the joint inference function J(x; f, g, t) \u2208 [1, C] for a classification task with C classes can then be expressed by:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_34",
            "content": "J(x; f, g, t) = f (x) if \u2212 F (x; f ) \u2265 t g(x) otherwise. (6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_35",
            "content": "Encoder-Decoder Architectures",
            "ntype": "title",
            "meta": {
                "section": "3.2.1"
            }
        },
        {
            "ix": "29-ARR_v1_36",
            "content": "The proposed energy-based joint inference solution can be directly applied to the encoder-only models such as BERT that are designed for text classification tasks. To this end, the energy scores corresponding to the BERT-based Swift model are obtained using Equation (3) and the joint inference is performed based on Equation 6. On the other hand, for the encoder-decoder (autoencoder) architectures such as T5, which are usually considered as generative models, some modifications are required. Encoder-decoder models are basically designed for sequence-to-sequence (e.g., text-to-text) problems such as translation or summarization. Although such models can also be employed for classification tasks, they still consider the task as a text generation (sequence-to-sequence) problem, where the target labels and the output predictions are treated as a sequence or a piece of text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_37",
            "content": "In Section 3.1, it was discussed that there is an inherent connection between the discriminative classifiers and the EBMs. In order to benefit from this characteristic for encoder-decoder architectures, we consider adding an extra classification head (i.e., a single linear layer) to the Swift model. As encoders are commonly considered as better feature extractors for training a classifier rather than the decoders, we place the extra head after the Swift encoder. While freezing the pre-trained encoder model (denoted by f E ), the extra energy head (denoted by h) is trained as a regular classifier head with C class labels. Note that the decoder is not required for training the head. The corresponding free energy function is then defined as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_38",
            "content": "F (x; f E , h) = \u2212log C i e h i f E (x) ,(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_39",
            "content": "where f E (x) denotes the outputs of the encoder's last hidden state. These features are then fed to the extra head h to obtain the logits for the ith class required for computing the energy scores.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_40",
            "content": "In this approach, as the decoder part of the Swift model is not required for calculating the energy scores, less computations are involved and the joint inference is performed more efficiently.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_41",
            "content": "For text-to-text (or sequence-to-sequence) problems such as translation, the output is a sequence of M word-pieces from a vocabulary/dictionary of size N . To still utilize the relationship of discriminative models and EBMs in designing and training the extra energy head, we can treat the text-to-text models as M multi-class classifiers. In this case, the number of class labels, i.e., C in (7), is equal to N . The final energy score is then calculated as the average of M energy values as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_42",
            "content": "F (x; f E , h) = \u2212 1 M M m log C i e h m,i f E (x) , (8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_43",
            "content": "where h m,i (.) denotes the logits corresponding to the mth word in the sequence and ith class label.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_44",
            "content": "Denote the Swift's decoder by f D , the joint inference function, J(x; f, g, h, t), based on energy scores in either Equation ( 7) or ( 8) is expressed as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_45",
            "content": "J = f D f E (x) if \u2212 F (x; f E , h) \u2265 t g(x)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_46",
            "content": "otherwise.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_47",
            "content": "(9)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_48",
            "content": "Softmax and Entropy Mechanisms",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "29-ARR_v1_49",
            "content": "In addition to energy, softmax and entropy scores can also be used for analyzing the Swift model's performance in the routing mechanism. In this sub-section, we study the mathematical connection of them with the energy score and their potential to solve our problem.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_50",
            "content": "Softmax-Based Mechanism",
            "ntype": "title",
            "meta": {
                "section": "3.3.1"
            }
        },
        {
            "ix": "29-ARR_v1_51",
            "content": "The softmax score for a classifier is expressed by: x) . (10) By taking the logarithm of both sides, we see the connection between the log of the softmax and the free energy score formulated in Equation (3):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_52",
            "content": "max y p(y|x) = max y e f y (x) C i e f i (x) = e f max (x) C i e f i (",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_53",
            "content": "log max y p(y|x) = log(e fmax(x) ) \u2212 log C i e f i (x) = f max (x) + F (x; f ),(11)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_54",
            "content": "where all logits are shifted by their maximum f max (x). Plugging in the energy term to (5) yields:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_55",
            "content": "log max y p(y|x) = \u2212log(p(x)) + f max (x) \u2212log x e \u2212F (x;f ) .(12)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_56",
            "content": "It is observed that for the samples with high likelihood of being in the Swift's distribution, the free energy goes lower, but the max logit tends to go higher. Due to this shifting, unlike the energy score, the softmax score is not well-aligned with the probability density p(x). As a result, the softmax score is less reliable for our routing module to analyze the performance of the Swift.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_57",
            "content": "Entropy-Based Mechanism",
            "ntype": "title",
            "meta": {
                "section": "3.3.2"
            }
        },
        {
            "ix": "29-ARR_v1_58",
            "content": "The entropy score is a measure of randomness in the processed information, and is calculated as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_59",
            "content": "H(x; f ) = \u2212 C i f i .log(f i ),(13)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_60",
            "content": "where f i (x) is the probability (logit) corresponding to the ith class label. Let U be the internal energy, i.e., the expectation value of the energy function (Oh et al., 2020), defined by:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_61",
            "content": "U (x; f ) = C i E(x, i)f i .(14)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_62",
            "content": "According to Oh et al. (2020), the entropy can be defined in terms of the internal and free energy functions as: H(x; f ) = U (x; f ) \u2212 F (x; f ), where all logits are shifted by the internal energy U . Substituting the free energy from (5) yields: (15) which shows, due to the shifting caused by internal energy, the entropy is not reliably aligned with the probability density p(x). Thus, it is a less suitable routing mechanism unlike the energy score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_63",
            "content": "H(x; f ) = log(p(x)) + U (x; f ) + log x e \u2212F (x;f ) ,",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_64",
            "content": "Experimental Results",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "29-ARR_v1_65",
            "content": "In this section, the performance of E-LANG on different architectures such as T5 and BERT; and benchmarks such as GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a), and WMT (Bojar et al., 2016) is evaluated and compared with the Super models and previous works.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_66",
            "content": "T5-Based Joint Inference",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "29-ARR_v1_67",
            "content": "In Table 1, the T5-based results on GLUE, Super-GLUE, and WMT benchmarks are reported. For all the tasks, we use T5-11B (with 87\u00d710 11 FLOPs) and T5-large (with 4.25\u00d710 11 FLOPs) as our Super and Swift models, respectively. The average GPUbased running time and accuracy of both models compared with E-LANG are also summarized in the table. Note that the T5 models used in this experiment have been separately fine-tuned on each of the downstream tasks given in Table 1. The extra energy head for each of these tasks was also separately trained and used based on the task-specific number of classes, i.e., C in Equation ( 7).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_68",
            "content": "The total FLOPs for our method is measured as a weighted average of the Super and Swift FLOPs based on their usage frequency as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_69",
            "content": "F LOP s = 1 N sw + N su N sw .(F E sw +F h +F D sw ) + N su .(F E sw + F h + F su ) ,(16)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_70",
            "content": "where N su and N sw are respectively the number of samples processed by the Super (with F su FLOPs) and Swift (with F E sw , F D sw , and F h FLOPs for the encoder, decoder, and energy head). Note that F h is equal to \u22480.00001\u00d710 11 , which has a very insignificant overhead in our framework.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_71",
            "content": "As presented in Table 1, E-LANG can reach the Super model's accuracy on all GLUE tasks with an average 3.3X in FLOPs and 1.8X in running time speed-ups. For some tasks such as QNLI, MRPC, and COLA, we even outperform the Super model, which leads to a higher average accuracy of 89.7% than the Super model with 89.5% on GLUE. For the SuperGLUE benchmark, with an average FLOPs and running time speed-up of 2.9X and 2.0X, our method achieves the same accuracy as the Super model on MRC and CB; and better accuracy on RTE and WIC. On BoolQ and COPA, although 99% and 97% of the Super's accuracy are respectively obtained, it is with 1.7X and 1.4X less FLOPs and latency, on average. In order to analyze the generality of E-LANG to other NLP problems rather than text classification (Section 3.2.1), we also apply our method to two text-to-text tasks including SuperGLUE's WSC and WMT's English-to-Romanian (EnRo) translation. As given in the table, our method achieves the Super model's accuracy on both WSC and EnRo with 4.2X and 1.4X less FLOPs, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_72",
            "content": "Figure 2 illustrates the accuracy vs. FLOPs trade-off curves for some tasks in GLUE and Super-GLUE benchmarks. The curves related to all tasks are given in the appendix. The trade-off points on the curves are dynamically achieved at the inference time by selecting different thresholds, i.e., t in Equations ( 6) and ( 9). Larger values for t will result in routing more input data to the Super model, which consequently provides more accurate, but slower inference. As the Swift is able to make accurate predictions for the majority of input data, the dynamic inference with a small enough t can reach the Super model's accuracy but with a much lower computational cost and latency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_73",
            "content": "Ablation Studies",
            "ntype": "title",
            "meta": {
                "section": "4.1.1"
            }
        },
        {
            "ix": "29-ARR_v1_74",
            "content": "In Sections 3.3.1 and 3.3.2, the possibility of using softmax and entropy scores instead of energy score was theoretically analyzed. To support that analysis and also experimentally evaluate the performance of different routing mechanisms, an ablation study on GLUE is performed, which is presented in Table 2. In this study, we report the joint inference results based on softmax, entropy, and random scores (i.e., randomly distributing the samples between Super and Swift). Our experiments show that, compared to the random score, softmax and entropy can result in satisfactory performance on routing the samples. However, as also discussed in Sections 3.3.1 and 3.3.2, the energy score is still a better routing mechanism with about 14% less FLOPs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_75",
            "content": "The results with the usage of different Swift models including T5-small (with 0.33\u00d710 11 FLOPs) and T5-base (with 1.24\u00d710 11 FLOPs) are also given in Table 2. Using these models as Swifts can lead to good performance on some tasks, but not all of them. For example, on SST2, the joint inference with T5-small and T5-base Swifts can respectively reach the Super's accuracy with 1.9X and 2.X less computations. In general, although these models are smaller and require less FLOPs, our results in Table 2 indicate that they perform worse than T5-large in our joint inference structure.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_76",
            "content": "In Figure 2, the trade-off curves for different Swift models are shown for GLUE and SuperGLUE. Moreover, to show the effectiveness of the extra energy head for the Swift encoder, the E-LANG results based on last linear layer of the Swift decoder is also given and compared in Table 2. As reported, the E-LANG empowered by the energy head on the Swift encoder outperforms the case with the decoder's head in both FLOPs (36.8% less) and accuracy (0.7% better). As explained in Section 3.2.1, this shows the deep connection between the encoder's features, discriminative models, and the proposed routing mechanism via the energy head.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_77",
            "content": "We observed that E-LANG can achieve a high performance even when applied to individually pretrained Super and Swift models. However, more improvement can still be obtained by performing KD from the Super model to the Swift model, especially at the fine-tuning process for downstream tasks. To study this, we apply the KD technique in (Sanh et al., 2019) to the Super and Swift models for some GLUE tasks. As summarized in Table 3, the Super model's accuracy for QNLI, SST2, and COLA is respectively attained by the distillationbased E-LANG with 29.2%, 48.5%, and 14.3% less FLOPs than E-LANG (without distillation). The results in this experiment show the effectiveness of E-LANG along with other compression techniques such as distillation. The trade-off curves for this experiment will be provided in the appendix.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_78",
            "content": "BERT-Based Joint Inference",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "29-ARR_v1_79",
            "content": "In this section, the proposed energy-based joint inference method is applied to the BERT architecture (Devlin et al., 2019) and compared with BERT-based SOTA in both fixed-size and dynamic inference. The majority of the previous methods employ knowledge distillation and data augmentation techniques for training their student models. For a fair comparison, we follow the same practice and use the transformer distillation and augmentation strategies in TinyBERT (Jiao et al., 2020) to train and prepare our Swift model (i.e., BERT T iny with 1.2 \u00d7 10 9 FLOPs). Moreover, similar to the other works, we use BERT Base (with 21.8 \u00d7 10 9 FLOPs) as our Super (i.e., teacher) model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_80",
            "content": "In Table 4, the comparison results with the baseline BERT Base and SOTA on GLUE benchmark are presented in terms of accuracy, FLOPs, and latency. Compared to the Super model, E-LANG delivers better accuracy on SST2 and RTE with 3.5X and 2.0X FLOPs speed-up; and the same accuracy on QNLI, MRPC, and QQP with 2.4X, 2.7X, and 7.0X FLOPs speed-up, respectively. On MNLI and COLA, 99.8% and 97.3% of the Super model's accuracy are achieved, but with an average FLOPs speed-up of 2.3X. On average, E-LANG outperforms the Super model with 0.1% higher accuracy, 3.2X less FLOPs, and 1.6X less latency.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_81",
            "content": "Compared with SOTA, our method achieves the best performance on all GLUE tasks, except MRPC for which SqueezeBERT outperforms all due to having a more accurate teacher (Iandola et al., 2020). There are some works such as ELECTRA (Clark et al., 2020) and MobileBERT (Sun et al., 2020) that require less FLOPs than our method, but they only reach 95% of the baseline's accuracy. Compared to other methods, GhostBERT and DynaBERT give the closest performance to the baseline and even the same as ours on some tasks such as QNLI. However, on average, they still need about 30% more FLOPs on GLUE compared to E-LANG.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_82",
            "content": "The E-LANG accuracy vs. FLOPs trade-off curves compared to SOTA on some of GLUE tasks are shown in Figure 3. The trade-off curves for all the tasks are reported in the appendix. Among the SOTA methods presented in Table 4 and Fig To investigate the orthogonality of E-LANG with others, we integrate our energy-based joint inference strategy with DynaBERT that is SOTA in BERT-based adaptive inference. In other words, we analyze whether E-LANG can be added on top of other efficient methods to benefit both from their designs and our approach. In this experiment, the DynaBERT configurations with the highest accuracy (i.e., width=0.75 & depth=1.0) and the lowest FLOPs (i.e., width=0.5 & depth=0.25) are respectively employed as the Super and Swift models in our framework. The corresponding joint inference results on MNLI, SST2, and QQP are reported in Table 5. As observed, we accomplish the Dyn-aBERT Super's accuracy for MNLI and SST2 with 1.7X and 3.1X less FLOPs. For QQP, our method combined with DynaBERT even outperforms Dyn-aBERT by 0.1% with 2.6X FLOPs speed-up.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_83",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "29-ARR_v1_84",
            "content": "In this paper, we introduced E-LANG, an energybased joint inference approach, which integrates Super and Swift language models for achieving efficient inference without sacrificing the accuracy.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_85",
            "content": "Our method can work with both encoder-only (e.g., BERT) and encoder-decoder (e.g., T5) architectures, and is also applicable for text classification and sequence-to-sequence problems. The proposed joint inference strategy was theoretically and experimentally analyzed with an extensive set of experiments and ablation studies. Our results showed that E-LANG outperforms SOTA in both fixed-size and dynamic inference over different benchmarks such as GLUE and SuperGLUE. One future direction to this work is to apply E-LANG to multiple Super and Swift models with different sizes.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "29-ARR_v1_86",
            "content": "UNKNOWN, None, 2020, Binarybert: Pushing the limit of bert quantization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Binarybert: Pushing the limit of bert quantization",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_87",
            "content": "Amin Banitalebi-Dehkordi, Naveen Vedula, Jian Pei, Fei Xia, Lanjun Wang, Yong Zhang, Auto-split: A general framework of collaborative edge-cloud ai, 2021, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Amin Banitalebi-Dehkordi",
                    "Naveen Vedula",
                    "Jian Pei",
                    "Fei Xia",
                    "Lanjun Wang",
                    "Yong Zhang"
                ],
                "title": "Auto-split: A general framework of collaborative edge-cloud ai",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_88",
            "content": "Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Findings of the 2016 conference on machine translation, 2016, Proceedings of the First Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Ond\u0159ej Bojar",
                    "Rajen Chatterjee",
                    "Christian Federmann",
                    "Yvette Graham",
                    "Barry Haddow",
                    "Matthias Huck",
                    "Antonio Yepes",
                    "Philipp Koehn",
                    "Varvara Logacheva",
                    "Christof Monz"
                ],
                "title": "Findings of the 2016 conference on machine translation",
                "pub_date": "2016",
                "pub_title": "Proceedings of the First Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_89",
            "content": "UNKNOWN, None, , , .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_90",
            "content": "Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, Michael Carbin, The lottery ticket hypothesis for pretrained bert networks, 2020, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Tianlong Chen",
                    "Jonathan Frankle",
                    "Shiyu Chang",
                    "Sijia Liu",
                    "Yang Zhang",
                    "Zhangyang Wang",
                    "Michael Carbin"
                ],
                "title": "The lottery ticket hypothesis for pretrained bert networks",
                "pub_date": "2020",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_91",
            "content": "UNKNOWN, None, 2017, A survey of model compression and acceleration for deep neural networks, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "A survey of model compression and acceleration for deep neural networks",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_92",
            "content": "UNKNOWN, None, 2020, Electra: Pre-training text encoders as discriminators rather than generators. International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Electra: Pre-training text encoders as discriminators rather than generators. International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_93",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, NAACL-HLT (1), .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "NAACL-HLT (1)",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_94",
            "content": "UNKNOWN, None, 2020, Compressing bert: Studying the effects of weight pruning on transfer learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Compressing bert: Studying the effects of weight pruning on transfer learning",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_95",
            "content": "UNKNOWN, None, 2020, Compression of deep learning models for text: A survey, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Compression of deep learning models for text: A survey",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_96",
            "content": "Manish Gupta, Vasudeva Varma, Sonam Damani, Kedhar Nath Narahari, Compression of deep learning models for nlp, 2020, Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Manish Gupta",
                    "Vasudeva Varma",
                    "Sonam Damani",
                    "Kedhar Nath Narahari"
                ],
                "title": "Compression of deep learning models for nlp",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_97",
            "content": "Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Deberta: Decoding-enhanced bert with disentangled attention, 2021, International Conference on Learning Representations, ICLR, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Pengcheng He",
                    "Xiaodong Liu",
                    "Jianfeng Gao",
                    "Weizhu Chen"
                ],
                "title": "Deberta: Decoding-enhanced bert with disentangled attention",
                "pub_date": "2021",
                "pub_title": "International Conference on Learning Representations, ICLR",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_98",
            "content": "G Hinton, O Vinyals, J Dean, Distilling the knowledge in a neural network, 2015, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "G Hinton",
                    "O Vinyals",
                    "J Dean"
                ],
                "title": "Distilling the knowledge in a neural network",
                "pub_date": "2015",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_99",
            "content": "Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu, Dynabert: Dynamic bert with adaptive width and depth, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Lu Hou",
                    "Zhiqi Huang",
                    "Lifeng Shang",
                    "Xin Jiang",
                    "Xiao Chen",
                    "Qun Liu"
                ],
                "title": "Dynabert: Dynamic bert with adaptive width and depth",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_100",
            "content": "UNKNOWN, None, 2021, Ghostbert: Generate more features with cheap operations for bert, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Ghostbert: Generate more features with cheap operations for bert",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_101",
            "content": "Forrest Iandola, Albert Shaw, Ravi Krishna, Kurt Keutzer, SqueezeBERT: What can computer vision teach NLP about efficient neural networks?, 2020, Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Forrest Iandola",
                    "Albert Shaw",
                    "Ravi Krishna",
                    "Kurt Keutzer"
                ],
                "title": "SqueezeBERT: What can computer vision teach NLP about efficient neural networks?",
                "pub_date": "2020",
                "pub_title": "Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_102",
            "content": "Xiaoqi Jiao, Huating Chang, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Improving task-agnostic bert distillation with layer mapping search, 2021, Neurocomputing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Xiaoqi Jiao",
                    "Huating Chang",
                    "Yichun Yin",
                    "Lifeng Shang",
                    "Xin Jiang",
                    "Xiao Chen",
                    "Linlin Li",
                    "Fang Wang",
                    "Qun Liu"
                ],
                "title": "Improving task-agnostic bert distillation with layer mapping search",
                "pub_date": "2021",
                "pub_title": "Neurocomputing",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_103",
            "content": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Tinybert: Distilling bert for natural language understanding, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Xiaoqi Jiao",
                    "Yichun Yin",
                    "Lifeng Shang",
                    "Xin Jiang",
                    "Xiao Chen",
                    "Linlin Li",
                    "Fang Wang",
                    "Qun Liu"
                ],
                "title": "Tinybert: Distilling bert for natural language understanding",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_104",
            "content": "UNKNOWN, None, 2021, Kdlsq-bert: A quantized bert combining knowledge distillation with learned step size quantization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Kdlsq-bert: A quantized bert combining knowledge distillation with learned step size quantization",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_105",
            "content": "UNKNOWN, None, 2021, Lengthadaptive transformer: Train once with length drop, use anytime with search, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Lengthadaptive transformer: Train once with length drop, use anytime with search",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_106",
            "content": "UNKNOWN, None, , 2021. I-bert: Integer-only bert quantization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "2021. I-bert: Integer-only bert quantization",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_107",
            "content": "Yann Lecun, Sumit Chopra, Raia Hadsell, M Ranzato, F Huang, A tutorial on energy-based learning, 2006, Predicting structured data, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Yann Lecun",
                    "Sumit Chopra",
                    "Raia Hadsell",
                    "M Ranzato",
                    "F Huang"
                ],
                "title": "A tutorial on energy-based learning",
                "pub_date": "2006",
                "pub_title": "Predicting structured data",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_108",
            "content": "Youngwan Lee, Joong-Won Hwang, Sangrok Lee, Yuseok Bae, Jongyoul Park, An energy and gpu-computation efficient backbone network for real-time object detection, 2019, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Youngwan Lee",
                    "Joong-Won Hwang",
                    "Sangrok Lee",
                    "Yuseok Bae",
                    "Jongyoul Park"
                ],
                "title": "An energy and gpu-computation efficient backbone network for real-time object detection",
                "pub_date": "2019",
                "pub_title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_109",
            "content": "UNKNOWN, None, 2021, Gshard: Scaling giant models with conditional computation and automatic sharding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Gshard: Scaling giant models with conditional computation and automatic sharding",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_110",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Roberta: A robustly optimized bert pretraining approach",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_111",
            "content": "Sangchul Oh, Abdelkader Baggag, Hyunchul Nha, Entropy, free energy, and work of restricted boltzmann machines, 2020, Entropy, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Sangchul Oh",
                    "Abdelkader Baggag",
                    "Hyunchul Nha"
                ],
                "title": "Entropy, free energy, and work of restricted boltzmann machines",
                "pub_date": "2020",
                "pub_title": "Entropy",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_112",
            "content": "A Polino, R Pascanu, Dan Alistarh, Model compression via distillation and quantization, 2018, ArXiv, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "A Polino",
                    "R Pascanu",
                    "Dan Alistarh"
                ],
                "title": "Model compression via distillation and quantization",
                "pub_date": "2018",
                "pub_title": "ArXiv",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_113",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter J Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_114",
            "content": "Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2019, 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing -NeurIPS, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Victor Sanh",
                    "Lysandre Debut",
                    "Julien Chaumond",
                    "Thomas Wolf"
                ],
                "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
                "pub_date": "2019",
                "pub_title": "5th Workshop on Energy Efficient Machine Learning and Cognitive Computing -NeurIPS",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_115",
            "content": "Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, W Michael, Kurt Mahoney,  Keutzer, Q-bert: Hessian based ultra low precision quantization of bert, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Sheng Shen",
                    "Zhen Dong",
                    "Jiayu Ye",
                    "Linjian Ma",
                    "Zhewei Yao",
                    "Amir Gholami",
                    "W Michael",
                    "Kurt Mahoney",
                    " Keutzer"
                ],
                "title": "Q-bert: Hessian based ultra low precision quantization of bert",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_116",
            "content": "Yelong Shen, Po-Sen Huang, Jianfeng Gao, Weizhu Chen, Reasonet: Learning to stop reading in machine comprehension, 2017, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Yelong Shen",
                    "Po-Sen Huang",
                    "Jianfeng Gao",
                    "Weizhu Chen"
                ],
                "title": "Reasonet: Learning to stop reading in machine comprehension",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_117",
            "content": "Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou, Mobilebert: a compact task-agnostic bert for resource-limited devices, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": [
                    "Zhiqing Sun",
                    "Hongkun Yu",
                    "Xiaodan Song",
                    "Renjie Liu",
                    "Yiming Yang",
                    "Denny Zhou"
                ],
                "title": "Mobilebert: a compact task-agnostic bert for resource-limited devices",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_118",
            "content": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Superglue: a stickier benchmark for general-purpose language understanding systems, 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Alex Wang",
                    "Yada Pruksachatkun",
                    "Nikita Nangia",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel R Bowman"
                ],
                "title": "Superglue: a stickier benchmark for general-purpose language understanding systems",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 33rd International Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_119",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019, 7th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Alex Wang",
                    "Amanpreet Singh",
                    "Julian Michael",
                    "Felix Hill",
                    "Omer Levy",
                    "Samuel R Bowman"
                ],
                "title": "Glue: A multi-task benchmark and analysis platform for natural language understanding",
                "pub_date": "2019",
                "pub_title": "7th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_120",
            "content": "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin, Deebert: Dynamic early exiting for accelerating bert inference, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Ji Xin",
                    "Raphael Tang",
                    "Jaejun Lee",
                    "Yaoliang Yu",
                    "Jimmy Lin"
                ],
                "title": "Deebert: Dynamic early exiting for accelerating bert inference",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_121",
            "content": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": [
                    "Zhilin Yang",
                    "Zihang Dai",
                    "Yiming Yang",
                    "Jaime Carbonell",
                    "R Russ",
                    "Quoc V Salakhutdinov",
                    " Le"
                ],
                "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
                "pub_date": "2019",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_122",
            "content": "Keyi Yu, Yang Liu, Alexander Schwing, Jian Peng, Fast and accurate text classification: Skimming, rereading and early stopping, 2018, 6th International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b36",
                "authors": [
                    "Keyi Yu",
                    "Yang Liu",
                    "Alexander Schwing",
                    "Jian Peng"
                ],
                "title": "Fast and accurate text classification: Skimming, rereading and early stopping",
                "pub_date": "2018",
                "pub_title": "6th International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "29-ARR_v1_123",
            "content": "Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu, Ternarybert: Distillation-aware ultra-low bit bert, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b37",
                "authors": [
                    "Wei Zhang",
                    "Lu Hou",
                    "Yichun Yin",
                    "Lifeng Shang",
                    "Xiao Chen",
                    "Xin Jiang",
                    "Qun Liu"
                ],
                "title": "Ternarybert: Distillation-aware ultra-low bit bert",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "29-ARR_v1_0@0",
            "content": "E-LANG: Energy-Based Joint Inferencing of Super and Swift Language Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_0",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@0",
            "content": "Building huge and highly capable language models has been a trend in the past years.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@1",
            "content": "Despite their great performance, they incur high computational cost.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 85,
            "end": 152,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@2",
            "content": "A common solution is to apply model compression or choose light-weight architectures, which often need a separate fixed-size model for each desirable computational budget, and may lose performance in case of heavy compression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 154,
            "end": 379,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@3",
            "content": "This paper proposes an effective dynamic inference approach, called E-LANG, which distributes the inference between large accurate Supermodels and light-weight Swift models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 381,
            "end": 553,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@4",
            "content": "To this end, a decision making module routes the inputs to Super or Swift models based on the energy characteristics of the representations in the latent space.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 555,
            "end": 714,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@5",
            "content": "This method is easily adoptable and architecture agnostic.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 716,
            "end": 773,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@6",
            "content": "As such, it can be applied to black-box pre-trained models without a need for architectural manipulations, reassembling of modules, or re-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 775,
            "end": 921,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@7",
            "content": "Unlike existing methods that are only applicable to encoder-only backbones and classification tasks, our method also works for encoderdecoder structures and sequence-to-sequence tasks such as translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 923,
            "end": 1126,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@8",
            "content": "The E-LANG performance is verified through a set of experiments with T5 and BERT backbones on GLUE, Su-perGLUE, and WMT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 1128,
            "end": 1247,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@9",
            "content": "In particular, we outperform T5-11B with an average computations speed-up of 3.3\u00d7 on GLUE and 2.9\u00d7 on SuperGLUE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 1249,
            "end": 1360,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@10",
            "content": "We also achieve BERT-based SOTA on GLUE with 3.2\u00d7 less computations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 1362,
            "end": 1429,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_2@11",
            "content": "Code is available in supplementary materials.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_2",
            "start": 1431,
            "end": 1475,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_4@0",
            "content": "With the introduction of influential language models such as BERT (Devlin et al., 2019), a trend in natural language processing (NLP) research has been to develop high capacity models and push their performance to new levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_4",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_4@1",
            "content": "Consequently, state-of-the-art (SOTA) results were achieved on various benchmarks using these models; GPT-3 (Brown et al., 2020), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), T5 (Raffel et al., 2020), ELECTRA (Clark et al., 2020), and DeBERTa (He et al., 2021) to name a few.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_4",
            "start": 226,
            "end": 510,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_4@2",
            "content": "A potential down-side, however, is that the number of parameters or floating point operations (FLOPs) for these models can get extremely large.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_4",
            "start": 512,
            "end": 654,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_4@3",
            "content": "For example, Gshard (Lepikhin et al., 2021) comes with 600B parameters with an enormous amount of computation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_4",
            "start": 656,
            "end": 765,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_4@4",
            "content": "This in turn results in a higher inference latency, which is not desirable for latency-sensitive applications.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_4",
            "start": 767,
            "end": 876,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_5@0",
            "content": "A common solution to speed-up the large language models is to apply model compression .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_5",
            "start": 0,
            "end": 86,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_5@1",
            "content": "Although generally successful, compression does come with a trade-off on accuracy, and may lose performance if compression is heavy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_5",
            "start": 88,
            "end": 219,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_5@2",
            "content": "In addition, these methods usually compress a model to a fixed smaller size, where a separate model is required for each possible computational budget.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_5",
            "start": 221,
            "end": 371,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_5@3",
            "content": "An alternative approach explored in the literature is to leverage dynamic inferencing in a way that examples may be routed to different (potentially lower cost) paths throughout the network.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_5",
            "start": 373,
            "end": 562,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_5@4",
            "content": "For example, a temporal early-exit model (Shen et al., 2017;Yu et al., 2018) terminates the procedure of reading the input sequence when sufficient evidence has been found for accurate predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_5",
            "start": 564,
            "end": 760,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_5@5",
            "content": "Instance-wise early-exiting is another technique, which allows a sample to adaptively choose from multiple available exit nodes if some conditions are met.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_5",
            "start": 762,
            "end": 916,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_5@6",
            "content": "Consequently, earlier exists require less computation and lead to a lower latency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_5",
            "start": 918,
            "end": 999,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_5@7",
            "content": "Adjusting the size of the model at the inference time by choosing adaptive width and depth is also another approach employed for dynamic inference (Kim and Cho, 2021;.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_5",
            "start": 1001,
            "end": 1167,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_5@8",
            "content": "There is a variety of adaptive/dynamic inference approaches proposed, however, a general down-side for many of these methods is that often times they require a careful architecture design, manipulation of network modules, or even re-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_5",
            "start": 1169,
            "end": 1410,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_6@0",
            "content": "In this paper, we propose a simple but rather effective approach of dynamically distributing the inference between the original large model (called the Super model) and a light-weight (e.g., compressed) model referred to as the Swift model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_6",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_6@1",
            "content": "To this end, we design an energy-based decision making module that routes examples to the appropriate model based on the negative free energy of the latent space representations, such that the Swift model attains a high accuracy on the examples sent to it.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_6",
            "start": 241,
            "end": 496,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_6@2",
            "content": "The remaining samples are then forwarded to the Super model that is supposed to have a good performance on all examples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_6",
            "start": 498,
            "end": 617,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_6@3",
            "content": "Since the Swift model can make highly accurate predictions over the majority of the samples, E-LANG significantly reduces the overall computational cost, while maintains the high accuracy of the Super model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_6",
            "start": 619,
            "end": 825,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_6@4",
            "content": "Although simple, this strategy achieves SOTA results on multiple structures (e.g., T5 and BERT) and benchmarks (e.g., GLUE and SuperGLUE).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_6",
            "start": 827,
            "end": 964,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_6@5",
            "content": "Due to its desirable practical characteristics, this method is a strong candidate for the practical application of Super models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_6",
            "start": 966,
            "end": 1093,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_6@6",
            "content": "The main contributions of the paper are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_6",
            "start": 1095,
            "end": 1145,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_7@0",
            "content": "\u2022 Combining Super models with high accuracy and latency and Swift models with lower accuracy and latency, to achieve high accuracy and low latency. In other words, by employing our method, we can achieve the high levels of accuracy provided by Super models, but at a lower computational cost. Our method is easily adoptable, architecture agnostic, and orthogonal to many other existing methods. It can be applied to black-box pre-trained models without a need for architectural manipulations, careful reassembling of modules, or re-training. \u2022 An energy-based routing mechanism for directing examples to the Super or Swift. This provides a dynamic trade-off between the accuracy and computational cost that outperforms the previous works in both fixed-size and dynamic inference (with zero overhead for real-time adjustment of speed/accuracy). As such, E-LANG acts like a knob for adjusting the accuracy-latency trade-off in real-time during model serving. \u2022 To the best of our knowledge, our method is the first generic approach to apply dynamic inference on both encoder-only and encoderdecoder architectures (e.g., T5) and also can extend the usage beyond classification tasks, to sequence-to-sequence tasks such as translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_7",
            "start": 0,
            "end": 1230,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_8@0",
            "content": "Related Works",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_8",
            "start": 0,
            "end": 12,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_9@0",
            "content": "As mentioned, compression is a widely used strategy to speed-up the large language models Gupta and Agrawal, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_9",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_9@1",
            "content": "This involves incorporating techniques such as quantization of weights and activations (Bai et al., 2020;Shen et al., 2020;Kim et al., 2021;Jin et al., 2021), knowledge distillation (KD) (Hinton et al., 2015;Jiao et al., 2020;Sanh et al., 2019), pruning/sharing (Gordon et al., 2020;, multi-device distribution (Banitalebi-Dehkordi et al., 2021), or a combination of these techniques (Cheng et al., 2017;Polino et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_9",
            "start": 116,
            "end": 540,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_10@0",
            "content": "Among all the compression techniques, creating a fixed-size small version of large models along with distillation has been popular in the recent years.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_10",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_10@1",
            "content": "Sanh et al. (2019) introduced DistillBERT, which was a smaller version of BERT trained with distillation for general purposes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_10",
            "start": 152,
            "end": 277,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_10@2",
            "content": "Another compact variant of BERT was proposed by Mobile-BERT (Sun et al., 2020) in which inverted bottleneck structures and progressive knowledge transfer were used.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_10",
            "start": 279,
            "end": 442,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_10@3",
            "content": "TinyBERT (Jiao et al., 2020) also presented a novel two-stage transformer distillation for both pre-training and task-specific fine-tuning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_10",
            "start": 444,
            "end": 582,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_10@4",
            "content": "In (Iandola et al., 2020), the usage of grouped convolutions was studied to design SqueezeBERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_10",
            "start": 584,
            "end": 678,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_10@5",
            "content": "ELM (Jiao et al., 2021), a layer mapping search framework, was also proposed for improving downstream BERT distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_10",
            "start": 680,
            "end": 799,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_10@6",
            "content": "A recent method, Ghost-BERT , employed softmaxnormalized 1D convolutions as ghost modules to generate more features with cheap operations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_10",
            "start": 801,
            "end": 938,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_11@0",
            "content": "Although compression techniques in general are effective, they come with a trade-off on accuracy, and may lose performance in case of high ratio compression.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_11",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_11@1",
            "content": "In addition, an individual fixed-size model is required for each possible computational budget.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_11",
            "start": 158,
            "end": 252,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_11@2",
            "content": "As stated in the introduction, the alternative solution is dynamic inference, which can be achieved with either early-exit or length/depthadaptive models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_11",
            "start": 254,
            "end": 407,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_11@3",
            "content": "One of the first temporal earlyexit strategies was proposed by ReasoNet (Shen et al., 2017), which stops its reading procedure when sufficient evidence has been found for answering a question.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_11",
            "start": 409,
            "end": 600,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_11@4",
            "content": "Similarly, in (Yu et al., 2018), an early stopping method applicable to classification tasks was presented.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_11",
            "start": 602,
            "end": 708,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_11@5",
            "content": "DeeBERT also proposed an instance-wise multi-exit method via the entropy of the output probability distribution to speed-up BERT inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_11",
            "start": 710,
            "end": 848,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_12@0",
            "content": "As a length-adaptive method, Kim and Cho (2021) introduced a dynamic inference framework with one-shot training of transformers for both sequence-and token-level classification.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_12",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_12@1",
            "content": "Also, in , an architecture named Dyn-aBERT was proposed for adaptively adjusting the computations by choosing sub-networks of different widths and depths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_12",
            "start": 178,
            "end": 331,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_12@2",
            "content": "Both Length-Adaptive and DynaBERT utilized knowledge distillation and data augmentation to improve their performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_12",
            "start": 333,
            "end": 449,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_13@0",
            "content": "Although early-exit and adaptive methods have made significant progress and work well in practice, they often require architectural manipulation and re-training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_13",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_13@1",
            "content": "In addition, they are only applicable to encoder-only backbones and classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_13",
            "start": 162,
            "end": 250,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_13@2",
            "content": "In contrast, our method can work with out-of-the-box pre-trained models without a need for re-training and are also applicable for encoder-decoder structures and sequence-to-sequence tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_13",
            "start": 252,
            "end": 440,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_14@0",
            "content": "Proposed Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_14",
            "start": 0,
            "end": 14,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_15@0",
            "content": "We propose a new energy-based joint inference method called E-LANG, where a large/accurate language model (Super) is jointly employed with a small/fast one (Swift) to achieve efficient inference without sacrificing the accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_15",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_15@1",
            "content": "To this end, a routing mechanism empowered by energy-based models (EBM) is introduced to dynamically distribute the input samples between the Super and Swift models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_15",
            "start": 229,
            "end": 393,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_15@2",
            "content": "Similar to the out-of-distribution (OOD) detection problem, our goal is to identify the OOD samples that are hard to handle for the Swift and forward them to the Super model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_15",
            "start": 395,
            "end": 568,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_15@3",
            "content": "On the other hand, we have the in-distribution data for which the Swift can make highly reliable and accurate predictions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_15",
            "start": 570,
            "end": 691,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_15@4",
            "content": "In other words, the routing mechanism needs to detect whether or not the input data fits in the Swift's distribution (i.e., the one the Swift has been trained with).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_15",
            "start": 693,
            "end": 857,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_15@5",
            "content": "Inspired by the success of EBMs in dealing with OOD detection problems , the energy characteristics of data samples for an efficient and effective routing are investigated in our work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_15",
            "start": 859,
            "end": 1042,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_15@6",
            "content": "The overall framework of E-LANG is shown in Figure 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_15",
            "start": 1044,
            "end": 1096,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_16@0",
            "content": "Energy-Based Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_16",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_17@0",
            "content": "The goal of EBM is to build an energy function denoted by E(x) : R D \u2192 R that maps an input data x \u2208 R D to a non-probabilistic energy value y \u2208 R. To turn a collection of arbitrary energies for all possible outputs (denoted by Y ) into a normalized probability distribution, Gibbs distribution can be used as follows (LeCun et al., 2006):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_17",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_18@0",
            "content": "p(y|x) = e \u2212E(x,y) y \u2208Y e \u2212E(x,y ) ,(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_18",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_19@0",
            "content": "where the negative log of the denominator expresses the Helmholtz free energy (LeCun et al., 2006) defined as F (x) = \u2212log y \u2208Y e \u2212E(x,y ) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_19",
            "start": 0,
            "end": 139,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_19@1",
            "content": "In machine learning, there is a deep relationship between the EBMs and discriminative models, which can be seen by connecting the Gibbs distribution in Equation ( 1) and the categorical distribution derived for a discriminative model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_19",
            "start": 141,
            "end": 374,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_19@2",
            "content": "A discriminative classifier is defined as a function for mapping the input x to C real-valued logits (i.e., for C number of class labels): f (x) : R D \u2192 R C .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_19",
            "start": 376,
            "end": 533,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_19@3",
            "content": "In order to derive a categorical distribution over C possible outputs, the softmax function is utilized:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_19",
            "start": 535,
            "end": 638,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_20@0",
            "content": "p(y|x) = e fy(x) C i e f i (x) ,(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_20",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_21@0",
            "content": "where f y (x) denotes the logit (probability) of the yth class label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_21",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_21@1",
            "content": "Based on the inherent connection between the Gibbs and categorical distributions defined in ( 1) and ( 2), the energy function for a given input (x, y) can be defined as E(x, y) = \u2212f y (x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_21",
            "start": 70,
            "end": 258,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_22@0",
            "content": "The free energy function F (x; f ) can then be obtained by taking the negative log of the categorical distribution denominator as: x) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_22",
            "start": 0,
            "end": 134,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_23@0",
            "content": "F (x; f ) = \u2212log C i e f i (",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_23",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_24@0",
            "content": "(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_24",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_25@0",
            "content": "Energy-Based Joint Inference",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_25",
            "start": 0,
            "end": 27,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_26@0",
            "content": "Our goal is to detect the easy samples suitable for the Swift, which are indeed the ones with high likelihood in the density function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_26",
            "start": 0,
            "end": 133,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_26@1",
            "content": "The energy-based density function for Swift is then defined as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_26",
            "start": 135,
            "end": 197,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_27@0",
            "content": "p(x) = e \u2212F (x;f ) x e \u2212F (x;f ) ,(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_27",
            "start": 0,
            "end": 36,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_28@0",
            "content": "where the denominator is the normalized densities, which can be intractable to compute or estimate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_28",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_28@1",
            "content": "By taking the logarithm of both sides, we obtain:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_28",
            "start": 100,
            "end": 148,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_29@0",
            "content": "log p(x) = \u2212F (x; f ) \u2212 log( x e \u2212F (x;f ) ). (5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_29",
            "start": 0,
            "end": 48,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_30@0",
            "content": "The log( x e \u2212F (x;f ) ) term has no effect on the distribution of the overall energy values because it is constant for all x. As a result, \u2212F (x; f ), i.e., the negative free energy, has a linear alignment with the log likelihood function, which makes it a well-suited solution to the easy vs. hard detection problem in our framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_30",
            "start": 0,
            "end": 334,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_30@1",
            "content": "To this end, lower energy values indicate higher likelihood and represent easier (more fit) samples for the Swift model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_30",
            "start": 336,
            "end": 455,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_31@0",
            "content": "More precisely, for a threshold \u03b4 on the density function such that p(x) < \u03b4, then a threshold t on the negative free energy can be calculated according to (5) as \u2212F (x; f ) < t = log(\u03b4 x e \u2212F (x;f ) ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_31",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_32@0",
            "content": "In practice, for a given input, an energy function is applied to the outputs of the Swift model during inference time to calculate the energy score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_32",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_32@1",
            "content": "Then, if the negative energy value is smaller than a threshold, the input is identified as a bad sample for the Swift, and is sent to the Super model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_32",
            "start": 149,
            "end": 298,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_33@0",
            "content": "Given the energy threshold t, the Swift classifier f (x), and the Super classifier defined as g(x) : R D \u2192 R C , the joint inference function J(x; f, g, t) \u2208 [1, C] for a classification task with C classes can then be expressed by:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_33",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_34@0",
            "content": "J(x; f, g, t) = f (x) if \u2212 F (x; f ) \u2265 t g(x) otherwise. (6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_34",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_35@0",
            "content": "Encoder-Decoder Architectures",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_35",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_36@0",
            "content": "The proposed energy-based joint inference solution can be directly applied to the encoder-only models such as BERT that are designed for text classification tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_36",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_36@1",
            "content": "To this end, the energy scores corresponding to the BERT-based Swift model are obtained using Equation (3) and the joint inference is performed based on Equation 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_36",
            "start": 164,
            "end": 327,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_36@2",
            "content": "On the other hand, for the encoder-decoder (autoencoder) architectures such as T5, which are usually considered as generative models, some modifications are required.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_36",
            "start": 329,
            "end": 494,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_36@3",
            "content": "Encoder-decoder models are basically designed for sequence-to-sequence (e.g., text-to-text) problems such as translation or summarization.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_36",
            "start": 496,
            "end": 633,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_36@4",
            "content": "Although such models can also be employed for classification tasks, they still consider the task as a text generation (sequence-to-sequence) problem, where the target labels and the output predictions are treated as a sequence or a piece of text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_36",
            "start": 635,
            "end": 880,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_37@0",
            "content": "In Section 3.1, it was discussed that there is an inherent connection between the discriminative classifiers and the EBMs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_37",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_37@1",
            "content": "In order to benefit from this characteristic for encoder-decoder architectures, we consider adding an extra classification head (i.e., a single linear layer) to the Swift model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_37",
            "start": 123,
            "end": 299,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_37@2",
            "content": "As encoders are commonly considered as better feature extractors for training a classifier rather than the decoders, we place the extra head after the Swift encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_37",
            "start": 301,
            "end": 465,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_37@3",
            "content": "While freezing the pre-trained encoder model (denoted by f E ), the extra energy head (denoted by h) is trained as a regular classifier head with C class labels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_37",
            "start": 467,
            "end": 627,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_37@4",
            "content": "Note that the decoder is not required for training the head.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_37",
            "start": 629,
            "end": 688,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_37@5",
            "content": "The corresponding free energy function is then defined as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_37",
            "start": 690,
            "end": 755,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_38@0",
            "content": "F (x; f E , h) = \u2212log C i e h i f E (x) ,(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_38",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_39@0",
            "content": "where f E (x) denotes the outputs of the encoder's last hidden state.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_39",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_39@1",
            "content": "These features are then fed to the extra head h to obtain the logits for the ith class required for computing the energy scores.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_39",
            "start": 70,
            "end": 197,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_40@0",
            "content": "In this approach, as the decoder part of the Swift model is not required for calculating the energy scores, less computations are involved and the joint inference is performed more efficiently.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_40",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_41@0",
            "content": "For text-to-text (or sequence-to-sequence) problems such as translation, the output is a sequence of M word-pieces from a vocabulary/dictionary of size N .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_41",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_41@1",
            "content": "To still utilize the relationship of discriminative models and EBMs in designing and training the extra energy head, we can treat the text-to-text models as M multi-class classifiers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_41",
            "start": 156,
            "end": 338,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_41@2",
            "content": "In this case, the number of class labels, i.e., C in (7), is equal to N .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_41",
            "start": 340,
            "end": 412,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_41@3",
            "content": "The final energy score is then calculated as the average of M energy values as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_41",
            "start": 414,
            "end": 500,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_42@0",
            "content": "F (x; f E , h) = \u2212 1 M M m log C i e h m,i f E (x) , (8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_42",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_43@0",
            "content": "where h m,i (.) denotes the logits corresponding to the mth word in the sequence and ith class label.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_43",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_44@0",
            "content": "Denote the Swift's decoder by f D , the joint inference function, J(x; f, g, h, t), based on energy scores in either Equation ( 7) or ( 8) is expressed as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_44",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_45@0",
            "content": "J = f D f E (x) if \u2212 F (x; f E , h) \u2265 t g(x)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_45",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_46@0",
            "content": "otherwise.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_46",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_47@0",
            "content": "(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_47",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_48@0",
            "content": "Softmax and Entropy Mechanisms",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_48",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_49@0",
            "content": "In addition to energy, softmax and entropy scores can also be used for analyzing the Swift model's performance in the routing mechanism.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_49",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_49@1",
            "content": "In this sub-section, we study the mathematical connection of them with the energy score and their potential to solve our problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_49",
            "start": 137,
            "end": 265,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_50@0",
            "content": "Softmax-Based Mechanism",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_50",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_51@0",
            "content": "The softmax score for a classifier is expressed by: x) .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_51",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_51@1",
            "content": "(10) By taking the logarithm of both sides, we see the connection between the log of the softmax and the free energy score formulated in Equation (3):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_51",
            "start": 57,
            "end": 206,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_52@0",
            "content": "max y p(y|x) = max y e f y (x) C i e f i (x) = e f max (x) C i e f i (",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_52",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_53@0",
            "content": "log max y p(y|x) = log(e fmax(x) ) \u2212 log C i e f i (x) = f max (x) + F (x; f ),(11)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_53",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_54@0",
            "content": "where all logits are shifted by their maximum f max (x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_54",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_54@1",
            "content": "Plugging in the energy term to (5) yields:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_54",
            "start": 57,
            "end": 98,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_55@0",
            "content": "log max y p(y|x) = \u2212log(p(x)) + f max (x) \u2212log x e \u2212F (x;f ) .(12)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_55",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_56@0",
            "content": "It is observed that for the samples with high likelihood of being in the Swift's distribution, the free energy goes lower, but the max logit tends to go higher. Due to this shifting, unlike the energy score, the softmax score is not well-aligned with the probability density p(x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_56",
            "start": 0,
            "end": 279,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_56@1",
            "content": "As a result, the softmax score is less reliable for our routing module to analyze the performance of the Swift.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_56",
            "start": 281,
            "end": 391,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_57@0",
            "content": "Entropy-Based Mechanism",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_57",
            "start": 0,
            "end": 22,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_58@0",
            "content": "The entropy score is a measure of randomness in the processed information, and is calculated as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_58",
            "start": 0,
            "end": 95,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_59@0",
            "content": "H(x; f ) = \u2212 C i f i .log(f i ),(13)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_59",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_60@0",
            "content": "where f i (x) is the probability (logit) corresponding to the ith class label. Let U be the internal energy, i.e., the expectation value of the energy function (Oh et al., 2020), defined by:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_60",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_61@0",
            "content": "U (x; f ) = C i E(x, i)f i .(14)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_61",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_62@0",
            "content": "According to Oh et al. (2020), the entropy can be defined in terms of the internal and free energy functions as: H(x; f ) = U (x; f ) \u2212 F (x; f ), where all logits are shifted by the internal energy U .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_62",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_62@1",
            "content": "Substituting the free energy from (5) yields: (15) which shows, due to the shifting caused by internal energy, the entropy is not reliably aligned with the probability density p(x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_62",
            "start": 203,
            "end": 383,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_62@2",
            "content": "Thus, it is a less suitable routing mechanism unlike the energy score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_62",
            "start": 385,
            "end": 454,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_63@0",
            "content": "H(x; f ) = log(p(x)) + U (x; f ) + log x e \u2212F (x;f ) ,",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_63",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_64@0",
            "content": "Experimental Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_64",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_65@0",
            "content": "In this section, the performance of E-LANG on different architectures such as T5 and BERT; and benchmarks such as GLUE (Wang et al., 2019b), SuperGLUE (Wang et al., 2019a), and WMT (Bojar et al., 2016) is evaluated and compared with the Super models and previous works.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_65",
            "start": 0,
            "end": 268,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_66@0",
            "content": "T5-Based Joint Inference",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_66",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_67@0",
            "content": "In Table 1, the T5-based results on GLUE, Super-GLUE, and WMT benchmarks are reported.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_67",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_67@1",
            "content": "For all the tasks, we use T5-11B (with 87\u00d710 11 FLOPs) and T5-large (with 4.25\u00d710 11 FLOPs) as our Super and Swift models, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_67",
            "start": 87,
            "end": 222,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_67@2",
            "content": "The average GPUbased running time and accuracy of both models compared with E-LANG are also summarized in the table.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_67",
            "start": 224,
            "end": 339,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_67@3",
            "content": "Note that the T5 models used in this experiment have been separately fine-tuned on each of the downstream tasks given in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_67",
            "start": 341,
            "end": 469,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_67@4",
            "content": "The extra energy head for each of these tasks was also separately trained and used based on the task-specific number of classes, i.e., C in Equation ( 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_67",
            "start": 471,
            "end": 624,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_68@0",
            "content": "The total FLOPs for our method is measured as a weighted average of the Super and Swift FLOPs based on their usage frequency as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_68",
            "start": 0,
            "end": 127,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_69@0",
            "content": "F LOP s = 1 N sw + N su N sw .(F E sw +F h +F D sw ) + N su .(F E sw + F h + F su ) ,(16)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_69",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_70@0",
            "content": "where N su and N sw are respectively the number of samples processed by the Super (with F su FLOPs) and Swift (with F E sw , F D sw , and F h FLOPs for the encoder, decoder, and energy head).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_70",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_70@1",
            "content": "Note that F h is equal to \u22480.00001\u00d710 11 , which has a very insignificant overhead in our framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_70",
            "start": 192,
            "end": 291,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_71@0",
            "content": "As presented in Table 1, E-LANG can reach the Super model's accuracy on all GLUE tasks with an average 3.3X in FLOPs and 1.8X in running time speed-ups.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_71",
            "start": 0,
            "end": 151,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_71@1",
            "content": "For some tasks such as QNLI, MRPC, and COLA, we even outperform the Super model, which leads to a higher average accuracy of 89.7% than the Super model with 89.5% on GLUE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_71",
            "start": 153,
            "end": 323,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_71@2",
            "content": "For the SuperGLUE benchmark, with an average FLOPs and running time speed-up of 2.9X and 2.0X, our method achieves the same accuracy as the Super model on MRC and CB; and better accuracy on RTE and WIC.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_71",
            "start": 325,
            "end": 526,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_71@3",
            "content": "On BoolQ and COPA, although 99% and 97% of the Super's accuracy are respectively obtained, it is with 1.7X and 1.4X less FLOPs and latency, on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_71",
            "start": 528,
            "end": 678,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_71@4",
            "content": "In order to analyze the generality of E-LANG to other NLP problems rather than text classification (Section 3.2.1), we also apply our method to two text-to-text tasks including SuperGLUE's WSC and WMT's English-to-Romanian (EnRo) translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_71",
            "start": 680,
            "end": 921,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_71@5",
            "content": "As given in the table, our method achieves the Super model's accuracy on both WSC and EnRo with 4.2X and 1.4X less FLOPs, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_71",
            "start": 923,
            "end": 1057,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_72@0",
            "content": "Figure 2 illustrates the accuracy vs. FLOPs trade-off curves for some tasks in GLUE and Super-GLUE benchmarks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_72",
            "start": 0,
            "end": 109,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_72@1",
            "content": "The curves related to all tasks are given in the appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_72",
            "start": 111,
            "end": 168,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_72@2",
            "content": "The trade-off points on the curves are dynamically achieved at the inference time by selecting different thresholds, i.e., t in Equations ( 6) and ( 9).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_72",
            "start": 170,
            "end": 321,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_72@3",
            "content": "Larger values for t will result in routing more input data to the Super model, which consequently provides more accurate, but slower inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_72",
            "start": 323,
            "end": 465,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_72@4",
            "content": "As the Swift is able to make accurate predictions for the majority of input data, the dynamic inference with a small enough t can reach the Super model's accuracy but with a much lower computational cost and latency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_72",
            "start": 467,
            "end": 682,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_73@0",
            "content": "Ablation Studies",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_73",
            "start": 0,
            "end": 15,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_74@0",
            "content": "In Sections 3.3.1 and 3.3.2, the possibility of using softmax and entropy scores instead of energy score was theoretically analyzed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_74",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_74@1",
            "content": "To support that analysis and also experimentally evaluate the performance of different routing mechanisms, an ablation study on GLUE is performed, which is presented in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_74",
            "start": 133,
            "end": 309,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_74@2",
            "content": "In this study, we report the joint inference results based on softmax, entropy, and random scores (i.e., randomly distributing the samples between Super and Swift).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_74",
            "start": 311,
            "end": 474,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_74@3",
            "content": "Our experiments show that, compared to the random score, softmax and entropy can result in satisfactory performance on routing the samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_74",
            "start": 476,
            "end": 614,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_74@4",
            "content": "However, as also discussed in Sections 3.3.1 and 3.3.2, the energy score is still a better routing mechanism with about 14% less FLOPs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_74",
            "start": 616,
            "end": 750,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_75@0",
            "content": "The results with the usage of different Swift models including T5-small (with 0.33\u00d710 11 FLOPs) and T5-base (with 1.24\u00d710 11 FLOPs) are also given in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_75",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_75@1",
            "content": "Using these models as Swifts can lead to good performance on some tasks, but not all of them.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_75",
            "start": 159,
            "end": 251,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_75@2",
            "content": "For example, on SST2, the joint inference with T5-small and T5-base Swifts can respectively reach the Super's accuracy with 1.9X and 2.X less computations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_75",
            "start": 253,
            "end": 407,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_75@3",
            "content": "In general, although these models are smaller and require less FLOPs, our results in Table 2 indicate that they perform worse than T5-large in our joint inference structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_75",
            "start": 409,
            "end": 581,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_76@0",
            "content": "In Figure 2, the trade-off curves for different Swift models are shown for GLUE and SuperGLUE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_76",
            "start": 0,
            "end": 93,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_76@1",
            "content": "Moreover, to show the effectiveness of the extra energy head for the Swift encoder, the E-LANG results based on last linear layer of the Swift decoder is also given and compared in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_76",
            "start": 95,
            "end": 283,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_76@2",
            "content": "As reported, the E-LANG empowered by the energy head on the Swift encoder outperforms the case with the decoder's head in both FLOPs (36.8% less) and accuracy (0.7% better).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_76",
            "start": 285,
            "end": 457,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_76@3",
            "content": "As explained in Section 3.2.1, this shows the deep connection between the encoder's features, discriminative models, and the proposed routing mechanism via the energy head.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_76",
            "start": 459,
            "end": 630,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_77@0",
            "content": "We observed that E-LANG can achieve a high performance even when applied to individually pretrained Super and Swift models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_77",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_77@1",
            "content": "However, more improvement can still be obtained by performing KD from the Super model to the Swift model, especially at the fine-tuning process for downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_77",
            "start": 124,
            "end": 288,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_77@2",
            "content": "To study this, we apply the KD technique in (Sanh et al., 2019) to the Super and Swift models for some GLUE tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_77",
            "start": 290,
            "end": 403,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_77@3",
            "content": "As summarized in Table 3, the Super model's accuracy for QNLI, SST2, and COLA is respectively attained by the distillationbased E-LANG with 29.2%, 48.5%, and 14.3% less FLOPs than E-LANG (without distillation).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_77",
            "start": 405,
            "end": 614,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_77@4",
            "content": "The results in this experiment show the effectiveness of E-LANG along with other compression techniques such as distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_77",
            "start": 616,
            "end": 740,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_77@5",
            "content": "The trade-off curves for this experiment will be provided in the appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_77",
            "start": 742,
            "end": 815,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_78@0",
            "content": "BERT-Based Joint Inference",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_78",
            "start": 0,
            "end": 25,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_79@0",
            "content": "In this section, the proposed energy-based joint inference method is applied to the BERT architecture (Devlin et al., 2019) and compared with BERT-based SOTA in both fixed-size and dynamic inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_79",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_79@1",
            "content": "The majority of the previous methods employ knowledge distillation and data augmentation techniques for training their student models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_79",
            "start": 200,
            "end": 333,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_79@2",
            "content": "For a fair comparison, we follow the same practice and use the transformer distillation and augmentation strategies in TinyBERT (Jiao et al., 2020) to train and prepare our Swift model (i.e., BERT T iny with 1.2 \u00d7 10 9 FLOPs).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_79",
            "start": 335,
            "end": 560,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_79@3",
            "content": "Moreover, similar to the other works, we use BERT Base (with 21.8 \u00d7 10 9 FLOPs) as our Super (i.e., teacher) model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_79",
            "start": 562,
            "end": 676,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_80@0",
            "content": "In Table 4, the comparison results with the baseline BERT Base and SOTA on GLUE benchmark are presented in terms of accuracy, FLOPs, and latency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_80",
            "start": 0,
            "end": 144,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_80@1",
            "content": "Compared to the Super model, E-LANG delivers better accuracy on SST2 and RTE with 3.5X and 2.0X FLOPs speed-up; and the same accuracy on QNLI, MRPC, and QQP with 2.4X, 2.7X, and 7.0X FLOPs speed-up, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_80",
            "start": 146,
            "end": 357,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_80@2",
            "content": "On MNLI and COLA, 99.8% and 97.3% of the Super model's accuracy are achieved, but with an average FLOPs speed-up of 2.3X.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_80",
            "start": 359,
            "end": 479,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_80@3",
            "content": "On average, E-LANG outperforms the Super model with 0.1% higher accuracy, 3.2X less FLOPs, and 1.6X less latency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_80",
            "start": 481,
            "end": 593,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_81@0",
            "content": "Compared with SOTA, our method achieves the best performance on all GLUE tasks, except MRPC for which SqueezeBERT outperforms all due to having a more accurate teacher (Iandola et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_81",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_81@1",
            "content": "There are some works such as ELECTRA (Clark et al., 2020) and MobileBERT (Sun et al., 2020) that require less FLOPs than our method, but they only reach 95% of the baseline's accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_81",
            "start": 192,
            "end": 375,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_81@2",
            "content": "Compared to other methods, GhostBERT and DynaBERT give the closest performance to the baseline and even the same as ours on some tasks such as QNLI.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_81",
            "start": 377,
            "end": 524,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_81@3",
            "content": "However, on average, they still need about 30% more FLOPs on GLUE compared to E-LANG.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_81",
            "start": 526,
            "end": 610,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_82@0",
            "content": "The E-LANG accuracy vs. FLOPs trade-off curves compared to SOTA on some of GLUE tasks are shown in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_82",
            "start": 0,
            "end": 107,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_82@1",
            "content": "The trade-off curves for all the tasks are reported in the appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_82",
            "start": 109,
            "end": 176,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_82@2",
            "content": "Among the SOTA methods presented in Table 4 and Fig To investigate the orthogonality of E-LANG with others, we integrate our energy-based joint inference strategy with DynaBERT that is SOTA in BERT-based adaptive inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_82",
            "start": 178,
            "end": 400,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_82@3",
            "content": "In other words, we analyze whether E-LANG can be added on top of other efficient methods to benefit both from their designs and our approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_82",
            "start": 402,
            "end": 542,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_82@4",
            "content": "In this experiment, the DynaBERT configurations with the highest accuracy (i.e., width=0.75 & depth=1.0) and the lowest FLOPs (i.e., width=0.5 & depth=0.25) are respectively employed as the Super and Swift models in our framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_82",
            "start": 544,
            "end": 773,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_82@5",
            "content": "The corresponding joint inference results on MNLI, SST2, and QQP are reported in Table 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_82",
            "start": 775,
            "end": 863,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_82@6",
            "content": "As observed, we accomplish the Dyn-aBERT Super's accuracy for MNLI and SST2 with 1.7X and 3.1X less FLOPs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_82",
            "start": 865,
            "end": 970,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_82@7",
            "content": "For QQP, our method combined with DynaBERT even outperforms Dyn-aBERT by 0.1% with 2.6X FLOPs speed-up.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_82",
            "start": 972,
            "end": 1074,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_83@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_83",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_84@0",
            "content": "In this paper, we introduced E-LANG, an energybased joint inference approach, which integrates Super and Swift language models for achieving efficient inference without sacrificing the accuracy.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_84",
            "start": 0,
            "end": 193,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_85@0",
            "content": "Our method can work with both encoder-only (e.g., BERT) and encoder-decoder (e.g., T5) architectures, and is also applicable for text classification and sequence-to-sequence problems.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_85",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_85@1",
            "content": "The proposed joint inference strategy was theoretically and experimentally analyzed with an extensive set of experiments and ablation studies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_85",
            "start": 184,
            "end": 325,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_85@2",
            "content": "Our results showed that E-LANG outperforms SOTA in both fixed-size and dynamic inference over different benchmarks such as GLUE and SuperGLUE.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_85",
            "start": 327,
            "end": 468,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_85@3",
            "content": "One future direction to this work is to apply E-LANG to multiple Super and Swift models with different sizes.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_85",
            "start": 470,
            "end": 578,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_86@0",
            "content": "UNKNOWN, None, 2020, Binarybert: Pushing the limit of bert quantization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_86",
            "start": 0,
            "end": 73,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_87@0",
            "content": "Amin Banitalebi-Dehkordi, Naveen Vedula, Jian Pei, Fei Xia, Lanjun Wang, Yong Zhang, Auto-split: A general framework of collaborative edge-cloud ai, 2021, Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_87",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_88@0",
            "content": "Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Matthias Huck, Antonio Yepes, Philipp Koehn, Varvara Logacheva, Christof Monz, Findings of the 2016 conference on machine translation, 2016, Proceedings of the First Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_88",
            "start": 0,
            "end": 283,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_89@0",
            "content": "UNKNOWN, None, , , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_89",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_90@0",
            "content": "Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang Wang, Michael Carbin, The lottery ticket hypothesis for pretrained bert networks, 2020, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_90",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_91@0",
            "content": "UNKNOWN, None, 2017, A survey of model compression and acceleration for deep neural networks, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_91",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_92@0",
            "content": "UNKNOWN, None, 2020, Electra: Pre-training text encoders as discriminators rather than generators. International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_92",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_93@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, 2019, NAACL-HLT (1), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_93",
            "start": 0,
            "end": 165,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_94@0",
            "content": "UNKNOWN, None, 2020, Compressing bert: Studying the effects of weight pruning on transfer learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_94",
            "start": 0,
            "end": 100,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_95@0",
            "content": "UNKNOWN, None, 2020, Compression of deep learning models for text: A survey, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_95",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_96@0",
            "content": "Manish Gupta, Vasudeva Varma, Sonam Damani, Kedhar Nath Narahari, Compression of deep learning models for nlp, 2020, Proceedings of the 29th ACM International Conference on Information & Knowledge Management, CIKM, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_96",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_97@0",
            "content": "Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, Deberta: Decoding-enhanced bert with disentangled attention, 2021, International Conference on Learning Representations, ICLR, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_97",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_98@0",
            "content": "G Hinton, O Vinyals, J Dean, Distilling the knowledge in a neural network, 2015, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_98",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_99@0",
            "content": "Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, Qun Liu, Dynabert: Dynamic bert with adaptive width and depth, 2020, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_99",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_100@0",
            "content": "UNKNOWN, None, 2021, Ghostbert: Generate more features with cheap operations for bert, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_100",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_101@0",
            "content": "Forrest Iandola, Albert Shaw, Ravi Krishna, Kurt Keutzer, SqueezeBERT: What can computer vision teach NLP about efficient neural networks?, 2020, Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_101",
            "start": 0,
            "end": 234,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_102@0",
            "content": "Xiaoqi Jiao, Huating Chang, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Improving task-agnostic bert distillation with layer mapping search, 2021, Neurocomputing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_102",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_103@0",
            "content": "Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, Qun Liu, Tinybert: Distilling bert for natural language understanding, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_103",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_104@0",
            "content": "UNKNOWN, None, 2021, Kdlsq-bert: A quantized bert combining knowledge distillation with learned step size quantization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_104",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_105@0",
            "content": "UNKNOWN, None, 2021, Lengthadaptive transformer: Train once with length drop, use anytime with search, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_105",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_106@0",
            "content": "UNKNOWN, None, , 2021. I-bert: Integer-only bert quantization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_106",
            "start": 0,
            "end": 63,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_107@0",
            "content": "Yann Lecun, Sumit Chopra, Raia Hadsell, M Ranzato, F Huang, A tutorial on energy-based learning, 2006, Predicting structured data, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_107",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_108@0",
            "content": "Youngwan Lee, Joong-Won Hwang, Sangrok Lee, Yuseok Bae, Jongyoul Park, An energy and gpu-computation efficient backbone network for real-time object detection, 2019, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_108",
            "start": 0,
            "end": 259,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_109@0",
            "content": "UNKNOWN, None, 2021, Gshard: Scaling giant models with conditional computation and automatic sharding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_109",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_110@0",
            "content": "UNKNOWN, None, 2019, Roberta: A robustly optimized bert pretraining approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_110",
            "start": 0,
            "end": 78,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_111@0",
            "content": "Sangchul Oh, Abdelkader Baggag, Hyunchul Nha, Entropy, free energy, and work of restricted boltzmann machines, 2020, Entropy, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_111",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_112@0",
            "content": "A Polino, R Pascanu, Dan Alistarh, Model compression via distillation and quantization, 2018, ArXiv, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_112",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_113@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_113",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_114@0",
            "content": "Victor Sanh, Lysandre Debut, Julien Chaumond, Thomas Wolf, Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter, 2019, 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing -NeurIPS, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_114",
            "start": 0,
            "end": 228,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_115@0",
            "content": "Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami, W Michael, Kurt Mahoney,  Keutzer, Q-bert: Hessian based ultra low precision quantization of bert, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_115",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_116@0",
            "content": "Yelong Shen, Po-Sen Huang, Jianfeng Gao, Weizhu Chen, Reasonet: Learning to stop reading in machine comprehension, 2017, Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_116",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_117@0",
            "content": "Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, Denny Zhou, Mobilebert: a compact task-agnostic bert for resource-limited devices, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_117",
            "start": 0,
            "end": 242,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_118@0",
            "content": "Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Superglue: a stickier benchmark for general-purpose language understanding systems, 2019, Proceedings of the 33rd International Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_118",
            "start": 0,
            "end": 300,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_119@0",
            "content": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, Glue: A multi-task benchmark and analysis platform for natural language understanding, 2019, 7th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_119",
            "start": 0,
            "end": 235,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_120@0",
            "content": "Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, Jimmy Lin, Deebert: Dynamic early exiting for accelerating bert inference, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_120",
            "start": 0,
            "end": 217,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_121@0",
            "content": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, R Russ, Quoc V Salakhutdinov,  Le, Xlnet: Generalized autoregressive pretraining for language understanding, 2019, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_121",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_122@0",
            "content": "Keyi Yu, Yang Liu, Alexander Schwing, Jian Peng, Fast and accurate text classification: Skimming, rereading and early stopping, 2018, 6th International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_122",
            "start": 0,
            "end": 192,
            "label": {}
        },
        {
            "ix": "29-ARR_v1_123@0",
            "content": "Wei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao Chen, Xin Jiang, Qun Liu, Ternarybert: Distillation-aware ultra-low bit bert, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "29-ARR_v1_123",
            "start": 0,
            "end": 230,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "29-ARR_v1_0",
            "tgt_ix": "29-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_0",
            "tgt_ix": "29-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_1",
            "tgt_ix": "29-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_1",
            "tgt_ix": "29-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_0",
            "tgt_ix": "29-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_4",
            "tgt_ix": "29-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_5",
            "tgt_ix": "29-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_6",
            "tgt_ix": "29-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_3",
            "tgt_ix": "29-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_3",
            "tgt_ix": "29-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_3",
            "tgt_ix": "29-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_3",
            "tgt_ix": "29-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_3",
            "tgt_ix": "29-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_0",
            "tgt_ix": "29-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_9",
            "tgt_ix": "29-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_10",
            "tgt_ix": "29-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_11",
            "tgt_ix": "29-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_12",
            "tgt_ix": "29-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_8",
            "tgt_ix": "29-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_8",
            "tgt_ix": "29-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_8",
            "tgt_ix": "29-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_8",
            "tgt_ix": "29-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_8",
            "tgt_ix": "29-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_8",
            "tgt_ix": "29-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_0",
            "tgt_ix": "29-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_13",
            "tgt_ix": "29-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_14",
            "tgt_ix": "29-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_14",
            "tgt_ix": "29-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_14",
            "tgt_ix": "29-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_15",
            "tgt_ix": "29-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_17",
            "tgt_ix": "29-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_18",
            "tgt_ix": "29-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_19",
            "tgt_ix": "29-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_20",
            "tgt_ix": "29-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_21",
            "tgt_ix": "29-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_22",
            "tgt_ix": "29-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_23",
            "tgt_ix": "29-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_16",
            "tgt_ix": "29-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_16",
            "tgt_ix": "29-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_16",
            "tgt_ix": "29-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_16",
            "tgt_ix": "29-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_16",
            "tgt_ix": "29-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_16",
            "tgt_ix": "29-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_16",
            "tgt_ix": "29-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_16",
            "tgt_ix": "29-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_16",
            "tgt_ix": "29-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_14",
            "tgt_ix": "29-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_24",
            "tgt_ix": "29-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_26",
            "tgt_ix": "29-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_27",
            "tgt_ix": "29-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_28",
            "tgt_ix": "29-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_29",
            "tgt_ix": "29-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_30",
            "tgt_ix": "29-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_31",
            "tgt_ix": "29-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_32",
            "tgt_ix": "29-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_33",
            "tgt_ix": "29-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_14",
            "tgt_ix": "29-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_34",
            "tgt_ix": "29-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_36",
            "tgt_ix": "29-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_37",
            "tgt_ix": "29-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_38",
            "tgt_ix": "29-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_39",
            "tgt_ix": "29-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_40",
            "tgt_ix": "29-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_41",
            "tgt_ix": "29-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_42",
            "tgt_ix": "29-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_43",
            "tgt_ix": "29-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_44",
            "tgt_ix": "29-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_45",
            "tgt_ix": "29-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_46",
            "tgt_ix": "29-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_14",
            "tgt_ix": "29-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_47",
            "tgt_ix": "29-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_48",
            "tgt_ix": "29-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_48",
            "tgt_ix": "29-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_14",
            "tgt_ix": "29-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_49",
            "tgt_ix": "29-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_51",
            "tgt_ix": "29-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_52",
            "tgt_ix": "29-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_53",
            "tgt_ix": "29-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_54",
            "tgt_ix": "29-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_55",
            "tgt_ix": "29-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_50",
            "tgt_ix": "29-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_50",
            "tgt_ix": "29-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_50",
            "tgt_ix": "29-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_50",
            "tgt_ix": "29-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_50",
            "tgt_ix": "29-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_50",
            "tgt_ix": "29-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_50",
            "tgt_ix": "29-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_14",
            "tgt_ix": "29-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_56",
            "tgt_ix": "29-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_58",
            "tgt_ix": "29-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_59",
            "tgt_ix": "29-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_60",
            "tgt_ix": "29-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_61",
            "tgt_ix": "29-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_62",
            "tgt_ix": "29-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_57",
            "tgt_ix": "29-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_57",
            "tgt_ix": "29-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_57",
            "tgt_ix": "29-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_57",
            "tgt_ix": "29-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_57",
            "tgt_ix": "29-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_57",
            "tgt_ix": "29-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_57",
            "tgt_ix": "29-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_0",
            "tgt_ix": "29-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_63",
            "tgt_ix": "29-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_64",
            "tgt_ix": "29-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_64",
            "tgt_ix": "29-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_64",
            "tgt_ix": "29-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_65",
            "tgt_ix": "29-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_67",
            "tgt_ix": "29-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_68",
            "tgt_ix": "29-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_69",
            "tgt_ix": "29-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_70",
            "tgt_ix": "29-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_71",
            "tgt_ix": "29-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_66",
            "tgt_ix": "29-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_66",
            "tgt_ix": "29-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_66",
            "tgt_ix": "29-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_66",
            "tgt_ix": "29-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_66",
            "tgt_ix": "29-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_66",
            "tgt_ix": "29-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_66",
            "tgt_ix": "29-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_64",
            "tgt_ix": "29-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_72",
            "tgt_ix": "29-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_74",
            "tgt_ix": "29-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_75",
            "tgt_ix": "29-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_76",
            "tgt_ix": "29-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_73",
            "tgt_ix": "29-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_73",
            "tgt_ix": "29-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_73",
            "tgt_ix": "29-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_73",
            "tgt_ix": "29-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_73",
            "tgt_ix": "29-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_64",
            "tgt_ix": "29-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_77",
            "tgt_ix": "29-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_79",
            "tgt_ix": "29-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_80",
            "tgt_ix": "29-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_81",
            "tgt_ix": "29-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_78",
            "tgt_ix": "29-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_78",
            "tgt_ix": "29-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_78",
            "tgt_ix": "29-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_78",
            "tgt_ix": "29-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_78",
            "tgt_ix": "29-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_0",
            "tgt_ix": "29-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_82",
            "tgt_ix": "29-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_84",
            "tgt_ix": "29-ARR_v1_85",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_83",
            "tgt_ix": "29-ARR_v1_84",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_83",
            "tgt_ix": "29-ARR_v1_85",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_83",
            "tgt_ix": "29-ARR_v1_84",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "29-ARR_v1_0",
            "tgt_ix": "29-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_1",
            "tgt_ix": "29-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_2",
            "tgt_ix": "29-ARR_v1_2@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_3",
            "tgt_ix": "29-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_4",
            "tgt_ix": "29-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_4",
            "tgt_ix": "29-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_4",
            "tgt_ix": "29-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_4",
            "tgt_ix": "29-ARR_v1_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_4",
            "tgt_ix": "29-ARR_v1_4@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_5",
            "tgt_ix": "29-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_5",
            "tgt_ix": "29-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_5",
            "tgt_ix": "29-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_5",
            "tgt_ix": "29-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_5",
            "tgt_ix": "29-ARR_v1_5@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_5",
            "tgt_ix": "29-ARR_v1_5@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_5",
            "tgt_ix": "29-ARR_v1_5@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_5",
            "tgt_ix": "29-ARR_v1_5@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_5",
            "tgt_ix": "29-ARR_v1_5@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_6",
            "tgt_ix": "29-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_6",
            "tgt_ix": "29-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_6",
            "tgt_ix": "29-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_6",
            "tgt_ix": "29-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_6",
            "tgt_ix": "29-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_6",
            "tgt_ix": "29-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_6",
            "tgt_ix": "29-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_7",
            "tgt_ix": "29-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_8",
            "tgt_ix": "29-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_9",
            "tgt_ix": "29-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_9",
            "tgt_ix": "29-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_10",
            "tgt_ix": "29-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_10",
            "tgt_ix": "29-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_10",
            "tgt_ix": "29-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_10",
            "tgt_ix": "29-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_10",
            "tgt_ix": "29-ARR_v1_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_10",
            "tgt_ix": "29-ARR_v1_10@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_10",
            "tgt_ix": "29-ARR_v1_10@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_11",
            "tgt_ix": "29-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_11",
            "tgt_ix": "29-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_11",
            "tgt_ix": "29-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_11",
            "tgt_ix": "29-ARR_v1_11@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_11",
            "tgt_ix": "29-ARR_v1_11@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_11",
            "tgt_ix": "29-ARR_v1_11@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_12",
            "tgt_ix": "29-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_12",
            "tgt_ix": "29-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_12",
            "tgt_ix": "29-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_13",
            "tgt_ix": "29-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_13",
            "tgt_ix": "29-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_13",
            "tgt_ix": "29-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_14",
            "tgt_ix": "29-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_15",
            "tgt_ix": "29-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_15",
            "tgt_ix": "29-ARR_v1_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_15",
            "tgt_ix": "29-ARR_v1_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_15",
            "tgt_ix": "29-ARR_v1_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_15",
            "tgt_ix": "29-ARR_v1_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_15",
            "tgt_ix": "29-ARR_v1_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_15",
            "tgt_ix": "29-ARR_v1_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_16",
            "tgt_ix": "29-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_17",
            "tgt_ix": "29-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_18",
            "tgt_ix": "29-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_19",
            "tgt_ix": "29-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_19",
            "tgt_ix": "29-ARR_v1_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_19",
            "tgt_ix": "29-ARR_v1_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_19",
            "tgt_ix": "29-ARR_v1_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_20",
            "tgt_ix": "29-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_21",
            "tgt_ix": "29-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_21",
            "tgt_ix": "29-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_22",
            "tgt_ix": "29-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_23",
            "tgt_ix": "29-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_24",
            "tgt_ix": "29-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_25",
            "tgt_ix": "29-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_26",
            "tgt_ix": "29-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_26",
            "tgt_ix": "29-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_27",
            "tgt_ix": "29-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_28",
            "tgt_ix": "29-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_28",
            "tgt_ix": "29-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_29",
            "tgt_ix": "29-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_30",
            "tgt_ix": "29-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_30",
            "tgt_ix": "29-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_31",
            "tgt_ix": "29-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_32",
            "tgt_ix": "29-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_32",
            "tgt_ix": "29-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_33",
            "tgt_ix": "29-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_34",
            "tgt_ix": "29-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_35",
            "tgt_ix": "29-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_36",
            "tgt_ix": "29-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_36",
            "tgt_ix": "29-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_36",
            "tgt_ix": "29-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_36",
            "tgt_ix": "29-ARR_v1_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_36",
            "tgt_ix": "29-ARR_v1_36@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_37",
            "tgt_ix": "29-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_37",
            "tgt_ix": "29-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_37",
            "tgt_ix": "29-ARR_v1_37@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_37",
            "tgt_ix": "29-ARR_v1_37@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_37",
            "tgt_ix": "29-ARR_v1_37@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_37",
            "tgt_ix": "29-ARR_v1_37@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_38",
            "tgt_ix": "29-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_39",
            "tgt_ix": "29-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_39",
            "tgt_ix": "29-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_40",
            "tgt_ix": "29-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_41",
            "tgt_ix": "29-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_41",
            "tgt_ix": "29-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_41",
            "tgt_ix": "29-ARR_v1_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_41",
            "tgt_ix": "29-ARR_v1_41@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_42",
            "tgt_ix": "29-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_43",
            "tgt_ix": "29-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_44",
            "tgt_ix": "29-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_45",
            "tgt_ix": "29-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_46",
            "tgt_ix": "29-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_47",
            "tgt_ix": "29-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_48",
            "tgt_ix": "29-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_49",
            "tgt_ix": "29-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_49",
            "tgt_ix": "29-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_50",
            "tgt_ix": "29-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_51",
            "tgt_ix": "29-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_51",
            "tgt_ix": "29-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_52",
            "tgt_ix": "29-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_53",
            "tgt_ix": "29-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_54",
            "tgt_ix": "29-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_54",
            "tgt_ix": "29-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_55",
            "tgt_ix": "29-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_56",
            "tgt_ix": "29-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_56",
            "tgt_ix": "29-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_57",
            "tgt_ix": "29-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_58",
            "tgt_ix": "29-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_59",
            "tgt_ix": "29-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_60",
            "tgt_ix": "29-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_61",
            "tgt_ix": "29-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_62",
            "tgt_ix": "29-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_62",
            "tgt_ix": "29-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_62",
            "tgt_ix": "29-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_63",
            "tgt_ix": "29-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_64",
            "tgt_ix": "29-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_65",
            "tgt_ix": "29-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_66",
            "tgt_ix": "29-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_67",
            "tgt_ix": "29-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_67",
            "tgt_ix": "29-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_67",
            "tgt_ix": "29-ARR_v1_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_67",
            "tgt_ix": "29-ARR_v1_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_67",
            "tgt_ix": "29-ARR_v1_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_68",
            "tgt_ix": "29-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_69",
            "tgt_ix": "29-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_70",
            "tgt_ix": "29-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_70",
            "tgt_ix": "29-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_71",
            "tgt_ix": "29-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_71",
            "tgt_ix": "29-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_71",
            "tgt_ix": "29-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_71",
            "tgt_ix": "29-ARR_v1_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_71",
            "tgt_ix": "29-ARR_v1_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_71",
            "tgt_ix": "29-ARR_v1_71@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_72",
            "tgt_ix": "29-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_72",
            "tgt_ix": "29-ARR_v1_72@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_72",
            "tgt_ix": "29-ARR_v1_72@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_72",
            "tgt_ix": "29-ARR_v1_72@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_72",
            "tgt_ix": "29-ARR_v1_72@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_73",
            "tgt_ix": "29-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_74",
            "tgt_ix": "29-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_74",
            "tgt_ix": "29-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_74",
            "tgt_ix": "29-ARR_v1_74@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_74",
            "tgt_ix": "29-ARR_v1_74@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_74",
            "tgt_ix": "29-ARR_v1_74@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_75",
            "tgt_ix": "29-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_75",
            "tgt_ix": "29-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_75",
            "tgt_ix": "29-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_75",
            "tgt_ix": "29-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_76",
            "tgt_ix": "29-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_76",
            "tgt_ix": "29-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_76",
            "tgt_ix": "29-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_76",
            "tgt_ix": "29-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_77",
            "tgt_ix": "29-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_77",
            "tgt_ix": "29-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_77",
            "tgt_ix": "29-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_77",
            "tgt_ix": "29-ARR_v1_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_77",
            "tgt_ix": "29-ARR_v1_77@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_77",
            "tgt_ix": "29-ARR_v1_77@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_78",
            "tgt_ix": "29-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_79",
            "tgt_ix": "29-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_79",
            "tgt_ix": "29-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_79",
            "tgt_ix": "29-ARR_v1_79@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_79",
            "tgt_ix": "29-ARR_v1_79@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_80",
            "tgt_ix": "29-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_80",
            "tgt_ix": "29-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_80",
            "tgt_ix": "29-ARR_v1_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_80",
            "tgt_ix": "29-ARR_v1_80@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_81",
            "tgt_ix": "29-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_81",
            "tgt_ix": "29-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_81",
            "tgt_ix": "29-ARR_v1_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_81",
            "tgt_ix": "29-ARR_v1_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_82",
            "tgt_ix": "29-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_82",
            "tgt_ix": "29-ARR_v1_82@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_82",
            "tgt_ix": "29-ARR_v1_82@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_82",
            "tgt_ix": "29-ARR_v1_82@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_82",
            "tgt_ix": "29-ARR_v1_82@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_82",
            "tgt_ix": "29-ARR_v1_82@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_82",
            "tgt_ix": "29-ARR_v1_82@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_82",
            "tgt_ix": "29-ARR_v1_82@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_83",
            "tgt_ix": "29-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_84",
            "tgt_ix": "29-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_85",
            "tgt_ix": "29-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_85",
            "tgt_ix": "29-ARR_v1_85@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_85",
            "tgt_ix": "29-ARR_v1_85@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_85",
            "tgt_ix": "29-ARR_v1_85@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_86",
            "tgt_ix": "29-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_87",
            "tgt_ix": "29-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_88",
            "tgt_ix": "29-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_89",
            "tgt_ix": "29-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_90",
            "tgt_ix": "29-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_91",
            "tgt_ix": "29-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_92",
            "tgt_ix": "29-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_93",
            "tgt_ix": "29-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_94",
            "tgt_ix": "29-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_95",
            "tgt_ix": "29-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_96",
            "tgt_ix": "29-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_97",
            "tgt_ix": "29-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_98",
            "tgt_ix": "29-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_99",
            "tgt_ix": "29-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_100",
            "tgt_ix": "29-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_101",
            "tgt_ix": "29-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_102",
            "tgt_ix": "29-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_103",
            "tgt_ix": "29-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_104",
            "tgt_ix": "29-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_105",
            "tgt_ix": "29-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_106",
            "tgt_ix": "29-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_107",
            "tgt_ix": "29-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_108",
            "tgt_ix": "29-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_109",
            "tgt_ix": "29-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_110",
            "tgt_ix": "29-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_111",
            "tgt_ix": "29-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_112",
            "tgt_ix": "29-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_113",
            "tgt_ix": "29-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_114",
            "tgt_ix": "29-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_115",
            "tgt_ix": "29-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_116",
            "tgt_ix": "29-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_117",
            "tgt_ix": "29-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_118",
            "tgt_ix": "29-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_119",
            "tgt_ix": "29-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_120",
            "tgt_ix": "29-ARR_v1_120@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_121",
            "tgt_ix": "29-ARR_v1_121@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_122",
            "tgt_ix": "29-ARR_v1_122@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "29-ARR_v1_123",
            "tgt_ix": "29-ARR_v1_123@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1302,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "29-ARR",
        "version": 1
    }
}