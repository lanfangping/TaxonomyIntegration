{
    "nodes": [
        {
            "ix": "163-ARR_v2_0",
            "content": "Quality Controlled Paraphrase Generation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_2",
            "content": "Paraphrase generation has been widely used in various downstream tasks. Most tasks benefit mainly from high quality paraphrases, namely those that are semantically similar to, yet linguistically diverse from, the original sentence. Generating high-quality paraphrases is challenging as it becomes increasingly hard to preserve meaning as linguistic diversity increases. Recent works achieve nice results by controlling specific aspects of the paraphrase, such as its syntactic tree. However, they do not allow to directly control the quality of the generated paraphrase, and suffer from low flexibility and scalability. Here we propose QCPG, a quality-guided controlled paraphrase generation model, that allows directly controlling the quality dimensions. Furthermore, we suggest a method that given a sentence, identifies points in the quality control space that are expected to yield optimal generated paraphrases. We show that our method is able to generate paraphrases which maintain the original meaning while achieving higher diversity than the uncontrolled baseline. The models, the code, and the data can be found in https://github.com/IBM/quality-c ontrolled-paraphrase-generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "163-ARR_v2_4",
            "content": "Paraphrase generation, namely rewriting a sentence using different words and/or syntax while preserving its meaning (Bhagat and Hovy, 2013), is an important technique in natural language processing, that has been widely used in various downstream tasks including question answering (Fader et al., 2014a;McCann et al., 2018), summarization (Rush et al., 2015), data augmentation (Yu et al., 2018) and adversarial learning (Iyyer et al., 2018). However, not all paraphrases are equally useful. For most real-world applications, paraphrases which are too similar to the original sentence are of limited value, while those with high linguistic diversity, i.e. with large syntactic/lexical differences between the paraphrase and the original sentence, are more beneficial to the robustness and accuracy of automatic text evaluation and classification, and can avoid the blandness caused by repetitive patterns (Qian et al., 2019). The quality of paraphrases is often evaluated using three dimensions, where high quality paraphrases are those with high semantic similarity as well as high lexical and/or syntactic diversity (McCarthy et al., 2009).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_5",
            "content": "Generating high quality paraphrases can be challenging (for both humans and automatic models) since it is increasingly difficult to preserve meaning with increasing linguistic diversity. Indeed, when examining the quality of paraphrases among paraphrase generation datasets, one can find a wide range of paraphrase qualities, where the area of high quality is often very sparse (see Figure 1). This in turn results in scarcity of supervised data for high-quality paraphrase generation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_6",
            "content": "A recent approach aiming to produce high quality paraphrases is controlled paraphrase generation, which exposes control mechanisms that can be manipulated to produce diversity. While the controlled generation approaches have yielded impressive results, they require providing the model with very specific information regarding the target sentence, such as its parse tree (Iyyer et al., 2018) or the list of keywords it needs to contain (Zeng et al., 2019). However, for most downstream applications, the important property of the paraphrase is its overall quality, rather than its specific syntactic or lexical form. The over-specificity of existing controlbased methods not only complicates their usage and limits their scalability, but also hinders their coverage. Thus, it would be desirable to develop a paraphrase generation model, which uses a simple mechanism for directly controlling paraphrase quality, while avoiding unnecessary complications associated with fine-grained controls.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_7",
            "content": "In this paper we propose QCPG, a Quality Controlled Paraphrase Generation model, that given an input sentence and quality constraints, represented by a three dimensional vector of semantic similarity, and syntactic and lexical distances, produces a target sentence that conforms to the quality constraints.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_8",
            "content": "Our constraints are much simpler than previously suggested ones, such as parse trees or keyword lists, and leave the model the freedom to choose how to attain the desired quality levels.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_9",
            "content": "Enabling the direct control of the three quality dimensions, allows flexibility with respect to the specific requirements of the task at hand, and opens a range of generation possibilities: paraphrases of various flavors (e.g. syntactically vs. lexically diverse), quasi-paraphrases (with lower semantic similarity), and even non-paraphrases which may be useful for downstream tasks (e.g. hard negative examples of sentences that are linguistically similar but have different meanings (Guo et al., 2018;Reimers and Gurevych, 2020)).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_10",
            "content": "Our results show that the QCPG model indeed enables controlling paraphrase quality along the three quality dimensions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_11",
            "content": "Furthermore, even though the training data is of mixed quality, and exhibits scarcity in the high quality area (see Figure 1), our model is able to learn high quality paraphrasing behavior, i.e. it increases the linguistic diversity of the generated paraphrases without decreasing the semantic similarity compared to the uncontrolled baseline.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_12",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "163-ARR_v2_13",
            "content": "In this section we provide a general description of our approach. We first explain how the different quality dimensions are measured. We then describe the controlled paraphrase generation model, QCPG, and finally we suggest a method that given the task requirements, detects the input control values which maximize the quality of the generated paraphrases. Figure 2 summarizes our proposed solution for generating controlled paraphrases, which is detailed in the rest of the section.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_14",
            "content": "Quantifying Paraphrase Quality",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "163-ARR_v2_15",
            "content": "The most common dimensions for measuring paraphrase quality are the semantic, syntactic and lexical dimensions. Several previous works used also a fluency evaluation metric (Siddique et al., 2020). However, since our focus is on the supervised setting, we rely on the gold paraphrases as fluency guidance for the model (Mc-Carthy et al., 2009). Thus, given a sentence s and a paraphrase s \u2032 , we define the paraphrase quality as a three dimensional vector q(s, s \u2032 ) = (q sem (s, s \u2032 ), q syn (s, s \u2032 ), q lex (s, s \u2032 )), where q sem is a measure of semantic similarity, and q syn and q lex are measures of syntactic and lexical variation, respectively. For the syntactic score, inspired by Iyyer et al. (2018) we choose q syn (s, s \u2032 ) to be the normalized tree edit distance (Zhang and Shasha, 1989) between the third level constituency parsetrees of s and s \u2032 , after removing the tokens -to increase the decoupling from the lexical distance metric. We define the lexical score q lex (s, s \u2032 ) to be the normalized character-level minimal edit distance between the bag of words. This measure is independent of word order, and hence increases the decoupling from syntactic measures. Additionally, calculating the token distances on the character level enables to capture tokens that share the same stem/lemma. Character-level distance is also more robust to typos that may be found in noisy data. As for the semantic score, several strong metrics have been recently proposed for measuring semantic similarity between sentences. In order to select q sem (s, s \u2032 ), we studied the agreement between the candidate metrics and human judgments, using only development data, and found Bleurt (Sellam et al., 2020) to have the highest correlation with human judgments (see Appendix A). Thus, we define q sem (s, s \u2032 ) to be the Bleurt score, normalized using the sigmoid function to ensure a uniform range of values, [0, 1], for all three quality dimensions. For ease of presentation all metrics are presented on a 0 \u2212 100 scale.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_16",
            "content": "The QCPG Model",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "163-ARR_v2_17",
            "content": "The main component of our solution is a quality controlled paraphrase generation model (QCPG), which is an encoder-decoder model trained on the task of controlled paraphrase generation. Given an input sentence s and a control vector c = (c sem , c syn , c lex ), the goal of QCPG is to generate an output paraphrase QCP G(s, c) that conforms to c. We train QCPG using the training set pairs (s, t), by setting c to be q(s, t), and maximizing P (t|s, c = q(s, t)) over the training set via the autoregressive cross entropy loss.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_18",
            "content": "Control Values Selection",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "163-ARR_v2_19",
            "content": "A major challenge in the research of controlled paraphrase generation, is selecting appropriate input control values that can be achieved by the model (Goyal and Durrett, 2020). Clearly, given a sentence, not all paraphrase qualities are achievable. Some sentences are more amenable to paraphrasing than others. For example, named entities and numbers are much harder to be replaced while keeping sentence meaning, and hence, the potential lexical diversity of paraphrases involving such terms is relatively limited. Forcing QCPG to conform to quality control values that are too high with respect to the input sentence, may lead to suboptimal quality of the resultant paraphrases. Thus, for a more effective use of QCPG, the control values should be determined with respect to the input sentence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_20",
            "content": "Below we describe the second part of our solution, namely a method that given a sentence, predicts the input control values, c(s), that optimize the expected quality of the paraphrases generated by QCPG. For simplicity we assume that the quality distribution p(q|s) of all paraphrases of sentence s, is approximately normally distributed around a sentence dependent mean q 0 (s), and that the variance is approximately sentenceindependent. We further assume that given an input sentence s, the difficulty to generate a paraphrase of a given quality, q, is dominated by p(q|s) rather than by the quality vector q itself. Following our assumptions, the level of difficulty can be expressed by the offset, o = (o sem , o syn , o lex ) of q from q 0 (s). Thus, the input control, c(s), for QCPG, is the sum of q 0 (s) and an offset o.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_21",
            "content": "Our aim is to analyze the model results for varying levels of difficulty, namely under different offsets, o, from q 0 (s).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_22",
            "content": "The Quality Predictor (QP): Since q 0 (s) is unknown, we introduce QP, a regressor whose output, termed the reference of s, r(s) = (r sem (s), r syn (s), r lex (s)), approximates q 0 (s).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_23",
            "content": "During training, QP aims to predict q(s, t) given s, where (s, t) are the input-output pairs of the training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_24",
            "content": "To summarize, we define sentence-aware quality control by decomposing the QCPG input control, c, into a sum of a sentence dependent reference point, r(s), and a sentence independent offset, o.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_25",
            "content": "Data and Implementation Details 3.1 Datasets",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "163-ARR_v2_26",
            "content": "To test the ability of our model to learn high quality behavior from mixed quality data we use weakly annotated datasets. These datasets are large but noisy, and contain only a relatively small amount of high quality paraphrases.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_27",
            "content": "MSCOCO: This dataset consists of 123K images, where each image contains at most five human-labeled captions (Lin et al., 2014). Similar to previous works we consider different captions of the same image as paraphrases.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_28",
            "content": "WikiAnswers (WikiAns for short): The WikiAnswers corpus contains clusters of questions tagged by wiki-answers.com users as similar. There are 30, 370, 994 clusters with 25 question in each on average. In total, the corpus contains over 70 million question pairs (Fader et al., 2014b).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_29",
            "content": "ParaBank2.0: A dataset containing clusters of sentential paraphrases, produced from a bilingual corpus using negative constraints, inference sampling, and clustering (Hu et al., 2019). The dataset is composed of avarage of 5 paraphrases in every cluster and close to 100 million pairs in total.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_30",
            "content": "To get comparable results across all datasets, we randomly sub-sampled ParaBank2.0 and WikiAns to the same size as MSCOCO, and split them to train, dev and test sets, of sizes 900K, 14K and 14K respectively. We carefully made sure that there are no pairs from the same cluster in different splits of the data. The full data splits will be published with our code.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_31",
            "content": "Implementation Details",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "163-ARR_v2_32",
            "content": "All models are trained with batch size of 32 on 2 NVIDIA A100 GPUs for 6 epochs. Full details as well as train and dev results can be found in Appendix C.1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_33",
            "content": "QCPG: We use the pre-trained T5-base (Raffel et al., 2020) as the encoder-decoder model. The control input vector to QCPG is quantized at every dimension into 20 equally spaced values ranging from 0 to 100. Each value is assigned to a special saved-token. The three tokens corresponding to the quantized values of the control vector c, are concatenated to the head of the input sentence, and together used as input to the model. r(s) and o are also quantized in a similar way.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_34",
            "content": "QP: An Electra base model (Clark et al., 2020) finetuned with MSE loss to predict the typical quality values (see Section 2.3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_35",
            "content": "Baseline Model (BL): A T5-base model finetuned on the training data.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_36",
            "content": "For all the models, we adopt the experimental setup used in (Devlin et al., 2019), i.e. we train the model with several learning rates and choose the one that achieves the highest dev set performance (see appendix C.1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_37",
            "content": "Results",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "163-ARR_v2_38",
            "content": "Controlling the Quality Dimensions",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "163-ARR_v2_39",
            "content": "The aim of the following analysis is to study the level of control achieved by QCPG. To this end, we measure the model response to changes in the input offsets. We compute the expected difference in paraphrase quality, as a result of applying an input offset o compared to zero offset as a reference. More formally, we define the 3-dimensional responsiveness vector of QCPG at an offset o, Specifically, in the following analysis we are interested in studying the model response to each of the dimensions separately, i.e. how changing the input offset along a given quality dimension dim -the controlled dimension -while keeping the two other dimensions constant, affects the responsiveness in each of the three dimensions. A good control mechanism would imply that increasing the input offset in one dimension will result in a monotonically increasing responsiveness in that dimension, with relatively small responsiveness in the other two dimensions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_40",
            "content": "R(o) as Q(o) \u2212 Q((0, 0, 0)), where Q(o)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_41",
            "content": "Figure 3 shows, for each of the three datasets, the responsiveness in the three quality dimensions, when changing the input offset along each of the three dimensions, while fixing the input offsets in the other two dimensions at 0. Examining the actual values of quality in the paraphrases of the dev sets, reveals that the standard deviation is different in each dimension. Hence, for clarity of presentation, we present the input offset values and the responsiveness in units of standard deviation as measured in the respective dimension and dev set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_42",
            "content": "For the range of offsets displayed in Figure 3, the responsiveness in the controlled dimension increases monotonically with the input offsets across all datasets and dimensions. As expected, the responsiveness in the uncontrolled dimensions does not zeros due to the inherent coupling between the dimensions. For example, many changes that increase syntactic diversity, also increase lexical diversity (e.g. a move from passive to active voice). Still, our control mechanism is able to increase the responsiveness in the controlled dimension with relative low responsiveness in the uncontrolled dimensions. Specifically, focusing on the relation between semantic similarity and expression diversity, the figure shows that there is a minor decrease in semantic similarity in response to an increase in lexical and syntactic diversity. In the next section, we will show that this does not prevent our model from generating paraphrases that are not only more lexically and syntactically diverse, but also more semantically similar to the source sentences, compared to the paraphrases generated by the uncontrolled baseline.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_43",
            "content": "Figure 3 focused on small to moderate input offsets, i.e. offsets up to 2 stds from the reference point. However, as we speculated before, with increasing offsets, i.e. the more the requested control value deviates from the typical value, it becomes increasingly difficult to generate a paraphrase that conforms to the requested control value. Figure 4 depicts the responsiveness in the syntactic and lexical dimensions for a larger range of offset values. For the semantic dimension, the typical values are too high to allow large positive offsets, which for most sentences result in exceeding the upper limit of the semantic score. Indeed, as can be seen in Figure 4, when moving to high offset values, the responsiveness in the syntactic and lexical dimensions starts to decrease. This behavior is in line with our aforementioned hypothesis, and reflects the detrimental effect of feeding QCPG with input control values that are too far from the typical paraphrase qualities of the input sentence. The nonmonotonic behavior of the responsiveness implies that the input offsets should be selected carefully in order to optimize the quality of the resultant paraphrases. In Section 4.2 we suggest a method for identifying these optimal offsets.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_44",
            "content": "Selecting Optimal Input Control Values",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "163-ARR_v2_45",
            "content": "In this section, we suggest a method that given task requirements, selects the input offsets that are expected to yield the desired quality of paraphrases. The idea is to compute the estimated expected quality, Q(o), for each input offset o, using the dev set as described in Section 4.1, and then search the 3D grid of input offsets to find the point for which Q(o) is best suited for the user's requirements. We envision this analysis as a preliminary step in which the user chooses the input control parameters that best achieve his desired paraphrasing operation point, and then uses the chosen values at inference -which is why we use the dev set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_46",
            "content": "We study the behavior of Q(o) as a function of the 3D grid of offset points in the relevant range, i.e every o where o sem , o syn and o lex in 0, 5, 10...50. The QCPG results are compared to two reference points, which are invariant to o and are marked on the colorbars with black squares: 'Dataset' is the semantic-similarity/linguistic-diversity average value over the corresponding dev set paraphrases, and 'Baseline' is the average semanticsimilarity/linguistic-diversity of the uncontrolled baseline over the corresponding dev set. Notice that the average diversity level achieved by the uncontrolled baseline is lower than that of the dev set mean, reflecting the difficulty of this model to generate diverse paraphrases. QCPG on the other hand, with suitable input offset values, is able to generate paraphrases which are on average higher than the baseline both in their linguistic diversity and in their semantic similarity, and in fact even higher in many cases than the values of the ground truth paraphrases in the dev-set.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_47",
            "content": "In general, the estimates of the expected quality achieved by QCPG at different input offsets, enable a user to generate paraphrases at different operation points, by manipulating the input offset control o to meet her desired quality values. Consider for example a typical use case, of aiming to maximize linguistic diversity under a constraint on semantic similarity. An example of such a case is an operation point, denoted by QCP G \u22c6 , which aims to exemplify the advantage of QCPG over the baseline, by maximizing linguistic diversity under the constraint that the semantic similarity is at least 5 points higher than the baseline. The input offset values to obtain this operation point depend on the dataset, and can be found using heatmaps such as in Figure 5. For WikiAns the input offset for the QCP G \u22c6 operation point values are (50, 35, 5) (entry marked by the black square).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_48",
            "content": "Quality Evaluation on the Test Set",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "163-ARR_v2_49",
            "content": "In the previous section we saw, using estimates based on the dev sets, that there are many operation points which generate paraphrases with higher quality than those achieved by the uncontrolled baseline. We now turn to evaluate one such operation point, namely QCP G \u22c6 , using the source sentences of the test sets which were not used in the selection of the input offset values.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_50",
            "content": "We use four quality measures to evaluate different aspects of generated paraphrases. The three quality measures used in the control of QCPG (Section 2.1) and Self-BLEU (Zhu et al., 2018) as adapted in ; Liu et al. (2020a), which aims to measure the linguistic diversity in the generated paraphrases by penalizing copying from input sentences. As can be seen in Table 1, QCP G \u22c6 outperforms the baseline in all metrics across all datasets, as predicted using the dev-set heatmaps. A clear advantage is obtained even for Self-BLEU, which was not part of the metrics used as input controls. Importantly, the quality of the paraphrases generated by our model is comparable to, or at times better than the quality of the paraphrases in the ground truth of the datasets. Examples of paraphrases generated by QCP G \u22c6 compared to the ground truth paraphrases appear in Table 10. This is an important step towards the goal of obtaining paraphrases in the sparse area of high quality (recall the top right corner of Figure 1).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_51",
            "content": "Additionally, we examined QCPG from another perspective: the effect of the quality guidance on the model's ability to predict the ground truth paraphrases. Tables 5 and 6 show the BLEU scores (Papineni et al., 2002) obtained by QCPG and the uncontrolled baseline respectively. The results verify that the input quality vectors induced by the target sentences are effectively utilized by QCPG to achieve better prediction performance.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_52",
            "content": "Human Evaluation While linguistic diversity can be automatically measured by reliable metrics such as Self-BLEU, measuring semantic similarity is more challenging. We therefore rely on automatic metrics for evaluating the lexical and syntactic diversity, but use human annotation for validating the semantic evaluation. To this end, we selected a sample of 50 source sentences from each test set, and generated one paraphrase using the uncontrolled baseline and one using QCP G \u22c6 . The annotators were shown the source sentence, along with the two generated paraphrases (randomly ordered), and were asked which of the two better preserves the semantic meaning of the source sentence (ties are also allowed). In total, 150 triplets were evaluated by 5 judges. Table 2 demonstrates an advantage for QCP G \u22c6 in all datasets, with a large margin in MSCOCO and WikiAns. This advantage is statistically significant (p \u2212 value < 0.05) as obtained by applying the Wilcoxon signed-rank test to the difference between the number of annotators that voted for QCP G \u22c6 and those voted for the baseline, across all datasets. Thus, the human evaluation is in line with the results of the automatic semantic similarity measure. We also verified, that the results of this sample, in terms of linguistic diversity, are very similar to those shown in Table 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_53",
            "content": "For examples of paraphrases generated by QCP G \u22c6 see Table 10 in the Appendix.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_54",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "163-ARR_v2_55",
            "content": "Many recent works on paraphrase generation have been focused on attempting to achieve high-quality paraphrases. These works can be divided into supervised and unsupervised approaches.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_56",
            "content": "Supervised Approaches To achieve diversity, some works focused on diverse decoding using heuristics such as Hamming distance or distinct n-grams to preserve diverse options during beam search (Vijayakumar et al., 2018). Other works generate multiple outputs by perturbing latent representations (Gupta et al., 2018;Park et al., 2019). or by using distinct generators (Qian et al., 2019). These methods achieve some diversity, but do not control generation in an interpretable manner.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_57",
            "content": "The works that are most similar to ours strive to gain diversity using controlled-paraphrase generation, by exposing control mechanisms that are manipulated to produce either lexically (Zeng et al., 2019;Thompson and Post, 2020) or syntactically Goyal and Durrett, 2020) diverse paraphrases. One approach is to use an exemplar sentence for guiding the syntax of the generated paraphrase Bao et al., 2019;Hosking and Lapata, 2021). An alternative is to directly employ constituency tree as the syntax guidance (Iyyer et al., 2018;Li and Choi, 2020). Goyal and Durrett (2020) promote syntactic diversity by conditioning over possible syntactic rearrangements of the input. Zeng et al. ( 2019) use keywords as lexical guidance for the generation process. Here we introduce a simple model for jointly controlling the lexical, syntactic and semantic aspects of the generated paraphrases.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_58",
            "content": "Unsupervised Approaches Niu et al. (2020) rely on neural models to generate high quality paraphrases, using a decoding method that enforces diversity by preventing repetitive copying of the input tokens. Liu et al. (2020b) optimize a quality oriented objective by casting paraphrase generation as an optimization problem, and searching the sentence space to find the optimal point. Garg et al. (2021) and Siddique et al. (2020) use reinforcement learning with quality-oriented reward combining textual entailment, semantic similarity, expression diversity and fluency. In this work, we employ similar metrics for guiding the generation of paraphrases within the supervised framework.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_59",
            "content": "Discussion",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "163-ARR_v2_60",
            "content": "In this paper, we propose a novel controlled paraphrase generation model, that leverages measures of paraphrase quality for encouraging the generation of paraphrases with desired quality. We demonstrate the high level of control achieved by the model, and suggest a method for coping with the challenging problem of finding suitable control values.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_61",
            "content": "Aside from offering a simple and effective way for controlling models' output quality, the quality control paradigm enables a holistic view of the data, the training process and the final model analysis. Namely: (I) Examination of the training data through the lens of data quality enables to characterize the data at hand, its strengths and limitations. (II) A quality-aware training process can be viewed as multi-task learning, where each quality level is a separate task with its own accurate supervision, as opposed to the standard quality-agnostic approach, where low quality data is in fact used as a poor supervision for a model which aims at generating higher quality output. (III) Analyzing the model behavior under different quality controls, allows finer understanding of the different model behaviors and the trade-offs between their output qualities. Better understanding the expected output quality of neural NLG models, for different input quality controls, can increase the trust in their output.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_62",
            "content": "Finally, our model analysis consistently shows that although the models generally follow the quality requirements, there is still room for improvement. A possible direction for future research is exploring methods, such as reinforcement learning, for further improving the ability of the model to satisfy the quality requirements.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_63",
            "content": "Recently, several strong metrics have been proposed for measuring semantic similarity between sentences (Reimers and Gurevych, 2019;?;Sellam et al., 2020). In order to select the semantic similarity metric for QCPG, we performed a small experiment over the three dev sets, with the aim of measuring the agreement of the candidate metrics with human judgments. To this end, we leveraged two properties that characterize weakly labeled datasets, the underlying clusters of sentences, and the high variability of semantic similarity. Given a dataset, we randomly selected 100 clusters, and picked three sentences at random from each cluster. For each triplet of sentences t = (t 1 , t 2 , t 3 ) we asked 5 human annotators to choose which of the two sentences, t 2 or t 3 , better preserves the semantic meaning of t 1 . In order to find the candidate similarity measure with the highest agreement with human judgments, we first computed, for each triplet, the difference between the number of annotators voted for t 2 and those voted for t 3 . We then computed for each candidate measure, the difference between the similarity of t 2 to t 1 and and of t 3 to t 1 . We then measured Kendall's Tau correlation (Daniel, 1990) between the difference vector of the human judgments and that of the judgments of each of the candidate measures. Table 3 shows the resultant correlations. The highest correlations are obtained for SBERT (Reimers and Gurevych, 2019), but since it was trained on WikiAns and MSCOCO, we could not use it in our study. We selected Bleurt due to its highest correlation with human judgments over the three datasets (among the methods that were not exposed to the considered datasets). We normalize Bleurt score using the sigmoid function to ensure a uniform range of values, [0, 1], for the three quality dimensions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_64",
            "content": "We study the coupling between the different semantic similarity measures and the linguistic diversity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_65",
            "content": "We assume that the level of coupling of a good similarity measure will resemble that of humans, and will be less sensitive to lexical and syntactic properties of the paraphrase. Table 4 presents the Kendall tau correlation between the different similarity measures and the linguistic diversity. Results for human judgments are also shown for a reference. The correlation calculation is performed between the vectors of differences as described in section A).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_66",
            "content": "The results show that Bleurt demonstrates the lowest coupling with linguistic diversity among the automatic measures (aside from SBERT which, as mentioned before, was trained with MSCOCO and WikiAns). The comparison to human judgments shows that Bleurt is more influenced by linguistic features, indicating that automatic measures need to be further improved to reach the decoupling level achieved by humans.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_67",
            "content": "The learning rates for the QCPG and the Baseline models were selected in the following way. For a given dataset, we finetuned the models with 4 learning rates (1e-3, 1e-4, 5e-3, 5e-4) (The training results of the baseline presented in Table 5 and the results of QCPG presented in Table 6.). For the baseline we selected the one which yielded the best BLEU score (Papineni et al., 2002) on the corresponding dev set The best learning rate for every dataset was chosen based on the Dev set BLEU score. For the QCPG we chose the model that best conforms to the control input as measured by the MSE between the input control vector and the output quality vector (see Table 9). The QP model is an Electra-Base model finetuned with 4 different learning rates (1.5e-4, 1e-4, 3e-5, 5e-5).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_68",
            "content": "We choose the learning rate the yields the minimal MSE on the dev set (For full results see Table 8)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_69",
            "content": "The full heatmaps can be found in Figure 6.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "163-ARR_v2_70",
            "content": "Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xin-Yu Dai, Jiajun Chen, Generating sentences from disentangled syntactic and semantic spaces, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Yu Bao",
                    "Hao Zhou",
                    "Shujian Huang",
                    "Lei Li",
                    "Lili Mou",
                    "Olga Vechtomova",
                    "Xin-Yu Dai",
                    "Jiajun Chen"
                ],
                "title": "Generating sentences from disentangled syntactic and semantic spaces",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_71",
            "content": "Rahul Bhagat, Eduard Hovy, Squibs: What is a paraphrase?, 2013, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Rahul Bhagat",
                    "Eduard Hovy"
                ],
                "title": "Squibs: What is a paraphrase?",
                "pub_date": "2013",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_72",
            "content": "Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel, Controllable paraphrase generation with a syntactic exemplar, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Mingda Chen",
                    "Qingming Tang",
                    "Sam Wiseman",
                    "Kevin Gimpel"
                ],
                "title": "Controllable paraphrase generation with a syntactic exemplar",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_73",
            "content": "UNKNOWN, None, 2020, Electra: Pre-training text encoders as discriminators rather than generators, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2020",
                "pub_title": "Electra: Pre-training text encoders as discriminators rather than generators",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_74",
            "content": "UNKNOWN, None, 1990, Applied nonparametric statistics pws, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "1990",
                "pub_title": "Applied nonparametric statistics pws",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_75",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "Jacob Devlin",
                    "Ming-Wei Chang",
                    "Kenton Lee",
                    "Kristina Toutanova"
                ],
                "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_76",
            "content": "Anthony Fader, Luke Zettlemoyer, Oren Etzioni, Open question answering over curated and extracted knowledge bases, 2014, Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Anthony Fader",
                    "Luke Zettlemoyer",
                    "Oren Etzioni"
                ],
                "title": "Open question answering over curated and extracted knowledge bases",
                "pub_date": "2014",
                "pub_title": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_77",
            "content": "Anthony Fader, Luke Zettlemoyer, Oren Etzioni, Open Question Answering Over Curated and Extracted Knowledge Bases, 2014, KDD, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Anthony Fader",
                    "Luke Zettlemoyer",
                    "Oren Etzioni"
                ],
                "title": "Open Question Answering Over Curated and Extracted Knowledge Bases",
                "pub_date": "2014",
                "pub_title": "KDD",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_78",
            "content": "UNKNOWN, None, 2021, Unsupervised contextual paraphrase generation using lexical control and reinforcement learning, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Unsupervised contextual paraphrase generation using lexical control and reinforcement learning",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_79",
            "content": "Tanya Goyal, Greg Durrett, Neural syntactic preordering for controlled paraphrase generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Tanya Goyal",
                    "Greg Durrett"
                ],
                "title": "Neural syntactic preordering for controlled paraphrase generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_80",
            "content": "Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Ge, Daniel Cer, Gustavo Abrego, Keith Stevens, Noah Constant, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil, Effective parallel corpus mining using bilingual sentence embeddings, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Mandy Guo",
                    "Qinlan Shen",
                    "Yinfei Yang",
                    "Heming Ge",
                    "Daniel Cer",
                    "Gustavo Abrego",
                    "Keith Stevens",
                    "Noah Constant",
                    "Yun-Hsuan Sung",
                    "Brian Strope",
                    "Ray Kurzweil"
                ],
                "title": "Effective parallel corpus mining using bilingual sentence embeddings",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_81",
            "content": "Ankush Gupta, Arvind Agarwal, Prawaan Singh, Piyush Rai, A deep generative framework for paraphrase generation, 2018, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Ankush Gupta",
                    "Arvind Agarwal",
                    "Prawaan Singh",
                    "Piyush Rai"
                ],
                "title": "A deep generative framework for paraphrase generation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_82",
            "content": "Tom Hosking, Mirella Lapata, Factorising meaning and form for intent-preserving paraphrasing, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Tom Hosking",
                    "Mirella Lapata"
                ],
                "title": "Factorising meaning and form for intent-preserving paraphrasing",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_83",
            "content": "J Hu, Abhinav Singh, Nils Holzenberger, Matt Post, Benjamin Van Durme, Large-scale, diverse, paraphrastic bitexts via sampling and clustering, 2019, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "J Hu",
                    "Abhinav Singh",
                    "Nils Holzenberger",
                    "Matt Post",
                    "Benjamin Van Durme"
                ],
                "title": "Large-scale, diverse, paraphrastic bitexts via sampling and clustering",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_84",
            "content": "Mohit Iyyer, John Wieting, Kevin Gimpel, Luke Zettlemoyer, Adversarial example generation with syntactically controlled paraphrase networks, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Mohit Iyyer",
                    "John Wieting",
                    "Kevin Gimpel",
                    "Luke Zettlemoyer"
                ],
                "title": "Adversarial example generation with syntactically controlled paraphrase networks",
                "pub_date": "2018",
                "pub_title": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "163-ARR_v2_85",
            "content": "Changmao Li, Jinho Choi, Transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Changmao Li",
                    "Jinho Choi"
                ],
                "title": "Transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_86",
            "content": "Zichao Li, Xin Jiang, Lifeng Shang, Qun Liu, Decomposable neural paraphrase generation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Zichao Li",
                    "Xin Jiang",
                    "Lifeng Shang",
                    "Qun Liu"
                ],
                "title": "Decomposable neural paraphrase generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_87",
            "content": "Tsung-Yi Lin, M Maire, Serge Belongie, James Hays, P Perona, D Ramanan, Piotr Doll\u00e1r, C , Microsoft coco: Common objects in context, 2014, ECCV, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Tsung-Yi Lin",
                    "M Maire",
                    "Serge Belongie",
                    "James Hays",
                    "P Perona",
                    "D Ramanan",
                    "Piotr Doll\u00e1r",
                    "C "
                ],
                "title": "Microsoft coco: Common objects in context",
                "pub_date": "2014",
                "pub_title": "ECCV",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_88",
            "content": "Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song, Unsupervised paraphrasing by simulated annealing, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Xianggen Liu",
                    "Lili Mou",
                    "Fandong Meng",
                    "Hao Zhou",
                    "Jie Zhou",
                    "Sen Song"
                ],
                "title": "Unsupervised paraphrasing by simulated annealing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_89",
            "content": "Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song, Unsupervised paraphrasing by simulated annealing, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Xianggen Liu",
                    "Lili Mou",
                    "Fandong Meng",
                    "Hao Zhou",
                    "Jie Zhou",
                    "Sen Song"
                ],
                "title": "Unsupervised paraphrasing by simulated annealing",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_90",
            "content": "UNKNOWN, None, 2018, The natural language decathlon: Multitask learning as question answering, CoRR.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "The natural language decathlon: Multitask learning as question answering",
                "pub": "CoRR"
            }
        },
        {
            "ix": "163-ARR_v2_91",
            "content": "Philip Mccarthy, Rebekah Guess, D Mcnamara, The components of paraphrase evaluations, 2009, Behavior Research Methods, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Philip Mccarthy",
                    "Rebekah Guess",
                    "D Mcnamara"
                ],
                "title": "The components of paraphrase evaluations",
                "pub_date": "2009",
                "pub_title": "Behavior Research Methods",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_92",
            "content": "UNKNOWN, None, , Nitish Shirish Keskar, and Caiming Xiong. 2020. Unsupervised paraphrase generation via dynamic blocking, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Nitish Shirish Keskar, and Caiming Xiong. 2020. Unsupervised paraphrase generation via dynamic blocking",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_93",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_94",
            "content": "Sunghyun Park,  Seung-Won, Fuxiang Hwang, Jaegul Chen, Jung-Woo Choo, Sunghun Ha, Jinyeong Kim,  Yim, Paraphrase diversification using counterfactual debiasing, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "Sunghyun Park",
                    " Seung-Won",
                    "Fuxiang Hwang",
                    "Jaegul Chen",
                    "Jung-Woo Choo",
                    "Sunghun Ha",
                    "Jinyeong Kim",
                    " Yim"
                ],
                "title": "Paraphrase diversification using counterfactual debiasing",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_95",
            "content": "Lihua Qian, Lin Qiu, Weinan Zhang, Xin Jiang, Yong Yu, Exploring diverse expressions for paraphrase generation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Lihua Qian",
                    "Lin Qiu",
                    "Weinan Zhang",
                    "Xin Jiang",
                    "Yong Yu"
                ],
                "title": "Exploring diverse expressions for paraphrase generation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_96",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "Journal of Machine Learning Research",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_97",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-BERT: Sentence embeddings using Siamese BERTnetworks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Nils Reimers",
                    "Iryna Gurevych"
                ],
                "title": "Sentence-BERT: Sentence embeddings using Siamese BERTnetworks",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "163-ARR_v2_98",
            "content": "Nils Reimers, Iryna Gurevych, Making monolingual sentence embeddings multilingual using knowledge distillation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Nils Reimers",
                    "Iryna Gurevych"
                ],
                "title": "Making monolingual sentence embeddings multilingual using knowledge distillation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_99",
            "content": "Alexander Rush, Sumit Chopra, Jason Weston, A neural attention model for abstractive sentence summarization, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Alexander Rush",
                    "Sumit Chopra",
                    "Jason Weston"
                ],
                "title": "A neural attention model for abstractive sentence summarization",
                "pub_date": "2015",
                "pub_title": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "163-ARR_v2_100",
            "content": "Thibault Sellam, Dipanjan Das, Ankur Parikh, BLEURT: Learning robust metrics for text generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Thibault Sellam",
                    "Dipanjan Das",
                    "Ankur Parikh"
                ],
                "title": "BLEURT: Learning robust metrics for text generation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Online. Association for Computational Linguistics"
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "163-ARR_v2_0@0",
            "content": "Quality Controlled Paraphrase Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_0",
            "start": 0,
            "end": 39,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_2@0",
            "content": "Paraphrase generation has been widely used in various downstream tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_2",
            "start": 0,
            "end": 70,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_2@1",
            "content": "Most tasks benefit mainly from high quality paraphrases, namely those that are semantically similar to, yet linguistically diverse from, the original sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_2",
            "start": 72,
            "end": 230,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_2@2",
            "content": "Generating high-quality paraphrases is challenging as it becomes increasingly hard to preserve meaning as linguistic diversity increases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_2",
            "start": 232,
            "end": 368,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_2@3",
            "content": "Recent works achieve nice results by controlling specific aspects of the paraphrase, such as its syntactic tree.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_2",
            "start": 370,
            "end": 481,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_2@4",
            "content": "However, they do not allow to directly control the quality of the generated paraphrase, and suffer from low flexibility and scalability.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_2",
            "start": 483,
            "end": 618,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_2@5",
            "content": "Here we propose QCPG, a quality-guided controlled paraphrase generation model, that allows directly controlling the quality dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_2",
            "start": 620,
            "end": 754,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_2@6",
            "content": "Furthermore, we suggest a method that given a sentence, identifies points in the quality control space that are expected to yield optimal generated paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_2",
            "start": 756,
            "end": 915,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_2@7",
            "content": "We show that our method is able to generate paraphrases which maintain the original meaning while achieving higher diversity than the uncontrolled baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_2",
            "start": 917,
            "end": 1072,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_2@8",
            "content": "The models, the code, and the data can be found in https://github.com/IBM/quality-c ontrolled-paraphrase-generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_2",
            "start": 1074,
            "end": 1189,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_4@0",
            "content": "Paraphrase generation, namely rewriting a sentence using different words and/or syntax while preserving its meaning (Bhagat and Hovy, 2013), is an important technique in natural language processing, that has been widely used in various downstream tasks including question answering (Fader et al., 2014a;McCann et al., 2018), summarization (Rush et al., 2015), data augmentation (Yu et al., 2018) and adversarial learning (Iyyer et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_4",
            "start": 0,
            "end": 441,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_4@1",
            "content": "However, not all paraphrases are equally useful.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_4",
            "start": 443,
            "end": 490,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_4@2",
            "content": "For most real-world applications, paraphrases which are too similar to the original sentence are of limited value, while those with high linguistic diversity, i.e. with large syntactic/lexical differences between the paraphrase and the original sentence, are more beneficial to the robustness and accuracy of automatic text evaluation and classification, and can avoid the blandness caused by repetitive patterns (Qian et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_4",
            "start": 492,
            "end": 924,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_4@3",
            "content": "The quality of paraphrases is often evaluated using three dimensions, where high quality paraphrases are those with high semantic similarity as well as high lexical and/or syntactic diversity (McCarthy et al., 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_4",
            "start": 926,
            "end": 1141,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_5@0",
            "content": "Generating high quality paraphrases can be challenging (for both humans and automatic models) since it is increasingly difficult to preserve meaning with increasing linguistic diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_5",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_5@1",
            "content": "Indeed, when examining the quality of paraphrases among paraphrase generation datasets, one can find a wide range of paraphrase qualities, where the area of high quality is often very sparse (see Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_5",
            "start": 187,
            "end": 392,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_5@2",
            "content": "This in turn results in scarcity of supervised data for high-quality paraphrase generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_5",
            "start": 394,
            "end": 484,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_6@0",
            "content": "A recent approach aiming to produce high quality paraphrases is controlled paraphrase generation, which exposes control mechanisms that can be manipulated to produce diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_6",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_6@1",
            "content": "While the controlled generation approaches have yielded impressive results, they require providing the model with very specific information regarding the target sentence, such as its parse tree (Iyyer et al., 2018) or the list of keywords it needs to contain (Zeng et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_6",
            "start": 177,
            "end": 455,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_6@2",
            "content": "However, for most downstream applications, the important property of the paraphrase is its overall quality, rather than its specific syntactic or lexical form.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_6",
            "start": 457,
            "end": 615,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_6@3",
            "content": "The over-specificity of existing controlbased methods not only complicates their usage and limits their scalability, but also hinders their coverage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_6",
            "start": 617,
            "end": 765,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_6@4",
            "content": "Thus, it would be desirable to develop a paraphrase generation model, which uses a simple mechanism for directly controlling paraphrase quality, while avoiding unnecessary complications associated with fine-grained controls.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_6",
            "start": 767,
            "end": 990,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_7@0",
            "content": "In this paper we propose QCPG, a Quality Controlled Paraphrase Generation model, that given an input sentence and quality constraints, represented by a three dimensional vector of semantic similarity, and syntactic and lexical distances, produces a target sentence that conforms to the quality constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_7",
            "start": 0,
            "end": 305,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_8@0",
            "content": "Our constraints are much simpler than previously suggested ones, such as parse trees or keyword lists, and leave the model the freedom to choose how to attain the desired quality levels.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_8",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_9@0",
            "content": "Enabling the direct control of the three quality dimensions, allows flexibility with respect to the specific requirements of the task at hand, and opens a range of generation possibilities: paraphrases of various flavors (e.g. syntactically vs. lexically diverse), quasi-paraphrases (with lower semantic similarity), and even non-paraphrases which may be useful for downstream tasks (e.g. hard negative examples of sentences that are linguistically similar but have different meanings (Guo et al., 2018;Reimers and Gurevych, 2020)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_9",
            "start": 0,
            "end": 531,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_10@0",
            "content": "Our results show that the QCPG model indeed enables controlling paraphrase quality along the three quality dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_10",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_11@0",
            "content": "Furthermore, even though the training data is of mixed quality, and exhibits scarcity in the high quality area (see Figure 1), our model is able to learn high quality paraphrasing behavior, i.e. it increases the linguistic diversity of the generated paraphrases without decreasing the semantic similarity compared to the uncontrolled baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_11",
            "start": 0,
            "end": 342,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_12@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_12",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_13@0",
            "content": "In this section we provide a general description of our approach.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_13",
            "start": 0,
            "end": 64,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_13@1",
            "content": "We first explain how the different quality dimensions are measured.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_13",
            "start": 66,
            "end": 132,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_13@2",
            "content": "We then describe the controlled paraphrase generation model, QCPG, and finally we suggest a method that given the task requirements, detects the input control values which maximize the quality of the generated paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_13",
            "start": 134,
            "end": 355,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_13@3",
            "content": "Figure 2 summarizes our proposed solution for generating controlled paraphrases, which is detailed in the rest of the section.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_13",
            "start": 357,
            "end": 482,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_14@0",
            "content": "Quantifying Paraphrase Quality",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_14",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@0",
            "content": "The most common dimensions for measuring paraphrase quality are the semantic, syntactic and lexical dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@1",
            "content": "Several previous works used also a fluency evaluation metric (Siddique et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 112,
            "end": 196,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@2",
            "content": "However, since our focus is on the supervised setting, we rely on the gold paraphrases as fluency guidance for the model (Mc-Carthy et al., 2009).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 198,
            "end": 343,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@3",
            "content": "Thus, given a sentence s and a paraphrase s \u2032 , we define the paraphrase quality as a three dimensional vector q(s, s \u2032 ) = (q sem (s, s \u2032 ), q syn (s, s \u2032 ), q lex (s, s \u2032 )), where q sem is a measure of semantic similarity, and q syn and q lex are measures of syntactic and lexical variation, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 345,
            "end": 652,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@4",
            "content": "For the syntactic score, inspired by Iyyer et al. (2018) we choose q syn (s, s \u2032 ) to be the normalized tree edit distance (Zhang and Shasha, 1989) between the third level constituency parsetrees of s and s \u2032 , after removing the tokens -to increase the decoupling from the lexical distance metric.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 654,
            "end": 951,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@5",
            "content": "We define the lexical score q lex (s, s \u2032 ) to be the normalized character-level minimal edit distance between the bag of words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 953,
            "end": 1080,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@6",
            "content": "This measure is independent of word order, and hence increases the decoupling from syntactic measures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 1082,
            "end": 1183,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@7",
            "content": "Additionally, calculating the token distances on the character level enables to capture tokens that share the same stem/lemma.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 1185,
            "end": 1310,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@8",
            "content": "Character-level distance is also more robust to typos that may be found in noisy data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 1312,
            "end": 1397,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@9",
            "content": "As for the semantic score, several strong metrics have been recently proposed for measuring semantic similarity between sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 1399,
            "end": 1528,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@10",
            "content": "In order to select q sem (s, s \u2032 ), we studied the agreement between the candidate metrics and human judgments, using only development data, and found Bleurt (Sellam et al., 2020) to have the highest correlation with human judgments (see Appendix A).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 1530,
            "end": 1779,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@11",
            "content": "Thus, we define q sem (s, s \u2032 ) to be the Bleurt score, normalized using the sigmoid function to ensure a uniform range of values, [0, 1], for all three quality dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 1781,
            "end": 1952,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_15@12",
            "content": "For ease of presentation all metrics are presented on a 0 \u2212 100 scale.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_15",
            "start": 1954,
            "end": 2023,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_16@0",
            "content": "The QCPG Model",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_16",
            "start": 0,
            "end": 13,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_17@0",
            "content": "The main component of our solution is a quality controlled paraphrase generation model (QCPG), which is an encoder-decoder model trained on the task of controlled paraphrase generation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_17",
            "start": 0,
            "end": 184,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_17@1",
            "content": "Given an input sentence s and a control vector c = (c sem , c syn , c lex ), the goal of QCPG is to generate an output paraphrase QCP G(s, c) that conforms to c. We train QCPG using the training set pairs (s, t), by setting c to be q(s, t), and maximizing P (t|s, c = q(s, t)) over the training set via the autoregressive cross entropy loss.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_17",
            "start": 186,
            "end": 526,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_18@0",
            "content": "Control Values Selection",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_18",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_19@0",
            "content": "A major challenge in the research of controlled paraphrase generation, is selecting appropriate input control values that can be achieved by the model (Goyal and Durrett, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_19",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_19@1",
            "content": "Clearly, given a sentence, not all paraphrase qualities are achievable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_19",
            "start": 178,
            "end": 248,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_19@2",
            "content": "Some sentences are more amenable to paraphrasing than others.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_19",
            "start": 250,
            "end": 310,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_19@3",
            "content": "For example, named entities and numbers are much harder to be replaced while keeping sentence meaning, and hence, the potential lexical diversity of paraphrases involving such terms is relatively limited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_19",
            "start": 312,
            "end": 515,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_19@4",
            "content": "Forcing QCPG to conform to quality control values that are too high with respect to the input sentence, may lead to suboptimal quality of the resultant paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_19",
            "start": 517,
            "end": 680,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_19@5",
            "content": "Thus, for a more effective use of QCPG, the control values should be determined with respect to the input sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_19",
            "start": 682,
            "end": 796,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_20@0",
            "content": "Below we describe the second part of our solution, namely a method that given a sentence, predicts the input control values, c(s), that optimize the expected quality of the paraphrases generated by QCPG.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_20",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_20@1",
            "content": "For simplicity we assume that the quality distribution p(q|s) of all paraphrases of sentence s, is approximately normally distributed around a sentence dependent mean q 0 (s), and that the variance is approximately sentenceindependent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_20",
            "start": 204,
            "end": 438,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_20@2",
            "content": "We further assume that given an input sentence s, the difficulty to generate a paraphrase of a given quality, q, is dominated by p(q|s) rather than by the quality vector q itself.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_20",
            "start": 440,
            "end": 618,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_20@3",
            "content": "Following our assumptions, the level of difficulty can be expressed by the offset, o = (o sem , o syn , o lex ) of q from q 0 (s).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_20",
            "start": 620,
            "end": 749,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_20@4",
            "content": "Thus, the input control, c(s), for QCPG, is the sum of q 0 (s) and an offset o.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_20",
            "start": 751,
            "end": 829,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_21@0",
            "content": "Our aim is to analyze the model results for varying levels of difficulty, namely under different offsets, o, from q 0 (s).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_21",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_22@0",
            "content": "The Quality Predictor (QP): Since q 0 (s) is unknown, we introduce QP, a regressor whose output, termed the reference of s, r(s) = (r sem (s), r syn (s), r lex (s)), approximates q 0 (s).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_22",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_23@0",
            "content": "During training, QP aims to predict q(s, t) given s, where (s, t) are the input-output pairs of the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_23",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_24@0",
            "content": "To summarize, we define sentence-aware quality control by decomposing the QCPG input control, c, into a sum of a sentence dependent reference point, r(s), and a sentence independent offset, o.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_24",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_25@0",
            "content": "Data and Implementation Details 3.1 Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_25",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_26@0",
            "content": "To test the ability of our model to learn high quality behavior from mixed quality data we use weakly annotated datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_26",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_26@1",
            "content": "These datasets are large but noisy, and contain only a relatively small amount of high quality paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_26",
            "start": 122,
            "end": 228,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_27@0",
            "content": "MSCOCO: This dataset consists of 123K images, where each image contains at most five human-labeled captions (Lin et al., 2014).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_27",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_27@1",
            "content": "Similar to previous works we consider different captions of the same image as paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_27",
            "start": 128,
            "end": 217,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_28@0",
            "content": "WikiAnswers (WikiAns for short): The WikiAnswers corpus contains clusters of questions tagged by wiki-answers.com users as similar.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_28",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_28@1",
            "content": "There are 30, 370, 994 clusters with 25 question in each on average.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_28",
            "start": 132,
            "end": 199,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_28@2",
            "content": "In total, the corpus contains over 70 million question pairs (Fader et al., 2014b).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_28",
            "start": 201,
            "end": 283,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_29@0",
            "content": "ParaBank2.0: A dataset containing clusters of sentential paraphrases, produced from a bilingual corpus using negative constraints, inference sampling, and clustering (Hu et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_29",
            "start": 0,
            "end": 183,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_29@1",
            "content": "The dataset is composed of avarage of 5 paraphrases in every cluster and close to 100 million pairs in total.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_29",
            "start": 185,
            "end": 293,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_30@0",
            "content": "To get comparable results across all datasets, we randomly sub-sampled ParaBank2.0 and WikiAns to the same size as MSCOCO, and split them to train, dev and test sets, of sizes 900K, 14K and 14K respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_30",
            "start": 0,
            "end": 206,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_30@1",
            "content": "We carefully made sure that there are no pairs from the same cluster in different splits of the data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_30",
            "start": 208,
            "end": 308,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_30@2",
            "content": "The full data splits will be published with our code.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_30",
            "start": 310,
            "end": 362,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_31@0",
            "content": "Implementation Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_31",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_32@0",
            "content": "All models are trained with batch size of 32 on 2 NVIDIA A100 GPUs for 6 epochs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_32",
            "start": 0,
            "end": 79,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_32@1",
            "content": "Full details as well as train and dev results can be found in Appendix C.1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_32",
            "start": 81,
            "end": 155,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_33@0",
            "content": "QCPG: We use the pre-trained T5-base (Raffel et al., 2020) as the encoder-decoder model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_33",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_33@1",
            "content": "The control input vector to QCPG is quantized at every dimension into 20 equally spaced values ranging from 0 to 100.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_33",
            "start": 89,
            "end": 205,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_33@2",
            "content": "Each value is assigned to a special saved-token.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_33",
            "start": 207,
            "end": 254,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_33@3",
            "content": "The three tokens corresponding to the quantized values of the control vector c, are concatenated to the head of the input sentence, and together used as input to the model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_33",
            "start": 256,
            "end": 427,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_33@4",
            "content": "r(s) and o are also quantized in a similar way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_33",
            "start": 429,
            "end": 475,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_34@0",
            "content": "QP: An Electra base model (Clark et al., 2020) finetuned with MSE loss to predict the typical quality values (see Section 2.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_34",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_35@0",
            "content": "Baseline Model (BL): A T5-base model finetuned on the training data.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_35",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_36@0",
            "content": "For all the models, we adopt the experimental setup used in (Devlin et al., 2019), i.e. we train the model with several learning rates and choose the one that achieves the highest dev set performance (see appendix C.1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_36",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_37@0",
            "content": "Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_37",
            "start": 0,
            "end": 6,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_38@0",
            "content": "Controlling the Quality Dimensions",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_38",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_39@0",
            "content": "The aim of the following analysis is to study the level of control achieved by QCPG.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_39",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_39@1",
            "content": "To this end, we measure the model response to changes in the input offsets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_39",
            "start": 85,
            "end": 159,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_39@2",
            "content": "We compute the expected difference in paraphrase quality, as a result of applying an input offset o compared to zero offset as a reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_39",
            "start": 161,
            "end": 299,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_39@3",
            "content": "More formally, we define the 3-dimensional responsiveness vector of QCPG at an offset o, Specifically, in the following analysis we are interested in studying the model response to each of the dimensions separately, i.e. how changing the input offset along a given quality dimension dim -the controlled dimension -while keeping the two other dimensions constant, affects the responsiveness in each of the three dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_39",
            "start": 301,
            "end": 722,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_39@4",
            "content": "A good control mechanism would imply that increasing the input offset in one dimension will result in a monotonically increasing responsiveness in that dimension, with relatively small responsiveness in the other two dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_39",
            "start": 724,
            "end": 951,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_40@0",
            "content": "R(o) as Q(o) \u2212 Q((0, 0, 0)), where Q(o)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_40",
            "start": 0,
            "end": 38,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_41@0",
            "content": "Figure 3 shows, for each of the three datasets, the responsiveness in the three quality dimensions, when changing the input offset along each of the three dimensions, while fixing the input offsets in the other two dimensions at 0.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_41",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_41@1",
            "content": "Examining the actual values of quality in the paraphrases of the dev sets, reveals that the standard deviation is different in each dimension.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_41",
            "start": 232,
            "end": 373,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_41@2",
            "content": "Hence, for clarity of presentation, we present the input offset values and the responsiveness in units of standard deviation as measured in the respective dimension and dev set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_41",
            "start": 375,
            "end": 551,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_42@0",
            "content": "For the range of offsets displayed in Figure 3, the responsiveness in the controlled dimension increases monotonically with the input offsets across all datasets and dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_42",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_42@1",
            "content": "As expected, the responsiveness in the uncontrolled dimensions does not zeros due to the inherent coupling between the dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_42",
            "start": 178,
            "end": 307,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_42@2",
            "content": "For example, many changes that increase syntactic diversity, also increase lexical diversity (e.g. a move from passive to active voice).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_42",
            "start": 309,
            "end": 444,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_42@3",
            "content": "Still, our control mechanism is able to increase the responsiveness in the controlled dimension with relative low responsiveness in the uncontrolled dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_42",
            "start": 446,
            "end": 605,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_42@4",
            "content": "Specifically, focusing on the relation between semantic similarity and expression diversity, the figure shows that there is a minor decrease in semantic similarity in response to an increase in lexical and syntactic diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_42",
            "start": 607,
            "end": 832,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_42@5",
            "content": "In the next section, we will show that this does not prevent our model from generating paraphrases that are not only more lexically and syntactically diverse, but also more semantically similar to the source sentences, compared to the paraphrases generated by the uncontrolled baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_42",
            "start": 834,
            "end": 1119,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_43@0",
            "content": "Figure 3 focused on small to moderate input offsets, i.e. offsets up to 2 stds from the reference point.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_43",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_43@1",
            "content": "However, as we speculated before, with increasing offsets, i.e. the more the requested control value deviates from the typical value, it becomes increasingly difficult to generate a paraphrase that conforms to the requested control value.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_43",
            "start": 105,
            "end": 342,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_43@2",
            "content": "Figure 4 depicts the responsiveness in the syntactic and lexical dimensions for a larger range of offset values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_43",
            "start": 344,
            "end": 455,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_43@3",
            "content": "For the semantic dimension, the typical values are too high to allow large positive offsets, which for most sentences result in exceeding the upper limit of the semantic score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_43",
            "start": 457,
            "end": 632,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_43@4",
            "content": "Indeed, as can be seen in Figure 4, when moving to high offset values, the responsiveness in the syntactic and lexical dimensions starts to decrease.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_43",
            "start": 634,
            "end": 782,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_43@5",
            "content": "This behavior is in line with our aforementioned hypothesis, and reflects the detrimental effect of feeding QCPG with input control values that are too far from the typical paraphrase qualities of the input sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_43",
            "start": 784,
            "end": 999,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_43@6",
            "content": "The nonmonotonic behavior of the responsiveness implies that the input offsets should be selected carefully in order to optimize the quality of the resultant paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_43",
            "start": 1001,
            "end": 1170,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_43@7",
            "content": "In Section 4.2 we suggest a method for identifying these optimal offsets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_43",
            "start": 1172,
            "end": 1244,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_44@0",
            "content": "Selecting Optimal Input Control Values",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_44",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_45@0",
            "content": "In this section, we suggest a method that given task requirements, selects the input offsets that are expected to yield the desired quality of paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_45",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_45@1",
            "content": "The idea is to compute the estimated expected quality, Q(o), for each input offset o, using the dev set as described in Section 4.1, and then search the 3D grid of input offsets to find the point for which Q(o) is best suited for the user's requirements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_45",
            "start": 156,
            "end": 409,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_45@2",
            "content": "We envision this analysis as a preliminary step in which the user chooses the input control parameters that best achieve his desired paraphrasing operation point, and then uses the chosen values at inference -which is why we use the dev set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_45",
            "start": 411,
            "end": 651,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_46@0",
            "content": "We study the behavior of Q(o) as a function of the 3D grid of offset points in the relevant range, i.e every o where o sem , o syn and o lex in 0, 5, 10...50.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_46",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_46@1",
            "content": "The QCPG results are compared to two reference points, which are invariant to o and are marked on the colorbars with black squares: 'Dataset' is the semantic-similarity/linguistic-diversity average value over the corresponding dev set paraphrases, and 'Baseline' is the average semanticsimilarity/linguistic-diversity of the uncontrolled baseline over the corresponding dev set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_46",
            "start": 159,
            "end": 536,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_46@2",
            "content": "Notice that the average diversity level achieved by the uncontrolled baseline is lower than that of the dev set mean, reflecting the difficulty of this model to generate diverse paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_46",
            "start": 538,
            "end": 727,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_46@3",
            "content": "QCPG on the other hand, with suitable input offset values, is able to generate paraphrases which are on average higher than the baseline both in their linguistic diversity and in their semantic similarity, and in fact even higher in many cases than the values of the ground truth paraphrases in the dev-set.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_46",
            "start": 729,
            "end": 1035,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_47@0",
            "content": "In general, the estimates of the expected quality achieved by QCPG at different input offsets, enable a user to generate paraphrases at different operation points, by manipulating the input offset control o to meet her desired quality values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_47",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_47@1",
            "content": "Consider for example a typical use case, of aiming to maximize linguistic diversity under a constraint on semantic similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_47",
            "start": 243,
            "end": 368,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_47@2",
            "content": "An example of such a case is an operation point, denoted by QCP G \u22c6 , which aims to exemplify the advantage of QCPG over the baseline, by maximizing linguistic diversity under the constraint that the semantic similarity is at least 5 points higher than the baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_47",
            "start": 370,
            "end": 635,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_47@3",
            "content": "The input offset values to obtain this operation point depend on the dataset, and can be found using heatmaps such as in Figure 5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_47",
            "start": 637,
            "end": 766,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_47@4",
            "content": "For WikiAns the input offset for the QCP G \u22c6 operation point values are (50, 35, 5) (entry marked by the black square).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_47",
            "start": 768,
            "end": 886,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_48@0",
            "content": "Quality Evaluation on the Test Set",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_48",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_49@0",
            "content": "In the previous section we saw, using estimates based on the dev sets, that there are many operation points which generate paraphrases with higher quality than those achieved by the uncontrolled baseline.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_49",
            "start": 0,
            "end": 203,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_49@1",
            "content": "We now turn to evaluate one such operation point, namely QCP G \u22c6 , using the source sentences of the test sets which were not used in the selection of the input offset values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_49",
            "start": 205,
            "end": 379,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_50@0",
            "content": "We use four quality measures to evaluate different aspects of generated paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_50",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_50@1",
            "content": "The three quality measures used in the control of QCPG (Section 2.1) and Self-BLEU (Zhu et al., 2018) as adapted in ; Liu et al. (2020a), which aims to measure the linguistic diversity in the generated paraphrases by penalizing copying from input sentences.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_50",
            "start": 85,
            "end": 341,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_50@2",
            "content": "As can be seen in Table 1, QCP G \u22c6 outperforms the baseline in all metrics across all datasets, as predicted using the dev-set heatmaps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_50",
            "start": 343,
            "end": 478,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_50@3",
            "content": "A clear advantage is obtained even for Self-BLEU, which was not part of the metrics used as input controls.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_50",
            "start": 480,
            "end": 586,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_50@4",
            "content": "Importantly, the quality of the paraphrases generated by our model is comparable to, or at times better than the quality of the paraphrases in the ground truth of the datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_50",
            "start": 588,
            "end": 763,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_50@5",
            "content": "Examples of paraphrases generated by QCP G \u22c6 compared to the ground truth paraphrases appear in Table 10.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_50",
            "start": 765,
            "end": 869,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_50@6",
            "content": "This is an important step towards the goal of obtaining paraphrases in the sparse area of high quality (recall the top right corner of Figure 1).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_50",
            "start": 871,
            "end": 1015,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_51@0",
            "content": "Additionally, we examined QCPG from another perspective: the effect of the quality guidance on the model's ability to predict the ground truth paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_51",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_51@1",
            "content": "Tables 5 and 6 show the BLEU scores (Papineni et al., 2002) obtained by QCPG and the uncontrolled baseline respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_51",
            "start": 156,
            "end": 275,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_51@2",
            "content": "The results verify that the input quality vectors induced by the target sentences are effectively utilized by QCPG to achieve better prediction performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_51",
            "start": 277,
            "end": 432,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_52@0",
            "content": "Human Evaluation While linguistic diversity can be automatically measured by reliable metrics such as Self-BLEU, measuring semantic similarity is more challenging.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_52",
            "start": 0,
            "end": 162,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_52@1",
            "content": "We therefore rely on automatic metrics for evaluating the lexical and syntactic diversity, but use human annotation for validating the semantic evaluation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_52",
            "start": 164,
            "end": 318,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_52@2",
            "content": "To this end, we selected a sample of 50 source sentences from each test set, and generated one paraphrase using the uncontrolled baseline and one using QCP G \u22c6 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_52",
            "start": 320,
            "end": 480,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_52@3",
            "content": "The annotators were shown the source sentence, along with the two generated paraphrases (randomly ordered), and were asked which of the two better preserves the semantic meaning of the source sentence (ties are also allowed).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_52",
            "start": 482,
            "end": 706,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_52@4",
            "content": "In total, 150 triplets were evaluated by 5 judges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_52",
            "start": 708,
            "end": 757,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_52@5",
            "content": "Table 2 demonstrates an advantage for QCP G \u22c6 in all datasets, with a large margin in MSCOCO and WikiAns.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_52",
            "start": 759,
            "end": 863,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_52@6",
            "content": "This advantage is statistically significant (p \u2212 value < 0.05) as obtained by applying the Wilcoxon signed-rank test to the difference between the number of annotators that voted for QCP G \u22c6 and those voted for the baseline, across all datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_52",
            "start": 865,
            "end": 1109,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_52@7",
            "content": "Thus, the human evaluation is in line with the results of the automatic semantic similarity measure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_52",
            "start": 1111,
            "end": 1210,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_52@8",
            "content": "We also verified, that the results of this sample, in terms of linguistic diversity, are very similar to those shown in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_52",
            "start": 1212,
            "end": 1339,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_53@0",
            "content": "For examples of paraphrases generated by QCP G \u22c6 see Table 10 in the Appendix.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_53",
            "start": 0,
            "end": 77,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_54@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_54",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_55@0",
            "content": "Many recent works on paraphrase generation have been focused on attempting to achieve high-quality paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_55",
            "start": 0,
            "end": 110,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_55@1",
            "content": "These works can be divided into supervised and unsupervised approaches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_55",
            "start": 112,
            "end": 182,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_56@0",
            "content": "Supervised Approaches To achieve diversity, some works focused on diverse decoding using heuristics such as Hamming distance or distinct n-grams to preserve diverse options during beam search (Vijayakumar et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_56",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_56@1",
            "content": "Other works generate multiple outputs by perturbing latent representations (Gupta et al., 2018;Park et al., 2019). or by using distinct generators (Qian et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_56",
            "start": 220,
            "end": 386,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_56@2",
            "content": "These methods achieve some diversity, but do not control generation in an interpretable manner.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_56",
            "start": 388,
            "end": 482,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_57@0",
            "content": "The works that are most similar to ours strive to gain diversity using controlled-paraphrase generation, by exposing control mechanisms that are manipulated to produce either lexically (Zeng et al., 2019;Thompson and Post, 2020) or syntactically Goyal and Durrett, 2020) diverse paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_57",
            "start": 0,
            "end": 290,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_57@1",
            "content": "One approach is to use an exemplar sentence for guiding the syntax of the generated paraphrase Bao et al., 2019;Hosking and Lapata, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_57",
            "start": 292,
            "end": 429,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_57@2",
            "content": "An alternative is to directly employ constituency tree as the syntax guidance (Iyyer et al., 2018;Li and Choi, 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_57",
            "start": 431,
            "end": 547,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_57@3",
            "content": "Goyal and Durrett (2020) promote syntactic diversity by conditioning over possible syntactic rearrangements of the input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_57",
            "start": 549,
            "end": 669,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_57@4",
            "content": "Zeng et al. ( 2019) use keywords as lexical guidance for the generation process.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_57",
            "start": 671,
            "end": 750,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_57@5",
            "content": "Here we introduce a simple model for jointly controlling the lexical, syntactic and semantic aspects of the generated paraphrases.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_57",
            "start": 752,
            "end": 881,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_58@0",
            "content": "Unsupervised Approaches Niu et al. (2020) rely on neural models to generate high quality paraphrases, using a decoding method that enforces diversity by preventing repetitive copying of the input tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_58",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_58@1",
            "content": "Liu et al. (2020b) optimize a quality oriented objective by casting paraphrase generation as an optimization problem, and searching the sentence space to find the optimal point.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_58",
            "start": 204,
            "end": 380,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_58@2",
            "content": "Garg et al. (2021) and Siddique et al. (2020) use reinforcement learning with quality-oriented reward combining textual entailment, semantic similarity, expression diversity and fluency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_58",
            "start": 382,
            "end": 567,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_58@3",
            "content": "In this work, we employ similar metrics for guiding the generation of paraphrases within the supervised framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_58",
            "start": 569,
            "end": 682,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_59@0",
            "content": "Discussion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_59",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_60@0",
            "content": "In this paper, we propose a novel controlled paraphrase generation model, that leverages measures of paraphrase quality for encouraging the generation of paraphrases with desired quality.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_60",
            "start": 0,
            "end": 186,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_60@1",
            "content": "We demonstrate the high level of control achieved by the model, and suggest a method for coping with the challenging problem of finding suitable control values.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_60",
            "start": 188,
            "end": 347,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_61@0",
            "content": "Aside from offering a simple and effective way for controlling models' output quality, the quality control paradigm enables a holistic view of the data, the training process and the final model analysis.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_61",
            "start": 0,
            "end": 202,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_61@1",
            "content": "Namely: (I) Examination of the training data through the lens of data quality enables to characterize the data at hand, its strengths and limitations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_61",
            "start": 204,
            "end": 353,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_61@2",
            "content": "(II) A quality-aware training process can be viewed as multi-task learning, where each quality level is a separate task with its own accurate supervision, as opposed to the standard quality-agnostic approach, where low quality data is in fact used as a poor supervision for a model which aims at generating higher quality output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_61",
            "start": 355,
            "end": 683,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_61@3",
            "content": "(III) Analyzing the model behavior under different quality controls, allows finer understanding of the different model behaviors and the trade-offs between their output qualities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_61",
            "start": 685,
            "end": 863,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_61@4",
            "content": "Better understanding the expected output quality of neural NLG models, for different input quality controls, can increase the trust in their output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_61",
            "start": 865,
            "end": 1012,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_62@0",
            "content": "Finally, our model analysis consistently shows that although the models generally follow the quality requirements, there is still room for improvement.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_62",
            "start": 0,
            "end": 150,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_62@1",
            "content": "A possible direction for future research is exploring methods, such as reinforcement learning, for further improving the ability of the model to satisfy the quality requirements.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_62",
            "start": 152,
            "end": 329,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@0",
            "content": "Recently, several strong metrics have been proposed for measuring semantic similarity between sentences (Reimers and Gurevych, 2019;?;Sellam et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 0,
            "end": 154,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@1",
            "content": "In order to select the semantic similarity metric for QCPG, we performed a small experiment over the three dev sets, with the aim of measuring the agreement of the candidate metrics with human judgments.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 156,
            "end": 358,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@2",
            "content": "To this end, we leveraged two properties that characterize weakly labeled datasets, the underlying clusters of sentences, and the high variability of semantic similarity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 360,
            "end": 529,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@3",
            "content": "Given a dataset, we randomly selected 100 clusters, and picked three sentences at random from each cluster.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 531,
            "end": 637,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@4",
            "content": "For each triplet of sentences t = (t 1 , t 2 , t 3 ) we asked 5 human annotators to choose which of the two sentences, t 2 or t 3 , better preserves the semantic meaning of t 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 639,
            "end": 816,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@5",
            "content": "In order to find the candidate similarity measure with the highest agreement with human judgments, we first computed, for each triplet, the difference between the number of annotators voted for t 2 and those voted for t 3 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 818,
            "end": 1040,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@6",
            "content": "We then computed for each candidate measure, the difference between the similarity of t 2 to t 1 and and of t 3 to t 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 1042,
            "end": 1161,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@7",
            "content": "We then measured Kendall's Tau correlation (Daniel, 1990) between the difference vector of the human judgments and that of the judgments of each of the candidate measures.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 1163,
            "end": 1333,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@8",
            "content": "Table 3 shows the resultant correlations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 1335,
            "end": 1375,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@9",
            "content": "The highest correlations are obtained for SBERT (Reimers and Gurevych, 2019), but since it was trained on WikiAns and MSCOCO, we could not use it in our study.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 1377,
            "end": 1535,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@10",
            "content": "We selected Bleurt due to its highest correlation with human judgments over the three datasets (among the methods that were not exposed to the considered datasets).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 1537,
            "end": 1700,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_63@11",
            "content": "We normalize Bleurt score using the sigmoid function to ensure a uniform range of values, [0, 1], for the three quality dimensions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_63",
            "start": 1702,
            "end": 1832,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_64@0",
            "content": "We study the coupling between the different semantic similarity measures and the linguistic diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_64",
            "start": 0,
            "end": 101,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_65@0",
            "content": "We assume that the level of coupling of a good similarity measure will resemble that of humans, and will be less sensitive to lexical and syntactic properties of the paraphrase.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_65",
            "start": 0,
            "end": 176,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_65@1",
            "content": "Table 4 presents the Kendall tau correlation between the different similarity measures and the linguistic diversity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_65",
            "start": 178,
            "end": 293,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_65@2",
            "content": "Results for human judgments are also shown for a reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_65",
            "start": 295,
            "end": 353,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_65@3",
            "content": "The correlation calculation is performed between the vectors of differences as described in section A).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_65",
            "start": 355,
            "end": 457,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_66@0",
            "content": "The results show that Bleurt demonstrates the lowest coupling with linguistic diversity among the automatic measures (aside from SBERT which, as mentioned before, was trained with MSCOCO and WikiAns).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_66",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_66@1",
            "content": "The comparison to human judgments shows that Bleurt is more influenced by linguistic features, indicating that automatic measures need to be further improved to reach the decoupling level achieved by humans.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_66",
            "start": 201,
            "end": 407,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_67@0",
            "content": "The learning rates for the QCPG and the Baseline models were selected in the following way.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_67",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_67@1",
            "content": "For a given dataset, we finetuned the models with 4 learning rates (1e-3, 1e-4, 5e-3, 5e-4) (The training results of the baseline presented in Table 5 and the results of QCPG presented in Table 6.).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_67",
            "start": 92,
            "end": 289,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_67@2",
            "content": "For the baseline we selected the one which yielded the best BLEU score (Papineni et al., 2002) on the corresponding dev set The best learning rate for every dataset was chosen based on the Dev set BLEU score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_67",
            "start": 291,
            "end": 498,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_67@3",
            "content": "For the QCPG we chose the model that best conforms to the control input as measured by the MSE between the input control vector and the output quality vector (see Table 9).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_67",
            "start": 500,
            "end": 671,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_67@4",
            "content": "The QP model is an Electra-Base model finetuned with 4 different learning rates (1.5e-4, 1e-4, 3e-5, 5e-5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_67",
            "start": 673,
            "end": 779,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_68@0",
            "content": "We choose the learning rate the yields the minimal MSE on the dev set (For full results see Table 8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_68",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_69@0",
            "content": "The full heatmaps can be found in Figure 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_69",
            "start": 0,
            "end": 42,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_70@0",
            "content": "Yu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou, Olga Vechtomova, Xin-Yu Dai, Jiajun Chen, Generating sentences from disentangled syntactic and semantic spaces, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_70",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_71@0",
            "content": "Rahul Bhagat, Eduard Hovy, Squibs: What is a paraphrase?, 2013, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_71",
            "start": 0,
            "end": 91,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_72@0",
            "content": "Mingda Chen, Qingming Tang, Sam Wiseman, Kevin Gimpel, Controllable paraphrase generation with a syntactic exemplar, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_72",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_73@0",
            "content": "UNKNOWN, None, 2020, Electra: Pre-training text encoders as discriminators rather than generators, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_73",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_74@0",
            "content": "UNKNOWN, None, 1990, Applied nonparametric statistics pws, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_74",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_75@0",
            "content": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, BERT: Pre-training of deep bidirectional transformers for language understanding, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_75",
            "start": 0,
            "end": 335,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_76@0",
            "content": "Anthony Fader, Luke Zettlemoyer, Oren Etzioni, Open question answering over curated and extracted knowledge bases, 2014, Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_76",
            "start": 0,
            "end": 221,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_77@0",
            "content": "Anthony Fader, Luke Zettlemoyer, Oren Etzioni, Open Question Answering Over Curated and Extracted Knowledge Bases, 2014, KDD, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_77",
            "start": 0,
            "end": 126,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_78@0",
            "content": "UNKNOWN, None, 2021, Unsupervised contextual paraphrase generation using lexical control and reinforcement learning, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_78",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_79@0",
            "content": "Tanya Goyal, Greg Durrett, Neural syntactic preordering for controlled paraphrase generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_79",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_80@0",
            "content": "Mandy Guo, Qinlan Shen, Yinfei Yang, Heming Ge, Daniel Cer, Gustavo Abrego, Keith Stevens, Noah Constant, Yun-Hsuan Sung, Brian Strope, Ray Kurzweil, Effective parallel corpus mining using bilingual sentence embeddings, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_80",
            "start": 0,
            "end": 303,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_81@0",
            "content": "Ankush Gupta, Arvind Agarwal, Prawaan Singh, Piyush Rai, A deep generative framework for paraphrase generation, 2018, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_81",
            "start": 0,
            "end": 181,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_82@0",
            "content": "Tom Hosking, Mirella Lapata, Factorising meaning and form for intent-preserving paraphrasing, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_82",
            "start": 0,
            "end": 313,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_83@0",
            "content": "J Hu, Abhinav Singh, Nils Holzenberger, Matt Post, Benjamin Van Durme, Large-scale, diverse, paraphrastic bitexts via sampling and clustering, 2019, Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_83",
            "start": 0,
            "end": 277,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_84@0",
            "content": "Mohit Iyyer, John Wieting, Kevin Gimpel, Luke Zettlemoyer, Adversarial example generation with syntactically controlled paraphrase networks, 2018, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_84",
            "start": 0,
            "end": 302,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_85@0",
            "content": "Changmao Li, Jinho Choi, Transformers to learn hierarchical contexts in multiparty dialogue for span-based question answering, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_85",
            "start": 0,
            "end": 271,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_86@0",
            "content": "Zichao Li, Xin Jiang, Lifeng Shang, Qun Liu, Decomposable neural paraphrase generation, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_86",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_87@0",
            "content": "Tsung-Yi Lin, M Maire, Serge Belongie, James Hays, P Perona, D Ramanan, Piotr Doll\u00e1r, C , Microsoft coco: Common objects in context, 2014, ECCV, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_87",
            "start": 0,
            "end": 145,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_88@0",
            "content": "Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song, Unsupervised paraphrasing by simulated annealing, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_88",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_89@0",
            "content": "Xianggen Liu, Lili Mou, Fandong Meng, Hao Zhou, Jie Zhou, Sen Song, Unsupervised paraphrasing by simulated annealing, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_89",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_90@0",
            "content": "UNKNOWN, None, 2018, The natural language decathlon: Multitask learning as question answering, CoRR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_90",
            "start": 0,
            "end": 99,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_91@0",
            "content": "Philip Mccarthy, Rebekah Guess, D Mcnamara, The components of paraphrase evaluations, 2009, Behavior Research Methods, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_91",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_92@0",
            "content": "UNKNOWN, None, , Nitish Shirish Keskar, and Caiming Xiong. 2020. Unsupervised paraphrase generation via dynamic blocking, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_92",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_93@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_93",
            "start": 0,
            "end": 257,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_94@0",
            "content": "Sunghyun Park,  Seung-Won, Fuxiang Hwang, Jaegul Chen, Jung-Woo Choo, Sunghun Ha, Jinyeong Kim,  Yim, Paraphrase diversification using counterfactual debiasing, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_94",
            "start": 0,
            "end": 230,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_95@0",
            "content": "Lihua Qian, Lin Qiu, Weinan Zhang, Xin Jiang, Yong Yu, Exploring diverse expressions for paraphrase generation, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_95",
            "start": 0,
            "end": 336,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_96@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, Journal of Machine Learning Research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_96",
            "start": 0,
            "end": 246,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_97@0",
            "content": "Nils Reimers, Iryna Gurevych, Sentence-BERT: Sentence embeddings using Siamese BERTnetworks, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_97",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_98@0",
            "content": "Nils Reimers, Iryna Gurevych, Making monolingual sentence embeddings multilingual using knowledge distillation, 2020, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_98",
            "start": 0,
            "end": 263,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_99@0",
            "content": "Alexander Rush, Sumit Chopra, Jason Weston, A neural attention model for abstractive sentence summarization, 2015, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_99",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "163-ARR_v2_100@0",
            "content": "Thibault Sellam, Dipanjan Das, Ankur Parikh, BLEURT: Learning robust metrics for text generation, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "163-ARR_v2_100",
            "start": 0,
            "end": 242,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "163-ARR_v2_0",
            "tgt_ix": "163-ARR_v2_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_0",
            "tgt_ix": "163-ARR_v2_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_1",
            "tgt_ix": "163-ARR_v2_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_1",
            "tgt_ix": "163-ARR_v2_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_0",
            "tgt_ix": "163-ARR_v2_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_2",
            "tgt_ix": "163-ARR_v2_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_4",
            "tgt_ix": "163-ARR_v2_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_5",
            "tgt_ix": "163-ARR_v2_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_6",
            "tgt_ix": "163-ARR_v2_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_7",
            "tgt_ix": "163-ARR_v2_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_8",
            "tgt_ix": "163-ARR_v2_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_9",
            "tgt_ix": "163-ARR_v2_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_10",
            "tgt_ix": "163-ARR_v2_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_3",
            "tgt_ix": "163-ARR_v2_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_3",
            "tgt_ix": "163-ARR_v2_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_3",
            "tgt_ix": "163-ARR_v2_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_3",
            "tgt_ix": "163-ARR_v2_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_3",
            "tgt_ix": "163-ARR_v2_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_3",
            "tgt_ix": "163-ARR_v2_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_3",
            "tgt_ix": "163-ARR_v2_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_3",
            "tgt_ix": "163-ARR_v2_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_3",
            "tgt_ix": "163-ARR_v2_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_0",
            "tgt_ix": "163-ARR_v2_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_11",
            "tgt_ix": "163-ARR_v2_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_12",
            "tgt_ix": "163-ARR_v2_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_12",
            "tgt_ix": "163-ARR_v2_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_12",
            "tgt_ix": "163-ARR_v2_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_13",
            "tgt_ix": "163-ARR_v2_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_14",
            "tgt_ix": "163-ARR_v2_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_14",
            "tgt_ix": "163-ARR_v2_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_12",
            "tgt_ix": "163-ARR_v2_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_16",
            "tgt_ix": "163-ARR_v2_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_16",
            "tgt_ix": "163-ARR_v2_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_12",
            "tgt_ix": "163-ARR_v2_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_17",
            "tgt_ix": "163-ARR_v2_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_19",
            "tgt_ix": "163-ARR_v2_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_20",
            "tgt_ix": "163-ARR_v2_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_21",
            "tgt_ix": "163-ARR_v2_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_22",
            "tgt_ix": "163-ARR_v2_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_23",
            "tgt_ix": "163-ARR_v2_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_18",
            "tgt_ix": "163-ARR_v2_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_18",
            "tgt_ix": "163-ARR_v2_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_18",
            "tgt_ix": "163-ARR_v2_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_18",
            "tgt_ix": "163-ARR_v2_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_18",
            "tgt_ix": "163-ARR_v2_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_18",
            "tgt_ix": "163-ARR_v2_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_18",
            "tgt_ix": "163-ARR_v2_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_0",
            "tgt_ix": "163-ARR_v2_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_24",
            "tgt_ix": "163-ARR_v2_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_26",
            "tgt_ix": "163-ARR_v2_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_27",
            "tgt_ix": "163-ARR_v2_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_28",
            "tgt_ix": "163-ARR_v2_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_29",
            "tgt_ix": "163-ARR_v2_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_25",
            "tgt_ix": "163-ARR_v2_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_25",
            "tgt_ix": "163-ARR_v2_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_25",
            "tgt_ix": "163-ARR_v2_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_25",
            "tgt_ix": "163-ARR_v2_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_25",
            "tgt_ix": "163-ARR_v2_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_25",
            "tgt_ix": "163-ARR_v2_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_25",
            "tgt_ix": "163-ARR_v2_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_30",
            "tgt_ix": "163-ARR_v2_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_32",
            "tgt_ix": "163-ARR_v2_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_33",
            "tgt_ix": "163-ARR_v2_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_34",
            "tgt_ix": "163-ARR_v2_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_35",
            "tgt_ix": "163-ARR_v2_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_31",
            "tgt_ix": "163-ARR_v2_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_31",
            "tgt_ix": "163-ARR_v2_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_31",
            "tgt_ix": "163-ARR_v2_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_31",
            "tgt_ix": "163-ARR_v2_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_31",
            "tgt_ix": "163-ARR_v2_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_31",
            "tgt_ix": "163-ARR_v2_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_0",
            "tgt_ix": "163-ARR_v2_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_36",
            "tgt_ix": "163-ARR_v2_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_37",
            "tgt_ix": "163-ARR_v2_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_37",
            "tgt_ix": "163-ARR_v2_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_39",
            "tgt_ix": "163-ARR_v2_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_40",
            "tgt_ix": "163-ARR_v2_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_41",
            "tgt_ix": "163-ARR_v2_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_42",
            "tgt_ix": "163-ARR_v2_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_38",
            "tgt_ix": "163-ARR_v2_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_38",
            "tgt_ix": "163-ARR_v2_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_38",
            "tgt_ix": "163-ARR_v2_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_38",
            "tgt_ix": "163-ARR_v2_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_38",
            "tgt_ix": "163-ARR_v2_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_38",
            "tgt_ix": "163-ARR_v2_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_37",
            "tgt_ix": "163-ARR_v2_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_43",
            "tgt_ix": "163-ARR_v2_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_45",
            "tgt_ix": "163-ARR_v2_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_46",
            "tgt_ix": "163-ARR_v2_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_44",
            "tgt_ix": "163-ARR_v2_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_44",
            "tgt_ix": "163-ARR_v2_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_44",
            "tgt_ix": "163-ARR_v2_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_44",
            "tgt_ix": "163-ARR_v2_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_37",
            "tgt_ix": "163-ARR_v2_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_47",
            "tgt_ix": "163-ARR_v2_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_48",
            "tgt_ix": "163-ARR_v2_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_48",
            "tgt_ix": "163-ARR_v2_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_50",
            "tgt_ix": "163-ARR_v2_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_52",
            "tgt_ix": "163-ARR_v2_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_48",
            "tgt_ix": "163-ARR_v2_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_48",
            "tgt_ix": "163-ARR_v2_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_48",
            "tgt_ix": "163-ARR_v2_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_48",
            "tgt_ix": "163-ARR_v2_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_49",
            "tgt_ix": "163-ARR_v2_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_0",
            "tgt_ix": "163-ARR_v2_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_53",
            "tgt_ix": "163-ARR_v2_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_55",
            "tgt_ix": "163-ARR_v2_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_56",
            "tgt_ix": "163-ARR_v2_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_57",
            "tgt_ix": "163-ARR_v2_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_54",
            "tgt_ix": "163-ARR_v2_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_54",
            "tgt_ix": "163-ARR_v2_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_54",
            "tgt_ix": "163-ARR_v2_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_54",
            "tgt_ix": "163-ARR_v2_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_54",
            "tgt_ix": "163-ARR_v2_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_0",
            "tgt_ix": "163-ARR_v2_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_58",
            "tgt_ix": "163-ARR_v2_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_60",
            "tgt_ix": "163-ARR_v2_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_61",
            "tgt_ix": "163-ARR_v2_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_62",
            "tgt_ix": "163-ARR_v2_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_64",
            "tgt_ix": "163-ARR_v2_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_65",
            "tgt_ix": "163-ARR_v2_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_67",
            "tgt_ix": "163-ARR_v2_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_66",
            "tgt_ix": "163-ARR_v2_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_68",
            "tgt_ix": "163-ARR_v2_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "163-ARR_v2_0",
            "tgt_ix": "163-ARR_v2_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_1",
            "tgt_ix": "163-ARR_v2_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_2",
            "tgt_ix": "163-ARR_v2_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_2",
            "tgt_ix": "163-ARR_v2_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_2",
            "tgt_ix": "163-ARR_v2_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_2",
            "tgt_ix": "163-ARR_v2_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_2",
            "tgt_ix": "163-ARR_v2_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_2",
            "tgt_ix": "163-ARR_v2_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_2",
            "tgt_ix": "163-ARR_v2_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_2",
            "tgt_ix": "163-ARR_v2_2@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_2",
            "tgt_ix": "163-ARR_v2_2@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_3",
            "tgt_ix": "163-ARR_v2_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_4",
            "tgt_ix": "163-ARR_v2_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_4",
            "tgt_ix": "163-ARR_v2_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_4",
            "tgt_ix": "163-ARR_v2_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_4",
            "tgt_ix": "163-ARR_v2_4@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_5",
            "tgt_ix": "163-ARR_v2_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_5",
            "tgt_ix": "163-ARR_v2_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_5",
            "tgt_ix": "163-ARR_v2_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_6",
            "tgt_ix": "163-ARR_v2_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_6",
            "tgt_ix": "163-ARR_v2_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_6",
            "tgt_ix": "163-ARR_v2_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_6",
            "tgt_ix": "163-ARR_v2_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_6",
            "tgt_ix": "163-ARR_v2_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_7",
            "tgt_ix": "163-ARR_v2_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_8",
            "tgt_ix": "163-ARR_v2_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_9",
            "tgt_ix": "163-ARR_v2_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_10",
            "tgt_ix": "163-ARR_v2_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_11",
            "tgt_ix": "163-ARR_v2_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_12",
            "tgt_ix": "163-ARR_v2_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_13",
            "tgt_ix": "163-ARR_v2_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_13",
            "tgt_ix": "163-ARR_v2_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_13",
            "tgt_ix": "163-ARR_v2_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_13",
            "tgt_ix": "163-ARR_v2_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_14",
            "tgt_ix": "163-ARR_v2_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_15",
            "tgt_ix": "163-ARR_v2_15@12",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_16",
            "tgt_ix": "163-ARR_v2_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_17",
            "tgt_ix": "163-ARR_v2_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_17",
            "tgt_ix": "163-ARR_v2_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_18",
            "tgt_ix": "163-ARR_v2_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_19",
            "tgt_ix": "163-ARR_v2_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_19",
            "tgt_ix": "163-ARR_v2_19@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_19",
            "tgt_ix": "163-ARR_v2_19@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_19",
            "tgt_ix": "163-ARR_v2_19@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_19",
            "tgt_ix": "163-ARR_v2_19@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_19",
            "tgt_ix": "163-ARR_v2_19@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_20",
            "tgt_ix": "163-ARR_v2_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_20",
            "tgt_ix": "163-ARR_v2_20@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_20",
            "tgt_ix": "163-ARR_v2_20@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_20",
            "tgt_ix": "163-ARR_v2_20@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_20",
            "tgt_ix": "163-ARR_v2_20@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_21",
            "tgt_ix": "163-ARR_v2_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_22",
            "tgt_ix": "163-ARR_v2_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_23",
            "tgt_ix": "163-ARR_v2_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_24",
            "tgt_ix": "163-ARR_v2_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_25",
            "tgt_ix": "163-ARR_v2_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_26",
            "tgt_ix": "163-ARR_v2_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_26",
            "tgt_ix": "163-ARR_v2_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_27",
            "tgt_ix": "163-ARR_v2_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_27",
            "tgt_ix": "163-ARR_v2_27@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_28",
            "tgt_ix": "163-ARR_v2_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_28",
            "tgt_ix": "163-ARR_v2_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_28",
            "tgt_ix": "163-ARR_v2_28@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_29",
            "tgt_ix": "163-ARR_v2_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_29",
            "tgt_ix": "163-ARR_v2_29@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_30",
            "tgt_ix": "163-ARR_v2_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_30",
            "tgt_ix": "163-ARR_v2_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_30",
            "tgt_ix": "163-ARR_v2_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_31",
            "tgt_ix": "163-ARR_v2_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_32",
            "tgt_ix": "163-ARR_v2_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_32",
            "tgt_ix": "163-ARR_v2_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_33",
            "tgt_ix": "163-ARR_v2_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_33",
            "tgt_ix": "163-ARR_v2_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_33",
            "tgt_ix": "163-ARR_v2_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_33",
            "tgt_ix": "163-ARR_v2_33@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_33",
            "tgt_ix": "163-ARR_v2_33@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_34",
            "tgt_ix": "163-ARR_v2_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_35",
            "tgt_ix": "163-ARR_v2_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_36",
            "tgt_ix": "163-ARR_v2_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_37",
            "tgt_ix": "163-ARR_v2_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_38",
            "tgt_ix": "163-ARR_v2_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_39",
            "tgt_ix": "163-ARR_v2_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_39",
            "tgt_ix": "163-ARR_v2_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_39",
            "tgt_ix": "163-ARR_v2_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_39",
            "tgt_ix": "163-ARR_v2_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_39",
            "tgt_ix": "163-ARR_v2_39@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_40",
            "tgt_ix": "163-ARR_v2_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_41",
            "tgt_ix": "163-ARR_v2_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_41",
            "tgt_ix": "163-ARR_v2_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_41",
            "tgt_ix": "163-ARR_v2_41@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_42",
            "tgt_ix": "163-ARR_v2_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_42",
            "tgt_ix": "163-ARR_v2_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_42",
            "tgt_ix": "163-ARR_v2_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_42",
            "tgt_ix": "163-ARR_v2_42@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_42",
            "tgt_ix": "163-ARR_v2_42@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_42",
            "tgt_ix": "163-ARR_v2_42@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_43",
            "tgt_ix": "163-ARR_v2_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_43",
            "tgt_ix": "163-ARR_v2_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_43",
            "tgt_ix": "163-ARR_v2_43@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_43",
            "tgt_ix": "163-ARR_v2_43@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_43",
            "tgt_ix": "163-ARR_v2_43@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_43",
            "tgt_ix": "163-ARR_v2_43@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_43",
            "tgt_ix": "163-ARR_v2_43@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_43",
            "tgt_ix": "163-ARR_v2_43@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_44",
            "tgt_ix": "163-ARR_v2_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_45",
            "tgt_ix": "163-ARR_v2_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_45",
            "tgt_ix": "163-ARR_v2_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_45",
            "tgt_ix": "163-ARR_v2_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_46",
            "tgt_ix": "163-ARR_v2_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_46",
            "tgt_ix": "163-ARR_v2_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_46",
            "tgt_ix": "163-ARR_v2_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_46",
            "tgt_ix": "163-ARR_v2_46@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_47",
            "tgt_ix": "163-ARR_v2_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_47",
            "tgt_ix": "163-ARR_v2_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_47",
            "tgt_ix": "163-ARR_v2_47@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_47",
            "tgt_ix": "163-ARR_v2_47@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_47",
            "tgt_ix": "163-ARR_v2_47@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_48",
            "tgt_ix": "163-ARR_v2_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_49",
            "tgt_ix": "163-ARR_v2_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_49",
            "tgt_ix": "163-ARR_v2_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_50",
            "tgt_ix": "163-ARR_v2_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_50",
            "tgt_ix": "163-ARR_v2_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_50",
            "tgt_ix": "163-ARR_v2_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_50",
            "tgt_ix": "163-ARR_v2_50@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_50",
            "tgt_ix": "163-ARR_v2_50@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_50",
            "tgt_ix": "163-ARR_v2_50@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_50",
            "tgt_ix": "163-ARR_v2_50@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_51",
            "tgt_ix": "163-ARR_v2_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_51",
            "tgt_ix": "163-ARR_v2_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_51",
            "tgt_ix": "163-ARR_v2_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_52",
            "tgt_ix": "163-ARR_v2_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_52",
            "tgt_ix": "163-ARR_v2_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_52",
            "tgt_ix": "163-ARR_v2_52@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_52",
            "tgt_ix": "163-ARR_v2_52@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_52",
            "tgt_ix": "163-ARR_v2_52@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_52",
            "tgt_ix": "163-ARR_v2_52@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_52",
            "tgt_ix": "163-ARR_v2_52@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_52",
            "tgt_ix": "163-ARR_v2_52@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_52",
            "tgt_ix": "163-ARR_v2_52@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_53",
            "tgt_ix": "163-ARR_v2_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_54",
            "tgt_ix": "163-ARR_v2_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_55",
            "tgt_ix": "163-ARR_v2_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_55",
            "tgt_ix": "163-ARR_v2_55@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_56",
            "tgt_ix": "163-ARR_v2_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_56",
            "tgt_ix": "163-ARR_v2_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_56",
            "tgt_ix": "163-ARR_v2_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_57",
            "tgt_ix": "163-ARR_v2_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_57",
            "tgt_ix": "163-ARR_v2_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_57",
            "tgt_ix": "163-ARR_v2_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_57",
            "tgt_ix": "163-ARR_v2_57@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_57",
            "tgt_ix": "163-ARR_v2_57@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_57",
            "tgt_ix": "163-ARR_v2_57@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_58",
            "tgt_ix": "163-ARR_v2_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_58",
            "tgt_ix": "163-ARR_v2_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_58",
            "tgt_ix": "163-ARR_v2_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_58",
            "tgt_ix": "163-ARR_v2_58@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_59",
            "tgt_ix": "163-ARR_v2_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_60",
            "tgt_ix": "163-ARR_v2_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_60",
            "tgt_ix": "163-ARR_v2_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_61",
            "tgt_ix": "163-ARR_v2_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_61",
            "tgt_ix": "163-ARR_v2_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_61",
            "tgt_ix": "163-ARR_v2_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_61",
            "tgt_ix": "163-ARR_v2_61@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_61",
            "tgt_ix": "163-ARR_v2_61@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_62",
            "tgt_ix": "163-ARR_v2_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_62",
            "tgt_ix": "163-ARR_v2_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@10",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_63",
            "tgt_ix": "163-ARR_v2_63@11",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_64",
            "tgt_ix": "163-ARR_v2_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_65",
            "tgt_ix": "163-ARR_v2_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_65",
            "tgt_ix": "163-ARR_v2_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_65",
            "tgt_ix": "163-ARR_v2_65@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_65",
            "tgt_ix": "163-ARR_v2_65@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_66",
            "tgt_ix": "163-ARR_v2_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_66",
            "tgt_ix": "163-ARR_v2_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_67",
            "tgt_ix": "163-ARR_v2_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_67",
            "tgt_ix": "163-ARR_v2_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_67",
            "tgt_ix": "163-ARR_v2_67@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_67",
            "tgt_ix": "163-ARR_v2_67@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_67",
            "tgt_ix": "163-ARR_v2_67@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_68",
            "tgt_ix": "163-ARR_v2_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_69",
            "tgt_ix": "163-ARR_v2_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_70",
            "tgt_ix": "163-ARR_v2_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_71",
            "tgt_ix": "163-ARR_v2_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_72",
            "tgt_ix": "163-ARR_v2_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_73",
            "tgt_ix": "163-ARR_v2_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_74",
            "tgt_ix": "163-ARR_v2_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_75",
            "tgt_ix": "163-ARR_v2_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_76",
            "tgt_ix": "163-ARR_v2_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_77",
            "tgt_ix": "163-ARR_v2_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_78",
            "tgt_ix": "163-ARR_v2_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_79",
            "tgt_ix": "163-ARR_v2_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_80",
            "tgt_ix": "163-ARR_v2_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_81",
            "tgt_ix": "163-ARR_v2_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_82",
            "tgt_ix": "163-ARR_v2_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_83",
            "tgt_ix": "163-ARR_v2_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_84",
            "tgt_ix": "163-ARR_v2_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_85",
            "tgt_ix": "163-ARR_v2_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_86",
            "tgt_ix": "163-ARR_v2_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_87",
            "tgt_ix": "163-ARR_v2_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_88",
            "tgt_ix": "163-ARR_v2_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_89",
            "tgt_ix": "163-ARR_v2_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_90",
            "tgt_ix": "163-ARR_v2_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_91",
            "tgt_ix": "163-ARR_v2_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_92",
            "tgt_ix": "163-ARR_v2_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_93",
            "tgt_ix": "163-ARR_v2_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_94",
            "tgt_ix": "163-ARR_v2_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_95",
            "tgt_ix": "163-ARR_v2_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_96",
            "tgt_ix": "163-ARR_v2_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_97",
            "tgt_ix": "163-ARR_v2_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_98",
            "tgt_ix": "163-ARR_v2_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_99",
            "tgt_ix": "163-ARR_v2_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "163-ARR_v2_100",
            "tgt_ix": "163-ARR_v2_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 749,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "doc_id": "163-ARR",
        "version": 2
    }
}