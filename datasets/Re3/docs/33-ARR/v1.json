{
    "nodes": [
        {
            "ix": "33-ARR_v1_0",
            "content": "Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_2",
            "content": "Contrastive learning has achieved impressive success in generation tasks to militate the \"exposure bias\" problem and discriminatively exploit the different quality of references. Existing works mostly focus on contrastive learning on the instance-level without discriminating the contribution of each word, while keywords are the gist of the text and dominant the constrained mapping relationships. Hence, in this work, we propose a hierarchical contrastive learning mechanism, which can unify hybrid granularities semantic meaning in the input text. Concretely, we first propose a keyword graph via contrastive correlations of positive-negative pairs to iteratively polish the keyword representations. Then, we construct intra-contrasts within instance-level and keyword-level, where we assume words are sampled nodes from a sentence distribution. Finally, to bridge the gap between independent contrast levels and tackle the common contrast vanishing problem, we propose an inter-contrast mechanism that measures the discrepancy between contrastive keyword nodes respectively to the instance distribution. Experiments demonstrate that our model outperforms competitive baselines on paraphrasing, dialogue generation, and storytelling tasks.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "33-ARR_v1_4",
            "content": "Generation tasks such as storytelling, paraphrasing, and dialogue generation aim at learning a certain correlation between text pairs that maps an arbitrary-length input text to another arbitrarylength output text. Traditional methods are mostly trained with \"teacher forcing\" and lead to an \"exposure bias\" problem. Incorporating the generation method with contrastive learning achieved impressive performance on tackling such issues, which takes an extra consideration of synthetic negative samples contrastively (Lee et al., 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_5",
            "content": "Existing contrastive mechanisms are mainly focused on the instance level (Lee et al., 2021;Cai are the best books on cosmology?\" would be greatly changed if the keyword \"cosmology\" is changed to \"astrophysic\". et al., 2020). However, word-level information is also of great importance. Take the case shown in the upper part of Figure 1 for example, the keyword covers the gist of the input text and determines the embedding space of the text. The text representation will be significantly affected if adding a slight perturbation on the keyword, i.e., changing \"cosmology\" to \"astrophysics\". In addition, as shown on the bottom part, under some circumstances, it is too easy for the model to do the classification since the semantic gap between contrastive pairs is huge. Thus, the model fails to distinguish the actual discrepancy, which causes a \"contrast vanishing\" problem at both instance-level and keyword-level.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_6",
            "content": "Based on the above motivation, in this paper, we propose a hierarchical contrastive learning method built on top of the classic CVAE structure. We choose CVAE due to its ability in modeling global properties such as syntactic, semantic, and discourse coherence (Li et al., 2015;Yu et al., 2020). We first learn different granularity representations through two independent contrast, i.e., instancelevel and keyword-level. Specifically, we use the universal and classic TextRank (Mihalcea and Tarau, 2004) method to extract keywords from each text, which contain the most important information and need to be highlighted. On the instancelevel, we treat the keyword in the input text as an additional condition for a better prior semantic distribution. Then, we utilize Kullback-Leibler divergence (Kullback and Leibler, 1951) to reduce the distance between prior distribution and positive posterior distribution, and increase the distance with the negative posterior distribution. While on the keyword-level, we propose a keyword graph via contrastive correlations of positive-negative pairs to learn informative and accurate keyword representations. By treating the keyword in the output text as an anchor, the imposter keyword is produced by neighboring nodes of the anchor keyword and forms the keyword-level contrast, where the similarity between the imposter keyword and the anchor keyword is poorer than the positive keyword.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_7",
            "content": "To unify individual intra-contrasts and tackle the \"contrast vanishing\" problem in independent contrastive granularities, we leverage an inter-contrast, the Mahalanobis contrast, to investigate the contrastive enhancement based on the Mahalanobis distance (De Maesschalck et al., 2000), a measure of the distance between a point and a distribution, between the instance distribution and the keyword representation. Concretely, we ensure the distance from the anchor instance distribution to the groundtruth keyword vector is closer than to the imposter keyword vector. The Mahalanobis contrast plays an intermediate role that joins the different granularities contrast via incorporating the distribution of instance with the representation of its crucial part, and makes up a more comprehensive keyworddriven hierarchical contrastive mechanism, so as to ameliorate the generated results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_8",
            "content": "We empirically show that our model outperforms CVAE and other baselines significantly on three generation tasks: paraphrasing, dialogue generation, and storytelling.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_9",
            "content": "\u2022 To our best knowledge, we are the first to propose an inter-level contrastive learning method, which unifies instance-level and keyword-level contrasts in the CVAE framework. \u2022 We propose three contrastive learning measurements: KL divergence for semantic distribution, cosine distance for points, and Mahalanobis distance for points with distribution. \u2022 We introduce a global keyword graph to obtain polished keyword representations and construct imposter keywords for contrastive learning.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_10",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "33-ARR_v1_11",
            "content": "Contrastive Learning",
            "ntype": "title",
            "meta": {
                "section": "2.1"
            }
        },
        {
            "ix": "33-ARR_v1_12",
            "content": "Contrastive learning is used to learn representations by teaching the model which data points are similar or not. Due to the excellent performance on self-supervised and semi-supervised learning, it has been widely used in natural language processing (NLP). Firstly, Mikolov et al. (2013) proposed to predict neighboring words from context with noise-contrastive estimation. Then, based on word representations, contrastive learning for sentence has been utilized to learn semantic representations. Lee et al. (2021) generated positive and negative examples by adding perturbations to the hidden states. Cai et al. (2020) augmented contrastive dialogue learning with group-wise dual sampling. Moreover, contrastive learning has also been utilized in caption generation (Mao et al., 2016), summarization (Liu and Liu, 2021) and machine translation (Yang et al., 2019). Our work differs from previous works in focusing on hierarchical contrastive learning on hybrid granularities.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_13",
            "content": "Mahalanobis Distance",
            "ntype": "title",
            "meta": {
                "section": "2.2"
            }
        },
        {
            "ix": "33-ARR_v1_14",
            "content": "The Mahalanobis distance is a measure of the distance between a point and a distribution (De Maesschalck et al., 2000). The distance is zero if the point is on the distribution. Recently, Mahalanobis distance is popularly applied to the NLP tasks (Tran et al., 2019;Denouden et al., 2018). Podolskiy et al. (2021) showed that while Transformer is capable of constructing homogeneous representations of in-domain utterances, the Mahalanobis distance captures geometrical disparity from out of domain utterances. Further, Ren et al. (2021) considered that the raw density from deep generative models may fail at out-of-domain detection and proposed to fix this using a likelihood ratio between two generative models as a confidence score.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_15",
            "content": "Conditional Variational Auto-Encoder",
            "ntype": "title",
            "meta": {
                "section": "2.3"
            }
        },
        {
            "ix": "33-ARR_v1_16",
            "content": "Variational autoencoder (VAE) was proposed by Kingma and Welling (2013), and has been widely used in various tasks such as headline generation , dialogue generation (Serban et al., 2017) and story generation (Yu et al., 2020). Based on VAE, a more advanced model, Conditional VAE (CVAE), was proposed to generate diverse images conditioned on certain attributes, which was also applied to generate diverse outputs in NLP tasks (Zhao et al., 2017;Qiu et al., 2019). Existing works concentrate on generating diverse outputs, and we take one step further to utilize prior and posterior latent distribution to compare positive and negative samples, which helps to learn more accurate semantic information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_17",
            "content": "Method",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "33-ARR_v1_18",
            "content": "3.1 Background VAE: Variational auto-encoder (VAE) is a typical encoder-decoder structural model with certain types of latent variables. Given an input x, VAE models the latent variable z through the prior distribution p \u03b8 (z) , and the observed data x is reconstructed by the generative distribution p \u03b8 (x|z) which is the likelihood function that generates x conditioned on z. Since z is unknown, it should be estimated according to the given data x as p \u03b8 (z|x). While the posterior density p \u03b8 (z|x) = p \u03b8 (x|z)p \u03b8 (z)/p \u03b8 (x) is intractable, VAE introduces a recognition posterior distribution q \u03d5 (z|x) approximates to the true posterior p \u03b8 (z|x). Thus, VAE is trained by optimizing the lower bound on the marginal likelihood of data x as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_19",
            "content": "p \u03b8 (x) \u2265 E z\u223cq \u03d5 (z|x) [logp \u03b8 (x|z)] \u2212D KL (q \u03d5 (z|x)||p \u03b8 (z)),(1)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_20",
            "content": "where D KL is the Kullback-Leibler divergence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_21",
            "content": "The conditional variational auto-encoder (CVAE) is the supervised version of VAE with an additional output variable. Giving a dataset {x i , y i } N i=1 consisting of N samples, CVAE is trained to maximize the conditional log-likelihood, and the variational lower bound of the model is written as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_22",
            "content": "p \u03b8 (y|x) \u2265 E z\u223cq \u03d5 (z|x,y) [logp(y|x, z)] \u2212D KL (q \u03d5 (z|x, y)||p \u03b8 (z|x)).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_23",
            "content": "(2)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_24",
            "content": "Assuming the type of latent variable obeys Gaussian distribution, the first right-hand side term can be approximated by drawing samples {z i } N i=1 from the recognition posterior distribution q \u03d5 (z|x, y), where z \u223c N (\u00b5, \u03c3 2 I), and then objective of the CVAE with Gaussian distribution can be written as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_25",
            "content": "L cvae (x, y; \u03b8, \u03d5) = \u2212 1 N N i=1 logp \u03b8 (y|x, z i ) +D KL (q \u03d5 (z|x, y)||p \u03b8 (z|x)),(3)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_26",
            "content": "where z i = g \u03d5 (x, y, \u03f5 i ), \u03f5 i \u223c N (0, I). The distribution q \u03d5 (z|x, y) is reparameterized with a differentiable function g \u03d5 , which enables the model trainable via stochastic gradient descent.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_27",
            "content": "Inspired by Wu et al. (2019), we add keyword u as an additional condition to the prior distribution to control the generation process, which turns the p \u03b8 (z|x) in Equaton 3 into p \u03b8 (z|x, u).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_28",
            "content": "Hierarchical Contrastive Learning",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "33-ARR_v1_29",
            "content": "In this section, we introduce our hierarchical contrastive learning method, which is comprised of three parts: instance-level contrast based on KL divergence (sec.3.2.1), keyword-level contrast based on keyword graph (sec.3.2.2), and inter-contrast: Mahalanobis contrast (sec.3.2.3).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_30",
            "content": "Instance-level Contrastive Learning",
            "ntype": "title",
            "meta": {
                "section": "3.2.1"
            }
        },
        {
            "ix": "33-ARR_v1_31",
            "content": "To tackle the \"exposure bias\" problem and discriminatively exploit the different quality of references, instance-level contrastive learning is introduced to learn discrepancies of targets. Specifically, in addition to the observed input data x and positive output y + , a negative output y \u2212 is added to construct a contrastive pair {(x, y + ), (x, y \u2212 )}. In this case, the prior distribution p \u03b8 (z|x) is learned from a prior network, which is denoted as f \u03b8 (x). The approximate posteriors q \u03d5 (z|x, y + ) and q \u03d5 (z|x, y \u2212 ) are learned from a posterior network and represented as f \u03d5 (x, y + ) and f \u03d5 (x, y \u2212 ), respectively. The objective here is to make the distance between a prior distribution and positive posterior distribution closer than with the negative posterior distribution. Thus, the instance-level contrastive loss function can be written as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_32",
            "content": "Lins = \u2212E f \u03d5 [log(1 \u2212 e h(f \u03d5 (x,y + ),f \u03b8 (x))/\u03c4 y * \u2208Y e h(f \u03d5 (x,y * ),f \u03b8 (x))/\u03c4 )],",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_33",
            "content": "where the y * \u2208 Y can be positive sample y + or negative sample y \u2212 , and the \u03c4 is a temperature parameter to control push and pull force. The function h(\u2022) denotes the distance between elements, which is set as Kullback-Leibler divergence (Kullback and Leibler, 1951) in instance-level contrast, D KL (f \u03d5 (x, y * )||f \u03b8 (x)), to measure the difference between two distributions.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_34",
            "content": "Keyword-level Contrastive Learning",
            "ntype": "title",
            "meta": {
                "section": "3.2.2"
            }
        },
        {
            "ix": "33-ARR_v1_35",
            "content": "Since the instance-level contrast focuses on learning high-level information and fails to discriminate the contribution of each word, we incorporate it with a keyword-level contrast to pay more attention to the specific keyword.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_36",
            "content": "Keyword Graph: Given an input-output text pair (x, y), keywords k x , k y can be extracted from x and y, respectively. For an input text x i with keyword k x,i , input texts that contain the same keyword are gathered into a cluster C i = {x j } n j=1 , k x,j \u2208 x j , where n is the number of texts in C i . Each text x j \u2208 C i has a positive-negative output text pair {(y + j , y \u2212 j )} containing a positive output keyword k + y,j and a negative one k \u2212 y,j , respectively. Thus, spreading to the entire cluster C i , for the output text y i , there exists positive relations r + i,j between its keyword k y,i and each of the surrounded positive keywords {k + y,j } n j=1 . Likewise, negative relations r \u2212 i,j correlates the output keyword k y,i and the surrounded negative ones {k \u2212 y,j } n j=1 . Based on these keywords as nodes and their relations as edges, the keyword graph G k is constructed. Each node representation h 0 i is initialized as the average BERT embedding (Devlin et al., 2018) of texts in the cluster C i with the same corresponding keyword k x,i . Then, the relation edge r 0 ij that connects node i and node j is learned via a feedforward layer r 0 ij = FFN([h 0 i ; h 0 j ]). Then, the representations of nodes and relation edges are iteratively updated with their connected nodes via the graph attention (GAT) layer and the feed-forward (FFN) layer. In the t-th iteration, we first update each edge representation by paying attention to the connected nodes, denoted as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_37",
            "content": "\u03b2 t r * = softmax( (r t ij W p )(h t * W h ) T \u221a d ),(4)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_38",
            "content": "p t ij = \u03b2 t ri h t i + \u03b2 t rj h t j ,(5)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_39",
            "content": "r t+1 ij = FFN(r t ij + p t ij ),(6)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_40",
            "content": "where h t * can be h t i or h t j . Then, based on the obtained edge representation r t+1 ij , we update the node representations considering both the related nodes and relation edges by the graph attention layer, GAT(h t i , h t j , r t ij ), which is designed as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_41",
            "content": "e t ij = (h t i Wq)(h t j W k +r t+1 ij Wr) T \u221a d ,(7)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_42",
            "content": "\u03b1 t ij = exp(e t ij ) l\u2208N i exp(e t il ) ,(8)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_43",
            "content": "u t i = j\u2208N i \u03b1 t ij (h t j W v + r t+1 ij ),(9)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_44",
            "content": "where W q , W k , W r and W v are all learnable parameters, and the \u03b1 t ij is the attention weight between h t i and h t j . Besides, to avoid gradient vanishing after several iterations, a residual connection is added to the output u t i and the updated node representations h t+1 i is obtained. In this way, the new representation of each keyword node consists of the relation dependency information from neighbor nodes N i . We take the node representations from the last iteration as the final keyword representations, denoted as u for brevity.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_45",
            "content": "Keyword-level Contrast: The keyword-level contrastive learning arises from input keywords against positive output keywords and negative impostor keywords. The input keyword u in is extracted from the input text as an anchor, and the output keyword u out is extracted from ground-truth output text. While the impostor keyword is calculated from the negative neighbours of the output keyword u out , written as u imp = i W i u i , where u i is the representation of keyword node which is obtained by the keyword graph learning procedure described above. In this way, with the help of neighbour nodes in the graph, we can obtain a more indistinguishable and difficult negative sample. The loss of keyword level contrastive learning thus can be written as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_46",
            "content": "L keyword = \u2212E[log e h ( u in ,uout)/\u03c4 u * \u2208U e h(u in ,u * )/\u03c4 ],(10)",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_47",
            "content": "where u * \u2208 U denotes the positive output keyword u out or imposter keyword u imp . In keyword-level contrast, h(\u2022) utilizes cosine similarity to calculate the distance between points.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_48",
            "content": "Mahalanobis Contrastive Learning",
            "ntype": "title",
            "meta": {
                "section": "3.2.3"
            }
        },
        {
            "ix": "33-ARR_v1_49",
            "content": "Note that there exists a space gap between the instance-level contrast and the keyword-level contrast, which disturbs the completeness of this hierarchical contrastive architecture. Besides, the contrastive values vanish when the distance metric is hard to measure the actual discrepancy between positive and negative merely in instance distributions or in keyword representations. To mitigate such problems, we design a Mahalanobis contrastive mechanism to correlate the instance distribution and keyword representation, where the objective is to minimize the margin between the output keyword u out and the posterior semantic distribution q \u03d5 (z|x, y) \u225c f \u03d5 (x, y) and maximize the margin between the imposter keyword u imp and the posterior distribution f \u03d5 (x, y):",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_50",
            "content": "L ma = \u2212E f \u03d5 [log(1 \u2212 e h(f \u03d5 (x,y),uout)/\u03c4",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_51",
            "content": "u * \u2208U e h(f \u03d5 (x,y),u * )/\u03c4 )], (11) where u * \u2208 U can be the positive output keyword u out or negative imposter keyword u imp . In Mahalanobis contrast, h(\u2022) utilizes Mahalanobis distance (De Maesschalck et al., 2000) to measure the similarity from keyword point to the instance distribution. In the univariate Gaussian case, z \u223c p(z|x, y) = N (\u00b5, \u03c3 2 ), then the h(",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_52",
            "content": "f \u03d5 (x, y), u * ) \u225c D M A (p \u03b8 (z|x, y)||u * ) = (u * \u2212 \u00b5)\u03c3 2 I(u * \u2212 \u00b5).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_53",
            "content": "Finally, we equip the CVAE model with the proposed hierarchical contrastive learning framework to unify hybrid granularities by adding L ins , L keyword and L ma to the reconstructed loss of Equation 3.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_54",
            "content": "Experiment",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "33-ARR_v1_55",
            "content": "Tasks and Datasets",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "33-ARR_v1_56",
            "content": "We conduct experiments on three public datasets QQP, Douban, RocStories for paraphrasing, dialogue generation, and storytelling task, respectively. The details of the datasets are as follows:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_57",
            "content": "Dialogue (Douban) Douban (Cai et al., 2020) consists of Chinese daily conversations between pairs of speakers, collected from a popular social network website, Douban group 1 . The dataset contains 218,039/10,000/10,000 context-response pairs for training/validation/test, with an average of 3.94 turns per context and 38.32 characters per utterance. We concatenate historical dialogues and turn it into a single-turn dialogue training corpus.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_58",
            "content": "Paraphrasing (QQP) QQP (Iyer et al., 2017;) is a dataset published by the community question-answering website Quora on whether a pair of questions is semantically consistent. To adapt it to the contrastive learning task, we only keep question pairs that have positive and negative rewriting for the same input. Thus, there remain 44,949 samples in the dataset, which are split into training/validation/test sets of 40,441/2,254/2,254 samples.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_59",
            "content": "Storytelling (RocStories) RocStories consists of 98,163 high-quality hand-crafted stories, which capture causal and temporal commonsense relations of daily events (Mostafazadeh et al., 2016). Each story paragraph contains 5 sentences with an average of 43 words. Following the previous work Yu et al. (2021), we split the dataset into 8:1:1 for training, validation, and test.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_60",
            "content": "For the above three datasets, in order to construct different levels of contrastive learning, we performed the same preprocessing of extracting keywords. We utilize the TextRank model (Mihalcea and Tarau, 2004) to extract keywords from each input and output sample, respectively. Besides, the vocabulary size of both datasets is the same as BERT (Devlin et al., 2018) setting.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_61",
            "content": "Implementation Details",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "33-ARR_v1_62",
            "content": "Our experiments are implemented in Tensorflow (Abadi et al., 2016) on an NVIDIA Tesla P100 GPU. For our model and all baselines, we follow the same setting as described below. We pad or cut the input to 100, 20, 100 words for dialogue generation, paraphrasing, and storytelling, respectively. The truncation length is decided based on the observation that there is no significant improvement when increasing input length. The minimum decoding step is 5, and the maximum step is 20 for all tasks. Experiments were performed with a batch size of 256, and we use Adam optimizer (Kingma and Ba, 2015) as our optimizing algorithm. During the test stage, the beam-search size is set to 4 for all methods and the checkpoint with the smallest validation loss is chosen. Note that for better performance, our model is built based on BERT. Finally, due to the limitation of time and memory, small settings are used in the pre-training baselines.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_63",
            "content": "Compared Baselines",
            "ntype": "title",
            "meta": {
                "section": "4.3"
            }
        },
        {
            "ix": "33-ARR_v1_64",
            "content": "We compare our method against several traditional generation models, pretrained-based generation models, and contrastive learning models.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_65",
            "content": "Traditional generation models: (1) CVAE (Zhao et al., 2017) generates sentences based on latent variables, sampling from potential semantic distribution. (2) Seq2Seq (Sutskever et al., 2014) is a sequence-to-sequence framework combined with attention mechanism and pointer network. (3) Transformer (Vaswani et al., 2017) is an abstractive generation method based solely on attention mechanisms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_66",
            "content": "Pretrained-based generation models: (4) Seq2Seq-DU (Feng et al., 2021) is concerned with dialogue state tracking in a task-oriented dialogue system. ( 5) DialoGPT proposes a large, tunable neural conversational response generation model trained on more conversation-like exchanges. ( 6) BERT-GEN (Devlin et al., 2018) augments Seq2Seq with BERT as the encoder. ( 7) T5 (Raffel et al., 2020) introduces a unified framework that converts all text-based language problems into a text-to-text format.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_67",
            "content": "Contrastive learning methods: (8) Groupwise (Cai et al., 2020) augments contrastive dialogue learning with group-wise dual sampling. (9) T5-CLAPS (Lee et al., 2021) generates negative and positive samples for contrastive learning by adding small and large perturbations, respectively.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_68",
            "content": "Evaluation Metrics",
            "ntype": "title",
            "meta": {
                "section": "4.4"
            }
        },
        {
            "ix": "33-ARR_v1_69",
            "content": "To evaluate the performance of our model against baselines, we adopt the following metrics widely used in existing studies.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_70",
            "content": "BLEU We utilize BLEU score (Papineni et al., 2002) to measure word overlap between the generated text and the ground-truth. Specifically, following the conventional setting of (Gu et al., 2019), we adopt BLEU-1\u223c4 scores under the smoothing techniques (smoothing 7).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_71",
            "content": "Embedding To evaluate our model more comprehensively, we also capture the semantic matching degrees between the bag-of-words (BOW) embeddings of generated text and reference (Gu et al., 2019). Particularly we adopt three metrics: 1) Extrema, cosine similarity between the largest extreme values among the word embeddings in the two texts; 2) Average, cosine similarity between the averaged word embeddings of generated text and reference; 3) Greedy, greedily matching words in the two texts based on cosine similarities. Seq2Seq and Transformer, and the lower part shows the latest pretrained-based methods including DialoGPT and T5. Overall, pretrained-based methods generally outperform traditional methods, and this also proves the effectiveness of the pretrained language model on the generation tasks. Secondly, we can find that the performance is significantly improved after adding contrast learning. Finally, our method outperforms T5-CLAPS by 2.7%, 3.6% on QQP, by 20.3%, 24.9% on Douban, and by 3.9%, 6.3% on RocStories in terms of BLEU-1, BLEU-2, respectively, which proves the superiority of our model.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_72",
            "content": "Experimental Results",
            "ntype": "title",
            "meta": {
                "section": "4.5"
            }
        },
        {
            "ix": "33-ARR_v1_73",
            "content": "Overall Performance Automatic Evaluation The experimental results ars summarized in",
            "ntype": "title",
            "meta": {
                "section": "4.5.1"
            }
        },
        {
            "ix": "33-ARR_v1_74",
            "content": "Human Evaluation We also assessed system performance by eliciting human judgments on 100 randomly selected test instances on QQP dataset.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_75",
            "content": "The annotators are asked to rate paraphrasing questions generated by T5-CLAPS, DialoGPT, Seq2Seq-DU, and our model according to Fluency (Flu), Meaningfulness (Mean), and Differential (Diff). The rating score ranges from 1 to 3, with 3 being the best. instance-level contrastive, the effect of our model is greatly reduced by about 10.4%, which illustrates the desirability of considering the contributions of words in a sentence. On this basis, adding keyword contrastive learning with removing the keyword graph, the effect of the model has been improved but is still lower than our model by 2.1%. This shows that keywords are indeed conducive to capturing important information, and it also illustrates the significance of a keyword graph. Finally, the experiment of removing the Mahalanobis contrastive loss indicates that only with granularity independent contrast is not sufficient, and the Mahalanobis contrast plays a critical intermediate role.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_76",
            "content": "Visualization of Different Levels of Contrastive Learning",
            "ntype": "title",
            "meta": {
                "section": "4.5.3"
            }
        },
        {
            "ix": "33-ARR_v1_77",
            "content": "To study the hierarchical contrastive learning, we visualize the vectors of keyword, input text, positive and negative output text on randomly sampled cases from QQP dataset, as shown in Figure 3. For visualization purposes, we reduce the dimension of the latent vector with t-SNE (Maaten and Hinton, 2008). It can be observed that the input sentence representation is located close to the keyword, which shows that the keyword, as the most important information in the sentence, determines the semantic distribution. Moreover, in contrastive learning, it can be seen that after training, the position of the input sentence is close to the positive samples and far away from the negative samples. This suggests that contrastive learning can correct the semantic distribution.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_78",
            "content": "Analysis of Different Keywords",
            "ntype": "title",
            "meta": {
                "section": "4.5.4"
            }
        },
        {
            "ix": "33-ARR_v1_79",
            "content": "We finally investigate the influence of sampling different keywords. As shown in Table 4, for an input question, we provide keywords extracted by Tex- What are the best online sites or apps with games for learning German?",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_80",
            "content": "Which is the best site to learn German ? tRank and randomly-selected keywords as the condition to control the semantic distribution and examine the quality of the generated text. As the most important information unit, different keywords lead to different semantic distributions and will result in different generated texts. The more properly the keywords are selected, the more accurately the sentences will be generated. When utilizing the keywords extracted by TextRank as a condition, the information \"belly fat\" is focused during the generation of paraphrasing questions, and the generated sentences are more accurate. On the contrary, after adding the random-selected keyword \"disposable\", the generated question emphasizes \"one-off exercise\", which brings incorrect information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_81",
            "content": "We also compare our model with several baselines in Table 4. Most baselines can generate fluent questions in this case. However, they focus on \"lose weight\", and miss the significant information \"belly fat\". Based on the above analysis, we can observe that keywords can emphasize and protect the highlight information in sentences, and affect the semantic distribution of as a condition.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_82",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "33-ARR_v1_83",
            "content": "In this paper, we propose a hierarchical contrastive learning mechanism, which consists of intra-contrasts within instance-level and keywordlevel and inter-contrast with Mahalanobis contrast. The experimental results yield significant outperformance over baselines when applied in the CVAE framework. In the future, we aim to extend the contrastive learning mechanism to different basic models, and will explore contrastive learning methods based on external knowledge.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "33-ARR_v1_84",
            "content": "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Tensorflow: A system for large-scale machine learning, 2016, OSDI, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Mart\u00edn Abadi",
                    "Paul Barham",
                    "Jianmin Chen",
                    "Zhifeng Chen",
                    "Andy Davis",
                    "Jeffrey Dean",
                    "Matthieu Devin",
                    "Sanjay Ghemawat",
                    "Geoffrey Irving",
                    "Michael Isard",
                    "Manjunath Kudlur",
                    "Josh Levenberg"
                ],
                "title": "Tensorflow: A system for large-scale machine learning",
                "pub_date": "2016",
                "pub_title": "OSDI",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_85",
            "content": "Hengyi Cai, Hongshen Chen, Yonghao Song, Zhuoye Ding, Yongjun Bao, Weipeng Yan, Xiaofang Zhao, Group-wise contrastive learning for neural dialogue generation, 2020-11-20, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Hengyi Cai",
                    "Hongshen Chen",
                    "Yonghao Song",
                    "Zhuoye Ding",
                    "Yongjun Bao",
                    "Weipeng Yan",
                    "Xiaofang Zhao"
                ],
                "title": "Group-wise contrastive learning for neural dialogue generation",
                "pub_date": "2020-11-20",
                "pub_title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "33-ARR_v1_86",
            "content": "UNKNOWN, None, 2000, The mahalanobis distance. Chemometrics and intelligent laboratory systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": null,
                "title": null,
                "pub_date": "2000",
                "pub_title": "The mahalanobis distance. Chemometrics and intelligent laboratory systems",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_87",
            "content": "UNKNOWN, None, 2018, Improving reconstruction autoencoder outof-distribution detection with mahalanobis distance, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Improving reconstruction autoencoder outof-distribution detection with mahalanobis distance",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_88",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": null,
                "title": null,
                "pub_date": "2018",
                "pub_title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_89",
            "content": "UNKNOWN, None, 2021, A sequenceto-sequence approach to dialogue state tracking, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "A sequenceto-sequence approach to dialogue state tracking",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_90",
            "content": "Xiaodong Gu, Kyunghyun Cho, Jung-Woo Ha, Sunghun Kim, DialogWAE: Multimodal response generation with conditional wasserstein autoencoder, 2019, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Xiaodong Gu",
                    "Kyunghyun Cho",
                    "Jung-Woo Ha",
                    "Sunghun Kim"
                ],
                "title": "DialogWAE: Multimodal response generation with conditional wasserstein autoencoder",
                "pub_date": "2019",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_91",
            "content": "UNKNOWN, None, 2017, First quora dataset release: Question pairs, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "First quora dataset release: Question pairs",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_92",
            "content": "UNKNOWN, None, 2015, Adam: A method for stochastic optimization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "Adam: A method for stochastic optimization",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_93",
            "content": "UNKNOWN, None, 2013, Autoencoding variational bayes, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": null,
                "title": null,
                "pub_date": "2013",
                "pub_title": "Autoencoding variational bayes",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_94",
            "content": "UNKNOWN, None, 1951, On information and sufficiency. The annals of mathematical statistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": null,
                "title": null,
                "pub_date": "1951",
                "pub_title": "On information and sufficiency. The annals of mathematical statistics",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_95",
            "content": "Seanie Lee, Dong Lee, Sung Hwang, Contrastive learning with adversarial perturbations for conditional text generation, 2021-05-03, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Seanie Lee",
                    "Dong Lee",
                    "Sung Hwang"
                ],
                "title": "Contrastive learning with adversarial perturbations for conditional text generation",
                "pub_date": "2021-05-03",
                "pub_title": "9th International Conference on Learning Representations, ICLR 2021, Virtual Event",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_96",
            "content": "UNKNOWN, None, 2015, A hierarchical neural autoencoder for paragraphs and documents, .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": null,
                "title": null,
                "pub_date": "2015",
                "pub_title": "A hierarchical neural autoencoder for paragraphs and documents",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_97",
            "content": "Mingzhe Li, Xiuying Chen, Min Yang, Shen Gao, Dongyan Zhao, Rui Yan, The stylecontent duality of attractiveness: Learning to write eye-catching headlines via disentanglement, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Mingzhe Li",
                    "Xiuying Chen",
                    "Min Yang",
                    "Shen Gao",
                    "Dongyan Zhao",
                    "Rui Yan"
                ],
                "title": "The stylecontent duality of attractiveness: Learning to write eye-catching headlines via disentanglement",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_98",
            "content": "UNKNOWN, None, 2021, Simcls: A simple framework for contrastive learning of abstractive summarization, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": null,
                "title": null,
                "pub_date": "2021",
                "pub_title": "Simcls: A simple framework for contrastive learning of abstractive summarization",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_99",
            "content": "Laurens Van Der Maaten, Geoffrey Hinton, Visualizing data using t-sne, 2008-11, Journal of machine learning research, .",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Laurens Van Der Maaten",
                    "Geoffrey Hinton"
                ],
                "title": "Visualizing data using t-sne",
                "pub_date": "2008-11",
                "pub_title": "Journal of machine learning research",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_100",
            "content": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, Kevin Murphy, Generation and comprehension of unambiguous object descriptions, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Junhua Mao",
                    "Jonathan Huang",
                    "Alexander Toshev",
                    "Oana Camburu",
                    "Alan Yuille",
                    "Kevin Murphy"
                ],
                "title": "Generation and comprehension of unambiguous object descriptions",
                "pub_date": "2016",
                "pub_title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_101",
            "content": "Rada Mihalcea, Paul Tarau, Textrank: Bringing order into text, 2004, Proceedings of the 2004 conference on empirical methods in natural language processing, .",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Rada Mihalcea",
                    "Paul Tarau"
                ],
                "title": "Textrank: Bringing order into text",
                "pub_date": "2004",
                "pub_title": "Proceedings of the 2004 conference on empirical methods in natural language processing",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_102",
            "content": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeff Dean, Distributed representations of words and phrases and their compositionality, 2013, Advances in neural information processing systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Tomas Mikolov",
                    "Ilya Sutskever",
                    "Kai Chen",
                    "Greg Corrado",
                    "Jeff Dean"
                ],
                "title": "Distributed representations of words and phrases and their compositionality",
                "pub_date": "2013",
                "pub_title": "Advances in neural information processing systems",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_103",
            "content": "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen, A corpus and cloze evaluation for deeper understanding of commonsense stories, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Nasrin Mostafazadeh",
                    "Nathanael Chambers",
                    "Xiaodong He",
                    "Devi Parikh",
                    "Dhruv Batra",
                    "Lucy Vanderwende",
                    "Pushmeet Kohli",
                    "James Allen"
                ],
                "title": "A corpus and cloze evaluation for deeper understanding of commonsense stories",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_104",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, ACL, ACL.",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Kishore Papineni",
                    "Salim Roukos",
                    "Todd Ward",
                    "Wei-Jing Zhu"
                ],
                "title": "Bleu: a method for automatic evaluation of machine translation",
                "pub_date": "2002",
                "pub_title": "ACL",
                "pub": "ACL"
            }
        },
        {
            "ix": "33-ARR_v1_105",
            "content": "Alexander Podolskiy, Dmitry Lipin, Andrey Bout, Ekaterina Artemova, and Irina Piontkovskaya. 2021. Revisiting mahalanobis distance for transformer-based out-of-domain detection, , Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Alexander Podolskiy",
                    "Dmitry Lipin",
                    "Andrey Bout"
                ],
                "title": "Ekaterina Artemova, and Irina Piontkovskaya. 2021. Revisiting mahalanobis distance for transformer-based out-of-domain detection",
                "pub_date": null,
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_106",
            "content": "Lisong Qiu, Juntao Li, Wei Bi, Dongyan Zhao, Rui Yan, Are training samples correlated? learning to generate dialogue responses with multiple references, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Lisong Qiu",
                    "Juntao Li",
                    "Wei Bi",
                    "Dongyan Zhao",
                    "Rui Yan"
                ],
                "title": "Are training samples correlated? learning to generate dialogue responses with multiple references",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_107",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, J. Mach. Learn. Res, .",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Colin Raffel",
                    "Noam Shazeer",
                    "Adam Roberts",
                    "Katherine Lee",
                    "Sharan Narang",
                    "Michael Matena",
                    "Yanqi Zhou",
                    "Wei Li",
                    "Peter Liu"
                ],
                "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
                "pub_date": "2020",
                "pub_title": "J. Mach. Learn. Res",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_108",
            "content": "UNKNOWN, None, , Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. 2021. A simple fix to mahalanobis distance for improving near-ood detection, .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. 2021. A simple fix to mahalanobis distance for improving near-ood detection",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_109",
            "content": "Iulian Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, Yoshua Bengio, A hierarchical latent variable encoder-decoder model for generating dialogues, 2017, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Iulian Serban",
                    "Alessandro Sordoni",
                    "Ryan Lowe",
                    "Laurent Charlin",
                    "Joelle Pineau",
                    "Aaron Courville",
                    "Yoshua Bengio"
                ],
                "title": "A hierarchical latent variable encoder-decoder model for generating dialogues",
                "pub_date": "2017",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_110",
            "content": "Ilya Sutskever, Oriol Vinyals, V Quoc,  Le, Sequence to sequence learning with neural networks, 2014-12-08, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Ilya Sutskever",
                    "Oriol Vinyals",
                    "V Quoc",
                    " Le"
                ],
                "title": "Sequence to sequence learning with neural networks",
                "pub_date": "2014-12-08",
                "pub_title": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_111",
            "content": "Thanh Tran, Renee Sweeney, Kyumin Lee, Adversarial mahalanobis distance-based attentive song recommender for automatic playlist continuation, 2019, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Thanh Tran",
                    "Renee Sweeney",
                    "Kyumin Lee"
                ],
                "title": "Adversarial mahalanobis distance-based attentive song recommender for automatic playlist continuation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_112",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Lukasz Kaiser",
                    "Illia Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017-12-04",
                "pub_title": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_113",
            "content": "UNKNOWN, None, 2019, GLUE: A multi-task benchmark and analysis platform for natural language understanding, .",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_114",
            "content": "UNKNOWN, None, 2019, Guiding variational response generator to exploit persona, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Guiding variational response generator to exploit persona",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_115",
            "content": "UNKNOWN, None, 2019, Reducing word omission errors in neural machine translation: A contrastive learning approach, .",
            "ntype": "ref",
            "meta": {
                "xid": "b31",
                "authors": null,
                "title": null,
                "pub_date": "2019",
                "pub_title": "Reducing word omission errors in neural machine translation: A contrastive learning approach",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_116",
            "content": "Meng-Hsuan Yu, Juntao Li, Zhangming Chan, Dongyan Zhao, Rui Yan, Content learning with structure-aware writing: A graph-infused dual conditional variational autoencoder for automatic storytelling, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b32",
                "authors": [
                    "Meng-Hsuan Yu",
                    "Juntao Li",
                    "Zhangming Chan",
                    "Dongyan Zhao",
                    "Rui Yan"
                ],
                "title": "Content learning with structure-aware writing: A graph-infused dual conditional variational autoencoder for automatic storytelling",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_117",
            "content": "Meng-Hsuan Yu, Juntao Li, Danyang Liu, Dongyan Zhao, Rui Yan, Bo Tang, Haisong Zhang, Draft and edit: Automatic storytelling through multipass hierarchical conditional variational autoencoder, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b33",
                "authors": [
                    "Meng-Hsuan Yu",
                    "Juntao Li",
                    "Danyang Liu",
                    "Dongyan Zhao",
                    "Rui Yan",
                    "Bo Tang",
                    "Haisong Zhang"
                ],
                "title": "Draft and edit: Automatic storytelling through multipass hierarchical conditional variational autoencoder",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_118",
            "content": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan, Dialogpt: Large-scale generative pre-training for conversational response generation, 2020, ACL, system demonstration, .",
            "ntype": "ref",
            "meta": {
                "xid": "b34",
                "authors": [
                    "Yizhe Zhang",
                    "Siqi Sun",
                    "Michel Galley",
                    "Yen-Chun Chen",
                    "Chris Brockett",
                    "Xiang Gao",
                    "Jianfeng Gao",
                    "Jingjing Liu",
                    "Bill Dolan"
                ],
                "title": "Dialogpt: Large-scale generative pre-training for conversational response generation",
                "pub_date": "2020",
                "pub_title": "ACL, system demonstration",
                "pub": null
            }
        },
        {
            "ix": "33-ARR_v1_119",
            "content": "UNKNOWN, None, 2017, Learning discourse-level diversity for neural dialog models using conditional variational autoencoders, .",
            "ntype": "ref",
            "meta": {
                "xid": "b35",
                "authors": null,
                "title": null,
                "pub_date": "2017",
                "pub_title": "Learning discourse-level diversity for neural dialog models using conditional variational autoencoders",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "33-ARR_v1_0@0",
            "content": "Keywords and Instances: A Hierarchical Contrastive Learning Framework Unifying Hybrid Granularities for Text Generation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_0",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_2@0",
            "content": "Contrastive learning has achieved impressive success in generation tasks to militate the \"exposure bias\" problem and discriminatively exploit the different quality of references.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_2",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_2@1",
            "content": "Existing works mostly focus on contrastive learning on the instance-level without discriminating the contribution of each word, while keywords are the gist of the text and dominant the constrained mapping relationships.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_2",
            "start": 179,
            "end": 397,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_2@2",
            "content": "Hence, in this work, we propose a hierarchical contrastive learning mechanism, which can unify hybrid granularities semantic meaning in the input text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_2",
            "start": 399,
            "end": 549,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_2@3",
            "content": "Concretely, we first propose a keyword graph via contrastive correlations of positive-negative pairs to iteratively polish the keyword representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_2",
            "start": 551,
            "end": 701,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_2@4",
            "content": "Then, we construct intra-contrasts within instance-level and keyword-level, where we assume words are sampled nodes from a sentence distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_2",
            "start": 703,
            "end": 847,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_2@5",
            "content": "Finally, to bridge the gap between independent contrast levels and tackle the common contrast vanishing problem, we propose an inter-contrast mechanism that measures the discrepancy between contrastive keyword nodes respectively to the instance distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_2",
            "start": 849,
            "end": 1106,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_2@6",
            "content": "Experiments demonstrate that our model outperforms competitive baselines on paraphrasing, dialogue generation, and storytelling tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_2",
            "start": 1108,
            "end": 1241,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_4@0",
            "content": "Generation tasks such as storytelling, paraphrasing, and dialogue generation aim at learning a certain correlation between text pairs that maps an arbitrary-length input text to another arbitrarylength output text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_4",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_4@1",
            "content": "Traditional methods are mostly trained with \"teacher forcing\" and lead to an \"exposure bias\" problem.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_4",
            "start": 215,
            "end": 315,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_4@2",
            "content": "Incorporating the generation method with contrastive learning achieved impressive performance on tackling such issues, which takes an extra consideration of synthetic negative samples contrastively (Lee et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_4",
            "start": 317,
            "end": 533,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_5@0",
            "content": "Existing contrastive mechanisms are mainly focused on the instance level (Lee et al., 2021;Cai are the best books on cosmology?\" would be greatly changed if the keyword \"cosmology\" is changed to \"astrophysic\". et al., 2020). However, word-level information is also of great importance. Take the case shown in the upper part of Figure 1 for example, the keyword covers the gist of the input text and determines the embedding space of the text. The text representation will be significantly affected if adding a slight perturbation on the keyword, i.e., changing \"cosmology\" to \"astrophysics\". In addition, as shown on the bottom part, under some circumstances, it is too easy for the model to do the classification since the semantic gap between contrastive pairs is huge. Thus, the model fails to distinguish the actual discrepancy, which causes a \"contrast vanishing\" problem at both instance-level and keyword-level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_5",
            "start": 0,
            "end": 917,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_6@0",
            "content": "Based on the above motivation, in this paper, we propose a hierarchical contrastive learning method built on top of the classic CVAE structure.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_6",
            "start": 0,
            "end": 142,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_6@1",
            "content": "We choose CVAE due to its ability in modeling global properties such as syntactic, semantic, and discourse coherence (Li et al., 2015;Yu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_6",
            "start": 144,
            "end": 294,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_6@2",
            "content": "We first learn different granularity representations through two independent contrast, i.e., instancelevel and keyword-level.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_6",
            "start": 296,
            "end": 420,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_6@3",
            "content": "Specifically, we use the universal and classic TextRank (Mihalcea and Tarau, 2004) method to extract keywords from each text, which contain the most important information and need to be highlighted.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_6",
            "start": 422,
            "end": 619,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_6@4",
            "content": "On the instancelevel, we treat the keyword in the input text as an additional condition for a better prior semantic distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_6",
            "start": 621,
            "end": 749,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_6@5",
            "content": "Then, we utilize Kullback-Leibler divergence (Kullback and Leibler, 1951) to reduce the distance between prior distribution and positive posterior distribution, and increase the distance with the negative posterior distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_6",
            "start": 751,
            "end": 978,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_6@6",
            "content": "While on the keyword-level, we propose a keyword graph via contrastive correlations of positive-negative pairs to learn informative and accurate keyword representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_6",
            "start": 980,
            "end": 1148,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_6@7",
            "content": "By treating the keyword in the output text as an anchor, the imposter keyword is produced by neighboring nodes of the anchor keyword and forms the keyword-level contrast, where the similarity between the imposter keyword and the anchor keyword is poorer than the positive keyword.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_6",
            "start": 1150,
            "end": 1429,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_7@0",
            "content": "To unify individual intra-contrasts and tackle the \"contrast vanishing\" problem in independent contrastive granularities, we leverage an inter-contrast, the Mahalanobis contrast, to investigate the contrastive enhancement based on the Mahalanobis distance (De Maesschalck et al., 2000), a measure of the distance between a point and a distribution, between the instance distribution and the keyword representation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_7",
            "start": 0,
            "end": 413,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_7@1",
            "content": "Concretely, we ensure the distance from the anchor instance distribution to the groundtruth keyword vector is closer than to the imposter keyword vector.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_7",
            "start": 415,
            "end": 567,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_7@2",
            "content": "The Mahalanobis contrast plays an intermediate role that joins the different granularities contrast via incorporating the distribution of instance with the representation of its crucial part, and makes up a more comprehensive keyworddriven hierarchical contrastive mechanism, so as to ameliorate the generated results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_7",
            "start": 569,
            "end": 886,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_8@0",
            "content": "We empirically show that our model outperforms CVAE and other baselines significantly on three generation tasks: paraphrasing, dialogue generation, and storytelling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_8",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_9@0",
            "content": "\u2022 To our best knowledge, we are the first to propose an inter-level contrastive learning method, which unifies instance-level and keyword-level contrasts in the CVAE framework. \u2022 We propose three contrastive learning measurements: KL divergence for semantic distribution, cosine distance for points, and Mahalanobis distance for points with distribution. \u2022 We introduce a global keyword graph to obtain polished keyword representations and construct imposter keywords for contrastive learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_9",
            "start": 0,
            "end": 492,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_10@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_10",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_11@0",
            "content": "Contrastive Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_11",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_12@0",
            "content": "Contrastive learning is used to learn representations by teaching the model which data points are similar or not.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_12",
            "start": 0,
            "end": 112,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_12@1",
            "content": "Due to the excellent performance on self-supervised and semi-supervised learning, it has been widely used in natural language processing (NLP).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_12",
            "start": 114,
            "end": 256,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_12@2",
            "content": "Firstly, Mikolov et al. (2013) proposed to predict neighboring words from context with noise-contrastive estimation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_12",
            "start": 258,
            "end": 373,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_12@3",
            "content": "Then, based on word representations, contrastive learning for sentence has been utilized to learn semantic representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_12",
            "start": 375,
            "end": 497,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_12@4",
            "content": "Lee et al. (2021) generated positive and negative examples by adding perturbations to the hidden states.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_12",
            "start": 499,
            "end": 602,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_12@5",
            "content": "Cai et al. (2020) augmented contrastive dialogue learning with group-wise dual sampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_12",
            "start": 604,
            "end": 691,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_12@6",
            "content": "Moreover, contrastive learning has also been utilized in caption generation (Mao et al., 2016), summarization (Liu and Liu, 2021) and machine translation (Yang et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_12",
            "start": 693,
            "end": 866,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_12@7",
            "content": "Our work differs from previous works in focusing on hierarchical contrastive learning on hybrid granularities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_12",
            "start": 868,
            "end": 977,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_13@0",
            "content": "Mahalanobis Distance",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_13",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_14@0",
            "content": "The Mahalanobis distance is a measure of the distance between a point and a distribution (De Maesschalck et al., 2000).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_14",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_14@1",
            "content": "The distance is zero if the point is on the distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_14",
            "start": 120,
            "end": 176,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_14@2",
            "content": "Recently, Mahalanobis distance is popularly applied to the NLP tasks (Tran et al., 2019;Denouden et al., 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_14",
            "start": 178,
            "end": 288,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_14@3",
            "content": "Podolskiy et al. (2021) showed that while Transformer is capable of constructing homogeneous representations of in-domain utterances, the Mahalanobis distance captures geometrical disparity from out of domain utterances.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_14",
            "start": 290,
            "end": 509,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_14@4",
            "content": "Further, Ren et al. (2021) considered that the raw density from deep generative models may fail at out-of-domain detection and proposed to fix this using a likelihood ratio between two generative models as a confidence score.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_14",
            "start": 511,
            "end": 735,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_15@0",
            "content": "Conditional Variational Auto-Encoder",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_15",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_16@0",
            "content": "Variational autoencoder (VAE) was proposed by Kingma and Welling (2013), and has been widely used in various tasks such as headline generation , dialogue generation (Serban et al., 2017) and story generation (Yu et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_16",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_16@1",
            "content": "Based on VAE, a more advanced model, Conditional VAE (CVAE), was proposed to generate diverse images conditioned on certain attributes, which was also applied to generate diverse outputs in NLP tasks (Zhao et al., 2017;Qiu et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_16",
            "start": 227,
            "end": 463,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_16@2",
            "content": "Existing works concentrate on generating diverse outputs, and we take one step further to utilize prior and posterior latent distribution to compare positive and negative samples, which helps to learn more accurate semantic information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_16",
            "start": 465,
            "end": 700,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_17@0",
            "content": "Method",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_17",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_18@0",
            "content": "3.1 Background VAE: Variational auto-encoder (VAE) is a typical encoder-decoder structural model with certain types of latent variables.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_18",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_18@1",
            "content": "Given an input x, VAE models the latent variable z through the prior distribution p \u03b8 (z) , and the observed data x is reconstructed by the generative distribution p \u03b8 (x|z) which is the likelihood function that generates x conditioned on z. Since z is unknown, it should be estimated according to the given data x as p \u03b8 (z|x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_18",
            "start": 137,
            "end": 464,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_18@2",
            "content": "While the posterior density p \u03b8 (z|x) = p \u03b8 (x|z)p \u03b8 (z)/p \u03b8 (x) is intractable, VAE introduces a recognition posterior distribution q \u03d5 (z|x) approximates to the true posterior p \u03b8 (z|x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_18",
            "start": 466,
            "end": 653,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_18@3",
            "content": "Thus, VAE is trained by optimizing the lower bound on the marginal likelihood of data x as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_18",
            "start": 655,
            "end": 745,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_19@0",
            "content": "p \u03b8 (x) \u2265 E z\u223cq \u03d5 (z|x) [logp \u03b8 (x|z)] \u2212D KL (q \u03d5 (z|x)||p \u03b8 (z)),(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_19",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_20@0",
            "content": "where D KL is the Kullback-Leibler divergence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_20",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_21@0",
            "content": "The conditional variational auto-encoder (CVAE) is the supervised version of VAE with an additional output variable.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_21",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_21@1",
            "content": "Giving a dataset {x i , y i } N i=1 consisting of N samples, CVAE is trained to maximize the conditional log-likelihood, and the variational lower bound of the model is written as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_21",
            "start": 117,
            "end": 304,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_22@0",
            "content": "p \u03b8 (y|x) \u2265 E z\u223cq \u03d5 (z|x,y) [logp(y|x, z)] \u2212D KL (q \u03d5 (z|x, y)||p \u03b8 (z|x)).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_22",
            "start": 0,
            "end": 74,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_23@0",
            "content": "(2)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_23",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_24@0",
            "content": "Assuming the type of latent variable obeys Gaussian distribution, the first right-hand side term can be approximated by drawing samples {z i } N i=1 from the recognition posterior distribution q \u03d5 (z|x, y), where z \u223c N (\u00b5, \u03c3 2 I), and then objective of the CVAE with Gaussian distribution can be written as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_24",
            "start": 0,
            "end": 306,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_25@0",
            "content": "L cvae (x, y; \u03b8, \u03d5) = \u2212 1 N N i=1 logp \u03b8 (y|x, z i ) +D KL (q \u03d5 (z|x, y)||p \u03b8 (z|x)),(3)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_25",
            "start": 0,
            "end": 87,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_26@0",
            "content": "where z i = g \u03d5 (x, y, \u03f5 i ), \u03f5 i \u223c N (0, I).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_26",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_26@1",
            "content": "The distribution q \u03d5 (z|x, y) is reparameterized with a differentiable function g \u03d5 , which enables the model trainable via stochastic gradient descent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_26",
            "start": 46,
            "end": 197,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_27@0",
            "content": "Inspired by Wu et al. (2019), we add keyword u as an additional condition to the prior distribution to control the generation process, which turns the p \u03b8 (z|x) in Equaton 3 into p \u03b8 (z|x, u).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_27",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_28@0",
            "content": "Hierarchical Contrastive Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_28",
            "start": 0,
            "end": 32,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_29@0",
            "content": "In this section, we introduce our hierarchical contrastive learning method, which is comprised of three parts: instance-level contrast based on KL divergence (sec.3.2.1), keyword-level contrast based on keyword graph (sec.3.2.2), and inter-contrast: Mahalanobis contrast (sec.3.2.3).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_29",
            "start": 0,
            "end": 282,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_30@0",
            "content": "Instance-level Contrastive Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_30",
            "start": 0,
            "end": 34,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_31@0",
            "content": "To tackle the \"exposure bias\" problem and discriminatively exploit the different quality of references, instance-level contrastive learning is introduced to learn discrepancies of targets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_31",
            "start": 0,
            "end": 187,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_31@1",
            "content": "Specifically, in addition to the observed input data x and positive output y + , a negative output y \u2212 is added to construct a contrastive pair {(x, y + ), (x, y \u2212 )}.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_31",
            "start": 189,
            "end": 355,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_31@2",
            "content": "In this case, the prior distribution p \u03b8 (z|x) is learned from a prior network, which is denoted as f \u03b8 (x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_31",
            "start": 357,
            "end": 464,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_31@3",
            "content": "The approximate posteriors q \u03d5 (z|x, y + ) and q \u03d5 (z|x, y \u2212 ) are learned from a posterior network and represented as f \u03d5 (x, y + ) and f \u03d5 (x, y \u2212 ), respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_31",
            "start": 466,
            "end": 630,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_31@4",
            "content": "The objective here is to make the distance between a prior distribution and positive posterior distribution closer than with the negative posterior distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_31",
            "start": 632,
            "end": 792,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_31@5",
            "content": "Thus, the instance-level contrastive loss function can be written as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_31",
            "start": 794,
            "end": 862,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_32@0",
            "content": "Lins = \u2212E f \u03d5 [log(1 \u2212 e h(f \u03d5 (x,y + ),f \u03b8 (x))/\u03c4 y * \u2208Y e h(f \u03d5 (x,y * ),f \u03b8 (x))/\u03c4 )],",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_32",
            "start": 0,
            "end": 88,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_33@0",
            "content": "where the y * \u2208 Y can be positive sample y + or negative sample y \u2212 , and the \u03c4 is a temperature parameter to control push and pull force.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_33",
            "start": 0,
            "end": 137,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_33@1",
            "content": "The function h(\u2022) denotes the distance between elements, which is set as Kullback-Leibler divergence (Kullback and Leibler, 1951) in instance-level contrast, D KL (f \u03d5 (x, y * )||f \u03b8 (x)), to measure the difference between two distributions.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_33",
            "start": 139,
            "end": 379,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_34@0",
            "content": "Keyword-level Contrastive Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_34",
            "start": 0,
            "end": 33,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_35@0",
            "content": "Since the instance-level contrast focuses on learning high-level information and fails to discriminate the contribution of each word, we incorporate it with a keyword-level contrast to pay more attention to the specific keyword.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_35",
            "start": 0,
            "end": 227,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_36@0",
            "content": "Keyword Graph: Given an input-output text pair (x, y), keywords k x , k y can be extracted from x and y, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_36",
            "start": 0,
            "end": 117,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_36@1",
            "content": "For an input text x i with keyword k x,i , input texts that contain the same keyword are gathered into a cluster C i = {x j } n j=1 , k x,j \u2208 x j , where n is the number of texts in C i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_36",
            "start": 119,
            "end": 305,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_36@2",
            "content": "Each text x j \u2208 C i has a positive-negative output text pair {(y + j , y \u2212 j )} containing a positive output keyword k + y,j and a negative one k \u2212 y,j , respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_36",
            "start": 307,
            "end": 473,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_36@3",
            "content": "Thus, spreading to the entire cluster C i , for the output text y i , there exists positive relations r + i,j between its keyword k y,i and each of the surrounded positive keywords {k + y,j } n j=1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_36",
            "start": 475,
            "end": 673,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_36@4",
            "content": "Likewise, negative relations r \u2212 i,j correlates the output keyword k y,i and the surrounded negative ones {k \u2212 y,j } n j=1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_36",
            "start": 675,
            "end": 798,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_36@5",
            "content": "Based on these keywords as nodes and their relations as edges, the keyword graph G k is constructed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_36",
            "start": 800,
            "end": 899,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_36@6",
            "content": "Each node representation h 0 i is initialized as the average BERT embedding (Devlin et al., 2018) of texts in the cluster C i with the same corresponding keyword k x,i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_36",
            "start": 901,
            "end": 1069,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_36@7",
            "content": "Then, the relation edge r 0 ij that connects node i and node j is learned via a feedforward layer r 0 ij = FFN([h 0 i ; h 0 j ]).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_36",
            "start": 1071,
            "end": 1199,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_36@8",
            "content": "Then, the representations of nodes and relation edges are iteratively updated with their connected nodes via the graph attention (GAT) layer and the feed-forward (FFN) layer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_36",
            "start": 1201,
            "end": 1374,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_36@9",
            "content": "In the t-th iteration, we first update each edge representation by paying attention to the connected nodes, denoted as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_36",
            "start": 1376,
            "end": 1494,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_37@0",
            "content": "\u03b2 t r * = softmax( (r t ij W p )(h t * W h ) T \u221a d ),(4)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_37",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_38@0",
            "content": "p t ij = \u03b2 t ri h t i + \u03b2 t rj h t j ,(5)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_38",
            "start": 0,
            "end": 40,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_39@0",
            "content": "r t+1 ij = FFN(r t ij + p t ij ),(6)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_39",
            "start": 0,
            "end": 35,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_40@0",
            "content": "where h t * can be h t i or h t j . Then, based on the obtained edge representation r t+1 ij , we update the node representations considering both the related nodes and relation edges by the graph attention layer, GAT(h t i , h t j , r t ij ), which is designed as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_40",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_41@0",
            "content": "e t ij = (h t i Wq)(h t j W k +r t+1 ij Wr) T \u221a d ,(7)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_41",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_42@0",
            "content": "\u03b1 t ij = exp(e t ij ) l\u2208N i exp(e t il ) ,(8)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_42",
            "start": 0,
            "end": 44,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_43@0",
            "content": "u t i = j\u2208N i \u03b1 t ij (h t j W v + r t+1 ij ),(9)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_43",
            "start": 0,
            "end": 47,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_44@0",
            "content": "where W q , W k , W r and W v are all learnable parameters, and the \u03b1 t ij is the attention weight between h t i and h t j .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_44",
            "start": 0,
            "end": 123,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_44@1",
            "content": "Besides, to avoid gradient vanishing after several iterations, a residual connection is added to the output u t i and the updated node representations h t+1 i is obtained.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_44",
            "start": 125,
            "end": 295,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_44@2",
            "content": "In this way, the new representation of each keyword node consists of the relation dependency information from neighbor nodes N i .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_44",
            "start": 297,
            "end": 426,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_44@3",
            "content": "We take the node representations from the last iteration as the final keyword representations, denoted as u for brevity.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_44",
            "start": 428,
            "end": 547,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_45@0",
            "content": "Keyword-level Contrast: The keyword-level contrastive learning arises from input keywords against positive output keywords and negative impostor keywords.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_45",
            "start": 0,
            "end": 153,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_45@1",
            "content": "The input keyword u in is extracted from the input text as an anchor, and the output keyword u out is extracted from ground-truth output text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_45",
            "start": 155,
            "end": 296,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_45@2",
            "content": "While the impostor keyword is calculated from the negative neighbours of the output keyword u out , written as u imp = i W i u i , where u i is the representation of keyword node which is obtained by the keyword graph learning procedure described above.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_45",
            "start": 298,
            "end": 550,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_45@3",
            "content": "In this way, with the help of neighbour nodes in the graph, we can obtain a more indistinguishable and difficult negative sample.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_45",
            "start": 552,
            "end": 680,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_45@4",
            "content": "The loss of keyword level contrastive learning thus can be written as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_45",
            "start": 682,
            "end": 751,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_46@0",
            "content": "L keyword = \u2212E[log e h ( u in ,uout)/\u03c4 u * \u2208U e h(u in ,u * )/\u03c4 ],(10)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_46",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_47@0",
            "content": "where u * \u2208 U denotes the positive output keyword u out or imposter keyword u imp .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_47",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_47@1",
            "content": "In keyword-level contrast, h(\u2022) utilizes cosine similarity to calculate the distance between points.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_47",
            "start": 84,
            "end": 183,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_48@0",
            "content": "Mahalanobis Contrastive Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_48",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_49@0",
            "content": "Note that there exists a space gap between the instance-level contrast and the keyword-level contrast, which disturbs the completeness of this hierarchical contrastive architecture.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_49",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_49@1",
            "content": "Besides, the contrastive values vanish when the distance metric is hard to measure the actual discrepancy between positive and negative merely in instance distributions or in keyword representations.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_49",
            "start": 182,
            "end": 380,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_49@2",
            "content": "To mitigate such problems, we design a Mahalanobis contrastive mechanism to correlate the instance distribution and keyword representation, where the objective is to minimize the margin between the output keyword u out and the posterior semantic distribution q \u03d5 (z|x, y) \u225c f \u03d5 (x, y) and maximize the margin between the imposter keyword u imp and the posterior distribution f \u03d5 (x, y):",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_49",
            "start": 382,
            "end": 767,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_50@0",
            "content": "L ma = \u2212E f \u03d5 [log(1 \u2212 e h(f \u03d5 (x,y),uout)/\u03c4",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_50",
            "start": 0,
            "end": 43,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_51@0",
            "content": "u * \u2208U e h(f \u03d5 (x,y),u * )/\u03c4 )], (11) where u * \u2208 U can be the positive output keyword u out or negative imposter keyword u imp .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_51",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_51@1",
            "content": "In Mahalanobis contrast, h(\u2022) utilizes Mahalanobis distance (De Maesschalck et al., 2000) to measure the similarity from keyword point to the instance distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_51",
            "start": 130,
            "end": 293,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_51@2",
            "content": "In the univariate Gaussian case, z \u223c p(z|x, y) = N (\u00b5, \u03c3 2 ), then the h(",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_51",
            "start": 295,
            "end": 367,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_52@0",
            "content": "f \u03d5 (x, y), u * ) \u225c D M A (p \u03b8 (z|x, y)||u * ) = (u * \u2212 \u00b5)\u03c3 2 I(u * \u2212 \u00b5).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_52",
            "start": 0,
            "end": 72,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_53@0",
            "content": "Finally, we equip the CVAE model with the proposed hierarchical contrastive learning framework to unify hybrid granularities by adding L ins , L keyword and L ma to the reconstructed loss of Equation 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_53",
            "start": 0,
            "end": 201,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_54@0",
            "content": "Experiment",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_54",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_55@0",
            "content": "Tasks and Datasets",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_55",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_56@0",
            "content": "We conduct experiments on three public datasets QQP, Douban, RocStories for paraphrasing, dialogue generation, and storytelling task, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_56",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_56@1",
            "content": "The details of the datasets are as follows:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_56",
            "start": 148,
            "end": 190,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_57@0",
            "content": "Dialogue (Douban) Douban (Cai et al., 2020) consists of Chinese daily conversations between pairs of speakers, collected from a popular social network website, Douban group 1 .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_57",
            "start": 0,
            "end": 175,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_57@1",
            "content": "The dataset contains 218,039/10,000/10,000 context-response pairs for training/validation/test, with an average of 3.94 turns per context and 38.32 characters per utterance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_57",
            "start": 177,
            "end": 349,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_57@2",
            "content": "We concatenate historical dialogues and turn it into a single-turn dialogue training corpus.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_57",
            "start": 351,
            "end": 442,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_58@0",
            "content": "Paraphrasing (QQP) QQP (Iyer et al., 2017;) is a dataset published by the community question-answering website Quora on whether a pair of questions is semantically consistent.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_58",
            "start": 0,
            "end": 174,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_58@1",
            "content": "To adapt it to the contrastive learning task, we only keep question pairs that have positive and negative rewriting for the same input.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_58",
            "start": 176,
            "end": 310,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_58@2",
            "content": "Thus, there remain 44,949 samples in the dataset, which are split into training/validation/test sets of 40,441/2,254/2,254 samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_58",
            "start": 312,
            "end": 442,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_59@0",
            "content": "Storytelling (RocStories) RocStories consists of 98,163 high-quality hand-crafted stories, which capture causal and temporal commonsense relations of daily events (Mostafazadeh et al., 2016).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_59",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_59@1",
            "content": "Each story paragraph contains 5 sentences with an average of 43 words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_59",
            "start": 192,
            "end": 261,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_59@2",
            "content": "Following the previous work Yu et al. (2021), we split the dataset into 8:1:1 for training, validation, and test.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_59",
            "start": 263,
            "end": 375,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_60@0",
            "content": "For the above three datasets, in order to construct different levels of contrastive learning, we performed the same preprocessing of extracting keywords.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_60",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_60@1",
            "content": "We utilize the TextRank model (Mihalcea and Tarau, 2004) to extract keywords from each input and output sample, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_60",
            "start": 154,
            "end": 278,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_60@2",
            "content": "Besides, the vocabulary size of both datasets is the same as BERT (Devlin et al., 2018) setting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_60",
            "start": 280,
            "end": 375,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_61@0",
            "content": "Implementation Details",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_61",
            "start": 0,
            "end": 21,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_62@0",
            "content": "Our experiments are implemented in Tensorflow (Abadi et al., 2016) on an NVIDIA Tesla P100 GPU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_62",
            "start": 0,
            "end": 94,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_62@1",
            "content": "For our model and all baselines, we follow the same setting as described below.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_62",
            "start": 96,
            "end": 174,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_62@2",
            "content": "We pad or cut the input to 100, 20, 100 words for dialogue generation, paraphrasing, and storytelling, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_62",
            "start": 176,
            "end": 291,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_62@3",
            "content": "The truncation length is decided based on the observation that there is no significant improvement when increasing input length.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_62",
            "start": 293,
            "end": 420,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_62@4",
            "content": "The minimum decoding step is 5, and the maximum step is 20 for all tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_62",
            "start": 422,
            "end": 494,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_62@5",
            "content": "Experiments were performed with a batch size of 256, and we use Adam optimizer (Kingma and Ba, 2015) as our optimizing algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_62",
            "start": 496,
            "end": 624,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_62@6",
            "content": "During the test stage, the beam-search size is set to 4 for all methods and the checkpoint with the smallest validation loss is chosen.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_62",
            "start": 626,
            "end": 760,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_62@7",
            "content": "Note that for better performance, our model is built based on BERT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_62",
            "start": 762,
            "end": 828,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_62@8",
            "content": "Finally, due to the limitation of time and memory, small settings are used in the pre-training baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_62",
            "start": 830,
            "end": 934,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_63@0",
            "content": "Compared Baselines",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_63",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_64@0",
            "content": "We compare our method against several traditional generation models, pretrained-based generation models, and contrastive learning models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_64",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_65@0",
            "content": "Traditional generation models: (1) CVAE (Zhao et al., 2017) generates sentences based on latent variables, sampling from potential semantic distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_65",
            "start": 0,
            "end": 152,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_65@1",
            "content": "(2) Seq2Seq (Sutskever et al., 2014) is a sequence-to-sequence framework combined with attention mechanism and pointer network. (3) Transformer (Vaswani et al., 2017) is an abstractive generation method based solely on attention mechanisms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_65",
            "start": 154,
            "end": 393,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_66@0",
            "content": "Pretrained-based generation models: (4) Seq2Seq-DU (Feng et al., 2021) is concerned with dialogue state tracking in a task-oriented dialogue system.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_66",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_66@1",
            "content": "( 5) DialoGPT proposes a large, tunable neural conversational response generation model trained on more conversation-like exchanges.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_66",
            "start": 149,
            "end": 280,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_66@2",
            "content": "( 6) BERT-GEN (Devlin et al., 2018) augments Seq2Seq with BERT as the encoder.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_66",
            "start": 282,
            "end": 359,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_66@3",
            "content": "( 7) T5 (Raffel et al., 2020) introduces a unified framework that converts all text-based language problems into a text-to-text format.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_66",
            "start": 361,
            "end": 495,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_67@0",
            "content": "Contrastive learning methods: (8) Groupwise (Cai et al., 2020) augments contrastive dialogue learning with group-wise dual sampling.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_67",
            "start": 0,
            "end": 131,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_67@1",
            "content": "(9) T5-CLAPS (Lee et al., 2021) generates negative and positive samples for contrastive learning by adding small and large perturbations, respectively.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_67",
            "start": 133,
            "end": 283,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_68@0",
            "content": "Evaluation Metrics",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_68",
            "start": 0,
            "end": 17,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_69@0",
            "content": "To evaluate the performance of our model against baselines, we adopt the following metrics widely used in existing studies.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_69",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_70@0",
            "content": "BLEU We utilize BLEU score (Papineni et al., 2002) to measure word overlap between the generated text and the ground-truth.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_70",
            "start": 0,
            "end": 122,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_70@1",
            "content": "Specifically, following the conventional setting of (Gu et al., 2019), we adopt BLEU-1\u223c4 scores under the smoothing techniques (smoothing 7).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_70",
            "start": 124,
            "end": 264,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_71@0",
            "content": "Embedding To evaluate our model more comprehensively, we also capture the semantic matching degrees between the bag-of-words (BOW) embeddings of generated text and reference (Gu et al., 2019).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_71",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_71@1",
            "content": "Particularly we adopt three metrics: 1) Extrema, cosine similarity between the largest extreme values among the word embeddings in the two texts; 2) Average, cosine similarity between the averaged word embeddings of generated text and reference; 3) Greedy, greedily matching words in the two texts based on cosine similarities.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_71",
            "start": 193,
            "end": 519,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_71@2",
            "content": "Seq2Seq and Transformer, and the lower part shows the latest pretrained-based methods including DialoGPT and T5.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_71",
            "start": 521,
            "end": 632,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_71@3",
            "content": "Overall, pretrained-based methods generally outperform traditional methods, and this also proves the effectiveness of the pretrained language model on the generation tasks.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_71",
            "start": 634,
            "end": 805,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_71@4",
            "content": "Secondly, we can find that the performance is significantly improved after adding contrast learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_71",
            "start": 807,
            "end": 906,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_71@5",
            "content": "Finally, our method outperforms T5-CLAPS by 2.7%, 3.6% on QQP, by 20.3%, 24.9% on Douban, and by 3.9%, 6.3% on RocStories in terms of BLEU-1, BLEU-2, respectively, which proves the superiority of our model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_71",
            "start": 908,
            "end": 1113,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_72@0",
            "content": "Experimental Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_72",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_73@0",
            "content": "Overall Performance Automatic Evaluation The experimental results ars summarized in",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_73",
            "start": 0,
            "end": 82,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_74@0",
            "content": "Human Evaluation We also assessed system performance by eliciting human judgments on 100 randomly selected test instances on QQP dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_74",
            "start": 0,
            "end": 136,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_75@0",
            "content": "The annotators are asked to rate paraphrasing questions generated by T5-CLAPS, DialoGPT, Seq2Seq-DU, and our model according to Fluency (Flu), Meaningfulness (Mean), and Differential (Diff).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_75",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_75@1",
            "content": "The rating score ranges from 1 to 3, with 3 being the best.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_75",
            "start": 191,
            "end": 249,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_75@2",
            "content": "instance-level contrastive, the effect of our model is greatly reduced by about 10.4%, which illustrates the desirability of considering the contributions of words in a sentence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_75",
            "start": 251,
            "end": 428,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_75@3",
            "content": "On this basis, adding keyword contrastive learning with removing the keyword graph, the effect of the model has been improved but is still lower than our model by 2.1%.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_75",
            "start": 430,
            "end": 597,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_75@4",
            "content": "This shows that keywords are indeed conducive to capturing important information, and it also illustrates the significance of a keyword graph.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_75",
            "start": 599,
            "end": 740,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_75@5",
            "content": "Finally, the experiment of removing the Mahalanobis contrastive loss indicates that only with granularity independent contrast is not sufficient, and the Mahalanobis contrast plays a critical intermediate role.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_75",
            "start": 742,
            "end": 951,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_76@0",
            "content": "Visualization of Different Levels of Contrastive Learning",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_76",
            "start": 0,
            "end": 56,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_77@0",
            "content": "To study the hierarchical contrastive learning, we visualize the vectors of keyword, input text, positive and negative output text on randomly sampled cases from QQP dataset, as shown in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_77",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_77@1",
            "content": "For visualization purposes, we reduce the dimension of the latent vector with t-SNE (Maaten and Hinton, 2008).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_77",
            "start": 197,
            "end": 306,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_77@2",
            "content": "It can be observed that the input sentence representation is located close to the keyword, which shows that the keyword, as the most important information in the sentence, determines the semantic distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_77",
            "start": 308,
            "end": 516,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_77@3",
            "content": "Moreover, in contrastive learning, it can be seen that after training, the position of the input sentence is close to the positive samples and far away from the negative samples.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_77",
            "start": 518,
            "end": 695,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_77@4",
            "content": "This suggests that contrastive learning can correct the semantic distribution.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_77",
            "start": 697,
            "end": 774,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_78@0",
            "content": "Analysis of Different Keywords",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_78",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_79@0",
            "content": "We finally investigate the influence of sampling different keywords.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_79",
            "start": 0,
            "end": 67,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_79@1",
            "content": "As shown in Table 4, for an input question, we provide keywords extracted by Tex- What are the best online sites or apps with games for learning German?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_79",
            "start": 69,
            "end": 220,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_80@0",
            "content": "Which is the best site to learn German ? tRank and randomly-selected keywords as the condition to control the semantic distribution and examine the quality of the generated text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_80",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_80@1",
            "content": "As the most important information unit, different keywords lead to different semantic distributions and will result in different generated texts.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_80",
            "start": 179,
            "end": 323,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_80@2",
            "content": "The more properly the keywords are selected, the more accurately the sentences will be generated.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_80",
            "start": 325,
            "end": 421,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_80@3",
            "content": "When utilizing the keywords extracted by TextRank as a condition, the information \"belly fat\" is focused during the generation of paraphrasing questions, and the generated sentences are more accurate.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_80",
            "start": 423,
            "end": 622,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_80@4",
            "content": "On the contrary, after adding the random-selected keyword \"disposable\", the generated question emphasizes \"one-off exercise\", which brings incorrect information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_80",
            "start": 624,
            "end": 784,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_81@0",
            "content": "We also compare our model with several baselines in Table 4.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_81",
            "start": 0,
            "end": 59,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_81@1",
            "content": "Most baselines can generate fluent questions in this case.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_81",
            "start": 61,
            "end": 118,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_81@2",
            "content": "However, they focus on \"lose weight\", and miss the significant information \"belly fat\".",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_81",
            "start": 120,
            "end": 206,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_81@3",
            "content": "Based on the above analysis, we can observe that keywords can emphasize and protect the highlight information in sentences, and affect the semantic distribution of as a condition.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_81",
            "start": 208,
            "end": 386,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_82@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_82",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_83@0",
            "content": "In this paper, we propose a hierarchical contrastive learning mechanism, which consists of intra-contrasts within instance-level and keywordlevel and inter-contrast with Mahalanobis contrast.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_83",
            "start": 0,
            "end": 190,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_83@1",
            "content": "The experimental results yield significant outperformance over baselines when applied in the CVAE framework.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_83",
            "start": 192,
            "end": 299,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_83@2",
            "content": "In the future, we aim to extend the contrastive learning mechanism to different basic models, and will explore contrastive learning methods based on external knowledge.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_83",
            "start": 301,
            "end": 468,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_84@0",
            "content": "Mart\u00edn Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Tensorflow: A system for large-scale machine learning, 2016, OSDI, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_84",
            "start": 0,
            "end": 247,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_85@0",
            "content": "Hengyi Cai, Hongshen Chen, Yonghao Song, Zhuoye Ding, Yongjun Bao, Weipeng Yan, Xiaofang Zhao, Group-wise contrastive learning for neural dialogue generation, 2020-11-20, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_85",
            "start": 0,
            "end": 322,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_86@0",
            "content": "UNKNOWN, None, 2000, The mahalanobis distance. Chemometrics and intelligent laboratory systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_86",
            "start": 0,
            "end": 96,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_87@0",
            "content": "UNKNOWN, None, 2018, Improving reconstruction autoencoder outof-distribution detection with mahalanobis distance, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_87",
            "start": 0,
            "end": 114,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_88@0",
            "content": "UNKNOWN, None, 2018, Bert: Pre-training of deep bidirectional transformers for language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_88",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_89@0",
            "content": "UNKNOWN, None, 2021, A sequenceto-sequence approach to dialogue state tracking, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_89",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_90@0",
            "content": "Xiaodong Gu, Kyunghyun Cho, Jung-Woo Ha, Sunghun Kim, DialogWAE: Multimodal response generation with conditional wasserstein autoencoder, 2019, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_90",
            "start": 0,
            "end": 198,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_91@0",
            "content": "UNKNOWN, None, 2017, First quora dataset release: Question pairs, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_91",
            "start": 0,
            "end": 66,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_92@0",
            "content": "UNKNOWN, None, 2015, Adam: A method for stochastic optimization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_92",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_93@0",
            "content": "UNKNOWN, None, 2013, Autoencoding variational bayes, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_93",
            "start": 0,
            "end": 53,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_94@0",
            "content": "UNKNOWN, None, 1951, On information and sufficiency. The annals of mathematical statistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_94",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_95@0",
            "content": "Seanie Lee, Dong Lee, Sung Hwang, Contrastive learning with adversarial perturbations for conditional text generation, 2021-05-03, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_95",
            "start": 0,
            "end": 215,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_96@0",
            "content": "UNKNOWN, None, 2015, A hierarchical neural autoencoder for paragraphs and documents, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_96",
            "start": 0,
            "end": 85,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_97@0",
            "content": "Mingzhe Li, Xiuying Chen, Min Yang, Shen Gao, Dongyan Zhao, Rui Yan, The stylecontent duality of attractiveness: Learning to write eye-catching headlines via disentanglement, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_97",
            "start": 0,
            "end": 244,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_98@0",
            "content": "UNKNOWN, None, 2021, Simcls: A simple framework for contrastive learning of abstractive summarization, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_98",
            "start": 0,
            "end": 103,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_99@0",
            "content": "Laurens Van Der Maaten, Geoffrey Hinton, Visualizing data using t-sne, 2008-11, Journal of machine learning research, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_99",
            "start": 0,
            "end": 118,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_100@0",
            "content": "Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, Kevin Murphy, Generation and comprehension of unambiguous object descriptions, 2016, Proceedings of the IEEE conference on computer vision and pattern recognition, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_100",
            "start": 0,
            "end": 237,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_101@0",
            "content": "Rada Mihalcea, Paul Tarau, Textrank: Bringing order into text, 2004, Proceedings of the 2004 conference on empirical methods in natural language processing, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_101",
            "start": 0,
            "end": 157,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_102@0",
            "content": "Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeff Dean, Distributed representations of words and phrases and their compositionality, 2013, Advances in neural information processing systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_102",
            "start": 0,
            "end": 200,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_103@0",
            "content": "Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen, A corpus and cloze evaluation for deeper understanding of commonsense stories, 2016, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_103",
            "start": 0,
            "end": 356,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_104@0",
            "content": "Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Bleu: a method for automatic evaluation of machine translation, 2002, ACL, ACL.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_104",
            "start": 0,
            "end": 135,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_105@0",
            "content": "Alexander Podolskiy, Dmitry Lipin, Andrey Bout, Ekaterina Artemova, and Irina Piontkovskaya. 2021. Revisiting mahalanobis distance for transformer-based out-of-domain detection, , Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_105",
            "start": 0,
            "end": 243,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_106@0",
            "content": "Lisong Qiu, Juntao Li, Wei Bi, Dongyan Zhao, Rui Yan, Are training samples correlated? learning to generate dialogue responses with multiple references, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_106",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_107@0",
            "content": "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter Liu, Exploring the limits of transfer learning with a unified text-to-text transformer, 2020, J. Mach. Learn. Res, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_107",
            "start": 0,
            "end": 229,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_108@0",
            "content": "UNKNOWN, None, , Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. 2021. A simple fix to mahalanobis distance for improving near-ood detection, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_108",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_109@0",
            "content": "Iulian Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, Yoshua Bengio, A hierarchical latent variable encoder-decoder model for generating dialogues, 2017, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_109",
            "start": 0,
            "end": 258,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_110@0",
            "content": "Ilya Sutskever, Oriol Vinyals, V Quoc,  Le, Sequence to sequence learning with neural networks, 2014-12-08, Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_110",
            "start": 0,
            "end": 222,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_111@0",
            "content": "Thanh Tran, Renee Sweeney, Kyumin Lee, Adversarial mahalanobis distance-based attentive song recommender for automatic playlist continuation, 2019, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_111",
            "start": 0,
            "end": 261,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_112@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin, Attention is all you need, 2017-12-04, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_112",
            "start": 0,
            "end": 272,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_113@0",
            "content": "UNKNOWN, None, 2019, GLUE: A multi-task benchmark and analysis platform for natural language understanding, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_113",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_114@0",
            "content": "UNKNOWN, None, 2019, Guiding variational response generator to exploit persona, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_114",
            "start": 0,
            "end": 80,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_115@0",
            "content": "UNKNOWN, None, 2019, Reducing word omission errors in neural machine translation: A contrastive learning approach, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_115",
            "start": 0,
            "end": 115,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_116@0",
            "content": "Meng-Hsuan Yu, Juntao Li, Zhangming Chan, Dongyan Zhao, Rui Yan, Content learning with structure-aware writing: A graph-infused dual conditional variational autoencoder for automatic storytelling, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_116",
            "start": 0,
            "end": 266,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_117@0",
            "content": "Meng-Hsuan Yu, Juntao Li, Danyang Liu, Dongyan Zhao, Rui Yan, Bo Tang, Haisong Zhang, Draft and edit: Automatic storytelling through multipass hierarchical conditional variational autoencoder, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_117",
            "start": 0,
            "end": 262,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_118@0",
            "content": "Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, Bill Dolan, Dialogpt: Large-scale generative pre-training for conversational response generation, 2020, ACL, system demonstration, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_118",
            "start": 0,
            "end": 239,
            "label": {}
        },
        {
            "ix": "33-ARR_v1_119@0",
            "content": "UNKNOWN, None, 2017, Learning discourse-level diversity for neural dialog models using conditional variational autoencoders, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "33-ARR_v1_119",
            "start": 0,
            "end": 125,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "33-ARR_v1_0",
            "tgt_ix": "33-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_0",
            "tgt_ix": "33-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_1",
            "tgt_ix": "33-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_1",
            "tgt_ix": "33-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_0",
            "tgt_ix": "33-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_2",
            "tgt_ix": "33-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_4",
            "tgt_ix": "33-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_5",
            "tgt_ix": "33-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_6",
            "tgt_ix": "33-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_7",
            "tgt_ix": "33-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_3",
            "tgt_ix": "33-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_3",
            "tgt_ix": "33-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_3",
            "tgt_ix": "33-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_3",
            "tgt_ix": "33-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_3",
            "tgt_ix": "33-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_3",
            "tgt_ix": "33-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_3",
            "tgt_ix": "33-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_8",
            "tgt_ix": "33-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_0",
            "tgt_ix": "33-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_10",
            "tgt_ix": "33-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_10",
            "tgt_ix": "33-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_11",
            "tgt_ix": "33-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_11",
            "tgt_ix": "33-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_10",
            "tgt_ix": "33-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_12",
            "tgt_ix": "33-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_13",
            "tgt_ix": "33-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_13",
            "tgt_ix": "33-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_10",
            "tgt_ix": "33-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_14",
            "tgt_ix": "33-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_15",
            "tgt_ix": "33-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_15",
            "tgt_ix": "33-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_0",
            "tgt_ix": "33-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_16",
            "tgt_ix": "33-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_18",
            "tgt_ix": "33-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_19",
            "tgt_ix": "33-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_21",
            "tgt_ix": "33-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_22",
            "tgt_ix": "33-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_23",
            "tgt_ix": "33-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_24",
            "tgt_ix": "33-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_25",
            "tgt_ix": "33-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_26",
            "tgt_ix": "33-ARR_v1_27",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_20",
            "tgt_ix": "33-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_27",
            "tgt_ix": "33-ARR_v1_28",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_28",
            "tgt_ix": "33-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_28",
            "tgt_ix": "33-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_29",
            "tgt_ix": "33-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_31",
            "tgt_ix": "33-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_32",
            "tgt_ix": "33-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_30",
            "tgt_ix": "33-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_30",
            "tgt_ix": "33-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_30",
            "tgt_ix": "33-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_30",
            "tgt_ix": "33-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_33",
            "tgt_ix": "33-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_35",
            "tgt_ix": "33-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_37",
            "tgt_ix": "33-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_38",
            "tgt_ix": "33-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_39",
            "tgt_ix": "33-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_40",
            "tgt_ix": "33-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_41",
            "tgt_ix": "33-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_42",
            "tgt_ix": "33-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_43",
            "tgt_ix": "33-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_44",
            "tgt_ix": "33-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_45",
            "tgt_ix": "33-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_46",
            "tgt_ix": "33-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_47",
            "tgt_ix": "33-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_49",
            "tgt_ix": "33-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_50",
            "tgt_ix": "33-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_51",
            "tgt_ix": "33-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_52",
            "tgt_ix": "33-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_48",
            "tgt_ix": "33-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_48",
            "tgt_ix": "33-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_48",
            "tgt_ix": "33-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_48",
            "tgt_ix": "33-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_48",
            "tgt_ix": "33-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_48",
            "tgt_ix": "33-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_0",
            "tgt_ix": "33-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_53",
            "tgt_ix": "33-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_54",
            "tgt_ix": "33-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_54",
            "tgt_ix": "33-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_56",
            "tgt_ix": "33-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_57",
            "tgt_ix": "33-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_58",
            "tgt_ix": "33-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_59",
            "tgt_ix": "33-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_55",
            "tgt_ix": "33-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_55",
            "tgt_ix": "33-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_55",
            "tgt_ix": "33-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_55",
            "tgt_ix": "33-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_55",
            "tgt_ix": "33-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_55",
            "tgt_ix": "33-ARR_v1_56",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_54",
            "tgt_ix": "33-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_60",
            "tgt_ix": "33-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_61",
            "tgt_ix": "33-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_61",
            "tgt_ix": "33-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_54",
            "tgt_ix": "33-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_62",
            "tgt_ix": "33-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_64",
            "tgt_ix": "33-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_65",
            "tgt_ix": "33-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_66",
            "tgt_ix": "33-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_63",
            "tgt_ix": "33-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_63",
            "tgt_ix": "33-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_63",
            "tgt_ix": "33-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_63",
            "tgt_ix": "33-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_63",
            "tgt_ix": "33-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_54",
            "tgt_ix": "33-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_67",
            "tgt_ix": "33-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_69",
            "tgt_ix": "33-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_70",
            "tgt_ix": "33-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_68",
            "tgt_ix": "33-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_68",
            "tgt_ix": "33-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_68",
            "tgt_ix": "33-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_68",
            "tgt_ix": "33-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_54",
            "tgt_ix": "33-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_71",
            "tgt_ix": "33-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_54",
            "tgt_ix": "33-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_72",
            "tgt_ix": "33-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_74",
            "tgt_ix": "33-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_73",
            "tgt_ix": "33-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_73",
            "tgt_ix": "33-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_73",
            "tgt_ix": "33-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_54",
            "tgt_ix": "33-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_75",
            "tgt_ix": "33-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_76",
            "tgt_ix": "33-ARR_v1_77",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_76",
            "tgt_ix": "33-ARR_v1_77",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_54",
            "tgt_ix": "33-ARR_v1_78",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_77",
            "tgt_ix": "33-ARR_v1_78",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_79",
            "tgt_ix": "33-ARR_v1_80",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_80",
            "tgt_ix": "33-ARR_v1_81",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_78",
            "tgt_ix": "33-ARR_v1_79",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_78",
            "tgt_ix": "33-ARR_v1_80",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_78",
            "tgt_ix": "33-ARR_v1_81",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_78",
            "tgt_ix": "33-ARR_v1_79",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_0",
            "tgt_ix": "33-ARR_v1_82",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_81",
            "tgt_ix": "33-ARR_v1_82",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_82",
            "tgt_ix": "33-ARR_v1_83",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_82",
            "tgt_ix": "33-ARR_v1_83",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "33-ARR_v1_0",
            "tgt_ix": "33-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_1",
            "tgt_ix": "33-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_2",
            "tgt_ix": "33-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_2",
            "tgt_ix": "33-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_2",
            "tgt_ix": "33-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_2",
            "tgt_ix": "33-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_2",
            "tgt_ix": "33-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_2",
            "tgt_ix": "33-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_2",
            "tgt_ix": "33-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_3",
            "tgt_ix": "33-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_4",
            "tgt_ix": "33-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_4",
            "tgt_ix": "33-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_4",
            "tgt_ix": "33-ARR_v1_4@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_5",
            "tgt_ix": "33-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_6",
            "tgt_ix": "33-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_6",
            "tgt_ix": "33-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_6",
            "tgt_ix": "33-ARR_v1_6@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_6",
            "tgt_ix": "33-ARR_v1_6@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_6",
            "tgt_ix": "33-ARR_v1_6@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_6",
            "tgt_ix": "33-ARR_v1_6@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_6",
            "tgt_ix": "33-ARR_v1_6@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_6",
            "tgt_ix": "33-ARR_v1_6@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_7",
            "tgt_ix": "33-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_7",
            "tgt_ix": "33-ARR_v1_7@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_7",
            "tgt_ix": "33-ARR_v1_7@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_8",
            "tgt_ix": "33-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_9",
            "tgt_ix": "33-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_10",
            "tgt_ix": "33-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_11",
            "tgt_ix": "33-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_12",
            "tgt_ix": "33-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_12",
            "tgt_ix": "33-ARR_v1_12@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_12",
            "tgt_ix": "33-ARR_v1_12@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_12",
            "tgt_ix": "33-ARR_v1_12@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_12",
            "tgt_ix": "33-ARR_v1_12@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_12",
            "tgt_ix": "33-ARR_v1_12@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_12",
            "tgt_ix": "33-ARR_v1_12@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_12",
            "tgt_ix": "33-ARR_v1_12@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_13",
            "tgt_ix": "33-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_14",
            "tgt_ix": "33-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_14",
            "tgt_ix": "33-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_14",
            "tgt_ix": "33-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_14",
            "tgt_ix": "33-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_14",
            "tgt_ix": "33-ARR_v1_14@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_15",
            "tgt_ix": "33-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_16",
            "tgt_ix": "33-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_16",
            "tgt_ix": "33-ARR_v1_16@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_16",
            "tgt_ix": "33-ARR_v1_16@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_17",
            "tgt_ix": "33-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_18",
            "tgt_ix": "33-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_18",
            "tgt_ix": "33-ARR_v1_18@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_18",
            "tgt_ix": "33-ARR_v1_18@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_18",
            "tgt_ix": "33-ARR_v1_18@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_19",
            "tgt_ix": "33-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_20",
            "tgt_ix": "33-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_21",
            "tgt_ix": "33-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_21",
            "tgt_ix": "33-ARR_v1_21@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_22",
            "tgt_ix": "33-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_23",
            "tgt_ix": "33-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_24",
            "tgt_ix": "33-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_25",
            "tgt_ix": "33-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_26",
            "tgt_ix": "33-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_26",
            "tgt_ix": "33-ARR_v1_26@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_27",
            "tgt_ix": "33-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_28",
            "tgt_ix": "33-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_29",
            "tgt_ix": "33-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_30",
            "tgt_ix": "33-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_31",
            "tgt_ix": "33-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_31",
            "tgt_ix": "33-ARR_v1_31@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_31",
            "tgt_ix": "33-ARR_v1_31@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_31",
            "tgt_ix": "33-ARR_v1_31@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_31",
            "tgt_ix": "33-ARR_v1_31@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_31",
            "tgt_ix": "33-ARR_v1_31@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_32",
            "tgt_ix": "33-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_33",
            "tgt_ix": "33-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_33",
            "tgt_ix": "33-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_34",
            "tgt_ix": "33-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_35",
            "tgt_ix": "33-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_36@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_36@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_36@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_36@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_36@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_36@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_36@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_36@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_36",
            "tgt_ix": "33-ARR_v1_36@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_37",
            "tgt_ix": "33-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_38",
            "tgt_ix": "33-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_39",
            "tgt_ix": "33-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_40",
            "tgt_ix": "33-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_41",
            "tgt_ix": "33-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_42",
            "tgt_ix": "33-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_43",
            "tgt_ix": "33-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_44",
            "tgt_ix": "33-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_44",
            "tgt_ix": "33-ARR_v1_44@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_44",
            "tgt_ix": "33-ARR_v1_44@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_44",
            "tgt_ix": "33-ARR_v1_44@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_45",
            "tgt_ix": "33-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_45",
            "tgt_ix": "33-ARR_v1_45@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_45",
            "tgt_ix": "33-ARR_v1_45@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_45",
            "tgt_ix": "33-ARR_v1_45@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_45",
            "tgt_ix": "33-ARR_v1_45@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_46",
            "tgt_ix": "33-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_47",
            "tgt_ix": "33-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_47",
            "tgt_ix": "33-ARR_v1_47@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_48",
            "tgt_ix": "33-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_49",
            "tgt_ix": "33-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_49",
            "tgt_ix": "33-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_49",
            "tgt_ix": "33-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_50",
            "tgt_ix": "33-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_51",
            "tgt_ix": "33-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_51",
            "tgt_ix": "33-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_51",
            "tgt_ix": "33-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_52",
            "tgt_ix": "33-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_53",
            "tgt_ix": "33-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_54",
            "tgt_ix": "33-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_55",
            "tgt_ix": "33-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_56",
            "tgt_ix": "33-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_56",
            "tgt_ix": "33-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_57",
            "tgt_ix": "33-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_57",
            "tgt_ix": "33-ARR_v1_57@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_57",
            "tgt_ix": "33-ARR_v1_57@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_58",
            "tgt_ix": "33-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_58",
            "tgt_ix": "33-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_58",
            "tgt_ix": "33-ARR_v1_58@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_59",
            "tgt_ix": "33-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_59",
            "tgt_ix": "33-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_59",
            "tgt_ix": "33-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_60",
            "tgt_ix": "33-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_60",
            "tgt_ix": "33-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_60",
            "tgt_ix": "33-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_61",
            "tgt_ix": "33-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_62",
            "tgt_ix": "33-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_62",
            "tgt_ix": "33-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_62",
            "tgt_ix": "33-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_62",
            "tgt_ix": "33-ARR_v1_62@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_62",
            "tgt_ix": "33-ARR_v1_62@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_62",
            "tgt_ix": "33-ARR_v1_62@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_62",
            "tgt_ix": "33-ARR_v1_62@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_62",
            "tgt_ix": "33-ARR_v1_62@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_62",
            "tgt_ix": "33-ARR_v1_62@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_63",
            "tgt_ix": "33-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_64",
            "tgt_ix": "33-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_65",
            "tgt_ix": "33-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_65",
            "tgt_ix": "33-ARR_v1_65@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_66",
            "tgt_ix": "33-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_66",
            "tgt_ix": "33-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_66",
            "tgt_ix": "33-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_66",
            "tgt_ix": "33-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_67",
            "tgt_ix": "33-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_67",
            "tgt_ix": "33-ARR_v1_67@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_68",
            "tgt_ix": "33-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_69",
            "tgt_ix": "33-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_70",
            "tgt_ix": "33-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_70",
            "tgt_ix": "33-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_71",
            "tgt_ix": "33-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_71",
            "tgt_ix": "33-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_71",
            "tgt_ix": "33-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_71",
            "tgt_ix": "33-ARR_v1_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_71",
            "tgt_ix": "33-ARR_v1_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_71",
            "tgt_ix": "33-ARR_v1_71@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_72",
            "tgt_ix": "33-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_73",
            "tgt_ix": "33-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_74",
            "tgt_ix": "33-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_75",
            "tgt_ix": "33-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_75",
            "tgt_ix": "33-ARR_v1_75@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_75",
            "tgt_ix": "33-ARR_v1_75@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_75",
            "tgt_ix": "33-ARR_v1_75@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_75",
            "tgt_ix": "33-ARR_v1_75@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_75",
            "tgt_ix": "33-ARR_v1_75@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_76",
            "tgt_ix": "33-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_77",
            "tgt_ix": "33-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_77",
            "tgt_ix": "33-ARR_v1_77@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_77",
            "tgt_ix": "33-ARR_v1_77@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_77",
            "tgt_ix": "33-ARR_v1_77@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_77",
            "tgt_ix": "33-ARR_v1_77@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_78",
            "tgt_ix": "33-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_79",
            "tgt_ix": "33-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_79",
            "tgt_ix": "33-ARR_v1_79@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_80",
            "tgt_ix": "33-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_80",
            "tgt_ix": "33-ARR_v1_80@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_80",
            "tgt_ix": "33-ARR_v1_80@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_80",
            "tgt_ix": "33-ARR_v1_80@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_80",
            "tgt_ix": "33-ARR_v1_80@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_81",
            "tgt_ix": "33-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_81",
            "tgt_ix": "33-ARR_v1_81@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_81",
            "tgt_ix": "33-ARR_v1_81@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_81",
            "tgt_ix": "33-ARR_v1_81@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_82",
            "tgt_ix": "33-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_83",
            "tgt_ix": "33-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_83",
            "tgt_ix": "33-ARR_v1_83@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_83",
            "tgt_ix": "33-ARR_v1_83@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_84",
            "tgt_ix": "33-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_85",
            "tgt_ix": "33-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_86",
            "tgt_ix": "33-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_87",
            "tgt_ix": "33-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_88",
            "tgt_ix": "33-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_89",
            "tgt_ix": "33-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_90",
            "tgt_ix": "33-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_91",
            "tgt_ix": "33-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_92",
            "tgt_ix": "33-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_93",
            "tgt_ix": "33-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_94",
            "tgt_ix": "33-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_95",
            "tgt_ix": "33-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_96",
            "tgt_ix": "33-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_97",
            "tgt_ix": "33-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_98",
            "tgt_ix": "33-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_99",
            "tgt_ix": "33-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_100",
            "tgt_ix": "33-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_101",
            "tgt_ix": "33-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_102",
            "tgt_ix": "33-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_103",
            "tgt_ix": "33-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_104",
            "tgt_ix": "33-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_105",
            "tgt_ix": "33-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_106",
            "tgt_ix": "33-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_107",
            "tgt_ix": "33-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_108",
            "tgt_ix": "33-ARR_v1_108@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_109",
            "tgt_ix": "33-ARR_v1_109@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_110",
            "tgt_ix": "33-ARR_v1_110@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_111",
            "tgt_ix": "33-ARR_v1_111@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_112",
            "tgt_ix": "33-ARR_v1_112@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_113",
            "tgt_ix": "33-ARR_v1_113@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_114",
            "tgt_ix": "33-ARR_v1_114@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_115",
            "tgt_ix": "33-ARR_v1_115@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_116",
            "tgt_ix": "33-ARR_v1_116@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_117",
            "tgt_ix": "33-ARR_v1_117@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_118",
            "tgt_ix": "33-ARR_v1_118@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "33-ARR_v1_119",
            "tgt_ix": "33-ARR_v1_119@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1179,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "33-ARR",
        "version": 1
    }
}