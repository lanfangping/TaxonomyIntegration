{
    "nodes": [
        {
            "ix": "315-ARR_v1_0",
            "content": "Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints",
            "ntype": "article-title",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_1",
            "content": "Abstract",
            "ntype": "abstract",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_2",
            "content": "Lexically constrained neural machine translation (NMT) draws much industrial attention for its practical usage in specific domains. However, current autoregressive approaches suffer from high latency. In this paper, we focus on non-autoregressive translation (NAT) for this problem for its efficiency advantage. We identify that current constrained NAT models, which are based on iterative editing, do not handle low-frequency constraints well. To this end, we propose a plug-in algorithm for this line of work, i.e., Aligned Constrained Training (ACT), which alleviates this problem by familiarizing the model with the source-side context of the constraints. Experiments on the general and domain datasets show that our model improves over the backbone constrained NAT model in constraint preservation and translation quality, especially for rare constraints. 1",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_3",
            "content": "Introduction",
            "ntype": "title",
            "meta": {
                "section": "1"
            }
        },
        {
            "ix": "315-ARR_v1_4",
            "content": "Despite the success of neural machine translation (NMT) (Bahdanau et al., 2015;Vaswani et al., 2017;Barrault et al., 2020), real applications usually require the precise (if not exact) translation of specific terms. One popular solution is to incorporate dictionaries of pre-defined terminologies as lexical constraints to ensure the correct translation of terms, which has been demonstrated to be effective in many areas such as domain adaptation, interactive translation, etc.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_5",
            "content": "Previous methods on lexically constrained translation are mainly built upon Autoregressive Translation (AT) models, imposing constraints at inference-time (Ture et al., 2012;Hokamp and Liu, 2017;Post and Vilar, 2018) or training-time (Luong et al., 2015;Ailem et al., 2021). However, such methods either are time-consuming in real-time applications or do not ensure the appearance of constraints in the output. To develop faster MT models for industrial applications, Non-Autoregressive 1 Code will be released upon publication. \u21d2 incomplete sentence",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_6",
            "content": "Table 1: Translation examples of a lexically constrained non-autoregressive translation (NAT) model (Gu et al., 2019) under a low-frequency word as constraint. The underbraced word frequencies (uncased) are calculated from the vast WMT14 English-German translation (En-De) datasets (Vaswani et al., 2017).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_7",
            "content": "Translation (NAT) has been put forth (Gu et al., 2018;Ghazvininejad et al., 2019;Gu et al., 2019;Qian et al., 2021), which aims to generate tokens in parallel, boosting inference efficiency compared with left-to-right autoregressive decoding.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_8",
            "content": "Researches on lexically constrained NAT are relatively under-explored. Recent studies (Susanto et al., 2020;Xu and Carpuat, 2021) impose lexical constraints at inference time upon editing-based iterative NAT models, where constraint tokens are set as the initial sequence for further editing. However, such methods are vulnerable when encountered with low-frequency words as constraints. As illustrated in Table 1, when translated with a rare constraint, the model is unable to generate the correct context of the term \"geschrien\" as if it does not understand the constraint at all. It is dangerous since terms in specific domains are usually lowfrequency words. We argue that the main reasons behind this problem are 1) the inconsistency between training and constrained inference and 2) the unawareness of the source-side context of the constraints.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_9",
            "content": "To solve this problem, we build our algorithm based on the idea that the context of a rare constraint tends not to be rare as well, i.e., \"a stranger's neighbors are not necessarily strangers\", as demonstrated in Table 1. We believe that, when the constraint is aligned to the source text, the context of its source-side counterpart can be utilized to be translated into the context of the target-side constraint, even if the constraint itself is rare. Also, when enforced to learn to preserve designated constraints at training-time, a model should be better at coping with constraints during inference-time.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_10",
            "content": "Driven by these motivations, we propose a plugin algorithm to improve constrained NAT, namely Aligned Constrained Training (ACT). ACT extends the family of editing-based iterative NAT (Gu et al., 2019;Susanto et al., 2020;Xu and Carpuat, 2021), the current paradigm of constrained NAT. Specifically, ACT is composed of two major components: Constrained Training and Alignment Prompting. The former extends regular training of iterative NAT with pseudo training-time constraints into the state transition of imitation learning. The latter incorporates source alignment information of constraints into training and inference, indicating the context of the potentially rare terms.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_11",
            "content": "In summary, this work makes the following contributions: 1) We identify and analyse the problems w.r.t. rare lexical constraints in current constrained NAT methods; 2) We propose a plug-in algorithm for current constrained NAT models, i.e., aligned constrained training, to improve the translation under rare constraints; 3) Experiments show that our approach improves the backbone model w.r.t. constraint preservation and translation quality, especially for rare constraints.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_12",
            "content": "Related Work",
            "ntype": "title",
            "meta": {
                "section": "2"
            }
        },
        {
            "ix": "315-ARR_v1_13",
            "content": "Lexically Constrained Translation Existing translation methods impose lexical constraints during either inference or training. At training time, constrained MT models include code-switching data augmentation (Dinu et al., 2019;Song et al., 2019; and training with auxiliary tasks such as token or span-level mask-prediction (Ailem et al., 2021;Lee et al., 2021). At inference time, autoregressive constrained decoding algorithms include utilizing placeholder tag (Luong et al., 2015;Crego et al., 2016), grid beam search (Hokamp and Liu, 2017;Post and Vilar, 2018) and alignment-enhanced decoding (Alkhouli et al., 2018;Song et al., 2020;Chen et al., 2021). For the purpose of efficiency, recent studies also focus on non-autoregressive constrained translation. Susanto et al. (2020) proposes to modify the inference procedure of Levenshtein Transformer (Gu et al., 2019) where they disallow the deletion of constraint words during iterative editing. Xu and Carpuat (2021) further develops this idea and introduces a reposition operation that can reorder the constraint tokens. Our work absorbs the idea of both lines of work. Based on NAT methods, we brings alignment information by terminologies to help learn the contextual information for lexical constraints, especially the rare ones.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_14",
            "content": "Non-Autoregressive Translation Although enjoy the speed advantage, NAT models suffer from performance degradation due to the multi-modality problem, i.e., generating text when multiple translations are plausible. Gu et al. (2018) applies sequence-level knowledge distillation (KD) (Kim and Rush, 2016) that uses an AT's output as an NAT's new target, which reduces word diversity and reordering complexity in reference, resulting in fewer modes (Zhou et al., 2020;Xu et al., 2021). Various algorithms have also been proposed to alleviate this problem, including incorporating latent variables (Kaiser et al., 2018;Shu et al., 2020), iterative refinement (Ghazvininejad et al., 2019;Stern et al., 2019;Gu et al., 2019;Guo et al., 2020), advanced training objective Du et al., 2021) and gradually learning targetside word inter-dependency by curriculum learning (Qian et al., 2021). Our work extends the family of editing-based iterative NAT models for its flexibility to impose lexical constraints (Susanto et al., 2020;Xu and Carpuat, 2021).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_15",
            "content": "Background",
            "ntype": "title",
            "meta": {
                "section": "3"
            }
        },
        {
            "ix": "315-ARR_v1_16",
            "content": "Non-Autoregressive Translation",
            "ntype": "title",
            "meta": {
                "section": "3.1"
            }
        },
        {
            "ix": "315-ARR_v1_17",
            "content": "Given a source sentence as x and a target sentence as y = {y 1 , \u2022 \u2022 \u2022 , y n }, an AT model generates in a left-to-right order, i.e., generating y t by conditioning on x and y <t . An NAT model (Gu et al., 2018), however, discards the word inter-dependency in output tokens, with the conditional independent probability distribution modeled as:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_18",
            "content": "P (y|x) = n t=1 P (y t |x).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_19",
            "content": "(1)",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_20",
            "content": "Such factorization is featured with high efficiency at the cost of performance drop in translation tasks due to the multi-modality problem, i.e., translating in mixed modes and resulting in token repetition, missing, or incoherence.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_21",
            "content": "Editing-based Iterative NAT",
            "ntype": "title",
            "meta": {
                "section": "3.2"
            }
        },
        {
            "ix": "315-ARR_v1_22",
            "content": "For NATs, iterative refinement by editing is an NAT paradigm that suits constrained translations due to its flexibility. It alleviates the multi-modality problem by being autoregressive in editing previously generated sequences while maintaining nonautoregressiveness within each iteration. Thus, it achieves better performance than fully NATs while is faster than ATs.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_23",
            "content": "Levenshtein Transformer To better illustrate our idea, we use Levenshtein Transformer (LevT, Gu et al., 2019) as the backbone model in this work, which is a representative model for constrained NAT based on iterative editing.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_24",
            "content": "LevT is based on the Transformer architecture (Vaswani et al., 2017), but more flexible and fast than autoregressive ones. It models the generation of sentences as Markov Decision Process (MDP) defined by a tuple (Y, A, E, R, y 0 ). At each decoding iteration, the agent E receives an input y \u2208 Y, chooses an action a \u2208 A and gets reward r. Y is a set of discrete sentences and R is the reward function. y 0 \u2208 A is the initial sentence to be edited.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_25",
            "content": "Each iteration consists of two basic operations, i.e., deletion and insertion, which is described in Table 2. For the k-th iteration of the sentence y k = (<s>, y 1 , ..., y n , </s>), the insertion consists of placeholder and token classifiers, and the deletion is achieved by a deletion classifier. LevT trains the model with imitation learning to insert and delete, which lets the agent imitate the behaviors drawn from the expert policy:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_26",
            "content": "\u2022 Learning to insert: edit to reference by inserting tokens from a fragmented sentence (e.g., random deletion of reference). (Dinu et al., 2019). \u2022 Learning to delete: delete from the insertion result of the current training status to the reference.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_27",
            "content": "[0 % ,1 0% ) [1 0% ,3 0% ) [3 0% ,5 0% ) [5 0% ,7 0% ) [7 0% ,9 0% ) [9 0% ,1 00 % ]",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_28",
            "content": "The key idea is to learn how to edit from a ground truth after adding noise or the output of an adversary policy to the reference. The ground truth of the editing process is derived from the Levenshtein distance (Levenshtein, 1965).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_29",
            "content": "Lexically Constrained Inference Lexical constraints can be imposed upon a translation model in: 1) soft constraints: allowing the constraints not to appear in the translation; and 2) hard constraints: forcing the constraints to appear in the translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_30",
            "content": "In NAT, the constraints are generally incorporated at inference time. Susanto et al. (2020) injects constraints as the initial sequence for iterative editing in Levenshtein Transformer (LevT, Gu et al., 2019), achieving soft constrained translation. And hard constrained translation can be easily done by disallowing the deletion of the constraints. Xu and Carpuat (2021) alters the deletion action in LevT with the reposition operation, allowing the reordering of multiple constraints.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_31",
            "content": "Motivating Study: Self-Constrained Translation",
            "ntype": "title",
            "meta": {
                "section": "3.3"
            }
        },
        {
            "ix": "315-ARR_v1_32",
            "content": "According to Table 1, constrained NAT models seem to suffer from the low-frequency of lexical constraints, which is dangerous as most terms in practice are rare. To further explore the impact of constraint frequency upon NATs, we conduct a preliminary analysis on constrained LevT (Susanto et al., 2020). We sort words in each reference text based on frequency, dividing them into six buckets by frequency order (as in Figure 1), and sample a word from each bucket as lexical constraints for translation. We denote these constraints as selfconstraints. In this way, we have six times the data, and the six samples derived from one raw sample only differ in the lexical constraints. As shown in Figure 1, translation performance generally keeps improving as the self-constraint gets rarer. This is because setting low-frequency words in a sentence as constraints, which are often hard to translate, actually lightens the load of an NAT model. However, there are two noticeable performance drops around relative frequency ranges of 10%-30% and 90%-100%, denoted as Drop#1 (-0.3 BLEU) and Drop#2 (-0.6 BLEU). Drop#1 is probably because the constraint words within this range are mostly functional or less important. Such words are not as universal as ones at the leftmost that can fit in most contexts and do not have to appear in the target due to multiple modes in translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_33",
            "content": "However, we are more interested in the reasons for Drop#2 when constraints are low-frequency words. We assume a trade-off in self-constrained NAT: the model does not have to translate rare words as they are set as an initial sequence (constraints), but it will have a hard time understanding the context of the rare constraint due to 1) the rareness itself and 2) the lack of the alignment information between target-side constraint tokens and source tokens. Thus, the model does not know how many tokens should be inserted to the left and right of the constraint, which is consistent with the findings in Table 1.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_34",
            "content": "Proposed Approach",
            "ntype": "title",
            "meta": {
                "section": "4"
            }
        },
        {
            "ix": "315-ARR_v1_35",
            "content": "The findings and assumptions discussed above motivate us to propose a plug-in algorithm for lexically constrained NAT models, i.e., Aligned Constrained Training (ACT). ACT is designed based on two major ideas: 1) Constrained Training: bridging the discrepancy between training and constrained inference; 2) Alignment Prompting: helping the model understand the context of the constraints.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_36",
            "content": "Constrained Training",
            "ntype": "title",
            "meta": {
                "section": "4.1"
            }
        },
        {
            "ix": "315-ARR_v1_37",
            "content": "As introduced in \u00a73.2, constraints are typically imposed during inference time in NAT (Susanto et al., 2020;Xu and Carpuat, 2021). Specifically, lexical constraints are imposed by setting the initial sequence y 0 as (<S>, C 1 , C 2 , ..., C k , </S>), where C i = (c 1 , c 2 , ..., c l ) is the i-th lexical constrained word, l is the number of tokens in the i-th con-straint, and k is the number of constraints.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_38",
            "content": "However, such mandatory preservation of the constraints is not carried out during training. During imitation learning, random deletion is applied for ground-truth y * to get the incomplete sentences y \u2032 , producing the data samples for expert policies of how to insert from y \u2032 to y * . This leads to a situation where the model does not learn to preserve fixed tokens and organize the translation around the tokens. Such discrepancy could harm the applications of soft constrained translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_39",
            "content": "To solve this problem, we propose a simple but effective Constrained Training (CT) algorithm. We first build pseudo terms from the target by sampling 0-3 words from reference as the pre-defined constraints for training. 2 Afterward, we disallow the deletion of pseudo term tokens during building data samples for imitation learning. This encourages the model to edit incomplete sentences containing lexical constraints into complete ones, bridging the gap between training and inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_40",
            "content": "Alignment Prompting",
            "ntype": "title",
            "meta": {
                "section": "4.2"
            }
        },
        {
            "ix": "315-ARR_v1_41",
            "content": "As stated in \u00a73.3, we assume the rareness of constraints hinders the model to insert proper tokens of its contexts (i.e., a stranger's neighbors are also strangers). To make the matter worse, previous research (Ding et al., 2021) has also shown that lexical choice errors on low-frequency words tend to be propagated from the teacher (an AT model) to the student (an NAT model) in knowledge distillation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_42",
            "content": "However, terminologies, by nature, provide hard alignment information for source and target which the model can conveniently utilize. Thus, on top of constrained training, we propose an enhanced approach named Aligned Constrained Training (ACT). We propose to directly align the target-side constraints with the source words and prompt the alignment information to the model during both training and inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_43",
            "content": "Building Alignment for Constraints We first align the source words to the target-side constraints, which are either pseudo constraints during training or actual constraints during inference. For each translated sentence constraints et al., 1993;Och and Ney, 2003), to find the corresponding source words, denoted as",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_44",
            "content": "C tgt = (C 1 , C 2 , ..., C k ),",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_45",
            "content": "C src = (C \u2032 1 , C \u2032 2 , ..., C \u2032 k ).",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_46",
            "content": "Prompting Alignment into LevT The encoder in LevT, besides token embedding and position embedding, is further added with a learnable alignment embedding that comes from C src and C tgt . We set the alignment value for each token in C \u2032 i to i and the others to 0, which are further encoded into embeddings. The prompting of alignment is not limited to training, as we also add such alignment embeddings to source tokens aligned to target-side constraints during inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_47",
            "content": "Experiments",
            "ntype": "title",
            "meta": {
                "section": "5"
            }
        },
        {
            "ix": "315-ARR_v1_48",
            "content": "Data and Evaluation",
            "ntype": "title",
            "meta": {
                "section": "5.1"
            }
        },
        {
            "ix": "315-ARR_v1_49",
            "content": "Parallel Data and Knowledge Distillation We consider the English\u2192German (En\u2192De) translation task and train all of the MT models on WMT14 En-De (3,961K sentence pairs), a benchmark translation dataset. All sentences are pre-processed via byte-pair encoding (BPE) (Sennrich et al., 2016) into sub-word units. Following the common practice of training an NAT model, we use the sentencelevel knowledge distillation data generated by a Transformer, (Vaswani et al., 2017) provided by Kasai et al. (2020).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_50",
            "content": "Given models trained on the above-mentioned training sets, we evaluate them on the test sets of several lexically constrained translation datasets. These test sets are categorized into two types of standard lexically constrained translation datasets: 1) Type#1: tasks from WMT14 (Vaswani et al., 2017) and WMT17 (Bojar et al., 2017), which are of the same general domain (news) as training sets; 2) Type#2: tasks from OPUS (Tiedemann, 2012) that are of specific domains (medical and law). Particularly, the real application scenarios of lexically constrained MT models are usually domain-specific, and the constrained words in these domain datasets are relatively less frequent and more important.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_51",
            "content": "Following previous work (Dinu et al., 2019;Susanto et al., 2020;Xu and Carpuat, 2021), the lexical constraints in Type#1 tasks are extracted from existing terminology databases such as Interactive Terminology for Europe (IATE) 3 and Wiktionary (WIKT) 4 accordingly. The OPUS-EMEA (medical domain) and OPUS-JRC (legal domain) in Type#2 tasks are datasets from OPUS. The constraints are extracted by randomly sampling 1 to 3 words from the reference (Post and Vilar, 2018). These constraints are then tokenized with BPE, yielding a larger number of tokens as constraints. The statistical report is shown in Table 3, indicating the frequencies of Type#2 datasets are generally much lower than Type#1 ones.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_52",
            "content": "Evaluation Metrics We use BLEU (Papineni et al., 2002) for estimating the general quality of translation. We also use Term Usage Rate (Term%, Dinu et al., 2019;Susanto et al., 2020;Lee et al., 2021) to evaluate lexically constrained translation, which is the ratio of term constraints appearing in the translated text.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_53",
            "content": "Models",
            "ntype": "title",
            "meta": {
                "section": "5.2"
            }
        },
        {
            "ix": "315-ARR_v1_54",
            "content": "We use Levenshtein Transformer (LevT, Gu et al., 2019) as the backbone model to ACT algorithm for constrained NAT. We compare our approach with a series of previous MT models on applying lexical constraints:",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_55",
            "content": "\u2022 Transformer (Vaswani et al., 2017), set as the AT baseline; \u2022 Dynamic Beam Allocation (DBA) (Post and Vilar, 2018) for constrained decoding with dynamic beam allocation over Transformer; \u2022 Train-by-sep (Dinu et al., 2019), trained on augmented code-switched data by replacing the source terms with target constraints or append on source terms during training; \u2022 Constrained LevT (Susanto et al., 2020), which develops LevT (Gu et al., 2019) \u2022 EDITOR (Xu and Carpuat, 2021), a variant of LevT, replacing the delete action with a reposition action.",
            "ntype": "list",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_56",
            "content": "Implementation Details We use and extend the FairSeq framework (Ott et al., 2019) for training our models. We keep mostly the default parameters of FairSeq, such as setting d model = 512, d hidden = 2,048, n heads = 8, n layers = 6 and p dropout = 0.3. The learning rate is set as 0.0005, the warmup step is set as 4,000 steps. All models are trained with a batch size of 16,000 tokens for maximum of 300,000 steps with Adam optimizer (Kingma and Ba, 2014) on 2 NVIDIA GeForce RTX 3090 GPUs with gradient accumulation of 4 batches. Checkpoints for testing are selected from the average weights of the last 5 checkpoints. For Transformer (Vaswani et al., 2017), we use the checkpoint released by Ott et al. (2018).",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_57",
            "content": "Main Results",
            "ntype": "title",
            "meta": {
                "section": "5.3"
            }
        },
        {
            "ix": "315-ARR_v1_58",
            "content": "Table 4 reports the performance of LevT with ACT (as well as the CT ablation) on the type 1 tasks (WIKT and IATE as terminologies), compared with baselines. In general, the results indicate the proposed CT/ACT algorithms achieve a consistent gain in performance, term coverage, and speed over the backbone model mainly in the setting of constrained translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_59",
            "content": "When translating with soft constraints, i.e., the constraints need not appear in the output, adding ACT to LevT helps preserve the terminology constraints (+\u223c5 Term%) and improves translation performance (+0.31-0.88 on BLEU). If we enforce hard constraints, the term usage rate doubtlessly reaches 100%, with reasonable improvements on BLEU. When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_60",
            "content": "As for the ablation for CT and ACT, we have two observations: 1) term usage rate increases mainly because of CT, and can be further improved by ACT; 2) translation quality (BLEU) increases due to the additional hard alignment of ACT over CT. The former could be attributed to the behavior of not deleting the constraints in CT. The latter is because of the introduction of source-side information of constraints that familiarize the model with the constraint context.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_61",
            "content": "Table 3 also shows the efficiency advantage of non-autoregressive methods compared with autoregressive ones, which is widely reported in the NAT research literature. The proposed methods do not cause drops in translation speed against the backbone LevT. When translating with lexical constraints, LevT with CT or ACT is even faster than LevT. In contrast, constrained decoding methods for autoregressive models (i.e., DBA) nearly double the translation latency. Since the main purpose of non-autoregressive research is developing efficient algorithms, such findings could facilitate the industrial usage for constrained translation.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_62",
            "content": "Translation Results on Domain Datasets For a generalized evaluation of our methods, we apply the models trained on the general domain dataset (WMT14 En-De) to medical (OPUS-EMEA) and legal domains (OPUS-JRC). As seen in Table 5, even greater performance boosts are witnessed. When trained with ACT, both term usage (+\u223c8-10 Term%) and translation performance (up to 4 BLEU points) largely increase, which is more significant than the general domain.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_63",
            "content": "The reason behind this observation is that the backbone LevT would have a hard time recognizing them as constraints since the lexical constraints in these datasets are much rarer. Therefore, forcing LevT to translate with these rare constraints would generate worse text, e.g., BLEU drops for 2.45 points on OPUS-JRC than with soft constraints. And when translating with soft constraints, LevT over-deletes these rare constraints. In contrast, the context information around constraints is effectively pin-pointed by ACT, so ACT would know the context (\"neighbors\") of the rare constraint (\"strangers\") and insert the translated context around the lexical constraints. In this way, more terms are preserved by ACT, and the translation achieves better results.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_64",
            "content": "Analysis",
            "ntype": "title",
            "meta": {
                "section": "6"
            }
        },
        {
            "ix": "315-ARR_v1_65",
            "content": "Self-Constrained Translation Revisited",
            "ntype": "title",
            "meta": {
                "section": "6.1"
            }
        },
        {
            "ix": "315-ARR_v1_66",
            "content": "As a direct response to our motivation in this paper, we revisit the ablation study of self-constrained NAT in \u00a73.3 with the proposed ACT algorithm. Same as before, we build self-constraints from each target sentence and sort them by frequency. As shown in Figure 2(a), different from constrained LevT that suffers from Drop#2 ( \u00a73.3), ACT managed to handle this scenario pretty well. Following the motivations given in \u00a73.3, when constraints become rarer, ACT successfully breaks the trade-off with better understanding of the provided contextual information.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_67",
            "content": "[0 % ,1 0% ) [1 0% ,3 0% ) [3",
            "ntype": "formula",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_68",
            "content": "What if the self-constraints are sorted based on TF-IDF? We also study the importance of different words in a sentence via TF-IDF by forcing them as constraints. As results in Figure 2(b) show, we have very similar observations from frequencybased self-constraints at Figure 2(a), and the gap between LevT and LevT + ACT is even higher as TF-IDF score reaches the highest.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_69",
            "content": "How does ACT perform under different kinds of lexical constraints?",
            "ntype": "title",
            "meta": {
                "section": "6.2"
            }
        },
        {
            "ix": "315-ARR_v1_70",
            "content": "The experiments in \u00a76.1 create pseudo lexical constraints by traversing the target-side reference for understanding the proposed ACT. In the following analyses, we study different properties of lexical constraints, e.g., frequency and numbers, and how they affect constrained translation. Are improvements by ACT robust against constraints of different frequencies? Given terminology constraints in the samples, we sort them by (averaged) frequency and evenly divide the corresponding data samples into high, medium and low categories.The results on translation quality of each category for the En\u2192De translation tasks are presented in Table 6. We find that LevT benefits mostly from ACT in the scenarios of lowerfrequency terms for three datasets. Although, in some settings such as HIGH in WMT14-WIKT and MED in WMT17-WIKT, the introduction of ACT for constrained LevT seems to bring performance drops for those higher-frequency terms. Since terms from IATE are rarer than WIKT as in Table 3, the improvements brought by ACT are steady.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_71",
            "content": "Are improvements by ACT robust against constraints of different numbers? In more practical settings, the number of constraints is usually more than one. To simulate this, we randomly sample 1-5 words from each reference as lexical constraints, and results are presented in Figure 3. We find that, as the number of constraints grows, the translation quality ostensibly becomes better for LevT with or without ACT. And ACT consistently brings extra improvements, indicating the help by ACT for constrained decoding in constrained NAT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_72",
            "content": "Limitations",
            "ntype": "title",
            "meta": {
                "section": "6.3"
            }
        },
        {
            "ix": "315-ARR_v1_73",
            "content": "Although the proposed ACT algorithm is effective to improve NAT models on constrained translation, we also find it does not bring much performance gain on translation quality (i.e., BLEU) over the backbone LevT for unconstrained translation. The results on the full set of WMT14 En\u2192De test set further corroborate this finding, which is shown in Appendix A. Another limitation of our work is that we do not propose a new paradigm for constrained NAT. The purpose of this work is to enhance existing methods for constrained NAT, i.e., editing-based iterative NAT methods, under rare lexical constraints. It would be interesting for future research to explore new ways to impose lexical constraints on NAT models, perhaps on non-iterative NAT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_74",
            "content": "Note that, machine translation in real scenario still falls behind human performance. Moreover, since we primary focus on improving constrained NAT, real applications calls for refinement in various aspects that we do not consider in this work.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_75",
            "content": "Conclusion",
            "ntype": "title",
            "meta": {
                "section": "7"
            }
        },
        {
            "ix": "315-ARR_v1_76",
            "content": "In this work, we propose a plug-in algorithm (ACT) to improve lexically constrained nonautoregressive translation, especially under lowfrequency constraints. ACT bridges the gap between training and constrained inference and prompts the context information of the constraints to the constrained NAT model. Experiments show that ACT improves translation quality and term preservation over the backbone NAT model Levenshtein Transformer. Further analyses show that the findings are consistent over constraints varied from frequency, TF-IDF, and lengths. In the future, we will explore the application of this approach to more languages. We also encourage future research to explore a new paradigm of constrained NAT methods beyond editing-based iterative NAT.",
            "ntype": "p",
            "meta": null
        },
        {
            "ix": "315-ARR_v1_77",
            "content": "Melissa Ailem, Jingshu Liu, Raheel Qader, Encouraging neural machine translation to satisfy terminology constraints, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "ref",
            "meta": {
                "xid": "b0",
                "authors": [
                    "Melissa Ailem",
                    "Jingshu Liu",
                    "Raheel Qader"
                ],
                "title": "Encouraging neural machine translation to satisfy terminology constraints",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_78",
            "content": "Tamer Alkhouli, Gabriel Bretschner, Hermann Ney, On the alignment problem in multi-head attention-based neural machine translation, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b1",
                "authors": [
                    "Tamer Alkhouli",
                    "Gabriel Bretschner",
                    "Hermann Ney"
                ],
                "title": "On the alignment problem in multi-head attention-based neural machine translation",
                "pub_date": "2018",
                "pub_title": "Proceedings of the Third Conference on Machine Translation: Research Papers",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "315-ARR_v1_79",
            "content": "Dzmitry Bahdanau, Kyung Cho, Yoshua Bengio, Neural machine translation by jointly learning to align and translate, 2015, 3rd International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b2",
                "authors": [
                    "Dzmitry Bahdanau",
                    "Kyung Cho",
                    "Yoshua Bengio"
                ],
                "title": "Neural machine translation by jointly learning to align and translate",
                "pub_date": "2015",
                "pub_title": "3rd International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_80",
            "content": "UNKNOWN, None, , Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b3",
                "authors": null,
                "title": null,
                "pub_date": null,
                "pub_title": "Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_81",
            "content": "Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, Marco Turchi, Findings of the 2017 conference on machine translation (WMT17), 2017, Proceedings of the Second Conference on Machine Translation, .",
            "ntype": "ref",
            "meta": {
                "xid": "b4",
                "authors": [
                    "Ond\u0159ej Bojar",
                    "Rajen Chatterjee",
                    "Christian Federmann",
                    "Yvette Graham",
                    "Barry Haddow",
                    "Shujian Huang",
                    "Matthias Huck",
                    "Philipp Koehn",
                    "Qun Liu",
                    "Varvara Logacheva",
                    "Christof Monz",
                    "Matteo Negri",
                    "Matt Post",
                    "Raphael Rubino",
                    "Lucia Specia",
                    "Marco Turchi"
                ],
                "title": "Findings of the 2017 conference on machine translation (WMT17)",
                "pub_date": "2017",
                "pub_title": "Proceedings of the Second Conference on Machine Translation",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_82",
            "content": "F Peter, Stephen Brown, Vincent Pietra, Robert Della Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b5",
                "authors": [
                    "F Peter",
                    "Stephen Brown",
                    "Vincent Pietra",
                    "Robert Della Pietra",
                    " Mercer"
                ],
                "title": "The mathematics of statistical machine translation: Parameter estimation",
                "pub_date": "1993",
                "pub_title": "Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_83",
            "content": "Guanhua Chen, Yun Chen, O Victor,  Li, Lexically constrained neural machine translation with explicit alignment guidance, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b6",
                "authors": [
                    "Guanhua Chen",
                    "Yun Chen",
                    "O Victor",
                    " Li"
                ],
                "title": "Lexically constrained neural machine translation with explicit alignment guidance",
                "pub_date": "2021",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_84",
            "content": "Guanhua Chen, Yun Chen, Yong Wang, O Victor,  Li, Lexical-constraint-aware neural machine translation via data augmentation, 2020, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, .",
            "ntype": "ref",
            "meta": {
                "xid": "b7",
                "authors": [
                    "Guanhua Chen",
                    "Yun Chen",
                    "Yong Wang",
                    "O Victor",
                    " Li"
                ],
                "title": "Lexical-constraint-aware neural machine translation via data augmentation",
                "pub_date": "2020",
                "pub_title": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_85",
            "content": "UNKNOWN, None, 2016, , .",
            "ntype": "ref",
            "meta": {
                "xid": "b8",
                "authors": null,
                "title": null,
                "pub_date": "2016",
                "pub_title": null,
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_86",
            "content": "Liang Ding, Longyue Wang, Xuebo Liu, Derek Wong, Dacheng Tao, Zhaopeng Tu, Understanding and improving lexical choice in nonautoregressive translation, 2021, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b9",
                "authors": [
                    "Liang Ding",
                    "Longyue Wang",
                    "Xuebo Liu",
                    "Derek Wong",
                    "Dacheng Tao",
                    "Zhaopeng Tu"
                ],
                "title": "Understanding and improving lexical choice in nonautoregressive translation",
                "pub_date": "2021",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_87",
            "content": "Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan, Training neural machine translation to apply terminology constraints, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b10",
                "authors": [
                    "Georgiana Dinu",
                    "Prashant Mathur",
                    "Marcello Federico",
                    "Yaser Al-Onaizan"
                ],
                "title": "Training neural machine translation to apply terminology constraints",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "315-ARR_v1_88",
            "content": "Cunxiao Du, Zhaopeng Tu, Jing Jiang, Orderagnostic cross entropy for non-autoregressive machine translation, 2021, Proc. of ICML, .",
            "ntype": "ref",
            "meta": {
                "xid": "b11",
                "authors": [
                    "Cunxiao Du",
                    "Zhaopeng Tu",
                    "Jing Jiang"
                ],
                "title": "Orderagnostic cross entropy for non-autoregressive machine translation",
                "pub_date": "2021",
                "pub_title": "Proc. of ICML",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_89",
            "content": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer, Mask-predict: Parallel decoding of conditional masked language models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "ref",
            "meta": {
                "xid": "b12",
                "authors": [
                    "Marjan Ghazvininejad",
                    "Omer Levy",
                    "Yinhan Liu",
                    "Luke Zettlemoyer"
                ],
                "title": "Mask-predict: Parallel decoding of conditional masked language models",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_90",
            "content": "Jiatao Gu, James Bradbury, Caiming Xiong, O Victor, Richard Li,  Socher, Non-autoregressive neural machine translation, 2018, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b13",
                "authors": [
                    "Jiatao Gu",
                    "James Bradbury",
                    "Caiming Xiong",
                    "O Victor",
                    "Richard Li",
                    " Socher"
                ],
                "title": "Non-autoregressive neural machine translation",
                "pub_date": "2018",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_91",
            "content": "Jiatao Gu, Changhan Wang, Junbo Zhao, Levenshtein transformer, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "ref",
            "meta": {
                "xid": "b14",
                "authors": [
                    "Jiatao Gu",
                    "Changhan Wang",
                    "Junbo Zhao"
                ],
                "title": "Levenshtein transformer",
                "pub_date": "2019",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_92",
            "content": "Junliang Guo, Zhirui Zhang, Linli Xu,  Hao-Ran, Boxing Wei, Enhong Chen,  Chen, Incorporating bert into parallel sequence decoding with adapters, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b15",
                "authors": [
                    "Junliang Guo",
                    "Zhirui Zhang",
                    "Linli Xu",
                    " Hao-Ran",
                    "Boxing Wei",
                    "Enhong Chen",
                    " Chen"
                ],
                "title": "Incorporating bert into parallel sequence decoding with adapters",
                "pub_date": "2020",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "315-ARR_v1_93",
            "content": "Chris Hokamp, Qun Liu, Lexically constrained decoding for sequence generation using grid beam search, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b16",
                "authors": [
                    "Chris Hokamp",
                    "Qun Liu"
                ],
                "title": "Lexically constrained decoding for sequence generation using grid beam search",
                "pub_date": "2017",
                "pub_title": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "315-ARR_v1_94",
            "content": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li, Glancing transformer for non-autoregressive neural machine translation, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b17",
                "authors": [
                    "Lihua Qian",
                    "Hao Zhou",
                    "Yu Bao",
                    "Mingxuan Wang",
                    "Lin Qiu",
                    "Weinan Zhang",
                    "Yong Yu",
                    "Lei Li"
                ],
                "title": "Glancing transformer for non-autoregressive neural machine translation",
                "pub_date": "2021",
                "pub_title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "315-ARR_v1_95",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b18",
                "authors": [
                    "Rico Sennrich",
                    "Barry Haddow",
                    "Alexandra Birch"
                ],
                "title": "Neural machine translation of rare words with subword units",
                "pub_date": "2016",
                "pub_title": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Long Papers"
            }
        },
        {
            "ix": "315-ARR_v1_96",
            "content": "Raphael Shu, Jason Lee, Hideki Nakayama, Kyunghyun Cho, Latent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b19",
                "authors": [
                    "Raphael Shu",
                    "Jason Lee",
                    "Hideki Nakayama",
                    "Kyunghyun Cho"
                ],
                "title": "Latent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_97",
            "content": "Kai Song, Kun Wang, Heng Yu, Yue Zhang, Zhongqiang Huang, Weihua Luo, Xiangyu Duan, Min Zhang, Alignment-enhanced transformer for constraining nmt with pre-specified translations, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b20",
                "authors": [
                    "Kai Song",
                    "Kun Wang",
                    "Heng Yu",
                    "Yue Zhang",
                    "Zhongqiang Huang",
                    "Weihua Luo",
                    "Xiangyu Duan",
                    "Min Zhang"
                ],
                "title": "Alignment-enhanced transformer for constraining nmt with pre-specified translations",
                "pub_date": "2020",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_98",
            "content": "Kai Song, Yue Zhang, Heng Yu, Weihua Luo, Kun Wang, Min Zhang, Code-switching for enhancing NMT with pre-specified translation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "ref",
            "meta": {
                "xid": "b21",
                "authors": [
                    "Kai Song",
                    "Yue Zhang",
                    "Heng Yu",
                    "Weihua Luo",
                    "Kun Wang",
                    "Min Zhang"
                ],
                "title": "Code-switching for enhancing NMT with pre-specified translation",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": "Long and Short Papers"
            }
        },
        {
            "ix": "315-ARR_v1_99",
            "content": "Mitchell Stern, William Chan, Jamie Kiros, Jakob Uszkoreit, Insertion transformer: Flexible sequence generation via insertion operations, 2019, Proceedings of the 36th International Conference on Machine Learning, PMLR.",
            "ntype": "ref",
            "meta": {
                "xid": "b22",
                "authors": [
                    "Mitchell Stern",
                    "William Chan",
                    "Jamie Kiros",
                    "Jakob Uszkoreit"
                ],
                "title": "Insertion transformer: Flexible sequence generation via insertion operations",
                "pub_date": "2019",
                "pub_title": "Proceedings of the 36th International Conference on Machine Learning",
                "pub": "PMLR"
            }
        },
        {
            "ix": "315-ARR_v1_100",
            "content": "Raymond Hendy Susanto, Shamil Chollampatt, Liling Tan, Lexically constrained neural machine translation with Levenshtein transformer, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b23",
                "authors": [
                    "Raymond Hendy Susanto",
                    "Shamil Chollampatt",
                    "Liling Tan"
                ],
                "title": "Lexically constrained neural machine translation with Levenshtein transformer",
                "pub_date": "2020",
                "pub_title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "pub": "Association for Computational Linguistics"
            }
        },
        {
            "ix": "315-ARR_v1_101",
            "content": "J\u00f6rg Tiedemann, Parallel data, tools and interfaces in OPUS, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "ref",
            "meta": {
                "xid": "b24",
                "authors": [
                    "J\u00f6rg Tiedemann"
                ],
                "title": "Parallel data, tools and interfaces in OPUS",
                "pub_date": "2012",
                "pub_title": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_102",
            "content": "Ferhan Ture, Douglas Oard, Philip Resnik, consistent translation choices, 2012, Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "ref",
            "meta": {
                "xid": "b25",
                "authors": [
                    "Ferhan Ture",
                    "Douglas Oard",
                    "Philip Resnik"
                ],
                "title": "consistent translation choices",
                "pub_date": "2012",
                "pub_title": "Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_103",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "ref",
            "meta": {
                "xid": "b26",
                "authors": [
                    "Ashish Vaswani",
                    "Noam Shazeer",
                    "Niki Parmar",
                    "Jakob Uszkoreit",
                    "Llion Jones",
                    "Aidan Gomez",
                    "Illia Kaiser",
                    " Polosukhin"
                ],
                "title": "Attention is all you need",
                "pub_date": "2017",
                "pub_title": "Advances in Neural Information Processing Systems",
                "pub": "Curran Associates, Inc"
            }
        },
        {
            "ix": "315-ARR_v1_104",
            "content": "Yiren Wang, Fei Tian, Di He, Tao Qin, Chengxiang Zhai, Tie-Yan Liu, Non-autoregressive machine translation with auxiliary regularization, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "ref",
            "meta": {
                "xid": "b27",
                "authors": [
                    "Yiren Wang",
                    "Fei Tian",
                    "Di He",
                    "Tao Qin",
                    "Chengxiang Zhai",
                    "Tie-Yan Liu"
                ],
                "title": "Non-autoregressive machine translation with auxiliary regularization",
                "pub_date": "2019",
                "pub_title": "Proceedings of the AAAI Conference on Artificial Intelligence",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_105",
            "content": ", EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints, , Transactions of the Association for Computational Linguistics, .",
            "ntype": "ref",
            "meta": {
                "xid": "b28",
                "authors": [],
                "title": "EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints",
                "pub_date": null,
                "pub_title": "Transactions of the Association for Computational Linguistics",
                "pub": null
            }
        },
        {
            "ix": "315-ARR_v1_106",
            "content": "Weijia Xu, Shuming Ma, Dongdong Zhang, Marine Carpuat, How does distilled data complexity impact the quality and confidence of nonautoregressive machine translation?, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "ref",
            "meta": {
                "xid": "b29",
                "authors": [
                    "Weijia Xu",
                    "Shuming Ma",
                    "Dongdong Zhang",
                    "Marine Carpuat"
                ],
                "title": "How does distilled data complexity impact the quality and confidence of nonautoregressive machine translation?",
                "pub_date": "2021",
                "pub_title": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "pub": "Online. Association for Computational Linguistics"
            }
        },
        {
            "ix": "315-ARR_v1_107",
            "content": "Chunting Zhou, Jiatao Gu, Graham Neubig, Understanding knowledge distillation in nonautoregressive machine translation, 2020, International Conference on Learning Representations, .",
            "ntype": "ref",
            "meta": {
                "xid": "b30",
                "authors": [
                    "Chunting Zhou",
                    "Jiatao Gu",
                    "Graham Neubig"
                ],
                "title": "Understanding knowledge distillation in nonautoregressive machine translation",
                "pub_date": "2020",
                "pub_title": "International Conference on Learning Representations",
                "pub": null
            }
        }
    ],
    "span_nodes": [
        {
            "ix": "315-ARR_v1_0@0",
            "content": "Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_0",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_1@0",
            "content": "Abstract",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_1",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_2@0",
            "content": "Lexically constrained neural machine translation (NMT) draws much industrial attention for its practical usage in specific domains.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_2",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_2@1",
            "content": "However, current autoregressive approaches suffer from high latency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_2",
            "start": 132,
            "end": 199,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_2@2",
            "content": "In this paper, we focus on non-autoregressive translation (NAT) for this problem for its efficiency advantage.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_2",
            "start": 201,
            "end": 310,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_2@3",
            "content": "We identify that current constrained NAT models, which are based on iterative editing, do not handle low-frequency constraints well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_2",
            "start": 312,
            "end": 443,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_2@4",
            "content": "To this end, we propose a plug-in algorithm for this line of work, i.e., Aligned Constrained Training (ACT), which alleviates this problem by familiarizing the model with the source-side context of the constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_2",
            "start": 445,
            "end": 658,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_2@5",
            "content": "Experiments on the general and domain datasets show that our model improves over the backbone constrained NAT model in constraint preservation and translation quality, especially for rare constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_2",
            "start": 660,
            "end": 859,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_2@6",
            "content": "1",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_2",
            "start": 861,
            "end": 861,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_3@0",
            "content": "Introduction",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_3",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_4@0",
            "content": "Despite the success of neural machine translation (NMT) (Bahdanau et al., 2015;Vaswani et al., 2017;Barrault et al., 2020), real applications usually require the precise (if not exact) translation of specific terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_4",
            "start": 0,
            "end": 214,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_4@1",
            "content": "One popular solution is to incorporate dictionaries of pre-defined terminologies as lexical constraints to ensure the correct translation of terms, which has been demonstrated to be effective in many areas such as domain adaptation, interactive translation, etc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_4",
            "start": 216,
            "end": 477,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_5@0",
            "content": "Previous methods on lexically constrained translation are mainly built upon Autoregressive Translation (AT) models, imposing constraints at inference-time (Ture et al., 2012;Hokamp and Liu, 2017;Post and Vilar, 2018) or training-time (Luong et al., 2015;Ailem et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_5",
            "start": 0,
            "end": 273,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_5@1",
            "content": "However, such methods either are time-consuming in real-time applications or do not ensure the appearance of constraints in the output.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_5",
            "start": 275,
            "end": 409,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_5@2",
            "content": "To develop faster MT models for industrial applications, Non-Autoregressive 1 Code will be released upon publication.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_5",
            "start": 411,
            "end": 527,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_5@3",
            "content": "\u21d2 incomplete sentence",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_5",
            "start": 529,
            "end": 549,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_6@0",
            "content": "Table 1: Translation examples of a lexically constrained non-autoregressive translation (NAT) model (Gu et al., 2019) under a low-frequency word as constraint.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_6",
            "start": 0,
            "end": 158,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_6@1",
            "content": "The underbraced word frequencies (uncased) are calculated from the vast WMT14 English-German translation (En-De) datasets (Vaswani et al., 2017).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_6",
            "start": 160,
            "end": 304,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_7@0",
            "content": "Translation (NAT) has been put forth (Gu et al., 2018;Ghazvininejad et al., 2019;Gu et al., 2019;Qian et al., 2021), which aims to generate tokens in parallel, boosting inference efficiency compared with left-to-right autoregressive decoding.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_7",
            "start": 0,
            "end": 241,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_8@0",
            "content": "Researches on lexically constrained NAT are relatively under-explored.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_8",
            "start": 0,
            "end": 69,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_8@1",
            "content": "Recent studies (Susanto et al., 2020;Xu and Carpuat, 2021) impose lexical constraints at inference time upon editing-based iterative NAT models, where constraint tokens are set as the initial sequence for further editing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_8",
            "start": 71,
            "end": 291,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_8@2",
            "content": "However, such methods are vulnerable when encountered with low-frequency words as constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_8",
            "start": 293,
            "end": 386,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_8@3",
            "content": "As illustrated in Table 1, when translated with a rare constraint, the model is unable to generate the correct context of the term \"geschrien\" as if it does not understand the constraint at all.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_8",
            "start": 388,
            "end": 581,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_8@4",
            "content": "It is dangerous since terms in specific domains are usually lowfrequency words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_8",
            "start": 583,
            "end": 661,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_8@5",
            "content": "We argue that the main reasons behind this problem are 1) the inconsistency between training and constrained inference and 2) the unawareness of the source-side context of the constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_8",
            "start": 663,
            "end": 850,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_9@0",
            "content": "To solve this problem, we build our algorithm based on the idea that the context of a rare constraint tends not to be rare as well, i.e., \"a stranger's neighbors are not necessarily strangers\", as demonstrated in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_9",
            "start": 0,
            "end": 220,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_9@1",
            "content": "We believe that, when the constraint is aligned to the source text, the context of its source-side counterpart can be utilized to be translated into the context of the target-side constraint, even if the constraint itself is rare. Also, when enforced to learn to preserve designated constraints at training-time, a model should be better at coping with constraints during inference-time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_9",
            "start": 222,
            "end": 608,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_10@0",
            "content": "Driven by these motivations, we propose a plugin algorithm to improve constrained NAT, namely Aligned Constrained Training (ACT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_10",
            "start": 0,
            "end": 128,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_10@1",
            "content": "ACT extends the family of editing-based iterative NAT (Gu et al., 2019;Susanto et al., 2020;Xu and Carpuat, 2021), the current paradigm of constrained NAT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_10",
            "start": 130,
            "end": 284,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_10@2",
            "content": "Specifically, ACT is composed of two major components: Constrained Training and Alignment Prompting.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_10",
            "start": 286,
            "end": 385,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_10@3",
            "content": "The former extends regular training of iterative NAT with pseudo training-time constraints into the state transition of imitation learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_10",
            "start": 387,
            "end": 525,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_10@4",
            "content": "The latter incorporates source alignment information of constraints into training and inference, indicating the context of the potentially rare terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_10",
            "start": 527,
            "end": 676,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_11@0",
            "content": "In summary, this work makes the following contributions: 1) We identify and analyse the problems w.r.t.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_11",
            "start": 0,
            "end": 102,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_11@1",
            "content": "rare lexical constraints in current constrained NAT methods; 2) We propose a plug-in algorithm for current constrained NAT models, i.e., aligned constrained training, to improve the translation under rare constraints; 3) Experiments show that our approach improves the backbone model w.r.t.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_11",
            "start": 104,
            "end": 393,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_11@2",
            "content": "constraint preservation and translation quality, especially for rare constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_11",
            "start": 395,
            "end": 475,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_12@0",
            "content": "Related Work",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_12",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_13@0",
            "content": "Lexically Constrained Translation Existing translation methods impose lexical constraints during either inference or training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_13",
            "start": 0,
            "end": 125,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_13@1",
            "content": "At training time, constrained MT models include code-switching data augmentation (Dinu et al., 2019;Song et al., 2019; and training with auxiliary tasks such as token or span-level mask-prediction (Ailem et al., 2021;Lee et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_13",
            "start": 127,
            "end": 361,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_13@2",
            "content": "At inference time, autoregressive constrained decoding algorithms include utilizing placeholder tag (Luong et al., 2015;Crego et al., 2016), grid beam search (Hokamp and Liu, 2017;Post and Vilar, 2018) and alignment-enhanced decoding (Alkhouli et al., 2018;Song et al., 2020;Chen et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_13",
            "start": 363,
            "end": 656,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_13@3",
            "content": "For the purpose of efficiency, recent studies also focus on non-autoregressive constrained translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_13",
            "start": 658,
            "end": 760,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_13@4",
            "content": "Susanto et al. (2020) proposes to modify the inference procedure of Levenshtein Transformer (Gu et al., 2019) where they disallow the deletion of constraint words during iterative editing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_13",
            "start": 762,
            "end": 949,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_13@5",
            "content": "Xu and Carpuat (2021) further develops this idea and introduces a reposition operation that can reorder the constraint tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_13",
            "start": 951,
            "end": 1076,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_13@6",
            "content": "Our work absorbs the idea of both lines of work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_13",
            "start": 1078,
            "end": 1125,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_13@7",
            "content": "Based on NAT methods, we brings alignment information by terminologies to help learn the contextual information for lexical constraints, especially the rare ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_13",
            "start": 1127,
            "end": 1288,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_14@0",
            "content": "Non-Autoregressive Translation Although enjoy the speed advantage, NAT models suffer from performance degradation due to the multi-modality problem, i.e., generating text when multiple translations are plausible.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_14",
            "start": 0,
            "end": 211,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_14@1",
            "content": "Gu et al. (2018) applies sequence-level knowledge distillation (KD) (Kim and Rush, 2016) that uses an AT's output as an NAT's new target, which reduces word diversity and reordering complexity in reference, resulting in fewer modes (Zhou et al., 2020;Xu et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_14",
            "start": 213,
            "end": 480,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_14@2",
            "content": "Various algorithms have also been proposed to alleviate this problem, including incorporating latent variables (Kaiser et al., 2018;Shu et al., 2020), iterative refinement (Ghazvininejad et al., 2019;Stern et al., 2019;Gu et al., 2019;Guo et al., 2020), advanced training objective Du et al., 2021) and gradually learning targetside word inter-dependency by curriculum learning (Qian et al., 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_14",
            "start": 482,
            "end": 879,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_14@3",
            "content": "Our work extends the family of editing-based iterative NAT models for its flexibility to impose lexical constraints (Susanto et al., 2020;Xu and Carpuat, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_14",
            "start": 881,
            "end": 1040,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_15@0",
            "content": "Background",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_15",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_16@0",
            "content": "Non-Autoregressive Translation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_16",
            "start": 0,
            "end": 29,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_17@0",
            "content": "Given a source sentence as x and a target sentence as y = {y 1 , \u2022 \u2022 \u2022 , y n }, an AT model generates in a left-to-right order, i.e., generating y t by conditioning on x and y <t .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_17",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_17@1",
            "content": "An NAT model (Gu et al., 2018), however, discards the word inter-dependency in output tokens, with the conditional independent probability distribution modeled as:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_17",
            "start": 181,
            "end": 343,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_18@0",
            "content": "P (y|x) = n t=1 P (y t |x).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_18",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_19@0",
            "content": "(1)",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_19",
            "start": 0,
            "end": 2,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_20@0",
            "content": "Such factorization is featured with high efficiency at the cost of performance drop in translation tasks due to the multi-modality problem, i.e., translating in mixed modes and resulting in token repetition, missing, or incoherence.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_20",
            "start": 0,
            "end": 231,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_21@0",
            "content": "Editing-based Iterative NAT",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_21",
            "start": 0,
            "end": 26,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_22@0",
            "content": "For NATs, iterative refinement by editing is an NAT paradigm that suits constrained translations due to its flexibility.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_22",
            "start": 0,
            "end": 119,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_22@1",
            "content": "It alleviates the multi-modality problem by being autoregressive in editing previously generated sequences while maintaining nonautoregressiveness within each iteration.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_22",
            "start": 121,
            "end": 289,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_22@2",
            "content": "Thus, it achieves better performance than fully NATs while is faster than ATs.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_22",
            "start": 291,
            "end": 368,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_23@0",
            "content": "Levenshtein Transformer To better illustrate our idea, we use Levenshtein Transformer (LevT, Gu et al., 2019) as the backbone model in this work, which is a representative model for constrained NAT based on iterative editing.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_23",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_24@0",
            "content": "LevT is based on the Transformer architecture (Vaswani et al., 2017), but more flexible and fast than autoregressive ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_24",
            "start": 0,
            "end": 121,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_24@1",
            "content": "It models the generation of sentences as Markov Decision Process (MDP) defined by a tuple (Y, A, E, R, y 0 ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_24",
            "start": 123,
            "end": 231,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_24@2",
            "content": "At each decoding iteration, the agent E receives an input y \u2208 Y, chooses an action a \u2208 A and gets reward r. Y is a set of discrete sentences and R is the reward function.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_24",
            "start": 233,
            "end": 402,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_24@3",
            "content": "y 0 \u2208 A is the initial sentence to be edited.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_24",
            "start": 404,
            "end": 448,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_25@0",
            "content": "Each iteration consists of two basic operations, i.e., deletion and insertion, which is described in Table 2.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_25",
            "start": 0,
            "end": 108,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_25@1",
            "content": "For the k-th iteration of the sentence y k = (<s>, y 1 , ..., y n , </s>), the insertion consists of placeholder and token classifiers, and the deletion is achieved by a deletion classifier.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_25",
            "start": 110,
            "end": 299,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_25@2",
            "content": "LevT trains the model with imitation learning to insert and delete, which lets the agent imitate the behaviors drawn from the expert policy:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_25",
            "start": 301,
            "end": 440,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_26@0",
            "content": "\u2022 Learning to insert: edit to reference by inserting tokens from a fragmented sentence (e.g., random deletion of reference). (Dinu et al., 2019). \u2022 Learning to delete: delete from the insertion result of the current training status to the reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_26",
            "start": 0,
            "end": 248,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_27@0",
            "content": "[0 % ,1 0% ) [1 0% ,3 0% ) [3 0% ,5 0% ) [5 0% ,7 0% ) [7 0% ,9 0% ) [9 0% ,1 00 % ]",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_27",
            "start": 0,
            "end": 83,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_28@0",
            "content": "The key idea is to learn how to edit from a ground truth after adding noise or the output of an adversary policy to the reference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_28",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_28@1",
            "content": "The ground truth of the editing process is derived from the Levenshtein distance (Levenshtein, 1965).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_28",
            "start": 131,
            "end": 231,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_29@0",
            "content": "Lexically Constrained Inference Lexical constraints can be imposed upon a translation model in: 1) soft constraints: allowing the constraints not to appear in the translation; and 2) hard constraints: forcing the constraints to appear in the translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_29",
            "start": 0,
            "end": 253,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_30@0",
            "content": "In NAT, the constraints are generally incorporated at inference time.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_30",
            "start": 0,
            "end": 68,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_30@1",
            "content": "Susanto et al. (2020) injects constraints as the initial sequence for iterative editing in Levenshtein Transformer (LevT, Gu et al., 2019), achieving soft constrained translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_30",
            "start": 70,
            "end": 248,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_30@2",
            "content": "And hard constrained translation can be easily done by disallowing the deletion of the constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_30",
            "start": 250,
            "end": 348,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_30@3",
            "content": "Xu and Carpuat (2021) alters the deletion action in LevT with the reposition operation, allowing the reordering of multiple constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_30",
            "start": 350,
            "end": 485,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_31@0",
            "content": "Motivating Study: Self-Constrained Translation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_31",
            "start": 0,
            "end": 45,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_32@0",
            "content": "According to Table 1, constrained NAT models seem to suffer from the low-frequency of lexical constraints, which is dangerous as most terms in practice are rare.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_32",
            "start": 0,
            "end": 160,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_32@1",
            "content": "To further explore the impact of constraint frequency upon NATs, we conduct a preliminary analysis on constrained LevT (Susanto et al., 2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_32",
            "start": 162,
            "end": 303,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_32@2",
            "content": "We sort words in each reference text based on frequency, dividing them into six buckets by frequency order (as in Figure 1), and sample a word from each bucket as lexical constraints for translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_32",
            "start": 305,
            "end": 503,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_32@3",
            "content": "We denote these constraints as selfconstraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_32",
            "start": 505,
            "end": 551,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_32@4",
            "content": "In this way, we have six times the data, and the six samples derived from one raw sample only differ in the lexical constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_32",
            "start": 553,
            "end": 680,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_32@5",
            "content": "As shown in Figure 1, translation performance generally keeps improving as the self-constraint gets rarer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_32",
            "start": 682,
            "end": 787,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_32@6",
            "content": "This is because setting low-frequency words in a sentence as constraints, which are often hard to translate, actually lightens the load of an NAT model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_32",
            "start": 789,
            "end": 940,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_32@7",
            "content": "However, there are two noticeable performance drops around relative frequency ranges of 10%-30% and 90%-100%, denoted as Drop#1 (-0.3 BLEU) and Drop#2 (-0.6 BLEU).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_32",
            "start": 942,
            "end": 1104,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_32@8",
            "content": "Drop#1 is probably because the constraint words within this range are mostly functional or less important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_32",
            "start": 1106,
            "end": 1211,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_32@9",
            "content": "Such words are not as universal as ones at the leftmost that can fit in most contexts and do not have to appear in the target due to multiple modes in translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_32",
            "start": 1213,
            "end": 1375,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_33@0",
            "content": "However, we are more interested in the reasons for Drop#2 when constraints are low-frequency words.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_33",
            "start": 0,
            "end": 98,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_33@1",
            "content": "We assume a trade-off in self-constrained NAT: the model does not have to translate rare words as they are set as an initial sequence (constraints), but it will have a hard time understanding the context of the rare constraint due to 1) the rareness itself and 2) the lack of the alignment information between target-side constraint tokens and source tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_33",
            "start": 100,
            "end": 457,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_33@2",
            "content": "Thus, the model does not know how many tokens should be inserted to the left and right of the constraint, which is consistent with the findings in Table 1.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_33",
            "start": 459,
            "end": 613,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_34@0",
            "content": "Proposed Approach",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_34",
            "start": 0,
            "end": 16,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_35@0",
            "content": "The findings and assumptions discussed above motivate us to propose a plug-in algorithm for lexically constrained NAT models, i.e., Aligned Constrained Training (ACT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_35",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_35@1",
            "content": "ACT is designed based on two major ideas: 1) Constrained Training: bridging the discrepancy between training and constrained inference; 2) Alignment Prompting: helping the model understand the context of the constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_35",
            "start": 168,
            "end": 387,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_36@0",
            "content": "Constrained Training",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_36",
            "start": 0,
            "end": 19,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_37@0",
            "content": "As introduced in \u00a73.2, constraints are typically imposed during inference time in NAT (Susanto et al., 2020;Xu and Carpuat, 2021).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_37",
            "start": 0,
            "end": 129,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_37@1",
            "content": "Specifically, lexical constraints are imposed by setting the initial sequence y 0 as (<S>, C 1 , C 2 , ..., C k , </S>), where C i = (c 1 , c 2 , ..., c l ) is the i-th lexical constrained word, l is the number of tokens in the i-th con-straint, and k is the number of constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_37",
            "start": 131,
            "end": 411,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_38@0",
            "content": "However, such mandatory preservation of the constraints is not carried out during training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_38",
            "start": 0,
            "end": 90,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_38@1",
            "content": "During imitation learning, random deletion is applied for ground-truth y * to get the incomplete sentences y \u2032 , producing the data samples for expert policies of how to insert from y \u2032 to y * .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_38",
            "start": 92,
            "end": 285,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_38@2",
            "content": "This leads to a situation where the model does not learn to preserve fixed tokens and organize the translation around the tokens.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_38",
            "start": 287,
            "end": 415,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_38@3",
            "content": "Such discrepancy could harm the applications of soft constrained translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_38",
            "start": 417,
            "end": 493,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_39@0",
            "content": "To solve this problem, we propose a simple but effective Constrained Training (CT) algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_39",
            "start": 0,
            "end": 92,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_39@1",
            "content": "We first build pseudo terms from the target by sampling 0-3 words from reference as the pre-defined constraints for training.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_39",
            "start": 94,
            "end": 218,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_39@2",
            "content": "2 Afterward, we disallow the deletion of pseudo term tokens during building data samples for imitation learning.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_39",
            "start": 220,
            "end": 331,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_39@3",
            "content": "This encourages the model to edit incomplete sentences containing lexical constraints into complete ones, bridging the gap between training and inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_39",
            "start": 333,
            "end": 486,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_40@0",
            "content": "Alignment Prompting",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_40",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_41@0",
            "content": "As stated in \u00a73.3, we assume the rareness of constraints hinders the model to insert proper tokens of its contexts (i.e., a stranger's neighbors are also strangers).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_41",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_41@1",
            "content": "To make the matter worse, previous research (Ding et al., 2021) has also shown that lexical choice errors on low-frequency words tend to be propagated from the teacher (an AT model) to the student (an NAT model) in knowledge distillation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_41",
            "start": 166,
            "end": 403,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_42@0",
            "content": "However, terminologies, by nature, provide hard alignment information for source and target which the model can conveniently utilize.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_42",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_42@1",
            "content": "Thus, on top of constrained training, we propose an enhanced approach named Aligned Constrained Training (ACT).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_42",
            "start": 134,
            "end": 244,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_42@2",
            "content": "We propose to directly align the target-side constraints with the source words and prompt the alignment information to the model during both training and inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_42",
            "start": 246,
            "end": 409,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_43@0",
            "content": "Building Alignment for Constraints We first align the source words to the target-side constraints, which are either pseudo constraints during training or actual constraints during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_43",
            "start": 0,
            "end": 189,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_43@1",
            "content": "For each translated sentence constraints et al., 1993;Och and Ney, 2003), to find the corresponding source words, denoted as",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_43",
            "start": 191,
            "end": 314,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_44@0",
            "content": "C tgt = (C 1 , C 2 , ..., C k ),",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_44",
            "start": 0,
            "end": 31,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_45@0",
            "content": "C src = (C \u2032 1 , C \u2032 2 , ..., C \u2032 k ).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_45",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_46@0",
            "content": "Prompting Alignment into LevT The encoder in LevT, besides token embedding and position embedding, is further added with a learnable alignment embedding that comes from C src and C tgt .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_46",
            "start": 0,
            "end": 185,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_46@1",
            "content": "We set the alignment value for each token in C \u2032 i to i and the others to 0, which are further encoded into embeddings.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_46",
            "start": 187,
            "end": 305,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_46@2",
            "content": "The prompting of alignment is not limited to training, as we also add such alignment embeddings to source tokens aligned to target-side constraints during inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_46",
            "start": 307,
            "end": 471,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_47@0",
            "content": "Experiments",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_47",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_48@0",
            "content": "Data and Evaluation",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_48",
            "start": 0,
            "end": 18,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_49@0",
            "content": "Parallel Data and Knowledge Distillation We consider the English\u2192German (En\u2192De) translation task and train all of the MT models on WMT14 En-De (3,961K sentence pairs), a benchmark translation dataset.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_49",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_49@1",
            "content": "All sentences are pre-processed via byte-pair encoding (BPE) (Sennrich et al., 2016) into sub-word units.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_49",
            "start": 201,
            "end": 305,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_49@2",
            "content": "Following the common practice of training an NAT model, we use the sentencelevel knowledge distillation data generated by a Transformer, (Vaswani et al., 2017) provided by Kasai et al. (2020).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_49",
            "start": 307,
            "end": 498,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_50@0",
            "content": "Given models trained on the above-mentioned training sets, we evaluate them on the test sets of several lexically constrained translation datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_50",
            "start": 0,
            "end": 146,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_50@1",
            "content": "These test sets are categorized into two types of standard lexically constrained translation datasets: 1) Type#1: tasks from WMT14 (Vaswani et al., 2017) and WMT17 (Bojar et al., 2017), which are of the same general domain (news) as training sets; 2) Type#2: tasks from OPUS (Tiedemann, 2012) that are of specific domains (medical and law).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_50",
            "start": 148,
            "end": 487,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_50@2",
            "content": "Particularly, the real application scenarios of lexically constrained MT models are usually domain-specific, and the constrained words in these domain datasets are relatively less frequent and more important.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_50",
            "start": 489,
            "end": 696,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_51@0",
            "content": "Following previous work (Dinu et al., 2019;Susanto et al., 2020;Xu and Carpuat, 2021), the lexical constraints in Type#1 tasks are extracted from existing terminology databases such as Interactive Terminology for Europe (IATE) 3 and Wiktionary (WIKT) 4 accordingly.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_51",
            "start": 0,
            "end": 264,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_51@1",
            "content": "The OPUS-EMEA (medical domain) and OPUS-JRC (legal domain) in Type#2 tasks are datasets from OPUS.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_51",
            "start": 266,
            "end": 363,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_51@2",
            "content": "The constraints are extracted by randomly sampling 1 to 3 words from the reference (Post and Vilar, 2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_51",
            "start": 365,
            "end": 470,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_51@3",
            "content": "These constraints are then tokenized with BPE, yielding a larger number of tokens as constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_51",
            "start": 472,
            "end": 568,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_51@4",
            "content": "The statistical report is shown in Table 3, indicating the frequencies of Type#2 datasets are generally much lower than Type#1 ones.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_51",
            "start": 570,
            "end": 701,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_52@0",
            "content": "Evaluation Metrics We use BLEU (Papineni et al., 2002) for estimating the general quality of translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_52",
            "start": 0,
            "end": 104,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_52@1",
            "content": "We also use Term Usage Rate (Term%, Dinu et al., 2019;Susanto et al., 2020;Lee et al., 2021) to evaluate lexically constrained translation, which is the ratio of term constraints appearing in the translated text.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_52",
            "start": 106,
            "end": 317,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_53@0",
            "content": "Models",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_53",
            "start": 0,
            "end": 5,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_54@0",
            "content": "We use Levenshtein Transformer (LevT, Gu et al., 2019) as the backbone model to ACT algorithm for constrained NAT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_54",
            "start": 0,
            "end": 113,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_54@1",
            "content": "We compare our approach with a series of previous MT models on applying lexical constraints:",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_54",
            "start": 115,
            "end": 206,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_55@0",
            "content": "\u2022 Transformer (Vaswani et al., 2017), set as the AT baseline; \u2022 Dynamic Beam Allocation (DBA) (Post and Vilar, 2018) for constrained decoding with dynamic beam allocation over Transformer; \u2022 Train-by-sep (Dinu et al., 2019), trained on augmented code-switched data by replacing the source terms with target constraints or append on source terms during training; \u2022 Constrained LevT (Susanto et al., 2020), which develops LevT (Gu et al., 2019) \u2022 EDITOR (Xu and Carpuat, 2021), a variant of LevT, replacing the delete action with a reposition action.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_55",
            "start": 0,
            "end": 547,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_56@0",
            "content": "Implementation Details We use and extend the FairSeq framework (Ott et al., 2019) for training our models.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_56",
            "start": 0,
            "end": 105,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_56@1",
            "content": "We keep mostly the default parameters of FairSeq, such as setting d model = 512, d hidden = 2,048, n heads = 8, n layers = 6 and p dropout = 0.3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_56",
            "start": 107,
            "end": 251,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_56@2",
            "content": "The learning rate is set as 0.0005, the warmup step is set as 4,000 steps.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_56",
            "start": 253,
            "end": 326,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_56@3",
            "content": "All models are trained with a batch size of 16,000 tokens for maximum of 300,000 steps with Adam optimizer (Kingma and Ba, 2014) on 2 NVIDIA GeForce RTX 3090 GPUs with gradient accumulation of 4 batches.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_56",
            "start": 328,
            "end": 530,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_56@4",
            "content": "Checkpoints for testing are selected from the average weights of the last 5 checkpoints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_56",
            "start": 532,
            "end": 619,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_56@5",
            "content": "For Transformer (Vaswani et al., 2017), we use the checkpoint released by Ott et al. (2018).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_56",
            "start": 621,
            "end": 712,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_57@0",
            "content": "Main Results",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_57",
            "start": 0,
            "end": 11,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_58@0",
            "content": "Table 4 reports the performance of LevT with ACT (as well as the CT ablation) on the type 1 tasks (WIKT and IATE as terminologies), compared with baselines.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_58",
            "start": 0,
            "end": 155,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_58@1",
            "content": "In general, the results indicate the proposed CT/ACT algorithms achieve a consistent gain in performance, term coverage, and speed over the backbone model mainly in the setting of constrained translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_58",
            "start": 157,
            "end": 360,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_59@0",
            "content": "When translating with soft constraints, i.e., the constraints need not appear in the output, adding ACT to LevT helps preserve the terminology constraints (+\u223c5 Term%) and improves translation performance (+0.31-0.88 on BLEU).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_59",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_59@1",
            "content": "If we enforce hard constraints, the term usage rate doubtlessly reaches 100%, with reasonable improvements on BLEU.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_59",
            "start": 226,
            "end": 340,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_59@2",
            "content": "When translating without constraints, however, adding ACT does not bring consistent improvements as hard and soft constraints do, which could be attributed to the discrepancy between training and inference.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_59",
            "start": 342,
            "end": 547,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_60@0",
            "content": "As for the ablation for CT and ACT, we have two observations: 1) term usage rate increases mainly because of CT, and can be further improved by ACT; 2) translation quality (BLEU) increases due to the additional hard alignment of ACT over CT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_60",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_60@1",
            "content": "The former could be attributed to the behavior of not deleting the constraints in CT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_60",
            "start": 242,
            "end": 326,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_60@2",
            "content": "The latter is because of the introduction of source-side information of constraints that familiarize the model with the constraint context.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_60",
            "start": 328,
            "end": 466,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_61@0",
            "content": "Table 3 also shows the efficiency advantage of non-autoregressive methods compared with autoregressive ones, which is widely reported in the NAT research literature.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_61",
            "start": 0,
            "end": 164,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_61@1",
            "content": "The proposed methods do not cause drops in translation speed against the backbone LevT. When translating with lexical constraints, LevT with CT or ACT is even faster than LevT. In contrast, constrained decoding methods for autoregressive models (i.e., DBA) nearly double the translation latency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_61",
            "start": 166,
            "end": 460,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_61@2",
            "content": "Since the main purpose of non-autoregressive research is developing efficient algorithms, such findings could facilitate the industrial usage for constrained translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_61",
            "start": 462,
            "end": 631,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_62@0",
            "content": "Translation Results on Domain Datasets For a generalized evaluation of our methods, we apply the models trained on the general domain dataset (WMT14 En-De) to medical (OPUS-EMEA) and legal domains (OPUS-JRC).",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_62",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_62@1",
            "content": "As seen in Table 5, even greater performance boosts are witnessed.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_62",
            "start": 209,
            "end": 274,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_62@2",
            "content": "When trained with ACT, both term usage (+\u223c8-10 Term%) and translation performance (up to 4 BLEU points) largely increase, which is more significant than the general domain.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_62",
            "start": 276,
            "end": 447,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_63@0",
            "content": "The reason behind this observation is that the backbone LevT would have a hard time recognizing them as constraints since the lexical constraints in these datasets are much rarer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_63",
            "start": 0,
            "end": 178,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_63@1",
            "content": "Therefore, forcing LevT to translate with these rare constraints would generate worse text, e.g., BLEU drops for 2.45 points on OPUS-JRC than with soft constraints. And when translating with soft constraints, LevT over-deletes these rare constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_63",
            "start": 180,
            "end": 429,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_63@2",
            "content": "In contrast, the context information around constraints is effectively pin-pointed by ACT, so ACT would know the context (\"neighbors\") of the rare constraint (\"strangers\") and insert the translated context around the lexical constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_63",
            "start": 431,
            "end": 667,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_63@3",
            "content": "In this way, more terms are preserved by ACT, and the translation achieves better results.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_63",
            "start": 669,
            "end": 758,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_64@0",
            "content": "Analysis",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_64",
            "start": 0,
            "end": 7,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_65@0",
            "content": "Self-Constrained Translation Revisited",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_65",
            "start": 0,
            "end": 37,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_66@0",
            "content": "As a direct response to our motivation in this paper, we revisit the ablation study of self-constrained NAT in \u00a73.3 with the proposed ACT algorithm.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_66",
            "start": 0,
            "end": 147,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_66@1",
            "content": "Same as before, we build self-constraints from each target sentence and sort them by frequency.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_66",
            "start": 149,
            "end": 243,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_66@2",
            "content": "As shown in Figure 2(a), different from constrained LevT that suffers from Drop#2 ( \u00a73.3), ACT managed to handle this scenario pretty well.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_66",
            "start": 245,
            "end": 383,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_66@3",
            "content": "Following the motivations given in \u00a73.3, when constraints become rarer, ACT successfully breaks the trade-off with better understanding of the provided contextual information.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_66",
            "start": 385,
            "end": 559,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_67@0",
            "content": "[0 % ,1 0% ) [1 0% ,3 0% ) [3",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_67",
            "start": 0,
            "end": 28,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_68@0",
            "content": "What if the self-constraints are sorted based on TF-IDF?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_68",
            "start": 0,
            "end": 55,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_68@1",
            "content": "We also study the importance of different words in a sentence via TF-IDF by forcing them as constraints. As results in Figure 2(b) show, we have very similar observations from frequencybased self-constraints at Figure 2(a), and the gap between LevT and LevT + ACT is even higher as TF-IDF score reaches the highest.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_68",
            "start": 57,
            "end": 371,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_69@0",
            "content": "How does ACT perform under different kinds of lexical constraints?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_69",
            "start": 0,
            "end": 65,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_70@0",
            "content": "The experiments in \u00a76.1 create pseudo lexical constraints by traversing the target-side reference for understanding the proposed ACT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_70",
            "start": 0,
            "end": 132,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_70@1",
            "content": "In the following analyses, we study different properties of lexical constraints, e.g., frequency and numbers, and how they affect constrained translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_70",
            "start": 134,
            "end": 287,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_70@2",
            "content": "Are improvements by ACT robust against constraints of different frequencies?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_70",
            "start": 289,
            "end": 364,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_70@3",
            "content": "Given terminology constraints in the samples, we sort them by (averaged) frequency and evenly divide the corresponding data samples into high, medium and low categories.The results on translation quality of each category for the En\u2192De translation tasks are presented in Table 6.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_70",
            "start": 366,
            "end": 643,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_70@4",
            "content": "We find that LevT benefits mostly from ACT in the scenarios of lowerfrequency terms for three datasets.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_70",
            "start": 645,
            "end": 747,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_70@5",
            "content": "Although, in some settings such as HIGH in WMT14-WIKT and MED in WMT17-WIKT, the introduction of ACT for constrained LevT seems to bring performance drops for those higher-frequency terms.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_70",
            "start": 749,
            "end": 936,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_70@6",
            "content": "Since terms from IATE are rarer than WIKT as in Table 3, the improvements brought by ACT are steady.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_70",
            "start": 938,
            "end": 1037,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_71@0",
            "content": "Are improvements by ACT robust against constraints of different numbers?",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_71",
            "start": 0,
            "end": 71,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_71@1",
            "content": "In more practical settings, the number of constraints is usually more than one.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_71",
            "start": 73,
            "end": 151,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_71@2",
            "content": "To simulate this, we randomly sample 1-5 words from each reference as lexical constraints, and results are presented in Figure 3.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_71",
            "start": 153,
            "end": 281,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_71@3",
            "content": "We find that, as the number of constraints grows, the translation quality ostensibly becomes better for LevT with or without ACT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_71",
            "start": 283,
            "end": 411,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_71@4",
            "content": "And ACT consistently brings extra improvements, indicating the help by ACT for constrained decoding in constrained NAT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_71",
            "start": 413,
            "end": 531,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_72@0",
            "content": "Limitations",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_72",
            "start": 0,
            "end": 10,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_73@0",
            "content": "Although the proposed ACT algorithm is effective to improve NAT models on constrained translation, we also find it does not bring much performance gain on translation quality (i.e., BLEU) over the backbone LevT for unconstrained translation.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_73",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_73@1",
            "content": "The results on the full set of WMT14 En\u2192De test set further corroborate this finding, which is shown in Appendix A. Another limitation of our work is that we do not propose a new paradigm for constrained NAT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_73",
            "start": 242,
            "end": 449,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_73@2",
            "content": "The purpose of this work is to enhance existing methods for constrained NAT, i.e., editing-based iterative NAT methods, under rare lexical constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_73",
            "start": 451,
            "end": 601,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_73@3",
            "content": "It would be interesting for future research to explore new ways to impose lexical constraints on NAT models, perhaps on non-iterative NAT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_73",
            "start": 603,
            "end": 740,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_74@0",
            "content": "Note that, machine translation in real scenario still falls behind human performance.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_74",
            "start": 0,
            "end": 84,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_74@1",
            "content": "Moreover, since we primary focus on improving constrained NAT, real applications calls for refinement in various aspects that we do not consider in this work.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_74",
            "start": 86,
            "end": 243,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_75@0",
            "content": "Conclusion",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_75",
            "start": 0,
            "end": 9,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_76@0",
            "content": "In this work, we propose a plug-in algorithm (ACT) to improve lexically constrained nonautoregressive translation, especially under lowfrequency constraints.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_76",
            "start": 0,
            "end": 156,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_76@1",
            "content": "ACT bridges the gap between training and constrained inference and prompts the context information of the constraints to the constrained NAT model.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_76",
            "start": 158,
            "end": 304,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_76@2",
            "content": "Experiments show that ACT improves translation quality and term preservation over the backbone NAT model Levenshtein Transformer.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_76",
            "start": 306,
            "end": 434,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_76@3",
            "content": "Further analyses show that the findings are consistent over constraints varied from frequency, TF-IDF, and lengths.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_76",
            "start": 436,
            "end": 550,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_76@4",
            "content": "In the future, we will explore the application of this approach to more languages.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_76",
            "start": 552,
            "end": 633,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_76@5",
            "content": "We also encourage future research to explore a new paradigm of constrained NAT methods beyond editing-based iterative NAT.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_76",
            "start": 635,
            "end": 756,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_77@0",
            "content": "Melissa Ailem, Jingshu Liu, Raheel Qader, Encouraging neural machine translation to satisfy terminology constraints, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_77",
            "start": 0,
            "end": 199,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_78@0",
            "content": "Tamer Alkhouli, Gabriel Bretschner, Hermann Ney, On the alignment problem in multi-head attention-based neural machine translation, 2018, Proceedings of the Third Conference on Machine Translation: Research Papers, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_78",
            "start": 0,
            "end": 256,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_79@0",
            "content": "Dzmitry Bahdanau, Kyung Cho, Yoshua Bengio, Neural machine translation by jointly learning to align and translate, 2015, 3rd International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_79",
            "start": 0,
            "end": 179,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_80@0",
            "content": "UNKNOWN, None, , Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_80",
            "start": 0,
            "end": 195,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_81@0",
            "content": "Ond\u0159ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, Marco Turchi, Findings of the 2017 conference on machine translation (WMT17), 2017, Proceedings of the Second Conference on Machine Translation, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_81",
            "start": 0,
            "end": 370,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_82@0",
            "content": "F Peter, Stephen Brown, Vincent Pietra, Robert Della Pietra,  Mercer, The mathematics of statistical machine translation: Parameter estimation, 1993, Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_82",
            "start": 0,
            "end": 177,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_83@0",
            "content": "Guanhua Chen, Yun Chen, O Victor,  Li, Lexically constrained neural machine translation with explicit alignment guidance, 2021, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_83",
            "start": 0,
            "end": 191,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_84@0",
            "content": "Guanhua Chen, Yun Chen, Yong Wang, O Victor,  Li, Lexical-constraint-aware neural machine translation via data augmentation, 2020, Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI-20, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_84",
            "start": 0,
            "end": 232,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_85@0",
            "content": "UNKNOWN, None, 2016, , .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_85",
            "start": 0,
            "end": 23,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_86@0",
            "content": "Liang Ding, Longyue Wang, Xuebo Liu, Derek Wong, Dacheng Tao, Zhaopeng Tu, Understanding and improving lexical choice in nonautoregressive translation, 2021, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_86",
            "start": 0,
            "end": 212,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_87@0",
            "content": "Georgiana Dinu, Prashant Mathur, Marcello Federico, Yaser Al-Onaizan, Training neural machine translation to apply terminology constraints, 2019, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_87",
            "start": 0,
            "end": 276,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_88@0",
            "content": "Cunxiao Du, Zhaopeng Tu, Jing Jiang, Orderagnostic cross entropy for non-autoregressive machine translation, 2021, Proc. of ICML, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_88",
            "start": 0,
            "end": 130,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_89@0",
            "content": "Marjan Ghazvininejad, Omer Levy, Yinhan Liu, Luke Zettlemoyer, Mask-predict: Parallel decoding of conditional masked language models, 2019, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_89",
            "start": 0,
            "end": 317,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_90@0",
            "content": "Jiatao Gu, James Bradbury, Caiming Xiong, O Victor, Richard Li,  Socher, Non-autoregressive neural machine translation, 2018, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_90",
            "start": 0,
            "end": 180,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_91@0",
            "content": "Jiatao Gu, Changhan Wang, Junbo Zhao, Levenshtein transformer, 2019, Advances in Neural Information Processing Systems, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_91",
            "start": 0,
            "end": 120,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_92@0",
            "content": "Junliang Guo, Zhirui Zhang, Linli Xu,  Hao-Ran, Boxing Wei, Enhong Chen,  Chen, Incorporating bert into parallel sequence decoding with adapters, 2020, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_92",
            "start": 0,
            "end": 225,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_93@0",
            "content": "Chris Hokamp, Qun Liu, Lexically constrained decoding for sequence generation using grid beam search, 2017, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_93",
            "start": 0,
            "end": 238,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_94@0",
            "content": "Lihua Qian, Hao Zhou, Yu Bao, Mingxuan Wang, Lin Qiu, Weinan Zhang, Yong Yu, Lei Li, Glancing transformer for non-autoregressive neural machine translation, 2021, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_94",
            "start": 0,
            "end": 338,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_95@0",
            "content": "Rico Sennrich, Barry Haddow, Alexandra Birch, Neural machine translation of rare words with subword units, 2016, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, Long Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_95",
            "start": 0,
            "end": 213,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_96@0",
            "content": "Raphael Shu, Jason Lee, Hideki Nakayama, Kyunghyun Cho, Latent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_96",
            "start": 0,
            "end": 240,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_97@0",
            "content": "Kai Song, Kun Wang, Heng Yu, Yue Zhang, Zhongqiang Huang, Weihua Luo, Xiangyu Duan, Min Zhang, Alignment-enhanced transformer for constraining nmt with pre-specified translations, 2020, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_97",
            "start": 0,
            "end": 249,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_98@0",
            "content": "Kai Song, Yue Zhang, Heng Yu, Weihua Luo, Kun Wang, Min Zhang, Code-switching for enhancing NMT with pre-specified translation, 2019, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Long and Short Papers.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_98",
            "start": 0,
            "end": 299,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_99@0",
            "content": "Mitchell Stern, William Chan, Jamie Kiros, Jakob Uszkoreit, Insertion transformer: Flexible sequence generation via insertion operations, 2019, Proceedings of the 36th International Conference on Machine Learning, PMLR.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_99",
            "start": 0,
            "end": 218,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_100@0",
            "content": "Raymond Hendy Susanto, Shamil Chollampatt, Liling Tan, Lexically constrained neural machine translation with Levenshtein transformer, 2020, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_100",
            "start": 0,
            "end": 270,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_101@0",
            "content": "J\u00f6rg Tiedemann, Parallel data, tools and interfaces in OPUS, 2012, Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_101",
            "start": 0,
            "end": 166,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_102@0",
            "content": "Ferhan Ture, Douglas Oard, Philip Resnik, consistent translation choices, 2012, Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_102",
            "start": 0,
            "end": 224,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_103@0",
            "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Illia Kaiser,  Polosukhin, Attention is all you need, 2017, Advances in Neural Information Processing Systems, Curran Associates, Inc.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_103",
            "start": 0,
            "end": 219,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_104@0",
            "content": "Yiren Wang, Fei Tian, Di He, Tao Qin, Chengxiang Zhai, Tie-Yan Liu, Non-autoregressive machine translation with auxiliary regularization, 2019, Proceedings of the AAAI Conference on Artificial Intelligence, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_104",
            "start": 0,
            "end": 207,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_105@0",
            "content": ", EDITOR: An Edit-Based Transformer with Repositioning for Neural Machine Translation with Soft Lexical Constraints, , Transactions of the Association for Computational Linguistics, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_105",
            "start": 0,
            "end": 182,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_106@0",
            "content": "Weijia Xu, Shuming Ma, Dongdong Zhang, Marine Carpuat, How does distilled data complexity impact the quality and confidence of nonautoregressive machine translation?, 2021, Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, Online. Association for Computational Linguistics.",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_106",
            "start": 0,
            "end": 298,
            "label": {}
        },
        {
            "ix": "315-ARR_v1_107@0",
            "content": "Chunting Zhou, Jiatao Gu, Graham Neubig, Understanding knowledge distillation in nonautoregressive machine translation, 2020, International Conference on Learning Representations, .",
            "ntype": "s",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            },
            "src_ix": "315-ARR_v1_107",
            "start": 0,
            "end": 180,
            "label": {}
        }
    ],
    "edges": [
        {
            "src_ix": "315-ARR_v1_0",
            "tgt_ix": "315-ARR_v1_1",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_0",
            "tgt_ix": "315-ARR_v1_1",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_1",
            "tgt_ix": "315-ARR_v1_2",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_1",
            "tgt_ix": "315-ARR_v1_2",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_0",
            "tgt_ix": "315-ARR_v1_3",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_2",
            "tgt_ix": "315-ARR_v1_3",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_4",
            "tgt_ix": "315-ARR_v1_5",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_5",
            "tgt_ix": "315-ARR_v1_6",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_6",
            "tgt_ix": "315-ARR_v1_7",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_7",
            "tgt_ix": "315-ARR_v1_8",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_8",
            "tgt_ix": "315-ARR_v1_9",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_9",
            "tgt_ix": "315-ARR_v1_10",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_10",
            "tgt_ix": "315-ARR_v1_11",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_3",
            "tgt_ix": "315-ARR_v1_4",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_3",
            "tgt_ix": "315-ARR_v1_5",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_3",
            "tgt_ix": "315-ARR_v1_6",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_3",
            "tgt_ix": "315-ARR_v1_7",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_3",
            "tgt_ix": "315-ARR_v1_8",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_3",
            "tgt_ix": "315-ARR_v1_9",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_3",
            "tgt_ix": "315-ARR_v1_10",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_3",
            "tgt_ix": "315-ARR_v1_11",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_3",
            "tgt_ix": "315-ARR_v1_4",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_0",
            "tgt_ix": "315-ARR_v1_12",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_11",
            "tgt_ix": "315-ARR_v1_12",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_13",
            "tgt_ix": "315-ARR_v1_14",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_12",
            "tgt_ix": "315-ARR_v1_13",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_12",
            "tgt_ix": "315-ARR_v1_14",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_12",
            "tgt_ix": "315-ARR_v1_13",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_0",
            "tgt_ix": "315-ARR_v1_15",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_14",
            "tgt_ix": "315-ARR_v1_15",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_15",
            "tgt_ix": "315-ARR_v1_16",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_15",
            "tgt_ix": "315-ARR_v1_16",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_17",
            "tgt_ix": "315-ARR_v1_18",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_18",
            "tgt_ix": "315-ARR_v1_19",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_19",
            "tgt_ix": "315-ARR_v1_20",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_16",
            "tgt_ix": "315-ARR_v1_17",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_16",
            "tgt_ix": "315-ARR_v1_18",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_16",
            "tgt_ix": "315-ARR_v1_19",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_16",
            "tgt_ix": "315-ARR_v1_20",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_16",
            "tgt_ix": "315-ARR_v1_17",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_15",
            "tgt_ix": "315-ARR_v1_21",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_20",
            "tgt_ix": "315-ARR_v1_21",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_22",
            "tgt_ix": "315-ARR_v1_23",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_23",
            "tgt_ix": "315-ARR_v1_24",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_24",
            "tgt_ix": "315-ARR_v1_25",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_25",
            "tgt_ix": "315-ARR_v1_26",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_28",
            "tgt_ix": "315-ARR_v1_29",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_29",
            "tgt_ix": "315-ARR_v1_30",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_22",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_23",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_24",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_25",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_26",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_27",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_28",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_29",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_30",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_22",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_15",
            "tgt_ix": "315-ARR_v1_31",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_30",
            "tgt_ix": "315-ARR_v1_31",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_33",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_31",
            "tgt_ix": "315-ARR_v1_32",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_31",
            "tgt_ix": "315-ARR_v1_33",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_31",
            "tgt_ix": "315-ARR_v1_32",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_0",
            "tgt_ix": "315-ARR_v1_34",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_33",
            "tgt_ix": "315-ARR_v1_34",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_34",
            "tgt_ix": "315-ARR_v1_35",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_34",
            "tgt_ix": "315-ARR_v1_35",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_34",
            "tgt_ix": "315-ARR_v1_36",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_35",
            "tgt_ix": "315-ARR_v1_36",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_37",
            "tgt_ix": "315-ARR_v1_38",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_38",
            "tgt_ix": "315-ARR_v1_39",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_36",
            "tgt_ix": "315-ARR_v1_37",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_36",
            "tgt_ix": "315-ARR_v1_38",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_36",
            "tgt_ix": "315-ARR_v1_39",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_36",
            "tgt_ix": "315-ARR_v1_37",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_34",
            "tgt_ix": "315-ARR_v1_40",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_39",
            "tgt_ix": "315-ARR_v1_40",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_41",
            "tgt_ix": "315-ARR_v1_42",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_42",
            "tgt_ix": "315-ARR_v1_43",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_43",
            "tgt_ix": "315-ARR_v1_44",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_44",
            "tgt_ix": "315-ARR_v1_45",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_45",
            "tgt_ix": "315-ARR_v1_46",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_40",
            "tgt_ix": "315-ARR_v1_41",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_40",
            "tgt_ix": "315-ARR_v1_42",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_40",
            "tgt_ix": "315-ARR_v1_43",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_40",
            "tgt_ix": "315-ARR_v1_44",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_40",
            "tgt_ix": "315-ARR_v1_45",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_40",
            "tgt_ix": "315-ARR_v1_46",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_40",
            "tgt_ix": "315-ARR_v1_41",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_0",
            "tgt_ix": "315-ARR_v1_47",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_46",
            "tgt_ix": "315-ARR_v1_47",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_47",
            "tgt_ix": "315-ARR_v1_48",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_47",
            "tgt_ix": "315-ARR_v1_48",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_48",
            "tgt_ix": "315-ARR_v1_49",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_48",
            "tgt_ix": "315-ARR_v1_49",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_50",
            "tgt_ix": "315-ARR_v1_51",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_51",
            "tgt_ix": "315-ARR_v1_52",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_48",
            "tgt_ix": "315-ARR_v1_50",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_48",
            "tgt_ix": "315-ARR_v1_51",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_48",
            "tgt_ix": "315-ARR_v1_52",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_49",
            "tgt_ix": "315-ARR_v1_50",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_47",
            "tgt_ix": "315-ARR_v1_53",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_52",
            "tgt_ix": "315-ARR_v1_53",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_54",
            "tgt_ix": "315-ARR_v1_55",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_53",
            "tgt_ix": "315-ARR_v1_54",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_53",
            "tgt_ix": "315-ARR_v1_55",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_53",
            "tgt_ix": "315-ARR_v1_56",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_53",
            "tgt_ix": "315-ARR_v1_54",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_47",
            "tgt_ix": "315-ARR_v1_57",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_56",
            "tgt_ix": "315-ARR_v1_57",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_58",
            "tgt_ix": "315-ARR_v1_59",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_59",
            "tgt_ix": "315-ARR_v1_60",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_60",
            "tgt_ix": "315-ARR_v1_61",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_61",
            "tgt_ix": "315-ARR_v1_62",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_62",
            "tgt_ix": "315-ARR_v1_63",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_57",
            "tgt_ix": "315-ARR_v1_58",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_57",
            "tgt_ix": "315-ARR_v1_59",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_57",
            "tgt_ix": "315-ARR_v1_60",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_57",
            "tgt_ix": "315-ARR_v1_61",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_57",
            "tgt_ix": "315-ARR_v1_62",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_57",
            "tgt_ix": "315-ARR_v1_63",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_57",
            "tgt_ix": "315-ARR_v1_58",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_0",
            "tgt_ix": "315-ARR_v1_64",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_63",
            "tgt_ix": "315-ARR_v1_64",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_64",
            "tgt_ix": "315-ARR_v1_65",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_64",
            "tgt_ix": "315-ARR_v1_65",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_66",
            "tgt_ix": "315-ARR_v1_67",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_67",
            "tgt_ix": "315-ARR_v1_68",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_65",
            "tgt_ix": "315-ARR_v1_66",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_65",
            "tgt_ix": "315-ARR_v1_67",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_65",
            "tgt_ix": "315-ARR_v1_68",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_65",
            "tgt_ix": "315-ARR_v1_66",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_64",
            "tgt_ix": "315-ARR_v1_69",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_68",
            "tgt_ix": "315-ARR_v1_69",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_70",
            "tgt_ix": "315-ARR_v1_71",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_69",
            "tgt_ix": "315-ARR_v1_70",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_69",
            "tgt_ix": "315-ARR_v1_71",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_69",
            "tgt_ix": "315-ARR_v1_70",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_64",
            "tgt_ix": "315-ARR_v1_72",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_71",
            "tgt_ix": "315-ARR_v1_72",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_73",
            "tgt_ix": "315-ARR_v1_74",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_72",
            "tgt_ix": "315-ARR_v1_73",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_72",
            "tgt_ix": "315-ARR_v1_74",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_72",
            "tgt_ix": "315-ARR_v1_73",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_0",
            "tgt_ix": "315-ARR_v1_75",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_74",
            "tgt_ix": "315-ARR_v1_75",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_75",
            "tgt_ix": "315-ARR_v1_76",
            "etype": "parent",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_75",
            "tgt_ix": "315-ARR_v1_76",
            "etype": "next",
            "meta": null
        },
        {
            "src_ix": "315-ARR_v1_0",
            "tgt_ix": "315-ARR_v1_0@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_1",
            "tgt_ix": "315-ARR_v1_1@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_2",
            "tgt_ix": "315-ARR_v1_2@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_2",
            "tgt_ix": "315-ARR_v1_2@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_2",
            "tgt_ix": "315-ARR_v1_2@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_2",
            "tgt_ix": "315-ARR_v1_2@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_2",
            "tgt_ix": "315-ARR_v1_2@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_2",
            "tgt_ix": "315-ARR_v1_2@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_2",
            "tgt_ix": "315-ARR_v1_2@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_3",
            "tgt_ix": "315-ARR_v1_3@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_4",
            "tgt_ix": "315-ARR_v1_4@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_4",
            "tgt_ix": "315-ARR_v1_4@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_5",
            "tgt_ix": "315-ARR_v1_5@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_5",
            "tgt_ix": "315-ARR_v1_5@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_5",
            "tgt_ix": "315-ARR_v1_5@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_5",
            "tgt_ix": "315-ARR_v1_5@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_6",
            "tgt_ix": "315-ARR_v1_6@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_6",
            "tgt_ix": "315-ARR_v1_6@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_7",
            "tgt_ix": "315-ARR_v1_7@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_8",
            "tgt_ix": "315-ARR_v1_8@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_8",
            "tgt_ix": "315-ARR_v1_8@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_8",
            "tgt_ix": "315-ARR_v1_8@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_8",
            "tgt_ix": "315-ARR_v1_8@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_8",
            "tgt_ix": "315-ARR_v1_8@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_8",
            "tgt_ix": "315-ARR_v1_8@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_9",
            "tgt_ix": "315-ARR_v1_9@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_9",
            "tgt_ix": "315-ARR_v1_9@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_10",
            "tgt_ix": "315-ARR_v1_10@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_10",
            "tgt_ix": "315-ARR_v1_10@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_10",
            "tgt_ix": "315-ARR_v1_10@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_10",
            "tgt_ix": "315-ARR_v1_10@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_10",
            "tgt_ix": "315-ARR_v1_10@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_11",
            "tgt_ix": "315-ARR_v1_11@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_11",
            "tgt_ix": "315-ARR_v1_11@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_11",
            "tgt_ix": "315-ARR_v1_11@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_12",
            "tgt_ix": "315-ARR_v1_12@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_13",
            "tgt_ix": "315-ARR_v1_13@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_13",
            "tgt_ix": "315-ARR_v1_13@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_13",
            "tgt_ix": "315-ARR_v1_13@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_13",
            "tgt_ix": "315-ARR_v1_13@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_13",
            "tgt_ix": "315-ARR_v1_13@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_13",
            "tgt_ix": "315-ARR_v1_13@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_13",
            "tgt_ix": "315-ARR_v1_13@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_13",
            "tgt_ix": "315-ARR_v1_13@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_14",
            "tgt_ix": "315-ARR_v1_14@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_14",
            "tgt_ix": "315-ARR_v1_14@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_14",
            "tgt_ix": "315-ARR_v1_14@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_14",
            "tgt_ix": "315-ARR_v1_14@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_15",
            "tgt_ix": "315-ARR_v1_15@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_16",
            "tgt_ix": "315-ARR_v1_16@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_17",
            "tgt_ix": "315-ARR_v1_17@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_17",
            "tgt_ix": "315-ARR_v1_17@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_18",
            "tgt_ix": "315-ARR_v1_18@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_19",
            "tgt_ix": "315-ARR_v1_19@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_20",
            "tgt_ix": "315-ARR_v1_20@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_21",
            "tgt_ix": "315-ARR_v1_21@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_22",
            "tgt_ix": "315-ARR_v1_22@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_22",
            "tgt_ix": "315-ARR_v1_22@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_22",
            "tgt_ix": "315-ARR_v1_22@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_23",
            "tgt_ix": "315-ARR_v1_23@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_24",
            "tgt_ix": "315-ARR_v1_24@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_24",
            "tgt_ix": "315-ARR_v1_24@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_24",
            "tgt_ix": "315-ARR_v1_24@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_24",
            "tgt_ix": "315-ARR_v1_24@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_25",
            "tgt_ix": "315-ARR_v1_25@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_25",
            "tgt_ix": "315-ARR_v1_25@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_25",
            "tgt_ix": "315-ARR_v1_25@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_26",
            "tgt_ix": "315-ARR_v1_26@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_27",
            "tgt_ix": "315-ARR_v1_27@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_28",
            "tgt_ix": "315-ARR_v1_28@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_28",
            "tgt_ix": "315-ARR_v1_28@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_29",
            "tgt_ix": "315-ARR_v1_29@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_30",
            "tgt_ix": "315-ARR_v1_30@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_30",
            "tgt_ix": "315-ARR_v1_30@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_30",
            "tgt_ix": "315-ARR_v1_30@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_30",
            "tgt_ix": "315-ARR_v1_30@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_31",
            "tgt_ix": "315-ARR_v1_31@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_32@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_32@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_32@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_32@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_32@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_32@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_32@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_32@7",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_32@8",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_32",
            "tgt_ix": "315-ARR_v1_32@9",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_33",
            "tgt_ix": "315-ARR_v1_33@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_33",
            "tgt_ix": "315-ARR_v1_33@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_33",
            "tgt_ix": "315-ARR_v1_33@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_34",
            "tgt_ix": "315-ARR_v1_34@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_35",
            "tgt_ix": "315-ARR_v1_35@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_35",
            "tgt_ix": "315-ARR_v1_35@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_36",
            "tgt_ix": "315-ARR_v1_36@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_37",
            "tgt_ix": "315-ARR_v1_37@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_37",
            "tgt_ix": "315-ARR_v1_37@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_38",
            "tgt_ix": "315-ARR_v1_38@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_38",
            "tgt_ix": "315-ARR_v1_38@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_38",
            "tgt_ix": "315-ARR_v1_38@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_38",
            "tgt_ix": "315-ARR_v1_38@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_39",
            "tgt_ix": "315-ARR_v1_39@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_39",
            "tgt_ix": "315-ARR_v1_39@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_39",
            "tgt_ix": "315-ARR_v1_39@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_39",
            "tgt_ix": "315-ARR_v1_39@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_40",
            "tgt_ix": "315-ARR_v1_40@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_41",
            "tgt_ix": "315-ARR_v1_41@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_41",
            "tgt_ix": "315-ARR_v1_41@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_42",
            "tgt_ix": "315-ARR_v1_42@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_42",
            "tgt_ix": "315-ARR_v1_42@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_42",
            "tgt_ix": "315-ARR_v1_42@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_43",
            "tgt_ix": "315-ARR_v1_43@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_43",
            "tgt_ix": "315-ARR_v1_43@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_44",
            "tgt_ix": "315-ARR_v1_44@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_45",
            "tgt_ix": "315-ARR_v1_45@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_46",
            "tgt_ix": "315-ARR_v1_46@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_46",
            "tgt_ix": "315-ARR_v1_46@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_46",
            "tgt_ix": "315-ARR_v1_46@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_47",
            "tgt_ix": "315-ARR_v1_47@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_48",
            "tgt_ix": "315-ARR_v1_48@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_49",
            "tgt_ix": "315-ARR_v1_49@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_49",
            "tgt_ix": "315-ARR_v1_49@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_49",
            "tgt_ix": "315-ARR_v1_49@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_50",
            "tgt_ix": "315-ARR_v1_50@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_50",
            "tgt_ix": "315-ARR_v1_50@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_50",
            "tgt_ix": "315-ARR_v1_50@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_51",
            "tgt_ix": "315-ARR_v1_51@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_51",
            "tgt_ix": "315-ARR_v1_51@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_51",
            "tgt_ix": "315-ARR_v1_51@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_51",
            "tgt_ix": "315-ARR_v1_51@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_51",
            "tgt_ix": "315-ARR_v1_51@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_52",
            "tgt_ix": "315-ARR_v1_52@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_52",
            "tgt_ix": "315-ARR_v1_52@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_53",
            "tgt_ix": "315-ARR_v1_53@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_54",
            "tgt_ix": "315-ARR_v1_54@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_54",
            "tgt_ix": "315-ARR_v1_54@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_55",
            "tgt_ix": "315-ARR_v1_55@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_56",
            "tgt_ix": "315-ARR_v1_56@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_56",
            "tgt_ix": "315-ARR_v1_56@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_56",
            "tgt_ix": "315-ARR_v1_56@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_56",
            "tgt_ix": "315-ARR_v1_56@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_56",
            "tgt_ix": "315-ARR_v1_56@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_56",
            "tgt_ix": "315-ARR_v1_56@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_57",
            "tgt_ix": "315-ARR_v1_57@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_58",
            "tgt_ix": "315-ARR_v1_58@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_58",
            "tgt_ix": "315-ARR_v1_58@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_59",
            "tgt_ix": "315-ARR_v1_59@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_59",
            "tgt_ix": "315-ARR_v1_59@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_59",
            "tgt_ix": "315-ARR_v1_59@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_60",
            "tgt_ix": "315-ARR_v1_60@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_60",
            "tgt_ix": "315-ARR_v1_60@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_60",
            "tgt_ix": "315-ARR_v1_60@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_61",
            "tgt_ix": "315-ARR_v1_61@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_61",
            "tgt_ix": "315-ARR_v1_61@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_61",
            "tgt_ix": "315-ARR_v1_61@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_62",
            "tgt_ix": "315-ARR_v1_62@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_62",
            "tgt_ix": "315-ARR_v1_62@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_62",
            "tgt_ix": "315-ARR_v1_62@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_63",
            "tgt_ix": "315-ARR_v1_63@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_63",
            "tgt_ix": "315-ARR_v1_63@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_63",
            "tgt_ix": "315-ARR_v1_63@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_63",
            "tgt_ix": "315-ARR_v1_63@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_64",
            "tgt_ix": "315-ARR_v1_64@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_65",
            "tgt_ix": "315-ARR_v1_65@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_66",
            "tgt_ix": "315-ARR_v1_66@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_66",
            "tgt_ix": "315-ARR_v1_66@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_66",
            "tgt_ix": "315-ARR_v1_66@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_66",
            "tgt_ix": "315-ARR_v1_66@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_67",
            "tgt_ix": "315-ARR_v1_67@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_68",
            "tgt_ix": "315-ARR_v1_68@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_68",
            "tgt_ix": "315-ARR_v1_68@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_69",
            "tgt_ix": "315-ARR_v1_69@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_70",
            "tgt_ix": "315-ARR_v1_70@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_70",
            "tgt_ix": "315-ARR_v1_70@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_70",
            "tgt_ix": "315-ARR_v1_70@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_70",
            "tgt_ix": "315-ARR_v1_70@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_70",
            "tgt_ix": "315-ARR_v1_70@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_70",
            "tgt_ix": "315-ARR_v1_70@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_70",
            "tgt_ix": "315-ARR_v1_70@6",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_71",
            "tgt_ix": "315-ARR_v1_71@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_71",
            "tgt_ix": "315-ARR_v1_71@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_71",
            "tgt_ix": "315-ARR_v1_71@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_71",
            "tgt_ix": "315-ARR_v1_71@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_71",
            "tgt_ix": "315-ARR_v1_71@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_72",
            "tgt_ix": "315-ARR_v1_72@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_73",
            "tgt_ix": "315-ARR_v1_73@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_73",
            "tgt_ix": "315-ARR_v1_73@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_73",
            "tgt_ix": "315-ARR_v1_73@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_73",
            "tgt_ix": "315-ARR_v1_73@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_74",
            "tgt_ix": "315-ARR_v1_74@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_74",
            "tgt_ix": "315-ARR_v1_74@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_75",
            "tgt_ix": "315-ARR_v1_75@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_76",
            "tgt_ix": "315-ARR_v1_76@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_76",
            "tgt_ix": "315-ARR_v1_76@1",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_76",
            "tgt_ix": "315-ARR_v1_76@2",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_76",
            "tgt_ix": "315-ARR_v1_76@3",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_76",
            "tgt_ix": "315-ARR_v1_76@4",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_76",
            "tgt_ix": "315-ARR_v1_76@5",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_77",
            "tgt_ix": "315-ARR_v1_77@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_78",
            "tgt_ix": "315-ARR_v1_78@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_79",
            "tgt_ix": "315-ARR_v1_79@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_80",
            "tgt_ix": "315-ARR_v1_80@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_81",
            "tgt_ix": "315-ARR_v1_81@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_82",
            "tgt_ix": "315-ARR_v1_82@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_83",
            "tgt_ix": "315-ARR_v1_83@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_84",
            "tgt_ix": "315-ARR_v1_84@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_85",
            "tgt_ix": "315-ARR_v1_85@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_86",
            "tgt_ix": "315-ARR_v1_86@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_87",
            "tgt_ix": "315-ARR_v1_87@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_88",
            "tgt_ix": "315-ARR_v1_88@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_89",
            "tgt_ix": "315-ARR_v1_89@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_90",
            "tgt_ix": "315-ARR_v1_90@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_91",
            "tgt_ix": "315-ARR_v1_91@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_92",
            "tgt_ix": "315-ARR_v1_92@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_93",
            "tgt_ix": "315-ARR_v1_93@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_94",
            "tgt_ix": "315-ARR_v1_94@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_95",
            "tgt_ix": "315-ARR_v1_95@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_96",
            "tgt_ix": "315-ARR_v1_96@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_97",
            "tgt_ix": "315-ARR_v1_97@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_98",
            "tgt_ix": "315-ARR_v1_98@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_99",
            "tgt_ix": "315-ARR_v1_99@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_100",
            "tgt_ix": "315-ARR_v1_100@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_101",
            "tgt_ix": "315-ARR_v1_101@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_102",
            "tgt_ix": "315-ARR_v1_102@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_103",
            "tgt_ix": "315-ARR_v1_103@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_104",
            "tgt_ix": "315-ARR_v1_104@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_105",
            "tgt_ix": "315-ARR_v1_105@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_106",
            "tgt_ix": "315-ARR_v1_106@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        },
        {
            "src_ix": "315-ARR_v1_107",
            "tgt_ix": "315-ARR_v1_107@0",
            "etype": "link",
            "meta": {
                "created_by": "IntertextSentenceSplitter_all"
            }
        }
    ],
    "prefix": "paper.tei",
    "meta": {
        "ix_counter": 1224,
        "sentence_split_type": "HybridSplitterLessAndLong",
        "sentence_split_model": "HybridSplitterLessAndLong_SciSpacy+Spacy",
        "position_tag_type": "from_draft",
        "doc_id": "315-ARR",
        "version": 1
    }
}